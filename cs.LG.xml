<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01440</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65306;&#20174;&#20803;&#23398;&#20064;&#21040;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#22270;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26089;&#26399;&#30340;&#25216;&#26415;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#20013;&#36816;&#34892;&#65292;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20805;&#36275;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20010;&#38480;&#21046;&#24341;&#21457;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#21482;&#26377;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#21487;&#29992;&#12290;&#37492;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#26412;&#32508;&#36848;&#35797;&#22270;&#32508;&#21512;&#26368;&#36817;&#30340;&#21457;&#23637;&#65292;&#25552;&#20379;&#27604;&#36739;&#24615;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;&#35835;&#32773;&#36827;&#34892;&#26041;&#27861;&#36873;&#25321;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>DPLM&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18567</link><description>&lt;p&gt;
&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Are Versatile Protein Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18567
&lt;/p&gt;
&lt;p&gt;
DPLM&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25193;&#25955;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;DPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#20855;&#26377;&#24378;&#22823;&#30340;&#29983;&#25104;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31181;&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#26694;&#26550;&#20013;&#20174;&#36827;&#21270;&#35268;&#27169;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#39044;&#35757;&#32451;&#21487;&#25193;&#23637;&#30340;DPLM&#65292;&#36825;&#20026;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;&#24314;&#27169;&#25552;&#20379;&#20102;&#22522;&#26412;&#26041;&#27861;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;DPLM&#23637;&#31034;&#20102;&#29983;&#25104;&#20986;&#31526;&#21512;&#32467;&#26500;&#30340;&#12289;&#26032;&#39062;&#30340;&#12289;&#22810;&#26679;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#24471;DPLM&#23545;&#34507;&#30333;&#36136;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35299;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20248;&#31168;&#30340;&#34920;&#31034;&#23398;&#20064;&#32773;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#19982;ESM2&#65288;Lin et al., 2022&#65289;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;DPLM&#21487;&#20197;&#38024;&#23545;&#21508;&#31181;&#38656;&#27714;&#36827;&#34892;&#23450;&#21046;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#30340;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18567v1 Announce Type: new  Abstract: This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#21518;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20154;&#31867;&#39044;&#27979;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.18563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Approaching Human-Level Forecasting with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#21518;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20154;&#31867;&#39044;&#27979;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#23545;&#25919;&#31574;&#21644;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;(LMs)&#26159;&#21542;&#33021;&#22815;&#22312;&#31454;&#20105;&#24615;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#27700;&#24179;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#26088;&#22312;&#33258;&#21160;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#12289;&#29983;&#25104;&#39044;&#27979;&#21644;&#32858;&#21512;&#39044;&#27979;&#12290;&#20026;&#20102;&#20419;&#36827;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#30340;&#22823;&#37327;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;&#22312;LM&#30340;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#21457;&#24067;&#30340;&#27979;&#35797;&#38598;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#19982;&#20154;&#31867;&#39044;&#27979;&#30340;&#32858;&#21512;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#25509;&#36817;&#20110;&#31454;&#20105;&#39044;&#27979;&#32773;&#30340;&#32858;&#21512;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#23427;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;LM&#26469;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20250;&#25552;&#20379;&#20934;&#30830;&#30340;&#22823;&#35268;&#27169;&#39044;&#27979;&#65292;&#24182;&#26377;&#21161;&#20110;&#20026;&#26426;&#26500;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18563v1 Announce Type: cross  Abstract: Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2402.18551</link><description>&lt;p&gt;
&#38544;&#24615;&#20559;&#35265;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18551
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65288;NTP&#65289;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39318;&#36873;&#33539;&#24335;&#65292;&#23427;&#28041;&#21450;&#39044;&#27979;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#19982;&#20256;&#32479;&#30340;&#29420;&#28909;&#20998;&#31867;&#19981;&#21516;&#65292;&#22312;NTP&#20013;&#65292;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#30340;&#26631;&#35760;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#21518;&#32487;&#12290;&#26412;&#25991;&#23558;NTP&#35757;&#32451;&#26694;&#26550;&#21270;&#20026;&#36328;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#65292;&#27599;&#20010;&#19978;&#19979;&#25991;&#37117;&#19982;&#26377;&#38480;&#35789;&#27719;&#34920;&#20013;&#30340;&#31232;&#30095;&#32463;&#39564;&#27010;&#29575;&#21521;&#37327;&#30456;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#23427;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#24403;NTP&#35757;&#32451;&#25439;&#22833;&#36798;&#21040;&#20854;&#19979;&#30028;&#65288;&#29109;&#65289;&#26102;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#26159;&#21542;&#20250;&#23545;&#20855;&#26377;&#29305;&#23450;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20559;&#35265;&#65311;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#65292;&#25105;&#20204;&#20570;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#19978;&#30340;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;GD&#33021;&#22815;&#36798;&#21040;&#20854;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18551v1 Announce Type: cross  Abstract: Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that th
&lt;/p&gt;</description></item><item><title>TOTEM&#27169;&#22411;&#22312;&#24212;&#23545;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18546</link><description>&lt;p&gt;
&#20256;&#24863;&#22120;&#25925;&#38556;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;Tokenization + Transformers &#23454;&#29616;&#26356;&#20581;&#22766;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18546
&lt;/p&gt;
&lt;p&gt;
TOTEM&#27169;&#22411;&#22312;&#24212;&#23545;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#21457;&#29616;&#33021;&#22815;&#27867;&#21270;&#30340;&#31070;&#32463;&#25968;&#25454;&#34920;&#31034;&#12290;&#36825;&#19968;&#30446;&#26631;&#21463;&#21040;&#35760;&#24405;&#20250;&#35805;&#65288;&#20363;&#22914;&#29615;&#22659;&#65289;&#12289;&#21463;&#35797;&#32773;&#65288;&#20363;&#22914;&#21464;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#65289;&#21644;&#20256;&#24863;&#22120;&#65288;&#20363;&#22914;&#20256;&#24863;&#22120;&#22122;&#22768;&#65289;&#31561;&#22240;&#32032;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#22987;&#35299;&#20915;&#36328;&#20250;&#35805;&#21644;&#21463;&#35797;&#32773;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#38024;&#23545;&#22312;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27867;&#21270;&#24615;&#32500;&#24230;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20250;&#35805;&#12289;&#21463;&#35797;&#32773;&#21644;&#20256;&#24863;&#22120;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65306;EEGNet&#65288;Lawhern&#31561;&#20154;&#65292;2018&#65289;&#21644;TOTEM&#65288;Talukder&#31561;&#20154;&#65292;2024&#65289;&#12290;EEGNet &#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780; TOTEM &#26159;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#26631;&#35760;&#22120;&#21644; Transformer &#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#27867;&#21270;&#26696;&#20363;&#20013;&#65292;TOTEM &#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#19982; EEGNet &#30456;&#21305;&#37197;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20998;&#26512; TOTEM &#30340;&#28508;&#22312;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18546v1 Announce Type: new  Abstract: A major goal in neuroscience is to discover neural data representations that generalize. This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others. Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments. In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024). EEGNet is a widely used convolutional neural network, while TOTEM is a discrete time series tokenizer and transformer model. We find that TOTEM outperforms or matches EEGNet across all generalizability cases. Finally through analysis of TOTEM's latent cod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017;&#65292;&#21363;&#22312;&#24494;&#35843;&#26102;&#19981;&#21253;&#21547;&#23433;&#20840;&#25552;&#31034;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21152;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;LLMs&#20013;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18540</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#21518;&#20445;&#25345;LLMs&#30340;&#23545;&#40784;&#24615;:&#25552;&#31034;&#27169;&#26495;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017;&#65292;&#21363;&#22312;&#24494;&#35843;&#26102;&#19981;&#21253;&#21547;&#23433;&#20840;&#25552;&#31034;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21152;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;LLMs&#20013;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;LLMs&#65292;&#22914;Llama 2-Chat&#65292;&#25512;&#21160;&#20102;LLM&#30740;&#31350;&#30340;&#24040;&#22823;&#27963;&#21160;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#21382;&#20102;&#23545;&#40784;&#24615;&#35757;&#32451;&#65292;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#26368;&#36817;&#65292;&#40784;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#25253;&#21578;&#31216;&#65292;&#21363;&#20351;&#26159;&#33391;&#24615;&#30340;&#24494;&#35843;&#65288;&#20363;&#22914;&#65292;&#22312;&#30475;&#20284;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#65289;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20943;&#36731;&#36825;&#31181;&#23545;&#40784;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;&#36890;&#36807;&#23545;&#20960;&#20010;&#32842;&#22825;&#27169;&#22411;&#65288;Meta&#30340;Llama 2-Chat&#65292;Mistral AI&#30340;Mistral 7B Instruct v0.2&#21644;OpenAI&#30340;GPT-3.5 Turbo&#65289;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#26412;&#25991;&#21457;&#29616;&#24494;&#35843;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#27169;&#26495;&#22312;&#20445;&#25345;&#23433;&#20840;&#23545;&#40784;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017; - &#22312;&#27979;&#35797;&#26102;&#19981;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21253;&#21547;&#23427;&#12290;&#23545;GSM8K&#65292;ChatDoctor&#21644;OpenOrca&#36827;&#34892;&#30340;&#24494;&#35843;&#23454;&#39564;&#34920;&#26126;&#65292;PTST&#26174;&#30528;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#22686;&#21152;&#65292;&#29978;&#33267;&#20960;&#20046;&#28040;&#38500;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18540v1 Announce Type: cross  Abstract: Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the "Pure Tuning, Safe Testing" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost elimin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36718;&#32974;X&#23556;&#32447;&#22270;&#20687;&#33258;&#21160;&#26816;&#27979;&#32570;&#38519;&#30340;&#31283;&#20581;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#21644;&#29616;&#20195;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#29305;&#24449;&#24037;&#31243;&#23545;&#20110;&#25552;&#39640;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20256;&#32479;&#29305;&#24449;&#22312;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18527</link><description>&lt;p&gt;
&#36718;&#32974;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#65306;&#20256;&#32479;&#26041;&#27861;&#36935;&#35265;&#28145;&#23618;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36718;&#32974;X&#23556;&#32447;&#22270;&#20687;&#33258;&#21160;&#26816;&#27979;&#32570;&#38519;&#30340;&#31283;&#20581;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#21644;&#29616;&#20195;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#29305;&#24449;&#24037;&#31243;&#23545;&#20110;&#25552;&#39640;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20256;&#32479;&#29305;&#24449;&#22312;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65288;&#22914;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#65288;LBP&#65289;&#21644;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#65288;GLCM&#65289;&#29305;&#24449;&#65292;&#20197;&#21450;&#22522;&#20110;&#20613;&#37324;&#21494;&#21644;&#23567;&#27874;&#30340;&#29305;&#24449;&#65289;&#20197;&#21450;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36718;&#32974;X&#23556;&#32447;&#22270;&#20687;&#33258;&#21160;&#26816;&#27979;&#32570;&#38519;&#30340;&#31283;&#20581;&#26041;&#27861;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#36718;&#32974;X&#23556;&#32447;&#22270;&#20687;&#22797;&#26434;&#22270;&#26696;&#21644;&#32441;&#29702;&#30340;&#25361;&#25112;&#30340;&#35748;&#35782;&#65292;&#24182;&#24378;&#35843;&#20102;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#26469;&#25552;&#39640;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#21512;&#19982;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#20998;&#31867;&#22120;&#36827;&#34892;&#31934;&#24515;&#25972;&#21512;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20808;&#36827;&#27169;&#22411;&#22914;YOLOv8&#36827;&#34892;&#27604;&#36739;&#65292;&#30740;&#31350;&#19981;&#20165;&#23545;&#20256;&#32479;&#29305;&#24449;&#22312;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#36824;&#25506;&#35752;&#20102;&#32463;&#20856;&#26041;&#27861;&#21644;&#29616;&#20195;&#26041;&#27861;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20256;&#32479;&#29305;&#24449;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#19982;&#20808;&#36827;&#27169;&#22411;&#23218;&#32654;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20248;&#24322;&#30340;&#32570;&#38519;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18527v1 Announce Type: cross  Abstract: This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques. Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance the performance of defect detection systems. By meticulously integrating combinations of these features with a Random Forest (RF) classifier and comparing them against advanced models like YOLOv8, the research not only benchmarks the performance of traditional features in defect detection but also explores the synergy between classical and modern approaches. The experimental results demonstrate that these traditional features, when fine-tuned a
&lt;/p&gt;</description></item><item><title>Log-NCDEs&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;NCDEs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Log-ODE&#26041;&#27861;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#36817;&#20284;CDE&#30340;&#35299;&#65292;&#24182;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18512</link><description>&lt;p&gt;
Log&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65306;&#26446;&#25324;&#21495;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18512
&lt;/p&gt;
&lt;p&gt;
Log-NCDEs&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;NCDEs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Log-ODE&#26041;&#27861;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#36817;&#20284;CDE&#30340;&#35299;&#65292;&#24182;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25511;&#24494;&#20998;&#26041;&#31243;&#65288;CDE&#65289;&#30340;&#30690;&#37327;&#22330;&#25551;&#36848;&#20102;&#25511;&#21046;&#36335;&#24452;&#19982;&#35299;&#36335;&#24452;&#28436;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#31070;&#32463;CDE&#65288;NCDE&#65289;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35270;&#20026;&#23545;&#25511;&#21046;&#36335;&#24452;&#30340;&#35266;&#27979;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;CDE&#30340;&#30690;&#37327;&#22330;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#23558;&#35299;&#36335;&#24452;&#20316;&#20026;&#25345;&#32493;&#28436;&#21270;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#30001;&#20110;&#20854;&#26500;&#36896;&#20351;&#20854;&#33021;&#22815;&#25269;&#25239;&#19981;&#35268;&#21017;&#37319;&#26679;&#29575;&#65292;NCDE&#26159;&#24314;&#27169;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#22312;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#65288;NRDE&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Log-NCDE&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;NCDE&#30340;&#26032;&#39062;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;Log-NCDE&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;Log-ODE&#26041;&#27861;&#65292;&#36825;&#26159;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#30340;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;CDE&#35299;&#30340;&#24037;&#20855;&#12290;&#22312;&#19968;&#31995;&#21015;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#65292;&#23637;&#31034;&#20102;Log-NCDE&#27604;NCDE&#65292;NRDE&#21644;&#20004;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;S5&#21644;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#27979;&#35797;&#38598;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18512v1 Announce Type: new  Abstract: The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurren
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.18510</link><description>&lt;p&gt;
RNNs&#36824;&#19981;&#26159;Transformer&#65306;&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;Transformer&#22312;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;RNNs&#26159;&#21542;&#33021;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36890;&#36807;Chain-of-Thought (CoT)&#25552;&#31034;&#65292;&#19982;Transformer&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;CoT&#21487;&#20197;&#25913;&#36827;RNNs&#65292;&#20294;&#26080;&#27861;&#24357;&#34917;&#19982;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20851;&#38190;&#29942;&#39048;&#22312;&#20110;RNNs&#26080;&#27861;&#23436;&#20840;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#21363;&#20351;&#32463;&#36807;CoT&#30340;&#22686;&#24378;&#65306;&#23545;&#20110;&#20960;&#20010;&#26126;&#30830;&#25110;&#38544;&#24335;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#22914;&#32852;&#24819;&#21484;&#22238;&#21644;&#30830;&#23450;&#22270;&#26159;&#21542;&#20026;&#26641;&#65292;&#25105;&#20204;&#35777;&#26126;RNNs&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;Transformer&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#22686;&#24378;RNNs&#19978;&#19979;&#25991;&#26816;&#32034;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
&lt;/p&gt;</description></item><item><title>&#20848;&#33457;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#21367;&#31215;&#26426;&#21046;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#21367;&#31215;&#26680;&#65292;&#23454;&#29616;&#20102;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.18508</link><description>&lt;p&gt;
&#20848;&#33457;&#65306;&#28789;&#27963;&#19988;&#25968;&#25454;&#30456;&#20851;&#30340;&#21367;&#31215;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18508
&lt;/p&gt;
&lt;p&gt;
&#20848;&#33457;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#21367;&#31215;&#26426;&#21046;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#21367;&#31215;&#26680;&#65292;&#23454;&#29616;&#20102;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#19981;&#26029;&#21457;&#23637;&#30340;&#26684;&#23616;&#20013;&#65292;&#24179;&#34913;&#34920;&#36798;&#33021;&#21147;&#19982;&#35745;&#31639;&#25928;&#29575;&#30340;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20848;&#33457;&#65288;Orchid&#65289;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#21253;&#21547;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#21367;&#31215;&#26426;&#21046;&#26469;&#37325;&#26032;&#26500;&#24819;&#24207;&#21015;&#24314;&#27169;&#12290;&#20848;&#33457;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#27880;&#24847;&#21147;&#26426;&#21046;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#25429;&#25417;&#36828;&#31243;&#20381;&#36182;&#24615;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20848;&#33457;&#30340;&#26680;&#24515;&#26159;&#25968;&#25454;&#30456;&#20851;&#21367;&#31215;&#23618;&#65292;&#23427;&#21033;&#29992;&#19987;&#38376;&#30340;&#26465;&#20214;&#21270;&#31070;&#32463;&#32593;&#32476;&#26681;&#25454;&#36755;&#20837;&#25968;&#25454;&#21160;&#24577;&#35843;&#25972;&#20854;&#21367;&#31215;&#26680;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#26465;&#20214;&#21270;&#32593;&#32476;&#65292;&#20197;&#22312;&#33258;&#36866;&#24212;&#21367;&#31215;&#25805;&#20316;&#20013;&#32500;&#25345;&#24179;&#31227;&#31561;&#21464;&#24615;&#12290;&#25968;&#25454;&#30456;&#20851;&#21367;&#31215;&#26680;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#21152;&#19978;&#38376;&#25511;&#25805;&#20316;&#65292;&#36171;&#20104;&#20102;&#20848;&#33457;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18508v1 Announce Type: new  Abstract: In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical. This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent convolution mechanism. Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and in-context learning. At the core of Orchid lies the data-dependent convolution layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in the adaptive convolution operation. The dynamic nature of data-dependent convolution kernel, coupled with gating operations, grants Orchid high expressivity while mai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#22522;&#22240;&#35268;&#21010;&#31639;&#27861;\ourmethod&#65292;&#20801;&#35768;&#29992;&#25143;&#21160;&#24577;&#20462;&#25913;&#35821;&#27861;&#20197;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#24182;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.18505</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;AutoML&#28436;&#36827;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
Evolving machine learning workflows through interactive AutoML
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#22522;&#22240;&#35268;&#21010;&#31639;&#27861;\ourmethod&#65292;&#20801;&#35768;&#29992;&#25143;&#21160;&#24577;&#20462;&#25913;&#35821;&#27861;&#20197;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#24182;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24037;&#20316;&#27969;&#32452;&#21512;&#65288;AWC&#65289;&#26159;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#20013;&#19968;&#20010;&#30456;&#20851;&#38382;&#39064;&#65292;&#23427;&#20801;&#35768;&#25214;&#21040;&#36866;&#24403;&#30340;&#39044;&#22788;&#29702;&#21644;&#39044;&#27979;&#27169;&#22411;&#24207;&#21015;&#20197;&#21450;&#23427;&#20204;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#20351;&#29992;&#28436;&#21270;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#22240;&#35268;&#21010;&#65288;G3P&#65289;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;\ourmethod&#65292;&#36825;&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;G3P&#31639;&#27861;&#65292;&#20801;&#35768;&#29992;&#25143;&#21160;&#24577;&#20462;&#25913;&#35821;&#27861;&#20197;&#21098;&#26525;&#25628;&#32034;&#31354;&#38388;&#24182;&#19987;&#27880;&#20110;&#20182;&#20204;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26159;&#39318;&#27425;&#23558;G3P&#26041;&#27861;&#30340;&#20248;&#21183;&#19982;&#20132;&#20114;&#24335;&#20248;&#21270;&#21644;&#20154;&#31867;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#24605;&#24819;&#30456;&#32467;&#21512;&#65292;&#36825;&#22312;AutoML&#30340;&#32972;&#26223;&#19979;&#24456;&#23569;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18505v1 Announce Type: new  Abstract: Automatic workflow composition (AWC) is a relevant problem in automated machine learning (AutoML) that allows finding suitable sequences of preprocessing and prediction models together with their optimal hyperparameters. This problem can be solved using evolutionary algorithms and, in particular, grammar-guided genetic programming (G3P). Current G3P approaches to AWC define a fixed grammar that formally specifies how workflow elements can be combined and which algorithms can be included. In this paper we present \ourmethod, an interactive G3P algorithm that allows users to dynamically modify the grammar to prune the search space and focus on their regions of interest. Our proposal is the first to combine the advantages of a G3P method with ideas from interactive optimisation and human-guided machine learning, an area little explored in the context of AutoML. To evaluate our approach, we present an experimental study in which 20 participa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ROG$_PL$&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#21407;&#22411;&#23398;&#20064;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24320;&#25918;&#38598;&#22270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18495</link><description>&lt;p&gt;
ROG$_{PL}$: &#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#21407;&#22411;&#23398;&#20064;&#23454;&#29616;&#31283;&#20581;&#30340;&#24320;&#25918;&#38598;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18495
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ROG$_PL$&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#21407;&#22411;&#23398;&#20064;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24320;&#25918;&#38598;&#22270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#22270;&#23398;&#20064;&#26159;&#19968;&#20010;&#23454;&#38469;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23545;&#24050;&#30693;&#20998;&#31867;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#26410;&#30693;&#31867;&#21035;&#26679;&#26412;&#35782;&#21035;&#20026;&#26410;&#30693;&#12290;&#20256;&#32479;&#30340;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#22312;&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#25152;&#36935;&#21040;&#30340;&#22797;&#26434;&#25968;&#25454;&#65292;&#22914;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#21644;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#22122;&#22768;&#12290;OOD&#25968;&#25454;&#26159;&#19981;&#23646;&#20110;&#20219;&#20309;&#24050;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#22914;&#26524;&#23427;&#20204;&#22312;&#35757;&#32451;&#20013;&#20986;&#29616;&#65288;OOD&#22122;&#22768;&#65289;&#65292;&#22312;&#27979;&#35797;&#20013;&#20986;&#29616;&#21017;&#20026;&#24320;&#25918;&#38598;&#26679;&#26412;&#12290;IND&#22122;&#22768;&#26159;&#34987;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;IND&#22122;&#22768;&#21644;OOD&#22122;&#22768;&#30340;&#23384;&#22312;&#26159;&#26222;&#36941;&#30340;&#65292;&#36890;&#24120;&#20250;&#24341;&#36215;&#27169;&#31946;&#38382;&#39064;&#65292;&#21253;&#25324;&#31867;&#20869;&#21464;&#21270;&#38382;&#39064;&#21644;&#31867;&#38388;&#28151;&#28102;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#31283;&#20581;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#26041;&#27861;&#26159;&#24517;&#35201;&#19988;&#22256;&#38590;&#30340;&#65292;&#23545;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#26469;&#35828;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ROG$_PL$&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18495v1 Announce Type: new  Abstract: Open-set graph learning is a practical task that aims to classify the known class nodes and to identify unknown class samples as unknowns. Conventional node classification methods usually perform unsatisfactorily in open-set scenarios due to the complex data they encounter, such as out-of-distribution (OOD) data and in-distribution (IND) noise. OOD data are samples that do not belong to any known classes. They are outliers if they occur in training (OOD noise), and open-set samples if they occur in testing. IND noise are training samples which are assigned incorrect labels. The existence of IND noise and OOD noise is prevalent, which usually cause the ambiguity problem, including the intra-class variety problem and the inter-class confusion problem. Thus, to explore robust open-set learning methods is necessary and difficult, and it becomes even more difficult for non-IID graph data.To this end, we propose a unified framework named ROG$_
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#26041;&#27861;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#26041;&#26696;&#65292;&#22312;&#21453;&#21521;&#29983;&#25104;&#24335;&#25193;&#25955;&#36807;&#31243;&#20013;&#25581;&#31034;&#20102;&#20998;&#21270;&#21644;&#22349;&#32553;&#20004;&#31181;&#36716;&#21464;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#30456;&#20851;&#30697;&#38453;&#36827;&#34892;&#35889;&#20998;&#26512;&#21644;&#20272;&#35745;&#25968;&#25454;&#20013;&#30340;&#36807;&#37327;&#29109;&#26469;&#25214;&#21040;&#36825;&#20004;&#31181;&#36716;&#21464;&#30340;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.18491</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Dynamical Regimes of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#26041;&#27861;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#26041;&#26696;&#65292;&#22312;&#21453;&#21521;&#29983;&#25104;&#24335;&#25193;&#25955;&#36807;&#31243;&#20013;&#25581;&#31034;&#20102;&#20998;&#21270;&#21644;&#22349;&#32553;&#20004;&#31181;&#36716;&#21464;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#30456;&#20851;&#30697;&#38453;&#36827;&#34892;&#35889;&#20998;&#26512;&#21644;&#20272;&#35745;&#25968;&#25454;&#20013;&#30340;&#36807;&#37327;&#29109;&#26469;&#25214;&#21040;&#36825;&#20004;&#31181;&#36716;&#21464;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31354;&#38388;&#32500;&#24230;&#21644;&#25968;&#25454;&#37327;&#36739;&#22823;&#65292;&#19988;&#35780;&#20998;&#20989;&#25968;&#24050;&#32463;&#34987;&#26368;&#20248;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;&#21453;&#21521;&#29983;&#25104;&#24335;&#25193;&#25955;&#36807;&#31243;&#20013;&#19977;&#31181;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#26041;&#26696;&#12290;&#29983;&#25104;&#21160;&#21147;&#23398;&#20174;&#32431;&#22122;&#22768;&#24320;&#22987;&#65292;&#39318;&#20808;&#36935;&#21040;&#8220;&#20998;&#21270;&#8221;&#36716;&#21464;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#30340;&#24635;&#20307;&#32467;&#26500;&#34987;&#25581;&#31034;&#20986;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#31867;&#20284;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26426;&#21046;&#12290;&#38543;&#21518;&#22312;&#36739;&#36831;&#30340;&#26102;&#38388;&#20986;&#29616;&#8220;&#22349;&#32553;&#8221;&#36716;&#21464;&#65292;&#20854;&#36807;&#31243;&#26159;&#21160;&#21147;&#23398;&#36712;&#36857;&#24320;&#22987;&#34987;&#21560;&#24341;&#21040;&#24050;&#32463;&#35760;&#24518;&#30340;&#25968;&#25454;&#28857;&#20043;&#19968;&#65292;&#27492;&#26426;&#21046;&#31867;&#20284;&#20110;&#29627;&#29827;&#30456;&#20013;&#30340;&#20957;&#32858;&#29616;&#35937;&#12290;&#23545;&#20110;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#20998;&#21270;&#26102;&#38388;&#21487;&#20197;&#36890;&#36807;&#23545;&#30456;&#20851;&#30697;&#38453;&#36827;&#34892;&#35889;&#20998;&#26512;&#24471;&#21040;&#65292;&#32780;&#22349;&#32553;&#26102;&#38388;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#20013;&#30340;&#8220;&#36807;&#37327;&#29109;&#8221;&#26469;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18491v1 Announce Type: new  Abstract: Using statistical physics methods, we study generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally. Our analysis reveals three distinct dynamical regimes during the backward generative diffusion process. The generative dynamics, starting from pure noise, encounters first a 'speciation' transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions. It is followed at later time by a 'collapse' transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase. For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an 'excess entropy' in the data. The dependence of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#26412;&#20851;&#27880;&#22240;&#32032;&#22312;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#20013;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#12289;AHP&#21644;&#22522;&#20110;&#30456;&#20284;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#26032;&#26041;&#27861;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#65292;&#24179;&#34913;&#20102;&#36816;&#33829;&#30446;&#26631;&#19982;&#20154;&#31867;&#33298;&#36866;&#21644;&#23433;&#20840;&#32771;&#34385;&#12290;</title><link>https://arxiv.org/abs/2402.18487</link><description>&lt;p&gt;
&#20154;&#26412;&#20851;&#27880;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#19982;AHP&#21450;&#22522;&#20110;&#30456;&#20284;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#22312;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#26412;&#20851;&#27880;&#22240;&#32032;&#22312;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#20013;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#12289;AHP&#21644;&#22522;&#20110;&#30456;&#20284;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#26032;&#26041;&#27861;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#65292;&#24179;&#34913;&#20102;&#36816;&#33829;&#30446;&#26631;&#19982;&#20154;&#31867;&#33298;&#36866;&#21644;&#23433;&#20840;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26080;&#20154;&#26426;&#65288;UAVs&#65289;&#21040;&#25628;&#32034;&#21644;&#25937;&#25588;&#65288;SAR&#65289;&#20219;&#21153;&#20013;&#20026;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#21644;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#25104;&#21151;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26080;&#20154;&#26426;&#30340;&#25216;&#26415;&#33021;&#21147;&#65292;&#20063;&#21462;&#20915;&#20110;&#23427;&#20204;&#19982;&#22320;&#38754;&#20154;&#21592;&#30340;&#25509;&#21463;&#21644;&#20114;&#21160;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#26412;&#22240;&#32032;&#22312;SAR&#20219;&#21153;&#20013;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23618;&#27425;&#20998;&#26512;&#36807;&#31243;&#21644;&#26032;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#32463;&#39564;&#22238;&#25918;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#65292;&#24179;&#34913;&#25805;&#20316;&#30446;&#26631;&#19982;&#20154;&#31867;&#33298;&#36866;&#24615;&#21644;&#23433;&#20840;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#39033;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#20154;&#26426;&#35774;&#35745;&#20013;&#24615;&#21035;&#26263;&#31034;&#21644;&#25311;&#20154;&#21270;&#23545;&#20844;&#20247;&#25509;&#21463;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23545;SAR&#20013;&#26080;&#20154;&#26426;&#20114;&#21160;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18487v1 Announce Type: cross  Abstract: The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue (SAR) missions presents a promising avenue for enhancing operational efficiency and effectiveness. However, the success of these missions is not solely dependent on the technical capabilities of the drones but also on their acceptance and interaction with humans on the ground. This paper explores the effect of human-centric factor in UAV trajectory planning for SAR missions. We introduce a novel approach based on the reinforcement learning augmented with Analytic Hierarchy Process and novel similarity-based experience replay to optimize UAV trajectories, balancing operational objectives with human comfort and safety considerations. Additionally, through a comprehensive survey, we investigate the impact of gender cues and anthropomorphism in UAV design on public acceptance and trust, revealing significant implications for drone interaction strategies in SAR. Our c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#20405;&#20837;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#31895;&#20998;&#36776;&#29575;&#27668;&#20505;&#27169;&#25311;&#21644;&#37327;&#21270;&#32597;&#35265;&#20107;&#20214;&#32479;&#35745;&#65292;&#24182;&#33021;&#22815;&#26657;&#27491;&#21160;&#24577;&#24182;&#37327;&#21270;&#20855;&#26377;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#38271;&#37325;&#29616;&#26399;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.18484</link><description>&lt;p&gt;
&#19968;&#20010;&#38750;&#20405;&#20837;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#31895;&#20998;&#36776;&#29575;&#27668;&#20505;&#27169;&#25311;&#21644;&#37327;&#21270;&#32597;&#35265;&#20107;&#20214;&#32479;&#35745;
&lt;/p&gt;
&lt;p&gt;
A non-intrusive machine learning framework for debiasing long-time coarse resolution climate simulations and quantifying rare events statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18484
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#20405;&#20837;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#31895;&#20998;&#36776;&#29575;&#27668;&#20505;&#27169;&#25311;&#21644;&#37327;&#21270;&#32597;&#35265;&#20107;&#20214;&#32479;&#35745;&#65292;&#24182;&#33021;&#22815;&#26657;&#27491;&#21160;&#24577;&#24182;&#37327;&#21270;&#20855;&#26377;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#38271;&#37325;&#29616;&#26399;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27668;&#20505;&#24555;&#36895;&#21464;&#21270;&#65292;&#39044;&#35745;&#26410;&#26469;&#20960;&#21313;&#24180;&#26497;&#31471;&#22825;&#27668;&#30340;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#23558;&#20250;&#22686;&#21152;&#12290; &#30001;&#20110;&#20840;&#20998;&#36776;&#29575;&#27668;&#20505;&#27169;&#25311;&#22312;&#35745;&#31639;&#19978;&#26159;&#26840;&#25163;&#30340;&#65292;&#25919;&#31574;&#21046;&#23450;&#32773;&#24517;&#39035;&#20381;&#36182;&#20110;&#31895;&#27169;&#22411;&#26469;&#37327;&#21270;&#26497;&#31471;&#39118;&#38505;&#12290; &#28982;&#32780;&#65292;&#31895;&#27169;&#22411;&#30001;&#20110;&#24573;&#35270;&#20102;&#8220;&#20122;&#26684;&#8221;&#23610;&#24230;&#32780;&#21463;&#21040;&#22266;&#26377;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26657;&#27491;&#31639;&#23376;&#23545;&#31895;&#20998;&#36776;&#29575;&#27668;&#20505;&#39044;&#27979;&#36827;&#34892;&#38750;&#20405;&#20837;&#24335;&#21435;&#20559;&#12290; &#20808;&#21069;&#30340;&#21162;&#21147;&#35797;&#22270;&#20351;&#29992;&#19982;&#32479;&#35745;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36825;&#26679;&#30340;&#31639;&#23376;&#12290; &#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20855;&#26377;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#38271;&#37325;&#29616;&#26399;&#30340;&#20107;&#20214;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#21442;&#32771;&#32479;&#35745;&#25968;&#25454;&#23578;&#26410;&#25910;&#25947;&#12290; &#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30340;&#33539;&#22260;&#26159;&#21046;&#23450;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#26657;&#27491;&#21160;&#24577;&#24182;&#37327;&#21270;&#20855;&#26377;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#38271;&#37325;&#29616;&#26399;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18484v1 Announce Type: cross  Abstract: Due to the rapidly changing climate, the frequency and severity of extreme weather is expected to increase over the coming decades. As fully-resolved climate simulations remain computationally intractable, policy makers must rely on coarse-models to quantify risk for extremes. However, coarse models suffer from inherent bias due to the ignored "sub-grid" scales. We propose a framework to non-intrusively debias coarse-resolution climate predictions using neural-network (NN) correction operators. Previous efforts have attempted to train such operators using loss functions that match statistics. However, this approach falls short with events that have longer return period than that of the training data, since the reference statistics have not converged. Here, the scope is to formulate a learning method that allows for correction of dynamics and quantification of extreme events with longer return period than the training data. The key obst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18477</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#31614;&#21517;&#26680;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#29992;&#20110;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#22312;&#31185;&#23398;&#12289;&#20581;&#24247;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#31614;&#21517;&#26680;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#8220;&#36335;&#24452;&#31354;&#38388;&#8221;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#27979;&#35797;&#65292;&#29992;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#36335;&#24452;&#31354;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CI&#27979;&#35797;&#34920;&#29616;&#20986;&#20005;&#26684;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#38750;&#24490;&#29615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#24320;&#21457;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;&#22312;&#20551;&#35774;&#24544;&#23454;&#24615;&#21644;CI&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23436;&#22791;&#19988;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;HOP&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#21521;&#20197;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36328;&#20219;&#21153;&#21644;&#39046;&#22495;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18449</link><description>&lt;p&gt;
HOP&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36830;&#32493;&#23398;&#20064;&#30340;&#19979;&#19968;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
HOP to the Next Tasks and Domains for Continual Learning in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18449
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;HOP&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#21521;&#20197;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36328;&#20219;&#21153;&#21644;&#39046;&#22495;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26088;&#22312;&#36890;&#36807;&#36716;&#31227;&#20808;&#21069;&#38382;&#39064;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#38382;&#39064;&#65288;&#21363;&#20219;&#21153;&#21644;&#39046;&#22495;&#65289;&#65292;&#21516;&#26102;&#36991;&#20813;&#36951;&#24536;&#36807;&#21435;&#30340;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#19987;&#27880;&#20110;&#29305;&#23450;&#29992;&#20363;&#20013;&#19968;&#20010;NLP&#20219;&#21153;&#25110;&#39046;&#22495;&#30340;CL&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;CL&#35774;&#32622;&#65292;&#20174;&#19968;&#20010;&#21807;&#19968;&#30340;&#26694;&#26550;&#20013;&#23398;&#20064;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;HOP&#36890;&#36807;&#27839;&#19977;&#20010;&#26041;&#21521;&#35299;&#20915;CL&#38382;&#39064;&#26469;&#20801;&#35768;&#22312;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#36339;&#36291;&#65306;&#65288;i&#65289;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#36866;&#37197;&#22120;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25512;&#24191;&#21040;&#26410;&#35265;&#38382;&#39064;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#35745;&#31639;&#23884;&#20837;&#34920;&#31034;&#20998;&#24067;&#19978;&#30340;&#39640;&#38454;&#30697;&#20197;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#29420;&#31435;&#21644;&#30456;&#20851;&#32479;&#35745;&#25968;&#25454;&#65292;&#65288;iii&#65289;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#26368;&#32456;&#38382;&#39064;&#19987;&#38376;&#35774;&#35745;&#30340;&#36741;&#21161;&#22836;&#22788;&#29702;&#36825;&#20123;&#20016;&#23500;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;4&#20010;NLP&#24212;&#29992;&#31243;&#24207;&#65292;5&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18449v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 
&lt;/p&gt;</description></item><item><title>LeMo-NADe&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#21442;&#25968;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#22823;&#37327;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#65292;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#38750;AI&#19987;&#23478;&#65292;&#26080;&#38656;&#39044;&#35774;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32771;&#34385;&#20102;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.18443</link><description>&lt;p&gt;
LeMo-NADe: &#22522;&#20110;LLMs&#30340;&#22810;&#21442;&#25968;&#31070;&#32463;&#26550;&#26500;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18443
&lt;/p&gt;
&lt;p&gt;
LeMo-NADe&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#21442;&#25968;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#22823;&#37327;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#65292;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#38750;AI&#19987;&#23478;&#65292;&#26080;&#38656;&#39044;&#35774;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32771;&#34385;&#20102;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21487;&#33021;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#38656;&#35201;&#24191;&#27867;&#19987;&#19994;&#30693;&#35782;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#36793;&#32536;&#35774;&#22791;&#26469;&#35828;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#20204;&#24517;&#39035;&#32771;&#34385;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#21151;&#32791;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#25512;&#29702;&#36895;&#24230;&#21644;CO2&#25490;&#25918;&#37327;&#31561;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#21442;&#25968;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#22312;&#22823;&#37327;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#19978;&#35757;&#32451;&#30340;LLM&#65292;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#24341;&#20837;&#30340;&#26694;&#26550;&#65288;LeMo-NADe&#65289;&#26088;&#22312;&#20379;&#38750;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#39044;&#20808;&#30830;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32771;&#34385;&#20102;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;GPT-4 Turbo&#21644;Gemini&#20316;&#20026;LLM&#32452;&#20214;&#65292;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet16-120&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#21644;&#39564;&#35777;&#20102;&#36825;&#19968;&#25552;&#20986;&#30340;&#31070;&#32463;&#26550;&#26500;&#21457;&#29616;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18443v1 Announce Type: cross  Abstract: Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge. This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions. In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an LLM trained on a large amount of open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters. We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo and Gemini as the LLM component. We obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;&#65292;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045; GCN &#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.18434</link><description>&lt;p&gt;
&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Graph Regularized Encoder Training for Extreme Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;&#65292;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045; GCN &#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18434v1 &#36890;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#28145;&#24230;&#26497;&#31471;&#20998;&#31867;&#65288;XC&#65289;&#26088;&#22312;&#35757;&#32451;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#37197;&#22871;&#30340;&#20998;&#31867;&#22120;&#26550;&#26500;&#65292;&#20197;&#20174;&#19968;&#20010;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#31614;&#38598;&#21512;&#20013;&#20026;&#25968;&#25454;&#28857;&#25171;&#19978;&#26368;&#30456;&#20851;&#30340;&#23376;&#26631;&#31614;&#38598;&#21512;&#12290;&#22312;&#25490;&#21517;&#12289;&#25512;&#33616;&#21644;&#26631;&#35760;&#20013;&#24120;&#35265;&#30340;XC&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#20250;&#36935;&#21040;&#35757;&#32451;&#25968;&#25454;&#26497;&#23569;&#30340;&#23614;&#26631;&#31614;&#12290;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#20294;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#20219;&#21153;&#20803;&#25968;&#25454;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#27491;&#24335;&#30830;&#23450;&#20102;&#22312;&#33509;&#24178;&#29992;&#20363;&#20013;&#65292;&#36890;&#36807;&#29992;&#38750;GCN&#26550;&#26500;&#26367;&#25442;GCNs&#65292;&#23436;&#20840;&#21487;&#20197;&#36991;&#20813;GCNs&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045;GCN&#26356;&#21152;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#33539;&#24335;RAMEN&#65292;&#29992;&#20110;&#21033;&#29992;XC&#35774;&#32622;&#20013;&#30340;&#22270;&#20803;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18434v1 Announce Type: new  Abstract: Deep extreme classification (XC) aims to train an encoder architecture and an accompanying classifier architecture to tag a data point with the most relevant subset of labels from a very large universe of labels. XC applications in ranking, recommendation and tagging routinely encounter tail labels for which the amount of training data is exceedingly small. Graph convolutional networks (GCN) present a convenient but computationally expensive way to leverage task metadata and enhance model accuracies in these settings. This paper formally establishes that in several use cases, the steep computational cost of GCNs is entirely avoidable by replacing GCNs with non-GCN architectures. The paper notices that in these settings, it is much more effective to use graph data to regularize encoder training than to implement a GCN. Based on these insights, an alternative paradigm RAMEN is presented to utilize graph metadata in XC settings that offers 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#31995;&#24402;&#32435;&#20559;&#22909;&#23545;&#32500;&#24230;&#25277;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20851;&#31995;&#29942;&#39048;&#26426;&#21046;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#21644;&#23398;&#20064;&#25928;&#29575;&#65292;&#20351;&#32593;&#32476;&#34920;&#29616;&#19982;&#20154;&#31867;&#34892;&#20026;&#20559;&#22909;&#19968;&#33268;&#12290; - &#20851;&#31995;&#29942;&#39048;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25277;&#35937;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#32593;&#32476;&#22312;&#32500;&#24230;&#19978;&#36827;&#34892;&#32452;&#21512;&#32534;&#30721;&#65292;&#25552;&#39640;&#22788;&#29702;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18426</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#31995;&#24402;&#32435;&#20559;&#22909;&#23545;&#32500;&#24230;&#25277;&#35937;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Relational Inductive Bias for Dimensional Abstraction in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18426
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#31995;&#24402;&#32435;&#20559;&#22909;&#23545;&#32500;&#24230;&#25277;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20851;&#31995;&#29942;&#39048;&#26426;&#21046;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#21644;&#23398;&#20064;&#25928;&#29575;&#65292;&#20351;&#32593;&#32476;&#34920;&#29616;&#19982;&#20154;&#31867;&#34892;&#20026;&#20559;&#22909;&#19968;&#33268;&#12290; - &#20851;&#31995;&#29942;&#39048;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25277;&#35937;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#32593;&#32476;&#22312;&#32500;&#24230;&#19978;&#36827;&#34892;&#32452;&#21512;&#32534;&#30721;&#65292;&#25552;&#39640;&#22788;&#29702;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#31995;&#32479;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#20854;&#33021;&#22815;&#24418;&#25104;&#29615;&#22659;&#30340;&#20302;&#32500;&#12289;&#32452;&#21512;&#34920;&#31034;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24120;&#24120;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#12289;&#36807;&#25311;&#21512;&#21644;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20851;&#31995;&#29942;&#39048;&#30340;&#24433;&#21709; - &#36825;&#26159;&#19968;&#31181;&#23558;&#22788;&#29702;&#38598;&#20013;&#22312;&#36755;&#20837;&#20043;&#38388;&#20851;&#31995;&#19978;&#30340;&#26426;&#21046; - &#23545;&#23398;&#20064;&#26377;&#21033;&#20110;&#32452;&#25104;&#32534;&#30721;&#21644;&#30456;&#24212;&#22788;&#29702;&#28789;&#27963;&#24615;&#30340;&#20998;&#35299;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#29942;&#39048;&#19981;&#20165;&#25552;&#39640;&#20102;&#27867;&#21270;&#21644;&#23398;&#20064;&#25928;&#29575;&#65292;&#36824;&#20351;&#32593;&#32476;&#34920;&#29616;&#19982;&#31867;&#20284;&#20154;&#31867;&#30340;&#34892;&#20026;&#20559;&#22909;&#19968;&#33268;&#12290;&#32463;&#36807;&#20851;&#31995;&#29942;&#39048;&#35757;&#32451;&#30340;&#32593;&#32476;&#21457;&#23637;&#20986;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#28508;&#22312;&#30340;&#29305;&#24449;&#32500;&#24230;&#19978;&#27491;&#20132;&#30340;&#34920;&#31034;&#65292;&#21453;&#26144;&#20102;&#34987;&#35748;&#20026;&#23384;&#22312;&#30340;&#20998;&#35299;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18426v1 Announce Type: new  Abstract: The human cognitive system exhibits remarkable flexibility and generalization capabilities, partly due to its ability to form low-dimensional, compositional representations of the environment. In contrast, standard neural network architectures often struggle with abstract reasoning tasks, overfitting, and requiring extensive data for training. This paper investigates the impact of the relational bottleneck -- a mechanism that focuses processing on relations among inputs -- on the learning of factorized representations conducive to compositional coding and the attendant flexibility of processing. We demonstrate that such a bottleneck not only improves generalization and learning efficiency, but also aligns network performance with human-like behavioral biases. Networks trained with the relational bottleneck developed orthogonal representations of feature dimensions latent in the dataset, reflecting the factorized structure thought to unde
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18424</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Emotion Classification in Low and Moderate Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20998;&#26512;&#20840;&#29699;&#33539;&#22260;&#20869;&#20154;&#20204;&#24773;&#32490;&#29366;&#24577;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20840;&#29699;&#26377;7100&#22810;&#31181;&#27963;&#36291;&#35821;&#35328;&#65292;&#20026;&#27599;&#31181;&#35821;&#35328;&#26500;&#24314;&#24773;&#24863;&#20998;&#31867;&#26159;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#24037;&#20316;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#21644;&#28626;&#21361;&#35821;&#35328;&#65292;&#24314;&#31435;&#24773;&#24863;&#20998;&#31867;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65288;&#20363;&#22914;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#30340;&#33521;&#35821;&#65289;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#23398;&#20064;&#36801;&#31227;&#21040;&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#27604;&#36739;&#24182;&#23545;&#27604;&#20102;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#25110;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#30340;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#27880;&#25237;&#24433;&#21040;&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#30452;&#25509;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#23398;&#20064;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;6&#31181;&#35821;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#65306;Fa
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18424v1 Announce Type: cross  Abstract: It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Fa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#65292;GPT&#33021;&#22815;&#39564;&#35777;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#65292;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.18419</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#22522;&#20110;&#25351;&#21335;&#30340;&#33258;&#21160;&#38382;&#31572;&#26469;&#25913;&#21892;GPT&#30340;&#20808;&#21069;&#25480;&#26435;&#29366;&#24577;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18419
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#65292;GPT&#33021;&#22815;&#39564;&#35777;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#65292;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#29983;&#20445;&#38505;&#20844;&#21496;&#26377;&#19968;&#20010;&#34987;&#31216;&#20026;&#20808;&#21069;&#25480;&#26435;&#65288;PA&#65289;&#30340;&#27969;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21355;&#29983;&#35745;&#21010;&#25104;&#26412;&#25511;&#21046;&#27969;&#31243;&#65292;&#35201;&#27714;&#21307;&#29983;&#21644;&#20854;&#20182;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#22312;&#23545;&#24739;&#32773;&#25191;&#34892;&#29305;&#23450;&#31243;&#24207;&#20043;&#21069;&#24517;&#39035;&#20107;&#20808;&#33719;&#24471;&#21355;&#29983;&#35745;&#21010;&#30340;&#25209;&#20934;&#65292;&#20197;&#20415;&#26377;&#36164;&#26684;&#33719;&#24471;&#25903;&#20184;&#35206;&#30422;&#12290;&#23545;&#21355;&#29983;&#20445;&#38505;&#20844;&#21496;&#26469;&#35828;&#65292;&#25209;&#20934;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#25361;&#25112;&#26159;&#39564;&#35777;&#35831;&#27714;&#26159;&#21542;&#31526;&#21512;&#26576;&#20123;&#26631;&#20934;&#65292;&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#26159;&#21542;&#33021;&#39564;&#35777;&#22823;&#37327;&#20851;&#38190;&#22240;&#32032;&#65292;&#20174;&#32780;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#38382;&#31572;&#20219;&#21153;&#65292;&#20419;&#20351;GPT&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#20256;&#32479;&#25552;&#31034;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#26032;&#39062;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18419v1 Announce Type: cross  Abstract: Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record. We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique. Mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;DockGen&#65292;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#23545;&#25509;&#30340;&#21487;&#25193;&#23637;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#32622;&#20449;&#24230;&#33258;&#20030;&#27861;&#35757;&#32451;&#33539;&#24335;&#65292;&#35813;&#30740;&#31350;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#25509;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18396</link><description>&lt;p&gt;
&#28145;&#20837;&#30340;&#23433;&#20840;&#27493;&#39588;&#36827;&#20837;&#26032;&#39046;&#22495;&#65306;&#23545;&#25509;&#24191;&#20041;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Deep Confident Steps to New Pockets: Strategies for Docking Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18396
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;DockGen&#65292;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#23545;&#25509;&#30340;&#21487;&#25193;&#23637;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#32622;&#20449;&#24230;&#33258;&#20030;&#27861;&#35757;&#32451;&#33539;&#24335;&#65292;&#35813;&#30740;&#31350;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#25509;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#30450;&#23545;&#25509;&#26377;&#28508;&#21147;&#24102;&#26469;&#26032;&#30340;&#29983;&#29289;&#31361;&#30772;&#65292;&#20294;&#35201;&#23454;&#29616;&#36825;&#19968;&#25215;&#35834;&#65292;&#23545;&#25509;&#26041;&#27861;&#24517;&#39035;&#22312;&#34507;&#30333;&#36136;&#32452;&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#25968;&#25454;&#26410;&#33021;&#20005;&#26684;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#34507;&#30333;&#36136;&#37197;&#20307;&#32467;&#21512;&#22495;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;DockGen&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23545;&#25509;&#27169;&#22411;&#20855;&#26377;&#38750;&#24120;&#24369;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23545;&#25509;&#30340;&#21487;&#25193;&#23637;&#35268;&#24459;&#65292;&#24182;&#34920;&#26126;&#65292;&#36890;&#36807;&#32553;&#25918;&#25968;&#25454;&#21644;&#27169;&#22411;&#22823;&#23567;&#65292;&#20197;&#21450;&#25972;&#21512;&#21512;&#25104;&#25968;&#25454;&#31574;&#30053;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26032;&#30340;&#26368;&#20248;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#8220;&#32622;&#20449;&#24230;&#33258;&#20030;&#27861;&#8221;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25193;&#25955;&#21644;&#32622;&#20449;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#20998;&#36776;&#29575;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18396v1 Announce Type: cross  Abstract: Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome. Existing benchmarks, however, fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that existing machine learning-based docking models have very weak generalization abilities. We carefully analyze the scaling laws of ML-based docking and show that, by scaling data and model size, as well as integrating synthetic data strategies, we are able to significantly increase the generalization capacity and set new state-of-the-art performance across benchmarks. Further, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between diffusion and confidence models and exploits the multi-resolution generation process of diffusion mo
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#29702;&#24819;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18392</link><description>&lt;p&gt;
&#25581;&#31034;&#20581;&#22766;&#24615;&#22312;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Robustness in Evaluating Causal Inference Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18392
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#29702;&#24819;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#23545;&#20010;&#24615;&#21270;&#20915;&#31574;&#21046;&#23450;&#30340;&#38656;&#27714;&#23548;&#33268;&#20154;&#20204;&#23545;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#20132;&#21449;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#21508;&#31181;&#26377;&#25928;&#30340;CATE&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#20123;&#20272;&#35745;&#22120;&#36890;&#24120;&#21463;&#21046;&#20110;&#32570;&#20047;&#21453;&#20107;&#23454;&#26631;&#31614;&#65292;&#22240;&#27492;&#20351;&#29992;&#20256;&#32479;&#30340;&#20132;&#21449;&#39564;&#35777;&#31561;&#27169;&#22411;&#36873;&#25321;&#31243;&#24207;&#26469;&#36873;&#25321;&#29702;&#24819;&#30340;CATE&#20272;&#35745;&#22120;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;CATE&#20272;&#35745;&#22120;&#36873;&#25321;&#26041;&#27861;&#65292;&#22914;&#25554;&#20540;&#21644;&#20266;&#32467;&#26524;&#24230;&#37327;&#65292;&#38754;&#20020;&#30528;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#38656;&#35201;&#30830;&#23450;&#24230;&#37327;&#24418;&#24335;&#21644;&#25311;&#21512;&#24178;&#25200;&#21442;&#25968;&#25110;&#25554;&#20214;&#23398;&#20064;&#32773;&#30340;&#22522;&#30784;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#32570;&#20047;&#38024;&#23545;&#36873;&#25321;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#29305;&#23450;&#37325;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18392v1 Announce Type: cross  Abstract: The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). The intersection of machine learning and causal inference has yielded various effective CATE estimators. However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. Secondly, they lack a specific focus on selecting a robust estimator. To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for 
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#22788;&#29702;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;&#22343;&#20540;&#32479;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#40657;&#30418;&#37325;&#32452;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.18381</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36827;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Evolution Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18381
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#22788;&#29702;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;&#22343;&#20540;&#32479;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#40657;&#30418;&#37325;&#32452;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#25152;&#35859;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#26799;&#24230;&#19979;&#38477;&#12289;&#20998;&#31867;&#12289;&#24207;&#21015;&#23436;&#25104;&#12289;&#36716;&#25442;&#21644;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#26410;&#26126;&#30830;&#36935;&#21040;&#36807;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#22522;&#26412;&#19978;&#33021;&#22815;&#23454;&#29616;&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#31163;&#25955;&#21270;&#30340;&#31181;&#32676;&#25104;&#21592;&#36827;&#34892;&#20174;&#23569;&#21040;&#22810;&#30340;&#25490;&#24207;&#65292;&#24182;&#35810;&#38382;LLM&#25552;&#20986;&#23545;&#22343;&#20540;&#32479;&#35745;&#30340;&#25913;&#36827;&#65292;&#25191;&#34892;&#19968;&#31181;&#40657;&#30418;&#37325;&#32452;&#25805;&#20316;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#35774;&#32622;&#20801;&#35768;&#29992;&#25143;&#33719;&#24471;&#22522;&#20110;LLM&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;`EvoLL`&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18381v1 Announce Type: new  Abstract: Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization. We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation. Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#27867;&#21270;&#38382;&#39064;&#30340;&#27491;&#24335;&#26694;&#26550;, &#24182;&#38416;&#36848;&#20102;&#36328;&#39046;&#22495;&#27867;&#21270;&#22312;DSR&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;</title><link>https://arxiv.org/abs/2402.18377</link><description>&lt;p&gt;
&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Generalization in Dynamical Systems Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#20013;&#27867;&#21270;&#38382;&#39064;&#30340;&#27491;&#24335;&#26694;&#26550;, &#24182;&#38416;&#36848;&#20102;&#36328;&#39046;&#22495;&#27867;&#21270;&#22312;DSR&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#25214;&#21040;&#22312;&#32463;&#39564;&#29616;&#35937;&#32972;&#21518;&#30340;&#25511;&#21046;&#26041;&#31243;&#21644;&#21160;&#21147;&#35268;&#21017;&#12290;&#20256;&#32479;&#19978;&#65292;&#31185;&#23398;&#27169;&#22411;&#26159;&#36890;&#36807;&#20154;&#31867;&#27934;&#23519;&#21644;&#23454;&#39564;&#21608;&#26399;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#29992;&#26469;&#30452;&#25509;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#37325;&#26500;&#21160;&#21147;&#31995;&#32479;&#65288;DS&#65289;&#12290;&#26368;&#20808;&#36827;&#30340;&#21160;&#21147;&#31995;&#32479;&#37325;&#26500;&#65288;DSR&#65289;&#26041;&#27861;&#22312;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;DS&#30340;&#19981;&#21464;&#21644;&#38271;&#26399;&#29305;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#27867;&#21270;&#21040;&#26410;&#35266;&#23519;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#25105;&#20204;&#26399;&#26395;&#20174;&#20219;&#20309;&#21487;&#34892;&#30340;&#31185;&#23398;&#29702;&#35770;&#20013;&#33719;&#24471;&#30340;&#33267;&#20851;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;DSR&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;DSR&#20013;&#30340;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#27867;&#21270;&#65288;OODG&#65289;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#32771;&#34385;&#30340;OODG&#26377;&#26681;&#26412;&#21306;&#21035;&#12290;&#25105;&#20204;&#20171;&#32461;&#22522;&#20110;&#25299;&#25169;&#27010;&#24565;&#21644;&#31526;&#21495;&#30340;&#25968;&#23398;&#27010;&#24565;&#65292;&#24182;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18377v1 Announce Type: new  Abstract: In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedUV&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#22312;&#24322;&#26500;&#20998;&#24067;&#25968;&#25454;&#20013;&#34920;&#29616;&#24471;&#26356;&#22343;&#21248;&#21644;&#31283;&#23450;</title><link>https://arxiv.org/abs/2402.18372</link><description>&lt;p&gt;
FedUV: &#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#22343;&#21248;&#24615;&#21644;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
FedUV: Uniformity and Variance for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18372
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedUV&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#22312;&#24322;&#26500;&#20998;&#24067;&#25968;&#25454;&#20013;&#34920;&#29616;&#24471;&#26356;&#22343;&#21248;&#21644;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20250;&#38543;&#30528;&#24322;&#26500;&#20998;&#24067;&#30340;&#25968;&#25454;&#32780;&#19979;&#38477;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#26159;&#30001;&#20110;&#32593;&#32476;&#30340;&#26368;&#32456;&#23618;&#26368;&#23481;&#26131;&#20986;&#29616;&#23616;&#37096;&#20559;&#24046;&#65292;&#19968;&#20123;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#23558;&#26368;&#32456;&#23618;&#20923;&#32467;&#20026;&#27491;&#20132;&#20998;&#31867;&#22120;&#21487;&#20197;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26435;&#37325;&#24212;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#30740;&#31350;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36825;&#26159;&#21463;&#21040;&#20923;&#32467;&#26435;&#37325;&#23548;&#33268;&#22855;&#24322;&#20540;&#24658;&#23450;&#30340;&#35266;&#23519;&#21551;&#21457;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;IID&#21644;&#38750;IID&#35774;&#32622;&#19979;&#35757;&#32451;&#26102;&#23384;&#22312;&#24046;&#24322;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20004;&#31181;&#23616;&#37096;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#25345;&#32493;&#27169;&#25311;IID&#35774;&#32622;&#65306;&#65288;1&#65289;&#20998;&#31867;&#22120;&#30340;&#32500;&#24230;&#27010;&#29575;&#20998;&#24067;&#26041;&#24046;&#21644;&#65288;2&#65289;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#36229;&#29699;&#22343;&#21248;&#24615;&#12290;&#36825;&#20123;&#27491;&#21017;&#21270;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#34920;&#29616;&#24471;&#22909;&#20687;&#22312;IID&#35774;&#32622;&#20013;&#19968;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18372v1 Announce Type: cross  Abstract: Federated learning is a promising framework to train neural networks with widely distributed data. However, performance degrades heavily with heterogeneously distributed data. Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier. We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values. We find that there are differences when training in IID and non-IID settings. Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder. These regularizations promote local models to act as if it were in an IID setting
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27491;&#35268;&#21270;&#27969;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#39044;&#31639;&#32422;&#26463;&#19979;&#26356;&#26032;&#31995;&#32479;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#19988;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.18337</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#27491;&#35268;&#21270;&#27969;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Bayesian optimal experimental design using conditional normalizing flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18337
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27491;&#35268;&#21270;&#27969;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#39044;&#31639;&#32422;&#26463;&#19979;&#26356;&#26032;&#31995;&#32479;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#19988;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;OED&#65289;&#26088;&#22312;&#22312;&#39044;&#31639;&#32422;&#26463;&#19979;&#24320;&#23637;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#39564;&#65292;&#20197;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#21033;&#29992;&#23454;&#39564;&#25968;&#25454;&#26356;&#26032;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#20026;&#21518;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#20351;&#36125;&#21494;&#26031;OED&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#19988;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18337v1 Announce Type: new  Abstract: Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework. Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs. To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach. This approach performs simultaneous (1) training of a scalable conditional no
&lt;/p&gt;</description></item><item><title>Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18334</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18334
&lt;/p&gt;
&lt;p&gt;
Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bonito&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#29992;&#20110;&#26465;&#20214;&#20219;&#21153;&#29983;&#25104;&#65306;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;&#30340;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#29992;&#25143;&#19987;&#38376;&#30340;&#31169;&#20154;&#25968;&#25454;&#19978;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;1.65M&#20010;&#31034;&#20363;&#30340;&#26032;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;Bonito&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#37325;&#26032;&#28151;&#21512;&#25104;&#20803;&#27169;&#26495;&#32780;&#21019;&#24314;&#30340;&#12290;&#25968;&#25454;&#38598;&#30340;&#20803;&#27169;&#26495;&#20135;&#29983;&#35757;&#32451;&#31034;&#20363;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#21644;&#20219;&#21153;&#23646;&#24615;&#65292;&#36755;&#20986;&#21253;&#25324;&#25351;&#20196;&#21644;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;Bonito&#20026;&#19971;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#21512;&#25104;&#20219;&#21153;&#65292;&#36328;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411; -- &#26159;&#38750;&#38382;&#31572;&#12289;&#25277;&#21462;&#24335;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702; -- &#24182;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Bonito&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18334v1 Announce Type: new  Abstract: We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#24378;&#21644;&#22810;&#26679;&#21270;LOTL&#24694;&#24847;&#27963;&#21160;&#30340;&#23384;&#22312;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#24694;&#24847;&#27963;&#21160;&#26816;&#27979;&#24615;&#33021;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.18329</link><description>&lt;p&gt;
&#20381;&#38752;&#30693;&#24773;&#25968;&#25454;&#22686;&#24378;&#30340;&#25269;&#21046;&#29983;&#27963;-&#20381;&#36182;-&#22303;&#22320;&#21453;&#21521;&#22806;&#22771;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18329
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#21644;&#22810;&#26679;&#21270;LOTL&#24694;&#24847;&#27963;&#21160;&#30340;&#23384;&#22312;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#24694;&#24847;&#27963;&#21160;&#26816;&#27979;&#24615;&#33021;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#27963;-&#20381;&#36182;-&#22303;&#22320;(LOTL)&#36827;&#25915;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#21512;&#27861;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#30340;&#21629;&#20196;&#38142;&#26469;&#29359;&#32618;&#34892;&#20026;&#65292;&#20165;&#21487;&#36890;&#36807;&#31995;&#32479;&#26085;&#24535;&#20998;&#26512;&#26469;&#35782;&#21035;&#12290;LOTL&#25216;&#26415;&#38544;&#34255;&#22312;&#26222;&#36890;&#21512;&#27861;&#27963;&#21160;&#20135;&#29983;&#30340;&#20107;&#20214;&#27969;&#20013;&#65292;&#23041;&#32961;&#34892;&#20026;&#32773;&#32463;&#24120;&#36890;&#36807;&#28151;&#28102;&#26469;&#20266;&#35013;&#27963;&#21160;&#65292;&#20351;&#20854;&#38590;&#20197;&#22312;&#19981;&#24341;&#36215;&#22823;&#37327;&#35823;&#35686;&#24773;&#20917;&#19979;&#26816;&#27979;&#65292;&#21363;&#20351;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#22312;&#36825;&#26679;&#24694;&#21155;&#30340;&#29615;&#22659;&#20013;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#21644;&#20351;&#22810;&#26679;&#21270;LOTL&#24694;&#24847;&#27963;&#21160;&#22312;&#21512;&#27861;&#26085;&#24535;&#20013;&#30340;&#23384;&#22312;&#12290;&#22312;&#23041;&#32961;&#24773;&#25253;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#24050;&#30693;&#22312;&#37326;&#22806;&#20351;&#29992;&#30340;&#25915;&#20987;&#27169;&#26495;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#20016;&#23500;&#21512;&#27861;&#27963;&#21160;&#30340;&#21487;&#22609;&#27169;&#24335;&#65292;&#20197;&#22797;&#21046;&#22238;&#36991;&#23041;&#32961;&#34892;&#20026;&#32773;&#34892;&#20026;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18329v1 Announce Type: cross  Abstract: The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs. LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning. To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs. Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors. We conduct an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#26426;&#26800;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36335;&#24452;&#29983;&#25104;&#31572;&#26696;&#65292;&#21516;&#26102;&#23384;&#22312;&#21151;&#33021;&#24615;&#20998;&#27495;&#12290;</title><link>https://arxiv.org/abs/2402.18312</link><description>&lt;p&gt;
&#22914;&#20309;&#36880;&#27493;&#24605;&#32771;&#65306;&#23545;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#26426;&#26800;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18312
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#26426;&#26800;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36335;&#24452;&#29983;&#25104;&#31572;&#26696;&#65292;&#21516;&#26102;&#23384;&#22312;&#21151;&#33021;&#24615;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#65292;&#20294;&#23545;&#20110;&#20419;&#36827;CoT&#29983;&#25104;&#30340;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#20173;&#23384;&#22312;&#32570;&#20047;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#26426;&#26800;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;LLMs&#20013;&#34920;&#29616;&#20986;CoT&#25512;&#29702;&#30340;&#31070;&#32463;&#23376;&#32467;&#26500;&#12290;&#36890;&#36807;&#23545;LLaMA-2 7B&#24212;&#29992;&#20110;&#34394;&#26500;&#26412;&#20307;&#35770;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#20026;&#36880;&#27493;&#25512;&#29702;&#37096;&#32626;&#20102;&#22810;&#20010;&#24182;&#34892;&#31572;&#26696;&#29983;&#25104;&#36335;&#24452;&#12290;&#36825;&#20123;&#24182;&#34892;&#36335;&#24452;&#25552;&#20379;&#20102;&#26469;&#33258;&#36755;&#20837;&#38382;&#39064;&#19978;&#19979;&#25991;&#20197;&#21450;&#29983;&#25104;&#30340;CoT&#30340;&#24207;&#36143;&#31572;&#26696;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#20013;&#38388;&#23618;&#23384;&#22312;&#24341;&#20154;&#30633;&#30446;&#30340;&#21151;&#33021;&#20998;&#27495;&#12290;&#21021;&#22987;&#19968;&#21322;&#30340;&#20196;&#29260;&#34920;&#31034;&#20173;&#28982;&#24378;&#28872;&#20559;&#21521;&#39044;&#35757;&#32451;&#20808;&#39564;&#65292;&#32780;&#21518;&#21322;&#37096;&#20998;&#31361;&#28982;&#34987;&#19978;&#19979;&#25991;&#25152;&#21462;&#20195;&#12290;&#36825;&#31181;&#20869;&#37096;&#30456;&#20301;&#36716;&#21464;&#22312;&#19981;&#21516;&#30340;&#21151;&#33021;&#21327;&#21516;&#20013;&#20307;&#29616;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18312v1 Announce Type: new  Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#25200;&#21160;&#24067;&#23616;&#32467;&#26524;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18311</link><description>&lt;p&gt;
&#22312;&#20840;&#23616;&#24067;&#23616;&#20013;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
Escaping Local Optima in Global Placement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#25200;&#21160;&#24067;&#23616;&#32467;&#26524;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#35774;&#35745;&#20013;&#65292;&#24067;&#23616;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23427;&#26497;&#22823;&#22320;&#24433;&#21709;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#25351;&#26631;&#12290;&#26368;&#36817;&#20998;&#26512;&#26041;&#27861;&#30340;&#36827;&#27493;&#65292;&#20363;&#22914;DREAMPlace&#65292;&#22312;&#20840;&#23616;&#24067;&#23616;&#20013;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;DREAMPlace &#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#21487;&#33021;&#26080;&#27861;&#22312;&#30456;&#21516;&#35774;&#32622;&#19979;&#20445;&#35777;&#21487;&#21512;&#27861;&#24067;&#23616;&#65292;&#23548;&#33268;&#33030;&#24369;&#19988;&#38590;&#20197;&#39044;&#27979;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#31361;&#20986;&#20102;&#34987;&#22256;&#22312;&#23616;&#37096;&#26368;&#20248;&#35299;&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#25200;&#21160;&#24067;&#23616;&#32467;&#26524;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18311v1 Announce Type: new  Abstract: Placement is crucial in the physical design, as it greatly affects power, performance, and area metrics. Recent advancements in analytical methods, such as DREAMPlace, have demonstrated impressive performance in global placement. However, DREAMPlace has some limitations, e.g., may not guarantee legalizable placements under the same settings, leading to fragile and unpredictable results. This paper highlights the main issue as being stuck in local optima, and proposes a hybrid optimization framework to efficiently escape the local optima, by perturbing the placement result iteratively. The proposed framework achieves significant improvements compared to state-of-the-art methods on two popular benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;XGBoost&#21644;Minirocket&#31639;&#27861;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22788;&#29702;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#35780;&#20215;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.18296</link><description>&lt;p&gt;
XGBoost&#21644;Minirocket&#31639;&#27861;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of XGBoost and Minirocket Algortihms for Human Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;XGBoost&#21644;Minirocket&#31639;&#27861;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22788;&#29702;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#35780;&#20215;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#26368;&#36817;&#24378;&#35843;&#20351;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;ML&#31639;&#27861;&#65292;&#21363;eXtreme Gradient Boosting&#65288;XGBoost&#65289;&#21644;MiniRocket&#65292;&#22312;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;HAR&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#20351;&#29992;&#20174;UCI&#23384;&#20648;&#24211;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;30&#21517;&#24535;&#24895;&#32773;&#22312;&#20329;&#25140;&#26234;&#33021;&#25163;&#26426;&#26102;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;&#26102;&#25429;&#33719;&#30340;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20449;&#21495;&#32452;&#25104;&#12290;&#22312;&#23558;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#31867;&#22120;&#20043;&#21069;&#65292;&#25968;&#25454;&#38598;&#32463;&#36807;&#39044;&#22788;&#29702;&#65292;&#21253;&#25324;&#22122;&#22768;&#36807;&#28388;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#33945;&#29305;&#21345;&#27931;&#20132;&#21449;&#39564;&#35777;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#21644;MiniRocket&#22312;&#27963;&#21160;&#20998;&#31867;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;F1&#20998;&#25968;&#21644;AUC&#20540;&#39640;&#36798;0.99&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18296v1 Announce Type: new  Abstract: Human Activity Recognition (HAR) has been extensively studied, with recent emphasis on the implementation of advanced Machine Learning (ML) and Deep Learning (DL) algorithms for accurate classification. This study investigates the efficacy of two ML algorithms, eXtreme Gradient Boosting (XGBoost) and MiniRocket, in the realm of HAR using data collected from smartphone sensors. The experiments are conducted on a dataset obtained from the UCI repository, comprising accelerometer and gyroscope signals captured from 30 volunteers performing various activities while wearing a smartphone. The dataset undergoes preprocessing, including noise filtering and feature extraction, before being utilized for training and testing the classifiers. Monte Carlo cross-validation is employed to evaluate the models' robustness. The findings reveal that both XGBoost and MiniRocket attain accuracy, F1 score, and AUC values as high as 0.99 in activity classifica
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18286</link><description>&lt;p&gt;
&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#36808;&#21521;&#39640;&#32423;&#22270;&#20687;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#26080;&#26631;&#31614;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36808;&#20986;&#20102;&#26500;&#24314;&#35813;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#20197;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#20998;&#21106;&#12289;&#21435;&#22122;&#12289;&#22122;&#22768;&#19982;&#32972;&#26223;&#21435;&#38500;&#20197;&#21450;&#36229;&#20998;&#36776;&#29575;&#12290;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#24863;&#21463;&#37326;&#22823;&#23567;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#26174;&#33879;&#30340;&#29616;&#35937;&#65292;&#21363;&#24494;&#35843;&#36807;&#30340;&#36739;&#20302;&#22797;&#26434;&#24230;&#27169;&#22411;&#22987;&#32456;&#32988;&#36807;&#20855;&#26377;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#26356;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#32972;&#26223;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#22810;&#25165;&#22810;&#33402;&#65292;&#20351;&#24471;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20652;&#21270;&#21058;&#65292;&#29305;&#21035;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#21487;&#29992;&#26102;&#21644; ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18286v1 Announce Type: cross  Abstract: In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise &amp; background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and ef
&lt;/p&gt;</description></item><item><title>PiShield&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#37117;&#33021;&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#24182;&#21487;&#26681;&#25454;&#20174;&#19994;&#32773;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.18285</link><description>&lt;p&gt;
PiShield&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20197;&#38656;&#27714;&#20026;&#22522;&#30784;&#23398;&#20064;&#30340;NeSy&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PiShield: A NeSy Framework for Learning with Requirements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18285
&lt;/p&gt;
&lt;p&gt;
PiShield&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#37117;&#33021;&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#24182;&#21487;&#26681;&#25454;&#20174;&#19994;&#32773;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#21183;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#28385;&#36275;&#20854;&#36755;&#20986;&#30340;&#23433;&#20840;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PiShield&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#12290;PiShield&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#20174;&#19994;&#32773;&#30340;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36843;&#20999;&#38656;&#35201;&#20801;&#35768;&#22312;&#21508;&#20010;&#39046;&#22495;&#38598;&#25104;&#38656;&#27714;&#30340;&#26694;&#26550;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#20010;&#24212;&#29992;&#22330;&#26223;&#65306;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18285v1 Announce Type: cross  Abstract: Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs. In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology. PiShield guarantees compliance with these requirements, regardless of input. Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs. Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains. Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#22522;&#20110;&#36866;&#24212;&#37319;&#26679;&#30340;&#21518;&#39564;GP&#30340;&#26368;&#39640;&#20540;&#20013;&#20540;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#36793;&#30028;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20272;&#35745;&#39640;&#23433;&#20840;&#27010;&#29575;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#21152;&#24555;&#20102;&#35780;&#20272;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#21644;&#25506;&#32034;&#36895;&#24230;</title><link>https://arxiv.org/abs/2402.18260</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#39640;&#25928;&#35745;&#31639;&#23433;&#20840;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#22522;&#20110;&#36866;&#24212;&#37319;&#26679;&#30340;&#21518;&#39564;GP&#30340;&#26368;&#39640;&#20540;&#20013;&#20540;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#36793;&#30028;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20272;&#35745;&#39640;&#23433;&#20840;&#27010;&#29575;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#21152;&#24555;&#20102;&#35780;&#20272;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#21644;&#25506;&#32034;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#24517;&#39035;&#26222;&#36941;&#36981;&#23432;&#23454;&#38469;&#23433;&#20840;&#32422;&#26463;&#65292;&#36825;&#38480;&#21046;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#25506;&#32034;&#12290;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#21450;&#20854;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#34987;&#24191;&#27867;&#29992;&#20110;&#27492;&#30446;&#30340;&#12290;&#22312;&#35768;&#22810;&#25216;&#26415;&#24212;&#29992;&#20013;&#65292;&#35774;&#35745;&#31354;&#38388;&#36890;&#36807;&#36830;&#32493;&#36712;&#36857;&#36827;&#34892;&#25506;&#32034;&#65292;&#27839;&#30528;&#36712;&#36857;&#38656;&#35201;&#35780;&#20272;&#23433;&#20840;&#24615;&#12290;&#36825;&#23545;GP&#26041;&#27861;&#20013;&#20005;&#26684;&#30340;&#23433;&#20840;&#35201;&#27714;&#26469;&#35828;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#30340;&#39640;&#20998;&#20301;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36866;&#24212;&#37319;&#26679;&#30340;&#21518;&#39564;GP&#30340;&#26368;&#39640;&#20540;&#20013;&#20540;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#23433;&#20840;&#36793;&#30028;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#20272;&#35745;&#39640;&#23433;&#20840;&#27010;&#29575;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#21644;&#25506;&#32034;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#21152;&#24555;&#20102;&#35780;&#20272;&#36895;&#24230;&#12290;&#25105;&#20204;&#23433;&#20840;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18260v1 Announce Type: new  Abstract: Active learning of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space. Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose. In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed. This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte-Carlo sampling of high quantiles. We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP. Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed. The effectiveness of our safe active learning approach is demonstrated through exten
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;fNIRs&#21644;&#26426;&#22120;&#23398;&#20064;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24773;&#24863;&#29366;&#24577;&#30340;&#20998;&#31867;&#65292;&#20998;&#31867;&#20934;&#30830;&#29575;&#22312;83%&#21040;84%&#20043;&#38388;&#65292;&#24182;&#21487;&#22312;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#21644;&#26234;&#33021;&#23089;&#20048;&#36873;&#25321;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18241</link><description>&lt;p&gt;
&#20351;&#29992;fNIRs&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#24773;&#24863;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Affective State Detection using fNIRs and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18241
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;fNIRs&#21644;&#26426;&#22120;&#23398;&#20064;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24773;&#24863;&#29366;&#24577;&#30340;&#20998;&#31867;&#65292;&#20998;&#31867;&#20934;&#30830;&#29575;&#22312;83%&#21040;84%&#20043;&#38388;&#65292;&#24182;&#21487;&#22312;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#21644;&#26234;&#33021;&#23089;&#20048;&#36873;&#25321;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#24773;&#29366;&#24577;&#35843;&#33410;&#25105;&#20204;&#26085;&#24120;&#21151;&#33021;&#65292;&#24182;&#23545;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#26816;&#27979;&#24773;&#24863;&#29366;&#24577;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#12289;&#26234;&#33021;&#23089;&#20048;&#36873;&#25321;&#21644;&#21160;&#24577;&#24037;&#20316;&#36127;&#33655;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#29983;&#29702;&#25968;&#25454;&#26816;&#27979;&#24773;&#24863;&#29366;&#24577;&#30340;&#30456;&#20851;&#25991;&#29486;&#65292;&#19981;&#21516;&#20256;&#24863;&#22120;&#21644;&#26041;&#27861;&#25910;&#38598;&#29983;&#29702;&#25968;&#25454;&#30340;&#20248;&#32570;&#28857;&#65292;&#20197;&#21450;&#25105;&#20204;&#36873;&#25321;&#21151;&#33021;&#24615;&#36817;&#32418;&#22806;&#20809;&#35889;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28041;&#21450;&#20061;&#21517;&#21463;&#35797;&#32773;&#24341;&#36215;&#20901;&#24819;&#12289;&#23089;&#20048;&#21644;&#35748;&#30693;&#36127;&#33655;&#24773;&#24863;&#29366;&#24577;&#30340;&#23454;&#39564;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20998;&#31867;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;83.04%&#30340;&#19977;&#31867;&#20998;&#31867;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65307;&#32452;&#27169;&#22411;&#23454;&#29616;&#20102;84.39%&#30340;&#20934;&#30830;&#29575;&#65307;&#29420;&#31435;&#20027;&#39064;&#27169;&#22411;&#20351;&#29992;&#30041;&#23384;&#27861;&#27169;&#22411;&#36798;&#21040;&#20102;60.57%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18241v1 Announce Type: cross  Abstract: Affective states regulate our day to day to function and has a tremendous effect on mental and physical health. Detection of affective states is of utmost importance for mental health monitoring, smart entertainment selection and dynamic workload management. In this paper, we discussed relevant literature on affective state detection using physiology data, the benefits and limitations of different sensors and methods used for collecting physiology data, and our rationale for selecting functional near-infrared spectroscopy. We present the design of an experiment involving nine subjects to evoke the affective states of meditation, amusement and cognitive load and the results of the attempt to classify using machine learning. A mean accuracy of 83.04% was achieved in three class classification with an individual model; 84.39% accuracy was achieved for a group model and 60.57% accuracy was achieved for subject independent model using leave
&lt;/p&gt;</description></item><item><title>CogBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#20986;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#24037;&#20855;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#24615;&#33021;&#25913;&#21892;&#21644;&#19982;&#20154;&#31867;&#34892;&#20026;&#19968;&#33268;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18225</link><description>&lt;p&gt;
CogBench: &#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27493;&#20837;&#24515;&#29702;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
CogBench: a large language model walks into a psychology lab
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18225
&lt;/p&gt;
&lt;p&gt;
CogBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#20986;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#24037;&#20855;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#24615;&#33021;&#25913;&#21892;&#21644;&#19982;&#20154;&#31867;&#34892;&#20026;&#19968;&#33268;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#37096;&#20998;&#26159;&#30001;&#20110;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#24615;&#33021;&#25351;&#26631;&#30340;&#20027;&#35201;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CogBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#30340;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#20026;&#34920;&#22411;&#21270;LLMs&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#12290;&#25105;&#20204;&#23558;CogBench&#24212;&#29992;&#20110;35&#20010;LLMs&#65292;&#24471;&#21040;&#20016;&#23500;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#22810;&#23618;&#24314;&#27169;&#25216;&#26415;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#65292;&#32771;&#34385;&#21040;&#29305;&#23450;LLMs&#30340;&#24494;&#35843;&#29256;&#26412;&#20043;&#38388;&#30340;&#23884;&#22871;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#25913;&#21892;&#24615;&#33021;&#24182;&#19982;&#20154;&#31867;&#34892;&#20026;&#20445;&#25345;&#19968;&#33268;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#28304;&#27169;&#22411;&#27604;&#19987;&#26377;&#27169;&#22411;&#26356;&#23569;&#39118;&#38505;&#65292;&#24182;&#19988;&#31934;&#32454;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18225v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.18213</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Differentiable Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20013;&#30340;Pareto&#21069;&#27839;&#36718;&#24275;&#21078;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36825;&#26679;&#30340;&#26114;&#36149;&#30446;&#26631;&#20013;&#12290; &#30456;&#23545;&#20110;&#20256;&#32479;&#30340;NAS&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#24182;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#32593;&#32476;&#21442;&#25968;&#21270;&#36328;&#22810;&#20010;&#35774;&#22791;&#21644;&#22810;&#20010;&#30446;&#26631;&#30340;&#32852;&#21512;&#26550;&#26500;&#20998;&#24067;&#65292;&#36229;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#30828;&#20214;&#29305;&#24449;&#21644;&#20559;&#22909;&#21521;&#37327;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#38646;&#27425;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18213v1 Announce Type: new  Abstract: Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#29305;&#24449;&#28608;&#27963;&#24046;&#24322;&#20998;&#26512;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#38024;&#23545;&#24615;&#25805;&#25511;&#29305;&#23450;&#36335;&#24452;&#20013;&#30340;&#29305;&#24449;&#28608;&#27963;&#24046;&#24322;&#21487;&#26377;&#25928;&#20943;&#36731;&#21644;&#35825;&#23548;CO&#12290;</title><link>https://arxiv.org/abs/2402.18211</link><description>&lt;p&gt;
&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#65306;&#19968;&#20010;&#28508;&#22312;&#30340;&#31119;&#31096;&#30456;&#20381;&#20043;&#38388;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Overfitting: A Potential Blessing in Disguise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18211
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#29305;&#24449;&#28608;&#27963;&#24046;&#24322;&#20998;&#26512;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#38024;&#23545;&#24615;&#25805;&#25511;&#29305;&#23450;&#36335;&#24452;&#20013;&#30340;&#29305;&#24449;&#28608;&#27963;&#24046;&#24322;&#21487;&#26377;&#25928;&#20943;&#36731;&#21644;&#35825;&#23548;CO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#65288;Fast Adversarial Training&#65292;FAT&#65289;&#22312;&#25913;&#21892;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#21457;&#30740;&#31350;&#30028;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#36825;&#19968;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#65288;Catastrophic Overfitting, CO&#65289;&#25361;&#25112;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;FAT&#26041;&#27861;&#22312;&#20943;&#36731;CO&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#20276;&#38543;&#30528;&#23545;&#28165;&#27905;&#26679;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#19981;&#21487;&#24573;&#30053;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#28165;&#27905;&#21644;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#30340;&#29305;&#24449;&#28608;&#27963;&#24046;&#24322;&#20998;&#26512;CO&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;CO&#21487;&#20197;&#24402;&#22240;&#20110;&#30001;&#23569;&#37327;&#29305;&#23450;&#36335;&#24452;&#24341;&#36215;&#30340;&#29305;&#24449;&#35206;&#30422;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#22320;&#25805;&#25511;&#36825;&#20123;&#36335;&#24452;&#20013;&#30340;&#29305;&#24449;&#28608;&#27963;&#24046;&#24322;&#24182;&#35774;&#35745;&#33391;&#22909;&#30340;&#27491;&#21017;&#39033;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#21644;&#35825;&#23548;CO&#65292;&#20026;&#36825;&#19968;&#35266;&#23519;&#25552;&#20379;&#36827;&#19968;&#27493;&#35777;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18211v1 Announce Type: new  Abstract: Fast Adversarial Training (FAT) has gained increasing attention within the research community owing to its efficacy in improving adversarial robustness. Particularly noteworthy is the challenge posed by catastrophic overfitting (CO) in this field. Although existing FAT approaches have made strides in mitigating CO, the ascent of adversarial robustness occurs with a non-negligible decline in classification accuracy on clean samples. To tackle this issue, we initially employ the feature activation differences between clean and adversarial examples to analyze the underlying causes of CO. Intriguingly, our findings reveal that CO can be attributed to the feature coverage induced by a few specific pathways. By intentionally manipulating feature activation differences in these pathways with well-designed regularization terms, we can effectively mitigate and induce CO, providing further evidence for this observation. Notably, models trained sta
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#39640;&#32500;&#24230;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.18198</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning for Multi-Label Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18198
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#39640;&#32500;&#24230;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18198v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#33258;&#21160;&#26426;&#22120;&#23398;&#20064;(AutoML)&#26088;&#22312;&#36873;&#25321;&#21644;&#37197;&#32622;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#12290;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#20108;&#20803;&#21644;&#22810;&#39033;&#20998;&#31867;&#65292;&#21363;&#21333;&#26631;&#31614;&#20998;&#31867;(SLC)&#65292;&#36825;&#20123;AutoML&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22810;&#26631;&#31614;&#20998;&#31867;(MLC)&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#19982;&#19968;&#32452;&#31867;&#26631;&#31614;&#30456;&#20851;&#32852;&#32780;&#19981;&#26159;&#21333;&#20010;&#31867;&#26631;&#31614;&#65292;&#36804;&#20170;&#20026;&#27490;&#21463;&#21040;&#30340;&#20851;&#27880;&#35201;&#23569;&#24471;&#22810;&#12290;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;&#29305;&#23450;&#20110;&#25968;&#25454;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#21644;&#37197;&#32622;&#21363;&#20351;&#23545;&#20110;&#39046;&#22495;&#19987;&#23478;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#22810;&#23618;&#27425;&#20381;&#36182;&#20851;&#31995;&#30340;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#12290;&#32780;&#23545;&#20110;SLC&#32780;&#35328;&#65292;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#31354;&#38388;&#24050;&#32463;&#38750;&#24120;&#24222;&#22823;&#65292;MLC&#25628;&#32034;&#31354;&#38388;&#30340;&#22823;&#23567;&#27604;SLC&#30340;&#19968;&#20010;&#25968;&#37327;&#32423;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18198v1 Announce Type: new  Abstract: Automated machine learning (AutoML) aims to select and configure machine learning algorithms and combine them into machine learning pipelines tailored to a dataset at hand. For supervised learning tasks, most notably binary and multinomial classification, aka single-label classification (SLC), such AutoML approaches have shown promising results. However, the task of multi-label classification (MLC), where data points are associated with a set of class labels instead of a single class label, has received much less attention so far. In the context of multi-label classification, the data-specific selection and configuration of multi-label classifiers are challenging even for experts in the field, as it is a high-dimensional optimization problem with multi-level hierarchical dependencies. While for SLC, the space of machine learning pipelines is already huge, the size of the MLC search space outnumbers the one of SLC by several orders.   In 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22312;&#29616;&#20195;&#20132;&#36890;&#22330;&#26223;&#20013;&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#29305;&#28857;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#22871;&#32034;&#36825;&#31181; less explored &#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26694;&#26550;&#65292;&#25581;&#31034;&#20986;&#20256;&#32479;&#30340; ML-based &#26816;&#27979;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18167</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#22871;&#32034;&#23454;&#29616;&#20998;&#25955;&#24335;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Decentralised Traffic Incident Detection via Network Lasso
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22312;&#29616;&#20195;&#20132;&#36890;&#22330;&#26223;&#20013;&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#29305;&#28857;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#22871;&#32034;&#36825;&#31181; less explored &#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26694;&#26550;&#65292;&#25581;&#31034;&#20986;&#20256;&#32479;&#30340; ML-based &#26816;&#27979;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22312;&#20132;&#36890;&#24037;&#31243;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#36807;&#21435;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38598;&#20013;&#24335;&#35745;&#31639;&#33539;&#24335;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#25152;&#26377;&#25968;&#25454;&#34987;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#20197;&#26500;&#24314;ML&#27169;&#22411;&#12290;&#22914;&#20170;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#25955;&#24335;&#30340;&#26041;&#24335;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#30830;&#20445;&#26412;&#22320;&#25968;&#25454;&#27835;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20013;&#24515;&#30340;&#25216;&#26415;&#25513;&#30422;&#20102;&#25104;&#29087;&#30340;&#22522;&#20110;ML&#30340;&#26816;&#27979;&#26041;&#27861;&#30340;&#25928;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#29615;&#22659;&#19979;&#65292;&#24378;&#21170;&#30340;&#20256;&#32479;ML-based&#26816;&#27979;&#27169;&#22411;&#22312;&#29616;&#20195;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#20248;&#38597;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26694;&#26550;&#65292;&#21517;&#20026;&#32593;&#32476;&#22871;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18167v1 Announce Type: new  Abstract: Traffic incident detection plays a key role in intelligent transportation systems, which has gained great attention in transport engineering. In the past, traditional machine learning (ML) based detection methods achieved good performance under a centralised computing paradigm, where all data are transmitted to a central server for building ML models therein. Nowadays, deep neural networks based federated learning (FL) has become a mainstream detection approach to enable the model training in a decentralised manner while warranting local data governance. Such neural networks-centred techniques, however, have overshadowed the utility of well-established ML-based detection methods. In this work, we aim to explore the potential of potent conventional ML-based detection models in modern traffic scenarios featured by distributed data. We leverage an elegant but less explored distributed optimisation framework named Network Lasso, with guarant
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#23637;&#31034;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#23558;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#26174;&#33879;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18164</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#36890;&#29992;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#23458;&#25143;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based General Purpose Representation Learning for Customer Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18164
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#23637;&#31034;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#23558;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#26174;&#33879;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#23450;&#22522;&#30784;&#32467;&#26500;&#21450;&#20854;&#29983;&#25104;&#22240;&#32032;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#29992;&#20363;&#26080;&#20851;&#24212;&#29992;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#36890;&#36807;&#22810;&#32500;&#21521;&#37327;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#34920;&#31034;&#36825;&#20123;&#32467;&#26500;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#39640;&#24230;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#25554;&#25300;&#24335;&#12289;&#20016;&#23500;&#21644;&#21311;&#21517;&#21270;&#30340;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#65292;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#39640;&#36798;45&#65285;&#65292;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#22810;&#23618;&#25910;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#35745;&#31639;&#30340;&#37325;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18164v1 Announce Type: cross  Abstract: In recent years, exploiting the domain-specific underlying structure of data and its generative factors for representation learning has shown success in various use-case agnostic applications. However, the diversity and complexity of tabular data have made it challenging to represent these structures in a latent space through multi-dimensional vectors. We design an autoencoder-based framework for building general purpose embeddings, we assess the performance of different autoencoder architectures, and show simpler models outperform complex ones in embedding highly complex tabular data. We apply our framework to produce plug-and-play, rich, and anonymized embeddings representing AWS customers for usage in any model, saving up to 45% of development time, and observe significant improvements in downstream models. Moreover, we propose a significant improvement to the calculation of reconstruction loss for multi-layer contractive autoencode
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#20851;&#20110;&#39118;&#38505;&#25935;&#24863;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#28085;&#30422;&#38745;&#24577;&#21033;&#26222;&#24076;&#33576;&#39118;&#38505;&#24230;&#37327;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#21019;&#26032;&#30340;&#20803;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18159</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;&#21487;&#35777;&#26126;&#39118;&#38505;&#25935;&#24863;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18159
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#20851;&#20110;&#39118;&#38505;&#25935;&#24863;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#28085;&#30422;&#38745;&#24577;&#21033;&#26222;&#24076;&#33576;&#39118;&#38505;&#24230;&#37327;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#21019;&#26032;&#30340;&#20803;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39046;&#22495;&#20013;&#65292;&#32771;&#34385;&#39118;&#38505;&#23545;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20851;&#20110;&#39118;&#38505;&#25935;&#24863;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;RS-DisRL&#65289;&#30340;&#19968;&#33324;&#24615;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#38745;&#24577;&#21033;&#26222;&#24076;&#33576;&#39118;&#38505;&#24230;&#37327;&#65288;LRM&#65289;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39118;&#38505;&#25935;&#24863;RL&#31867;&#21035;&#65292;&#24182;&#26377;&#21161;&#20110;&#20998;&#26512;&#20272;&#35745;&#20989;&#25968;&#23545;RSRL&#31574;&#30053;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#20197;&#21450;&#35780;&#20272;&#23427;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#21019;&#26032;&#30340;&#20803;&#31639;&#27861;&#65306;\texttt{RS-DisRL-M}&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#20989;&#25968;&#36924;&#36817;&#30340;&#27169;&#22411;&#21270;&#31574;&#30053;&#65292;&#20197;&#21450;\texttt{RS-DisRL-V}&#65292;&#19968;&#31181;&#29992;&#20110;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65288;LSR&#65289;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30340;&#26032;&#39062;&#20272;&#35745;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#20998;&#24067;&#24335;RL&#20013;&#36827;&#34892;&#20102;&#22686;&#24378;&#30340;Ma
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18159v1 Announce Type: new  Abstract: In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: \texttt{RS-DisRL-M}, a model-based strategy for model-based function approximation, and \texttt{RS-DisRL-V}, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Ma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#38598;&#26465;&#20214;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18153</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Neural Network Weights Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#38598;&#26465;&#20214;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26159;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#20855;&#26377;&#26174;&#33879;&#20852;&#36259;&#30340;&#35805;&#39064;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#25913;&#21892;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#28304;&#25968;&#25454;&#19982;&#30446;&#26631;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#26159;&#30450;&#30446;&#36873;&#25321;&#65292;&#24182;&#24076;&#26395;&#23427;&#20204;&#33021;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27425;&#20248;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#38598;&#26465;&#20214;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#37319;&#26679;&#23454;&#29616;&#39640;&#25928;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#37325;&#24314;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#27599;&#20010;&#25968;&#25454;&#38598;&#26465;&#20214;&#19979;&#19968;&#32452;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#24067;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18153v1 Announce Type: cross  Abstract: Transfer learning is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks. While the performance of transfer learning depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets. Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task. To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive transfer learning scheme through dataset-conditioned pretrained weights sampling. Specifically, we use a latent diffusion model with a variational autoencoder that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for transfer learning on unseen datasets. By learning the distribution of a neural network on a var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#24102;&#26377;&#20107;&#21518;&#35266;&#23519;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#29305;&#23450;&#31639;&#27861;&#24182;&#35777;&#26126;&#20854;&#22312;&#27492;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18149</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#20107;&#21518;&#35266;&#23519;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#35777;&#30340;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#24102;&#26377;&#20107;&#21518;&#35266;&#23519;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#29305;&#23450;&#31639;&#27861;&#24182;&#35777;&#26126;&#20854;&#22312;&#27492;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#22312;&#24102;&#26377;&#20107;&#21518;&#35266;&#23519;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#24724;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#29702;&#35770;&#25506;&#32034;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#23558;&#20107;&#21518;&#35266;&#23519;&#25972;&#21512;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#26159;&#22312;&#29109;&#39118;&#38505;&#24230;&#37327;&#19979;&#20248;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#30340;&#31532;&#19968;&#20010;&#21487;&#35777;&#39640;&#25928;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20005;&#26684;&#20998;&#26512;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#21518;&#24724;$\tilde{O}\left(\frac{e^{|{\gamma}|H}-1}{|{\gamma}|H}H^2\sqrt{KHS^2OA}\right)$&#65292;&#24403;&#27169;&#22411;&#36864;&#21270;&#20026;&#39118;&#38505;&#20013;&#24615;&#25110;&#23436;&#20840;&#21487;&#35266;&#27979;&#35774;&#32622;&#26102;&#65292;&#23427;&#36229;&#36234;&#25110;&#21305;&#37197;&#29616;&#26377;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#37319;&#29992;&#21464;&#25442;&#27979;&#24230;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;beta&#21521;&#37327;&#20998;&#26512;&#24037;&#20855;&#26469;&#31616;&#21270;&#25968;&#23398;&#25512;&#23548;&#12290;&#36825;&#20123;&#25216;&#26415;&#29305;&#21035;&#24341;&#20154;&#27880;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18149v1 Announce Type: new  Abstract: This work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. We introduce a novel formulation that integrates hindsight observations into a Partially Observable Markov Decision Process (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure. We develop the first provably efficient RL algorithm tailored for this setting. We also prove by rigorous analysis that our algorithm achieves polynomial regret $\tilde{O}\left(\frac{e^{|{\gamma}|H}-1}{|{\gamma}|H}H^2\sqrt{KHS^2OA}\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. These techniques are of particular interes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.18137</link><description>&lt;p&gt;
DecisionNCE: &#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#19977;&#22823;&#30446;&#26631;&#65306;1&#65289;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65307;2&#65289;&#24378;&#21270;&#35270;&#35273;&#34920;&#31034;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65307;3&#65289;&#25429;&#33719;&#36712;&#36857;&#32423;&#35821;&#35328;&#22522;&#30784;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#22823;&#37096;&#20998;&#24050;&#26377;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#30340;&#30446;&#26631;&#26469;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#65292;&#24448;&#24448;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32479;&#19968;&#30446;&#26631;&#65292;&#21487;&#20197;&#21516;&#26102;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35821;&#35328;&#25351;&#20196;&#26080;&#32541;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#65292;&#22312;&#35270;&#35273;&#36712;&#36857;&#19982;&#20854;&#23545;&#24212;&#30340;&#35821;&#35328;&#25351;&#20196;&#30456;&#27604;&#19981;&#21305;&#37197;&#23545;&#26356;&#22909;&#22320;&#23545;&#40784;&#26102;&#65292;&#27969;&#34892;&#30340; Bradley-Terry &#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#37325;&#26032;&#21442;&#25968;&#21270;&#32780;&#21464;&#20026;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340; DecisionNCE &#26694;&#26550;&#65292;&#31867;&#20284;&#20110; InfoNC
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#31867;&#20934;&#30830;&#29575;&#24046;&#24322;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26377;&#38382;&#39064;&#30340;&#34920;&#31034;&#26041;&#24335;&#23548;&#33268;&#27169;&#22411;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31867;&#21035;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.18133</link><description>&lt;p&gt;
&#31867;&#19981;&#24179;&#31561;&#65306;&#20851;&#20110;&#22270;&#20687;&#35782;&#21035;&#20844;&#24179;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Classes Are Not Equal: An Empirical Study on Image Recognition Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18133
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#31867;&#20934;&#30830;&#29575;&#24046;&#24322;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26377;&#38382;&#39064;&#30340;&#34920;&#31034;&#26041;&#24335;&#23548;&#33268;&#27169;&#22411;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31867;&#21035;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22270;&#20687;&#35782;&#21035;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21363;&#22312;&#35832;&#22914;ImageNet&#20043;&#31867;&#30340;&#24179;&#34913;&#25968;&#25454;&#19978;&#23384;&#22312;&#26497;&#31471;&#31867;&#20934;&#30830;&#29575;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19981;&#21516;&#31867;&#21035;&#24182;&#19981;&#30456;&#31561;&#65292;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#27169;&#22411;&#23481;&#37327;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#20844;&#24179;&#24615;&#30340;&#20960;&#20010;&#26377;&#36259;&#29305;&#24615;&#12290;&#39318;&#20808;&#65292;&#19981;&#20844;&#24179;&#24615;&#20027;&#35201;&#28304;&#20110;&#26377;&#38382;&#39064;&#30340;&#34920;&#31034;&#65292;&#32780;&#38750;&#20998;&#31867;&#22120;&#20559;&#24046;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#8220;&#27169;&#22411;&#39044;&#27979;&#20559;&#24046;&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#38382;&#39064;&#34920;&#31034;&#30340;&#36215;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27169;&#22411;&#20542;&#21521;&#20110;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31867;&#21035;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;&#36825;&#24847;&#21619;&#30528;&#26356;&#22810;&#20854;&#20182;&#31867;&#21035;&#23558;&#19982;&#36739;&#38590;&#35782;&#21035;&#30340;&#31867;&#21035;&#28151;&#28102;&#12290;&#28982;&#21518;&#65292;&#20551;&#38451;&#20363;&#65288;FPs&#65289;&#23558;&#20027;&#23548;&#20248;&#21270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#23548;&#33268;&#23427;&#20204;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18133v1 Announce Type: new  Abstract: In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further,
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#26631;&#20934; DP &#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.18129</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18129
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#26631;&#20934; DP &#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20998;&#37197;&#26631;&#31614;&#26102;&#24456;&#23569;&#20381;&#36182;&#25935;&#24863;&#23646;&#24615;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26631;&#20934;DP&#65288;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#65289;&#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#38750;&#22343;&#21248;&#20998;&#24067;&#25935;&#24863;&#23646;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20998;&#31867;&#35268;&#21017;&#20559;&#21521;&#21344;&#25454;&#22823;&#22810;&#25968;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#25511;&#21046;DP-based&#20844;&#24179;&#23398;&#20064;&#20013;&#30340;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#65288;SA&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18129v1 Announce Type: cross  Abstract: Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; - &#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#30340;&#36974;&#32617;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18128</link><description>&lt;p&gt;
&#22312;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;&#22810;&#32423;&#20248;&#21270;&#30340;&#19979;&#28216;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; - &#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#30340;&#36974;&#32617;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26159;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#19968;&#20010;&#26174;&#33879;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#38543;&#26426;&#36974;&#32617;&#22270;&#20687;&#34917;&#19969;&#65292;&#24182;&#20351;&#29992;&#26410;&#36974;&#32617;&#30340;&#34917;&#19969;&#37325;&#24314;&#36825;&#20123;&#36974;&#32617;&#34917;&#19969;&#12290; MAE&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#24615;&#22312;&#20110;&#20854;&#24573;&#35270;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#37327;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#20250;&#32479;&#19968;&#36873;&#25321;&#35201;&#36974;&#32617;&#30340;&#34917;&#19969;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20123;&#26041;&#27861;&#25552;&#20986;&#22522;&#20110;&#34917;&#19969;&#20449;&#24687;&#37327;&#36827;&#34892;&#36974;&#32617;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#32771;&#34385;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#20219;&#21153;&#30340;&#34920;&#31034;&#27425;&#20248;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#36974;&#32617;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;MLO-MAE&#22312;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18128v1 Announce Type: cross  Abstract: Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#22810;&#20851;&#31995;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;HMGRL&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#25429;&#33719;&#26174;&#24335;&#20851;&#31995;&#65292;&#24320;&#21457;&#20102;&#22810;&#35270;&#22270;&#21487;&#24494;&#35889;&#32858;&#31867;&#27169;&#22359;&#25429;&#33719;&#38544;&#24335;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18127</link><description>&lt;p&gt;
&#20998;&#23618;&#22810;&#20851;&#31995;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22823;&#35268;&#27169;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Relational Graph Representation Learning for Large-Scale Prediction of Drug-Drug Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#22810;&#20851;&#31995;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;HMGRL&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#25429;&#33719;&#26174;&#24335;&#20851;&#31995;&#65292;&#24320;&#21457;&#20102;&#22810;&#35270;&#22270;&#21487;&#24494;&#35889;&#32858;&#31867;&#27169;&#22359;&#25429;&#33719;&#38544;&#24335;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#20027;&#35201;&#38598;&#20013;&#20110;&#25429;&#33719;&#33647;&#29289;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#33647;&#29289;&#23545;&#20043;&#38388;&#23384;&#22312;&#30340;&#26377;&#20215;&#20540;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#39044;&#27979;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#22810;&#20851;&#31995;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;HMGRL&#65289;&#26041;&#27861;&#12290;&#22312;HMGRL&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#21033;&#29992;&#20016;&#23500;&#30340;&#33647;&#29289;&#30456;&#20851;&#30340;&#24322;&#36136;&#25968;&#25454;&#28304;&#26500;&#24314;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#33647;&#29289;&#65292;&#36793;&#34920;&#31034;&#28165;&#26224;&#19988;&#21508;&#31181;&#20851;&#32852;&#12290;&#25105;&#20204;&#37319;&#29992;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;RGCN&#65289;&#26469;&#25429;&#25417;&#36825;&#20123;&#24322;&#36136;&#22270;&#20013;&#33647;&#29289;&#20043;&#38388;&#19981;&#21516;&#30340;&#26174;&#24335;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#21487;&#24494;&#35889;&#32858;&#31867;&#65288;MVDSC&#65289;&#27169;&#22359;&#65292;&#26469;&#25429;&#25417;DP&#20043;&#38388;&#30340;&#22810;&#20010;&#26377;&#20215;&#20540;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18127v1 Announce Type: new  Abstract: Most existing methods for predicting drug-drug interactions (DDI) predominantly concentrate on capturing the explicit relationships among drugs, overlooking the valuable implicit correlations present between drug pairs (DPs), which leads to weak predictions. To address this issue, this paper introduces a hierarchical multi-relational graph representation learning (HMGRL) approach. Within the framework of HMGRL, we leverage a wealth of drug-related heterogeneous data sources to construct heterogeneous graphs, where nodes represent drugs and edges denote clear and various associations. The relational graph convolutional network (RGCN) is employed to capture diverse explicit relationships between drugs from these heterogeneous graphs. Additionally, a multi-view differentiable spectral clustering (MVDSC) module is developed to capture multiple valuable implicit correlations between DPs. Within the MVDSC, we utilize multiple DP features to co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PRCL&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20687;&#32032;&#32423;&#34920;&#31034;&#24314;&#27169;&#20026;&#27010;&#29575;&#34920;&#31034;&#65292;&#35843;&#25972;&#27169;&#31946;&#34920;&#31034;&#30340;&#36129;&#29486;&#65292;&#24182;&#24341;&#20837;&#20840;&#23616;&#20998;&#24067;&#21407;&#22411;&#26469;&#22686;&#24378;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18117</link><description>&lt;p&gt;
PRCL&#65306;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#27010;&#29575;&#34920;&#31034;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18117
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PRCL&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20687;&#32032;&#32423;&#34920;&#31034;&#24314;&#27169;&#20026;&#27010;&#29575;&#34920;&#31034;&#65292;&#35843;&#25972;&#27169;&#31946;&#34920;&#31034;&#30340;&#36129;&#29486;&#65292;&#24182;&#24341;&#20837;&#20840;&#23616;&#20998;&#24067;&#21407;&#22411;&#26469;&#22686;&#24378;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;S4&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#31361;&#30772;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#20102;&#19968;&#20123;&#26377;&#38480;&#26631;&#31614;&#30340;&#38750;&#30417;&#30563;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#25351;&#23548;&#23545;&#26080;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#20102;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#36825;&#24517;&#28982;&#23384;&#22312;&#22122;&#22768;&#24182;&#25200;&#20081;&#20102;&#26080;&#30417;&#30563;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20110;&#23545;&#27604;&#30340;S4&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#27010;&#29575;&#34920;&#31034;&#23545;&#27604;&#23398;&#20064;&#65288;PRCL&#65289;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#26080;&#30417;&#30563;&#35757;&#32451;&#36807;&#31243;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#22810;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#23558;&#20687;&#32032;&#32423;&#34920;&#31034;&#24314;&#27169;&#20026;&#27010;&#29575;&#34920;&#31034;&#65288;PR&#65289;&#65292;&#35843;&#25972;&#27169;&#31946;&#34920;&#31034;&#30340;&#36129;&#29486;&#20197;&#23481;&#24525;&#23545;&#27604;&#23398;&#20064;&#20013;&#19981;&#20934;&#30830;&#25351;&#23548;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#25910;&#38598;&#25152;&#26377;PR&#65292;&#24341;&#20837;&#20840;&#23616;&#20998;&#24067;&#21407;&#22411;&#65288;GDP&#65289;&#12290;&#30001;&#20110;GDP&#21253;&#21547;&#25152;&#26377;&#34920;&#31034;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18117v1 Announce Type: cross  Abstract: Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24230;&#37327;&#23398;&#20064;&#21644;&#30417;&#30563;&#26041;&#27861;&#25972;&#21512;&#21040;fNIRS&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#22312;&#35782;&#21035;&#21644;&#25490;&#38500;&#20998;&#24067;&#20043;&#22806;&#30340;&#24322;&#24120;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21508;&#31181;fNIRS&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;transformer-based&#30340;&#32593;&#32476;&#65292;&#26174;&#31034;&#20986;&#26356;&#22823;&#30340;&#21487;&#38752;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18112</link><description>&lt;p&gt;
&#31616;&#21333;&#32780;&#26377;&#25928;&#65306;&#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#23398;&#20064;&#22312;fNIRS&#20013;&#25490;&#38500;&#24322;&#24120;&#36755;&#20837;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18112
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24230;&#37327;&#23398;&#20064;&#21644;&#30417;&#30563;&#26041;&#27861;&#25972;&#21512;&#21040;fNIRS&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#22312;&#35782;&#21035;&#21644;&#25490;&#38500;&#20998;&#24067;&#20043;&#22806;&#30340;&#24322;&#24120;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21508;&#31181;fNIRS&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;transformer-based&#30340;&#32593;&#32476;&#65292;&#26174;&#31034;&#20986;&#26356;&#22823;&#30340;&#21487;&#38752;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#36817;&#32418;&#22806;&#20809;&#35889;&#65288;fNIRS&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#22823;&#33041;&#27963;&#21160;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#33041;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#24212;&#23545;fNIRS&#25968;&#25454;&#30340;&#20998;&#31867;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;fNIRS&#20013;&#30340;&#32593;&#32476;&#22312;&#20854;&#35757;&#32451;&#20998;&#24067;&#20869;&#36827;&#34892;&#39044;&#27979;&#26102;&#20934;&#30830;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#35782;&#21035;&#21644;&#25490;&#38500;&#20998;&#24067;&#20043;&#22806;&#30340;&#24322;&#24120;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24433;&#21709;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#24230;&#37327;&#23398;&#20064;&#21644;&#30417;&#30563;&#26041;&#27861;&#25972;&#21512;&#21040;fNIRS&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#22312;&#35782;&#21035;&#21644;&#25490;&#38500;&#20998;&#24067;&#20043;&#22806;&#30340;&#24322;&#24120;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;fNIRS&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;transformer&#30340;&#32593;&#32476;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#23558;&#22312;GitHub&#19978;&#20844;&#24320;&#25105;&#20204;&#30340;&#23454;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18112v1 Announce Type: cross  Abstract: Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for monitoring brain activity. To better understand the brain, researchers often use deep learning to address the classification challenges of fNIRS data. Our study shows that while current networks in fNIRS are highly accurate for predictions within their training distribution, they falter at identifying and excluding abnormal data which is out-of-distribution, affecting their reliability. We propose integrating metric learning and supervised methods into fNIRS research to improve networks capability in identifying and excluding out-of-distribution outliers. This method is simple yet effective. In our experiments, it significantly enhances the performance of various networks in fNIRS, particularly transformer-based one, which shows the great improvement in reliability. We will make our experiment data available on GitHub.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;KV&#32531;&#23384;&#21387;&#32553;&#20013;&#19981;&#20002;&#24323;&#20196;&#29260;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20445;&#30041;&#34987;&#39537;&#36880;KV&#23545;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#21487;&#20197;&#36991;&#20813;&#23433;&#20840;&#28431;&#27934;&#12289;&#24187;&#35273;&#21644;&#19978;&#19979;&#25991;&#20002;&#22833;&#12290;</title><link>https://arxiv.org/abs/2402.18096</link><description>&lt;p&gt;
&#19981;&#20002;&#24323;&#20219;&#20309;&#20196;&#29260;: &#36890;&#36807;&#37325;&#35201;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23454;&#29616;&#21487;&#38752;&#30340;KV&#32531;&#23384;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18096
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;KV&#32531;&#23384;&#21387;&#32553;&#20013;&#19981;&#20002;&#24323;&#20196;&#29260;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20445;&#30041;&#34987;&#39537;&#36880;KV&#23545;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#21487;&#20197;&#36991;&#20813;&#23433;&#20840;&#28431;&#27934;&#12289;&#24187;&#35273;&#21644;&#19978;&#19979;&#25991;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#36895;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#36895;&#24230;&#21644;&#21534;&#21520;&#37327;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25209;&#37327;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#38271;&#65292;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21344;&#29992;&#25104;&#20026;LLM&#37096;&#32626;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;&#65292;&#24120;&#24120;&#36229;&#36807;&#27169;&#22411;&#26412;&#36523;&#30340;&#22823;&#23567;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#36873;&#25321;&#21644;&#39537;&#36880;&#32531;&#23384;&#20013;&#30340;&#19981;&#37325;&#35201;KV&#23545;&#20197;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65292;&#20294;&#39537;&#36880;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#28508;&#22312;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#26816;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#32531;&#23384;&#39537;&#36880;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#30001;&#20110;KV&#23545;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#34987;&#24443;&#24213;&#20002;&#24323;&#32780;&#23548;&#33268;&#23433;&#20840;&#28431;&#27934;&#12289;&#24187;&#35273;&#21644;&#19978;&#19979;&#25991;&#20002;&#22833;&#30340;&#19981;&#33391;&#21518;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#36890;&#36807;&#38477;&#20302;&#31934;&#24230;&#20445;&#30041;&#34987;&#39537;&#36880;KV&#23545;&#20013;&#21253;&#21547;&#30340;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18096v1 Announce Type: cross  Abstract: Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced prec
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#35780;&#20272;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#35268;&#21010;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18064</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#36801;&#31227;&#23398;&#20064;&#33258;&#21160;&#27979;&#35797;&#31354;&#38388;&#30456;&#20851;&#29615;&#22659;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18064
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#35780;&#20272;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#35268;&#21010;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#37319;&#26679;&#23545;&#25143;&#22806;&#20449;&#24687;&#25910;&#38598;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#39640;&#26114;&#30340;&#37319;&#26679;&#25104;&#26412;&#65292;&#22914;&#26102;&#38388;&#12289;&#33021;&#37327;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#29615;&#22659;&#30772;&#22351;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#20808;&#39564;&#25968;&#25454;&#21487;&#20197;&#26159;&#25552;&#39640;&#25928;&#29575;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#19982;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#36890;&#24120;&#20107;&#20808;&#26410;&#30693;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21033;&#29992;&#27492;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#35268;&#21010;&#25928;&#29575;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#31181;&#32452;&#21512;&#65292;&#23427;&#21487;&#20197;&#30740;&#31350;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#31354;&#38388;&#65292;&#24182;&#21363;&#26102;&#35780;&#20272;&#36825;&#20123;&#20551;&#35774;&#65292;&#20351;&#27492;&#26032;&#30693;&#35782;&#33021;&#22815;&#31435;&#21363;&#20026;&#26410;&#26469;&#35745;&#21010;&#25152;&#21033;&#29992;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18064v1 Announce Type: cross  Abstract: The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18059</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#28508;&#22312;&#22320;&#23384;&#22312;&#35823;&#23548;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#21306;&#20998;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#26469;&#21152;&#20197;&#35268;&#33539;&#30340;&#24517;&#35201;&#24615;&#12290;&#27700;&#21360;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#22312;LLM&#25512;&#29702;&#38454;&#27573;&#21521;&#25991;&#26412;&#20013;&#23884;&#20837;&#38544;&#34255;&#26631;&#35760;&#65292;&#32780;&#36825;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#19981;&#21487;&#24863;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#31639;&#27861;&#38754;&#20020;&#30528;&#23454;&#29616;&#25554;&#20837;&#27700;&#21360;&#30340;&#21487;&#26816;&#27979;&#24615;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#22686;&#24378;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#24120;&#24120;&#20250;&#25439;&#23475;&#21478;&#19968;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27700;&#21360;&#25216;&#26415;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MOO&#26469;&#20248;&#21270;&#26816;&#27979;&#21644;&#35821;&#20041;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#25913;&#21892;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#32437;&#21521;&#27169;&#24335;&#24314;&#27169;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.18046</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#20581;&#24247;&#35760;&#24405;&#65292;&#24182;&#24212;&#29992;&#20110;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data augmentation method for modeling health records with applications to clopidogrel treatment failure detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18046
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#25913;&#21892;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#32437;&#21521;&#27169;&#24335;&#24314;&#27169;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#23545;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#24314;&#27169;&#32437;&#21521;&#27169;&#24335;&#26102;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#25490;&#21015;&#23601;&#35786;&#36807;&#31243;&#20013;&#21307;&#30103;&#35760;&#24405;&#30340;&#39034;&#24207;&#26469;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20854;&#20013;&#20803;&#32032;&#30340;&#39034;&#24207;&#19981;&#26126;&#26174;&#65292;&#22914;&#26524;&#26377;&#30340;&#35805;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#26816;&#27979;&#20219;&#21153;&#26102;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#35813;&#26041;&#27861;&#20351;&#24471; ROC-AUC &#32477;&#23545;&#25913;&#36827;&#36798;&#21040;&#20102; 5.3%&#65288;&#20174;&#26410;&#36827;&#34892;&#22686;&#24378;&#30340; 0.908 &#25913;&#36827;&#20026;&#20351;&#29992;&#22686;&#24378;&#30340; 0.961&#65289;&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#22312;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#65292;&#22686;&#24378;&#26377;&#21161;&#20110;&#25913;&#21892;&#24494;&#35843;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18046v1 Announce Type: new  Abstract: We present a novel data augmentation method to address the challenge of data scarcity in modeling longitudinal patterns in Electronic Health Records (EHR) of patients using natural language processing (NLP) algorithms. The proposed method generates augmented data by rearranging the orders of medical records within a visit where the order of elements are not obvious, if any. Applying the proposed method to the clopidogrel treatment failure detection task enabled up to 5.3% absolute improvement in terms of ROC-AUC (from 0.908 without augmentation to 0.961 with augmentation) when it was used during the pre-training procedure. It was also shown that the augmentation helped to improve performance during fine-tuning procedures, especially when the amount of labeled training data is limited.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#26032;&#21457;&#29616;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#65306;&#31215;&#20998;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18040</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#21457;&#29616;&#31215;&#20998;
&lt;/p&gt;
&lt;p&gt;
Automated Discovery of Integral with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#26032;&#21457;&#29616;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#65306;&#31215;&#20998;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#25110;&#35299;&#20915;&#32534;&#31243;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#35299;&#20915;&#26126;&#30830;&#23450;&#20041;&#38382;&#39064;&#30340;&#33021;&#21147;&#19982;&#36827;&#34892;&#31185;&#23398;&#21457;&#29616;&#30340;&#24494;&#22937;&#36807;&#31243;&#26377;&#30528;&#26174;&#33879;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#26032;&#21457;&#29616;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#65306;&#31215;&#20998; &#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23558;&#31215;&#20998;&#23450;&#20041;&#20026;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#25512;&#23548;&#32473;&#23450;&#20989;&#25968;&#30340;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18040v1 Announce Type: new  Abstract: Recent advancements in the realm of deep learning, particularly in the development of large language models (LLMs), have demonstrated AI's ability to tackle complex mathematical problems or solving programming challenges. However, the capability to solve well-defined problems based on extensive training data differs significantly from the nuanced process of making scientific discoveries. Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do.   In this study we delve into the potential of using deep learning to rediscover a fundamental mathematical concept: integrals. By defining integrals as area under the curve, we illustrate how AI can deduce the integral of a given function,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#35302;&#21457;&#30340;SAGA&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#20449;&#39640;&#25928;&#30340;ConFederated Learning&#65292;&#22312;&#22810;&#26381;&#21153;&#22120;FL&#26694;&#26550;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18018</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#30340;ConFederated Learning: &#22522;&#20110;&#20107;&#20214;&#35302;&#21457;&#30340;SAGA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Communication Efficient ConFederated Learning: An Event-Triggered SAGA Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#35302;&#21457;&#30340;SAGA&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#20449;&#39640;&#25928;&#30340;ConFederated Learning&#65292;&#22312;&#22810;&#26381;&#21153;&#22120;FL&#26694;&#26550;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26088;&#22312;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#32780;&#26080;&#38656;&#25910;&#38598;&#20998;&#25955;&#22312;&#21508;&#31181;&#25968;&#25454;&#28304;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#26631;&#20934;FL&#21482;&#33021;&#25903;&#25345;&#26377;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#20064;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#22810;&#26381;&#21153;&#22120;FL&#26694;&#26550;&#65292;&#31216;&#20026;"ConFederated Learning"&#65288;CFL&#65289;&#65292;&#20197;&#23481;&#32435;&#26356;&#22810;&#29992;&#25143;&#12290;CFL&#31995;&#32479;&#30001;&#22810;&#20010;&#32593;&#32476;&#36793;&#32536;&#26381;&#21153;&#22120;&#32452;&#25104;&#65292;&#27599;&#20010;&#26381;&#21153;&#22120;&#36830;&#25509;&#21040;&#19968;&#32452;&#29420;&#31435;&#30340;&#29992;&#25143;&#12290;&#21033;&#29992;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20998;&#25955;&#21327;&#20316;&#26469;&#21033;&#29992;&#25152;&#26377;&#29992;&#25143;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#30001;&#20110;&#28041;&#21450;&#30340;&#29992;&#25143;&#25968;&#37327;&#21487;&#33021;&#24456;&#22823;&#65292;&#22240;&#27492;&#20943;&#23569;CFL&#31995;&#32479;&#30340;&#36890;&#20449;&#24320;&#38144;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;CFL&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#31181;&#26377;&#26465;&#20214;&#35302;&#21457;&#30340;&#29992;&#25143;&#36873;&#25321;&#65288;CTU
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18018v1 Announce Type: new  Abstract: Federated learning (FL) is a machine learning paradigm that targets model training without gathering the local data dispersed over various data sources. Standard FL, which employs a single server, can only support a limited number of users, leading to degraded learning capability. In this work, we consider a multi-server FL framework, referred to as \emph{Confederated Learning} (CFL), in order to accommodate a larger number of users. A CFL system is composed of multiple networked edge servers, with each server connected to an individual set of users. Decentralized collaboration among servers is leveraged to harness all users' data for model training. Due to the potentially massive number of users involved, it is crucial to reduce the communication overhead of the CFL system. We propose a stochastic gradient method for distributed learning in the CFL framework. The proposed method incorporates a conditionally-triggered user selection (CTU
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18012</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#32422;&#26463;&#25277;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18012
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#22312;&#20998;&#26512;&#23458;&#35266;&#20989;&#25968;&#25110;&#32422;&#26463;&#19981;&#21487;&#29992;&#26102;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#26410;&#30693;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20294;&#26377;&#38480;&#30740;&#31350;&#20851;&#27880;&#20102;&#32422;&#26463;&#26465;&#20214;&#26410;&#26126;&#30830;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#36341;&#20013;&#19981;&#29616;&#23454;&#30340;&#34394;&#20551;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#26410;&#30693;&#32422;&#26463;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#23558;&#20248;&#21270;&#36807;&#31243;&#38480;&#21046;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#22686;&#24378;&#25277;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#39044;&#28909;&#65292;&#28982;&#21518;&#26159;Langevin&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18012v1 Announce Type: cross  Abstract: Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dyna
&lt;/p&gt;</description></item><item><title>Mixer&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#65292;&#20195;&#34920;&#20102;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#38656;&#27714;&#21019;&#24314;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#28151;&#21512;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.18007</link><description>&lt;p&gt;
Mixer&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixer is more than just a model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18007
&lt;/p&gt;
&lt;p&gt;
Mixer&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#65292;&#20195;&#34920;&#20102;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#38656;&#27714;&#21019;&#24314;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;MLP&#32467;&#26500;&#37325;&#26032;&#21463;&#21040;&#20851;&#27880;&#65292;&#20854;&#20013;MLP-Mixer&#20197;&#20854;&#31361;&#20986;&#30340;&#34920;&#29616;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;MLP-Mixer&#20197;&#20174;&#36890;&#36947;&#21644;&#20196;&#29260;&#20004;&#20010;&#35282;&#24230;&#25552;&#21462;&#25968;&#25454;&#20449;&#24687;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#26377;&#25928;&#22320;&#20316;&#20026;&#36890;&#36947;&#20449;&#24687;&#21644;&#20196;&#29260;&#20449;&#24687;&#30340;&#34701;&#21512;&#12290;&#20107;&#23454;&#19978;&#65292;Mixer&#20195;&#34920;&#20102;&#19968;&#31181;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;Mixer&#30340;&#31934;&#39635;&#22312;&#20110;&#23427;&#33021;&#22815;&#20174;&#22810;&#20803;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#65292;&#20856;&#22411;&#22320;&#20307;&#29616;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#39046;&#22495;&#30340;&#8220;&#28151;&#21512;&#8221;&#30495;&#27491;&#27010;&#24565;&#12290;&#38500;&#20102;&#32771;&#34385;&#36890;&#36947;&#21644;&#20196;&#29260;&#20197;&#22806;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#35282;&#24230;&#21019;&#36896;&#26356;&#36148;&#21512;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#30340;&#28151;&#21512;&#22120;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#38899;&#39057;&#35782;&#21035;&#39046;&#22495;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#24102;Roll-Time&#21644;Hermit FFT&#30340;&#38899;&#39057;&#39057;&#35889;&#28151;&#21512;&#22120;(ASM-RH)&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#23545;&#26102;&#38388;&#21644;&#39057;&#29575;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18007v1 Announce Type: cross  Abstract: Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information. The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of "mixing" in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and freq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#38646;&#20214;&#35013;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18002</link><description>&lt;p&gt;
&#32771;&#34385;&#23545;&#31216;&#24615;&#30340;&#36719;&#33109;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#26426;&#22120;&#20154;&#35013;&#37197;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#38646;&#20214;&#35013;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#36719;&#33109;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#20195;&#34920;&#24615;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23500;&#25509;&#35302;PEG-IN-HOLE&#20219;&#21153;&#65292;&#35813;&#36719;&#33109;&#21487;&#20197;&#27604;&#21018;&#24615;&#33109;&#37096;&#26356;&#23433;&#20840;&#22320;&#25805;&#20316;&#24182;&#23481;&#24525;&#36739;&#20302;&#39057;&#29575;&#30340;&#25511;&#21046;&#20449;&#21495;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#23436;&#20840;&#21487;&#35266;&#27979;&#20844;&#24335;&#19981;&#21516;&#65292;&#35813;&#20844;&#24335;&#38656;&#35201;&#22806;&#37096;&#35774;&#32622;&#25110;&#20272;&#35745;&#22120;&#26469;&#33719;&#21462;PEG-TO-HOLE&#23039;&#24577;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#20844;&#24335;&#21644;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31034;&#33539;&#26469;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#23436;&#20840;&#22522;&#20110;&#35302;&#35273;&#21644;&#26412;&#20307;&#24863;&#30693;&#20449;&#21495;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#26410;&#34701;&#21512;&#28508;&#22312;&#39046;&#22495;&#23545;&#31216;&#24615;&#65292;&#22240;&#27492;&#24517;&#39035;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#24182;&#26500;&#24314;&#36741;&#21161;&#25439;&#22833;&#26469;&#24378;&#36843;&#20195;&#29702;&#36981;&#23432;&#23545;&#31216;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#23545;&#31216;PEG&#24418;&#29366;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#21487;&#20197;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18002v1 Announce Type: cross  Abstract: This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.17992</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#38050;&#26694;&#26550;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20256;&#32479;&#25968;&#20540;&#27169;&#25311;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#20803;&#27169;&#22411;&#24314;&#27169;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#26174;&#31034;&#20986;&#27169;&#22411;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#20016;&#23500;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#28508;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#21551;&#21457;&#26426;&#22120;&#23398;&#20064;&#65288;PiML&#65289;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;&#22522;&#26412;&#27010;&#24565;&#26159;&#23558;ML&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#32422;&#26463;&#22312;&#24050;&#30693;&#30340;&#29289;&#29702;&#33539;&#22260;&#20869;&#12290;&#36825;&#26159;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#29305;&#28857;&#23454;&#29616;&#30340;&#65292;&#21363;&#27169;&#22411;&#38477;&#38454;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#21644;&#29275;&#39039;&#31532;&#20108;&#23450;&#24459;&#65288;&#20363;&#22914;&#65292;&#36816;&#21160;&#26041;&#31243;&#65289;&#12290;&#27169;&#22411;&#38477;&#38454;&#23545;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20887;&#20313;&#24615;&#21644;&#22686;&#24378;&#24615;&#30340;&#32467;&#26500;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17992v1 Announce Type: cross  Abstract: There is a growing interest in utilizing machine learning (ML) methods for structural metamodeling due to the substantial computational cost of traditional numerical simulations. The existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data. To address these challenges, this paper presents a novel physics-informed machine learning (PiML) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures. The basic concept is to constrain the solution space of the ML model within known physical bounds. This is made possible with three main features, namely, model order reduction, a long short-term memory (LSTM) networks, and Newton's second law (e.g., the equation of motion). Model order reduction is essential for handling structural systems with inherent redundancy and enh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#26089;&#26399;&#25298;&#32477;&#21644;&#23545;&#23436;&#25972;&#31243;&#24207;&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17988</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#22312;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#26089;&#26399;&#25298;&#32477;&#21644;&#23545;&#23436;&#25972;&#31243;&#24207;&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#31243;&#24207;&#21512;&#25104;&#21644;&#39640;&#32423;&#33258;&#21160;&#23436;&#25104;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#20854;&#36755;&#20986;&#20195;&#30721;&#22312;&#35821;&#27861;&#19978;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#20801;&#35768;&#26089;&#26399;&#25298;&#32477;&#35821;&#27861;&#19978;&#19981;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#29992;&#20110;&#22635;&#20805;&#20219;&#21153;&#30340;&#23436;&#25972;&#31243;&#24207;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#33021;&#22815;&#22312;&#20219;&#24847;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#30340;&#24038;&#21491;&#21830;&#19978;&#25805;&#20316;&#30340;Earley&#24335;&#35299;&#26512;&#22120;&#65292;&#24182;&#23558;&#22686;&#37327;&#35299;&#26512;&#21644;&#21830;&#25805;&#20316;&#25193;&#23637;&#21040;&#35768;&#22810;&#24120;&#35265;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#19978;&#19979;&#25991;&#25935;&#24863;&#29305;&#24615;&#12290;&#36825;&#20123;&#36129;&#29486;&#30340;&#32467;&#26524;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#36890;&#29992;&#21644;&#25166;&#23454;&#30340;&#24038;&#21491;&#21830;&#35299;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17988v1 Announce Type: cross  Abstract: Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct. This paper contributes an incremental parser that allows early rejection of syntactically incorrect code, as well as efficient detection of complete programs for fill-in-the-middle (FItM) tasks. We develop Earley-style parsers that operate over left and right quotients of arbitrary context-free grammars, and we extend our incremental parsing and quotient operations to several context-sensitive features present in the grammars of many common programming languages. The result of these contributions is an efficient, general, and well-grounded method for left and right quotient parsing.   To validate our theoretical contributions -- and the practical effectiveness of certain design decisions -- we evaluate our method on the particularly difficult case of FItM completion for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17987</link><description>&lt;p&gt;
&#22810;&#24577;&#38647;&#36798;&#23545;&#31354;&#20013;&#39134;&#34892;&#22120;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#26080;&#20154;&#26426;&#30340;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;RATR&#65289;&#28041;&#21450;&#21457;&#23556;&#30005;&#30913;&#27874;&#24182;&#23545;&#25509;&#25910;&#21040;&#30340;&#38647;&#36798;&#22238;&#27874;&#25191;&#34892;&#30446;&#26631;&#31867;&#22411;&#35782;&#21035;&#65292;&#23545;&#22269;&#38450;&#21644;&#33322;&#31354;&#33322;&#22825;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#22312;RATR&#20013;&#20248;&#20110;&#21333;&#24577;&#38647;&#36798;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#20013;&#30340;&#34701;&#21512;&#26041;&#27861;&#36890;&#24120;&#20197;&#27010;&#29575;&#26041;&#24335;&#27425;&#20248;&#22320;&#32452;&#21512;&#26469;&#33258;&#21508;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;RATR&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#65288;OBF&#65289;&#26469;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#12290;OBF&#22522;&#20110;&#26399;&#26395;0-1&#25439;&#22833;&#65292;&#26681;&#25454;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#21382;&#21490;&#35266;&#27979;&#26356;&#26032;&#30446;&#26631;&#26080;&#20154;&#26426;&#31867;&#22411;&#30340;&#36882;&#24402;&#36125;&#21494;&#26031;&#20998;&#31867;&#65288;RBC&#65289;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#30340;&#38543;&#26426;&#34892;&#36208;&#36712;&#36857;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#20849;&#28041;&#21450;&#19971;&#31181;&#26426;&#21160;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 Announce Type: cross  Abstract: Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven dro
&lt;/p&gt;</description></item><item><title>FlattenQuant&#26041;&#27861;&#36890;&#36807;&#23637;&#24179;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;</title><link>https://arxiv.org/abs/2402.17985</link><description>&lt;p&gt;
FlattenQuant: &#20351;&#29992;&#20998;&#24352;&#37327;&#37327;&#21270;&#25171;&#30772;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#35745;&#31639;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17985
&lt;/p&gt;
&lt;p&gt;
FlattenQuant&#26041;&#27861;&#36890;&#36807;&#23637;&#24179;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#25512;&#26029;&#30340;&#24310;&#36831;&#21644;LLMs&#30340;&#22823;GPU&#20869;&#23384;&#28040;&#32791;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#37096;&#32626;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlattenQuant&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#36827;&#34892;&#23637;&#24179;&#26469;&#26174;&#33879;&#38477;&#20302;&#24352;&#37327;&#30340;&#26368;&#22823;&#20540;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#20943;&#23567;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17985v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;LightGBM&#12289;XGBoost&#21644;LocalEnsemble&#27169;&#22359;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.17979</link><description>&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#35770;&#65306;&#20351;&#29992;LightGBM&#12289;XGBoost&#21644;LocalEnsemble&#36827;&#34892;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;LightGBM&#12289;XGBoost&#21644;LocalEnsemble&#27169;&#22359;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28040;&#36153;&#20449;&#36151;&#39046;&#22495;&#65292;&#20934;&#30830;&#30340;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#26159;&#39118;&#38505;&#32531;&#35299;&#21644;&#36151;&#27454;&#20915;&#31574;&#20248;&#21270;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#39046;&#22495;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#25361;&#25112;&#20256;&#32479;&#27169;&#22411;&#65292;&#24341;&#20837;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#31215;&#32047;&#22522;&#30784;&#30740;&#31350;&#21644;&#26368;&#26032;&#21019;&#26032;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#20934;&#30830;&#24615;&#26631;&#20934;&#65292;&#20026;&#35813;&#34892;&#19994;&#35774;&#31435;&#26032;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;LightGBM&#12289;XGBoost&#21644;LocalEnsemble&#27169;&#22359;&#30340;&#38598;&#25104;&#26041;&#27861;&#26694;&#26550;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#25552;&#20379;&#29420;&#29305;&#30340;&#36129;&#29486;&#65292;&#20197;&#22686;&#24378;&#22810;&#26679;&#24615;&#21644;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#35299;&#20915;&#20102;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17979v1 Announce Type: cross  Abstract: In the realm of consumer lending, accurate credit default prediction stands as a critical element in risk mitigation and lending decision optimization. Extensive research has sought continuous improvement in existing models to enhance customer experiences and ensure the sound economic functioning of lending institutions. This study responds to the evolving landscape of credit default prediction, challenging conventional models and introducing innovative approaches. By building upon foundational research and recent innovations, our work aims to redefine the standards of accuracy in credit default prediction, setting a new benchmark for the industry. To overcome these challenges, we present an Ensemble Methods framework comprising LightGBM, XGBoost, and LocalEnsemble modules, each making unique contributions to amplify diversity and improve generalization. By utilizing distinct feature sets, our methodology directly tackles limitations i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.17978</link><description>&lt;p&gt;
&#24819;&#35937;&#12289;&#21021;&#22987;&#21270;&#21644;&#25506;&#32034;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#25506;&#32034;&#23545;&#20110;&#22312;&#22797;&#26434;&#21327;&#35843;&#20219;&#21153;&#20013;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26368;&#20339;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20869;&#22312;&#22870;&#21169;&#26469;&#23454;&#29616;&#25215;&#35834;&#30340;&#25506;&#32034;&#65292;&#25110;&#32773;&#20351;&#29992;&#22522;&#20110;&#35282;&#33394;&#30340;&#23398;&#20064;&#26469;&#20998;&#35299;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#22312;&#25972;&#20010;&#21160;&#20316;-&#35266;&#23519;&#31354;&#38388;&#20013;&#36827;&#34892;&#38598;&#20307;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#24448;&#24448;&#38754;&#20020;&#33719;&#21462;&#29305;&#23450;&#32852;&#21512;&#21160;&#20316;&#24207;&#21015;&#20197;&#36798;&#21040;&#25104;&#21151;&#29366;&#24577;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;IIE&#21033;&#29992;&#19968;&#20010;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#24819;&#35937;&#26234;&#33021;&#20307;&#22914;&#20309;&#36798;&#21040;&#21487;&#20197;&#24433;&#21709;&#24444;&#27492;&#36716;&#31227;&#20989;&#25968;&#30340;&#20851;&#38190;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#22312;&#25506;&#32034;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22120;&#22312;&#36825;&#20010;&#29366;&#24577;&#19979;&#21021;&#22987;&#21270;&#29615;&#22659;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#23454;&#29616;&#36825;&#31181;&#24819;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17978v1 Announce Type: cross  Abstract: Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imag
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21482;&#38656;50&#20010;&#20559;&#22909;&#26631;&#31614;&#21363;&#21487;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;500&#20010;&#20559;&#22909;&#26631;&#31614;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#26356;&#22909;&#22320;&#24674;&#22797;&#22320;&#38754;&#30495;&#20540;&#22870;&#21169;&#31574;&#30053;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17975</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#26377;&#25928;&#24615;&#21450;&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17975
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21482;&#38656;50&#20010;&#20559;&#22909;&#26631;&#31614;&#21363;&#21487;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;500&#20010;&#20559;&#22909;&#26631;&#31614;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#26356;&#22909;&#22320;&#24674;&#22797;&#22320;&#38754;&#30495;&#20540;&#22870;&#21169;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#20174;&#23545;&#20195;&#29702;&#34892;&#20026;&#30340;&#20108;&#36827;&#21046;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#23558;&#26426;&#22120;&#20154;&#34892;&#20026;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#23558;PbRL&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#21160;&#24577;&#24863;&#30693;&#29366;&#24577;-&#21160;&#20316;&#34920;&#31034;&#65288;z^{sa&#65289;&#65289;&#21644;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#36845;&#20195;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#21487;&#20197;&#21152;&#24555;&#31574;&#30053;&#23398;&#20064;&#24182;&#25552;&#39640;&#26368;&#32456;&#31574;&#30053;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#22235;&#36275;&#34892;&#36208;&#12289;&#27493;&#34892;&#21644;&#29454;&#35961;&#22868;&#36305;&#31561;&#39046;&#22495;&#65292;&#20351;&#29992;50&#20010;&#20559;&#22909;&#26631;&#31614;&#30340;&#24615;&#33021;&#19982;&#20351;&#29992;500&#20010;&#20559;&#22909;&#26631;&#31614;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#21516;&#65292;&#24182;&#19988;&#25105;&#20204;&#24674;&#22797;&#20102;83\%&#21644;66\%&#30340;&#22320;&#38754;&#30495;&#20540;&#22870;&#21169;&#31574;&#30053;&#24615;&#33021;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21482;&#26377;38\%&#21644;21\%&#12290;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#23637;&#31034;&#20102;&#26126;&#30830;&#23398;&#20064;&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;&#27169;&#22411;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17975v1 Announce Type: new  Abstract: Preference-based reinforcement learning (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors. We show that dynamics-aware reward functions improve the sample efficiency of PbRL by an order of magnitude. In our experiments we iterate between: (1) learning a dynamics-aware state-action representation (z^{sa}) via a self-supervised temporal consistency task, and (2) bootstrapping the preference-based reward function from (z^{sa}), which results in faster policy learning and better final policy performance. For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover 83\% and 66\% of ground truth reward policy performance versus only 38\% and 21\%. The performance gains demonstrate the benefits of explicitly learning a dynamics-aware reward model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#27169;&#20223;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816;&#65288;I-OT&#65289;&#65292;&#36890;&#36807;&#27169;&#20223;&#20808;&#39564;&#20998;&#24067;&#26469;&#25552;&#39640;&#32593;&#32476;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17967</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#27169;&#20223;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816;&#65306;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#21450;&#20854;&#22312;&#29289;&#27969;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#27169;&#20223;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816;&#65288;I-OT&#65289;&#65292;&#36890;&#36807;&#27169;&#20223;&#20808;&#39564;&#20998;&#24067;&#26469;&#25552;&#39640;&#32593;&#32476;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#31995;&#32479;&#26500;&#25104;&#20102;&#29616;&#20195;&#31038;&#20250;&#30340;&#22522;&#30784;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#38754;&#20020;&#30528;&#30001;&#28798;&#38590;&#31561;&#19981;&#21487;&#39044;&#35265;&#24773;&#20917;&#24102;&#26469;&#30340;&#37325;&#22823;&#39118;&#38505;&#12290;&#37492;&#20110;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#30740;&#31350;&#21152;&#24378;&#32593;&#32476;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24050;&#32463;&#30830;&#23450;&#20102;&#33719;&#21462;&#40065;&#26834;&#24615;&#21644;&#27491;&#21017;&#21270;&#29109;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#19968;&#26694;&#26550;&#20869;&#20351;&#29992;&#20102;&#27169;&#20223;&#23398;&#20064;&#26469;&#21453;&#26144;&#19987;&#23478;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22312;&#32593;&#32476;&#19978;&#30340;&#26368;&#20248;&#36755;&#36816;&#20013;&#20351;&#29992;&#31867;&#20284;&#27169;&#20223;&#26694;&#26550;&#30340;&#20840;&#38754;&#30740;&#31350;&#36824;&#27809;&#26377;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#27169;&#20223;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816;&#65288;I-OT&#65289;&#12290;&#23427;&#36890;&#36807;&#27169;&#20223;&#32473;&#23450;&#30340;&#20808;&#39564;&#20998;&#24067;&#23545;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#32534;&#30721;&#12290;I-OT&#35299;&#20915;&#26041;&#26696;&#22312;&#32593;&#32476;&#19978;&#23450;&#20041;&#30340;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17967v1 Announce Type: new  Abstract: Network systems form the foundation of modern society, playing a critical role in various applications. However, these systems are at significant risk of being adversely affected by unforeseen circumstances, such as disasters. Considering this, there is a pressing need for research to enhance the robustness of network systems. Recently, in reinforcement learning, the relationship between acquiring robustness and regularizing entropy has been identified. Additionally, imitation learning is used within this framework to reflect experts' behavior. However, there are no comprehensive studies on the use of a similar imitation framework for optimal transport on networks. Therefore, in this study, imitation-regularized optimal transport (I-OT) on networks was investigated. It encodes prior knowledge on the network by imitating a given prior distribution. The I-OT solution demonstrated robustness in terms of the cost defined on the network. More
&lt;/p&gt;</description></item><item><title>Conformer&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.17966</link><description>&lt;p&gt;
Conformer&#65306;&#23558;&#36830;&#32493;&#27880;&#24847;&#21147;&#23884;&#20837;&#35270;&#35273;Transformer&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17966
&lt;/p&gt;
&lt;p&gt;
Conformer&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#24615;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#20381;&#36182;&#20110;&#35745;&#31639;&#26114;&#36149;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;Transformers&#26159;&#31163;&#25955;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#20854;&#23398;&#20064;&#21160;&#24577;&#22825;&#27668;&#31995;&#32479;&#36830;&#32493;&#26102;&#31354;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;Conformer&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#12290;Conformer&#26088;&#22312;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#34987;&#32534;&#30721;&#20026;Transformer&#26550;&#26500;&#20013;&#30340;&#21487;&#24494;&#20998;&#20989;&#25968;&#65292;&#20197;&#24314;&#27169;&#22797;&#26434;&#30340;&#22825;&#27668;&#21160;&#24577;&#12290;&#25105;&#20204;&#23558;Conformer&#19982;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#21644;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;Conformer&#22312;&#25152;&#26377;&#21069;&#23548;&#26102;&#38388;&#19978;&#20248;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17966v1 Announce Type: new  Abstract: Operational weather forecasting system relies on computationally expensive physics-based models. Although Transformers-based models have shown remarkable potential in weather forecasting, Transformers are discrete models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with Conformer, a spatio-temporal Continuous Vision Transformer for weather forecasting. Conformer is designed to learn the continuous weather evolution over time by implementing continuity in the multi-head attention mechanism. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. We evaluate Conformer against a state-of-the-art Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. Conformer outperforms some of the existing data-driven models at all lead times while 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#32780;&#23454;&#29616;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17943</link><description>&lt;p&gt;
&#20351;&#29992;SoS&#23494;&#24230;&#20272;&#35745;&#21644;&#945;-&#31163;&#25955;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Sequential transport maps using SoS density estimation and $\alpha$-divergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#32780;&#23454;&#29616;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20256;&#36755;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#36817;&#20284;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35843;&#26597;&#20102;&#25552;&#20986;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#31995;&#21015;&#32452;&#25104;&#30340;Knothe-Rosenblatt&#65288;KR&#65289;&#26144;&#23556;&#20043;&#19978;&#12290;&#20854;&#20013;&#27599;&#20010;&#26144;&#23556;&#37117;&#26159;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#20013;&#31561;&#22797;&#26434;&#24230;&#30340;&#20013;&#38388;&#23494;&#24230;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#20174;&#21442;&#32771;&#23494;&#24230;&#21040;&#39044;&#35745;&#31639;&#36817;&#20284;&#23494;&#24230;&#30340;&#31934;&#30830;KR&#26144;&#23556;&#32780;&#26500;&#24314;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23558;SoS&#23494;&#24230;&#19982;&#945;-&#31163;&#25955;&#24230;&#30456;&#32467;&#21512;&#20135;&#29983;&#20102;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#21322;&#23450;&#32534;&#31243;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#945;-&#31163;&#25955;&#24230;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20351;&#24471;&#33021;&#22815;&#22788;&#29702;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17943v1 Announce Type: cross  Abstract: Transport-based density estimation methods are receiving growing interest because of their ability to efficiently generate samples from the approximated density. We further invertigate the sequential transport maps framework proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first estimating an intermediate density of moderate complexity, and then by computing the exact KR map from a reference density to the precomputed approximate density. In our work, we explore the use of Sum-of-Squares (SoS) densities and $\alpha$-divergences for approximating the intermediate densities. Combining SoS densities with $\alpha$-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming. The main advantage of $\alpha$-divergences is to enable working with unnormalized densities, which provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#20197;&#23454;&#29616;&#23454;&#29992;&#30446;&#26631;&#36798;&#25104;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.17930</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35268;&#21010;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#20197;&#23454;&#29616;&#23454;&#29992;&#30446;&#26631;&#36798;&#25104;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#32463;&#24120;&#32473;&#20986;&#22312;&#32570;&#20047;&#36827;&#19968;&#27493;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#24847;&#20041;&#27169;&#31946;&#30340;&#25351;&#20196;&#65292;&#26399;&#26395;&#20182;&#20204;&#30340;&#34892;&#21160;&#25110;&#30446;&#26631;&#33021;&#28040;&#38500;&#19981;&#26126;&#30830;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#20197;&#28789;&#27963;&#12289;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26041;&#24335;&#36981;&#24490;&#36825;&#31867;&#25351;&#20196;&#30340;&#36741;&#21161;&#20195;&#29702;&#21602;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#19968;&#20010;&#21512;&#20316;&#35268;&#21010;&#32773;&#65292;&#23558;&#20849;&#21516;&#35745;&#21010;&#19982;&#21161;&#25163;&#36827;&#34892;&#36890;&#20449;&#65292;&#28982;&#21518;&#36890;&#36807;&#21160;&#20316;&#21644;&#35821;&#35328;&#25191;&#34892;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#22312;&#20551;&#35774;&#35745;&#21010;&#19979;&#32473;&#20986;&#30340;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#33719;&#24471;&#36825;&#19968;&#21518;&#39564;&#20998;&#24067;&#21518;&#65292;&#25105;&#20204;&#30340;&#21161;&#25163;&#36890;&#36807;&#34892;&#21160;&#26469;&#26368;&#23567;&#21270;&#26399;&#26395;&#30446;&#26631;&#23454;&#29616;&#25104;&#26412;&#65292;&#20351;&#20854;&#33021;&#22815;&#23454;&#29992;&#22320;&#36981;&#24490;&#21547;&#31946;&#30340;&#25351;&#20196;&#65292;&#24182;&#21363;&#20351;&#22312;&#23545;&#25351;&#20196;&#19981;&#30830;&#23450;&#26102;&#20063;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17930v1 Announce Type: new  Abstract: People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human's goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the g
&lt;/p&gt;</description></item><item><title>&#21487;&#20197;&#30452;&#25509;&#20174;&#24102;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#24517;&#35201;&#24615;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#27169;&#22411;&#65292;&#26174;&#33879;&#20943;&#23569;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;</title><link>https://arxiv.org/abs/2402.17926</link><description>&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#30340;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#30830;&#23450;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Certain and Approximately Certain Models for Statistical Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17926
&lt;/p&gt;
&lt;p&gt;
&#21487;&#20197;&#30452;&#25509;&#20174;&#24102;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#24517;&#35201;&#24615;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#27169;&#22411;&#65292;&#26174;&#33879;&#20943;&#23569;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#24182;&#19988;&#21253;&#21547;&#32570;&#22833;&#20540;&#12290;&#20026;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#29992;&#25143;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#22635;&#20805;&#21644;&#25214;&#21040;&#32570;&#22833;&#25968;&#25454;&#39033;&#30340;&#27491;&#30830;&#20540;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#35757;&#32451;&#25968;&#25454;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#20415;&#22312;&#21508;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#26816;&#26597;&#27492;&#24517;&#35201;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#32780;&#27809;&#26377;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17926v1 Announce Type: cross  Abstract: Real-world data is often incomplete and contains missing values. To train accurate models over real-world datasets, users need to spend a substantial amount of time and resources imputing and finding proper values for missing data items. In this paper, we demonstrate that it is possible to learn accurate models directly from data with missing values for certain training data and target models. We propose a unified approach for checking the necessity of data imputation to learn accurate models across various widely-used machine learning paradigms. We build efficient algorithms with theoretical guarantees to check this necessity and return accurate models in cases where imputation is unnecessary. Our extensive experiments indicate that our proposed algorithms significantly reduce the amount of time and effort needed for data imputation without imposing considerable computational overhead.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#20041;"&#25628;&#32034;&#32773;&#30340;&#22256;&#22659;"&#26469;&#26356;&#25509;&#36817;&#29616;&#23454;&#19990;&#30028;&#22320;&#24314;&#27169;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#38382;&#39064;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;HT&#24863;&#26579;&#21644;&#26080;&#24863;&#26579;&#30005;&#36335;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#22312;&#30005;&#36335;&#20998;&#31867;&#20013;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17918</link><description>&lt;p&gt;
&#25628;&#32034;&#32773;&#30340;&#22256;&#22659;&#65306;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#29616;&#23454;&#20844;&#24335;&#21270;&#21644;&#22522;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
The Seeker's Dilemma: Realistic Formulation and Benchmarking for Hardware Trojan Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#20041;"&#25628;&#32034;&#32773;&#30340;&#22256;&#22659;"&#26469;&#26356;&#25509;&#36817;&#29616;&#23454;&#19990;&#30028;&#22320;&#24314;&#27169;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#38382;&#39064;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;HT&#24863;&#26579;&#21644;&#26080;&#24863;&#26579;&#30005;&#36335;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#22312;&#30005;&#36335;&#20998;&#31867;&#20013;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#30828;&#20214;&#35774;&#35745;&#39046;&#22495;&#25512;&#36827;&#23433;&#20840;&#30740;&#31350;&#65292;&#36890;&#36807;&#27491;&#24335;&#23450;&#20041;&#30828;&#20214;&#29305;&#27931;&#20234;&#65288;HT&#65289;&#26816;&#27979;&#30340;&#29616;&#23454;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#26356;&#25509;&#36817;&#29616;&#23454;&#19990;&#30028;&#22320;&#24314;&#27169;HT&#26816;&#27979;&#65292;&#21363;&#20197;"&#25628;&#32034;&#32773;&#30340;&#22256;&#22659;"&#65288;&#22312;&#22270;&#19978;&#30340;&#25193;&#23637;&#29256;Hide&amp;Seek&#65289;&#25551;&#36848;&#38382;&#39064;&#65292;&#20854;&#20013;&#26816;&#27979;&#20195;&#29702;&#19981;&#30693;&#36947;&#30005;&#36335;&#26159;&#21542;&#34987;HT&#24863;&#26579;&#12290;&#21033;&#29992;&#36825;&#20010;&#29702;&#35770;&#38382;&#39064;&#20844;&#24335;&#21270;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#30001;&#28151;&#21512;&#30340;&#26080;HT&#21644;&#26377;HT&#24863;&#26579;&#30340;&#37325;&#26500;&#30005;&#36335;&#32452;&#25104;&#65292;&#21516;&#26102;&#20445;&#30041;&#23427;&#20204;&#30340;&#21407;&#22987;&#21151;&#33021;&#12290;&#37325;&#26500;&#30340;&#30005;&#36335;&#34987;&#38543;&#26426;&#24863;&#26579;HT&#65292;&#23548;&#33268;&#38450;&#24481;&#32773;&#19981;&#30830;&#23450;&#30005;&#36335;&#26159;&#21542;&#34987;&#24863;&#26579;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#21019;&#26032;&#25968;&#25454;&#38598;&#23558;&#24110;&#21161;&#31038;&#21306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#22312;&#30005;&#36335;&#20998;&#31867;&#20013;&#30340;&#25104;&#21151;&#29575;&#26469;&#26356;&#22909;&#22320;&#35780;&#21028;&#26816;&#27979;&#36136;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#24320;&#21457;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17918v1 Announce Type: cross  Abstract: This work focuses on advancing security research in the hardware design space by formally defining the realistic problem of Hardware Trojan (HT) detection. The goal is to model HT detection more closely to the real world, i.e., describing the problem as "The Seeker's Dilemma" (an extension of Hide&amp;Seek on a graph), where a detecting agent is unaware of whether circuits are infected by HTs or not. Using this theoretical problem formulation, we create a benchmark that consists of a mixture of HT-free and HT-infected restructured circuits while preserving their original functionalities. The restructured circuits are randomly infected by HTs, causing a situation where the defender is uncertain if a circuit is infected or not. We believe that our innovative dataset will help the community better judge the detection quality of different methods by comparing their success rates in circuit classification. We use our developed benchmark to eval
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;ICU&#20013;&#24120;&#35268;&#25910;&#38598;&#30340;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#24212;&#29992;&#32467;&#21512;&#20102;LSTM&#32593;&#32476;&#21644;&#21327;&#21516;&#36807;&#28388;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#35782;&#21035;&#24739;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#29983;&#29702;&#29366;&#24577;&#65292;&#24182;&#22312;&#23454;&#38469;&#20020;&#24202;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17917</link><description>&lt;p&gt;
ICU&#29983;&#29702;&#20449;&#21495;&#24120;&#35265;&#28508;&#22312;&#34920;&#31034;&#30340;&#21327;&#21516;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;ICU&#20013;&#24120;&#35268;&#25910;&#38598;&#30340;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#24212;&#29992;&#32467;&#21512;&#20102;LSTM&#32593;&#32476;&#21644;&#21327;&#21516;&#36807;&#28388;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#35782;&#21035;&#24739;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#29983;&#29702;&#29366;&#24577;&#65292;&#24182;&#22312;&#23454;&#38469;&#20020;&#24202;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#65292;&#20016;&#23500;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25552;&#20379;&#20102;&#22686;&#24378;&#24739;&#32773;&#34920;&#22411;&#30340;&#26426;&#20250;&#12290;&#19982;&#20808;&#21069;&#19987;&#27880;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24120;&#35268;&#25910;&#38598;&#30340;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#34920;&#22411;&#20998;&#31867;&#30340;ML&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#19982;&#21327;&#21516;&#36807;&#28388;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35782;&#21035;&#24739;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#29983;&#29702;&#29366;&#24577;&#12290;&#22312;&#29992;&#20110;&#33041;&#25439;&#20260;&#24739;&#32773;&#39045;&#20869;&#39640;&#21387;&#65288;IH&#65289;&#26816;&#27979;&#30340;&#23454;&#38469;ICU&#20020;&#24202;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;0.889&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#21644;0.725&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65288;AP&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23398;&#20064;&#29983;&#29702;&#20449;&#21495;&#30340;&#26356;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#26041;&#38754;&#20248;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21033;&#29992;&#24120;&#35268;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#24739;&#32773;&#34920;&#22411;&#20998;&#31867;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17917v1 Announce Type: new  Abstract: In Intensive Care Units (ICU), the abundance of multivariate time series presents an opportunity for machine learning (ML) to enhance patient phenotyping. In contrast to previous research focused on electronic health records (EHR), here we propose an ML approach for phenotyping using routinely collected physiological time series data. Our new algorithm integrates Long Short-Term Memory (LSTM) networks with collaborative filtering concepts to identify common physiological states across patients. Tested on real-world ICU clinical data for intracranial hypertension (IH) detection in patients with brain injury, our method achieved an area under the curve (AUC) of 0.889 and average precision (AP) of 0.725. Moreover, our algorithm outperforms autoencoders in learning more structured latent representations of the physiological signals. These findings highlight the promise of our methodology for patient phenotyping, leveraging routinely collecte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;AI&#36719;&#20214;&#21644;&#30828;&#20214;&#24212;&#29992;&#20110;&#25968;&#20540;&#24314;&#27169;&#39046;&#22495;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;AI&#26041;&#27861;&#65292;&#22914;CNN&#65292;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26631;&#20934;&#25805;&#20316;&#65292;&#24102;&#26469;&#39640;&#24615;&#33021;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#24615;&#21644;&#26131;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17913</link><description>&lt;p&gt;
&#20351;&#29992;AI&#24211;&#36827;&#34892;&#19981;&#21487;&#21387;&#32553;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Using AI libraries for Incompressible Computational Fluid Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;AI&#36719;&#20214;&#21644;&#30828;&#20214;&#24212;&#29992;&#20110;&#25968;&#20540;&#24314;&#27169;&#39046;&#22495;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;AI&#26041;&#27861;&#65292;&#22914;CNN&#65292;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26631;&#20934;&#25805;&#20316;&#65292;&#24102;&#26469;&#39640;&#24615;&#33021;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#24615;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#39640;&#25928;&#24320;&#28304;&#24211;&#65292;&#20197;&#22312;&#19981;&#21516;&#30340;&#35745;&#31639;&#26426;&#26550;&#26500;&#65288;&#20363;&#22914;CPU&#12289;GPU&#21644;&#26032;&#30340;AI&#22788;&#29702;&#22120;&#65289;&#19978;&#25191;&#34892;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#36825;&#19981;&#20165;&#20351;&#22522;&#20110;&#36825;&#20123;&#24211;&#30340;&#31639;&#27861;&#39640;&#25928;&#32780;&#19988;&#22312;&#19981;&#21516;&#26550;&#26500;&#20043;&#38388;&#21487;&#31227;&#26893;&#65292;&#36824;&#22823;&#22823;&#31616;&#21270;&#20102;&#20351;&#29992;AI&#24320;&#21457;&#26041;&#27861;&#30340;&#38376;&#27099;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#35770;&#65292;&#23558;AI&#36719;&#20214;&#21644;&#30828;&#20214;&#30340;&#24378;&#22823;&#21151;&#33021;&#24102;&#20837;&#25968;&#20540;&#24314;&#27169;&#39046;&#22495;&#65292;&#23558;AI&#26041;&#27861;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;CNN&#65289;&#37325;&#26032;&#29992;&#20110;&#25968;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26631;&#20934;&#25805;&#20316;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#23558;&#39640;&#24615;&#33021;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#24615;&#21644;&#26131;&#29992;&#24615;&#24341;&#20837;&#25968;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#20915;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17913v1 Announce Type: cross  Abstract: Recently, there has been a huge effort focused on developing highly efficient open source libraries to perform Artificial Intelligence (AI) related computations on different computer architectures (for example, CPUs, GPUs and new AI processors). This has not only made the algorithms based on these libraries highly efficient and portable between different architectures, but also has substantially simplified the entry barrier to develop methods using AI. Here, we present a novel methodology to bring the power of both AI software and hardware into the field of numerical modelling by repurposing AI methods, such as Convolutional Neural Networks (CNNs), for the standard operations required in the field of the numerical solution of Partial Differential Equations (PDEs). The aim of this work is to bring the high performance, architecture agnosticism and ease of use into the field of the numerical solution of PDEs. We use the proposed methodol
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27973;&#23618;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#26469;&#23398;&#20064;&#37327;&#23376;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#20581;&#22766;&#30340;&#27973;&#24433;&#21327;&#35758;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#35299;&#20915;&#37327;&#23376;&#22122;&#22768;&#21644;&#20559;&#24046;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.17911</link><description>&lt;p&gt;
&#29992;&#27973;&#24433;&#23398;&#20064;&#23637;&#31034;&#20581;&#22766;&#39640;&#25928;&#30340;&#37327;&#23376;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Demonstration of Robust and Efficient Quantum Property Learning with Shallow Shadows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17911
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27973;&#23618;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#26469;&#23398;&#20064;&#37327;&#23376;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#20581;&#22766;&#30340;&#27973;&#24433;&#21327;&#35758;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#35299;&#20915;&#37327;&#23376;&#22122;&#22768;&#21644;&#20559;&#24046;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#37327;&#23376;&#31995;&#32479;&#20013;&#39640;&#25928;&#25552;&#21462;&#20449;&#24687;&#26159;&#37327;&#23376;&#20449;&#24687;&#22788;&#29702;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38543;&#26426;&#21270;&#27979;&#37327;&#65292;&#25110;&#31216;&#32463;&#20856;&#38452;&#24433;&#65292;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#27979;&#37327;&#26469;&#39044;&#27979;&#20219;&#24847;&#37327;&#23376;&#24577;&#30340;&#35768;&#22810;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17911v1 Announce Type: cross  Abstract: Extracting information efficiently from quantum systems is a major component of quantum information processing tasks. Randomized measurements, or classical shadows, enable predicting many properties of arbitrary quantum states using few measurements. While random single qubit measurements are experimentally friendly and suitable for learning low-weight Pauli observables, they perform poorly for nonlocal observables. Prepending a shallow random quantum circuit before measurements maintains this experimental friendliness, but also has favorable sample complexities for observables beyond low-weight Paulis, including high-weight Paulis and global low-rank properties such as fidelity. However, in realistic scenarios, quantum noise accumulated with each additional layer of the shallow circuit biases the results. To address these challenges, we propose the robust shallow shadows protocol. Our protocol uses Bayesian inference to learn the expe
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#37325;&#22270;&#20013;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#20449;&#24687;&#34701;&#21512;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.17906</link><description>&lt;p&gt;
&#22810;&#37325;&#22270;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#20449;&#24687;&#34701;&#21512;&#30340;&#20309;&#22788;&#21644;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Representation learning in multiplex graphs: Where and how to fuse information?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17906
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#37325;&#22270;&#20013;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#20449;&#24687;&#34701;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#30740;&#31350;&#30028;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19987;&#27880;&#20110;&#21516;&#36136;&#32593;&#32476;&#65292;&#32780;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#36890;&#24120;&#21253;&#21547;&#22810;&#31181;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#12290;&#22810;&#37325;&#22270;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#24322;&#36136;&#22270;&#65292;&#25317;&#26377;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#25972;&#21512;&#26469;&#33258;&#28508;&#22312;&#19981;&#21516;&#26469;&#28304;&#30340;&#26356;&#35814;&#32454;&#25968;&#25454;&#12290;&#22810;&#37325;&#22270;&#20013;&#30340;&#22810;&#26679;&#36793;&#31867;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#21644;&#27934;&#23519;&#21147;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#26080;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#26041;&#24335;&#19979;&#23398;&#20064;&#22810;&#37325;&#32593;&#32476;&#20013;&#33410;&#28857;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#22270;&#22788;&#29702;&#31649;&#36947;&#30340;&#19981;&#21516;&#32423;&#21035;&#19978;&#25191;&#34892;&#30340;&#22810;&#26679;&#20449;&#24687;&#34701;&#21512;&#26041;&#26696;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17906v1 Announce Type: new  Abstract: In recent years, unsupervised and self-supervised graph representation learning has gained popularity in the research community. However, most proposed methods are focused on homogeneous networks, whereas real-world graphs often contain multiple node and edge types. Multiplex graphs, a special type of heterogeneous graphs, possess richer information, provide better modeling capabilities and integrate more detailed data from potentially different sources. The diverse edge types in multiplex graphs provide more context and insights into the underlying processes of representation learning. In this paper, we tackle the problem of learning representations for nodes in multiplex networks in an unsupervised or self-supervised manner. To that end, we explore diverse information fusion schemes performed at different levels of the graph processing pipeline. The detailed analysis and experimental evaluation of various scenarios inspired us to propo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#26469;&#26356;&#22909;&#22320;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17905</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
Using Graph Neural Networks to Predict Local Culture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#26469;&#26356;&#22909;&#22320;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#35748;&#35782;&#21040;&#31038;&#21306;&#26159;&#21160;&#24577;&#21644;&#20851;&#32852;&#30340;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#25968;&#25454;&#12289;&#26041;&#27861;&#35770;&#21644;&#35745;&#31639;&#22788;&#29702;&#33021;&#21147;&#38459;&#30861;&#20102;&#23545;&#31038;&#21306;&#20851;&#31995;&#21160;&#24577;&#36827;&#34892;&#27491;&#24335;&#23450;&#37327;&#20998;&#26512;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#32467;&#21512;&#21644;&#35780;&#20272;&#20851;&#20110;&#31038;&#21306;&#20869;&#37096;&#29305;&#24449;&#12289;&#23427;&#20204;&#30340;&#36807;&#21435;&#29305;&#24449;&#20197;&#21450;&#22312;&#23427;&#20204;&#20043;&#38388;&#27969;&#21160;&#30340;&#32676;&#20307;&#30340;&#22810;&#20010;&#20449;&#24687;&#28304;&#65292;&#28508;&#22312;&#22320;&#20026;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#25506;&#32034; Yelp &#30340;&#20844;&#24320;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#23545;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65288;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65289;&#30340;&#28508;&#21147;&#12290;&#20174;&#23454;&#36136;&#21644;&#26041;&#27861;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#32467;&#26524;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#12290;&#20174;&#23454;&#36136;&#19978;&#35762;&#65292;&#25105;&#20204;&#21457;&#29616;&#26080;&#35770;&#26159;&#24403;&#22320;&#21306;&#22495;&#20449;&#24687;&#65288;&#20363;&#22914;&#21306;&#22495;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17905v1 Announce Type: new  Abstract: Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or g
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;iable pruning&#19982;&#32452;&#21512;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#29992;&#20110;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;&#19968;&#33268;&#26694;&#26550;&#65292;&#20197;&#21487;&#24494;&#21098;&#26525;&#24341;&#23548;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#31232;&#30095;&#21442;&#25968;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17902</link><description>&lt;p&gt;
SequentialAttention++&#29992;&#20110;&#22359;&#31232;&#30095;&#21270;&#65306;&#21487;&#24494;&#21098;&#26525;&#36935;&#19978;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17902
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;iable pruning&#19982;&#32452;&#21512;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#29992;&#20110;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;&#19968;&#33268;&#26694;&#26550;&#65292;&#20197;&#21487;&#24494;&#21098;&#26525;&#24341;&#23548;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#31232;&#30095;&#21442;&#25968;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26159;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#22823;&#22411;&#19988;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#23558;&#20004;&#31181;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;&#19968;&#33268;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#24494;&#21098;&#26525;&#24341;&#23548;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#31232;&#30095;&#21442;&#25968;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17902v1 Announce Type: new  Abstract: Neural network pruning is a key technique towards engineering large yet scalable, interpretable, and generalizable models. Prior work on the subject has developed largely along two orthogonal directions: (1) differentiable pruning for efficiently and accurately scoring the importance of parameters, and (2) combinatorial optimization for efficiently searching over the space of sparse models. We unite the two approaches, both theoretically and empirically, to produce a coherent framework for structured neural network pruning in which differentiable pruning guides combinatorial optimization algorithms to select the most important sparse set of parameters. Theoretically, we show how many existing differentiable pruning techniques can be understood as nonconvex regularization for group sparse optimization, and prove that for a wide class of nonconvex regularizers, the global optimum is unique, group-sparse, and provably yields an approximate 
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#34892;&#26143;&#31995;&#32479;&#20013;&#36890;&#36807;&#23545;&#25968;&#38388;&#36317;&#20851;&#31995;&#21457;&#29616;&#20102;&#26032;&#30340;&#22806;&#34892;&#26143;&#65292;&#39044;&#27979;&#20986;&#20102;&#22823;&#37327;&#39069;&#22806;&#30340;&#22806;&#34892;&#26143;&#65292;&#20854;&#20013;&#21253;&#25324;47&#39063;&#20301;&#20110;&#23452;&#23621;&#21306;&#30340;&#34892;&#26143;&#12290;</title><link>https://arxiv.org/abs/2402.17898</link><description>&lt;p&gt;
&#22810;&#34892;&#26143;&#31995;&#32479;&#20013;&#30340;&#22806;&#34892;&#26143;&#39044;&#27979;&#21450;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30830;&#23450;&#34892;&#26143;&#21644;&#27597;&#26143;&#21442;&#25968;&#20043;&#38388;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exoplanets Prediction in Multi-Planetary Systems and Determining the Correlation Between the Parameters of Planets and Host Stars Using Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17898
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#34892;&#26143;&#31995;&#32479;&#20013;&#36890;&#36807;&#23545;&#25968;&#38388;&#36317;&#20851;&#31995;&#21457;&#29616;&#20102;&#26032;&#30340;&#22806;&#34892;&#26143;&#65292;&#39044;&#27979;&#20986;&#20102;&#22823;&#37327;&#39069;&#22806;&#30340;&#22806;&#34892;&#26143;&#65292;&#20854;&#20013;&#21253;&#25324;47&#39063;&#20301;&#20110;&#23452;&#23621;&#21306;&#30340;&#34892;&#26143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30830;&#35748;&#30340;&#22826;&#38451;&#31995;&#22806;&#34892;&#26143;&#25968;&#37327;&#27491;&#22312;&#22686;&#21152;&#65292;&#36804;&#20170;&#24050;&#30830;&#35748;&#36229;&#36807;&#20116;&#21315;&#39063;&#22806;&#34892;&#26143;&#12290;&#25105;&#20204;&#29616;&#22312;&#26377;&#26426;&#20250;&#27979;&#35797;&#32479;&#27835;&#34892;&#26143;&#31995;&#32479;&#30340;&#23450;&#24459;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37319;&#21462;&#25514;&#26045;&#21457;&#29616;&#34892;&#26143;&#21644;&#24658;&#26143;&#30340;&#29289;&#29702;&#21442;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#33267;&#23569;&#23481;&#32435;&#19977;&#39063;&#25110;&#26356;&#22810;&#24050;&#30830;&#35748;&#34892;&#26143;&#30340;229&#20010;&#22810;&#34892;&#26143;&#31995;&#32479;&#20013;&#25628;&#32034;&#39069;&#22806;&#22806;&#34892;&#26143;&#30340;&#32467;&#26524;&#65292;&#21033;&#29992;&#20102;&#34987;&#31216;&#20026;&#25552;&#20462;&#26031;-&#21338;&#24503;&#65288;TB&#65289;&#20851;&#31995;&#30340;&#22826;&#38451;&#31995;&#20013;&#34892;&#26143;&#20043;&#38388;&#30340;&#23545;&#25968;&#38388;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#32422;53&#65285;&#30340;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#34892;&#26143;&#36981;&#24490;&#23545;&#25968;&#38388;&#36317;&#20851;&#31995;&#65292;&#27604;&#22826;&#38451;&#31995;&#34892;&#26143;&#22909;&#24471;&#22810;&#12290;&#25105;&#20204;&#39044;&#27979;&#23384;&#22312;426&#39063;&#39069;&#22806;&#22806;&#34892;&#26143;&#65292;&#20854;&#20013;47&#39063;&#20301;&#20110;&#23452;&#23621;&#21306;&#65288;HZ&#65289;&#20869;&#65292;&#20854;&#20013;47&#39063;&#34892;&#26143;&#20013;&#26377;&#20116;&#39063;&#30340;&#26368;&#22823;&#36136;&#37327;&#38480;&#21046;&#20026;0.1-2$M_{\oplus}$&#65292;&#26368;&#22823;&#21322;&#24452;&#20302;&#20110;1.25$
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17898v1 Announce Type: cross  Abstract: The number of extrasolar planets discovered is increasing, so that more than five thousand exoplanets have been confirmed to date. Now we have an opportunity to test the validity of the laws governing planetary systems and take steps to discover the relationships between the physical parameters of planets and stars. Firstly, we present the results of a search for additional exoplanets in 229 multi-planetary systems that house at least three or more confirmed planets, employing a logarithmic spacing between planets in our Solar System known as the Titius-Bode (TB) relation. We find that the planets in $\sim53\%$ of these systems adhere to a logarithmic spacing relation remarkably better than the Solar System planets. We predict the presence of 426 additional exoplanets, 47 of which are located within the habitable zone (HZ), and five of the 47 planets have a maximum mass limit of 0.1-2$M_{\oplus}$ and a maximum radius lower than 1.25$R_
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24773;&#22659;&#36870;&#32447;&#24615;&#35268;&#21010;&#31616;&#21270;&#20026;&#20984;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#23558;&#20854;&#36827;&#19968;&#27493;&#31616;&#21270;&#20026;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17890</link><description>&lt;p&gt;
&#20174;&#36870;&#20248;&#21270;&#21040;&#21487;&#34892;&#24615;&#21040;ERM
&lt;/p&gt;
&lt;p&gt;
From Inverse Optimization to Feasibility to ERM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24773;&#22659;&#36870;&#32447;&#24615;&#35268;&#21010;&#31616;&#21270;&#20026;&#20984;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#23558;&#20854;&#36827;&#19968;&#27493;&#31616;&#21270;&#20026;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#20248;&#21270;&#28041;&#21450;&#20174;&#24050;&#30693;&#35299;&#20915;&#26041;&#26696;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#30005;&#21147;&#31995;&#32479;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#21033;&#29992;&#39069;&#22806;&#24773;&#22659;&#20449;&#24687;&#26469;&#26356;&#22909;&#39044;&#27979;&#26410;&#30693;&#38382;&#39064;&#21442;&#25968;&#30340;&#24773;&#22659;&#36870;&#20248;&#21270;&#35774;&#32622;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#24773;&#22659;&#36870;&#32447;&#24615;&#35268;&#21010;&#65288;CILP&#65289;&#65292;&#35299;&#20915;&#20102;LP&#38750;&#21487;&#24494;&#24615;&#36136;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;CILP&#31616;&#21270;&#20026;&#19968;&#20010;&#20984;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20801;&#35768;&#20351;&#29992;&#35832;&#22914;&#20132;&#26367;&#25237;&#24433;&#31561;&#26631;&#20934;&#31639;&#27861;&#12290;CILP&#30340;&#32467;&#26524;&#31639;&#27861;&#37197;&#22791;&#20102;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#65292;&#26080;&#38656;&#39069;&#22806;&#20551;&#35774;&#65292;&#22914;&#36864;&#21270;&#25110;&#25554;&#20540;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;CILP&#20943;&#23569;&#21040;&#19968;&#20010;&#20809;&#28369;&#30340;&#12289;&#20984;&#25439;&#22833;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#12290;&#36825;&#31181;&#31616;&#21270;&#33021;&#22815;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;fir
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17890v1 Announce Type: new  Abstract: Inverse optimization involves inferring unknown parameters of an optimization problem from known solutions, and is widely used in fields such as transportation, power systems and healthcare. We study the contextual inverse optimization setting that utilizes additional contextual information to better predict the unknown problem parameters. We focus on contextual inverse linear programming (CILP), addressing the challenges posed by the non-differentiable nature of LPs. For a linear prediction model, we reduce CILP to a convex feasibility problem allowing the use of standard algorithms such as alternating projections. The resulting algorithm for CILP is equipped with a linear convergence guarantee without additional assumptions such as degeneracy or interpolation. Next, we reduce CILP to empirical risk minimization (ERM) on a smooth, convex loss that satisfies the Polyak-Lojasiewicz condition. This reduction enables the use of scalable fir
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22522;&#20110;Bregman&#25955;&#24230;&#65292;&#36890;&#36807;&#24341;&#20837;&#20849;&#36717;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;\textsc{ConjNorm}&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#25628;&#32034;&#26368;&#20339;&#35268;&#33539;&#31995;&#25968;$p$&#26469;&#37325;&#26032;&#26500;&#24819;&#23494;&#24230;&#20989;&#25968;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.17888</link><description>&lt;p&gt;
ConjNorm&#65306;&#29992;&#20110;&#24322;&#24120;&#20998;&#24067;&#26816;&#27979;&#30340;&#21487;&#22788;&#29702;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22522;&#20110;Bregman&#25955;&#24230;&#65292;&#36890;&#36807;&#24341;&#20837;&#20849;&#36717;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;\textsc{ConjNorm}&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#25628;&#32034;&#26368;&#20339;&#35268;&#33539;&#31995;&#25968;$p$&#26469;&#37325;&#26032;&#26500;&#24819;&#23494;&#24230;&#20989;&#25968;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;&#24322;&#24120;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#22312;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#21040;&#23494;&#20999;&#20851;&#27880;&#12290;&#35768;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#25512;&#23548;&#22522;&#20110;logits&#12289;&#36317;&#31163;&#25110;&#20005;&#26684;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20197;&#35782;&#21035;&#24471;&#20998;&#20302;&#30340;OOD&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20272;&#35745;&#24471;&#20998;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#30495;&#23454;&#25968;&#25454;&#23494;&#24230;&#25110;&#26045;&#21152;&#19981;&#20999;&#23454;&#38469;&#30340;&#32422;&#26463;&#12290;&#20026;&#20102;&#22312;&#22522;&#20110;&#23494;&#24230;&#24471;&#20998;&#35774;&#35745;&#26041;&#38754;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;Bregman&#25955;&#24230;&#20026;&#22522;&#30784;&#30340;&#26032;&#39062;&#29702;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20998;&#24067;&#32771;&#34385;&#25193;&#23637;&#21040;&#28085;&#30422;&#19968;&#31995;&#21015;&#25351;&#25968;&#26063;&#20998;&#24067;&#12290;&#21033;&#29992;&#25105;&#20204;&#23450;&#29702;&#20013;&#25581;&#31034;&#30340;&#20849;&#36717;&#32422;&#26463;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;\textsc{ConjNorm}&#26041;&#27861;&#65292;&#23558;&#23494;&#24230;&#20989;&#25968;&#35774;&#35745;&#37325;&#26032;&#26500;&#24819;&#20026;&#38024;&#23545;&#32473;&#23450;&#25968;&#25454;&#38598;&#25628;&#32034;&#26368;&#20339;&#35268;&#33539;&#31995;&#25968;$p$&#30340;&#36807;&#31243;&#12290;&#37492;&#20110;&#24402;&#19968;&#21270;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#20559;&#21644;&#35299;&#26512;&#21487;&#36861;&#36394;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17888v1 Announce Type: cross  Abstract: Post-hoc out-of-distribution (OOD) detection has garnered intensive attention in reliable machine learning. Many efforts have been dedicated to deriving score functions based on logits, distances, or rigorous data distribution assumptions to identify low-scoring OOD samples. Nevertheless, these estimate scores may fail to accurately reflect the true data density or impose impractical constraints. To provide a unified perspective on density-based score design, we propose a novel theoretical framework grounded in Bregman divergence, which extends distribution considerations to encompass an exponential family of distributions. Leveraging the conjugation constraint revealed in our theorem, we introduce a \textsc{ConjNorm} method, reframing density function design as a search for the optimal norm coefficient $p$ against the given dataset. In light of the computational challenges of normalization, we devise an unbiased and analytically tract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.17886</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;&#38646;&#38454;&#37319;&#26679;&#26041;&#27861;&#65306;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#32531;&#35299;&#20122;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#20854;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#26410;&#24402;&#19968;&#21270;&#23494;&#24230;&#26597;&#35810;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#39318;&#20808;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#25311;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#21363;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;DMC&#65289;&#65292;&#20854;&#24471;&#20998;&#20989;&#25968;&#36890;&#36807;&#36890;&#29992;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#36924;&#36817;&#12290;DMC&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#35861;&#30340;&#20803;&#31639;&#27861;&#65292;&#20854;&#20013;&#31070;&#35861;&#26159;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;&#20998;&#25968;&#20272;&#35745;&#22120;&#30340;&#26679;&#26412;&#30340;&#35775;&#38382;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#30340;&#36825;&#20010;&#31070;&#35861;&#30340;&#23454;&#29616;&#65292;&#36825;&#23558;DMC&#36716;&#21270;&#20026;&#19968;&#20010;&#30495;&#27491;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;ZOD-MC&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21363;DMC&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#32780;&#19981;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#20026;&#23545;&#25968;&#20985;&#25110;&#28385;&#36275;&#20219;&#20309;&#31561;&#21608;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;ZOD-MC&#23545;&#25152;&#38656;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#65292;&#23613;&#31649;&#20173;&#28982;&#21463;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17886v1 Announce Type: cross  Abstract: This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit sti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36817;&#20284;&#30340;&#21463;&#32422;&#26463;&#32435;&#20160;&#22343;&#34913;&#65292;&#36890;&#36807;&#36817;&#31471;&#28857;&#26679;&#24335;&#30340;&#26356;&#26032;&#21644;&#38543;&#26426;&#20999;&#25442;&#26799;&#24230;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#28216;&#25103;&#20013;&#30340;&#29420;&#31435;&#23398;&#20064;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.17885</link><description>&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#28216;&#25103;&#20013;&#30340;&#29420;&#31435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Independent Learning in Constrained Markov Potential Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17885
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36817;&#20284;&#30340;&#21463;&#32422;&#26463;&#32435;&#20160;&#22343;&#34913;&#65292;&#36890;&#36807;&#36817;&#31471;&#28857;&#26679;&#24335;&#30340;&#26356;&#26032;&#21644;&#38543;&#26426;&#20999;&#25442;&#26799;&#24230;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#28216;&#25103;&#20013;&#30340;&#29420;&#31435;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#38480;&#21046;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#25552;&#20379;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#21463;&#21040;&#32422;&#26463;&#12290;&#26412;&#25991;&#20851;&#27880;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#31867;&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#28216;&#25103;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#36825;&#31867;&#21463;&#32422;&#26463;&#28216;&#25103;&#30340;&#38598;&#20013;&#24335;&#31639;&#27861;&#65292;&#20294;&#38024;&#23545;&#21463;&#32422;&#26463;&#35774;&#32622;&#30340;&#25910;&#25947;&#29420;&#31435;&#23398;&#20064;&#31639;&#27861;&#30340;&#35774;&#35745;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36817;&#20284;&#30340;&#21463;&#32422;&#26463;&#32435;&#20160;&#22343;&#34913;&#65306;&#27599;&#20010;&#26234;&#33021;&#20307;&#35266;&#23519;&#21040;&#33258;&#24049;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#65292;&#20197;&#21450;&#19968;&#20010;&#20849;&#20139;&#29366;&#24577;&#12290;&#21463;&#21040;&#20248;&#21270;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25191;&#34892;&#36817;&#31471;&#28857;&#26679;&#24335;&#30340;&#26356;&#26032;&#65292;&#38468;&#24102;&#26377;&#27491;&#21017;&#21270;&#30340;&#32422;&#26463;&#38598;&#12290;&#27599;&#20010;&#36817;&#31471;&#27493;&#39588;&#20351;&#29992;&#38543;&#26426;&#20999;&#25442;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#36817;&#20284;&#27714;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#29420;&#31435;&#23454;&#29616;&#32780;&#26080;&#38656;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17885v1 Announce Type: new  Abstract: Constrained Markov games offer a formal mathematical framework for modeling multi-agent reinforcement learning problems where the behavior of the agents is subject to constraints. In this work, we focus on the recently introduced class of constrained Markov Potential Games. While centralized algorithms have been proposed for solving such constrained games, the design of converging independent learning algorithms tailored for the constrained setting remains an open question. We propose an independent policy gradient algorithm for learning approximate constrained Nash equilibria: Each agent observes their own actions and rewards, along with a shared state. Inspired by the optimization literature, our algorithm performs proximal-point-like updates augmented with a regularized constraint set. Each proximal step is solved inexactly using a stochastic switching gradient algorithm. Notably, our algorithm can be implemented independently without
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#19981;&#20877;&#38656;&#35201;&#23450;&#20041;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.17879</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Automated Statistical Model Discovery with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17879
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#19981;&#20877;&#38656;&#35201;&#23450;&#20041;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#28041;&#21450;&#22312;&#21463;&#39046;&#22495;&#29305;&#23450;&#24314;&#27169;&#32422;&#26463;&#30340;&#24191;&#27867;&#27169;&#22411;&#31354;&#38388;&#19978;&#36827;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25628;&#32034;&#12290;&#39640;&#25928;&#25628;&#32034;&#36825;&#19968;&#31354;&#38388;&#38656;&#35201;&#20855;&#26377;&#24314;&#27169;&#21644;&#38382;&#39064;&#22495;&#20154;&#31867;&#19987;&#38271;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#39046;&#22495;&#30693;&#35782;&#21644;&#32534;&#31243;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#33258;&#21160;&#21270;&#27969;&#31243;&#32622;&#20110;Box&#30340;&#24490;&#29615;&#26694;&#26550;&#20043;&#20869;&#65306;LM&#22312;&#25552;&#20986;&#34920;&#31034;&#20026;&#27010;&#29575;&#31243;&#24207;&#30340;&#32479;&#35745;&#27169;&#22411;&#65288;&#20805;&#24403;&#24314;&#27169;&#32773;&#65289;&#20043;&#38388;&#36845;&#20195;&#65292;&#24182;&#25209;&#21028;&#36825;&#20123;&#27169;&#22411;&#65288;&#20805;&#24403;&#39046;&#22495;&#19987;&#23478;&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;LMs&#65292;&#25105;&#20204;&#19981;&#24517;&#23450;&#20041;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#65292;&#36825;&#26159;&#20808;&#21069;&#31995;&#32479;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#27010;&#29575;&#24314;&#27169;&#30340;&#19977;&#31181;&#24120;&#35265;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#22312;&#21463;&#38480;&#27169;&#22411;&#31354;&#38388;&#20869;&#25628;&#32034;&#65292;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17879v1 Announce Type: cross  Abstract: Statistical model discovery involves a challenging search over a vast space of models subject to domain-specific modeling constraints. Efficiently searching over this space requires human expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, key restrictions of previous systems. We evaluate our method in three common settings in probabilistic modeling: searching within a restricted space of models, searching ove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#20559;&#32622;MCMC&#27493;&#39588;&#30340;SAEM&#30340;&#28176;&#36817;&#24615;&#21644;&#38750;&#28176;&#36817;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#20559;&#32622;&#30340;&#24433;&#21709;&#65292;&#24182;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#29702;&#35770;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.17870</link><description>&lt;p&gt;
&#20351;&#29992;&#20559;&#32622;MCMC&#36827;&#34892;&#38543;&#26426;&#36924;&#36817;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Biased MCMC for Expectation Maximization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#20559;&#32622;MCMC&#27493;&#39588;&#30340;SAEM&#30340;&#28176;&#36817;&#24615;&#21644;&#38750;&#28176;&#36817;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#20559;&#32622;&#30340;&#24433;&#21709;&#65292;&#24182;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#29702;&#35770;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17870v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#32463;&#39564;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#26399;&#26395;&#27493;&#39588;&#65288;E&#27493;&#39588;&#65289;&#32463;&#24120;&#38590;&#20197;&#22788;&#29702;&#12290;&#37319;&#29992;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#21487;&#20197;&#36991;&#24320;&#36825;&#20010;&#38382;&#39064;&#65292;&#24471;&#21040;&#19968;&#31181;&#31216;&#20026;MCMC-SAEM&#30340;&#31639;&#27861;&#12290;&#34429;&#28982;&#20808;&#21069;&#24050;&#32463;&#30830;&#31435;&#20102;MCMC-SAEM&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#20351;&#29992;&#28176;&#36817;&#26080;&#20559;MCMC&#31639;&#27861;&#30340;&#24773;&#20917;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;MCMC-SAEM&#32463;&#24120;&#20351;&#29992;&#28176;&#36817;&#26377;&#20559;&#30340;MCMC&#36816;&#34892;&#65292;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#20854;&#29702;&#35770;&#21518;&#26524;&#23578;&#19981;&#20026;&#20154;&#20102;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#24102;&#26377;&#20559;&#32622;MCMC&#27493;&#39588;&#30340;SAEM&#30340;&#28176;&#36817;&#24615;&#21644;&#38750;&#28176;&#36817;&#24615;&#65292;&#29305;&#21035;&#26159;&#20559;&#32622;&#30340;&#24433;&#21709;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#27604;&#36739;Metropolis&#35843;&#25972;&#30340;&#26391;&#32500;&#26032;&#31639;&#27861;&#65288;MALA&#65289;&#21644;&#26410;&#32463;&#35843;&#25972;&#30340;&#26391;&#32500;&#26032;&#31639;&#27861;&#65288;ULA&#65289;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17870v1 Announce Type: cross  Abstract: The expectation maximization (EM) algorithm is a widespread method for empirical Bayesian inference, but its expectation step (E-step) is often intractable. Employing a stochastic approximation scheme with Markov chain Monte Carlo (MCMC) can circumvent this issue, resulting in an algorithm known as MCMC-SAEM. While theoretical guarantees for MCMC-SAEM have previously been established, these results are restricted to the case where asymptotically unbiased MCMC algorithms are used. In practice, MCMC-SAEM is often run with asymptotically biased MCMC, for which the consequences are theoretically less understood. In this work, we fill this gap by analyzing the asymptotics and non-asymptotics of SAEM with biased MCMC steps, particularly the effect of bias. We also provide numerical experiments comparing the Metropolis-adjusted Langevin algorithm (MALA), which is asymptotically unbiased, and the unadjusted Langevin algorithm (ULA), which is a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#24182;&#20351;&#29992;&#36739;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#22823;&#22823;&#31616;&#21270;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17853</link><description>&lt;p&gt;
&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65306;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#24182;&#20351;&#29992;&#36739;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#22823;&#22823;&#31616;&#21270;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21152;&#36895;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#31995;&#32479;&#30340;&#25968;&#20540;&#27169;&#25311;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#22312;&#39640;&#32500;&#31163;&#25955;&#21270;&#22330;&#19978;&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#35758;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#20351;&#29992;&#26356;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550; - &#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#20013;&#65292;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23558;&#31995;&#32479;&#30340;&#20840;&#38454;&#34920;&#31034;&#25237;&#24433;&#21040;&#32593;&#26684;&#20943;&#23569;&#30340;&#31354;&#38388;&#20013;&#65292;&#25509;&#30528;&#35757;&#32451;&#19968;&#20010;&#26102;&#38388;&#27169;&#22411;&#26469;&#39044;&#27979;&#36825;&#20010;&#32593;&#26684;&#20943;&#23569;&#30340;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#36825;&#31181;&#38477;&#38454;&#36807;&#31243;&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#20276;&#38543;&#32454;&#31890;&#24230;&#31163;&#25955;&#21270;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#31616;&#21270;&#20102;&#26102;&#38388;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#20197;&#21450;&#20960;&#31181;&#20854;&#20182;&#27969;&#34892;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#31995;&#32479;&#19978;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21333;&#30456;&#21644;&#22810;&#30456;&#27969;&#20307;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17853v1 Announce Type: cross  Abstract: Neural networks have shown promising potential in accelerating the numerical simulation of systems governed by partial differential equations (PDEs). Different from many existing neural network surrogates operating on high-dimensional discretized fields, we propose to learn the dynamics of the system in the latent space with much coarser discretizations. In our proposed framework - Latent Neural PDE Solver (LNS), a non-linear autoencoder is first trained to project the full-order representation of the system onto the mesh-reduced space, then a temporal model is trained to predict the future state in this mesh-reduced space. This reduction process simplifies the training of the temporal model by greatly reducing the computational cost accompanying a fine discretization. We study the capability of the proposed framework and several other popular neural PDE solvers on various types of systems including single-phase and multi-phase flows a
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.17840</link><description>&lt;p&gt;
&#36981;&#24490;&#25105;&#30340;&#25351;&#31034;&#24182;&#35828;&#20986;&#30495;&#30456;&#65306;&#26469;&#33258;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17840
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#36866;&#24212;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Retrieval-In-Context RAG&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23545;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LMs&#26500;&#24314;&#30340;RAG&#31995;&#32479;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#28431;&#27934;&#23384;&#22312;&#20110;&#35206;&#30422;Llama2&#12289;Mistral/Mixtral&#12289;Vicuna&#12289;SOLAR&#12289;WizardLM&#12289;Qwen1.5&#21644;Platypus2&#31561;&#22810;&#31181;&#29616;&#20195;LMs&#30340;&#24191;&#27867;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#21033;&#29992;&#33021;&#21147;&#21152;&#21095;&#12290;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23545;25&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#23450;&#21046;GPTs&#26045;&#21152;&#26368;&#22810;2&#20010;&#26597;&#35810;&#26102;&#20197;100%&#25104;&#21151;&#29575;&#23548;&#33268;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#20197;77,000&#23383;&#30340;&#20070;&#31821;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;41%&#65292;&#20197;&#21450;&#22312;&#21547;&#26377;1,569,00&#35789;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17812</link><description>&lt;p&gt;
DropBP&#65306;&#36890;&#36807;&#20002;&#24323;&#21453;&#21521;&#20256;&#25773;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17812
&lt;/p&gt;
&lt;p&gt;
DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#23618;&#27425;&#20002;&#24323;&#25216;&#26415;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#24323;&#26576;&#20123;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22312;&#27491;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#20002;&#24323;&#23618;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DropBP&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;DropBP&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#65292;&#19981;&#24433;&#21709;&#27491;&#21521;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;DropBP&#35745;&#31639;&#27599;&#20010;&#23618;&#30340;&#25935;&#24863;&#24615;&#20197;&#20998;&#37197;&#36866;&#24403;&#30340;&#20002;&#22833;&#29575;&#65292;&#20174;&#32780;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;DropBP&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17811</link><description>&lt;p&gt;
TruthX: &#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20943;&#36731;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#22238;&#24212;&#65292;&#23613;&#31649;&#25317;&#26377;&#27491;&#30830;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TruthX&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLMs&#20869;&#37096;&#34920;&#31034;&#20197;&#33719;&#21462;&#20854;&#30495;&#23454;&#24615;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#12290;TruthX&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;LLM&#30340;&#34920;&#31034;&#20998;&#21035;&#26144;&#23556;&#21040;&#35821;&#20041;&#21644;&#30495;&#23454;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#35782;&#21035;&#30495;&#23454;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLM&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;TruthX&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;LLMs&#30340;&#30495;&#23454;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TruthX&#36890;&#36807;20%&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;LLMs&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30495;&#23454;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17811v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs. Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that the truthful space
&lt;/p&gt;</description></item><item><title>BioT5+&#26159;BioT5&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;IUPAC&#21517;&#31216;&#12289;&#21253;&#21547;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#12289;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#21450;&#26032;&#39062;&#30340;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.17810</link><description>&lt;p&gt;
BioT5+: &#36890;&#36807;IUPAC&#38598;&#25104;&#21644;&#22810;&#20219;&#21153;&#35843;&#25972;&#23454;&#29616;&#24191;&#20041;&#29983;&#29289;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17810
&lt;/p&gt;
&lt;p&gt;
BioT5+&#26159;BioT5&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;IUPAC&#21517;&#31216;&#12289;&#21253;&#21547;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#12289;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#21450;&#26032;&#39062;&#30340;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35745;&#31639;&#29983;&#29289;&#23398;&#30340;&#30740;&#31350;&#36235;&#21183;&#36234;&#26469;&#36234;&#38598;&#20013;&#20110;&#25972;&#21512;&#25991;&#26412;&#21644;&#29983;&#29289;&#23454;&#20307;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;BioT5&#30340;&#20808;&#21069;&#24037;&#20316;&#22312;&#36328;&#36234;&#22810;&#26679;&#21270;&#20219;&#21153;&#21644;&#32570;&#20047;&#23545;&#20998;&#23376;&#32467;&#26500;&#30340;&#32454;&#33268;&#29702;&#35299;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30340;&#25991;&#26412;&#34920;&#31034;&#65288;&#20363;&#22914;IUPAC&#65289;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioT5+&#65292;&#36825;&#26159;BioT5&#26694;&#26550;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#26088;&#22312;&#22686;&#24378;&#29983;&#29289;&#30740;&#31350;&#21644;&#33647;&#29289;&#21457;&#29616;&#12290; BioT5+&#21253;&#21547;&#20960;&#20010;&#26032;&#39062;&#30340;&#29305;&#24615;&#65306;&#25972;&#21512;IUPAC&#21517;&#31216;&#20197;&#21152;&#28145;&#23545;&#20998;&#23376;&#30340;&#29702;&#35299;&#65292;&#21253;&#25324;&#26469;&#33258;bioRxiv&#21644;PubChem&#31561;&#28304;&#30340;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#65292;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#25968;&#23383;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#39062;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#12290; &#36825;&#20123;&#22686;&#24378;&#21151;&#33021;&#20351;BioT5+&#33021;&#22815;&#24357;&#21512;&#20998;&#23376;&#34920;&#31034;&#21644;&#23427;&#20204;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17810v1 Announce Type: cross  Abstract: Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a novel numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their text
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;ICA&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#28982;&#21518;&#21033;&#29992;&#38598;&#25104;&#31639;&#27861;&#23545;UWB NLOS&#20449;&#21495;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#38750;&#35270;&#36317;&#24773;&#20917;&#19979;&#22256;&#20154;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17808</link><description>&lt;p&gt;
&#22522;&#20110;ICA&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;UWB NLOS&#20449;&#21495;&#25968;&#25454;&#20998;&#31867;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AN An ica-ensemble learning approach for prediction of uwb nlos signals data classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;ICA&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#28982;&#21518;&#21033;&#29992;&#38598;&#25104;&#31639;&#27861;&#23545;UWB NLOS&#20449;&#21495;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#38750;&#35270;&#36317;&#24773;&#20917;&#19979;&#22256;&#20154;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#21644;&#25937;&#25588;&#65288;SAR&#65289;&#22330;&#26223;&#20013;&#65292;&#22256;&#20154;&#26816;&#27979;&#22312;&#26080;&#22788;&#19981;&#22312;&#35745;&#31639;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#21644;&#22024;&#26434;&#25968;&#25454;&#65292;&#22256;&#20154;&#30340;&#20934;&#30830;&#35782;&#21035;&#21463;&#21040;&#38459;&#30861;&#12290;&#29305;&#21035;&#26159;&#22312;&#28798;&#38590;&#20107;&#20214;&#26399;&#38388;&#30340;&#38750;&#35270;&#36317;&#65288;NLOS&#65289;&#24773;&#20917;&#19979;&#65292;&#32500;&#24230;&#35781;&#21650;&#21487;&#33021;&#23548;&#33268;&#22240;&#26816;&#27979;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#30456;&#20851;&#20540;&#32780;&#20135;&#29983;&#30450;&#21306;&#12290;&#36825;&#39033;&#30740;&#31350;&#32858;&#28966;&#20110;&#36890;&#36807;&#26080;&#32447;&#36890;&#20449;&#21327;&#35843;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#36229;&#23485;&#24102;&#65288;UWB&#65289;&#38647;&#36798;&#20449;&#21495;&#22312;NLOS&#24773;&#26223;&#20013;&#35782;&#21035;&#20010;&#20307;&#12290;&#21033;&#29992;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#25968;&#25454;&#38598;&#19978;&#38598;&#25104;&#31639;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38745;&#24577;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;88.37%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17808v1 Announce Type: cross  Abstract: Trapped human detection in search and rescue (SAR) scenarios poses a significant challenge in pervasive computing. This study addresses this issue by leveraging machine learning techniques, given their high accuracy. However, accurate identification of trapped individuals is hindered by the curse of dimensionality and noisy data. Particularly in non-line-of-sight (NLOS) situations during catastrophic events, the curse of dimensionality may lead to blind spots due to noise and uncorrelated values in detections. This research focuses on harmonizing information through wireless communication and identifying individuals in NLOS scenarios using ultra-wideband (UWB) radar signals. Employing independent component analysis (ICA) for feature extraction, the study evaluates classification performance using ensemble algorithms on both static and dynamic datasets. The experimental results demonstrate categorization accuracies of 88.37% for static 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#22522;&#22240;&#35843;&#25511;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65292;&#30740;&#31350;&#32773;&#39044;&#27979;&#20102;&#38024;&#23545;&#19979;&#21693;&#30284;&#21644;EGFR&#31361;&#21464;&#32954;&#33146;&#30284;&#30340;&#27835;&#30103;&#20998;&#23376;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2402.17807</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#22240;&#35843;&#25511;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#24182;&#39044;&#27979;&#19979;&#21693;&#30284;&#21644;EGFR&#31361;&#21464;&#32954;&#33146;&#30284;&#30340;&#27835;&#30103;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Exploring Gene Regulatory Interaction Networks and predicting therapeutic molecules for Hypopharyngeal Cancer and EGFR-mutated lung adenocarcinoma
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#22522;&#22240;&#35843;&#25511;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65292;&#30740;&#31350;&#32773;&#39044;&#27979;&#20102;&#38024;&#23545;&#19979;&#21693;&#30284;&#21644;EGFR&#31361;&#21464;&#32954;&#33146;&#30284;&#30340;&#27835;&#30103;&#20998;&#23376;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#24687;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#39046;&#22495;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#23398;&#32773;&#36234;&#26469;&#36234;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#26368;&#36817;&#21508;&#31181;&#29983;&#29289;&#20449;&#24687;&#23398;&#24037;&#20855;&#21253;&#30340;&#21457;&#23637;&#20419;&#36827;&#20102;&#22823;&#37327;&#29983;&#29289;&#25968;&#25454;&#30340;&#24555;&#36895;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#20197;&#20379;&#20154;&#31867;&#24863;&#30693;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20391;&#37325;&#20110;&#30830;&#23450;&#20004;&#31181;&#30456;&#20851;&#30142;&#30149;&#24182;&#36827;&#34892;&#19968;&#20123;&#35266;&#23519;&#65292;&#20197;&#26500;&#24314;&#22810;&#26679;&#30340;&#22522;&#22240;&#35843;&#25511;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65292;&#20026;&#27835;&#30103;&#30142;&#30149;&#30340;&#19968;&#33324;&#33647;&#29289;&#35774;&#35745;&#25171;&#19979;&#22522;&#30784;&#12290;&#20363;&#22914;&#65292;&#19979;&#21693;&#30284;&#26159;&#19968;&#31181;&#19982;EGFR&#31361;&#21464;&#32954;&#33146;&#30284;&#30456;&#20851;&#30340;&#30142;&#30149;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#19979;&#21693;&#30284;&#20013;&#25214;&#21040;&#32954;&#36716;&#31227;&#20301;&#28857;&#26469;&#36873;&#25321;EGFR&#31361;&#21464;&#32954;&#33146;&#30284;&#21644;&#19979;&#21693;&#30284;&#12290;&#20026;&#24320;&#23637;&#27492;&#30740;&#31350;&#65292;&#25105;&#20204;&#20174;&#30001;NCBI&#25511;&#21046;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;GEO&#65288;&#22522;&#22240;&#34920;&#36798;&#32434;&#30446;&#24405;&#65289;&#20013;&#25910;&#38598;&#20102;&#24494;&#38453;&#21015;&#25968;&#25454;&#38598;&#12290;&#24046;&#24322;&#34920;&#36798;&#22522;&#22240;&#12289;&#20849;&#21516;&#22522;&#22240;&#21644;&#20013;&#24515;&#22522;&#22240;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17807v1 Announce Type: cross  Abstract: With the advent of Information technology, the Bioinformatics research field is becoming increasingly attractive to researchers and academicians. The recent development of various Bioinformatics toolkits has facilitated the rapid processing and analysis of vast quantities of biological data for human perception. Most studies focus on locating two connected diseases and making some observations to construct diverse gene regulatory interaction networks, a forerunner to general drug design for curing illness. For instance, Hypopharyngeal cancer is a disease that is associated with EGFR-mutated lung adenocarcinoma. In this study, we select EGFR-mutated lung adenocarcinoma and Hypopharyngeal cancer by finding the Lung metastases in hypopharyngeal cancer. To conduct this study, we collect Mircorarray datasets from GEO (Gene Expression Omnibus), an online database controlled by NCBI. Differentially expressed genes, common genes, and hub genes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;VAE-&#22238;&#24402;&#19982;&#22810;&#27169;&#24577;&#20808;&#39564;&#35774;&#35745;&#26448;&#26009;&#24494;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;VAE&#19982;&#22238;&#24402;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#21452;&#23618;&#20808;&#39564;&#26469;&#38142;&#25509;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#23398;&#20064;&#20102;&#24494;&#32467;&#26500;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#27491;&#21521;&#21644;&#36870;&#21521;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17806</link><description>&lt;p&gt;
&#20351;&#29992;VAE-&#22238;&#24402;&#19982;&#22810;&#27169;&#24577;&#20808;&#39564;&#35774;&#35745;&#26448;&#26009;&#24494;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Material Microstructure Design Using VAE-Regression with Multimodal Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;VAE-&#22238;&#24402;&#19982;&#22810;&#27169;&#24577;&#20808;&#39564;&#35774;&#35745;&#26448;&#26009;&#24494;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;VAE&#19982;&#22238;&#24402;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#21452;&#23618;&#20808;&#39564;&#26469;&#38142;&#25509;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#23398;&#20064;&#20102;&#24494;&#32467;&#26500;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#27491;&#21521;&#21644;&#36870;&#21521;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#27491;&#21521;&#21644;&#36870;&#21521;&#32467;&#26500;-&#24615;&#33021;&#38142;&#25509;&#65292;&#22312;&#35745;&#31639;&#26448;&#26009;&#31185;&#23398;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31995;&#32479;&#22320;&#23558;VAE&#19982;&#22238;&#24402;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#19968;&#20010;&#21452;&#23618;&#20808;&#39564;&#26469;&#38142;&#25509;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#35813;&#20808;&#39564;&#21463;&#21040;&#22238;&#24402;&#21464;&#37327;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;&#22238;&#24402;&#25439;&#22833;&#19982;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#25439;&#22833;&#19968;&#36215;&#20248;&#21270;&#65292;&#23398;&#20064;&#19982;&#24615;&#33021;&#39044;&#27979;&#21644;&#37325;&#26500;&#30456;&#20851;&#30340;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#27491;&#21521;&#21644;&#36870;&#21521;&#39044;&#27979;&#65292;&#21363;&#29992;&#20110;&#39044;&#27979;&#32473;&#23450;&#24494;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#29992;&#20110;&#39044;&#27979;&#33719;&#21462;&#32473;&#23450;&#24615;&#33021;&#25152;&#38656;&#30340;&#24494;&#32467;&#26500;&#12290;&#30001;&#20110;&#36870;&#38382;&#39064;&#26159;&#19981;&#36866;&#23450;&#30340;&#65288;&#19968;&#23545;&#22810;&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24577;&#39640;&#26031;&#28151;&#21512;&#20808;&#39564;&#25512;&#23548;&#20986;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#30446;&#26631;&#24615;&#33021;&#38598;&#21512;&#30340;&#22810;&#20010;&#24494;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17806v1 Announce Type: new  Abstract: We propose a variational autoencoder (VAE)-based model for building forward and inverse structure-property linkages, a problem of paramount importance in computational materials science. Our model systematically combines VAE with regression, linking the two models through a two-level prior conditioned on the regression variables. The regression loss is optimized jointly with the reconstruction loss of the variational autoencoder, learning microstructure features relevant for property prediction and reconstruction. The resultant model can be used for both forward and inverse prediction i.e., for predicting the properties of a given microstructure as well as for predicting the microstructure required to obtain given properties. Since the inverse problem is ill-posed (one-to-many), we derive the objective function using a multi-modal Gaussian mixture prior enabling the model to infer multiple microstructures for a target set of properties. 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.17805</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks and Arithmetic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17805
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#24449;&#20102;&#36981;&#24490;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#19981;&#38480;&#20110;&#32858;&#21512;-&#32452;&#21512;GNN&#25110;&#20854;&#20182;&#29305;&#23450;&#31867;&#22411;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#32593;&#32476;&#30340;&#28608;&#27963;&#20989;&#25968;&#25104;&#20026;&#30005;&#36335;&#20013;&#30340;&#38376;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#24120;&#25968;&#28145;&#24230;&#30005;&#36335;&#21644;&#32593;&#32476;&#23478;&#26063;&#22343;&#25104;&#31435;&#65292;&#26080;&#35770;&#26159;&#22312;&#19968;&#33268;&#36824;&#26159;&#38750;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#25152;&#26377;&#24120;&#35265;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17805v1 Announce Type: cross  Abstract: We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#21382;&#21490;&#25968;&#25454;&#31383;&#21475;&#21644;&#39044;&#27979;&#31383;&#21475;&#23545;&#24037;&#19994;&#35774;&#22791;&#25925;&#38556;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.17804</link><description>&lt;p&gt;
&#20174;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26426;&#22120;&#25925;&#38556;: &#19968;&#20010;&#24037;&#19994;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predicting machine failures from multivariate time series: an industrial case study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#21382;&#21490;&#25968;&#25454;&#31383;&#21475;&#21644;&#39044;&#27979;&#31383;&#21475;&#23545;&#24037;&#19994;&#35774;&#22791;&#25925;&#38556;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31070;&#32463;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#24037;&#19994;&#32500;&#25252;&#32972;&#26223;&#19979;&#31995;&#32479;&#25925;&#38556;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#23569;&#25968;&#30740;&#31350;&#21516;&#26102;&#35780;&#20272;&#20102;&#29992;&#20110;&#20570;&#20986;&#39044;&#27979;&#30340;&#36807;&#21435;&#25968;&#25454;&#37327;&#21644;&#23545;&#26410;&#26469;&#39044;&#27979;&#30340;&#24310;&#20280;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35757;&#32451;&#27169;&#22411;&#20197;&#39044;&#27979;&#22312;(1)&#24037;&#19994;&#21253;&#35013;&#26426;&#22120;&#22312;&#31163;&#25955;&#20250;&#35805;&#20013;&#36816;&#34892;&#12289;(2)&#24037;&#19994;&#34880;&#28082;&#20912;&#31665;&#25345;&#32493;&#36816;&#34892;&#21644;(3)&#27694;&#27668;&#21457;&#29983;&#22120;&#25345;&#32493;&#36816;&#34892;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#25925;&#38556;&#30340;&#24615;&#33021;&#30340;&#35835;&#21462;&#31383;&#21475;&#22823;&#23567;&#21644;&#39044;&#27979;&#31383;&#21475;&#23545;&#20854;&#30340;&#24433;&#21709;&#12290;&#35813;&#38382;&#39064;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#22833;&#36133;&#22312;&#35813;&#38388;&#38548;&#20869;&#21487;&#33021;&#21457;&#29983;&#30340;&#27010;&#29575;&#23558;&#27491;&#26631;&#31614;&#20998;&#37197;&#32473;&#39044;&#27979;&#31383;&#21475;&#12290;&#20845;&#31181;&#31639;&#27861;&#65288;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;LS
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17804v1 Announce Type: new  Abstract: Non-neural Machine Learning (ML) and Deep Learning (DL) models are often used to predict system failures in the context of industrial maintenance. However, only a few researches jointly assess the effect of varying the amount of past data used to make a prediction and the extension in the future of the forecast. This study evaluates the impact of the size of the reading window and of the prediction window on the performances of models trained to forecast failures in three data sets concerning the operation of (1) an industrial wrapping machine working in discrete sessions, (2) an industrial blood refrigerator working continuously, and (3) a nitrogen generator working continuously. The problem is formulated as a binary classification task that assigns the positive label to the prediction window based on the probability of a failure to occur in such an interval. Six algorithms (logistic regression, random forest, support vector machine, LS
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#24212;&#29992;&#20110;&#21387;&#32553;&#26426;&#35774;&#22791;&#36816;&#34892;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#39044;&#27979;&#12289;&#39044;&#27979;&#21644;&#21464;&#28857;&#26816;&#27979;&#31561;&#20219;&#21153;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.17802</link><description>&lt;p&gt;
&#21387;&#32553;&#26426;&#35774;&#22791;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Time Series Analysis in Compressor-Based Machines: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#24212;&#29992;&#20110;&#21387;&#32553;&#26426;&#35774;&#22791;&#36816;&#34892;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#39044;&#27979;&#12289;&#39044;&#27979;&#21644;&#21464;&#28857;&#26816;&#27979;&#31561;&#20219;&#21153;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21644;&#23621;&#20303;&#29615;&#22659;&#20013;&#65292;&#22914;&#20912;&#31665;&#12289;&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#12289;&#28909;&#27893;&#21644;&#21046;&#20919;&#26426;&#31561;&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#35774;&#22791;&#23545;&#28385;&#36275;&#29983;&#20135;&#21644;&#28040;&#36153;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#24863;&#22120;&#21644;&#29289;&#32852;&#32593;&#36830;&#25509;&#30340;&#26222;&#21450;&#25903;&#25345;&#20102;&#30417;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#33021;&#22815;&#26816;&#27979;&#21644;&#39044;&#27979;&#25925;&#38556;&#65292;&#35782;&#21035;&#34892;&#20026;&#21464;&#21270;&#65292;&#24182;&#39044;&#27979;&#35774;&#22791;&#21644;&#20854;&#32452;&#20214;&#30340;&#25805;&#20316;&#29366;&#24577;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#35843;&#26597;&#26368;&#36817;&#23545;&#36825;&#20123;&#20219;&#21153;&#65288;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#39044;&#27979;&#12289;&#39044;&#27979;&#21644;&#21464;&#28857;&#26816;&#27979;&#65289;&#24212;&#29992;&#20110;&#34920;&#24449;&#21387;&#32553;&#26426;&#35774;&#22791;&#36816;&#34892;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25925;&#38556;&#26816;&#27979;&#21487;&#20197;&#26816;&#27979;&#21644;&#35786;&#26029;&#25925;&#38556;&#65292;&#25925;&#38556;&#39044;&#27979;&#21487;&#20197;&#39044;&#27979;&#36825;&#31181;&#21457;&#29983;&#65292;&#39044;&#27979;&#21487;&#20197;&#39044;&#27979;&#35774;&#22791;&#29305;&#24449;&#21464;&#37327;&#30340;&#26410;&#26469;&#20540;&#65292;&#21464;&#28857;&#26816;&#27979;&#21487;&#20197;&#35782;&#21035;&#35774;&#22791;&#34892;&#20026;&#20013;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17802v1 Announce Type: new  Abstract: In both industrial and residential contexts, compressor-based machines, such as refrigerators, HVAC systems, heat pumps and chillers, are essential to fulfil production and consumers' needs. The diffusion of sensors and IoT connectivity supports the development of monitoring systems able to detect and predict faults, identify behavioural shifts and forecast the operational status of machines and of their components. The focus of this paper is to survey the recent research on such tasks as Fault Detection, Fault Prediction, Forecasting and Change Point Detection applied to multivariate time series characterizing the operations of compressor-based machines. Specifically, Fault Detection detects and diagnoses faults, Fault Prediction predicts such occurrences, forecasting anticipates the future value of characteristic variables of machines and Change Point Detection identifies significant variations in the behaviour of the appliances, such 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#12289;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.17793</link><description>&lt;p&gt;
&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#65311;&#22810;&#27169;LLMs&#21644;NLVR&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Surprising Failure? Multimodal LLMs and the NLVR Challenge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17793
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#12289;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MLLMs&#8212;&#8212;GPT-4V&#12289;Gemini Pro&#21644;&#24320;&#28304;&#27169;&#22411;IDEFICS&#8212;&#8212;&#23545;&#20110;&#32452;&#21512;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#30340;&#34920;&#29616;&#12290;NLVR&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#19968;&#20010;&#20154;&#31867;&#20070;&#20889;&#30340;&#21477;&#23376;&#21644;&#19968;&#20010;&#21512;&#25104;&#22270;&#20687;&#26469;&#30830;&#23450;&#21477;&#23376;&#30456;&#23545;&#20110;&#22270;&#20687;&#30340;&#30495;&#20551;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;NLVR&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#65292;&#24182;&#19988;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17793v1 Announce Type: new  Abstract: This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and the open-source model IDEFICS -- on the compositional natural language vision reasoning task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to determine the truth value of the sentence with respect to the image. Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#28436;&#21270;&#39063;&#31890;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#22312;&#20998;&#31867;&#24369;&#30417;&#30563;EEG&#25968;&#25454;&#27969;&#26102;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#23545;&#24773;&#32490;&#30456;&#20851;&#27169;&#24335;&#30340;&#20998;&#31867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17792</link><description>&lt;p&gt;
EGNN-C+: &#21487;&#35299;&#37322;&#30340;&#28436;&#21270;&#39063;&#31890;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#24369;&#30417;&#30563;EEG&#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17792
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#28436;&#21270;&#39063;&#31890;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#22312;&#20998;&#31867;&#24369;&#30417;&#30563;EEG&#25968;&#25454;&#27969;&#26102;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#23545;&#24773;&#32490;&#30456;&#20851;&#27169;&#24335;&#30340;&#20998;&#31867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#28436;&#21270;&#39063;&#31890;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65288;eGNN-C+&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#36793;&#30028;&#36229;&#31435;&#26041;&#20307;&#26469;&#34920;&#31034;&#39063;&#31890;&#65292;&#24182;&#23450;&#21046;&#20102;&#36866;&#24212;&#24615;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#22806;&#37096;&#30418;&#23376;&#23545;&#25968;&#25454;&#35206;&#30422;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20869;&#37096;&#30418;&#23376;&#20445;&#25345;&#28789;&#27963;&#20197;&#25429;&#33719;&#28418;&#31227;&#12290;&#20998;&#31867;&#22120;&#20174;&#38646;&#24320;&#22987;&#28436;&#21270;&#65292;&#22312;&#36816;&#34892;&#36807;&#31243;&#20013;&#32467;&#21512;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#25191;&#34892;&#23616;&#37096;&#22686;&#37327;&#29305;&#24449;&#21152;&#26435;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#38598;&#20013;&#22312;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20013;&#19982;&#24773;&#32490;&#30456;&#20851;&#30340;&#27169;&#24335;&#36827;&#34892;&#20998;&#31867;&#12290;&#24773;&#32490;&#35782;&#21035;&#23545;&#20110;&#22686;&#24378;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#36924;&#30495;&#24615;&#21644;&#20114;&#21160;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20174;28&#21517;&#21442;&#19982;&#30005;&#33041;&#28216;&#25103;&#30340;&#20010;&#20307;&#33719;&#24471;&#30340;EEG&#20449;&#21495;&#30340;&#20613;&#31435;&#21494;&#35889;&#20013;&#25552;&#21462;&#29305;&#24449; -- &#36825;&#26159;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#28216;&#25103;&#24341;&#21457;&#19981;&#21516;&#30340;&#20027;&#23548;&#24773;&#32490;&#65306;&#26080;&#32842;&#12289;&#24179;&#38745;&#12289;&#24656;&#24598;&#25110;&#24555;&#20048;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20010;&#20307;&#38388;&#19981;&#21516;&#30340;&#24773;&#32490;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17792v1 Announce Type: cross  Abstract: We introduce a modified incremental learning algorithm for evolving Granular Neural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to represent granules, and customize the adaptation procedures to enhance the robustness of outer boxes for data coverage and noise suppression, while ensuring that inner boxes remain flexible to capture drifts. The classifier evolves from scratch, incorporates new classes on the fly, and performs local incremental feature weighting. As an application, we focus on the classification of emotion-related patterns within electroencephalogram (EEG) signals. Emotion recognition is crucial for enhancing the realism and interactivity of computer systems. We extract features from the Fourier spectrum of EEG signals obtained from 28 individuals engaged in playing computer games -- a public dataset. Each game elicits a different predominant emotion: boredom, calmness, horror, or joy. We analyze indi
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26631;&#31614;&#20449;&#24687;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#38382;&#39064;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;LICAP&#65289;&#65292;&#21033;&#29992;&#36830;&#32493;&#26631;&#31614;&#29983;&#25104;&#23545;&#27604;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#39640;&#37325;&#35201;&#24615;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.17791</link><description>&lt;p&gt;
&#26631;&#31614;&#20449;&#24687;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17791
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26631;&#31614;&#20449;&#24687;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#38382;&#39064;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;LICAP&#65289;&#65292;&#21033;&#29992;&#36830;&#32493;&#26631;&#31614;&#29983;&#25104;&#23545;&#27604;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#39640;&#37325;&#35201;&#24615;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#65288;NIE&#65289;&#26159;&#25512;&#26029;&#22270;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20852;&#36259;&#24050;&#32463;&#36716;&#21521;&#30693;&#35782;&#22270;&#35889;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#25110;&#32570;&#22833;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#30001;&#20110;&#25968;&#25454;&#21644;&#30693;&#35782;&#26356;&#20026;&#20016;&#23500;&#12290;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;NIE&#26041;&#27861;&#36890;&#36807;&#21487;&#29992;&#26631;&#31614;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#35757;&#32451;&#20043;&#21069;&#24179;&#31561;&#22320;&#32771;&#34385;&#27599;&#20010;&#24863;&#20852;&#36259;&#30340;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26356;&#37325;&#35201;&#30340;&#33410;&#28857;&#36890;&#24120;&#38656;&#35201;&#25110;&#20250;&#24471;&#21040;&#26356;&#22810;&#20851;&#27880;&#65292;&#20363;&#22914;&#65292;&#20154;&#20204;&#21487;&#33021;&#26356;&#20851;&#24515;&#20855;&#26377;&#26356;&#39640;&#37325;&#35201;&#24615;&#30340;&#30005;&#24433;&#25110;&#32593;&#39029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#31614;&#20449;&#24687;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#38382;&#39064;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;LICAP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20855;&#26377;&#39640;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#33410;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LICAP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36830;&#32493;&#26631;&#31614;&#29983;&#25104;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#23545;&#27604;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17791v1 Announce Type: new  Abstract: Node Importance Estimation (NIE) is a task of inferring importance scores of the nodes in a graph. Due to the availability of richer data and knowledge, recent research interests of NIE have been dedicating to knowledge graphs for predicting future or missing node importance scores. Existing state-of-the-art NIE methods train the model by available labels, and they consider every interested node equally before training. However, the nodes with higher importance often require or receive more attention in real-world scenarios, e.g., people may care more about the movies or webpages with higher importance. To this end, we introduce Label Informed ContrAstive Pretraining (LICAP) to the NIE problem for being better aware of the nodes with high importance scores. Specifically, LICAP is a novel type of contrastive learning framework that aims to fully utilize the continuous labels to generate contrastive samples for pretraining embeddings. Cons
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36991;&#20813;&#29305;&#23450;&#35757;&#32451;&#20250;&#35805;&#30340;EEG&#20998;&#31867;&#22120;&#36328;&#20219;&#21153;&#36716;&#31227;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#24247;&#22797;&#20013;&#30340;&#20010;&#24615;&#21270;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.17790</link><description>&lt;p&gt;
EEG&#20998;&#31867;&#22120;&#36328;&#20219;&#21153;&#36716;&#31227;&#65292;&#36991;&#20813;&#26426;&#22120;&#20154;&#36741;&#21161;&#24247;&#22797;&#20013;&#30340;&#35757;&#32451;&#20250;&#35805;
&lt;/p&gt;
&lt;p&gt;
EEG classifier cross-task transfer to avoid training sessions in robot-assisted rehabilitation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17790
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36991;&#20813;&#29305;&#23450;&#35757;&#32451;&#20250;&#35805;&#30340;EEG&#20998;&#31867;&#22120;&#36328;&#20219;&#21153;&#36716;&#31227;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#24247;&#22797;&#20013;&#30340;&#20010;&#24615;&#21270;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;:&#20026;&#20102;&#20010;&#24615;&#21270;&#25903;&#25345;&#24739;&#32773;&#22312;&#24247;&#22797;&#26399;&#38388;&#65292;&#38656;&#35201;&#20174;&#20154;&#31867;&#33041;&#30005;&#22270;(EEG)&#20013;&#23398;&#20064;&#20010;&#20307;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#35760;&#24405;&#24102;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#20250;&#35805;&#12290;&#26041;&#27861;:&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20998;&#31867;&#22120;&#36716;&#31227;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#27599;&#20010;&#21463;&#35797;&#32773;&#36827;&#34892;&#20102;3&#27425;40&#27425;&#33258;&#25105;&#25171;&#31639;&#30340;&#21333;&#20391;&#21644;&#21452;&#20391;&#20280;&#25163;&#36816;&#21160;&#65292;&#21521;&#19968;&#20010;&#30446;&#26631;&#36816;&#21160;&#65292;&#21516;&#26102;&#20174;64&#20010;&#36890;&#36947;&#35760;&#24405;&#20102;EEG&#25968;&#25454;&#12290;&#19968;&#20010;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#20998;&#31867;&#22120;&#22312;&#20004;&#31181;&#36816;&#21160;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17790v1 Announce Type: cross  Abstract: Background: For an individualized support of patients during rehabilitation, learning of individual machine learning models from the human electroencephalogram (EEG) is required. Our approach allows labeled training data to be recorded without the need for a specific training session. For this, the planned exoskeleton-assisted rehabilitation enables bilateral mirror therapy, in which movement intentions can be inferred from the activity of the unaffected arm. During this therapy, labeled EEG data can be collected to enable movement predictions of only the affected arm of a patient. Methods: A study was conducted with 8 healthy subjects and the performance of the classifier transfer approach was evaluated. Each subject performed 3 runs of 40 self-intended unilateral and bilateral reaching movements toward a target while EEG data was recorded from 64 channels. A support vector machine (SVM) classifier was trained under both movement cond
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#32570;&#22833;&#25110;&#22122;&#22768;&#27169;&#24577;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#23376;&#38598;&#21644;&#22122;&#22768;&#27700;&#24179;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.17788</link><description>&lt;p&gt;
&#20351;&#29992;&#32570;&#22833;&#25110;&#22122;&#22768;&#27169;&#24577;&#30340;&#22810;&#27169;&#24335;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sleep Apnea Detection with Missing or Noisy Modalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#32570;&#22833;&#25110;&#22122;&#22768;&#27169;&#24577;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#23376;&#38598;&#21644;&#22122;&#22768;&#27700;&#24179;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#26159;&#19968;&#31181;&#35760;&#24405;&#22810;&#27169;&#24335;&#29983;&#29702;&#20449;&#21495;&#30340;&#30561;&#30496;&#30740;&#31350;&#31867;&#22411;&#65292;&#24191;&#27867;&#29992;&#20110;&#30561;&#30496;&#20998;&#26399;&#21644;&#21628;&#21560;&#20107;&#20214;&#26816;&#27979;&#31561;&#30446;&#30340;&#12290;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#27599;&#20010;&#30561;&#30496;&#30740;&#31350;&#19982;&#19968;&#32452;&#22266;&#23450;&#30340;&#35266;&#23519;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#19988;&#27599;&#20010;&#26679;&#26412;&#37117;&#26377;&#25152;&#26377;&#27169;&#24577;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#22122;&#22768;&#21644;&#32570;&#22833;&#27169;&#24577;&#26159;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27969;&#31243;&#65292;&#26088;&#22312;&#22312;&#25191;&#34892;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#26816;&#27979;&#26102;&#24357;&#34917;&#32570;&#22833;&#25110;&#22122;&#22768;&#27169;&#24577;&#12290;&#19982;&#20854;&#20182;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20219;&#20309;&#21487;&#29992;&#27169;&#24577;&#32452;&#21512;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20351;&#29992;&#21508;&#31181;&#21487;&#29992;&#25968;&#25454;&#23376;&#38598;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#36827;&#34892;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#26816;&#27979;&#26102;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#23384;&#22312;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#39640;&#24615;&#33021;&#65288;AUROC&gt;0.9&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17788v1 Announce Type: cross  Abstract: Polysomnography (PSG) is a type of sleep study that records multimodal physiological signals and is widely used for purposes such as sleep staging and respiratory event detection. Conventional machine learning methods assume that each sleep study is associated with a fixed set of observed modalities and that all modalities are available for each sample. However, noisy and missing modalities are a common issue in real-world clinical settings. In this study, we propose a comprehensive pipeline aiming to compensate for the missing or noisy modalities when performing sleep apnea detection. Unlike other existing studies, our proposed model works with any combination of available modalities. Our experiments show that the proposed model outperforms other state-of-the-art approaches in sleep apnea detection using various subsets of available data and different levels of noise, and maintains its high performance (AUROC&gt;0.9) even in the presence
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSC-CoT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#21644;&#26597;&#35810;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.17786</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36880;&#27493;&#33258;&#27965;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Stepwise Self-Consistent Mathematical Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSC-CoT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#21644;&#26597;&#35810;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;Stepwise Self-Consistent Chain-of-Thought&#65288;SSC-CoT&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;SSC-CoT&#21033;&#29992;&#36873;&#25321;&#22522;&#20110;&#19981;&#21516;&#25512;&#29702;&#38142;&#20132;&#38598;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#26597;&#35810;&#21253;&#21547;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#26469;&#21457;&#29616;&#20851;&#38190;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17786v1 Announce Type: new  Abstract: Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions. To address these issues, we introduce a novel algorithm, namely Stepwise Self-Consistent Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting intermediate steps based on the intersection of various reasoning chains. Additionally, SSC-CoT enables the model to discover critical intermediate steps by querying a knowledge graph comprising relevant domain knowledge. To validate SSC-CoT, we present a new dataset, TriMaster100, tailored for complex trigonometry problems. This dataset contains 100 questions, with each solution broken down into scored intermediate steps, facilitating a comprehensive evaluation of the mathematical reasoni
&lt;/p&gt;</description></item><item><title>BagStacking&#26159;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#33258;&#20030;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#32452;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#22522;&#30784;&#27169;&#22411;&#36755;&#20986;&#21644;&#30495;&#23454;&#26631;&#31614;&#19978;&#35757;&#32451;&#20803;&#23398;&#20064;&#22120;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#27493;&#24577;&#20923;&#32467;&#26816;&#27979;&#30340;&#26174;&#30528;&#25913;&#36827;</title><link>https://arxiv.org/abs/2402.17783</link><description>&lt;p&gt;
BagStacking&#65306;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#27493;&#24577;&#20923;&#32467;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BagStacking: An Integrated Ensemble Learning Approach for Freezing of Gait Detection in Parkinson's Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17783
&lt;/p&gt;
&lt;p&gt;
BagStacking&#26159;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#33258;&#20030;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#32452;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#22522;&#30784;&#27169;&#22411;&#36755;&#20986;&#21644;&#30495;&#23454;&#26631;&#31614;&#19978;&#35757;&#32451;&#20803;&#23398;&#20064;&#22120;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#27493;&#24577;&#20923;&#32467;&#26816;&#27979;&#30340;&#26174;&#30528;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BagStacking&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19979;&#32972;&#20256;&#24863;&#22120;&#36319;&#36394;&#21152;&#36895;&#24230;&#26469;&#22686;&#24378;&#23545;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#27493;&#24577;&#20923;&#32467;&#65288;FOG&#65289;&#30340;&#26816;&#27979;&#12290;BagStacking&#24314;&#31435;&#22312;&#35013;&#34955;&#21644;&#22534;&#21472;&#30340;&#21407;&#21017;&#20043;&#19978;&#65292;&#26088;&#22312;&#23454;&#29616;&#35013;&#34955;&#30340;&#33258;&#20030;&#37319;&#26679;&#30340;&#26041;&#24046;&#20943;&#23569;&#22909;&#22788;&#65292;&#21516;&#26102;&#36890;&#36807;&#22534;&#21472;&#23398;&#20064;&#22797;&#26434;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17783v1 Announce Type: cross  Abstract: This paper introduces BagStacking, a novel ensemble learning method designed to enhance the detection of Freezing of Gait (FOG) in Parkinson's Disease (PD) by using a lower-back sensor to track acceleration. Building on the principles of bagging and stacking, BagStacking aims to achieve the variance reduction benefit of bagging's bootstrap sampling while also learning sophisticated blending through stacking. The method involves training a set of base models on bootstrap samples from the training data, followed by a meta-learner trained on the base model outputs and true labels to find an optimal aggregation scheme. The experimental evaluation demonstrates significant improvements over other state-of-the-art machine learning methods on the validation set. Specifically, BagStacking achieved a MAP score of 0.306, outperforming LightGBM (0.234) and classic Stacking (0.286). Additionally, the run-time of BagStacking was measured at 3828 sec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31354;&#38388;&#32422;&#26463;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22788;&#29702;&#28508;&#22312;&#31354;&#38388;&#36716;&#31227;&#22256;&#22659;&#65292;&#20174;&#32780;&#23558;&#20809;&#35889;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#36716;&#25442;&#20026;&#20934;&#30830;&#30340;&#21160;&#33033;&#34880;&#21387;&#31561;&#25928;&#29289;</title><link>https://arxiv.org/abs/2402.17780</link><description>&lt;p&gt;
&#32422;&#26463;&#28508;&#22312;&#31354;&#38388;&#30340;&#37325;&#35201;&#24615;&#65306;&#20174;&#28608;&#20809;&#33033;&#21160;&#27979;&#34880;&#27861;&#21040;&#21160;&#33033;&#34880;&#21387;&#30340;&#25239;&#24322;&#24120;&#27874;&#24418;&#36716;&#25442;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Constraint Latent Space Matters: An Anti-anomalous Waveform Transformation Solution from Photoplethysmography to Arterial Blood Pressure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31354;&#38388;&#32422;&#26463;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22788;&#29702;&#28508;&#22312;&#31354;&#38388;&#36716;&#31227;&#22256;&#22659;&#65292;&#20174;&#32780;&#23558;&#20809;&#35889;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#36716;&#25442;&#20026;&#20934;&#30830;&#30340;&#21160;&#33033;&#34880;&#21387;&#31561;&#25928;&#29289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#33033;&#34880;&#21387;&#65288;ABP&#65289;&#22312;&#31215;&#26497;&#30340;&#24515;&#34880;&#31649;&#20581;&#24247;&#31649;&#29702;&#26041;&#38754;&#25317;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#23613;&#31649;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;ABP&#27979;&#37327;&#30340;&#20405;&#20837;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#20027;&#35201;&#29992;&#36884;&#20165;&#38480;&#20110;&#20020;&#24202;&#29615;&#22659;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21307;&#30103;&#35774;&#26045;&#20043;&#22806;&#30340;&#36830;&#32493;&#30417;&#27979;&#36866;&#29992;&#24615;&#12290;&#23558;&#20809;&#35889;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#36716;&#25442;&#20026;ABP&#31561;&#25928;&#29289;&#24050;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20854;&#26377;&#26395;&#38761;&#26032;&#24515;&#34880;&#31649;&#30142;&#30149;&#31649;&#29702;&#12290;&#26368;&#36817;&#22312;PPG&#21040;ABP&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#21253;&#25324;&#29983;&#25104;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#30340;&#25972;&#21512;&#12290;&#23613;&#31649;&#36825;&#20123;&#36827;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#21147;&#21463;&#21040;&#28508;&#22312;&#31354;&#38388;&#36716;&#31227;&#22256;&#22659;&#30340;&#38480;&#21046;&#65292;&#28304;&#20110;PPG&#25968;&#25454;&#20998;&#24067;&#22312;&#19981;&#21516;&#30828;&#20214;&#21644;&#20010;&#20307;&#20043;&#38388;&#30340;&#21464;&#21270;&#65292;&#21487;&#33021;&#23548;&#33268;&#25197;&#26354;&#30340;ABP&#27874;&#24418;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31354;&#38388;&#32422;&#26463;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17780v1 Announce Type: cross  Abstract: Arterial blood pressure (ABP) holds substantial promise for proactive cardiovascular health management. Notwithstanding its potential, the invasive nature of ABP measurements confines their utility primarily to clinical environments, limiting their applicability for continuous monitoring beyond medical facilities. The conversion of photoplethysmography (PPG) signals into ABP equivalents has garnered significant attention due to its potential in revolutionizing cardiovascular disease management. Recent strides in PPG-to-ABP prediction encompass the integration of generative and discriminative models. Despite these advances, the efficacy of these models is curtailed by the latent space shift predicament, stemming from alterations in PPG data distribution across disparate hardware and individuals, potentially leading to distorted ABP waveforms. To tackle this problem, we present an innovative solution named the Latent Space Constraint Tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30561;&#30496;&#20998;&#26399;&#20013;&#38271;&#31243;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#36755;&#20837;&#23610;&#23544;&#26469;&#25506;&#32034;&#36827;&#19968;&#27493;&#25552;&#21319;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17779</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30561;&#30496;&#20998;&#26399;&#20013;&#38271;&#31243;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the importance of long-range correlations for deep-learning-based sleep staging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30561;&#30496;&#20998;&#26399;&#20013;&#38271;&#31243;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#36755;&#20837;&#23610;&#23544;&#26469;&#25506;&#32034;&#36827;&#19968;&#27493;&#25552;&#21319;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#38416;&#26126;&#38271;&#31243;&#30456;&#20851;&#24615;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#20998;&#26399;&#30340;&#37325;&#35201;&#24615;&#12290;&#23427;&#22260;&#32469;&#30528;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#30340;S4Sleep(TS)&#27169;&#22411;&#23637;&#24320;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20316;&#20026;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#36755;&#20837;&#65292;&#24182;&#20381;&#36182;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#65288;S4&#65289;&#27169;&#22411;&#20316;&#20026;&#22522;&#26412;&#30340;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#35813;&#27169;&#22411;&#24050;&#32463;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#36866;&#24230;&#25968;&#37327;&#30340;15&#20010;&#36755;&#20837;&#21608;&#26399;&#19978;&#30340;&#34920;&#29616;&#65292;&#20294;&#26368;&#36817;&#30340;&#25991;&#29486;&#32467;&#26524;&#26263;&#31034;&#20102;&#36890;&#36807;&#32435;&#20837;&#28085;&#30422;&#25968;&#30334;&#20010;&#36755;&#20837;&#21608;&#26399;&#30340;&#38750;&#24120;&#38271;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25193;&#22823;&#27169;&#22411;&#30340;&#36755;&#20837;&#23610;&#23544;&#26469;&#25506;&#35752;&#23454;&#29616;&#36827;&#19968;&#27493;&#20248;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#39044;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#24471;&#21040;&#25552;&#39640;&#12290;&#19982;&#25991;&#29486;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#22686;&#21152;&#36755;&#20837;&#23610;&#23544;&#24182;&#26410;&#26174;&#33879;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17779v1 Announce Type: cross  Abstract: This study aims to elucidate the significance of long-range correlations for deep-learning-based sleep staging. It is centered around S4Sleep(TS), a recently proposed model for automated sleep staging. This model utilizes electroencephalography (EEG) as raw time series input and relies on structured state space sequence (S4) models as essential model component. Although the model already surpasses state-of-the-art methods for a moderate number of 15 input epochs, recent literature results suggest potential benefits from incorporating very long correlations spanning hundreds of input epochs. In this submission, we explore the possibility of achieving further enhancements by systematically scaling up the model's input size, anticipating potential improvements in prediction accuracy. In contrast to findings in literature, our results demonstrate that augmenting the input size does not yield a significant enhancement in the performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17775</link><description>&lt;p&gt;
Wavelet&#25955;&#23556;&#21464;&#25442;&#22312;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Wavelet Scattering Transform for Bioacustics: Application to Watkins Marine Mammal Sound Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#30340;&#20132;&#27969;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#21463;&#21040;&#40483;&#21483;&#30340;&#22810;&#26679;&#24615;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#65288;WMMD&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#37325;&#28857;&#20171;&#32461;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#26368;&#26032;&#30340;&#22522;&#20934;&#35760;&#24405;&#65292;&#30528;&#37325;&#28548;&#28165;&#25968;&#25454;&#20934;&#22791;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;STFT&#22522;&#30784;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28145;&#23618;&#26550;&#26500;&#21644;&#27531;&#24046;&#23618;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20934;&#30830;&#29575;&#19978;&#20351;&#29992;WST&#27604;&#29616;&#26377;&#20998;&#31867;&#26550;&#26500;&#25552;&#39640;&#20102;6&#65285;&#65292;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#25552;&#39640;&#20102;8&#65285;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17775v1 Announce Type: cross  Abstract: Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) is an extensive labeled dataset used in machine learning applications. However, the methods for data preparation, preprocessing, and classification found in the literature are quite disparate. This study first focuses on a brief review of the state-of-the-art benchmarks on the dataset, with an emphasis on clarifying data preparation and preprocessing methods. Subsequently, we propose the application of the Wavelet Scattering Transform (WST) in place of standard methods based on the Short-Time Fourier Transform (STFT). The study also tackles a classification task using an ad-hoc deep architecture with residual layers. We outperform the existing classification architecture by $6\%$ in accuracy using WST and $8\%$ using Mel spectrogram preprocessing, effectively reducing by h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; CARLTON &#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#21160;&#24577;&#20449;&#36947;&#20998;&#37197;&#65292;&#20197;&#35299;&#20915;&#35748;&#30693;&#24178;&#25200;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#24178;&#25200;&#21152;&#22122;&#22768;&#27604;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17773</link><description>&lt;p&gt;
&#38754;&#21521;&#35748;&#30693;&#24178;&#25200;&#32593;&#32476;&#20013;&#20998;&#24067;&#24335;&#21160;&#24577;&#20449;&#36947;&#20998;&#37197;&#30340; SINR &#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SINR-Aware Deep Reinforcement Learning for Distributed Dynamic Channel Allocation in Cognitive Interference Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17773
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; CARLTON &#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#21160;&#24577;&#20449;&#36947;&#20998;&#37197;&#65292;&#20197;&#35299;&#20915;&#35748;&#30693;&#24178;&#25200;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#24178;&#25200;&#21152;&#22122;&#22768;&#27604;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#35748;&#30693;&#36890;&#20449;&#32593;&#32476;&#20013;&#21160;&#24577;&#20449;&#36947;&#20998;&#37197;&#65288;DCA&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#20026;&#27599;&#20010;&#32593;&#32476;&#35774;&#23450;&#30340;&#30446;&#26631;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;-SINR &#19979;&#26368;&#22823;&#21270;&#20840;&#23616;&#20449;&#21495;&#24178;&#25200;&#21152;&#22122;&#22768;&#27604;&#65288;SINR&#65289;&#24230;&#37327;&#12290;&#20849;&#20139;&#24102;&#23485;&#20998;&#20026;&#20855;&#26377;&#39057;&#29575;&#38388;&#38548;&#30340; K &#20010;&#20449;&#36947;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20551;&#35774;&#23436;&#20840;&#27491;&#20132;&#24615;&#25110;&#19968;&#23545;&#19968;&#29992;&#25143;-&#20449;&#36947;&#20998;&#37197;&#26144;&#23556;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#19987;&#27880;&#20110;&#23454;&#38469;&#31995;&#32479;&#20013;&#36935;&#21040;&#30340;&#36733;&#27874;&#38388;&#24178;&#25200;&#65288;ICI&#65289;&#21644;&#22810;&#20010;&#22823;&#35268;&#27169;&#32593;&#32476;&#30340;&#20449;&#36947;&#37325;&#29992;&#12290;&#36825;&#31181;&#29616;&#23454;&#22330;&#26223;&#26174;&#33879;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#32500;&#24230;&#65292;&#20351;&#24471;&#29616;&#26377;&#31639;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20998;&#24067;&#24335; DCA &#35774;&#35745;&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#21517;&#20026;&#37325;&#21472;&#32593;&#32476;&#20013;&#20449;&#36947;&#20998;&#37197; RL&#65288;CARLTON&#65289;&#12290;CARLTON&#26694;&#26550;&#22522;&#20110;&#38598;&#20013;&#24335;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21435;&#20013;&#24515;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17773v1 Announce Type: cross  Abstract: We consider the problem of dynamic channel allocation (DCA) in cognitive communication networks with the goal of maximizing a global signal-to-interference-plus-noise ratio (SINR) measure under a specified target quality of service (QoS)-SINR for each network. The shared bandwidth is partitioned into K channels with frequency separation. In contrast to the majority of existing studies that assume perfect orthogonality or a one- to-one user-channel allocation mapping, this paper focuses on real-world systems experiencing inter-carrier interference (ICI) and channel reuse by multiple large-scale networks. This realistic scenario significantly increases the problem dimension, rendering existing algorithms inefficient. We propose a novel multi-agent reinforcement learning (RL) framework for distributed DCA, named Channel Allocation RL To Overlapped Networks (CARLTON). The CARLTON framework is based on the Centralized Training with Decentra
&lt;/p&gt;</description></item><item><title>EEG2Rep&#36890;&#36807;&#22312;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#39044;&#27979;&#36974;&#34109;&#36755;&#20837;&#21644;&#20351;&#29992;&#26032;&#30340;&#35821;&#20041;&#23376;...</title><link>https://arxiv.org/abs/2402.17772</link><description>&lt;p&gt;
EEG2Rep&#65306;&#36890;&#36807;&#20449;&#24687;&#21270;&#36974;&#34109;&#36755;&#20837;&#22686;&#24378;&#33258;&#30417;&#30563;&#33041;&#30005;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17772
&lt;/p&gt;
&lt;p&gt;
EEG2Rep&#36890;&#36807;&#22312;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#39044;&#27979;&#36974;&#34109;&#36755;&#20837;&#21644;&#20351;&#29992;&#26032;&#30340;&#35821;&#20041;&#23376;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#34920;&#31034;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#38754;&#20020;EEG&#25968;&#25454;&#22266;&#26377;&#30340;&#19977;&#20010;&#29305;&#23450;&#25361;&#25112;&#65306;&#65288;1&#65289;&#20302;&#20449;&#22122;&#27604;&#25361;&#25112;&#23398;&#21040;&#30340;&#34920;&#31034;&#36136;&#37327;&#65292;&#65288;2&#65289;&#25391;&#24133;&#33539;&#22260;&#24191;&#65292;&#20174;&#38750;&#24120;&#23567;&#21040;&#30456;&#23545;&#36739;&#22823;&#65292;&#30001;&#20110;&#35832;&#22914;&#21463;&#35797;&#32773;&#38388;&#21464;&#24322;&#24615;&#31561;&#22240;&#32032;&#65292;&#39118;&#38505;&#23548;&#33268;&#27169;&#22411;&#34987;&#39640;&#25391;&#24133;&#33539;&#22260;&#20027;&#23548;&#65292;&#21644;&#65288;3&#65289;&#36830;&#32493;&#20540;&#24207;&#21015;&#20013;&#32570;&#20047;&#26126;&#30830;&#20998;&#21106;&#65292;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#36739;&#23569;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EEG2Rep&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;EEG&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#33258;&#39044;&#27979;&#26041;&#27861;&#12290;EEG2Rep&#30340;&#20004;&#20010;&#26680;&#24515;&#26032;&#39062;&#32452;&#25104;&#37096;&#20998;&#22914;&#19979;&#65306;1&#65289;EEG2Rep&#19981;&#26159;&#23398;&#20064;&#20174;&#21407;&#22987;EEG&#39044;&#27979;&#36974;&#34109;&#36755;&#20837;&#65292;&#32780;&#26159;&#23398;&#20064;&#22312;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#39044;&#27979;&#36974;&#34109;&#36755;&#20837;&#65292;2&#65289;EEG2Rep&#19981;&#20351;&#29992;&#20256;&#32479;&#30340;&#36974;&#34109;&#26041;&#27861;&#65292;&#32780;&#26159;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#35821;&#20041;&#23376;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17772v1 Announce Type: cross  Abstract: Self-supervised approaches for electroencephalography (EEG) representation learning face three specific challenges inherent to EEG data: (1) The low signal-to-noise ratio which challenges the quality of the representation learned, (2) The wide range of amplitudes from very small to relatively large due to factors such as the inter-subject variability, risks the models to be dominated by higher amplitude ranges, and (3) The absence of explicit segmentation in the continuous-valued sequences which can result in less informative representations. To address these challenges, we introduce EEG2Rep, a self-prediction approach for self-supervised representation learning from EEG. Two core novel components of EEG2Rep are as follows: 1) Instead of learning to predict the masked input from raw EEG, EEG2Rep learns to predict masked input in latent representation space, and 2) Instead of conventional masking methods, EEG2Rep uses a new semantic sub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19994;&#20313;&#26080;&#32447;&#30005;&#25805;&#20316;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20449;&#21495;&#20998;&#31867;&#21644;&#38477;&#22122;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#19994;&#20313;&#26080;&#32447;&#30005;&#36890;&#20449;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17771</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#19994;&#20313;&#26080;&#32447;&#30005;&#20449;&#21495;&#20998;&#31867;&#21644;&#38477;&#22122;
&lt;/p&gt;
&lt;p&gt;
Utilizing Machine Learning for Signal Classification and Noise Reduction in Amateur Radio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19994;&#20313;&#26080;&#32447;&#30005;&#25805;&#20316;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20449;&#21495;&#20998;&#31867;&#21644;&#38477;&#22122;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#19994;&#20313;&#26080;&#32447;&#30005;&#36890;&#20449;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19994;&#20313;&#26080;&#32447;&#30005;&#39046;&#22495;&#65292;&#26377;&#25928;&#20998;&#31867;&#20449;&#21495;&#21644;&#20943;&#23569;&#22122;&#38899;&#22312;&#30830;&#20445;&#21487;&#38752;&#36890;&#20449;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19994;&#20313;&#26080;&#32447;&#30005;&#25805;&#20316;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20449;&#21495;&#20998;&#31867;&#21644;&#38477;&#22122;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21306;&#20998;&#26399;&#26395;&#20449;&#21495;&#21644;&#19981;&#38656;&#35201;&#30340;&#24178;&#25200;&#65292;&#20197;&#21450;&#20943;&#23569;&#22122;&#38899;&#23545;&#25509;&#25910;&#20256;&#36755;&#30340;&#24433;&#21709;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#19994;&#20313;&#26080;&#32447;&#30005;&#36890;&#20449;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#65292;&#20026;&#26356;&#22810;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17771v1 Announce Type: cross  Abstract: In the realm of amateur radio, the effective classification of signals and the mitigation of noise play crucial roles in ensuring reliable communication. Traditional methods for signal classification and noise reduction often rely on manual intervention and predefined thresholds, which can be labor-intensive and less adaptable to dynamic radio environments. In this paper, we explore the application of machine learning techniques for signal classification and noise reduction in amateur radio operations. We investigate the feasibility and effectiveness of employing supervised and unsupervised learning algorithms to automatically differentiate between desired signals and unwanted interference, as well as to reduce the impact of noise on received transmissions. Experimental results demonstrate the potential of machine learning approaches to enhance the efficiency and robustness of amateur radio communication systems, paving the way for mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#38416;&#26126;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17690</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65306;&#20154;&#24037;&#26234;&#33021;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#38416;&#26126;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20132;&#36890;&#36816;&#36755;&#39046;&#22495;&#36814;&#26469;&#20102;&#19968;&#20010;&#21464;&#38761;&#26102;&#20195;&#65292;&#36890;&#36807;&#23574;&#31471;&#25216;&#26415;&#37325;&#22609;&#20102;&#31227;&#21160;&#24615;&#30340;&#26684;&#23616;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#25972;&#21512;&#26159;&#36825;&#19968;&#36827;&#21270;&#30340;&#26680;&#24515;&#65292;&#23558;&#36710;&#36742;&#25512;&#21521;&#21069;&#25152;&#26410;&#26377;&#30340;&#33258;&#20027;&#39046;&#22495;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#12290;&#20174;&#24403;&#21069;&#26223;&#35266;&#27010;&#36848;&#24320;&#22987;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;&#38416;&#26126;&#20102;AI&#39537;&#21160;&#30340;&#36710;&#36742;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#28041;&#21450;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20262;&#29702;&#32771;&#34385;&#21644;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;AI/&#23398;&#20064;&#30340;&#20351;&#29992;&#21644;&#31867;&#22411;&#30340;&#32479;&#35745;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17690v1 Announce Type: cross  Abstract: The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statis- tical insights into the usage and types of AI/learning
&lt;/p&gt;</description></item><item><title>DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.17599</link><description>&lt;p&gt;
DAGnosis&#65306;&#20351;&#29992;&#32467;&#26500;&#36827;&#34892;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#30340;&#23616;&#37096;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DAGnosis: Localized Identification of Data Inconsistencies using Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17599
&lt;/p&gt;
&lt;p&gt;
DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#26102;&#35782;&#21035;&#21644;&#36866;&#24403;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#26399;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#19982;&#35757;&#32451;&#38598;&#30456;&#20851;&#30340;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#29305;&#24449;&#23637;&#29616;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#65307;&#65288;2&#65289;&#32570;&#20047;&#23616;&#37096;&#21270;&#65292;&#26080;&#27861;&#20934;&#30830;&#23450;&#20301;&#26679;&#26412;&#20026;&#20309;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#65292;&#36825;&#23545;&#25351;&#23548;&#26410;&#26469;&#25968;&#25454;&#25910;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#26469;&#32534;&#30721;&#35757;&#32451;&#38598;&#30340;&#29305;&#24449;&#27010;&#29575;&#20998;&#24067;&#21644;&#29420;&#31435;&#24615;&#20316;&#20026;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;DAGnosis&#65292;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#20132;&#20114;&#24102;&#26469;&#26377;&#20215;&#20540;&#30340;&#12289;&#28145;&#21051;&#30340;&#25968;&#25454;&#20013;&#24515;&#32467;&#35770;&#12290;DAGnosis&#35299;&#38145;&#20102;&#22312;DAG&#19978;&#23450;&#20301;&#19981;&#19968;&#33268;&#24615;&#21407;&#22240;&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17599v1 Announce Type: cross  Abstract: Identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. While recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. We solve these two fundamental limitations using directed acyclic graphs (DAGs) to encode the training set's features probability distribution and independencies as a structure. Our method, called DAGnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. DAGnosis unlocks the localization of the causes of inconsistencies on a DAG, an aspec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#65292;&#24212;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#21644;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.17570</link><description>&lt;p&gt;
&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#65292;&#24212;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#21644;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22797;&#26434;&#21327;&#26041;&#24046;&#32467;&#26500;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;GP&#26694;&#26550;&#25193;&#23637;&#65292;&#20351;&#29992;&#21463;&#24178;&#25200;&#30340;&#27491;&#24577;&#20284;&#28982;&#20989;&#25968;&#26356;&#22909;&#22320;&#32771;&#34385;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;SVGP&#65289;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#25311;&#21512;&#20855;&#26377;&#21463;&#24178;&#25200;&#27491;&#24577;&#22122;&#22768;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#22320;&#30913;&#22320;&#38754;&#25200;&#21160;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#27169;&#22411;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#24037;&#23494;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#65292;&#20294;&#20855;&#26377;&#30456;&#20284;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17570v1 Announce Type: new  Abstract: Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.
&lt;/p&gt;</description></item><item><title>Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.17177</link><description>&lt;p&gt;
Sora: &#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#32972;&#26223;&#12289;&#25216;&#26415;&#12289;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17177
&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#30001;OpenAI&#20110;2024&#24180;2&#26376;&#21457;&#24067;&#30340;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#36924;&#30495;&#25110;&#24819;&#35937;&#30340;&#22330;&#26223;&#35270;&#39057;&#65292;&#24182;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#22522;&#20110;&#20844;&#24320;&#30340;&#25216;&#26415;&#25253;&#21578;&#21644;&#36870;&#21521;&#24037;&#31243;&#65292;&#23545;&#36825;&#20010;&#27169;&#22411;&#30340;&#32972;&#26223;&#12289;&#30456;&#20851;&#25216;&#26415;&#12289;&#24212;&#29992;&#12289;&#23578;&#23384;&#30340;&#25361;&#25112;&#20197;&#21450;&#25991;&#26412;&#21040;&#35270;&#39057;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#39318;&#20808;&#25105;&#20204;&#36861;&#28335;&#20102;Sora&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#24182;&#35843;&#26597;&#20102;&#29992;&#20110;&#26500;&#24314;&#36825;&#20010;"&#19990;&#30028;&#27169;&#25311;&#22120;"&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;Sora&#22312;&#20174;&#30005;&#24433;&#21046;&#20316;&#21644;&#25945;&#32946;&#21040;&#33829;&#38144;&#31561;&#22810;&#20010;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#20415;&#24191;&#27867;&#37096;&#32626;Sora&#65292;&#22914;&#30830;&#20445;&#23433;&#20840;&#21644;&#26080;&#20559;&#35265;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Sora&#20197;&#21450;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17177v1 Announce Type: cross  Abstract: Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video gene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#20914;&#31361;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.16897</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reliable Conflictive Multi-View Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#20914;&#31361;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#26088;&#22312;&#32467;&#21512;&#22810;&#20010;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#20840;&#38754;&#25551;&#36848;&#12290;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#20551;&#35774;&#22810;&#20010;&#35270;&#22270;&#26159;&#20005;&#26684;&#23545;&#40784;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#20302;&#36136;&#37327;&#30340;&#20914;&#31361;&#23454;&#20363;&#65292;&#21363;&#22312;&#19981;&#21516;&#35270;&#22270;&#20013;&#26174;&#31034;&#20914;&#31361;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#35201;&#27714;&#27169;&#22411;&#20026;&#20914;&#31361;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#20379;&#20915;&#31574;&#32467;&#26524;&#21644;&#38468;&#21152;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16897v1 Announce Type: cross  Abstract: Multi-view learning aims to combine multiple features to achieve more comprehensive descriptions of data. Most previous works assume that multiple views are strictly aligned. However, real-world multi-view data may contain low-quality conflictive instances, which show conflictive information in different views. Previous methods for this problem mainly focus on eliminating the conflictive data instances by removing them or replacing conflictive views. Nevertheless, real-world applications usually require making decisions for conflictive instances rather than only eliminating them. To solve this, we point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which requires the model to provide decision results and attached reliabilities for conflictive multi-view data. We develop an Evidential Conflictive Multi-view Learning (ECML) method for this problem. ECML first learns view-specific evidence, which could be termed as th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102; GPT-4 &#21644; MTurk &#31649;&#36947;&#30340;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#23613;&#31649; MTurk &#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;&#20294; GPT-4 &#30340;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#32467;&#21512; GPT-4 &#21644;&#20247;&#21253;&#26631;&#31614;&#20351;&#29992;&#32858;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16795</link><description>&lt;p&gt;
&#22914;&#26524;&#22312;&#19968;&#20010;&#20247;&#21253;&#25968;&#25454;&#26631;&#27880;&#31649;&#36947;&#20013;&#65292;GPT-4
&lt;/p&gt;
&lt;p&gt;
If in a Crowdsourced Data Annotation Pipeline, a GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102; GPT-4 &#21644; MTurk &#31649;&#36947;&#30340;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#23613;&#31649; MTurk &#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;&#20294; GPT-4 &#30340;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#32467;&#21512; GPT-4 &#21644;&#20247;&#21253;&#26631;&#31614;&#20351;&#29992;&#32858;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;GPT-4&#22312;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22312;&#32447;&#20247;&#21253;&#24037;&#20316;&#32773;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#65288;MTurk&#65289;&#30340;&#24037;&#20316;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22240;&#20559;&#31163;&#26631;&#20934;&#20247;&#21253;&#23454;&#36341;&#24182;&#24378;&#35843;&#20010;&#21035;&#24037;&#20316;&#32773;&#30340;&#34920;&#29616;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#25968;&#25454;&#26631;&#27880;&#36807;&#31243;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;GPT-4&#21644;&#19968;&#20010;&#36947;&#24503;&#19988;&#25191;&#34892;&#33391;&#22909;&#30340;MTurk&#31649;&#36947;&#65292;&#20351;&#29992;415&#21517;&#24037;&#20316;&#32773;&#26631;&#27880;&#20102;&#26469;&#33258;200&#31687;&#23398;&#26415;&#25991;&#31456;&#30340;3,177&#20010;&#21477;&#27573;&#65292;&#20351;&#29992;&#20102;CODA-19&#26041;&#26696;&#12290;&#20004;&#20010;&#24037;&#20316;&#32773;&#30028;&#38754;&#20135;&#29983;&#20102;127,080&#20010;&#26631;&#31614;&#65292;&#28982;&#21518;&#36890;&#36807;&#20843;&#31181;&#26631;&#31614;&#32858;&#21512;&#31639;&#27861;&#25512;&#26029;&#20986;&#26368;&#32456;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;MTurk&#31649;&#36947;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#20026;81.5%&#65292;&#32780;GPT-4&#36798;&#21040;&#20102;83.6%&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#23558;GPT-4&#30340;&#26631;&#31614;&#19982;&#36890;&#36807;&#20808;&#36827;&#24037;&#20316;&#32773;&#30028;&#38754;&#25910;&#38598;&#30340;&#20247;&#21253;&#26631;&#31614;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#32858;&#21512;&#26102;&#65292;8&#31181;&#31639;&#27861;&#20013;&#26377;2&#31181;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16795v1 Announce Type: cross  Abstract: Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#29616;&#26377;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#28040;&#38500;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.15808</link><description>&lt;p&gt;
&#22810;&#33218;&#25915;&#20987;&#30340;&#26368;&#20339;&#38646;&#23556;&#20987;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Optimal Zero-Shot Detector for Multi-Armed Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#29616;&#26377;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#28040;&#38500;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24694;&#24847;&#21442;&#19982;&#32773;&#37319;&#29992;&#22810;&#33218;&#25915;&#20987;&#31574;&#30053;&#25805;&#32437;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#20026;&#20854;&#25552;&#20379;&#20102;&#21508;&#31181;&#26041;&#24335;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#22122;&#38899;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#26816;&#27979;&#20219;&#20309;&#23545;&#36755;&#20837;&#30340;&#26356;&#25913;&#26469;&#20445;&#25252;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#38450;&#24481;&#31574;&#30053;&#20013;&#26497;&#24230;&#35880;&#24910;&#65292;&#25805;&#20316;&#22312;&#38450;&#23432;&#32773;&#25317;&#26377;&#20449;&#24687;&#26126;&#26174;&#23569;&#20110;&#25915;&#20987;&#32773;&#30340;&#29615;&#22659;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38450;&#23432;&#32773;&#26080;&#27861;&#21033;&#29992;&#20219;&#20309;&#25968;&#25454;&#26679;&#26412;&#26469;&#35757;&#32451;&#38450;&#24481;&#27169;&#22411;&#25110;&#39564;&#35777;&#20449;&#36947;&#30340;&#23436;&#25972;&#24615;&#12290;&#30456;&#21453;&#65292;&#38450;&#23432;&#32773;&#23436;&#20840;&#20381;&#36182;&#19968;&#32452;&#29616;&#25104;&#30340;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#25506;&#27979;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#36825;&#20123;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#20351;&#29992;&#26696;&#20363;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15808v1 Announce Type: cross  Abstract: This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available ``off the shelf''. To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario fo
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21367;&#31215;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#35768;&#22810;&#31867;&#22411;&#30340;CNNs&#21487;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;CNNs&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#26032;&#26550;&#26500;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.15490</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#21367;&#31215;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15490
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21367;&#31215;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#35768;&#22810;&#31867;&#22411;&#30340;CNNs&#21487;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;CNNs&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#26032;&#26550;&#26500;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNNs) &#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#22270;&#20687;&#20998;&#21106;&#12290;&#26377;&#35768;&#22810;&#31867;&#22411;&#30340;CNNs&#26088;&#22312;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#21644;&#35201;&#27714;&#65292;&#21253;&#25324;1D&#12289;2D&#21644;3D CNNs&#65292;&#20197;&#21450;&#25193;&#24352;&#30340;&#12289;&#20998;&#32452;&#30340;&#12289;&#27880;&#24847;&#21147;&#30340;&#12289;&#28145;&#24230;&#21487;&#20998;&#30340;&#21367;&#31215;&#21644;NAS&#31561;&#12290;&#27599;&#31181;&#31867;&#22411;&#30340;CNN&#20855;&#26377;&#20854;&#29420;&#29305;&#30340;&#32467;&#26500;&#21644;&#29305;&#28857;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#28145;&#20837;&#20102;&#35299;&#24182;&#23545;&#36825;&#20123;&#19981;&#21516;&#31867;&#22411;&#30340;CNN&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#27599;&#31181;&#31867;&#22411;CNN&#30340;&#24615;&#33021;&#12289;&#38480;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#21487;&#20197;&#24110;&#21161;&#26410;&#26469;&#24320;&#21457;&#26032;&#30340;&#25913;&#36827;&#26550;&#26500;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#30740;&#31350;&#20154;&#21592;&#29992;&#20110;&#30740;&#31350;&#25110;&#24320;&#21457;&#30340;&#24179;&#21488;&#21644;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15490v1 Announce Type: new  Abstract: In today's digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It's crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or develop
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15194</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#20316;&#20026;&#29109;&#27491;&#21017;&#21270;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15194
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#21644;&#34507;&#30333;&#36136;&#30340;&#20998;&#24067;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21487;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#65292;&#20294;&#25105;&#20204;&#36890;&#24120;&#26356;&#20851;&#27880;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#29983;&#25104;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#25110;&#29983;&#25104;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#23646;&#24615;&#12290;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26576;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#65289;&#20197;&#30446;&#26631;&#23548;&#21521;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#26679;&#26412;&#22810;&#26679;&#24615;&#20943;&#23569;&#65292;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20986;&#29616;&#26174;&#33879;&#20559;&#24046;&#65292;&#29978;&#33267;&#30001;&#20110;&#21033;&#29992;&#19981;&#23436;&#32654;&#30340;&#22870;&#21169;&#20989;&#25968;&#32780;&#23548;&#33268;&#26679;&#26412;&#36136;&#37327;&#36739;&#24046;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#22870;&#21169;&#20989;&#25968;&#26159;&#29992;&#20110;&#36817;&#20284;&#30495;&#23454;&#8220;&#30495;&#23454;&#8221;&#22870;&#21169;&#30340;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#32463;&#24120;&#20250;&#20135;&#29983;&#12290;&#36825;&#20123;&#25361;&#25112;&#24635;&#31216;&#20026;&#8220;&#22870;&#21169;&#23849;&#28291;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose
&lt;/p&gt;</description></item><item><title>RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15179</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#25512;&#36827;&#24494;&#35843;&#20013;&#30340;&#21442;&#25968;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Advancing Parameter Efficiency in Fine-tuning via Representation Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15179
&lt;/p&gt;
&lt;p&gt;
RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#22240;&#20854;&#33021;&#22815;&#22312;&#20165;&#26356;&#26032;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#26102;&#36798;&#21040;&#31454;&#20105;&#24615;&#32467;&#26524;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#34920;&#31034;&#32534;&#36753;&#65288;RED&#65289;&#65292;&#20854;&#25193;&#25918;&#21644;&#20559;&#32622;&#27599;&#19968;&#23618;&#20135;&#29983;&#30340;&#34920;&#31034;&#12290;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#27604;&#65292;RED&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38477;&#20302;&#20102;$25,700$&#20493;&#65292;&#24182;&#19982;LoRA&#30456;&#27604;&#38477;&#20302;&#20102;32&#20493;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RED&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#19981;&#21516;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15160</link><description>&lt;p&gt;
&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#35760;&#24518;&#20307;&#29992;&#20110;&#20307;&#39564;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Spatially-Aware Transformer Memory for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#33410;&#35760;&#24518;&#22312;&#21508;&#31181;&#35748;&#30693;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#27604;&#22914;&#33021;&#22815;&#22312;&#22836;&#33041;&#20013;&#22238;&#24518;&#36807;&#21435;&#20107;&#20214;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#35748;&#30693;&#31185;&#23398;&#24378;&#35843;&#31354;&#38388;&#19978;&#19979;&#25991;&#22312;&#24773;&#33410;&#35760;&#24518;&#30340;&#24418;&#25104;&#21644;&#26816;&#32034;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24403;&#21069;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#24773;&#33410;&#35760;&#24518;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#36890;&#36807;&#23384;&#20648;&#26102;&#38388;&#39034;&#24207;&#20307;&#39564;&#30340;&#21464;&#21387;&#22120;&#65292;&#36825;&#24573;&#30053;&#20102;&#31354;&#38388;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;&#22522;&#30784;&#32467;&#26500;&#25193;&#23637;&#21040;&#38500;&#20102;&#20165;&#26377;&#26102;&#38388;&#39034;&#24207;&#20043;&#22806;&#30340;&#31354;&#38388;&#36724;&#65292;&#24182;&#30001;&#27492;&#33021;&#22815;&#33719;&#24471;&#21738;&#20123;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#32771;&#34385;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#30340;&#22330;&#25152;&#20013;&#24515;&#24773;&#33410;&#35760;&#24518;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#65292;&#23548;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15160v1 Announce Type: cross  Abstract: Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#20803;&#21160;&#24577;&#20146;&#21644;&#20851;&#31995;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#19981;&#21516;&#26102;&#38388;&#28857;&#31070;&#32463;&#20803;&#20043;&#38388;&#24418;&#25104;&#30340;&#31038;&#21306;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#32452;&#12290;</title><link>https://arxiv.org/abs/2402.14102</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#29983;&#29289;&#32593;&#32476;&#20013;&#23398;&#20064;&#21151;&#33021;&#36830;&#25509;&#32452;&#30340;&#21160;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning dynamic representations of the functional connectome in neurobiological networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#20803;&#21160;&#24577;&#20146;&#21644;&#20851;&#31995;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#19981;&#21516;&#26102;&#38388;&#28857;&#31070;&#32463;&#20803;&#20043;&#38388;&#24418;&#25104;&#30340;&#31038;&#21306;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22238;&#36335;&#30340;&#38745;&#24577;&#31361;&#35302;&#36830;&#25509;&#19982;&#20854;&#21151;&#33021;&#30340;&#21160;&#24577;&#24418;&#25104;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#19981;&#21516;&#20110;&#38745;&#24577;&#36830;&#25509;&#65292;&#19981;&#21516;&#31070;&#32463;&#20803;&#21487;&#20197;&#22312;&#19981;&#21516;&#26102;&#38388;&#31215;&#26497;&#21442;&#19982;&#21508;&#31181;&#32452;&#21512;&#65292;&#23454;&#29616;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#27963;&#29983;&#29983;&#21160;&#30340;&#21160;&#29289;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#21160;&#24577;&#20146;&#21644;&#21147;&#65292;&#24182;&#25581;&#31034;&#19981;&#21516;&#26102;&#38388;&#28857;&#31070;&#32463;&#20803;&#20043;&#38388;&#24418;&#25104;&#30340;&#31038;&#21306;&#12290;&#25512;&#26029;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#38750;&#36127;&#24352;&#37327;&#22240;&#23376;&#20998;&#35299;(NTF)&#32452;&#32455;&#26469;&#33258;&#22823;&#33041;&#20840;&#38754;&#38041;&#27963;&#21160;&#30340;&#31070;&#32463;&#20803;&#30165;&#36857;&#20043;&#38388;&#30340;&#25104;&#23545;&#38750;&#32447;&#24615;&#20146;&#21644;&#21147;&#12290;&#27599;&#20010;&#22240;&#23376;&#25351;&#23450;&#20102;&#21738;&#20123;&#31070;&#32463;&#20803;&#32676;&#20307;&#22312;&#29305;&#23450;&#26102;&#38388;&#38388;&#38548;&#21644;&#21160;&#29289;&#19978;&#26368;&#26377;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#23558;&#20801;&#35768;&#21152;&#26435;&#31038;&#21306;&#26816;&#27979;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;NTF&#20135;&#29983;&#30340;&#21151;&#33021;&#22522;&#24207;&#65292;&#20197;&#25581;&#31034;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14102v1 Announce Type: cross  Abstract: The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function. As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times. We introduce an unsupervised approach to learn the dynamic affinities between neurons in live, behaving animals, and to reveal which communities form among neurons at different times. The inference occurs in two major steps. First, pairwise non-linear affinities between neuronal traces from brain-wide calcium activity are organized by non-negative tensor factorization (NTF). Each factor specifies which groups of neurons are most likely interacting for an inferred interval in time, and for which animals. Finally, a generative model that allows for weighted community detection is applied to the functional motifs produced by NTF to reveal a dynamic functional connectome. Since time 
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14095</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot generalization across architectures for visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14095
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26497;&#31616;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#27867;&#21270;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#20174;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;CNNs&#65289;&#21040;transformers&#30340;&#27969;&#34892;&#32593;&#32476;&#22312;&#36890;&#36807;&#23618;&#21644;&#26550;&#26500;&#27867;&#21270;&#21040;&#26410;&#35265;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#19988;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/dyballa/zero-shot-generalization &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13754</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-assisted quantum architecture search for variational quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13754
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#30830;&#23450;&#21151;&#33021;&#24615;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#30005;&#36335;&#24517;&#39035;&#21516;&#26102;&#31526;&#21512;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#38480;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQA&#65289;&#26159;&#19968;&#31867;&#37327;&#23376;-&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21487;&#29992;&#37327;&#23376;&#35774;&#22791;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#20391;&#37325;&#20110;&#30005;&#36335;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20248;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;&#35770;&#25991;&#20869;&#36890;&#36807;&#35780;&#20272;&#30005;&#36335;&#30340;&#28145;&#24230;&#12289;&#38376;&#21644;&#21442;&#25968;&#30340;&#24635;&#25968;&#20197;&#21450;&#20934;&#30830;&#24615;&#26469;&#30830;&#23450;&#30005;&#36335;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30417;&#27979;&#19977;&#31181;&#30693;&#21517;DL&#26694;&#26550;&#20197;&#21450;ONNX&#30340;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33021;&#32791;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#65292;&#21021;&#27493;&#25506;&#31350;&#20102;&#23427;&#20204;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13640</link><description>&lt;p&gt;
&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;: &#36328;&#19981;&#21516;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#32791;&#21021;&#27493;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30417;&#27979;&#19977;&#31181;&#30693;&#21517;DL&#26694;&#26550;&#20197;&#21450;ONNX&#30340;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33021;&#32791;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#65292;&#21021;&#27493;&#25506;&#31350;&#20102;&#23427;&#20204;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13640v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#23398;&#31185; &#25688;&#35201;: &#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#22914;PyTorch&#21644;TensorFlow&#21253;&#25324;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#65292;&#36127;&#36131;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#25191;&#34892;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#31649;&#29702;&#20869;&#23384;&#12289;&#25968;&#25454;&#20256;&#36755;&#20197;&#21450;&#22810;&#21152;&#36895;&#22120;&#25191;&#34892;&#65288;&#22914;&#26524;&#36866;&#29992;&#65289;&#12290;&#27492;&#22806;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#37096;&#32626;&#21040;&#19982;&#20854;&#21407;&#29983;&#24320;&#21457;&#29615;&#22659;&#19981;&#21516;&#30340;&#29615;&#22659;&#26159;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#36825;&#23548;&#33268;&#24341;&#20837;&#20102;&#35832;&#22914;ONNX&#20043;&#31867;&#30340;&#20132;&#25442;&#26684;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#20854;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#21450;ONNX Runtime&#65292;&#21487;&#20316;&#20026;&#21487;&#22312;&#19981;&#21516;DL&#26694;&#26550;&#21644;&#35821;&#35328;&#20043;&#38388;&#20351;&#29992;&#30340;&#26631;&#20934;&#26684;&#24335;&#12290;&#23613;&#31649;&#36825;&#20123;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#23545;&#25512;&#29702;&#24615;&#33021;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#20294;&#20197;&#21069;&#27809;&#26377;&#35770;&#25991;&#35843;&#26597;&#36807;&#23427;&#20204;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30417;&#27979;&#20102;&#19977;&#31181;&#30693;&#21517;DL&#26694;&#26550;&#20197;&#21450;ONNX&#30340;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33021;&#32791;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#35843;&#26597;&#26356;&#21152;&#32454;&#33268;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13640v1 Announce Type: cross  Abstract: Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime infrastructures responsible for executing trained models on target hardware, managing memory, data transfers, and multi-accelerator execution, if applicable. Additionally, it is a common practice to deploy pre-trained models on environments distinct from their native development settings. This led to the introduction of interchange formats such as ONNX, which includes its runtime infrastructure, and ONNX Runtime, which work as standard formats that can be used across diverse DL frameworks and languages. Even though these runtime infrastructures have a great impact on inference performance, no previous paper has investigated their energy efficiency. In this study, we monitor the energy consumption and inference time in the runtime infrastructures of three well-known DL frameworks as well as ONNX, using three various DL models. To have nuance in our investigatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.13573</link><description>&lt;p&gt;
&#20219;&#21153;&#24453;&#21150;&#65306;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#39640;&#25928;&#29983;&#25104;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
ToDo: Token Downsampling for Efficient Generation of High-Resolution Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13573
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#25105;&#20204;&#21487;&#20197;&#22312;&#21512;&#29702;&#26102;&#38388;&#21644;&#20869;&#23384;&#38480;&#21046;&#20869;&#22788;&#29702;&#30340;&#22270;&#20687;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#31232;&#30095;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; ToDo&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20851;&#38190;&#21644;&#20540;&#20196;&#29260;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;&#65292;&#21487;&#23558;&#24120;&#35265;&#22823;&#23567;&#30340; Stable Diffusion &#25512;&#29702;&#21152;&#36895;&#33267;&#22810;&#36798;2&#20493;&#65292;&#23545;&#20110;2048x2048&#31561;&#39640;&#20998;&#36776;&#29575;&#65292;&#21152;&#36895;&#27604;&#21487;&#36798;4.5&#20493;&#25110;&#26356;&#39640;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#34913;&#39640;&#25928;&#21534;&#21520;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13573v1 Announce Type: cross  Abstract: Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12921</link><description>&lt;p&gt;
&#20934;&#26102;&#21040;&#20301;&#65306;&#36890;&#36807;&#38480;&#21046;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#35299;&#37322;&#26469;&#20462;&#35746;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Right on Time: Revising Time Series Models by Constraining their Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12921
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#32463;&#24120;&#20250;&#21463;&#21040;&#20854;&#20381;&#36182;&#28151;&#26434;&#22240;&#32032;&#30340;&#20542;&#21521;&#30340;&#25439;&#23475;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26032;&#35760;&#24405;&#30340;&#12289;&#33258;&#28982;&#28151;&#26434;&#30340;&#25968;&#25454;&#38598;P2S&#26469;&#33258;&#30495;&#23454;&#30340;&#26426;&#26800;&#29983;&#20135;&#32447;&#65292;&#24378;&#35843;&#20102;&#36825;&#19968;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#21518;&#21033;&#29992;&#20004;&#20010;&#22495;&#20869;&#30340;&#35299;&#37322;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#20351;&#20854;&#36828;&#31163;&#26631;&#27880;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#28151;&#26434;&#22240;&#32032;&#26041;&#38754;&#65292;&#21452;&#22495;&#20132;&#20114;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;RioT&#33021;&#22815;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#36828;&#31163;P2S&#20197;&#21450;&#27969;&#34892;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12921v1 Announce Type: new  Abstract: The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to misleading results. Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this. To tackle the challenging problem of mitigating confounders in time series data, we introduce Right on Time (RioT). Our method enables interactions with model explanations across both the time and frequency domain. Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors. The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets. We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets.
&lt;/p&gt;</description></item><item><title>&#23545;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#30340;&#26426;&#29702;&#20998;&#26512;&#25581;&#31034;&#20854;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;</title><link>https://arxiv.org/abs/2402.11917</link><description>&lt;p&gt;
&#22312;&#31526;&#21495;&#21270;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#30340;&#26426;&#29702;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11917
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#30340;&#26426;&#29702;&#20998;&#26512;&#25581;&#31034;&#20854;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#19968;&#31995;&#21015;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#23454;&#38469;&#25512;&#29702;&#30340;&#32467;&#26524;&#65292;&#29616;&#26377;&#24037;&#20316;&#38598;&#20013;&#20110;&#24320;&#21457;&#22797;&#26434;&#30340;&#34892;&#20026;&#30740;&#31350;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#25552;&#20379;&#20851;&#20110;&#39537;&#21160;&#35266;&#23519;&#21040;&#30340;&#33021;&#21147;&#30340;&#20869;&#37096;&#26426;&#21046;&#30340;&#35265;&#35299;&#12290;&#20026;&#20102;&#25913;&#21892;&#25105;&#20204;&#23545;Transformer&#20869;&#37096;&#26426;&#21046;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#19968;&#20010;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#20840;&#38754;&#30340;&#26426;&#29702;&#20998;&#26512;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#27169;&#22411;&#29992;&#26469;&#35299;&#20915;&#20219;&#21153;&#30340;&#19968;&#32452;&#21487;&#35299;&#37322;&#26426;&#21046;&#65292;&#24182;&#21033;&#29992;&#30456;&#20851;&#21644;&#22240;&#26524;&#35777;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;&#12290;&#25105;&#20204;&#39044;&#26399;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#35782;&#21035;&#30340;&#20027;&#39064;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11917v1 Announce Type: new  Abstract: Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights
&lt;/p&gt;</description></item><item><title>PolypNextLSTM&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;&#65292;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#21442;&#25968;&#26368;&#23569;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20116;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11585</link><description>&lt;p&gt;
PolypNextLSTM&#65306;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#30340;&#36731;&#37327;&#32423;&#24555;&#36895;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11585
&lt;/p&gt;
&lt;p&gt;
PolypNextLSTM&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;&#65292;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#21442;&#25968;&#26368;&#23569;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20116;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#29992;&#20110;&#24687;&#32905;&#20998;&#21106;&#30340;&#21333;&#22270;&#20687;UNet&#26550;&#26500;&#32570;&#20047;&#20020;&#24202;&#21307;&#29983;&#22312;&#35786;&#26029;&#24687;&#32905;&#26102;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26102;&#38388;&#27934;&#23519;&#12290;&#20026;&#20102;&#26356;&#24544;&#23454;&#22320;&#21453;&#26144;&#20020;&#24202;&#23454;&#36341;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;PolypNextLSTM&#21033;&#29992;&#22522;&#20110;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#21442;&#25968;&#24320;&#38144;&#26368;&#23567;&#65292;&#21487;&#33021;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;PolypNextLSTM&#37319;&#29992;&#31867;&#20284;UNet&#30340;&#32467;&#26500;&#65292;ConvNext-Tiny&#20316;&#20026;&#20854;&#20027;&#24178;&#65292;&#31574;&#30053;&#24615;&#22320;&#30465;&#30053;&#26368;&#21518;&#20004;&#23618;&#20197;&#20943;&#23569;&#21442;&#25968;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#26102;&#38388;&#34701;&#21512;&#27169;&#22359;&#65292;&#19968;&#20010;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;ConvLSTM&#65289;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;PolypNextLSTM&#65292;&#22312;&#21442;&#25968;&#19978;&#26368;&#30246;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#36229;&#36234;&#20102;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;SUN-SEG&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#36328;&#36234;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11585v1 Announce Type: cross  Abstract: Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.10693</link><description>&lt;p&gt;
&#25506;&#32034;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#20197;&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Precision and Recall to assess the quality and diversity of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama-2&#21644;Mistral&#30340;&#26032;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#37325;&#28857;&#26159;&#23558;&#22270;&#20687;&#29983;&#25104;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#36827;&#34892;&#32454;&#33268;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#23545;&#40784;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24320;&#25918;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20256;&#32479;&#22522;&#20934;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#27169;&#22411;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#22522;&#20110;&#20998;&#24067;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20026;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#26041;&#38754;&#38754;&#20020;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#30452;&#25509;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#30340;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10240</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38382;&#39064;&#30340;&#21160;&#24577;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Dynamical View of the Question of Why
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#30452;&#25509;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#30340;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#30001;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25512;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#38745;&#24577;&#35774;&#32622;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#24615;&#21644;&#21464;&#21270;&#30340;&#21457;&#23556;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#33539;&#24335;&#65292;&#30452;&#25509;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#26469;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#65292;&#24182;&#23558;&#20854;&#26500;&#36896;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#21644;&#35745;&#31639;&#24037;&#20855;&#65292;&#21253;&#25324;&#21508;&#31181;&#37325;&#35201;&#35774;&#32622;&#65292;&#22914;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#30456;&#24403;&#22797;&#26434;&#30340;&#23454;&#39564;&#21644;&#36890;&#36807;&#32431;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#21644;&#37327;&#21270;&#20102;&#22240;&#26524;&#32852;&#31995;&#65292;&#21542;&#21017;&#30475;&#20284;&#33707;&#21517;&#20854;&#22937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10240v1 Announce Type: cross  Abstract: We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as reinforcement learning problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes. Finally, in fairly intricate experiments and through sheer learning, our framework reveals and quantifies causal links, which otherwise seem inexplicable.
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>BitDelta&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BitDelta&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#37327;&#21270;&#20026;&#19968;&#20010;&#27604;&#29305;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;GPU&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.10193</link><description>&lt;p&gt;
BitDelta&#65306;&#20320;&#30340;&#24494;&#35843;&#21487;&#33021;&#21482;&#26377;&#19968;&#20010;&#27604;&#29305;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
BitDelta: Your Fine-Tune May Only Be Worth One Bit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10193
&lt;/p&gt;
&lt;p&gt;
BitDelta&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BitDelta&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#37327;&#21270;&#20026;&#19968;&#20010;&#27604;&#29305;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;GPU&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#65306;&#22312;&#22823;&#35268;&#27169;&#20114;&#32852;&#32593;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#30452;&#35273;&#19978;&#35748;&#20026;&#24494;&#35843;&#23545;&#27169;&#22411;&#30340;&#20449;&#24687;&#28155;&#21152;&#36739;&#23569;&#65292;&#22240;&#27492;&#26356;&#20855;&#26377;&#21487;&#21387;&#32553;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#39044;&#35757;&#32451;&#32452;&#20214;&#21644;&#39069;&#22806;&#30340;&#22686;&#37327;&#26469;&#25506;&#31350;&#36825;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#8212;&#8212;BitDelta&#65292;&#25104;&#21151;&#22320;&#23558;&#36825;&#20010;&#22686;&#37327;&#37327;&#21270;&#20026;1&#27604;&#29305;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#19968;&#26377;&#36259;&#30340;&#21457;&#29616;&#19981;&#20165;&#31361;&#26174;&#20102;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#30340;&#28508;&#22312;&#20887;&#20313;&#24615;&#65292;&#32780;&#19988;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#20063;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#39640;&#31934;&#24230;&#30340;&#22522;&#30784;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;1&#27604;&#29305;&#30340;&#22686;&#37327;&#65292;BitDelta&#22823;&#22823;&#38477;&#20302;&#20102;GPU&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10193v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requir
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.04830</link><description>&lt;p&gt;
&#32553;&#23567;SGP4&#21644;&#39640;&#31934;&#24230;&#20256;&#25773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#36890;&#36807;&#21487;&#24494;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21270;&#30340;&#31532;&#22235;&#32423;&#25668;&#21160;(SGP4)&#36712;&#36947;&#20256;&#25773;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#24555;&#36895;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#29699;&#36712;&#36947;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#23613;&#31649;&#19981;&#26029;&#25913;&#36827;&#65292;SGP&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#25968;&#20540;&#20256;&#25773;&#22120;&#30340;&#31934;&#24230;&#65292;&#21518;&#32773;&#30340;&#35823;&#24046;&#26174;&#33879;&#36739;&#23567;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#26032;&#22411;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#20351;SGP4&#21487;&#24494;&#21270;&#65292;dSGP4&#20415;&#20110;&#36827;&#34892;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33322;&#22825;&#22120;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#36716;&#25442;&#12289;&#29366;&#24577;&#36716;&#31227;&#30697;&#38453;&#35745;&#31639;&#21644;&#21327;&#26041;&#24046;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;PyTorch&#23454;&#29616;&#20801;&#35768;&#22312;&#25209;&#37327;&#30340;TLE&#65288;&#20004;&#34892;&#21442;&#25968;&#65289;&#38598;&#19978;&#36827;&#34892;&#23604;&#23596;&#30340;&#24182;&#34892;&#36712;&#36947;&#20256;&#25773;&#65292;&#21033;&#29992;CPU&#12289;GPU&#21644;&#20998;&#24067;&#24335;&#39044;&#27979;&#21355;&#26143;&#20301;&#32622;&#30340;&#39640;&#32423;&#30828;&#20214;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;&#21487;&#24494;&#24615;&#20351;&#20854;&#33021;&#19982;&#27169;&#24335;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30456;&#20851;&#20302;&#31209;&#32467;&#26500;&#65288;CoLR&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#36890;&#20449;&#21644;&#23433;&#20840;&#24615;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#24320;&#38144;&#21516;&#26102;&#20445;&#25345;&#19982;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.03748</link><description>&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#21644;&#23433;&#20840;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Communication and Secure Federated Recommendation System via Low-rank Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03748
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30456;&#20851;&#20302;&#31209;&#32467;&#26500;&#65288;CoLR&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#36890;&#20449;&#21644;&#23433;&#20840;&#24615;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#24320;&#38144;&#21516;&#26102;&#20445;&#25345;&#19982;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03748v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#38024;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#30417;&#31649;&#20851;&#27880;&#24341;&#21457;&#30340;&#38656;&#27714;&#65292;&#32852;&#37030;&#25512;&#33616;&#65288;FedRec&#65289;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#20256;&#36755;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#21363;&#22312;&#29992;&#25143;&#35774;&#22791;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#12290;&#20808;&#21069;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#35832;&#22914;&#35745;&#31639;&#24320;&#38144;&#12289;&#27169;&#22411;&#29305;&#24615;&#38480;&#21046;&#20197;&#21450;&#19982;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290; &#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30456;&#20851;&#20302;&#31209;&#32467;&#26500;&#65288;CoLR&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#35843;&#25972;&#36731;&#37327;&#32423;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#22823;&#37096;&#20998;&#21442;&#25968;&#20923;&#32467;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#32780;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#23436;&#20840;&#20860;&#23481;&#65292;&#21253;&#25324;&#40065;&#26834;&#22320;&#20351;&#29992;&#21516;&#24577;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03748v2 Announce Type: replace  Abstract: Federated Recommendation (FedRec) systems have emerged as a solution to safeguard users' data in response to growing regulatory concerns. However, one of the major challenges in these systems lies in the communication costs that arise from the need to transmit neural network models between user devices and a central server. Prior approaches to these challenges often lead to issues such as computational overheads, model specificity constraints, and compatibility issues with secure aggregation protocols. In response, we propose a novel framework, called Correlated Low-rank Structure (CoLR), which leverages the concept of adjusting lightweight trainable parameters while keeping most parameters frozen. Our approach substantially reduces communication overheads without introducing additional computational burdens. Critically, our framework remains fully compatible with secure aggregation protocols, including the robust use of Homomorphic 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36136;&#37327;-&#22810;&#26679;&#24615;&#29983;&#25104;&#25277;&#26679;&#65288;QDGS&#65289;&#26694;&#26550;, &#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26102;&#20445;&#25252;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#30446;&#26631;, &#24182;&#25104;&#21151;&#29992;&#20110;&#21435;&#20559;&#21521;&#20998;&#31867;&#22120;&#21644;&#38754;&#37096;&#25968;&#25454;&#21512;&#25104;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2312.14369</link><description>&lt;p&gt;
&#22522;&#20110;&#36136;&#37327;-&#22810;&#26679;&#24615;&#29983;&#25104;&#25277;&#26679;&#30340;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity Generative Sampling for Learning with Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36136;&#37327;-&#22810;&#26679;&#24615;&#29983;&#25104;&#25277;&#26679;&#65288;QDGS&#65289;&#26694;&#26550;, &#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26102;&#20445;&#25252;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#30446;&#26631;, &#24182;&#25104;&#21151;&#29992;&#20110;&#21435;&#20559;&#21521;&#20998;&#31867;&#22120;&#21644;&#38754;&#37096;&#25968;&#25454;&#21512;&#25104;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#20316;&#20026;&#26576;&#20123;&#30495;&#23454;&#25968;&#25454;&#28304;&#30340;&#26367;&#20195;&#21697;&#65292;&#20294;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23558;&#20559;&#35265;&#20256;&#36882;&#32473;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#20851;&#27880;&#22312;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26102;&#22914;&#20309;&#20445;&#25252;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36136;&#37327;-&#22810;&#26679;&#24615;&#29983;&#25104;&#25277;&#26679;&#65288;QDGS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#22343;&#21248;&#25277;&#26679;&#25968;&#25454;&#65292;&#23613;&#31649;&#25968;&#25454;&#26469;&#33258;&#19968;&#20010;&#23384;&#22312;&#20559;&#35265;&#30340;&#29983;&#25104;&#22120;&#12290;QDGS&#26159;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25552;&#31034;&#25351;&#23548;&#26469;&#20248;&#21270;&#36328;&#24230;&#37327;&#22810;&#26679;&#24615;&#30340;&#36136;&#37327;&#30446;&#26631;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#12290;&#20351;&#29992;QDGS&#29983;&#25104;&#30340;&#24179;&#34913;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#28040;&#38500;&#20102;&#23545;&#22522;&#20110;&#39068;&#33394;&#20559;&#35265;&#30340;&#24418;&#29366;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#22120;&#30340;&#20559;&#35265;&#65292;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#36890;&#36807;&#23558;QDGS&#24212;&#29992;&#20110;&#38754;&#37096;&#25968;&#25454;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#31034;&#38656;&#35201;&#30340;&#35821;&#20041;&#27010;&#24565;&#65292;&#22914;&#32932;&#33394;&#21644;&#24180;&#40836;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20132;&#38598;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14369v2 Announce Type: replace-cross  Abstract: Generative models can serve as surrogates for some real data sources by creating synthetic training datasets, but in doing so they may transfer biases to downstream tasks. We focus on protecting quality and diversity when generating synthetic training datasets. We propose quality-diversity generative sampling (QDGS), a framework for sampling data uniformly across a user-defined measure space, despite the data coming from a biased generator. QDGS is a model-agnostic framework that uses prompt guidance to optimize a quality objective across measures of diversity for synthetically generated data, without fine-tuning the generative model. Using balanced synthetic datasets generated by QDGS, we first debias classifiers trained on color-biased shape datasets as a proof-of-concept. By applying QDGS to facial data synthesis, we prompt for desired semantic concepts, such as skin tone and age, to create an intersectional dataset with a c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.05440</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21644;&#24555;&#36895;&#27169;&#25311;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Consistency Models for Scalable and Fast Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05440
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25512;&#26029;&#65288;SBI&#65289;&#19981;&#26029;&#23547;&#25214;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#31639;&#27861;&#65292;&#20197;&#20934;&#30830;&#25512;&#26029;&#22797;&#26434;&#27169;&#22411;&#30340;&#21442;&#25968;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMPE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#30340;&#26032;&#33258;&#30001;&#24418;&#24335;&#26465;&#20214;&#37319;&#26679;&#22120;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;CMPE&#23558;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#21040;&#21333;&#20010;&#29983;&#25104;&#26550;&#26500;&#20013;&#65306;&#23427;&#26412;&#36136;&#19978;&#25552;&#28860;&#20102;&#36830;&#32493;&#27010;&#29575;&#27969;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#26080;&#32422;&#26463;&#30340;&#32467;&#26500;&#24555;&#36895;&#36827;&#34892;&#23569;&#23556;&#25512;&#26029;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23450;&#21046;&#21040;&#20272;&#35745;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;CMPE&#19981;&#20165;&#22312;&#19977;&#20010;&#22256;&#38590;&#30340;&#20302;&#32500;&#38382;&#39064;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#32780;&#19988;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#21435;&#22122;&#23454;&#39564;&#21644;&#20272;&#35745;&#35745;&#31639;&#23494;&#38598;&#22411;&#22810;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05440v2 Announce Type: replace-cross  Abstract: Simulation-based inference (SBI) is constantly in search of more expressive algorithms for accurately inferring the parameters of complex models from noisy data. We present consistency models for neural posterior estimation (CMPE), a new free-form conditional sampler for scalable, fast, and amortized SBI with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem. Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on three hard low-dimensional problems but also achieves competitive performance in a high-dimensional Bayesian denoising experiment and in estimating a computationally demanding multi-scale 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#21333;&#26632;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;SVR&#26500;&#24314;&#20026;&#21333;&#26632;&#36816;&#21160;&#20272;&#35745;&#20219;&#21153;&#65292;&#35757;&#32451;&#20840;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#22312;&#26497;&#31471;&#36816;&#21160;&#24773;&#20917;&#19979;&#30340;&#26368;&#20808;&#36827;3D&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2312.03102</link><description>&lt;p&gt;
&#21333;&#26632;MRI&#30340;&#20840;&#21367;&#31215;&#20999;&#29255;&#21040;&#20307;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#21333;&#26632;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;SVR&#26500;&#24314;&#20026;&#21333;&#26632;&#36816;&#21160;&#20272;&#35745;&#20219;&#21153;&#65292;&#35757;&#32451;&#20840;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#22312;&#26497;&#31471;&#36816;&#21160;&#24773;&#20917;&#19979;&#30340;&#26368;&#20808;&#36827;3D&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#65292;&#20999;&#29255;&#21040;&#20307;&#37325;&#24314;&#65288;SVR&#65289;&#25351;&#30340;&#26159;&#20174;&#21463;&#36816;&#21160;&#24178;&#25200;&#30340;2D&#20999;&#29255;&#22534;&#21472;&#20013;&#23545;&#26410;&#30693;&#30340;3D&#30913;&#20849;&#25391;&#20307;&#31215;&#36827;&#34892;&#35745;&#31639;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SVR&#26041;&#27861;&#65292;&#23427;&#20811;&#26381;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#32570;&#28857;&#65292;&#22312;&#26497;&#31471;&#20999;&#29255;&#38388;&#36816;&#21160;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#12290;&#21463;&#21333;&#35270;&#22270;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;SVR&#26500;&#24314;&#20026;&#21333;&#26632;&#36816;&#21160;&#20272;&#35745;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#20840;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#32473;&#23450;&#20999;&#29255;&#26632;&#30340;&#36816;&#21160;&#26632;&#65292;&#20174;&#32780;&#20135;&#29983;3D&#37325;&#24314;&#20316;&#20026;&#39044;&#27979;&#36816;&#21160;&#30340;&#21103;&#20135;&#21697;&#12290;&#23545;&#25104;&#20154;&#21644;&#32974;&#20799;&#22823;&#33041;&#30340;SVR&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03102v2 Announce Type: replace-cross  Abstract: In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR) refers to computational reconstruction of an unknown 3D magnetic resonance volume from stacks of 2D slices corrupted by motion. While promising, current SVR methods require multiple slice stacks for accurate 3D reconstruction, leading to long scans and limiting their use in time-sensitive applications such as fetal fMRI. Here, we propose a SVR method that overcomes the shortcomings of previous work and produces state-of-the-art reconstructions in the presence of extreme inter-slice motion. Inspired by the recent success of single-view depth estimation methods, we formulate SVR as a single-stack motion estimation task and train a fully convolutional network to predict a motion stack for a given slice stack, producing a 3D reconstruction as a byproduct of the predicted motion. Extensive experiments on the SVR of adult and fetal brains demonstrate that our f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#65292;&#36890;&#36807;&#37319;&#29992;CART&#31639;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.02959</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#26816;&#27979;&#31639;&#27861;&#20559;&#20506;
&lt;/p&gt;
&lt;p&gt;
Detecting algorithmic bias in medical AI-models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#65292;&#36890;&#36807;&#37319;&#29992;CART&#31639;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26085;&#30410;&#26222;&#21450;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#20197;&#20844;&#24179;&#12289;&#20844;&#27491;&#30340;&#26041;&#24335;&#25552;&#20379;&#24739;&#32773;&#32467;&#26524;&#21464;&#24471;&#21516;&#26679;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20998;&#31867;&#19982;&#22238;&#24402;&#26641;&#65288;CART&#65289;&#31639;&#27861;&#65292;&#22312;&#33043;&#27602;&#30151;&#39044;&#27979;&#32972;&#26223;&#19979;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#20559;&#20506;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20351;&#29992;&#20122;&#29305;&#20848;&#22823;&#20052;&#27835;&#20122;&#24030;&#26684;&#38647;&#36842;&#32426;&#24565;&#21307;&#38498;&#30340;&#30005;&#23376;&#30149;&#21382;&#36827;&#34892;&#23454;&#39564;&#36827;&#19968;&#27493;&#24471;&#21040;&#39564;&#35777;&#12290;&#36825;&#20123;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#31574;&#30053;&#22312;&#20020;&#24202;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02959v3 Announce Type: replace-cross  Abstract: With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clini
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Adversarial In-Context Learning (adv-ICL)&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#20462;&#25913;&#22120;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2312.02614</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompt Optimization via Adversarial In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Adversarial In-Context Learning (adv-ICL)&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#20462;&#25913;&#22120;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Adversarial In-Context Learning&#65288;adv-ICL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;LLM&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#21478;&#19968;&#20010;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#31532;&#19977;&#20010;&#20316;&#20026;&#25552;&#31034;&#20462;&#25913;&#22120;&#65292;&#26469;&#20248;&#21270;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25552;&#31034;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#23398;&#20064;&#65292;adv-ICL&#34987;&#23454;&#29616;&#20026;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#20043;&#38388;&#30340;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#35797;&#22270;&#29983;&#25104;&#36275;&#22815;&#36924;&#30495;&#30340;&#36755;&#20986;&#20197;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290; &#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#32473;&#23450;&#30001;&#20219;&#21153;&#35828;&#26126;&#21069;&#32512;&#21644;&#20960;&#20010;&#31034;&#20363;&#32452;&#25104;&#30340;&#36755;&#20837;&#65292;&#29983;&#25104;&#22120;&#20135;&#29983;&#19968;&#20010;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#37492;&#21035;&#22120;&#36127;&#36131;&#23558;&#29983;&#25104;&#22120;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20998;&#31867;&#20026;&#27169;&#22411;&#29983;&#25104;&#30340;&#36824;&#26159;&#30495;&#23454;&#25968;&#25454;&#12290;&#26681;&#25454;&#37492;&#21035;&#22120;&#25439;&#22833;&#65292;&#25552;&#31034;&#20462;&#25913;&#22120;&#25552;&#20986;&#20102;&#21487;&#33021;&#23545;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#25552;&#31034;&#36827;&#34892;&#30340;&#32534;&#36753;&#65292;&#24182;&#36873;&#25321;&#26368;&#22823;&#31243;&#24230;&#25913;&#21892;&#23545;&#25239;&#25439;&#22833;&#30340;&#32534;&#36753;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;adv-ICL&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#20248;&#21270;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02614v2 Announce Type: replace-cross  Abstract: We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optim
&lt;/p&gt;</description></item><item><title>TFMQ-DM&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.16503</link><description>&lt;p&gt;
TFMQ-DM&#65306;&#38754;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#29305;&#24449;&#32500;&#25345;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16503
&lt;/p&gt;
&lt;p&gt;
TFMQ-DM&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16503v2 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20294;&#30001;&#20110;&#20854;&#36739;&#38271;&#30340;&#25512;&#29702;&#26102;&#38388;&#21644;&#22823;&#37327;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#22312;&#24191;&#27867;&#36866;&#29992;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;&#25193;&#25955;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#26102;&#38388;&#27493;&#38271; $t$ &#26469;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#22810;&#36718;&#21435;&#22122;&#12290;&#36890;&#24120;&#65292;&#20174;&#26377;&#38480;&#38598;&#21512; $\{1, \ldots, T\}$ &#20013;&#30340; $t$&#20250;&#34987;&#20960;&#20010;&#27169;&#22359;&#32534;&#30721;&#20026;&#19968;&#20010;&#26102;&#38388;&#29305;&#24449;&#65292;&#36825;&#23436;&#20840;&#19981;&#32771;&#34385;&#37319;&#26679;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#24182;&#19981;&#20998;&#21035;&#20248;&#21270;&#36825;&#20123;&#27169;&#22359;&#12290;&#23427;&#20204;&#37319;&#29992;&#19981;&#24688;&#24403;&#30340;&#37325;&#26500;&#30446;&#26631;&#21644;&#22797;&#26434;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#23548;&#33268;&#26102;&#38388;&#29305;&#24449;&#21644;&#21435;&#22122;&#36712;&#36857;&#20005;&#37325;&#21463;&#21040;&#24178;&#25200;&#65292;&#21516;&#26102;&#21387;&#32553;&#25928;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16503v2 Announce Type: replace-cross  Abstract: The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\{1, \ldots, T\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TF
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32622;&#20449;&#20256;&#25773;&#35299;&#20915;&#35757;&#32451;&#21644;&#39044;&#27979;&#38382;&#39064;&#65292;&#25903;&#25345;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#65292;&#21487;&#25193;&#23637;&#33267;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20379;&#25345;&#32493;&#23398;&#20064;&#30340;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#21435;&#22122;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.14649</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#30340;&#28145;&#24230;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Deep Factor Graphs with Gaussian Belief Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14649
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32622;&#20449;&#20256;&#25773;&#35299;&#20915;&#35757;&#32451;&#21644;&#39044;&#27979;&#38382;&#39064;&#65292;&#25903;&#25345;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#65292;&#21487;&#25193;&#23637;&#33267;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20379;&#25345;&#32493;&#23398;&#20064;&#30340;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#21435;&#22122;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#30456;&#20851;&#25968;&#37327;&#65288;&#36755;&#20837;&#12289;&#36755;&#20986;&#12289;&#21442;&#25968;&#12289;&#28508;&#21464;&#37327;&#65289;&#35270;&#20026;&#22270;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#23558;&#35757;&#32451;&#21644;&#39044;&#27979;&#37117;&#35270;&#20026;&#20855;&#26377;&#19981;&#21516;&#35266;&#23519;&#33410;&#28857;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#26377;&#25928;&#22320;&#35299;&#20915;&#65292;&#20854;&#26356;&#26032;&#26412;&#36136;&#19978;&#26159;&#26412;&#22320;&#30340;&#65292;&#20026;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#28145;&#23618;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#24335;&#65306;&#20351;&#29992;&#24403;&#21069;&#20219;&#21153;&#30340;BP&#20272;&#35745;&#21442;&#25968;&#36793;&#38469;&#20316;&#20026;&#19979;&#19968;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#20808;&#39564;&#12290;&#22312;&#35270;&#39057;&#21435;&#22122;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#23398;&#20064;&#21442;&#25968;&#30456;&#23545;&#20110;&#20256;&#32479;&#22240;&#23376;&#22270;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#28145;&#24230;&#22240;&#23376;&#22270;&#22312;&#25345;&#32493;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#40723;&#33310;&#20154;&#24515;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14649v2 Announce Type: replace  Abstract: We propose an approach to do learning in Gaussian factor graphs. We treat all relevant quantities (inputs, outputs, parameters, latents) as random variables in a graphical model, and view both training and prediction as inference problems with different observed nodes. Our experiments show that these problems can be efficiently solved with belief propagation (BP), whose updates are inherently local, presenting exciting opportunities for distributed and asynchronous training. Our approach can be scaled to deep networks and provides a natural means to do continual learning: use the BP-estimated parameter marginals of the current task as parameter priors for the next. On a video denoising task we demonstrate the benefit of learnable parameters over a classical factor graph approach and we show encouraging performance of deep factor graphs for continual image classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#36807;&#25311;&#21512;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#26032;&#39062;&#30340;&#36328;&#25968;&#25454;&#38598;&#39044;&#25991;&#26412;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.14534</link><description>&lt;p&gt;
&#23547;&#25214;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#36827;&#34892;&#39044;&#25991;&#26412;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Finding Foundation Models for Time Series Classification with a PreText Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#36807;&#25311;&#21512;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#26032;&#39062;&#30340;&#36328;&#25968;&#25454;&#38598;&#39044;&#25991;&#26412;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#25506;&#32034;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;-&#29305;&#21035;&#26159;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;-&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#23450;&#20041;&#19968;&#20010;&#20811;&#26381;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;TSC&#22522;&#30784;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290; UCR&#26723;&#26696;&#24211;&#28085;&#30422;&#20174;&#21160;&#20316;&#35782;&#21035;&#21040;&#22522;&#20110;&#24515;&#30005;&#22270;&#30340;&#24515;&#33039;&#30149;&#26816;&#27979;&#31561;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#26159;&#25506;&#32034;&#19981;&#21516;TSC&#22330;&#26223;&#19979;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#20010;&#20027;&#35201;&#31034;&#20363;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#25361;&#25112;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#19968;&#20010;&#36328;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#26032;&#39046;&#22495;&#39044;&#25991;&#26412;&#20219;&#21153;&#12290;&#27492;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#30340;&#26469;&#28304;&#25968;&#25454;&#38598;&#65292;&#30446;&#30340;&#26159;&#21019;&#24314;&#28789;&#27963;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14534v2 Announce Type: replace  Abstract: Over the past decade, Time Series Classification (TSC) has gained an increasing attention. While various methods were explored, deep learning - particularly through Convolutional Neural Networks (CNNs)-stands out as an effective approach. However, due to the limited availability of training data, defining a foundation model for TSC that overcomes the overfitting problem is still a challenging task. The UCR archive, encompassing a wide spectrum of datasets ranging from motion recognition to ECG-based heart disease detection, serves as a prime example for exploring this issue in diverse TSC scenarios. In this paper, we address the overfitting challenge by introducing pre-trained domain foundation models. A key aspect of our methodology is a novel pretext task that spans multiple datasets. This task is designed to identify the originating dataset of each time series sample, with the goal of creating flexible convolution filters that can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffCPS&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21463;&#38480;&#31574;&#30053;&#25628;&#32034;&#38382;&#39064;&#65292;&#36890;&#36807;&#21442;&#25968;&#36817;&#20284;&#33719;&#24471;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;AWR&#26694;&#26550;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26102;&#36935;&#21040;&#30340;&#27010;&#29575;&#23494;&#24230;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.05333</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21463;&#38480;&#31574;&#30053;&#25628;&#32034;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffCPS&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21463;&#38480;&#31574;&#30053;&#25628;&#32034;&#38382;&#39064;&#65292;&#36890;&#36807;&#21442;&#25968;&#36817;&#20284;&#33719;&#24471;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;AWR&#26694;&#26550;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26102;&#36935;&#21040;&#30340;&#27010;&#29575;&#23494;&#24230;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#38480;&#31574;&#30053;&#25628;&#32034;&#65288;Constrained policy search&#65292;CPS&#65289;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#36890;&#24120;&#36890;&#36807;&#20248;&#21183;&#21152;&#26435;&#22238;&#24402;&#65288;AWR&#65289;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;&#39640;&#26031;&#31574;&#30053;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#20250;&#36935;&#21040;&#30001;&#20110;&#39640;&#26031;&#31574;&#30053;&#38480;&#21046;&#32780;&#23548;&#33268;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#21160;&#20316;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;AWR&#26694;&#26550;&#20013;&#30452;&#25509;&#24212;&#29992;&#20855;&#26377;&#20998;&#24067;&#34920;&#36798;&#33021;&#21147;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;&#21363;&#25193;&#25955;&#27169;&#22411;&#65289;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;AWR&#38656;&#35201;&#30830;&#20999;&#30340;&#31574;&#30053;&#27010;&#29575;&#23494;&#24230;&#65292;&#32780;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;DiffCPS&#65288;Diffusion-based Constrained Policy Search&#65289;&#65292;&#36890;&#36807;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#25193;&#25955;&#30340;&#21463;&#38480;&#31574;&#30053;&#25628;&#32034;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22522;&#30784;&#30340;CPS&#38382;&#39064;&#23384;&#22312;&#24378;&#23545;&#20598;&#24615;&#65292;&#24341;&#20837;&#21442;&#25968;&#36817;&#20284;&#21518;&#65292;&#21487;&#20197;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05333v2 Announce Type: replace  Abstract: Constrained policy search (CPS) is a fundamental problem in offline reinforcement learning, which is generally solved by advantage weighted regression (AWR). However, previous methods may still encounter out-of-distribution actions due to the limited expressivity of Gaussian-based policies. On the other hand, directly applying the state-of-the-art models with distribution expression capabilities (i.e., diffusion models) in the AWR framework is intractable since AWR requires exact policy probability densities, which is intractable in diffusion models. In this paper, we propose a novel approach, $\textbf{Diffusion-based Constrained Policy Search}$ (dubbed DiffCPS), which tackles the diffusion-based constrained policy search with the primal-dual method. The theoretical analysis reveals that strong duality holds for diffusion-based CPS problems, and upon introducing parameter approximation, an approximated solution can be obtained after 
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#25351;&#26631;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#21306;&#38388;&#20256;&#25773;&#36793;&#30028;&#65288;IBP&#65289;&#35757;&#32451;&#25104;&#21151;&#30340;&#26426;&#21046;&#12290;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#28145;&#24230;&#32447;&#24615;&#27169;&#22411;&#65292;IBP&#35757;&#32451;&#33021;&#22815;&#22312;&#36275;&#22815;&#23485;&#24230;&#30340;&#26465;&#20214;&#19979;&#25913;&#21892;&#36793;&#30028;&#32039;&#23494;&#24230;&#12290;</title><link>https://arxiv.org/abs/2306.10426</link><description>&lt;p&gt;
&#20102;&#35299;&#20351;&#29992;&#21306;&#38388;&#20256;&#25773;&#36793;&#30028;&#36827;&#34892;&#35748;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Understanding Certified Training with Interval Bound Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#25351;&#26631;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#21306;&#38388;&#20256;&#25773;&#36793;&#30028;&#65288;IBP&#65289;&#35757;&#32451;&#25104;&#21151;&#30340;&#26426;&#21046;&#12290;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#28145;&#24230;&#32447;&#24615;&#27169;&#22411;&#65292;IBP&#35757;&#32451;&#33021;&#22815;&#22312;&#36275;&#22815;&#23485;&#24230;&#30340;&#26465;&#20214;&#19979;&#25913;&#21892;&#36793;&#30028;&#32039;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#40065;&#26834;&#24615;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#26356;&#21152;&#31934;&#30830;&#65292;&#23545;&#35757;&#32451;&#20855;&#26377;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#35745;&#31639;&#24182;&#20248;&#21270;&#20102;&#23545;&#40065;&#26834;&#24615;&#35268;&#33539;&#19979;&#26368;&#22351;&#24773;&#20917;&#25439;&#22833;&#30340;&#19978;&#30028;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22522;&#20110;&#19981;&#31934;&#30830;&#30340;&#21306;&#38388;&#20256;&#25773;&#36793;&#30028;&#65288;IBP&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#19968;&#30452;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#21033;&#29992;&#26356;&#31934;&#30830;&#36793;&#30028;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#20351;IBP&#22914;&#27492;&#25104;&#21151;&#30340;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#34913;&#37327;IBP&#36793;&#30028;&#32039;&#23494;&#24230;&#30340;&#26032;&#39062;&#25351;&#26631;&#65292;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#26426;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#28145;&#24230;&#32447;&#24615;&#27169;&#22411;&#65292;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32039;&#23494;&#24230;&#38543;&#30528;&#23485;&#24230;&#21644;&#28145;&#24230;&#20943;&#23567;&#65292;&#20294;&#22312;&#32473;&#23450;&#36275;&#22815;&#32593;&#32476;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;IBP&#35757;&#32451;&#20250;&#24471;&#21040;&#25913;&#36827;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;IBP&#36793;&#30028;&#21464;&#24471;&#31934;&#30830;&#30340;&#26435;&#37325;&#30697;&#38453;&#30340;&#20805;&#20998;&#21644;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20102;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10426v2 Announce Type: replace-cross  Abstract: As robustness verification methods are becoming more precise, training certifiably robust neural networks is becoming ever more relevant. To this end, certified training methods compute and then optimize an upper bound on the worst-case loss over a robustness specification. Curiously, training methods based on the imprecise interval bound propagation (IBP) consistently outperform those leveraging more precise bounding methods. Still, we lack an understanding of the mechanisms making IBP so successful.   In this work, we thoroughly investigate these mechanisms by leveraging a novel metric measuring the tightness of IBP bounds. We first show theoretically that, for deep linear models, tightness decreases with width and depth at initialization, but improves with IBP training, given sufficient network width. We, then, derive sufficient and necessary conditions on weight matrices for IBP bounds to become exact and demonstrate that t
&lt;/p&gt;</description></item><item><title>&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#20855;&#26377;&#19968;&#23450;&#27010;&#24565;&#20248;&#21183;&#65292;&#19981;&#38656;&#35201;&#32531;&#23384;&#36807;&#21435;&#30340;&#28608;&#27963;&#29366;&#24577;&#21644;&#25130;&#26029;&#19978;&#19979;&#25991;&#65292;&#25903;&#25345;&#22312;&#32447;&#23398;&#20064;&#65292;&#22312;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#20013;&#25506;&#32034;&#20102;&#20854;&#23454;&#38469;&#28508;&#21147;&#65292;&#24182;&#22312;DMLab-30&#12289;ProcGen&#21644;Atari-2600&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22312;DMLab&#23384;&#20648;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#20248;&#20110;IMPALA&#21644;R2D2&#22522;&#32447;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#21147;&#65292;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#65292;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#20102;&#26576;&#20123;&#26041;&#38754;</title><link>https://arxiv.org/abs/2305.19044</link><description>&lt;p&gt;
&#25506;&#32034;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#30340;&#28508;&#21147;&#19982;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Exploring the Promise and Limits of Real-Time Recurrent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.19044
&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#20855;&#26377;&#19968;&#23450;&#27010;&#24565;&#20248;&#21183;&#65292;&#19981;&#38656;&#35201;&#32531;&#23384;&#36807;&#21435;&#30340;&#28608;&#27963;&#29366;&#24577;&#21644;&#25130;&#26029;&#19978;&#19979;&#25991;&#65292;&#25903;&#25345;&#22312;&#32447;&#23398;&#20064;&#65292;&#22312;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#20013;&#25506;&#32034;&#20102;&#20854;&#23454;&#38469;&#28508;&#21147;&#65292;&#24182;&#22312;DMLab-30&#12289;ProcGen&#21644;Atari-2600&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22312;DMLab&#23384;&#20648;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#20248;&#20110;IMPALA&#21644;R2D2&#22522;&#32447;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#21147;&#65292;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#65292;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#20102;&#26576;&#20123;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#29992;&#20110;&#24207;&#21015;&#22788;&#29702;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30456;&#27604;&#20110;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#20855;&#26377;&#19968;&#23450;&#30340;&#27010;&#24565;&#20248;&#21183;&#12290;RTRL&#26082;&#19981;&#38656;&#35201;&#32531;&#23384;&#36807;&#21435;&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20063;&#19981;&#38656;&#35201;&#25130;&#26029;&#19978;&#19979;&#25991;&#65292;&#32780;&#19988;&#25903;&#25345;&#22312;&#32447;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;RTRL&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20351;&#20854;&#23454;&#38469;&#24212;&#29992;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20851;&#20110;RTRL&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36817;&#20284;&#29702;&#35770;&#19978;&#65292;&#32780;&#23454;&#39564;&#36890;&#24120;&#23616;&#38480;&#20110;&#35786;&#26029;&#35774;&#32622;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#26356;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;RTRL&#30340;&#23454;&#38469;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32467;&#21512;&#20102;RTRL&#21644;&#31574;&#30053;&#26799;&#24230;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#24182;&#22312;DMLab-30&#12289;ProcGen&#21644;Atari-2600&#29615;&#22659;&#30340;&#20960;&#20010;&#23376;&#38598;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#22312;DMLab&#23384;&#20648;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23569;&#20110;1.2 B&#30340;&#29615;&#22659;&#24103;&#19978;&#35757;&#32451;&#65292;&#19982;&#25110;&#20248;&#20110;&#22312;10 B&#24103;&#19978;&#35757;&#32451;&#30340;&#33879;&#21517;IMPALA&#21644;R2D2&#22522;&#32447;&#12290;&#20026;&#20102;&#25193;&#23637;&#21040;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26576;&#20123;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.19044v2 Announce Type: replace  Abstract: Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity make it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system trained on fewer than 1.2 B environmental frames is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10 B frames. To scale to such challenging tasks, we focus on certain wel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTKs&#65289;&#30340;&#29305;&#24449;&#26469;&#25913;&#21892;&#38544;&#31169;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#26410;&#32463;&#35757;&#32451;&#30340;e-NTK&#29305;&#24449;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2303.01687</link><description>&lt;p&gt;
&#24046;&#20998;&#31169;&#26377;&#31070;&#32463;&#20999;&#21521;&#26680;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Neural Tangent Kernels for Privacy-Preserving Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTKs&#65289;&#30340;&#29305;&#24449;&#26469;&#25913;&#21892;&#38544;&#31169;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#26410;&#32463;&#35757;&#32451;&#30340;e-NTK&#29305;&#24449;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#26159;&#19968;&#31181;&#29305;&#21035;&#26377;&#29992;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#29992;&#20110;&#24046;&#20998;&#31169;&#26377;&#25968;&#25454;&#29983;&#25104;&#65306;&#24403;&#19982;&#26377;&#38480;&#32500;&#29305;&#24449;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#19968;&#27425;&#24635;&#32467;&#21644;&#31169;&#26377;&#21270;&#65292;&#28982;&#21518;&#20877;&#29983;&#25104;&#22120;&#35757;&#32451;&#36807;&#31243;&#20013;&#21453;&#22797;&#20351;&#29992;&#32780;&#26080;&#38656;&#20877;&#27425;&#25439;&#22833;&#38544;&#31169;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#65292;&#20160;&#20040;&#26679;&#30340;&#29305;&#24449;&#23545;&#20110;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#21644;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#26159;&#26377;&#29992;&#30340;&#65292;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#26159;&#21542;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTKs&#65289;&#30340;&#29305;&#24449;&#65292;&#26356;&#20934;&#30830;&#22320;&#35828;&#26159;&#32463;&#39564;NTKs&#65288;e-NTKs&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26410;&#32463;&#35757;&#32451;&#30340;e-NTK&#29305;&#24449;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#20013;&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#25552;&#21462;&#30340;&#29305;&#24449;&#30456;&#24403;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#38544;&#31169;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.01687v2 Announce Type: replace  Abstract: Maximum mean discrepancy (MMD) is a particularly useful distance metric for differentially private data generation: when used with finite-dimensional features it allows us to summarize and privatize the data distribution once, which we can repeatedly use during generator training without further privacy loss. An important question in this framework is, then, what features are useful to distinguish between real and synthetic data distributions, and whether those enable us to generate quality synthetic data. This work considers the using the features of $\textit{neural tangent kernels (NTKs)}$, more precisely $\textit{empirical}$ NTKs (e-NTKs). We find that, perhaps surprisingly, the expressiveness of the untrained e-NTK features is comparable to that of the features taken from pre-trained perceptual features using public data. As a result, our method improves the privacy-accuracy trade-off compared to other state-of-the-art methods, w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36125;&#21494;&#26031;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#26041;&#27861;&#65292;&#22312;&#39640;&#38454;&#12289;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#30340;&#27969;&#25968;&#25454;&#20013;&#65292;&#23454;&#29616;&#20102;&#27969;&#25968;&#25454;&#30340;&#24674;&#22797;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2302.12148</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#36827;&#34892;&#27969;&#25968;&#25454;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Streaming data recovery via Bayesian tensor train decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.12148
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#26041;&#27861;&#65292;&#22312;&#39640;&#38454;&#12289;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#30340;&#27969;&#25968;&#25454;&#20013;&#65292;&#23454;&#29616;&#20102;&#27969;&#25968;&#25454;&#30340;&#24674;&#22797;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24352;&#37327;&#21015;&#36710;(TT)&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39640;&#38454;&#27969;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#32467;&#26500;&#26469;&#24674;&#22797;&#27969;&#25968;&#25454;&#12290;&#20511;&#37492;&#27969;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;TT&#26684;&#24335;&#24341;&#20837;&#36125;&#21494;&#26031;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#20197;&#29992;&#20110;&#27969;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;TT&#26680;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#30001;&#20110;TT&#26684;&#24335;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;(SPTT)&#22312;&#24674;&#22797;&#20855;&#26377;&#39640;&#38454;&#12289;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#29305;&#24615;&#30340;&#27969;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#27969;&#25968;&#25454;&#30340;&#26368;&#26032;&#36125;&#21494;&#26031;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.12148v2 Announce Type: replace  Abstract: In this paper, we study a Bayesian tensor train (TT) decomposition method to recover streaming data by approximating the latent structure in high-order streaming data. Drawing on the streaming variational Bayes method, we introduce the TT format into Bayesian tensor decomposition methods for streaming data, and formulate posteriors of TT cores. Thanks to the Bayesian framework of the TT format, the proposed algorithm (SPTT) excels in recovering streaming data with high-order, incomplete, and noisy properties. The experiments in synthetic and real-world datasets show the accuracy of our method compared to state-of-the-art Bayesian tensor decomposition methods for streaming data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#37327;&#23376;&#23398;&#20064;&#22312;PAC&#21644;agnostic&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#20026;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#20854;&#20182;&#38382;&#39064;&#30340;&#26368;&#20248;&#19979;&#30028;&#22880;&#23450;&#20102;&#21487;&#33021;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2301.02227</link><description>&lt;p&gt;
&#37327;&#23376;&#23398;&#20064;&#30340;&#26368;&#20248;&#19979;&#30028;&#65306;&#20449;&#24687;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal lower bounds for Quantum Learning via Information Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.02227
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#37327;&#23376;&#23398;&#20064;&#22312;PAC&#21644;agnostic&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#20026;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#20854;&#20182;&#38382;&#39064;&#30340;&#26368;&#20248;&#19979;&#30028;&#22880;&#23450;&#20102;&#21487;&#33021;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#26576;&#20123;&#24773;&#26223;&#19979;&#65292;&#20351;&#29992;&#37327;&#23376;&#26679;&#26412;&#27604;&#20351;&#29992;&#32463;&#20856;&#26679;&#26412;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#31867;&#65292;Arunachalam&#21644;de Wolf (JMLR, 2018)&#35777;&#26126;&#20102;&#22312;&#37327;&#23376;PAC&#21644;Agnostic&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#37327;&#23376;&#23398;&#20064;&#32773;&#22312;&#28176;&#36817;&#19978;&#19981;&#27604;&#32463;&#20856;&#23398;&#20064;&#32773;&#26356;&#26377;&#25928;&#29575;&#12290;&#20182;&#20204;&#36890;&#36807;&#37327;&#23376;&#24577;&#35782;&#21035;&#21644;&#20613;&#37324;&#21494;&#20998;&#26512;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#26041;&#27861;&#25512;&#23548;&#20102;&#37327;&#23376;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;PAC&#21644;agnostic&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#19979;&#30028;&#12290;&#35777;&#26126;&#36807;&#31243;&#36739;&#31616;&#21333;&#65292;&#24182;&#19988;&#30456;&#21516;&#30340;&#24605;&#24819;&#21487;&#33021;&#34987;&#29992;&#26469;&#25512;&#23548;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#20013;&#20854;&#20182;&#38382;&#39064;&#30340;&#26368;&#20248;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.02227v3 Announce Type: replace-cross  Abstract: Although a concept class may be learnt more efficiently using quantum samples as compared with classical samples in certain scenarios, Arunachalam and de Wolf (JMLR, 2018) proved that quantum learners are asymptotically no more efficient than classical ones in the quantum PAC and Agnostic learning models. They established lower bounds on sample complexity via quantum state identification and Fourier analysis. In this paper, we derive optimal lower bounds for quantum sample complexity in both the PAC and agnostic models via an information-theoretic approach. The proofs are arguably simpler, and the same ideas can potentially be used to derive optimal bounds for other problems in quantum learning theory.   We then turn to a quantum analogue of the Coupon Collector problem, a classic problem from probability theory also of importance in the study of PAC learning. Arunachalam, Belovs, Childs, Kothari, Rosmanis, and de Wolf (TQC, 20
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#29983;&#25104;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2211.07343</link><description>&lt;p&gt;
&#26367;&#25442;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Replacing Language Model for Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.07343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#29983;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26367;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#65288;TST&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#22238;&#24402;&#22320;&#23558;&#28304;&#21477;&#23376;&#30340;&#27599;&#20010;&#26631;&#35760;&#26367;&#25442;&#20026;&#20855;&#26377;&#31867;&#20284;&#21547;&#20041;&#20294;&#20855;&#26377;&#30446;&#26631;&#39118;&#26684;&#30340;&#25991;&#26412;&#29255;&#27573;&#12290;&#26032;&#30340;&#29255;&#27573;&#26159;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26367;&#25442;&#26631;&#35760;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#36825;&#31181;RLM&#29983;&#25104;&#26041;&#26696;&#27719;&#38598;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24357;&#21512;&#20102;&#21477;&#23376;&#32423;&#21644;&#35789;&#32423;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#26356;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#39118;&#26684;&#65292;&#25105;&#20204;&#22312;RLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#26631;&#35760;&#32423;&#39118;&#26684;&#20869;&#23481;&#35299;&#32544;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;TST&#22522;&#32447;&#30456;&#27604;&#65292;RLM&#22312;&#30495;&#23454;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#22312;https://github.com/Linear95/RLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.07343v2 Announce Type: replace  Abstract: We introduce replacing language model (RLM), a sequence-to-sequence language modeling framework for text style transfer (TST). Our method autoregressively replaces each token of the source sentence with a text span that has a similar meaning but in the target style. The new span is generated via a non-autoregressive masked language model, which can better preserve the local-contextual meaning of the replaced token. This RLM generation scheme gathers the flexibility of autoregressive models and the accuracy of non-autoregressive models, which bridges the gap between sentence-level and word-level style transfer methods. To control the generation style more precisely, we conduct a token-level style-content disentanglement on the hidden representations of RLM. Empirical results on real-world text datasets demonstrate the effectiveness of RLM compared with other TST baselines. The code is at https://github.com/Linear95/RLM.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#27874;&#21160;&#30340;&#30005;&#21147;&#24066;&#22330;&#20013;&#36827;&#34892;&#33021;&#28304;&#20132;&#26131;&#65292;&#24182;&#25104;&#21151;&#23558;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#34701;&#20837;&#35774;&#35745;&#31454;&#20105;&#31639;&#27861;&#65292;&#23454;&#29616;&#19982;&#31163;&#32447;&#31639;&#27861;&#30456;&#31454;&#20105;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2211.06567</link><description>&lt;p&gt;
&#22312;&#32447;&#25628;&#32034;&#19982;&#39044;&#27979;&#65306;Pareto-&#26368;&#20248;&#31639;&#27861;&#21450;&#20854;&#22312;&#33021;&#28304;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Online Search with Predictions: Pareto-optimal Algorithm and its Applications in Energy Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.06567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#27874;&#21160;&#30340;&#30005;&#21147;&#24066;&#22330;&#20013;&#36827;&#34892;&#33021;&#28304;&#20132;&#26131;&#65292;&#24182;&#25104;&#21151;&#23558;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#34701;&#20837;&#35774;&#35745;&#31454;&#20105;&#31639;&#27861;&#65292;&#23454;&#29616;&#19982;&#31163;&#32447;&#31639;&#27861;&#30456;&#31454;&#20105;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#27874;&#21160;&#30340;&#30005;&#21147;&#24066;&#22330;&#20013;&#36827;&#34892;&#33021;&#28304;&#20132;&#26131;&#12290;&#22522;&#26412;&#38382;&#39064;&#26159;&#20197;&#26368;&#39640;&#25910;&#20837;&#65288;&#26368;&#20302;&#25104;&#26412;&#65289;&#20986;&#21806;&#65288;&#25110;&#36141;&#20080;&#65289;$k$&#21333;&#20301;&#33021;&#37327;&#65292;&#38754;&#23545;&#19981;&#30830;&#23450;&#30340;&#26102;&#38388;&#21464;&#21270;&#20215;&#26684;&#65292;&#22312;&#31454;&#20105;&#20998;&#26512;&#25991;&#29486;&#20013;&#65292;&#36825;&#21487;&#20197;&#34987;&#30475;&#20316;&#32463;&#20856;&#30340;&#22312;&#32447;&#25628;&#32034;&#38382;&#39064;&#12290;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#20551;&#35774;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#20570;&#20132;&#26131;&#20915;&#31574;&#26102;&#23545;&#26410;&#26469;&#24066;&#22330;&#20215;&#26684;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#65292;&#24182;&#19988;&#26088;&#22312;&#20445;&#35777;&#23545;&#26368;&#22351;&#24773;&#20917;&#20215;&#26684;&#24207;&#21015;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#33719;&#24471;&#23545;&#26410;&#26469;&#20215;&#26684;&#30340;&#39044;&#27979;&#21464;&#24471;&#26222;&#36941;&#21487;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#34701;&#20837;&#35774;&#35745;&#22312;&#32447;&#25628;&#32034;&#38382;&#39064;&#30340;&#31454;&#20105;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#26159;&#65292;&#24403;&#39044;&#27979;&#20934;&#30830;&#26102;&#65288;&#21363;&#19968;&#33268;&#24615;&#65289;&#65292;&#22312;&#20107;&#21518;&#33021;&#22815;&#23454;&#29616;&#19982;&#31163;&#32447;&#31639;&#27861;&#30456;&#31454;&#20105;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.06567v2 Announce Type: replace  Abstract: This paper develops learning-augmented algorithms for energy trading in volatile electricity markets. The basic problem is to sell (or buy) $k$ units of energy for the highest revenue (lowest cost) over uncertain time-varying prices, which can framed as a classic online search problem in the literature of competitive analysis. State-of-the-art algorithms assume no knowledge about future market prices when they make trading decisions in each time slot, and aim for guaranteeing the performance for the worst-case price sequence. In practice, however, predictions about future prices become commonly available by leveraging machine learning. This paper aims to incorporate machine-learned predictions to design competitive algorithms for online search problems. An important property of our algorithms is that they achieve performances competitive with the offline algorithm in hindsight when the predictions are accurate (i.e., consistency) and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;COVID-19&#26399;&#38388;&#19968;&#38431;BEVs&#30340;&#20986;&#21457;&#26102;&#38388;&#21644;&#36317;&#31163;&#12290;</title><link>https://arxiv.org/abs/2210.16002</link><description>&lt;p&gt;
COVID-19&#26399;&#38388;&#36710;&#36742;&#20351;&#29992;&#39044;&#27979;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Online Learning Models for Vehicle Usage Prediction During COVID-19
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.16002
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;COVID-19&#26399;&#38388;&#19968;&#38431;BEVs&#30340;&#20986;&#21457;&#26102;&#38388;&#21644;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#65292;&#21487;&#25345;&#32493;&#20132;&#36890;&#27491;&#22788;&#20110;&#25345;&#32493;&#36716;&#22411;&#38454;&#27573;&#65292;&#20854;&#20013;&#20851;&#38190;&#37096;&#20998;&#26159;&#20174;&#20869;&#29123;&#26426;&#36710;&#36742;&#36716;&#21521;&#30005;&#27744;&#30005;&#21160;&#36710;&#36742;&#65288;BEVs&#65289;&#12290; BEVs&#22312;&#21487;&#25345;&#32493;&#24615;&#35282;&#24230;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#65292;&#20294;&#35832;&#22914;&#26377;&#38480;&#30340;&#34892;&#39542;&#37324;&#31243;&#21644;&#38271;&#20805;&#30005;&#26102;&#38388;&#31561;&#38382;&#39064;&#20943;&#32531;&#20102;&#20174;&#29123;&#28903;&#21457;&#21160;&#26426;&#21521;&#30005;&#27744;&#30005;&#21160;&#36710;&#36742;&#30340;&#36716;&#21464;&#12290;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#25191;&#34892;&#30005;&#27744;&#28909;&#39044;&#35843;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#30005;&#27744;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26368;&#20339;&#25191;&#34892;&#30005;&#27744;&#28909;&#39044;&#35843;&#26465;&#20214;&#65292;&#38656;&#35201;&#20102;&#35299;&#36710;&#36742;&#30340;&#20351;&#29992;&#27169;&#24335;&#65292;&#21363;&#36710;&#36742;&#23558;&#22914;&#20309;&#20197;&#21450;&#20309;&#26102;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#27599;&#22825;&#31532;&#19968;&#27425;&#34892;&#39542;&#30340;&#20986;&#21457;&#26102;&#38388;&#21644;&#36317;&#31163;&#12290;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#22312;&#25910;&#38598;&#33258;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#19968;&#38431;BEVs&#30340;&#21382;&#21490;&#39550;&#39542;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.16002v2 Announce Type: replace  Abstract: Today, there is an ongoing transition to more sustainable transportation, for which an essential part is the switch from combustion engine vehicles to battery electric vehicles (BEVs). BEVs have many advantages from a sustainability perspective, but issues such as limited driving range and long recharge times slow down the transition from combustion engines. One way to mitigate these issues is by performing battery thermal preconditioning, which increases the energy efficiency of the battery. However, to optimally perform battery thermal preconditioning, the vehicle usage pattern needs to be known, i.e., how and when the vehicle will be used. This study attempts to predict the departure time and distance of the first drive each day using online machine learning models. The online machine learning models are trained and evaluated on historical driving data collected from a fleet of BEVs during the COVID-19 pandemic. Additionally, the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#32422;&#26463;&#31574;&#30053;&#25913;&#36827;&#26041;&#21521;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.07484</link><description>&lt;p&gt;
&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mutual Information Regularized Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.07484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#32422;&#26463;&#31574;&#30053;&#25913;&#36827;&#26041;&#21521;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#24403;&#36229;&#20986;&#20998;&#24067;&#30340;&#21160;&#20316;&#34987;&#26597;&#35810;&#26102;&#20986;&#29616;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#20351;&#24471;&#31574;&#30053;&#25913;&#36827;&#26041;&#21521;&#21463;&#21040;&#22806;&#25512;&#35823;&#24046;&#30340;&#20559;&#32622;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#22312;&#31574;&#30053;&#25913;&#36827;&#25110;&#35780;&#20272;&#36807;&#31243;&#20013;&#20559;&#31163;&#34892;&#20026;&#31574;&#30053;&#30340;&#31574;&#30053;&#25110;&#20215;&#20540;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MISA&#26694;&#26550;&#65292;&#20174;&#25968;&#25454;&#38598;&#20013;&#29366;&#24577;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#30452;&#25509;&#32422;&#26463;&#31574;&#30053;&#25913;&#36827;&#26041;&#21521;&#65292;&#20197;&#24212;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.07484v3 Announce Type: replace-cross  Abstract: The major challenge of offline RL is the distribution shift that appears when out-of-distribution actions are queried, which makes the policy improvement direction biased by extrapolation errors. Most existing methods address this problem by penalizing the policy or value for deviating from the behavior policy during policy improvement or evaluation. In this work, we propose a novel MISA framework to approach offline RL from the perspective of Mutual Information between States and Actions in the dataset by directly constraining the policy improvement direction. MISA constructs lower bounds of mutual information parameterized by the policy and Q-values. We show that optimizing this lower bound is equivalent to maximizing the likelihood of a one-step improved policy on the offline dataset. Hence, we constrain the policy improvement direction to lie in the data manifold. The resulting algorithm simultaneously augments the policy e
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22686;&#24378;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#36890;&#36807;&#38544;&#24335;&#35889;&#27491;&#21017;&#21270;&#23545;&#32447;&#24615;&#27169;&#22411;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2210.05021</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22351;&#19982;&#19985;&#38475;&#38754;&#65306;&#38544;&#24335;&#35889;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.05021
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#36890;&#36807;&#38544;&#24335;&#35889;&#27491;&#21017;&#21270;&#23545;&#32447;&#24615;&#27169;&#22411;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#22686;&#24378;&#24615;&#33021;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20256;&#32479;&#19978;&#35748;&#20026;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#29305;&#23450;&#22686;&#24378;&#65292;&#27604;&#22914;&#24179;&#31227;&#21644;&#32553;&#25918;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#30456;&#21516;&#20998;&#24067;&#29983;&#25104;&#26032;&#30340;&#65288;&#20154;&#24037;&#65289;&#25968;&#25454;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20256;&#32479;&#35266;&#28857;&#19981;&#33021;&#35299;&#37322;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#22686;&#24378;&#30340;&#25104;&#21151;&#65288;&#20363;&#22914;&#38543;&#26426;&#36974;&#25377;&#12289;cutout&#12289;mixup&#65289;&#65292;&#36825;&#20123;&#22686;&#24378;&#22823;&#22823;&#25913;&#21464;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#34920;&#24449;&#19968;&#33324;&#31867;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#23545;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#25928;&#24212;&#30340;&#32452;&#21512;&#65306;a)&#20197;&#35757;&#32451;&#25968;&#25454;&#20026;&#22522;&#30784;&#25805;&#32437;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#30340;&#30456;&#23545;&#27604;&#20363;&#65292;b)&#32479;&#19968;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.05021v3 Announce Type: replace  Abstract: Data augmentation (DA) is a powerful workhorse for bolstering performance in modern machine learning. Specific augmentations like translations and scaling in computer vision are traditionally believed to improve generalization by generating new (artificial) data from the same distribution. However, this traditional viewpoint does not explain the success of prevalent augmentations in modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the training data distribution. In this work, we develop a new theoretical framework to characterize the impact of a general class of DA on underparameterized and overparameterized linear model generalization. Our framework reveals that DA induces implicit spectral regularization through a combination of two distinct effects: a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a training-data-dependent manner, and b) uniformly boosting 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#39030;&#21521;&#19979;&#20449;&#29992;&#20998;&#37197;&#32593;&#32476;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#31639;&#27861;&#65292;&#20197;&#21462;&#20195;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2208.01416</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#39030;&#21521;&#19979;&#30340;&#20449;&#29992;&#20998;&#37197;&#32593;&#32476;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Biologically Plausible Training of Deep Neural Networks Using a Top-down Credit Assignment Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.01416
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#39030;&#21521;&#19979;&#20449;&#29992;&#20998;&#37197;&#32593;&#32476;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#31639;&#27861;&#65292;&#20197;&#21462;&#20195;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;BP&#31639;&#27861;&#30340;&#29983;&#29289;&#19981;&#21487;&#34892;&#24615;&#21487;&#33021;&#20250;&#38480;&#21046;&#26032;DNN&#27169;&#22411;&#30340;&#28436;&#36827;&#12290;&#20026;&#20102;&#25214;&#21040;&#19968;&#31181;&#29983;&#29289;&#23398;&#19978;&#21487;&#34892;&#30340;&#31639;&#27861;&#26469;&#21462;&#20195;BP&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#29983;&#29289;&#22823;&#33041;&#20869;&#22312;&#30340;&#33258;&#39030;&#21521;&#19979;&#26426;&#21046;&#12290;&#34429;&#28982;&#29983;&#29289;&#22823;&#33041;&#20013;&#30340;&#33258;&#39030;&#21521;&#19979;&#36830;&#25509;&#22312;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#33258;&#39030;&#21521;&#19979;&#20449;&#29992;&#20998;&#37197;&#32593;&#32476;&#65288;TDCA-network&#65289;&#26469;&#35757;&#32451;&#33258;&#19979;&#32780;&#19978;&#32593;&#32476;&#30340;&#20004;&#23618;&#35757;&#32451;&#26694;&#26550;&#12290;TDCA-network&#20805;&#24403;&#20102;&#24120;&#35268;&#25439;&#22833;&#20989;&#25968;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#26367;&#20195;&#21697;&#65292;&#21518;&#32773;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#22823;&#33041;&#30340;&#20449;&#29992;&#25193;&#25955;&#26426;&#21046;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;TDCA&#32593;&#32476;&#30340;&#21442;&#25968;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#21152;&#36895;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.01416v2 Announce Type: replace-cross  Abstract: Despite the widespread adoption of Backpropagation algorithm-based Deep Neural Networks, the biological infeasibility of the BP algorithm could potentially limit the evolution of new DNN models. To find a biologically plausible algorithm to replace BP, we focus on the top-down mechanism inherent in the biological brain. Although top-down connections in the biological brain play crucial roles in high-level cognitive functions, their application to neural network learning remains unclear. This study proposes a two-level training framework designed to train a bottom-up network using a Top-Down Credit Assignment Network (TDCA-network). The TDCA-network serves as a substitute for the conventional loss function and the back-propagation algorithm, widely used in neural network training. We further introduce a brain-inspired credit diffusion mechanism, significantly reducing the TDCA-network's parameter complexity, thereby greatly acce
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#24230;&#19979;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#26500;&#24314;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#32570;&#22833;&#25968;&#25454;&#21644;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2107.12365</link><description>&lt;p&gt;
&#20855;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#24322;&#26041;&#24046;PCA&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference for Heteroskedastic PCA with Missing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.12365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#24230;&#19979;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#26500;&#24314;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#32570;&#22833;&#25968;&#25454;&#21644;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#39640;&#32500;&#24230;&#20013;&#26500;&#24314;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#26410;&#34987;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#39640;&#32500;&#24230;&#20013;&#35745;&#31639;&#38750;&#32447;&#24615;/&#38750;&#20984;&#20272;&#35745;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#36825;&#19968;&#25361;&#25112;&#26356;&#21152;&#22797;&#26434;&#65292;&#22240;&#20026;&#32570;&#22833;&#25968;&#25454;&#21644;&#24322;&#26041;&#24046;&#22122;&#22768;&#26222;&#36941;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#23574;&#23792;&#21327;&#26041;&#24046;&#27169;&#22411;&#19979;&#23545;&#20027;&#23376;&#31354;&#38388;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#22522;&#20110;&#19968;&#31181;&#21517;&#20026;HeteroPCA&#30340;&#20272;&#35745;&#37327;&#65288;Zhang&#31561;&#20154;&#65292;2022&#24180;&#65289;&#12290;&#25105;&#20204;&#20026;HeteroPCA&#24320;&#21457;&#20102;&#38750;&#28176;&#36817;&#20998;&#24067;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20445;&#35777;&#26469;&#35745;&#31639;&#20027;&#23376;&#31354;&#38388;&#30340;&#32622;&#20449;&#21306;&#38388;&#20197;&#21450;&#23574;&#23792;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#26465;&#30446;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#25512;&#26029;&#31243;&#24207;&#23436;&#20840;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#24322;&#26041;&#24046;&#38543;&#26426;&#22122;&#22768;&#65292;&#32780;&#26080;&#38656;&#20808;&#39564;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2107.12365v2 Announce Type: replace-cross  Abstract: This paper studies how to construct confidence regions for principal component analysis (PCA) in high dimension, a problem that has been vastly under-explored. While computing measures of uncertainty for nonlinear/nonconvex estimators is in general difficult in high dimension, the challenge is further compounded by the prevalent presence of missing data and heteroskedastic noise. We propose a novel approach to performing valid inference on the principal subspace under a spiked covariance model with missing data, on the basis of an estimator called HeteroPCA (Zhang et al., 2022). We develop non-asymptotic distributional guarantees for HeteroPCA, and demonstrate how these can be invoked to compute both confidence regions for the principal subspace and entrywise confidence intervals for the spiked covariance matrix. Our inference procedures are fully data-driven and adaptive to heteroskedastic random noise, without requiring prior
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20943;&#36731;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#23427;&#33021;&#22815;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#30340;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#19982;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;ELLE&#26356;&#21152;&#39640;&#25928;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#31561;&#22256;&#38590;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.11618</link><description>&lt;p&gt;
&#29992;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#30340;&#39640;&#25928;&#26412;&#22320;&#32447;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient local linearity regularization to overcome catastrophic overfitting. (arXiv:2401.11618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20943;&#36731;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#23427;&#33021;&#22815;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#30340;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#19982;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;ELLE&#26356;&#21152;&#39640;&#25928;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#31561;&#22256;&#38590;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512; (CO) &#23548;&#33268;&#23545;&#25239;&#24615;&#27979;&#35797;&#20934;&#30830;&#29575;&#31361;&#28982;&#19979;&#38477;&#65288;&#29978;&#33267;&#38477;&#33267;0%&#65289;&#12290;&#23545;&#20110;&#20351;&#29992;&#22810;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24050;&#35266;&#23519;&#21040;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#20294;&#36825;&#31181;&#29305;&#24615;&#22312;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;CO&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#24378;&#21046;&#25439;&#22833;&#20989;&#25968;&#23616;&#37096;&#32447;&#24615;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21452;&#37325;&#21453;&#21521;&#20256;&#25773;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#39033;&#20250;&#26174;&#33879;&#20943;&#24930;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#22312;&#32463;&#20856;&#23545;&#25239;&#24615;&#35757;&#32451;&#35780;&#20272;&#20013;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#20943;&#36731;CO&#38382;&#39064;&#65292;&#22312;&#19968;&#20123;&#26356;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36215;&#20316;&#29992;&#65292;&#20363;&#22914;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#39033;&#22312;&#29702;&#35770;&#19978;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#26354;&#29575;&#26377;&#32852;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#36991;&#20813;&#21452;&#37325;&#21453;&#21521;&#20256;&#25773;&#32780;&#20855;&#26377;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#23454;&#39564;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Catastrophic overfitting (CO) in single-step adversarial training (AT) results in abrupt drops in the adversarial test accuracy (even down to 0%). For models trained with multi-step AT, it has been observed that the loss function behaves locally linearly with respect to the input, this is however lost in single-step AT. To address CO in single-step AT, several methods have been proposed to enforce local linearity of the loss via regularization. However, these regularization terms considerably slow down training due to Double Backpropagation. Instead, in this work, we introduce a regularization term, called ELLE, to mitigate CO effectively and efficiently in classical AT evaluations, as well as some more difficult regimes, e.g., large adversarial perturbations and long training schedules. Our regularization term can be theoretically linked to curvature of the loss function and is computationally cheaper than previous methods by avoiding Double Backpropagation. Our thorough experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;</title><link>http://arxiv.org/abs/2401.08876</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#23427;&#20204;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#34920;&#31034;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#19968;&#39033;&#22823;&#22411;&#39044;&#27880;&#20876;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#21644;&#26174;&#31034;Top-1&#21644;Top-k&#39044;&#27979;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#31616;&#21333;&#30340;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#30340;&#20934;&#30830;&#24615;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30456;&#24403;&#25110;&#31245;&#20302;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22270;&#20687;&#26102;&#65292;&#23588;&#20854;&#26159;&#24403;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#65292;&#39044;&#27979;&#38598;&#22312;&#36741;&#21161;&#20154;&#31867;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#23454;&#36341;&#20013;&#24378;&#35843;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#39044;&#27979;&#31639;&#27861;&#65292;&#21033;&#29992;&#32447;&#22270;&#36716;&#25442;&#36793;&#32536;&#20449;&#21495;&#20026;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#65292;&#20351;&#24471;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#26377;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2311.00656</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#22270;&#36716;&#25442;&#22312;&#32447;&#20272;&#35745;&#22270;&#36793;&#32536;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Online Signal Estimation on the Graph Edges via Line Graph Transformation. (arXiv:2311.00656v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#39044;&#27979;&#31639;&#27861;&#65292;&#21033;&#29992;&#32447;&#22270;&#36716;&#25442;&#36793;&#32536;&#20449;&#21495;&#20026;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#65292;&#20351;&#24471;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#26377;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#22270;&#24402;&#19968;&#21270;&#26368;&#23567;&#22343;&#26041;(LGNLMS)&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#30340;&#39044;&#27979;&#12290;LGNLMS&#21033;&#29992;&#32447;&#22270;&#23558;&#22270;&#36793;&#32536;&#20449;&#21495;&#36716;&#25442;&#20026;&#20854;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#12290;&#36825;&#20351;&#24471;&#36793;&#32536;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26080;&#38656;&#22312;&#22270;&#36793;&#32536;&#19978;&#37325;&#26032;&#23450;&#20041;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Line Graph Normalized Least Mean Square (LGNLMS) algorithm for online time-varying graph edge signals prediction. LGNLMS utilizes the Line Graph to transform graph edge signals into the node of its edge-to-vertex dual. This enables edge signals to be processed using established GSP concepts without redefining them on graph edges.
&lt;/p&gt;</description></item><item><title>FedPEAT&#26159;&#23558;&#36741;&#21161;&#35843;&#20248;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#27169;&#22411;&#38544;&#31169;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17491</link><description>&lt;p&gt;
FedPEAT: &#32852;&#37030;&#23398;&#20064;&#12289;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#19982;&#36741;&#21161;&#35843;&#20248;&#22312;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing. (arXiv:2310.17491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17491
&lt;/p&gt;
&lt;p&gt;
FedPEAT&#26159;&#23558;&#36741;&#21161;&#35843;&#20248;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#27169;&#22411;&#38544;&#31169;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#65292;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#39046;&#22495;&#65292;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#33021;&#21147;&#12290;&#37096;&#32626;&#21644;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;BERT&#65292;&#22312;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36741;&#21161;&#35843;&#20248;&#65288;EAT&#65289;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#21442;&#25968;&#39640;&#25928;&#36741;&#21161;&#35843;&#20248;&#65288;PEAT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#32852;&#37030;PEAT&#65288;FedPEAT&#65289;&#12290;FedPEAT&#20351;&#29992;&#36866;&#37197;&#22120;&#12289;&#20223;&#30495;&#22120;&#21644;PEFT&#36827;&#34892;&#32852;&#37030;&#27169;&#22411;&#35843;&#20248;&#65292;&#25552;&#39640;&#27169;&#22411;&#38544;&#31169;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#36866;&#37197;&#22120;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#20223;&#30495;&#22120;&#32473;&#20986;&#21407;&#22987;&#27169;&#22411;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#21516;&#26102;&#35299;&#20915;&#38544;&#31169;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29420;&#29305;&#30340;&#22330;&#26223;&#20013;&#20351;&#29992;FedPEAT&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#21442;&#19982;&#21327;&#20316;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of foundation models, including language and vision models, has reshaped AI's landscape, offering capabilities across various applications. Deploying and fine-tuning these large models, like GPT-3 and BERT, presents challenges, especially in the current foundation model era. We introduce Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning (PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses adapters, emulators, and PEFT for federated model tuning, enhancing model privacy and memory efficiency. Adapters adjust pre-trained models, while emulators give a compact representation of original models, addressing both privacy and efficiency. Adaptable to various neural networks, our approach also uses deep reinforcement learning for hyper-parameter optimization. We tested FedPEAT in a unique scenario with a server participating in collaborative federate
&lt;/p&gt;</description></item><item><title>&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17273</link><description>&lt;p&gt;
&#23558;&#24490;&#29615;&#24341;&#20837;&#20154;&#31867;&#65306;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17273
&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#35768;&#22810;&#20248;&#21270;&#22120;&#19968;&#26679;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#33719;&#24471;&#29992;&#25143;&#20449;&#20219;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#20854;&#19981;&#36879;&#26126;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23581;&#35797;&#24320;&#21457;&#38754;&#21521;&#20154;&#31867;&#30340;&#20248;&#21270;&#22120;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#29992;&#25143;&#30693;&#35782;&#26159;&#26126;&#30830;&#19988;&#26080;&#35823;&#30340;&#65292;&#24182;&#20027;&#35201;&#23558;&#29992;&#25143;&#20316;&#20026;&#20248;&#21270;&#36807;&#31243;&#30340;&#30417;&#30563;&#32773;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#34913;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65292;&#21363;&#25105;&#20204;&#30340;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoExBO&#65289;&#26694;&#26550;&#12290;CoExBO&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#26080;&#32541;&#22320;&#23558;&#20154;&#31867;&#35265;&#35299;&#25972;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#29992;&#25143;&#20351;&#29992;&#20559;&#22909;&#19968;&#33268;&#30340;&#31639;&#27861;&#24314;&#35758;&#12290;CoExBO&#35299;&#37322;&#20854;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20197;&#22521;&#20859;&#20449;&#20219;&#65292;&#20351;&#29992;&#25143;&#26356;&#28165;&#26970;&#22320;&#25484;&#25569;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;CoExBO&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#65292;&#20801;&#35768;&#29992;&#25143;&#29359;&#38169;&#35823;&#65307;&#21363;&#20351;&#22312;&#26497;&#31471;&#23545;&#25239;&#24615;&#24178;&#25200;&#19979;&#65292;&#31639;&#27861;&#20063;&#20250;&#28176;&#36827;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.11730</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#36890;&#36807;&#20803;&#36335;&#24452;&#25551;&#36848;&#20016;&#23500;&#30340;&#35821;&#20041;&#65292;&#24050;&#25104;&#20026;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;HIN&#30340;&#25512;&#33616;&#31995;&#32479;&#25345;&#26377;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#20551;&#35774;&#65292;&#24182;&#36827;&#34892;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#23384;&#20648;&#65292;&#23548;&#33268;&#38598;&#20013;&#24335;HIN&#25512;&#33616;&#26080;&#27861;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;HIN&#20998;&#20026;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#31169;&#26377;HIN&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;&#20849;&#20139;HIN&#12290;&#22312;&#27492;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;HIN&#19978;&#21327;&#20316;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#22522;&#20110;HIN&#30340;&#32852;&#21512;&#25512;&#33616;&#65292;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#20809;&#19979;&#30830;&#23450;&#20102;&#38544;&#31169;&#23450;&#20041;&#65292;&#26088;&#22312;&#20445;&#25252;&#31169;&#26377;HIN&#30340;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#65292;&#20197;&#21450;&#29992;&#25143;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05227</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#38761;&#21629;&#31185;&#23398;&#33539;&#24335;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#36807;&#31243;&#30340;&#27700;&#25991;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05227
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#23545;&#20110;&#35299;&#20915;&#27700;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#21160;&#24577;&#24433;&#21709;&#19979;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#27700;&#25991;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#29420;&#31435;&#30340;&#33539;&#24335;&#23384;&#22312;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#31181;&#35748;&#30693;&#38556;&#30861;&#65292;&#24182;&#38761;&#26032;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#35770;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31038;&#21306;&#65288;PaML&#65289;&#65292;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#29289;&#29702;&#25968;&#25454;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#24863;&#30693;&#28151;&#21512;&#23398;&#20064;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#36825;&#20123;PaML&#26041;&#27861;&#12290;PaML&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20551;&#35774;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15669</link><description>&lt;p&gt;
&#20851;&#20110;&#35745;&#31639;&#32416;&#32544;&#21450;&#20854;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
On Computational Entanglement and Its Interpretation in Adversarial Machine Learning. (arXiv:2309.15669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27450;&#39575;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#22240;&#27492;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22797;&#26434;&#24615;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#65292;&#36890;&#36807;&#32416;&#32544;&#30340;&#27010;&#24565;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23545;&#35745;&#31639;&#32416;&#32544;&#36827;&#34892;&#20102;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#65292;&#31867;&#20284;&#20110;&#37327;&#23376;&#39046;&#22495;&#20013;&#30340;&#32416;&#32544;&#12290;&#36825;&#19968;&#21457;&#29616;&#25361;&#25112;&#20102;&#23545;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples in machine learning has emerged as a focal point of research due to their remarkable ability to deceive models with seemingly inconspicuous input perturbations, potentially resulting in severe consequences. In this study, we embark on a comprehensive exploration of adversarial machine learning models, shedding light on their intrinsic complexity and interpretability. Our investigation reveals intriguing links between machine learning model complexity and Einstein's theory of special relativity, through the concept of entanglement. More specific, we define entanglement computationally and demonstrate that distant feature samples can exhibit strong correlations, akin to entanglement in quantum realm. This revelation challenges conventional perspectives in describing the phenomenon of adversarial transferability observed in contemporary machine learning models. By drawing parallels with the relativistic effects of time dilation and length contraction during computatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;SAM&#21644;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65292;&#20197;&#21450;&#36339;&#36291;&#36830;&#25509;&#34701;&#21512;&#29305;&#24449;&#65292;&#26412;&#26041;&#27861;&#22312;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11015</link><description>&lt;p&gt;
&#36890;&#36807;3D-U-SAM&#32593;&#32476;&#36827;&#34892;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images. (arXiv:2309.11015v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;SAM&#21644;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65292;&#20197;&#21450;&#36339;&#36291;&#36830;&#25509;&#34701;&#21512;&#29305;&#24449;&#65292;&#26412;&#26041;&#27861;&#22312;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29273;&#40831;&#20301;&#32622;&#30340;&#20934;&#30830;&#34920;&#31034;&#22312;&#27835;&#30103;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;3D&#29273;&#40831;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#26631;&#27880;&#30340;3D&#29273;&#40831;&#25968;&#25454;&#38598;&#26159;&#31232;&#32570;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#36825;&#20010;&#20219;&#21153;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#38754;&#20020;&#23567;&#26679;&#26412;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;SAM&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#29992;&#20110;3D&#29273;&#40831;&#22270;&#20687;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#22312;3D&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;2D&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65307;&#20026;&#20102;&#20445;&#30041;&#26356;&#22810;&#32454;&#33410;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36339;&#36291;&#36830;&#25509;&#65292;&#20197;&#21442;&#32771;U-Net&#22312;&#25152;&#26377;&#23618;&#32423;&#19978;&#34701;&#21512;&#29305;&#24449;&#12290;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#12289;&#23545;&#27604;&#23454;&#39564;&#21644;&#26679;&#26412;&#22823;&#23567;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate representation of tooth position is extremely important in treatment. 3D dental image segmentation is a widely used method, however labelled 3D dental datasets are a scarce resource, leading to the problem of small samples that this task faces in many cases. To this end, we address this problem with a pretrained SAM and propose a novel 3D-U-SAM network for 3D dental image segmentation. Specifically, in order to solve the problem of using 2D pre-trained weights on 3D datasets, we adopted a convolution approximation method; in order to retain more details, we designed skip connections to fuse features at all levels with reference to U-Net. The effectiveness of the proposed method is demonstrated in ablation experiments, comparison experiments, and sample size experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;</title><link>http://arxiv.org/abs/2309.10688</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#21516;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10688
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20851;&#38190;&#21442;&#25968;&#26159;&#27599;&#20010;&#27493;&#39588;&#32771;&#34385;&#30340;&#25968;&#25454;&#37327;&#25110;&#25209;&#37327;&#22823;&#23567;B&#20197;&#21450;&#27493;&#38271;&#25110;&#23398;&#20064;&#29575;&#951;&#12290;&#23545;&#20110;&#23567;&#30340;B&#21644;&#22823;&#30340;&#951;&#65292;SGD&#23545;&#24212;&#20110;&#21442;&#25968;&#30340;&#38543;&#26426;&#28436;&#21270;&#65292;&#20854;&#22122;&#22768;&#24133;&#24230;&#30001;&#8220;&#28201;&#24230;&#8221;T=&#951;/B&#25511;&#21046;&#12290;&#28982;&#32780;&#24403;&#25209;&#37327;&#22823;&#23567;B&#8805;B^*&#36275;&#22815;&#22823;&#26102;&#65292;&#36825;&#31181;&#25551;&#36848;&#34987;&#35266;&#23519;&#21040;&#22833;&#25928;&#65292;&#25110;&#32773;&#22312;&#28201;&#24230;&#36275;&#22815;&#23567;&#26102;&#31616;&#21270;&#20026;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#21449;&#21457;&#29983;&#30340;&#20301;&#32622;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#24863;&#30693;&#22120;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20851;&#38190;&#39044;&#27979;&#20173;&#36866;&#29992;&#20110;&#28145;&#24230;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;B-&#951;&#24179;&#38754;&#19978;&#33719;&#24471;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#65292;&#23558;&#19977;&#20010;&#21160;&#24577;&#38454;&#27573;&#20998;&#24320;&#65306;&#65288;i&#65289;&#21463;&#28201;&#24230;&#25511;&#21046;&#30340;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#65292;&#65288;ii&#65289;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#21644;
&lt;/p&gt;
&lt;p&gt;
Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16572</link><description>&lt;p&gt;
CL-MAE: &#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#29992;&#20110;&#29983;&#25104;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#36890;&#24120;&#65292;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#38543;&#26426;&#36974;&#32617;&#34917;&#19969;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19988;&#36974;&#32617;&#31574;&#30053;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#20197;&#25345;&#32493;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#36974;&#32617;&#27169;&#22359;&#65292;&#20855;&#26377;&#29983;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#36974;&#32617;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27169;&#22359;&#19982;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#19982;MAE&#19968;&#21516;&#35757;&#32451;&#65292;&#21516;&#26102;&#35843;&#25972;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;MAE&#30340;&#21442;&#19982;&#32773;&#36807;&#28193;&#21040;MAE&#65288;&#20248;&#21270;&#30456;&#21516;&#30340;&#37325;&#26500;&#30446;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;REINFORCE&#31639;&#27861;&#30340;&#24402;&#19968;&#21270;&#27969;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#22312;&#20108;&#32500;Schwinger&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22681;&#26102;&#38047;&#26102;&#38388;&#19978;&#24555;10&#20493;&#65292;&#19988;&#20869;&#23384;&#20351;&#29992;&#19978;&#33410;&#30465;30%&#12290;</title><link>http://arxiv.org/abs/2308.13294</link><description>&lt;p&gt;
&#35757;&#32451;&#20855;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Training normalizing flows with computationally intensive target probability distributions. (arXiv:2308.13294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;REINFORCE&#31639;&#27861;&#30340;&#24402;&#19968;&#21270;&#27969;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#22312;&#20108;&#32500;Schwinger&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22681;&#26102;&#38047;&#26102;&#38388;&#19978;&#24555;10&#20493;&#65292;&#19988;&#20869;&#23384;&#20351;&#29992;&#19978;&#33410;&#30465;30%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#25152;&#35859;&#30340;&#24402;&#19968;&#21270;&#27969;&#65292;&#22312;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#26684;&#28857;&#22330;&#35770;&#65288;LFT&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#20998;&#24067;&#30001;&#20316;&#29992;&#30340;&#25351;&#25968;&#32473;&#20986;&#12290;&#22522;&#20110;&#8220;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#8221;&#30340;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#38656;&#35201;&#23545;&#22330;&#30340;&#23548;&#25968;&#36827;&#34892;&#35745;&#31639;&#12290;&#23545;&#20110;&#22797;&#26434;&#30340;&#38750;&#23616;&#37096;&#20316;&#29992;&#65292;&#22914;QCD&#20013;&#30340;&#36153;&#31859;&#23376;&#20316;&#29992;&#65292;&#36825;&#21487;&#33021;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;REINFORCE&#31639;&#27861;&#30340;&#24402;&#19968;&#21270;&#27969;&#20272;&#35745;&#22120;&#65292;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20851;&#38190;&#24615;&#30340;&#20108;&#32500;Schwinger&#27169;&#22411;&#19982;Wilson&#36153;&#31859;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#20272;&#35745;&#22120;&#22312;&#22681;&#26102;&#38047;&#26102;&#38388;&#19978;&#24555;10&#20493;&#20197;&#21450;&#22312;&#20869;&#23384;&#20351;&#29992;&#19978;&#33410;&#30465;30%&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques, in particular the so-called normalizing flows, are becoming increasingly popular in the context of Monte Carlo simulations as they can effectively approximate target probability distributions. In the case of lattice field theories (LFT) the target distribution is given by the exponential of the action. The common loss function's gradient estimator based on the "reparametrization trick" requires the calculation of the derivative of the action with respect to the fields. This can present a significant computational cost for complicated, non-local actions like e.g. fermionic action in QCD. In this contribution, we propose an estimator for normalizing flows based on the REINFORCE algorithm that avoids this issue. We apply it to two dimensional Schwinger model with Wilson fermions at criticality and show that it is up to ten times faster in terms of the wall-clock time as well as requiring up to $30\%$ less memory than the reparameterization trick estimator. It 
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#20851;&#32852;&#25968;&#25454;&#21644;&#24178;&#20928;&#26631;&#31614;&#65292;&#36890;&#36807;&#20351;&#29992;&#21028;&#21035;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#38544;&#24335;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20844;&#24335;&#12289;&#38590;&#20197;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#26080;&#20449;&#24687;&#20808;&#39564;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01184</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#26631;&#31614;&#20808;&#39564;&#30340;&#38544;&#24335;&#21028;&#21035;&#36924;&#36817;&#36827;&#34892;&#29983;&#25104;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior. (arXiv:2308.01184v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#20851;&#32852;&#25968;&#25454;&#21644;&#24178;&#20928;&#26631;&#31614;&#65292;&#36890;&#36807;&#20351;&#29992;&#21028;&#21035;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#38544;&#24335;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20844;&#24335;&#12289;&#38590;&#20197;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#26080;&#20449;&#24687;&#20808;&#39564;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#12290;&#23613;&#31649;&#21028;&#21035;&#27169;&#22411;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#24314;&#27169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#35757;&#32451;&#36807;&#31243;&#32780;&#22312;&#35813;&#39046;&#22495;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20998;&#35299;&#24178;&#20928;&#21644;&#22122;&#22768;&#26631;&#31614;&#65292;&#24182;&#25913;&#21892;&#26631;&#31614;&#36716;&#25442;&#30697;&#38453;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#26041;&#27861;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#20844;&#24335;&#26469;&#26368;&#22823;&#21270;&#22122;&#22768;&#26631;&#31614;&#21644;&#25968;&#25454;&#30340;&#32852;&#21512;&#20284;&#28982;&#65292;&#36825;&#21482;&#38388;&#25509;&#20248;&#21270;&#20102;&#19982;&#25968;&#25454;&#21644;&#24178;&#20928;&#26631;&#31614;&#30456;&#20851;&#30340;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#24456;&#38590;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20542;&#21521;&#20110;&#20351;&#29992;&#26080;&#20449;&#24687;&#30340;&#24178;&#20928;&#26631;&#31614;&#20808;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19977;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#30452;&#25509;&#20851;&#32852;&#25968;&#25454;&#21644;&#24178;&#20928;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#20351;&#29992;&#21028;&#21035;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#38544;&#24335;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The learning with noisy labels has been addressed with both discriminative and generative models. Although discriminative models have dominated the field due to their simpler modeling and more efficient computational training processes, generative models offer a more effective means of disentangling clean and noisy labels and improving the estimation of the label transition matrix. However, generative approaches maximize the joint likelihood of noisy labels and data using a complex formulation that only indirectly optimizes the model of interest associating data and clean labels. Additionally, these approaches rely on generative models that are challenging to train and tend to use uninformative clean label priors. In this paper, we propose a new generative noisy-label learning approach that addresses these three issues. First, we propose a new model optimisation that directly associates data and clean labels. Second, the generative model is implicitly estimated using a discriminative m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NCART&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#22810;&#20010;&#21487;&#24494;&#24615;&#20915;&#31574;&#26641;&#26367;&#20195;&#20840;&#36830;&#25509;&#23618;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#25110;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.12198</link><description>&lt;p&gt;
NCART: &#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#31070;&#32463;&#20998;&#31867;&#19982;&#22238;&#24402;&#26641;
&lt;/p&gt;
&lt;p&gt;
NCART: Neural Classification and Regression Tree for Tabular Data. (arXiv:2307.12198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12198
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NCART&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#22810;&#20010;&#21487;&#24494;&#24615;&#20915;&#31574;&#26641;&#26367;&#20195;&#20840;&#36830;&#25509;&#23618;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20805;&#20998;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#25110;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#35299;&#20915;&#20102;&#20915;&#31574;&#26641;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26377;&#20215;&#20540;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#36935;&#21040;&#19968;&#20010;&#25240;&#34935;&#12290;&#19968;&#26041;&#38754;&#65292;&#24403;&#22788;&#29702;&#22823;&#35268;&#27169;&#25110;&#39640;&#32500;&#25968;&#25454;&#38598;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#21487;&#33021;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#19981;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#31070;&#32463;&#20998;&#31867;&#19982;&#22238;&#24402;&#26641;&#65288;NCART&#65289;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;NCART&#26159;&#27531;&#24046;&#32593;&#32476;&#30340;&#21464;&#20307;&#65292;&#23427;&#23558;&#20840;&#36830;&#25509;&#23618;&#26367;&#25442;&#20026;&#22810;&#20010;&#21487;&#24494;&#24615;&#30340;&#26080;&#35270;&#20915;&#31574;&#26641;&#12290;&#36890;&#36807;&#23558;&#20915;&#31574;&#26641;&#38598;&#25104;&#21040;&#26550;&#26500;&#20013;&#65292;NCART&#20445;&#25345;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;NCART&#26550;&#26500;&#30340;&#31616;&#27905;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally expensive when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.09055</link><description>&lt;p&gt;
&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#29992;&#20110;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#24352;&#37327;&#20998;&#26512;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24352;&#37327;&#25968;&#25454;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#20540;&#25110;&#26679;&#26412;&#29305;&#23450;&#30340;&#27745;&#26579;&#12290;&#22914;&#20309;&#24674;&#22797;&#34987;&#24322;&#24120;&#20540;&#25439;&#22351;&#30340;&#24352;&#37327;&#25968;&#25454;&#24182;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#24352;&#37327;&#25968;&#25454;&#32858;&#31867;&#30340;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#65288;OR-TLRR&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#24341;&#36215;&#30340;&#24352;&#37327;&#24352;&#37327;&#31215;&#30340;&#21551;&#21457;&#12290;&#23545;&#20110;&#24102;&#26377;&#20219;&#24847;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#24352;&#37327;&#35266;&#27979;&#65292;OR-TLRR&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#33021;&#22815;&#30830;&#20999;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#24182;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;OR-TLRR&#30340;&#25193;&#23637;&#26469;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#24418;&#24335;&#29305;&#24449;&#65292;&#23398;&#20064;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.11943</link><description>&lt;p&gt;
&#25506;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding What Code Language Models Learned. (arXiv:2306.11943v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#24418;&#24335;&#29305;&#24449;&#65292;&#23398;&#20064;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26377;&#20154;&#35748;&#20026;&#23427;&#20204;&#30340;&#33021;&#21147;&#19981;&#36275;&#20197;&#23436;&#20840;&#23398;&#20064;&#35821;&#35328;&#30340;&#24847;&#20041;&#25110;&#29702;&#35299;&#35821;&#35328;&#12290;&#20026;&#20102;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26576;&#31181;&#24418;&#24335;&#30340;&#24847;&#20041;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#25429;&#25417;&#20195;&#30721;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#34920;&#23618;&#39057;&#29575;&#21644;&#20849;&#29616;&#30340;&#38480;&#21046;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#27169;&#22411;&#35821;&#35328;&#29305;&#24449;&#30340;&#25506;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19968;&#31181;&#21487;&#20197;&#23458;&#35266;&#22320;&#12289;&#31616;&#21333;&#26126;&#20102;&#22320;&#35780;&#20272;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#33021;&#21147;&#30340;&#29615;&#22659;&#19979;&#30740;&#31350;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#25429;&#25417;&#31934;&#30830;&#32780;&#24418;&#24335;&#21270;&#23450;&#20041;&#30340;&#20195;&#30721;&#30340;&#35821;&#20041;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#29255;&#27573;&#30340;&#25805;&#32437;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#30721;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#20102;&#20195;&#30721;&#30340;&#35745;&#31639;&#35821;&#20041;&#30340;&#24378;&#26377;&#21147;&#30340;&#34920;&#24449;&#65292;&#36229;&#36234;&#20102;&#20195;&#30721;&#34920;&#38754;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models are effective in a variety of natural language tasks, but it has been argued their capabilities fall short of fully learning meaning or understanding language. To understand the extent to which language models can learn some form of meaning, we investigate their ability to capture semantics of code beyond superficial frequency and co-occurrence. In contrast to previous research on probing models for linguistic features, we study pre-trained models in a setting that allows for objective and straightforward evaluation of a model's ability to learn semantics. In this paper, we examine whether such models capture the semantics of code, which is precisely and formally defined. Through experiments involving the manipulation of code fragments, we show that code pre-trained models of code learn a robust representation of the computational semantics of code that goes beyond superficial features of form alone
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN&#26550;&#26500;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;QCNN&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07331</link><description>&lt;p&gt;
&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#35010;&#21644;&#24182;&#34892;&#21270;&#29992;&#20110;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Splitting and Parallelizing of Quantum Convolutional Neural Networks for Learning Translationally Symmetric Data. (arXiv:2306.07331v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN&#26550;&#26500;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;QCNN&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(QCNN)&#26159;&#19968;&#31181;&#26377;&#26395;&#22312;&#32463;&#20856;&#38590;&#39064;&#19978;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;(QML)&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;QCNN&#38656;&#35201;&#22823;&#37327;&#30340;&#27979;&#37327;&#29992;&#20110;&#25968;&#25454;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN(sp-QCNN)&#65292;&#23427;&#21033;&#29992;&#37327;&#23376;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#35774;&#35745;&#39640;&#25928;&#30005;&#36335;&#12290;&#36825;&#31181;&#26550;&#26500;&#20174;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#38024;&#23545;&#20957;&#32858;&#24577;&#29289;&#29702;&#20013;&#24120;&#35265;&#30340;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#12290;&#36890;&#36807;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#20998;&#35010;&#37327;&#23376;&#30005;&#36335;&#65292;sp-QCNN&#26497;&#22823;&#22320;&#24182;&#34892;&#21270;&#20102;&#20256;&#32479;&#30340;QCNN&#65292;&#32780;&#19981;&#22686;&#21152;&#37327;&#23376;&#27604;&#29305;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#37327;&#23376;&#30456;&#35782;&#21035;&#20219;&#21153;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A quantum convolutional neural network (QCNN) is a promising quantum machine learning (QML) model to achieve quantum advantages in classically intractable problems. However, QCNN requires a large number of measurements for data learning, limiting its practical applications for large-scale problems. To relieve this requirement, we propose a novel architecture called split-parallelizing QCNN (sp-QCNN), which exploits the prior knowledge of quantum data for designing efficient circuits. This architecture draws inspiration from geometric quantum machine learning and targets translationally symmetric quantum data commonly encountered in condensed matter physics. By splitting the quantum circuit based on translational symmetry, sp-QCNN substantially parallelizes conventional QCNN without increasing the number of qubits and further improves the measurement efficiency by an order of the number of qubits. To demonstrate its effectiveness, we apply sp-QCNN to a quantum phase recognition task and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#29992;&#20110;&#32570;&#20047;&#22240;&#26524;&#27169;&#22411;&#21644;&#30452;&#25509;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#24182;&#33021;&#25552;&#20379;&#20272;&#35745;&#32467;&#26524;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.05751</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#25512;&#36827;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Advancing Counterfactual Inference through Quantile Regression. (arXiv:2306.05751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#29992;&#20110;&#32570;&#20047;&#22240;&#26524;&#27169;&#22411;&#21644;&#30452;&#25509;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#24182;&#33021;&#25552;&#20379;&#20272;&#35745;&#32467;&#26524;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#21453;&#20107;&#23454;&#8220;&#20551;&#35774;&#8221;&#38382;&#39064;&#30340;&#33021;&#21147;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#22240;&#26524;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#36890;&#24120;&#20551;&#23450;&#23384;&#22312;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#26679;&#30340;&#22240;&#26524;&#27169;&#22411;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#29978;&#33267;&#19981;&#21487;&#36776;&#35782;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#22522;&#20110;&#65288;&#23398;&#20064;&#21040;&#30340;&#65289;&#23450;&#24615;&#22240;&#26524;&#32467;&#26500;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#32473;&#23450;&#22240;&#26524;&#27169;&#22411;&#29978;&#33267;&#19981;&#38656;&#35201;&#30452;&#25509;&#20272;&#35745;&#26465;&#20214;&#20998;&#24067;&#65292;&#23601;&#33021;&#36827;&#34892;&#21487;&#38752;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#21453;&#20107;&#23454;&#25512;&#29702;&#37325;&#26032;&#36716;&#21270;&#20026;&#19968;&#20010;&#25193;&#23637;&#20998;&#20301;&#25968;&#22238;&#24402;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#20351;&#24471;&#20272;&#35745;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#23545;&#26410;&#35265;&#25968;&#25454;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capacity to address counterfactual "what if" inquiries is crucial for understanding and making use of causal influences. Traditional counterfactual inference usually assumes a structural causal model is available. However, in practice, such a causal model is often unknown and may not be identifiable. This paper aims to perform reliable counterfactual inference based on the (learned) qualitative causal structure and observational data, without a given causal model or even directly estimating conditional distributions. We re-cast counterfactual reasoning as an extended quantile regression problem using neural networks. The approach is statistically more efficient than existing ones, and further makes it possible to develop the generalization ability of the estimated counterfactual outcome to unseen data and provide an upper bound on the generalization error. Experiment results on multiple datasets strongly support our theoretical claims.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65292;&#25552;&#39640;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05292</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Safe Collaborative Filtering. (arXiv:2306.05292v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65292;&#25552;&#39640;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#31639;&#27861;&#20844;&#24179;&#24615;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39118;&#38505;&#25935;&#24863;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#20248;&#31168;&#30340;&#23614;&#37096;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#30340;&#26377;&#25928;&#22788;&#29702;&#12290;&#23614;&#37096;&#24615;&#33021;&#20063;&#26159;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#25104;&#21151;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#65292;&#20197;&#20943;&#23569;&#23545;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#27969;&#22833;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#8220;&#23433;&#20840;&#8221;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#20851;&#27880;&#24179;&#22343;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;CVaR&#65289;&#65292;&#34920;&#31034;&#29992;&#25143;&#25439;&#22833;&#23614;&#37096;&#30340;&#24179;&#22343;&#39118;&#38505;&#12290;&#20026;&#20102;&#20811;&#26381;&#32593;&#32476;&#35268;&#27169;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#35745;&#31639;&#38590;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#26368;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#38544;&#24335;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;iALS&#65289;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#23614;&#37096;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. Tail performance is also a vital determinant of success for personalised recommender systems to reduce the risk of losing users with low satisfaction. This study introduces a "safe" collaborative filtering method that prioritises recommendation quality for less-satisfied users rather than focusing on the average performance. Our approach minimises the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss. To overcome computational challenges for web-scale recommender systems, we develop a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS). Empirical evaluation on real-world datasets demonstrates the excellent tail performance of our approach while maint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04366</link><description>&lt;p&gt;
&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20219;&#21153;&#24863;&#30693;&#30340;&#22242;&#38431;&#21512;&#20316;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#32780;&#24037;&#20154;&#25307;&#21215;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#24037;&#20154;&#26412;&#36523;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20219;&#21153;&#25928;&#29992;&#35780;&#20272;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;Mini-Batch K-Means&#32858;&#31867;&#31639;&#27861;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24037;&#20154;&#25307;&#21215;&#12290;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#20219;&#21153;&#35201;&#27714;&#33719;&#24471;&#24037;&#20154;&#30340;&#33021;&#21147;&#31867;&#22411;&#21644;&#36317;&#31163;&#12290;&#20351;&#29992;&#24037;&#20154;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#23548;&#21521;&#22270;&#36755;&#20837;&#33267;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#12290;&#36890;&#36807;&#24037;&#20154;&#20043;&#38388;&#30340;&#39640;&#20449;&#20219;&#20540;&#65292;&#38450;&#27490;CMCS&#22330;&#26223;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26368;&#32456;&#65292;&#21033;&#29992;&#39044;&#27979;&#30340;&#20449;&#20219;&#21644;&#24037;&#20154;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#21521;&#25307;&#21215;&#22270;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25307;&#21215;&#26041;&#27861;&#22312;&#25307;&#21215;&#20934;&#30830;&#24230;&#12289;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.18400</link><description>&lt;p&gt;
&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#65288;TFL&#65289;&#36890;&#24120;&#21033;&#29992;&#20445;&#25252;&#26426;&#21046;&#26469;&#20445;&#35777;&#38544;&#31169;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#26426;&#21046;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24341;&#20837;&#25928;&#29992;&#25439;&#22833;&#25110;&#25928;&#29575;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#26426;&#21046;&#21450;&#20854;&#21442;&#25968;&#24212;&#35813;&#20180;&#32454;&#36873;&#25321;&#65292;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#20174;&#19994;&#32773;&#38656;&#35201;&#24037;&#20855;&#26469;&#34913;&#37327;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#24182;&#20248;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#25163;&#22836;&#24212;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;(1)&#23558;TFL&#23450;&#20041;&#20026;&#25214;&#21040;&#20445;&#25252;&#26426;&#21046;&#26469;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#19977;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38382;&#39064;&#65307;(2)&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#19977;&#20010;&#22240;&#32032;&#30340;&#26377;&#30028;&#27979;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#27492;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#35777;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17224</link><description>&lt;p&gt;
&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#27861;&#24555;&#36895;&#26497;&#23567;&#21270;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast and Minimax Optimal Estimation of Low-Rank Matrices via Non-Convex Gradient Descent. (arXiv:2305.17224v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#35777;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22122;&#22768;&#27979;&#37327;&#20013;&#20272;&#35745;&#20302;&#31209;&#30697;&#38453;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26088;&#22312;&#23454;&#29616;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#30340;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#20351;&#29992;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#26469;&#35299;&#20915;&#12290;&#29702;&#35770;&#19978;&#65292;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#32463;&#24120;&#25910;&#25947;&#24471;&#38750;&#24120;&#32531;&#24930;&#65292;&#20197;&#33267;&#20110;&#29978;&#33267;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#36866;&#24230;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#37325;&#26032;&#32553;&#25918;&#25110;&#39044;&#22788;&#29702;&#25913;&#36827;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#26041;&#27861;&#20063;&#20250;&#22823;&#22823;&#25918;&#22823;&#27979;&#37327;&#35823;&#24046;&#65292;&#23548;&#33268;&#24471;&#21040;&#30340;&#20272;&#35745;&#27604;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#30340;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36890;&#24120;&#30340;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21487;&#35777;&#26126;&#20445;&#30041;&#20854;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating a low-rank matrix from noisy measurements, with the specific goal of achieving minimax optimal error. In practice, the problem is commonly solved using non-convex gradient descent, due to its ability to scale to large-scale real-world datasets. In theory, non-convex gradient descent is capable of achieving minimax error. But in practice, it often converges extremely slowly, such that it cannot even deliver estimations of modest accuracy within reasonable time. On the other hand, methods that improve the convergence of non-convex gradient descent, through rescaling or preconditioning, also greatly amplify the measurement noise, resulting in estimations that are orders of magnitude less accurate than what is theoretically achievable with minimax optimal error. In this paper, we propose a slight modification to the usual non-convex gradient descent method that remedies the issue of slow convergence, while provably preserving its minimax optimality. Our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#21152;&#36895;&#20248;&#21270;&#24182;&#22312;&#24635;&#20307;&#19978;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#26102;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13404</link><description>&lt;p&gt;
&#21033;&#29992;&#21442;&#25968;&#23545;&#31216;&#24615;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Convergence and Generalization Using Parameter Symmetries. (arXiv:2305.13404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#21152;&#36895;&#20248;&#21270;&#24182;&#22312;&#24635;&#20307;&#19978;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#26102;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#65292;&#21442;&#25968;&#30340;&#19981;&#21516;&#20540;&#21487;&#33021;&#23548;&#33268;&#30456;&#21516;&#30340;&#25439;&#22833;&#20540;&#12290;&#21442;&#25968;&#31354;&#38388;&#23545;&#31216;&#24615;&#26159;&#25913;&#21464;&#27169;&#22411;&#21442;&#25968;&#32780;&#20445;&#25345;&#25439;&#22833;&#19981;&#21464;&#30340;&#21464;&#25442;&#12290;&#20256;&#36865;&#24212;&#29992;&#36825;&#26679;&#30340;&#21464;&#25442;&#26469;&#21152;&#36895;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31639;&#27861;&#25104;&#21151;&#30340;&#30830;&#20999;&#26426;&#21046;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#22312;&#30701;&#26399;&#20869;&#21152;&#36895;&#20248;&#21270;&#65292;&#32780;&#19988;&#21487;&#20197;&#20351;&#24635;&#20307;&#25910;&#25947;&#26102;&#38388;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26368;&#23567;&#20540;&#26354;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20256;&#36865;&#38598;&#25104;&#21040;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#20013;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In overparametrized models, different values of the parameters may result in the same loss value. Parameter space symmetries are transformations that change the model parameters but leave the loss invariant. Teleportation applies such transformations to accelerate optimization. However, the exact mechanism behind this algorithm's success is not well understood. In this paper, we show that teleportation not only speeds up optimization in the short-term, but gives overall faster time to convergence. Additionally, we show that teleporting to minima with different curvatures improves generalization and provide insights on the connection between the curvature of the minima and generalization ability. Finally, we show that integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05836</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#33391;&#24615;&#21442;&#19982;&#32773;&#26088;&#22312;&#21327;&#21516;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#21322;&#35802;&#23454;&#30340;&#23545;&#25163;&#26102;&#65292;\textit{&#38544;&#31169;&#27844;&#28431;}&#30340;&#39118;&#38505;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#35774;&#35745;&#20445;&#25252;&#26426;&#21046;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#21457;&#26126;&#25915;&#20987;&#26426;&#21046;&#12290;&#34429;&#28982;&#20445;&#25252;&#32773;&#19982;&#25915;&#20987;&#32773;&#20043;&#38388;&#30340;&#26007;&#20105;&#20284;&#20046;&#27704;&#26080;&#27490;&#22659;&#65292;&#20294;&#25105;&#20204;&#20851;&#24515;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20107;&#20808;&#39044;&#38450;&#28508;&#22312;&#30340;&#25915;&#20987;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;FL&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#30456;&#24212;&#25910;&#30410;&#65292;&#20854;&#20013;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#27492;&#28216;&#25103;&#31216;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#65292;&#22312;&#20854;&#20013;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#37117;&#19981;&#30693;&#36947;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#22266;&#26377;&#30340;\textit{&#19981;&#23436;&#20840;&#20449;&#24687;}&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;FLSG&#19982;&#19968;&#20010;\textit{oracle}&#30456;&#20851;&#32852;&#65292;&#35813;oracle&#20855;&#26377;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#21508;&#31181;&#25928;&#29992;&#20989;&#25968;&#21644;&#25915;&#20987;&#27169;&#22411;&#32452;&#21512;&#19979;FLSG&#30340;&#32435;&#20160;&#22343;&#34913;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#23454;&#39564;&#32467;&#26524;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;FL&#22330;&#26223;&#20013;&#30340;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \textit{privacy leakage} cannot be ignored in the presence of \textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \textit{oracle} that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05055</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#39640;&#32500;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#32534;&#30721;&#25104;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#22270;&#23884;&#20837;&#26041;&#27861;&#36981;&#24490;&#36825;&#26679;&#19968;&#31181;&#22522;&#26412;&#24605;&#24819;&#65292;&#21363;&#22270;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#30340;&#23884;&#20837;&#30690;&#37327;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#30456;&#23545;&#25509;&#36817;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#23481;&#37327;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#24615;&#33021;; &#65288;ii&#65289;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#26080;&#27861;&#19982;&#26368;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30456;&#32467;&#21512;&#65307;&#65288;iii&#65289;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#20219;&#21153;&#30456;&#20114;&#20381;&#23384;&#65292;&#24212;&#20849;&#21516;&#21152;&#24378;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#30528;&#25104;&#21151;&#65292;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN DeepGOPlus &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25968;&#20540;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06361</link><description>&lt;p&gt;
DeepGOPlus &#25512;&#29702;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numerical Stability of DeepGOPlus Inference. (arXiv:2212.06361v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06361
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN DeepGOPlus &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25968;&#20540;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNNs) &#26159;&#30446;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#19968;&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#26368;&#21021;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294; CNNs &#19982;&#20855;&#26377;&#31354;&#38388;&#20851;&#31995;&#30340;&#20219;&#20309;&#25968;&#25454;&#37117;&#33021;&#24456;&#22909;&#22320;&#37197;&#21512;&#20351;&#29992;&#65292;&#24182;&#24050;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102; CNNs&#65292;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#22122;&#22768;&#27880;&#20837;&#30340;&#25935;&#24863;&#24615;&#21487;&#33021;&#20250;&#21361;&#21450;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#37327;&#21270;&#20102; DeepGOPlus &#30340;&#28014;&#28857;&#31934;&#24230;&#19981;&#30830;&#23450;&#24615;&#20197;&#30830;&#23450;&#20854;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;DeepGOPlus &#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892; DeepGOPlus &#25512;&#29702;&#20197;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#24310;&#36831;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807; Monte Carlo Arithmetic &#23454;&#29616;&#30340;&#65292;&#35813;&#25216;&#26415;&#23454;&#39564;&#24615;&#22320;&#37327;&#21270;&#20102;&#28014;&#28857;&#36816;&#31639;&#38169;&#35823;&#21644; VPR&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are currently among the most widely-used neural networks available and achieve state-of-the-art performance for many problems. While originally applied to computer vision tasks, CNNs work well with any data with a spatial relationship, besides images, and have been applied to different fields. However, recent works have highlighted how CNNs, like other deep learning models, are sensitive to noise injection which can jeopardise their performance. This paper quantifies the numerical uncertainty of the floating point arithmetic inaccuracies of the inference stage of DeepGOPlus, a CNN that predicts protein function, in order to determine its numerical stability. In addition, this paper investigates the possibility to use reduced-precision floating point formats for DeepGOPlus inference to reduce memory consumption and latency. This is achieved with Monte Carlo Arithmetic, a technique that experimentally quantifies floating point operation errors and VPR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#20855;&#26377;&#26799;&#24230;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2207.06154</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (arXiv:2207.06154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#20855;&#26377;&#26799;&#24230;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#35757;&#32451;&#20986;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;&#26799;&#24230;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#36864;&#21270;&#23548;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#24403;&#25968;&#25454;&#20301;&#20110;&#29615;&#22659;&#31354;&#38388;&#30340;&#19968;&#20010;&#20302;&#32500;&#23376;&#27969;&#24418;&#19978;&#26102;&#12290;&#20316;&#20026;&#30452;&#25509;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#23545;&#26799;&#24230;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#20174;&#21518;&#39564;&#20013;&#37319;&#26679;&#30340;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#26799;&#24230;&#25915;&#20987;&#37117;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#25439;&#22833;&#20989;&#25968;&#23545;BNN&#21518;&#39564;&#20998;&#24067;&#30340;&#26399;&#26395;&#26799;&#24230;&#20173;&#28982;&#36235;&#20110;&#38646;&#12290;&#22312;t&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability to adversarial attacks is one of the principal hurdles to the adoption of deep learning in safety-critical applications. Despite significant efforts, both practical and theoretical, training deep learning models robust to adversarial attacks is still an open problem. In this paper, we analyse the geometry of adversarial attacks in the large-data, overparameterized limit for Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to gradient-based attacks arises as a result of degeneracy in the data distribution, i.e., when the data lies on a lower-dimensional submanifold of the ambient space. As a direct consequence, we demonstrate that in this limit BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we prove that the expected gradient of the loss with respect to the BNN posterior distribution is vanishing, even when each neural network sampled from the posterior is vulnerable to gradient-based attacks. Experimental results on t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#31232;&#30095;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#32858;&#31867;&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.11677</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#30340;&#21322;&#30417;&#30563;&#32858;&#31867;&#65306;&#36328;&#36234;&#20102;&#20449;&#24687;&#29702;&#35770;&#38376;&#27099;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold. (arXiv:2205.11677v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#31232;&#30095;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#32858;&#31867;&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22359;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#32467;&#26500;&#25968;&#25454;&#32858;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#22522;&#26412;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#25968;&#21313;&#24180;&#26469;&#23545;&#35813;&#38382;&#39064;&#30340;&#24191;&#27867;&#30740;&#31350;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;Kesten-Stigum&#38376;&#27099;&#22788;&#30340;&#30456;&#21464;&#29616;&#35937;&#29305;&#21035;&#26377;&#36259;&#65292;&#20174;&#25968;&#23398;&#21644;&#24212;&#29992;&#35282;&#24230;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23427;&#34920;&#26126;&#65292;&#22914;&#26524;&#27169;&#22411;&#21442;&#25968;&#22312;&#26576;&#20010;&#38376;&#27099;&#20197;&#19979;&#65292;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#20219;&#20309;&#20272;&#35745;&#22120;&#22312;&#31232;&#30095;&#22270;&#19978;&#37117;&#19981;&#33021;&#27604;&#38543;&#26426;&#29468;&#27979;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#31245;&#24494;&#25193;&#23637;&#35270;&#37326;&#21040;&#26222;&#36941;&#23384;&#22312;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#65292;&#36825;&#26679;&#30340;&#22522;&#26412;&#38480;&#21046;&#23558;&#23436;&#20840;&#28040;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25581;&#31034;&#20986;&#20219;&#24847;&#19968;&#37096;&#20998;&#26631;&#35760;&#65292;&#21487;&#20197;&#22312;&#25972;&#20010;&#21442;&#25968;&#22495;&#20869;&#23545;&#26816;&#27979;&#38382;&#39064;&#36827;&#34892;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#32452;&#21512;&#30340;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#20248;&#21270;&#30340;&#65292;&#29992;&#20110;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#24102;&#26469;&#20102;&#20840;&#26032;&#30340;&#35270;&#35282;&#65292;&#26631;&#24535;&#30528;&#31232;&#30095;&#22270;&#32858;&#31867;&#39046;&#22495;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic block model is a canonical random graph model for clustering and community detection on network-structured data. Decades of extensive study on the problem have established many profound results, among which the phase transition at the Kesten-Stigum threshold is particularly interesting both from a mathematical and an applied standpoint. It states that no estimator based on the network topology can perform substantially better than chance on sparse graphs if the model parameter is below certain threshold. Nevertheless, if we slightly extend the horizon to the ubiquitous semi-supervised setting, such a fundamental limitation will disappear completely. We prove that with arbitrary fraction of the labels revealed, the detection problem is feasible throughout the parameter domain. Moreover, we introduce two efficient algorithms, one combinatorial and one based on optimization, to integrate label information with graph structures. Our work brings a new perspective to stochasti
&lt;/p&gt;</description></item></channel></rss>