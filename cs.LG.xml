<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13092</link><description>&lt;p&gt;
&#20174;&#26032;&#30340;&#35282;&#24230;&#21387;&#32553;ImageNet&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;SRe$^2$L
&lt;/p&gt;
&lt;p&gt;
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13092
&lt;/p&gt;
&lt;p&gt;
SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65292;&#31216;&#20026;Squeeze&#12289;Recover&#21644;Relabel&#65288;SRe$^2$L&#65289;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#20998;&#31163;&#20102;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#21452;&#23618;&#20248;&#21270;&#65292;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#35268;&#27169;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#22270;&#20687;&#20219;&#24847;&#20998;&#36776;&#29575;&#12289;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;Tiny-ImageNet&#21644;&#23436;&#25972;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312;50IPC&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;42.5&#65285;&#21644;60.8&#65285;&#30340;&#26368;&#39640;&#39564;&#35777;&#31934;&#24230;&#65292;&#36739;&#20043;&#21069;&#25152;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;14.5&#65285;&#21644;32.9&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Res&#19978;&#20063;&#27604;MTT&#24555;&#32422;52&#20493;(ConvNet-4)&#21644;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#26465;&#20214;&#30340;GAN&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#20266;&#36896;&#20154;&#33080;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#35268;&#36991;&#21462;&#35777;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13091</link><description>&lt;p&gt;
&#24102;&#23646;&#24615;&#26465;&#20214;&#30340;&#23545;&#25239;&#20154;&#33080;&#29983;&#25104;&#22120;&#33021;&#22815;&#36867;&#36991;&#21462;&#35777;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces. (arXiv:2306.13091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#26465;&#20214;&#30340;GAN&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#20266;&#36896;&#20154;&#33080;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#35268;&#36991;&#21462;&#35777;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#21512;&#25104;&#20154;&#33080;&#22270;&#20687;&#30340;&#33021;&#21147;&#24341;&#36215;&#20102;&#23433;&#20840;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#20551;&#38754;&#23380;&#30340;&#31532;&#19968;&#36947;&#38450;&#32447;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21462;&#35777;&#20998;&#31867;&#22120;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#34429;&#28982;&#36825;&#20123;&#21462;&#35777;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#20986;&#38754;&#37096;&#22270;&#20687;&#26159;&#21542;&#20026;&#21512;&#25104;&#30340;&#25110;&#30495;&#23454;&#30340;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#25915;&#20987;&#22312;&#36867;&#36991;&#37492;&#23450;&#20998;&#31867;&#22120;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20250;&#24341;&#20837;&#21487;&#36890;&#36807;&#20180;&#32454;&#30340;&#20154;&#31867;&#26816;&#26597;&#36827;&#34892;&#26816;&#27979;&#30340;&#21487;&#35265;&#22122;&#22768;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25915;&#20987;&#20551;&#35774;&#26377;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#27491;&#30830;&#30340;&#12290;&#24050;&#32463;&#23581;&#35797;&#30452;&#25509;&#25200;&#21160;GAN&#30340;&#28508;&#22312;&#31354;&#38388;&#20197;&#20135;&#29983;&#23545;&#25239;&#24615;&#30340;&#20266;&#36896;&#38754;&#23380;&#65292;&#21487;&#20197;&#35268;&#36991;&#21462;&#35777;&#20998;&#31867;&#22120;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26356;&#36827;&#19968;&#27493;&#65292;&#23637;&#31034;&#20102;&#21487;&#20197;&#25104;&#21151;&#29983;&#25104;&#24102;&#26377;&#25351;&#23450;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#22836;&#21457;&#39068;&#33394;&#12289;&#30524;&#30555;&#22823;&#23567;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#30340;&#23545;&#25239;&#20154;&#33080;&#65292;&#23427;&#20204;&#21487;&#20197;&#35268;&#36991;&#21462;&#35777;&#20998;&#31867;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#26465;&#20214;&#30340;GAN&#26041;&#27861;&#65292;&#20854;&#20013;GAN&#29983;&#25104;&#22120;&#26159;&#20197;&#25152;&#38656;&#20266;&#36896;&#38754;&#23380;&#30340;&#23646;&#24615;&#20316;&#20026;&#26465;&#20214;&#65292;&#29983;&#25104;&#36867;&#36991;&#21462;&#35777;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#38754;&#23380;&#22270;&#20687;&#65292;&#24182;&#32039;&#23494;&#21305;&#37197;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of generative models to produce highly realistic synthetic face images has raised security and ethical concerns. As a first line of defense against such fake faces, deep learning based forensic classifiers have been developed. While these forensic models can detect whether a face image is synthetic or real with high accuracy, they are also vulnerable to adversarial attacks. Although such attacks can be highly successful in evading detection by forensic classifiers, they introduce visible noise patterns that are detectable through careful human scrutiny. Additionally, these attacks assume access to the target model(s) which may not always be true. Attempts have been made to directly perturb the latent space of GANs to produce adversarial fake faces that can circumvent forensic classifiers. In this work, we go one step further and show that it is possible to successfully generate adversarial fake faces with a specified set of attributes (e.g., hair color, eye size, race, gend
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13089</link><description>&lt;p&gt;
GIMLET&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25351;&#20196;&#20998;&#23376;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#23454;&#39564;&#36896;&#25104;&#30340;&#26631;&#31614;&#19981;&#36275;&#38382;&#39064;&#23558;&#26159;&#20854;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#30693;&#35782;&#36827;&#34892;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#20998;&#23376;-&#25991;&#26412;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22788;&#29702;&#25351;&#20196;&#19981;&#36275;&#20197;&#21450;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIMLET&#65292;&#23427;&#32479;&#19968;&#20102;&#22270;&#24418;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#24191;&#20041;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#25193;&#23637;&#20197;&#32534;&#30721;&#22270;&#24418;&#32467;&#26500;&#21644;&#25351;&#20196;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22270;&#24418;&#32534;&#30721;&#27169;&#22359;&#12290;GIMLET&#36824;&#22312;&#27880;&#24847;&#26426;&#21046;&#20013;&#35299;&#32806;&#20102;&#22270;&#24418;&#30340;&#32534;&#30721;&#21644;&#20219;&#21153;&#25351;&#20196;&#65292;&#22686;&#24378;&#20102;&#36328;&#26032;&#20219;&#21153;&#30340;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#37319;&#26679;&#25968;&#25454;&#38598;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#34920;&#29616;&#30340;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#30446;&#26631;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.13085</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#21152;&#26435;&#21033;&#29992;&#28151;&#21512;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting. (arXiv:2306.13085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#37319;&#26679;&#25968;&#25454;&#38598;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#34920;&#29616;&#30340;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#30446;&#26631;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36820;&#22238;&#19968;&#20010;&#26368;&#22823;&#21270;&#39044;&#26399;&#34920;&#29616;&#19982;&#35825;&#23548;&#29366;&#24577;-&#21160;&#20316;&#21344;&#29992;&#30340;&#20998;&#24067;&#24046;&#24322;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#30446;&#26631;&#31574;&#30053;&#65292;&#22240;&#27492;&#65292;&#30446;&#26631;&#31574;&#30053;&#30340;&#34920;&#29616;&#19982;&#25968;&#25454;&#38598;&#25910;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#30340;&#34920;&#29616;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30001;&#22823;&#22810;&#25968;&#20302;&#22238;&#25253;&#36712;&#36857;&#21644;&#23569;&#25968;&#39640;&#22238;&#25253;&#36712;&#36857;&#32452;&#25104;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#20013;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21463;&#21040;&#20302;&#22238;&#25253;&#36712;&#36857;&#30340;&#36807;&#24230;&#21046;&#32422;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#39640;&#34920;&#29616;&#36712;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#38543;&#26426;&#21021;&#22987;&#29366;&#24577;&#30340;&#30830;&#23450;&#24615;MDPs&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#37319;&#26679;&#25968;&#25454;&#38598;&#26469;&#35825;&#23548;&#20855;&#26377;&#26356;&#39640;&#22238;&#25253;&#30340;&#34892;&#20026;&#31574;&#30053;&#30340;&#20154;&#24037;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#37319;&#26679;&#31574;&#30053;&#21487;&#20197;&#19982;&#20219;&#20309;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#24207;&#21015;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23558;&#22768;&#38899;&#25968;&#25454;&#36716;&#24405;&#25104;&#25991;&#26412;&#24182;&#19982;&#22270;&#20687;&#25968;&#25454;&#19968;&#36215;&#20998;&#26512;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13076</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#22522;&#20110;&#26102;&#38388;&#30340;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Time-based Models for Multimodal Emotion Recognition. (arXiv:2306.13076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#24207;&#21015;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23558;&#22768;&#38899;&#25968;&#25454;&#36716;&#24405;&#25104;&#25991;&#26412;&#24182;&#19982;&#22270;&#20687;&#25968;&#25454;&#19968;&#36215;&#20998;&#26512;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#24050;&#25104;&#20026;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#20851;&#20110;&#22768;&#38899;&#21644;&#35270;&#39057;&#20197;&#20102;&#35299;&#24773;&#32490;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#26512;&#38754;&#37096;&#34920;&#24773;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;6&#31181;&#22522;&#26412;&#24773;&#32490;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#24207;&#21015;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#22810;&#23618;CNN&#27169;&#22411;&#23545;&#22768;&#38899;&#21644;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#39304;&#36865;&#21040;&#21508;&#31181;&#24207;&#21015;&#27169;&#22411;&#20013;&#12290;&#24207;&#21015;&#27169;&#22411;&#21253;&#25324;GRU&#12289;Transformer&#12289;LSTM&#21644;Max Pooling&#12290;&#35745;&#31639;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#21644;F1 Score&#20540;&#12290;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;CREMA-D&#25968;&#25454;&#38598;&#12290;&#27604;&#36739;CREMA-D&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;GRU&#30340;&#26550;&#26500;&#22312;F1&#20998;&#25968;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24471;&#20998;&#20026;0.640&#65292;&#22522;&#20110;LSTM&#30340;&#26550;&#26500;&#22312;&#31934;&#24230;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24471;&#20998;&#20026;0.699&#65292;&#32780;&#22522;&#20110;Max Pooling&#30340;&#26550;&#26500;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#26174;&#31034;&#20986;&#26368;&#20339;&#30340;&#25935;&#24863;&#24230;&#65292;&#24471;&#20998;&#20026;0.620&#12290;&#22240;&#27492;&#65292;&#35266;&#23519;&#21040;&#23558;&#22768;&#38899;&#25968;&#25454;&#36716;&#24405;&#25104;&#25991;&#26412;&#24182;&#19982;&#22270;&#20687;&#25968;&#25454;&#19968;&#36215;&#20998;&#26512;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition has become an important research topic in the field of human-computer interaction. Studies on sound and videos to understand emotions focused mainly on analyzing facial expressions and classified 6 basic emotions. In this study, the performance of different sequence models in multi-modal emotion recognition was compared. The sound and images were first processed by multi-layered CNN models, and the outputs of these models were fed into various sequence models. The sequence model is GRU, Transformer, LSTM and Max Pooling. Accuracy, precision, and F1 Score values of all models were calculated. The multi-modal CREMA-D dataset was used in the experiments. As a result of the comparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed the best result in F1 score, LSTM-based architecture with 0.699 in precision metric, while sensitivity showed the best results over time with Max Pooling-based architecture with 0.620. As a result, it has been observed that t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#23457;&#35745;&#26694;&#26550;&#8212;&#8212;&#26465;&#20214;&#20559;&#24046;&#25195;&#25551;&#65288;CBS&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#20132;&#21449;&#20559;&#35265;&#12290;&#19982;&#23457;&#35745;&#23376;&#32452;&#20844;&#24179;&#24615;&#30340;&#31867;&#20284;&#26041;&#27861;&#30456;&#27604;&#65292;CBS&#33021;&#22815;&#26816;&#27979;&#21040;&#26356;&#22810;&#26410;&#26366;&#21457;&#29616;&#30340;&#20132;&#21449;&#21644;&#24773;&#22659;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.13064</link><description>&lt;p&gt;
&#23457;&#35745;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#20132;&#21449;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Auditing Predictive Models for Intersectional Biases. (arXiv:2306.13064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#23457;&#35745;&#26694;&#26550;&#8212;&#8212;&#26465;&#20214;&#20559;&#24046;&#25195;&#25551;&#65288;CBS&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#20132;&#21449;&#20559;&#35265;&#12290;&#19982;&#23457;&#35745;&#23376;&#32452;&#20844;&#24179;&#24615;&#30340;&#31867;&#20284;&#26041;&#27861;&#30456;&#27604;&#65292;CBS&#33021;&#22815;&#26816;&#27979;&#21040;&#26356;&#22810;&#26410;&#26366;&#21457;&#29616;&#30340;&#20132;&#21449;&#21644;&#24773;&#22659;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28385;&#36275;&#21463;&#20445;&#25252;&#31867;&#21035;&#25104;&#21592;&#32676;&#20307;&#20844;&#24179;&#20934;&#21017;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20294;&#19981;&#20445;&#35777;&#23376;&#32452;&#20844;&#24179;&#65292;&#21487;&#33021;&#20250;&#20026;&#20004;&#20010;&#25110;&#22810;&#20010;&#20445;&#25252;&#31867;&#21035;&#30340;&#20010;&#20307;&#20135;&#29983;&#20559;&#24046;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#23457;&#35745;&#26694;&#26550;&#8212;&#8212;&#26465;&#20214;&#20559;&#24046;&#25195;&#25551;&#65288;CBS&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#20132;&#21449;&#20559;&#35265;&#12290;CBS&#35782;&#21035;&#20102;&#23545;&#21463;&#20445;&#25252;&#31867;&#21035;&#26368;&#26377;&#20559;&#24046;&#30340;&#23376;&#32452;&#65292;&#30456;&#27604;&#20110;&#38750;&#21463;&#20445;&#25252;&#31867;&#21035;&#20013;&#30340;&#31561;&#20215;&#23376;&#32452;&#65292;&#20197;&#21450;&#21487;&#20197;&#21253;&#25324;&#22810;&#31181;&#36890;&#24120;&#29992;&#20110;&#27010;&#29575;&#21644;&#20108;&#20803;&#39044;&#27979;&#30340;&#20844;&#24179;&#23450;&#20041;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21040;COMPAS&#39044;&#23457;&#39118;&#38505;&#35780;&#20272;&#24037;&#20855;&#20013;&#20808;&#21069;&#26410;&#35782;&#21035;&#30340;&#20132;&#21449;&#21644;&#24773;&#22659;&#20559;&#35265;&#65292;&#24182;&#19988;&#19982;&#23457;&#35745;&#23376;&#32452;&#20844;&#24179;&#24615;&#30340;&#31867;&#20284;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20559;&#35265;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive models that satisfy group fairness criteria in aggregate for members of a protected class, but do not guarantee subgroup fairness, could produce biased predictions for individuals at the intersection of two or more protected classes. To address this risk, we propose Conditional Bias Scan (CBS), a flexible auditing framework for detecting intersectional biases in classification models. CBS identifies the subgroup for which there is the most significant bias against the protected class, as compared to the equivalent subgroup in the non-protected class, and can incorporate multiple commonly used fairness definitions for both probabilistic and binarized predictions. We show that this methodology can detect previously unidentified intersectional and contextual biases in the COMPAS pre-trial risk assessment tool and has higher bias detection power compared to similar methods that audit for subgroup fairness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#30028;&#21327;&#26041;&#24046;&#24773;&#20917;&#19979;&#23398;&#20064;&#20998;&#31163;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;SQ&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#19979;&#38480;&#20026;$d^{\Omega(1/\epsilon)}$&#12290;</title><link>http://arxiv.org/abs/2306.13057</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#26377;&#30028;&#21327;&#26041;&#24046;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;SQ&#19979;&#38480;
&lt;/p&gt;
&lt;p&gt;
SQ Lower Bounds for Learning Bounded Covariance GMMs. (arXiv:2306.13057v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#30028;&#21327;&#26041;&#24046;&#24773;&#20917;&#19979;&#23398;&#20064;&#20998;&#31163;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;SQ&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#19979;&#38480;&#20026;$d^{\Omega(1/\epsilon)}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30456;&#21516;&#26410;&#30693;&#26377;&#30028;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20998;&#31163;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#24418;&#24335;&#20026;$P = \sum_{i=1}^k w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$&#30340;$\mathbb{R}^d$&#19978;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#65292;&#20854;&#20013;$\mathbf \Sigma_i = \mathbf \Sigma \preceq \mathbf I$&#19988;$\min_{i \neq j} \| \boldsymbol \mu_i \boldsymbol \mu_j\|_2 \geq k^\epsilon$&#23545;&#20110;&#26576;&#20123;$\epsilon&gt; 0$&#12290;&#24050;&#30693;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#20026;$(dk)^{O(1/\epsilon)}$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#29992;&#20110;&#27492;&#38382;&#39064;&#30340;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#33267;&#23569;&#38656;&#35201;$d^{\Omega(1/\epsilon)}$&#12290;&#24403;&#20998;&#31163;&#22312;$k^{1/2}$&#25968;&#37327;&#32423;&#19978;&#26102;&#65292;&#25105;&#20204;&#21478;&#22806;&#33719;&#24471;&#20102;&#20855;&#26377;&#27491;&#30830;&#25351;&#25968;&#30340;&#32454;&#31890;&#24230;SQ&#19979;&#38480;&#12290;&#25105;&#20204;&#30340;SQ&#19979;&#38480;&#24847;&#21619;&#30528;&#20302;&#27425;&#22810;&#39033;&#24335;&#27979;&#35797;&#30340;&#31867;&#20284;&#19979;&#38480;&#12290;&#20174;&#27010;&#24565;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#24050;&#30693;&#31639;&#27861;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of learning mixtures of separated Gaussians with common unknown bounded covariance matrix. Specifically, we focus on learning Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i = \mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i \boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon&gt;0$. Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In this work, we prove that any Statistical Query (SQ) algorithm for this problem requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case where the separation is on the order of $k^{1/2}$, we additionally obtain fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds imply similar lower bounds for low-degree polynomial tests. Conceptually, our results provide evidence that known algorithms for this problem are nearly be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#27827;&#35930;&#38544;&#31169;(QPP)&#30340;&#36890;&#29992;&#38544;&#31169;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#25351;&#23450;&#31169;&#26377;&#20449;&#24687;&#12289;&#21487;&#34892;&#30340;&#27979;&#37327;&#21644;&#22495;&#30693;&#35782;&#26041;&#38754;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#25324;&#21644;&#20811;&#26381;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;QPP&#30340;&#25805;&#20316;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#20102;QPP&#30340;&#20984;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#21518;&#22788;&#29702;&#65292;&#25512;&#23548;&#20102;&#20445;&#35777;&#21435;&#26497;&#21270;&#26426;&#21046;QPP&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;QPP&#26694;&#26550;&#24212;&#29992;&#20110;&#38544;&#31169;&#23457;&#35745;&#65292;&#20197;&#35782;&#21035;&#38544;&#31169;&#20405;&#29359;&#12290;</title><link>http://arxiv.org/abs/2306.13054</link><description>&lt;p&gt;
&#37327;&#23376;&#27827;&#35930;&#38544;&#31169;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#37327;&#23376;&#31995;&#32479;&#38544;&#31169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum Systems. (arXiv:2306.13054v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#27827;&#35930;&#38544;&#31169;(QPP)&#30340;&#36890;&#29992;&#38544;&#31169;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#25351;&#23450;&#31169;&#26377;&#20449;&#24687;&#12289;&#21487;&#34892;&#30340;&#27979;&#37327;&#21644;&#22495;&#30693;&#35782;&#26041;&#38754;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#25324;&#21644;&#20811;&#26381;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;QPP&#30340;&#25805;&#20316;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#20102;QPP&#30340;&#20984;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#21518;&#22788;&#29702;&#65292;&#25512;&#23548;&#20102;&#20445;&#35777;&#21435;&#26497;&#21270;&#26426;&#21046;QPP&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;QPP&#26694;&#26550;&#24212;&#29992;&#20110;&#38544;&#31169;&#23457;&#35745;&#65292;&#20197;&#35782;&#21035;&#38544;&#31169;&#20405;&#29359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#31995;&#32479;&#30340;&#36890;&#29992;&#38544;&#31169;&#26694;&#26550;&#65292;&#31216;&#20026;&#37327;&#23376;&#27827;&#35930;&#38544;&#31169;&#65288;QPP&#65289;&#12290;&#21463;&#32463;&#20856;&#27827;&#35930;&#38544;&#31169;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#27010;&#25324;&#24182;&#35299;&#20915;&#20102;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#22312;&#25351;&#23450;&#31169;&#26377;&#20449;&#24687;&#12289;&#21487;&#34892;&#30340;&#27979;&#37327;&#21644;&#22495;&#30693;&#35782;&#26041;&#38754;&#25552;&#20379;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;QPP&#21487;&#20197;&#31561;&#25928;&#22320;&#29992;Datta-Leditzky&#20449;&#24687;&#35889;&#25955;&#24230;&#26469;&#34920;&#31034;&#65292;&#20174;&#32780;&#39318;&#27425;&#25552;&#20379;&#20102;&#23427;&#30340;&#25805;&#20316;&#35299;&#37322;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#24046;&#24322;&#37325;&#26032;&#34920;&#36848;&#20026;&#21322;&#23450;&#35268;&#21010;&#65292;&#24182;&#25512;&#23548;&#20986;&#23427;&#30340;&#20960;&#20010;&#23646;&#24615;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#23646;&#24615;&#35777;&#26126;&#20102;QPP&#26426;&#21046;&#30340;&#20984;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#21518;&#22788;&#29702;&#12290;&#36824;&#25512;&#23548;&#20102;&#20445;&#35777;&#21435;&#26497;&#21270;&#26426;&#21046;QPP&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26222;&#36890;QPP&#26426;&#21046;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#20877;&#27425;&#20197;&#21435;&#26497;&#21270;&#26426;&#21046;&#20026;&#20363;&#30740;&#31350;&#20854;&#26126;&#30830;&#23454;&#20363;&#12290;&#28982;&#21518;&#23558;QPP&#26694;&#26550;&#24212;&#29992;&#20110;&#38544;&#31169;&#23457;&#35745;&#65292;&#20197;&#35782;&#21035;&#38544;&#31169;&#20405;&#29359;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a versatile privacy framework for quantum systems, termed quantum pufferfish privacy (QPP). Inspired by classical pufferfish privacy, our formulation generalizes and addresses limitations of quantum differential privacy by offering flexibility in specifying private information, feasible measurements, and domain knowledge. We show that QPP can be equivalently formulated in terms of the Datta-Leditzky information spectrum divergence, thus providing the first operational interpretation thereof. We reformulate this divergence as a semi-definite program and derive several properties of it, which are then used to prove convexity, composability, and post-processing of QPP mechanisms. Parameters that guarantee QPP of the depolarization mechanism are also derived. We analyze the privacy-utility tradeoff of general QPP mechanisms and, again, study the depolarization mechanism as an explicit instance. The QPP framework is then applied to privacy auditing for identifying privacy violati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377; $S$ &#20010;&#19978;&#19979;&#25991;&#21644; $A$ &#31181;&#34892;&#21160;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#32452;&#30340;&#38543;&#26426;&#36172;&#24466;&#38382;&#39064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13053</link><description>&lt;p&gt;
&#21487;&#20998;&#32452;&#30340;&#38543;&#26426;&#36172;&#24466;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Context-lumpable stochastic bandits. (arXiv:2306.13053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377; $S$ &#20010;&#19978;&#19979;&#25991;&#21644; $A$ &#31181;&#34892;&#21160;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#32452;&#30340;&#38543;&#26426;&#36172;&#24466;&#38382;&#39064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377; $S$ &#20010;&#19978;&#19979;&#25991;&#21644; $A$ &#31181;&#34892;&#21160;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#12290;&#22312;&#27599;&#19968;&#36718; $t=1,2,\dots$ &#20013;&#65292;&#23398;&#20064;&#32773;&#35266;&#23519;&#19968;&#20010;&#38543;&#26426;&#19978;&#19979;&#25991;&#65292;&#24182;&#26681;&#25454;&#20854;&#20197;&#24448;&#30340;&#32463;&#39564;&#36873;&#25321;&#19968;&#20010;&#34892;&#21160;&#12290;&#28982;&#21518;&#65292;&#23398;&#20064;&#32773;&#35266;&#23519;&#19968;&#20010;&#38543;&#26426;&#22870;&#21169;&#65292;&#20854;&#24179;&#22343;&#20540;&#26159;&#35813;&#36718;&#19978;&#19979;&#25991;&#21644;&#34892;&#21160;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#22312;&#20551;&#35774;&#19978;&#19979;&#25991;&#21487;&#20197;&#20998;&#20026; $r\leq \min\{S,A\}$ &#32452;&#65292;&#20351;&#24471;&#20219;&#24847;&#20004;&#20010;&#22312;&#21516;&#19968;&#32452;&#20869;&#30340;&#19978;&#19979;&#25991;&#30340;&#21508;&#31181;&#34892;&#21160;&#30340;&#24179;&#22343;&#22870;&#21169;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23427;&#22312;&#20351;&#29992; $\widetilde O(r(S + A)/\epsilon^2)$ &#20010;&#26679;&#26412;&#21518;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010; $\epsilon$-&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#32622;&#20449;&#24230;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340; $\widetilde\Omega(r (S + A )/\epsilon^2)$ &#19979;&#38480;&#12290;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20854;&#32047;&#31215;&#36951;&#25022;&#22312;&#26102;&#38388; $T$ &#20869;&#26377;&#30028;&#65292;&#21363; $\widetilde O(\sqrt{r^3(S +A)T})$&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23637;&#31034;&#22312;&#21487;&#36817;&#20284;&#27491;&#30830;&#35774;&#32622;&#20013;&#25509;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#35774;&#32622;&#19979;&#30340; $\widetilde O(\sqrt{r^3(S +A)T})$ &#32047;&#31215;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a contextual bandit problem with $S $ contexts and $A $ actions. In each round $t=1,2,\dots$ the learner observes a random context and chooses an action based on its past experience. The learner then observes a random reward whose mean is a function of the context and the action for the round. Under the assumption that the contexts can be lumped into $r\le \min\{S ,A \}$ groups such that the mean reward for the various actions is the same for any two contexts that are in the same group, we give an algorithm that outputs an $\epsilon$-optimal policy after using at most $\widetilde O(r (S +A )/\epsilon^2)$ samples with high probability and provide a matching $\widetilde\Omega(r (S +A )/\epsilon^2)$ lower bound. In the regret minimization setting, we give an algorithm whose cumulative regret up to time $T$ is bounded by $\widetilde O(\sqrt{r^3(S +A )T})$. To the best of our knowledge, we are the first to show the near-optimal sample complexity in the PAC setting and $\widetild
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#22686;&#24191;&#21644;&#32454;&#21270;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#30340;&#35780;&#32423;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#26469;&#35780;&#20272;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13050</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#65306;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization. (arXiv:2306.13050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#22686;&#24191;&#21644;&#32454;&#21270;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#30340;&#35780;&#32423;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#26469;&#35780;&#20272;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#24050;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20854;&#20013;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#36807;&#21435;&#21916;&#22909;&#21644;&#20854;&#20182;&#29992;&#25143;&#30340;&#21487;&#29992;&#20559;&#22909;&#20449;&#24687;&#39044;&#27979;&#20854;&#23545;&#26032;&#29289;&#21697;&#30340;&#35780;&#20998;&#12290;&#23613;&#31649;CF&#26041;&#27861;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#35266;&#23519;&#21040;&#30340;&#26465;&#30446;&#30340;&#31232;&#30095;&#24615;&#30340;&#26497;&#22823;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#65288;MMMF&#65289;&#30340;&#25968;&#25454;&#22686;&#24191;&#21644;&#32454;&#21270;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#26159;&#24191;&#27867;&#25509;&#21463;&#30340;&#29992;&#20110;&#35780;&#32423;&#39044;&#27979;&#30340;CF&#25216;&#26415;&#65292;&#20043;&#21069;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;CF&#31639;&#27861;&#30340;&#22266;&#26377;&#29305;&#24615;&#26469;&#35780;&#20272;&#21333;&#20010;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#35780;&#32423;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#20219;&#20309;CF&#31639;&#27861;&#30340;&#39044;&#27979;&#20302;&#32622;&#20449;&#24230;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#26576;&#20123;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#36890;&#36807;&#37319;&#29992;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) has become a popular method for developing recommender systems (RS) where ratings of a user for new items is predicted based on her past preferences and available preference information of other users. Despite the popularity of CF-based methods, their performance is often greatly limited by the sparsity of observed entries. In this study, we explore the data augmentation and refinement aspects of Maximum Margin Matrix Factorization (MMMF), a widely accepted CF technique for the rating predictions, which have not been investigated before. We exploit the inherent characteristics of CF algorithms to assess the confidence level of individual ratings and propose a semi-supervised approach for rating augmentation based on self-training. We hypothesize that any CF algorithm's predictions with low confidence are due to some deficiency in the training data and hence, the performance of the algorithm can be improved by adopting a systematic data augmentation strategy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#26377;&#22280;&#29305;&#23450;&#27880;&#24847;&#26426;&#21046;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#65288;MLSA&#65289;&#65292;&#29992;&#20110;&#20849;&#21516;&#23398;&#20064;&#19977;&#20010;CDR&#29615;&#30340;&#32467;&#26500;&#39044;&#27979;&#65292;&#20854;&#20013;H3&#29615;&#30340;CDR&#32467;&#26500;&#30001;&#20110;&#38271;&#24230;&#21644;&#28789;&#27963;&#30340;&#32467;&#26500;&#20855;&#26377;&#26356;&#22823;&#25361;&#25112;&#24615;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.13045</link><description>&lt;p&gt;
&#24102;&#26377;&#22280;&#29305;&#23450;&#27880;&#24847;&#26426;&#21046;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;CDR&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning with Loop Specific Attention for CDR Structure Prediction. (arXiv:2306.13045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#26377;&#22280;&#29305;&#23450;&#27880;&#24847;&#26426;&#21046;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#65288;MLSA&#65289;&#65292;&#29992;&#20110;&#20849;&#21516;&#23398;&#20064;&#19977;&#20010;CDR&#29615;&#30340;&#32467;&#26500;&#39044;&#27979;&#65292;&#20854;&#20013;H3&#29615;&#30340;CDR&#32467;&#26500;&#30001;&#20110;&#38271;&#24230;&#21644;&#28789;&#27963;&#30340;&#32467;&#26500;&#20855;&#26377;&#26356;&#22823;&#25361;&#25112;&#24615;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#24037;&#31243;&#20013;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#32467;&#26500;&#39044;&#27979;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#22312;&#35774;&#35745;&#25239;&#20307;&#26102;&#65292;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#39044;&#27979;H3&#29615;&#30340;CDR&#32467;&#26500;&#65292;&#19982;&#20854;&#20182;CDR&#29615;&#65288;H1&#21644;H2 &#29615;&#65289;&#30456;&#27604;&#65292;H3&#29615;&#30340;CDR&#32467;&#26500;&#30001;&#20110;&#38271;&#24230;&#21644;&#28789;&#27963;&#30340;&#32467;&#26500;&#32780;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22280;&#29305;&#23450;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;MLSA&#12290;&#25105;&#20204;&#26159;&#39318;&#20301;&#36890;&#36807;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#20849;&#21516;&#23398;&#20064;&#19977;&#20010;CDR&#29615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32771;&#34385;&#19977;&#20010;CDR&#29615;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22280;&#29305;&#23450;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#25511;&#21046;MLSA&#27169;&#22411;&#35757;&#32451;&#20013;&#27599;&#20010;CDR&#29615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;MLSA&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Complementarity Determining Region (CDR) structure prediction of loops in antibody engineering has gained a lot of attraction by researchers. When designing antibodies, a main challenge is to predict the CDR structure of the H3 loop. Compared with the other CDR loops, that is the H1 and H2 loops, the CDR structure of the H3 loop is more challenging due to its varying length and flexible structure. In this paper, we propose a Multi-task learning model with Loop Specific Attention, namely MLSA. In particular, to the best of our knowledge we are the first to jointly learn the three CDR loops, via a novel multi-task learning strategy. In addition, to account for the structural and functional similarities and differences of the three CDR loops, we propose a loop specific attention mechanism to control the influence of each CDR loop on the training of MLSA. Our experimental evaluation on widely used benchmark data shows that the proposed MLSA method significantly reduces the prediction e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20379;&#32508;&#21512;&#32508;&#36848;&#21644;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#36129;&#29486;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.13041</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Evaluation Metrics for Machine Translation. (arXiv:2306.13041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20379;&#32508;&#21512;&#32508;&#36848;&#21644;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#36129;&#29486;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35789;&#27719;&#37325;&#21472;&#24230;&#37327;&#65288;&#22914;BLEU&#65289;&#19981;&#21516;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#25351;&#26631;&#65288;&#20363;&#22914;COMET&#25110;BERTScore&#65289;&#22522;&#20110;&#40657;&#30418;&#23376;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36739;&#20302;&#36136;&#37327;&#30340;&#20256;&#32479;&#25351;&#26631;&#20173;&#28982;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#26159;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#26356;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20419;&#36827;&#26032;&#30340;&#39640;&#36136;&#37327;&#25351;&#26631;&#30340;&#26356;&#24191;&#27867;&#25509;&#21463;&#65292;&#35299;&#37322;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#27010;&#24565;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#35299;&#37322;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#30446;&#26631;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#36817;&#25216;&#26415;&#30340;&#32508;&#21512;&#32508;&#36848;&#65292;&#23558;&#23427;&#20204;&#19982;&#25105;&#20204;&#30830;&#31435;&#30340;&#30446;&#26631;&#21644;&#23646;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;GPT4&#65289;&#30340;&#21487;&#35299;&#37322;&#25351;&#26631;&#30340;&#26368;&#26032;&#20808;&#36827;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;e&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#23436;&#20840;&#22312;&#32447;&#30340;IDS&#65292;&#26080;&#38656;&#31163;&#32447;&#23398;&#20064;&#25110;&#20154;&#24037;&#24178;&#39044;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#27969;&#37327;&#30340;&#26102;&#21464;&#29305;&#24615;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#36991;&#20813;&#20102;&#25968;&#25454;&#26631;&#35760;&#20013;&#30340;&#20154;&#20026;&#35823;&#24046;&#65292;&#20855;&#26377;&#20934;&#30830;&#29575;&#39640;&#12289;&#25104;&#26412;&#20302;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.13030</link><description>&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;&#20013;&#22522;&#20110;&#22312;&#32447;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things. (arXiv:2306.13030v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#23436;&#20840;&#22312;&#32447;&#30340;IDS&#65292;&#26080;&#38656;&#31163;&#32447;&#23398;&#20064;&#25110;&#20154;&#24037;&#24178;&#39044;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#27969;&#37327;&#30340;&#26102;&#21464;&#29305;&#24615;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#36991;&#20813;&#20102;&#25968;&#25454;&#26631;&#35760;&#20013;&#30340;&#20154;&#20026;&#35823;&#24046;&#65292;&#20855;&#26377;&#20934;&#30830;&#29575;&#39640;&#12289;&#25104;&#26412;&#20302;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#30417;&#30563;&#20837;&#20405;&#26816;&#27979;&#65288;SSID&#65289;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#23436;&#20840;&#22312;&#32447;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#25110;&#31163;&#32447;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20165;&#22522;&#20110;IDS&#26412;&#36523;&#30340;&#20915;&#31574;&#21644;&#22312;&#32447;&#20272;&#35745;&#30340;&#32479;&#35745;&#21487;&#20449;&#24230;&#65292;&#20351;&#29992;&#33258;&#32852;&#24819;&#28145;&#24230;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#26631;&#35760;&#20256;&#20837;&#30340;&#27969;&#37327;&#25968;&#25454;&#21253;&#12290;SSID&#26694;&#26550;&#20351;IDS&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#32593;&#32476;&#27969;&#37327;&#30340;&#26102;&#21464;&#29305;&#24615;&#65292;&#24182;&#28040;&#38500;&#20102;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#25968;&#25454;&#26631;&#35760;&#20013;&#30340;&#20154;&#20026;&#35823;&#24046;&#65292;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#21644;&#25968;&#25454;&#25910;&#38598;&#30340;&#20154;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#19982;&#30693;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20316;&#20026;&#20934;&#30830;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IDS&#65292;&#35813;SSID&#26694;&#26550;&#38750;&#24120;&#26377;&#29992;&#19988;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Machine Learning (ML) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning. The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness. The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection. This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection. The approach is experimentally evaluated on public datasets and compared with well-known ML models, showing that this SSID framework is very useful and advantageous as an accurate and online learning ML-based IDS for IoT systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#21644;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#65288;DOF-ID&#65289;&#26550;&#26500;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#29992;&#20110;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20174;&#20854;&#20182;&#32593;&#32476;&#31995;&#32479;&#20013;&#33719;&#24471;&#32463;&#39564;&#20197;&#21450;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#32780;&#19981;&#36829;&#21453;&#20854;&#20182;&#31995;&#32479;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27491;&#22312;&#21327;&#20316;&#30340;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13029</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22312;&#32447;&#32852;&#37030; G &#32593;&#32476;&#23398;&#20064;&#29992;&#20110;&#36731;&#37327;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Decentralized Online Federated G-Network Learning for Lightweight Intrusion Detection. (arXiv:2306.13029v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#21644;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#65288;DOF-ID&#65289;&#26550;&#26500;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#29992;&#20110;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20174;&#20854;&#20182;&#32593;&#32476;&#31995;&#32479;&#20013;&#33719;&#24471;&#32463;&#39564;&#20197;&#21450;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#32780;&#19981;&#36829;&#21453;&#20854;&#20182;&#31995;&#32479;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27491;&#22312;&#21327;&#20316;&#30340;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#22411;&#26410;&#30693;&#65288;&#38646;&#26085;&#65289;&#25915;&#20987;&#30340;&#20986;&#29616;&#21644;&#26131;&#21463;&#25915;&#20987;&#30340;&#35774;&#22791;&#30340;&#20852;&#36215;&#65292;&#32593;&#32476;&#31995;&#32479;&#38754;&#20020;&#30528;&#26085;&#30410;&#22686;&#21152;&#30340;&#32593;&#32476;&#25915;&#20987;&#23041;&#32961;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#22312;&#26816;&#27979;&#36825;&#20123;&#25915;&#20987;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#23398;&#20064;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#32463;&#24120;&#38480;&#21046;&#20102;&#20165;&#26377;&#31169;&#26377;&#26412;&#22320;&#25968;&#25454;&#35775;&#38382;&#30340;&#32593;&#32476;&#31995;&#32479;&#24212;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#21644;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#65288;DOF-ID&#65289;&#26550;&#26500;&#12290;DOF-ID &#26159;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#31995;&#32479;&#65292;&#20801;&#35768;&#27599;&#20010;&#29992;&#20110;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20174;&#20854;&#20182;&#32593;&#32476;&#31995;&#32479;&#20013;&#33719;&#24471;&#30340;&#32463;&#39564;&#20197;&#21450;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#32780;&#19981;&#36829;&#21453;&#20854;&#20182;&#31995;&#32479;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#36890;&#36807;&#20351;&#29992;&#20844;&#20849; Kitsune &#21644; Bot-IoT &#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;DOF-ID &#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27491;&#22312;&#21327;&#20316;&#30340;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyberattacks are increasingly threatening networked systems, often with the emergence of new types of unknown (zero-day) attacks and the rise of vulnerable devices. While Machine Learning (ML)-based Intrusion Detection Systems (IDSs) have been shown to be extremely promising in detecting these attacks, the need to learn large amounts of labelled data often limits the applicability of ML-based IDSs to cybersystems that only have access to private local data. To address this issue, this paper proposes a novel Decentralized and Online Federated Learning Intrusion Detection (DOF-ID) architecture. DOF-ID is a collaborative learning system that allows each IDS used for a cybersystem to learn from experience gained in other cybersystems in addition to its own local data without violating the data privacy of other systems. As the performance evaluation results using public Kitsune and Bot-IoT datasets show, DOF-ID significantly improves the intrusion detection performance in all collaborating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PERM&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#23545;&#29615;&#22659;&#38590;&#24230;&#21644;RL&#20195;&#29702;&#30340;&#33021;&#21147;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#21305;&#37197;&#29615;&#22659;&#30340;&#38590;&#24230;&#29983;&#25104;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#21442;&#25968;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#20195;&#29702;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13028</link><description>&lt;p&gt;
&#22522;&#20110;&#38590;&#24230;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#21487;&#36716;&#31227;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Transferable Curricula through Difficulty Conditioned Generators. (arXiv:2306.13028v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PERM&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#23545;&#29615;&#22659;&#38590;&#24230;&#21644;RL&#20195;&#29702;&#30340;&#33021;&#21147;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#21305;&#37197;&#29615;&#22659;&#30340;&#38590;&#24230;&#29983;&#25104;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#21442;&#25968;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#20195;&#29702;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#36229;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#22914;&#26143;&#38469;&#20105;&#38712;&#12289;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#31561;&#31561;&#12290;&#28982;&#32780;&#65292;&#23558;&#20154;&#24037;&#8220;&#19987;&#23478;&#8221;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#20154;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#35838;&#31243;&#20013;&#20351;&#29992;&#35838;&#31243;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36716;&#31227;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#35838;&#31243;&#29983;&#25104;&#26041;&#27861;&#20391;&#37325;&#20110;&#39640;&#25928;&#35757;&#32451;RL&#20195;&#29702;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#20195;&#29702;&#20154;&#36827;&#23637;&#30340;&#20195;&#29702;&#27979;&#37327;&#26631;&#20934;&#65292;&#19981;&#33021;&#29992;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#22521;&#35757;&#26426;&#22120;&#20154;&#65288;&#25110;&#26356;&#26377;&#38596;&#24515;&#30340;&#26159;&#20154;&#31867;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21442;&#25968;&#21270;&#29615;&#22659;&#21709;&#24212;&#27169;&#22411;&#65288;PERM&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#21463;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;PERM&#35797;&#22270;&#30452;&#25509;&#23545;&#29615;&#22659;&#38590;&#24230;&#21644;RL&#20195;&#29702;&#30340;&#33021;&#21147;&#24314;&#27169;&#12290;&#37492;&#20110;RL&#20195;&#29702;&#21644;&#20154;&#31867;&#22312;&#8220;&#21457;&#23637;&#30340;&#37051;&#22495;&#8221;&#20013;&#26356;&#26377;&#25928;&#22320;&#25509;&#21463;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21305;&#37197;&#29615;&#22659;&#30340;&#38590;&#24230;&#20197;&#20135;&#29983;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in reinforcement learning (RL) have demonstrated superhuman performance in complex tasks such as Starcraft, Go, Chess etc. However, knowledge transfer from Artificial "Experts" to humans remain a significant challenge. A promising avenue for such transfer would be the use of curricula. Recent methods in curricula generation focuses on training RL agents efficiently, yet such methods rely on surrogate measures to track student progress, and are not suited for training robots in the real world (or more ambitiously humans). In this paper, we introduce a method named Parameterized Environment Response Model (PERM) that shows promising results in training RL agents in parameterized environments. Inspired by Item Response Theory, PERM seeks to model difficulty of environments and ability of RL agents directly. Given that RL agents and humans are trained more efficiently under the "zone of proximal development", our method generates a curriculum by matching the difficulty of an e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#27604;&#20363;&#30340;&#22686;&#24378;&#23545;&#27604;&#65288;CE&#65289;&#25968;&#25454;&#35757;&#32451;&#23545;DCE-MRI&#22270;&#20687;&#20998;&#21106;&#21644;&#27880;&#20876;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;CE&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#20351;&#29992;&#38750;CE&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#20197;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12988</link><description>&lt;p&gt;
&#19968;&#31181;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#26159;&#21542;&#33021;&#22312;DCE-MRI&#30340;&#25152;&#26377;&#38454;&#27573;&#20013;&#21516;&#31561;&#26377;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can a single image processing algorithm work equally well across all phases of DCE-MRI?. (arXiv:2306.12988v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#27604;&#20363;&#30340;&#22686;&#24378;&#23545;&#27604;&#65288;CE&#65289;&#25968;&#25454;&#35757;&#32451;&#23545;DCE-MRI&#22270;&#20687;&#20998;&#21106;&#21644;&#27880;&#20876;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;CE&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#20351;&#29992;&#38750;CE&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#22411;&#65292;&#20197;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#21644;&#37197;&#20934;&#22312;&#24212;&#29992;&#20110;&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;MRI&#24207;&#21015;&#65288;DCE-MRI&#65289;&#26102;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23545;&#27604;&#21058;&#20250;&#23548;&#33268;&#24863;&#20852;&#36259;&#21306;&#22495;&#21644;&#20854;&#20182;&#21306;&#22495;&#30340;&#24378;&#24230;&#24555;&#36895;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20998;&#21106;&#20219;&#21153;&#30340;&#20551;&#38451;&#24615;&#39044;&#27979;&#24182;&#28151;&#28102;&#22270;&#20687;&#37197;&#20934;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;&#23545;&#27604;&#24230;&#21464;&#21270;&#20250;&#22686;&#21152;&#36825;&#20123;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#20219;&#20309;&#30740;&#31350;&#37327;&#21270;&#36825;&#20123;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#27604;&#20363;&#30340;&#22686;&#24378;&#23545;&#27604;&#65288;CE&#65289;&#25968;&#25454;&#35757;&#32451;&#23545;&#20004;&#20010;&#27969;&#34892;&#20219;&#21153;&#30340;&#24433;&#21709;&#65306;&#20351;&#29992;nnU-Net&#21644;Mask R-CNN&#36827;&#34892;&#20998;&#21106;&#20197;&#21450;&#20351;&#29992;VoxelMorph&#21644;VTN&#36827;&#34892;&#37197;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#36827;&#34892;&#20102;&#31574;&#30053;&#24615;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#20102;&#21019;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#20351;&#29992;CE&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#20351;&#29992;&#38750;CE&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#36825;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#21487;&#33021;&#21487;&#20197;&#25193;&#23637;...
&lt;/p&gt;
&lt;p&gt;
Image segmentation and registration are said to be challenging when applied to dynamic contrast enhanced MRI sequences (DCE-MRI). The contrast agent causes rapid changes in intensity in the region of interest and elsewhere, which can lead to false positive predictions for segmentation tasks and confound the image registration similarity metric. While it is widely assumed that contrast changes increase the difficulty of these tasks, to our knowledge no work has quantified these effects. In this paper we examine the effect of training with different ratios of contrast enhanced (CE) data on two popular tasks: segmentation with nnU-Net and Mask R-CNN and registration using VoxelMorph and VTN. We experimented further by strategically using the available datasets through pretraining and fine tuning with different splits of data. We found that to create a generalisable model, pretraining with CE data and fine tuning with non-CE data gave the best result. This interesting find could be expande
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20108;&#20998;&#29420;&#31435;&#24615;&#65292;&#21033;&#29992;i.i.d.&#27491;&#24577;&#20998;&#24067;&#25968;&#25454;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#26368;&#31934;&#32454;&#30340;&#20114;&#30456;&#29420;&#31435;&#27169;&#24335;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.12984</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#26368;&#31934;&#32454;&#30340;&#20114;&#30456;&#29420;&#31435;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Inferring the finest pattern of mutual independence from data. (arXiv:2306.12984v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12984
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20108;&#20998;&#29420;&#31435;&#24615;&#65292;&#21033;&#29992;i.i.d.&#27491;&#24577;&#20998;&#24067;&#25968;&#25454;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#26368;&#31934;&#32454;&#30340;&#20114;&#30456;&#29420;&#31435;&#27169;&#24335;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38543;&#26426;&#21464;&#37327;X&#65292;&#25105;&#20204;&#23545;&#20854;&#26368;&#31934;&#32454;&#30340;&#20114;&#30456;&#29420;&#31435;&#27169;&#24335;&#956;(X)&#30340;&#30450;&#30446;&#25552;&#21462;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#29420;&#31435;&#24615;&#65292;&#31216;&#20026;&#20108;&#20998;&#30340;&#29420;&#31435;&#24615;&#12290;&#22914;&#26524;&#916;(X)&#20195;&#34920;&#25152;&#26377;&#36866;&#29992;&#20110;X&#30340;&#20108;&#20998;&#29420;&#31435;&#24615;&#27169;&#24335;&#38598;&#65292;&#21017;&#25105;&#20204;&#35777;&#26126;&#956;(X)&#21487;&#20197;&#20316;&#20026;&#916;(X)&#30340;&#25152;&#26377;&#20803;&#32032;&#30340;&#20132;&#38598;&#26469;&#33719;&#24471;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#26381;&#20174;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#30340;&#26465;&#20214;&#19979;&#65292;&#20272;&#35745;&#916;(X)&#12290;&#22914;&#26524;^&#916;(X)&#26159;&#26377;&#25928;&#30340;&#20108;&#20998;&#29420;&#31435;&#27169;&#24335;&#30340;&#20272;&#35745;&#38598;&#65292;&#21017;&#25105;&#20204;&#23558;&#956;(X)&#20272;&#35745;&#20026;^&#916;(X)&#30340;&#25152;&#26377;&#27169;&#24335;&#30340;&#20132;&#38598;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#26126;&#20102;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#29609;&#20855;&#20363;&#23376;&#21644;&#23454;&#39564;&#25968;&#25454;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a random variable $X$, we are interested in the blind extraction of its finest mutual independence pattern $\mu ( X )$. We introduce a specific kind of independence that we call dichotomic. If $\Delta ( X )$ stands for the set of all patterns of dichotomic independence that hold for $X$, we show that $\mu ( X )$ can be obtained as the intersection of all elements of $\Delta ( X )$. We then propose a method to estimate $\Delta ( X )$ when the data are independent and identically (i.i.d.) realizations of a multivariate normal distribution. If $\hat{\Delta} ( X )$ is the estimated set of valid patterns of dichotomic independence, we estimate $\mu ( X )$ as the intersection of all patterns of $\hat{\Delta} ( X )$. The method is tested on simulated data, showing its advantages and limits. We also consider an application to a toy example as well as to experimental data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#38382;&#39064;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#20351;&#29992;&#12290;&#30740;&#31350;&#38598;&#20013;&#20110;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#25104;&#21592;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#35780;&#20272;&#35774;&#32622;&#19981;&#33021;&#24456;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25104;&#21592;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.12983</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#30340;&#26356;&#30495;&#23454;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Towards More Realistic Membership Inference Attacks on Large Diffusion Models. (arXiv:2306.12983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#38382;&#39064;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#20351;&#29992;&#12290;&#30740;&#31350;&#38598;&#20013;&#20110;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#25104;&#21592;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#35780;&#20272;&#35774;&#32622;&#19981;&#33021;&#24456;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25104;&#21592;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;&#31283;&#23450;&#25193;&#25955;&#21644;Midjourney&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#21560;&#24341;&#21147;&#12289;&#22810;&#26679;&#24615;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#25968;&#21313;&#20159;&#20010;&#20114;&#32852;&#32593;&#26469;&#28304;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#28508;&#22312;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#30340;&#37325;&#35201;&#25285;&#24551;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#30830;&#23450;&#29305;&#23450;&#22270;&#20687;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#20351;&#29992;&#20102;&#65292;&#36825;&#22312;&#32593;&#32476;&#23433;&#20840;&#31038;&#21306;&#20013;&#34987;&#31216;&#20026;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#31283;&#23450;&#25193;&#25955;&#65292;&#24182;&#35299;&#20915;&#20102;&#35774;&#35745;&#19968;&#20010;&#20844;&#24179;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#22238;&#31572;&#36825;&#20010;&#25104;&#21592;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24314;&#31435;&#19968;&#20010;&#20844;&#24179;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#31283;&#23450;&#25193;&#25955;&#65292;&#20351;&#28508;&#22312;&#30340;&#25193;&#23637;&#21040;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#21033;&#29992;&#36825;&#20010;&#35780;&#20272;&#35774;&#32622;&#65292;&#25105;&#20204;&#25191;&#34892;&#25104;&#21592;&#25915;&#20987;&#65288;&#21253;&#25324;&#24050;&#30693;&#21644;&#26032;&#24341;&#20837;&#30340;&#25915;&#20987;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#35780;&#20272;&#35774;&#32622;&#19981;&#33021;&#24456;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25104;&#21592;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred to as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a methodology to establish a fair evaluation setup and apply it to Stable Diffusion, enabling potential extensions to other generative models. Utilizing this evaluation setup, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33021;&#37327;&#25910;&#38598;&#30340; RSMA &#31354;&#20013;&#36890;&#20449;&#30340;&#21644;&#36895;&#29575;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26102;&#38388;&#30340;&#26368;&#22823;&#20256;&#36755;&#21151;&#29575;&#21644;&#20351;&#29992;&#24207;&#21015;&#26368;&#23567;&#20108;&#20056;&#35268;&#21010;&#31561;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#35774;&#22791;&#25552;&#20379;&#39640;&#25928;&#30340;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.12977</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33021;&#37327;&#25910;&#38598;&#30340; RSMA &#31354;&#20013;&#36890;&#20449;&#30340;&#21644;&#36895;&#29575;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sum-Rate Maximization of RSMA-based Aerial Communications with Energy Harvesting: A Reinforcement Learning Approach. (arXiv:2306.12977v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33021;&#37327;&#25910;&#38598;&#30340; RSMA &#31354;&#20013;&#36890;&#20449;&#30340;&#21644;&#36895;&#29575;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26102;&#38388;&#30340;&#26368;&#22823;&#20256;&#36755;&#21151;&#29575;&#21644;&#20351;&#29992;&#24207;&#21015;&#26368;&#23567;&#20108;&#20056;&#35268;&#21010;&#31561;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#35774;&#22791;&#25552;&#20379;&#39640;&#25928;&#30340;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25910;&#38598;&#30340;&#33258;&#25345;&#31354;&#20013;&#22522;&#31449;&#20026;&#22810;&#20010;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#36895;&#29575;&#20998;&#35010;&#22810;&#22336; (RSMA) &#31354;&#20013;&#36890;&#20449;&#30340;&#32852;&#21512;&#21151;&#29575;&#21644;&#27874;&#26463;&#36171;&#24418;&#35774;&#35745;&#38382;&#39064;&#12290;&#20026;&#20102;&#20174;&#38271;&#26399;&#35282;&#24230;&#26368;&#22823;&#21270;&#24635;&#36895;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65289;&#65292;&#22522;&#20110;&#20449;&#36947;&#29615;&#22659;&#12289;&#25910;&#38598;&#33021;&#37327;&#21644;&#30005;&#27744;&#30005;&#37327;&#20449;&#24687;&#30340;&#38543;&#26426;&#23646;&#24615;&#65292;&#38480;&#21046;&#27599;&#20010;&#26102;&#38388;&#30340;&#26368;&#22823;&#20256;&#36755;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35774;&#35745; RSMA &#30340;&#25152;&#26377;&#31169;&#26377;/&#20844;&#20849;&#27969;&#30340;&#39044;&#32534;&#30721;&#22120;&#21644;&#21151;&#29575;&#20998;&#37197;&#65292;&#25105;&#20204;&#37319;&#29992;&#24207;&#21015;&#26368;&#23567;&#20108;&#20056;&#35268;&#21010; (SLSQP)&#65292;&#20351;&#29992; Han-Powell &#25311;&#29275;&#39039;&#26041;&#27861;&#36890;&#36807; DRL &#26368;&#22823;&#21270;&#32473;&#23450;&#20256;&#36755;&#21151;&#29575;&#19979;&#30340;&#24635;&#36895;&#29575;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#26696;&#22312;&#24179;&#22343;&#24635;&#36895;&#29575;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this letter, we investigate a joint power and beamforming design problem for rate-splitting multiple access (RSMA)-based aerial communications with energy harvesting, where a self-sustainable aerial base station serves multiple users by utilizing the harvested energy. Considering maximizing the sum-rate from the long-term perspective, we utilize a deep reinforcement learning (DRL) approach, namely the soft actor-critic algorithm, to restrict the maximum transmission power at each time based on the stochastic property of the channel environment, harvested energy, and battery power information. Moreover, for designing precoders and power allocation among all the private/common streams of the RSMA, we employ sequential least squares programming (SLSQP) using the Han-Powell quasi-Newton method to maximize the sum-rate for the given transmission power via DRL. Numerical results show the superiority of the proposed scheme over several baseline methods in terms of the average sum-rate perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#27969;&#30340;&#33258;&#36866;&#24212;&#20271;&#24681;&#26031;&#22374;&#21464;&#21270;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#20934;&#30830;&#22320;&#35782;&#21035;&#21464;&#21270;&#21457;&#29983;&#30340;&#26102;&#38388;&#19982;&#23376;&#31354;&#38388;&#65292;&#24182;&#33021;&#22815;&#37327;&#21270;&#20005;&#37325;&#31243;&#24230;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12974</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#27969;&#33258;&#36866;&#24212;&#20271;&#24681;&#26031;&#22374;&#21464;&#21270;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adaptive Bernstein Change Detector for High-Dimensional Data Streams. (arXiv:2306.12974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#27969;&#30340;&#33258;&#36866;&#24212;&#20271;&#24681;&#26031;&#22374;&#21464;&#21270;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#20934;&#30830;&#22320;&#35782;&#21035;&#21464;&#21270;&#21457;&#29983;&#30340;&#26102;&#38388;&#19982;&#23376;&#31354;&#38388;&#65292;&#24182;&#33021;&#22815;&#37327;&#21270;&#20005;&#37325;&#31243;&#24230;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#25968;&#25454;&#27969;&#26102;&#65292;&#21464;&#21270;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#24555;&#36895;&#21644;&#20934;&#30830;&#22320;&#26816;&#27979;&#21040;&#21464;&#21270;&#21487;&#20197;&#20351;&#30417;&#27979;&#21644;&#39044;&#27979;&#31995;&#32479;&#20570;&#20986;&#21453;&#24212;&#65292;&#20363;&#22914;&#21457;&#20986;&#35686;&#25253;&#25110;&#26356;&#26032;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#26816;&#27979;&#21464;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#65292;&#21464;&#21270;&#26816;&#27979;&#22120;&#19981;&#20165;&#24212;&#33021;&#22815;&#35782;&#21035;&#21464;&#21270;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#36824;&#24212;&#33021;&#22815;&#35782;&#21035;&#21457;&#29983;&#22312;&#21738;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#19988;&#26368;&#22909;&#36824;&#24212;&#33021;&#37327;&#21270;&#23427;&#20204;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ABCd&#26041;&#27861;&#20855;&#22791;&#36825;&#20123;&#29305;&#24615;&#12290;ABCD&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#30417;&#27979;&#20854;&#22312;&#33258;&#36866;&#24212;&#22823;&#23567;&#30340;&#31383;&#21475;&#20869;&#30340;&#20934;&#30830;&#24615;&#12290;ABCD&#26681;&#25454;&#20271;&#24681;&#26031;&#22374;&#19981;&#31561;&#24335;&#35745;&#31639;&#21464;&#21270;&#24471;&#20998;&#65292;&#20197;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20559;&#24046;&#65292;&#25351;&#31034;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ABCD&#22312;&#24179;&#22343;F1-score&#20013;&#33267;&#23569;&#27604;&#20854;&#26368;&#20339;&#31454;&#20105;&#23545;&#25163;&#34920;&#29616;&#20248;&#36234;&#20102;8&#65285;&#65292;&#26368;&#22810;&#21487;&#25552;&#39640;23&#65285;&#12290;&#23427;&#36824;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#21464;&#21270;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#21450;&#19982;&#21464;&#21270;&#22823;&#23567;&#30456;&#20851;&#30340;&#20005;&#37325;&#31243;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection is of fundamental importance when analyzing data streams. Detecting changes both quickly and accurately enables monitoring and prediction systems to react, e.g., by issuing an alarm or by updating a learning algorithm. However, detecting changes is challenging when observations are high-dimensional. In high-dimensional data, change detectors should not only be able to identify when changes happen, but also in which subspace they occur. Ideally, one should also quantify how severe they are. Our approach, ABCD, has these properties. ABCD learns an encoder-decoder model and monitors its accuracy over a window of adaptive size. ABCD derives a change score based on Bernstein's inequality to detect deviations in terms of accuracy, which indicate changes. Our experiments demonstrate that ABCD outperforms its best competitor by at least 8% and up to 23% in F1-score on average. It can also accurately estimate changes' subspace, together with a severity measure that correlates w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#27604;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#39539;&#26021;&#20102;&#26377;&#25928;&#24066;&#22330;&#20551;&#35828;&#65292;&#25903;&#25345;&#28151;&#27788;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2306.12969</link><description>&lt;p&gt;
&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
Stock Price Prediction using Dynamic Neural Networks. (arXiv:2306.12969v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#27604;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#39539;&#26021;&#20102;&#26377;&#25928;&#24066;&#22330;&#20551;&#35828;&#65292;&#25903;&#25345;&#28151;&#27788;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26102;&#38388;&#24207;&#21015;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27599;&#26085;&#25910;&#30424;&#32929;&#31080;&#20215;&#26684;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#28151;&#27788;&#12289;&#38750;&#32447;&#24615;&#21644;&#30475;&#20284;&#38543;&#26426;&#30340;&#25968;&#25454;&#20013;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#25552;&#20379;&#20102;&#27604;&#35768;&#22810;&#24403;&#21069;&#25216;&#26415;&#26356;&#31934;&#20934;&#22320;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21464;&#21160;&#30340;&#26426;&#21046;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#21253;&#25324;&#22522;&#26412;&#38754;&#12289;&#25216;&#26415;&#38754;&#21644;&#22238;&#24402;&#25216;&#26415;&#22312;&#20869;&#30340;&#29616;&#20195;&#32929;&#31080;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#26377;&#25928;&#24066;&#22330;&#20551;&#35828;&#65288;EMH&#65289;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#28151;&#27788;&#29702;&#35770;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#26412;&#25991;&#39539;&#26021;&#20102;EMH&#24182;&#25903;&#25345;&#28151;&#27788;&#29702;&#35770;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22914;&#20309;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will analyze and implement a time series dynamic neural network to predict daily closing stock prices. Neural networks possess unsurpassed abilities in identifying underlying patterns in chaotic, non-linear, and seemingly random data, thus providing a mechanism to predict stock price movements much more precisely than many current techniques. Contemporary methods for stock analysis, including fundamental, technical, and regression techniques, are conversed and paralleled with the performance of neural networks. Also, the Efficient Market Hypothesis (EMH) is presented and contrasted with Chaos theory using neural networks. This paper will refute the EMH and support Chaos theory. Finally, recommendations for using neural networks in stock price prediction will be presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12968</link><description>&lt;p&gt;
&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31751;&#24674;&#22797;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model. (arXiv:2306.12968v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31751;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#24674;&#22797;&#38544;&#34255;&#30340;&#31038;&#32676;&#65292;&#20854;&#20013;&#31751;&#22823;&#23567;&#38543;&#30528;&#29289;&#21697;&#24635;&#25968;$n$&#30340;&#22686;&#38271;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#22312;LSBM&#20013;&#65292;&#20026;&#27599;&#23545;&#29289;&#21697;&#65288;&#29420;&#31435;&#22320;&#65289;&#35266;&#27979;&#21040;&#19968;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#35266;&#27979;&#21040;&#30340;&#26631;&#31614;&#26469;&#24674;&#22797;&#31751;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#26399;&#26395;&#34987;&#20219;&#20309;&#32858;&#31867;&#31639;&#27861;&#35823;&#20998;&#31867;&#30340;&#29289;&#21697;&#25968;&#37327;&#30340;&#23454;&#20363;&#29305;&#23450;&#19979;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#19979;&#37117;&#33021;&#21305;&#37197;&#36825;&#20123;&#19979;&#30028;&#34920;&#29616;&#30340;&#31639;&#27861;&#12290;IAC&#30001;&#19968;&#27425;&#35889;&#32858;&#31867;&#31639;&#27861;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#32452;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#23454;&#20363;&#29305;&#23450;&#30340;&#19979;&#30028;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#21253;&#25324;&#31751;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20165;&#25191;&#34892;&#19968;&#27425;&#35889;&#32858;&#31867;&#65292;IAC&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#37117;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters, where cluster sizes grow linearly with the total number $n$ of items. In the LSBM, a label is (independently) observed for each pair of items. Our objective is to devise an efficient algorithm that recovers clusters using the observed labels. To this end, we revisit instance-specific lower bounds on the expected number of misclassified items satisfied by any clustering algorithm. We present Instance-Adaptive Clustering (IAC), the first algorithm whose performance matches these lower bounds both in expectation and with high probability. IAC consists of a one-time spectral clustering algorithm followed by an iterative likelihood-based cluster assignment improvement. This approach is based on the instance-specific lower bound and does not require any model parameters, including the number of clusters. By performing the spectral clustering only once, IAC m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25552;&#21319;&#20102;&#37329;&#34701;&#39044;&#27979;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#26469;&#22686;&#24378;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#36827;&#34892;&#27969;&#22833;&#39044;&#27979;&#24182;&#35774;&#35745;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#36798;&#21040;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12965</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#37329;&#34701;&#39044;&#27979;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Financial Forecasting via Quantum Machine Learning. (arXiv:2306.12965v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25552;&#21319;&#20102;&#37329;&#34701;&#39044;&#27979;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#26469;&#22686;&#24378;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#36827;&#34892;&#27969;&#22833;&#39044;&#27979;&#24182;&#35774;&#35745;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#36798;&#21040;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26469;&#25913;&#36827;&#37329;&#34701;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#20856;&#21644;&#37327;&#23376;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#26469;&#22686;&#24378;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20197;&#36827;&#34892;&#27969;&#22833;&#39044;&#27979;&#65292;&#25552;&#39640;&#20102;&#36817;6&#65285;&#30340;&#31934;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#27491;&#20132;&#21644;&#22797;&#21512;&#23618;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#36798;&#21040;&#20102;&#19982;&#32463;&#20856;&#24615;&#33021;&#30456;&#24403;&#30340;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#37327;&#23376;&#24605;&#24819;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#30340;&#34920;&#29616;&#65292;&#26080;&#35770;&#26159;&#29616;&#22312;&#20316;&#20026;&#37327;&#23376;&#21551;&#21457;&#24335;&#30340;&#32463;&#20856;ML&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#26159;&#22312;&#26410;&#26469;&#26356;&#22909;&#30340;&#37327;&#23376;&#30828;&#20214;&#30340;&#21040;&#26469;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum algorithms have the potential to enhance machine learning across a variety of domains and applications. In this work, we show how quantum machine learning can be used to improve financial forecasting. First, we use classical and quantum Determinantal Point Processes to enhance Random Forest models for churn prediction, improving precision by almost 6%. Second, we design quantum neural network architectures with orthogonal and compound layers for credit risk assessment, which match classical performance with significantly fewer parameters. Our results demonstrate that leveraging quantum ideas can effectively enhance the performance of machine learning, both today as quantum-inspired classical ML solutions, and even more in the future, with the advent of better quantum hardware.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Alpha&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25366;&#25496;&#21327;&#21516;&#30340;&#20844;&#24335;&#21270;Alpha&#38598;&#21512;&#20197;&#20248;&#21270;&#23427;&#20204;&#20316;&#20026;&#32452;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12964</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21327;&#21516;&#20844;&#24335;&#21270;&#30340;Alpha&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning. (arXiv:2306.12964v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Alpha&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25366;&#25496;&#21327;&#21516;&#30340;&#20844;&#24335;&#21270;Alpha&#38598;&#21512;&#20197;&#20248;&#21270;&#23427;&#20204;&#20316;&#20026;&#32452;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#20132;&#26131;&#39046;&#22495;&#20013;&#65292;&#23558;&#21407;&#22987;&#32929;&#31080;&#21382;&#21490;&#25968;&#25454;&#36716;&#21270;&#20026;&#24066;&#22330;&#36235;&#21183;&#25351;&#31034;&#20449;&#21495;&#26159;&#24120;&#35265;&#30340;&#23454;&#36341;&#12290;&#36825;&#20123;&#20449;&#21495;&#34987;&#31216;&#20026;Alpha&#22240;&#23376;&#12290;&#20844;&#24335;&#24418;&#24335;&#30340;Alpha&#22240;&#23376;&#26356;&#26131;&#20110;&#35299;&#37322;&#65292;&#22240;&#27492;&#39118;&#38505;&#24847;&#35782;&#24378;&#30340;&#23454;&#36341;&#32773;&#26356;&#21916;&#27426;&#23427;&#20204;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24448;&#24448;&#21516;&#26102;&#20351;&#29992;&#19968;&#32452;&#20844;&#24335;&#21270;&#30340;Alpha&#22240;&#23376;&#36827;&#34892;&#26356;&#22909;&#30340;&#24314;&#27169;&#31934;&#24230;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#25214;&#21040;&#33021;&#22815;&#33391;&#22909;&#21327;&#21516;&#30340;&#20844;&#24335;&#21270;Alpha&#38598;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;Alpha&#29983;&#25104;&#22120;&#20998;&#21035;&#25366;&#25496;&#21333;&#20010;Alpha&#65292;&#24573;&#30053;&#20102;&#21518;&#32493;&#30340;Alpha&#32452;&#21512;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Alpha&#25366;&#25496;&#26694;&#26550;&#65292;&#23427;&#20248;&#20808;&#25366;&#25496;&#21327;&#21516;&#30340;Alpha&#38598;&#21512;&#65292;&#21363;&#30452;&#25509;&#20351;&#29992;&#19979;&#28216;&#32452;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#20248;&#21270;Alpha&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#24378;&#22823;&#25506;&#32034;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#20844;&#24335;&#24418;&#24335;&#30340;Alpha&#22240;&#23376;&#30340;&#22823;&#37327;&#25628;&#32034;&#31354;&#38388;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21327;&#21516;&#30340;&#20844;&#24335;&#21270;Alpha&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#23427;&#20204;&#20316;&#20026;&#32452;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;Alpha&#29983;&#25104;&#22120;&#20998;&#21035;&#25366;&#25496;&#21333;&#20010;Alpha&#30340;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning~(RL) to better explore the vast search space of formulaic alphas. The contribution to the combina
&lt;/p&gt;</description></item><item><title>PyKoopman&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;Koopman&#31639;&#23376;&#36924;&#36817;&#30340;Python&#21253;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#24378;&#38750;&#32447;&#24615;&#21160;&#24577;&#30340;&#39044;&#27979;&#12289;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;&#35813;&#21253;&#25552;&#20379;&#20102;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#35782;&#21035;&#24037;&#20855;&#21644;&#19981;&#38656;&#35201;&#26041;&#31243;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#20197;&#21450;&#20854;&#21464;&#31181;&#12290;</title><link>http://arxiv.org/abs/2306.12962</link><description>&lt;p&gt;
PyKoopman: &#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;Koopman&#31639;&#23376;&#36924;&#36817;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
PyKoopman: A Python Package for Data-Driven Approximation of the Koopman Operator. (arXiv:2306.12962v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12962
&lt;/p&gt;
&lt;p&gt;
PyKoopman&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;Koopman&#31639;&#23376;&#36924;&#36817;&#30340;Python&#21253;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#24378;&#38750;&#32447;&#24615;&#21160;&#24577;&#30340;&#39044;&#27979;&#12289;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;&#35813;&#21253;&#25552;&#20379;&#20102;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#35782;&#21035;&#24037;&#20855;&#21644;&#19981;&#38656;&#35201;&#26041;&#31243;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#20197;&#21450;&#20854;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyKoopman&#26159;&#19968;&#20010;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;Koopman&#31639;&#23376;&#36924;&#36817;&#30340;Python&#21253;&#12290;Koopman&#31639;&#23376;&#26159;&#38750;&#32447;&#24615;&#21160;&#24577;&#30340;&#21407;&#29702;&#32447;&#24615;&#23884;&#20837;, &#20351;&#29992;&#32447;&#24615;&#31995;&#32479;&#29702;&#35770;&#21487;&#23454;&#29616;&#23545;&#24378;&#38750;&#32447;&#24615;&#21160;&#24577;&#30340;&#39044;&#27979;&#12289;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;PyKoopman&#25552;&#20379;&#20102;&#29992;&#20110;&#24314;&#27169;&#33258;&#30001;&#21644;&#39537;&#21160;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#35782;&#21035;&#24037;&#20855;, &#36825;&#20123;&#24037;&#20855;&#24314;&#31435;&#22312;&#19981;&#38656;&#35201;&#26041;&#31243;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#21450;&#20854;&#21464;&#31181;&#19978;&#12290;&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;Koopman&#31639;&#23376;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#27010;&#36848;&#24182;&#28436;&#31034;&#20102;PyKoopman&#21253;&#20013;&#23454;&#29616;&#30340;&#21151;&#33021;&#65288;&#21253;&#25324;&#20195;&#30721;&#31034;&#20363;&#65289;&#65292;&#25552;&#20379;&#20102;&#32473;&#29992;&#25143;&#30340;&#23454;&#29992;&#24314;&#35758;&#65292;&#20197;&#21450;PyKoopman&#30340;&#25193;&#23637;&#21015;&#34920;&#12290;&#36719;&#20214;&#21487;&#22312;&#27492;&#32593;&#22336;&#33719;&#24471;&#65306;http URL&#12290;
&lt;/p&gt;
&lt;p&gt;
PyKoopman is a Python package for the data-driven approximation of the Koopman operator associated with a dynamical system. The Koopman operator is a principled linear embedding of nonlinear dynamics and facilitates the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. In particular, PyKoopman provides tools for data-driven system identification for unforced and actuated systems that build on the equation-free dynamic mode decomposition (DMD) and its variants. In this work, we provide a brief description of the mathematical underpinnings of the Koopman operator, an overview and demonstration of the features implemented in PyKoopman (with code examples), practical advice for users, and a list of potential extensions to PyKoopman. Software is available at this http URL
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;Siamese SIREN &#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#36739;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#26469;&#23454;&#29616;&#21331;&#36234;&#30340;&#38899;&#39057;&#37325;&#24314;&#20445;&#30495;&#24230;&#65292;&#25299;&#23637;&#20102;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.12957</link><description>&lt;p&gt;
&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#38899;&#39057;&#21387;&#32553;&#30340;&#36830;&#38145;SIREN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Siamese SIREN: Audio Compression with Implicit Neural Representations. (arXiv:2306.12957v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;Siamese SIREN &#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#36739;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#26469;&#23454;&#29616;&#21331;&#36234;&#30340;&#38899;&#39057;&#37325;&#24314;&#20445;&#30495;&#24230;&#65292;&#25299;&#23637;&#20102;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INRs)&#24050;&#25104;&#20026;&#34920;&#31034;&#22810;&#31181;&#25968;&#25454;&#24418;&#24335;(&#21253;&#25324;3D&#24418;&#29366;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;)&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#36817;&#30740;&#31350;&#24050;&#32463;&#25104;&#21151;&#22320;&#23558;INRs&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;3D&#24418;&#29366;&#30340;&#21387;&#32553;&#20013;&#65292;&#20294;&#23427;&#20204;&#22312;&#38899;&#39057;&#21387;&#32553;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#23545;INRs&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#38899;&#39057;&#21387;&#32553;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#22522;&#20110;&#27969;&#34892;&#30340;SIREN&#26550;&#26500;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;: &#36830;&#38145;SIREN&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;INRs&#26550;&#26500;&#30456;&#27604;&#65292;&#36830;&#38145;SIREN&#22312;&#21033;&#29992;&#26356;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#38899;&#39057;&#37325;&#24314;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representations (INRs) have emerged as a promising method for representing diverse data modalities, including 3D shapes, images, and audio. While recent research has demonstrated successful applications of INRs in image and 3D shape compression, their potential for audio compression remains largely unexplored. Motivated by this, we present a preliminary investigation into the use of INRs for audio compression. Our study introduces Siamese SIREN, a novel approach based on the popular SIREN architecture. Our experimental results indicate that Siamese SIREN achieves superior audio reconstruction fidelity while utilizing fewer network parameters compared to previous INR architectures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21452;&#33258;&#32534;&#30721;&#22120;&#65292;&#23545;&#25506;&#27979;&#22120;&#22270;&#20687;&#36827;&#34892;&#26263;&#29256;&#24378;&#21147;&#20449;&#21495;&#26032;&#29289;&#29702;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#23545;&#25758;&#26426;&#26032;&#29289;&#29702;&#25628;&#32034;&#30340;&#26377;&#25928;&#21644;&#36890;&#29992;&#24037;&#20855;&#65292;&#35777;&#26126;&#20102;AE&#20855;&#26377;&#20986;&#33394;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.12955</link><description>&lt;p&gt;
&#26465;&#20214;&#21452;&#33258;&#32534;&#30721;&#22120;&#35302;&#21457;&#26263;&#28107;&#28020;
&lt;/p&gt;
&lt;p&gt;
Triggering Dark Showers with Conditional Dual Auto-Encoders. (arXiv:2306.12955v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12955
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21452;&#33258;&#32534;&#30721;&#22120;&#65292;&#23545;&#25506;&#27979;&#22120;&#22270;&#20687;&#36827;&#34892;&#26263;&#29256;&#24378;&#21147;&#20449;&#21495;&#26032;&#29289;&#29702;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#23545;&#25758;&#26426;&#26032;&#29289;&#29702;&#25628;&#32034;&#30340;&#26377;&#25928;&#21644;&#36890;&#29992;&#24037;&#20855;&#65292;&#35777;&#26126;&#20102;AE&#20855;&#26377;&#20986;&#33394;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;(AEs)&#26377;&#28508;&#21147;&#25104;&#20026;&#23545;&#25758;&#26426;&#26032;&#29289;&#29702;&#25628;&#32034;&#30340;&#26377;&#25928;&#21644;&#36890;&#29992;&#24037;&#20855;&#65292;&#38656;&#35201;&#24456;&#23569;&#25110;&#19981;&#38656;&#35201;&#27169;&#22411;&#20381;&#36182;&#30340;&#20551;&#35774;&#12290;&#26032;&#30340;&#29702;&#35770;&#29289;&#29702;&#20449;&#21495;&#21487;&#20197;&#20316;&#20026;&#19982;&#36890;&#24120;&#26399;&#26395;&#29992;&#26469;&#25551;&#36848;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#24050;&#30693;&#32972;&#26223;&#36807;&#31243;&#20559;&#31163;&#30340;&#24322;&#24120;&#20540;&#26469;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;AE&#23450;&#20041;&#21028;&#23450;&#20107;&#20214;&#29289;&#29702;&#26412;&#36136;&#30340;&#20934;&#21017;&#30340;&#24322;&#24120;&#26816;&#27979;(AD)&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#25506;&#27979;&#22120;&#22270;&#20687;&#36827;&#34892;AD&#25628;&#32034;&#65292;&#23545;&#20110;&#26080;&#27861;&#21033;&#29992;&#20219;&#20309;&#22522;&#20110;&#29289;&#29702;&#30340;&#39044;&#22788;&#29702;&#25110;&#23545;&#20449;&#21495;&#30340;&#20551;&#35774;&#30340;&#22823;&#32780;&#31232;&#30095;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;&#26263;&#29256;&#24378;&#21147;&#30340;&#34920;&#29616;&#24418;&#24335;&#30340;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36890;&#36807;&#26465;&#20214;&#23398;&#20064;&#32039;&#20945;&#28508;&#22312;&#31354;&#38388;&#30340;&#21452;&#32534;&#30721;&#22120;&#35774;&#35745;&#12290;&#22312;&#22810;&#20010;AD&#25351;&#26631;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#27604;&#31454;&#20105;&#22522;&#32447;&#21644;&#20808;&#21069;&#26041;&#27861;&#30340;&#26126;&#26174;&#25913;&#36827;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23637;&#31034;AE&#20855;&#26377;&#20986;&#33394;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-encoders (AEs) have the potential to be effective and generic tools for new physics searches at colliders, requiring little to no model-dependent assumptions. New hypothetical physics signals can be considered anomalies that deviate from the well-known background processes generally expected to describe the whole dataset. We present a search formulated as an anomaly detection (AD) problem, using an AE to define a criterion to decide about the physics nature of an event. In this work, we perform an AD search for manifestations of a dark version of strong force using raw detector images, which are large and very sparse, without leveraging any physics-based pre-processing or assumption on the signals. We propose a dual-encoder design which can learn a compact latent space through conditioning. In the context of multiple AD metrics, we present a clear improvement over competitive baselines and prior approaches. It is the first time that an AE is shown to exhibit excellent discriminati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#35745;&#31639;&#22270;&#31639;&#27861;&#65288;ECGs&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#38024;&#23545;&#24322;&#36136;&#24615;&#25968;&#25454;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#65292;&#36890;&#36807;&#37325;&#36830;GNN&#30340;&#35745;&#31639;&#22270;&#22686;&#21152;&#36830;&#25509;&#21516;&#19968;&#31867;&#33410;&#28857;&#30340;&#36793;&#32536;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12943</link><description>&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#22270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evolving Computation Graphs. (arXiv:2306.12943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#35745;&#31639;&#22270;&#31639;&#27861;&#65288;ECGs&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#38024;&#23545;&#24322;&#36136;&#24615;&#25968;&#25454;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#65292;&#36890;&#36807;&#37325;&#36830;GNN&#30340;&#35745;&#31639;&#22270;&#22686;&#21152;&#36830;&#25509;&#21516;&#19968;&#31867;&#33410;&#28857;&#30340;&#36793;&#32536;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#31995;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#37027;&#20123;&#34920;&#29616;&#20986;&#21516;&#36136;&#24615;&#30340;&#25968;&#25454;&#65306;&#24403;&#33410;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#24448;&#24448;&#26263;&#31034;&#23427;&#20204;&#23646;&#20110;&#21516;&#19968;&#31867;&#26102;&#65292;&#36825;&#19968;&#28857;&#26356;&#20026;&#26126;&#26174;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20010;&#20551;&#35774;&#22312;&#35768;&#22810;&#30456;&#20851;&#24773;&#20917;&#19979;&#26159;&#25104;&#31435;&#30340;&#65292;&#20294;&#22312;&#37325;&#35201;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#20063;&#26377;&#36829;&#21453;&#36825;&#20010;&#20551;&#35774;&#30340;&#24773;&#20917;&#65292;&#36825;&#20419;&#36827;&#20102;&#23545;GNN&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#36827;&#34892;&#25913;&#36827;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#36827;&#21270;&#35745;&#31639;&#22270;&#31639;&#27861;&#65288;ECGs&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#38024;&#23545;&#24322;&#36136;&#24615;&#25968;&#25454;&#30340;GNN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#20808;&#21069;&#30340;&#29702;&#35770;&#27934;&#35265;&#20043;&#19978;&#65292;&#23558;&#33410;&#28857;&#24230;&#12289;&#39640;&#21516;&#36136;&#24615;&#21644;&#20869;&#37096;&#19982;&#31867;&#38388;&#23884;&#20837;&#30456;&#20284;&#24615;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#37325;&#36830;GNN&#30340;&#35745;&#31639;&#22270;&#26469;&#22686;&#21152;&#36830;&#25509;&#21516;&#19968;&#31867;&#33410;&#28857;&#30340;&#36793;&#32536;&#12290;&#25105;&#20204;&#21033;&#29992;&#36739;&#24369;&#30340;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#36825;&#20123;&#36793;&#32536;&#65292;&#20174;&#32780;&#26368;&#32456;&#25552;&#39640;&#20102;GNN&#23545;&#38750;&#21516;&#36136;&#24615;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;ECGs&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#26368;&#36817;&#25552;&#20986;&#30340;&#24322;&#36136;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have demonstrated success in modeling relational data, especially for data that exhibits homophily: when a connection between nodes tends to imply that they belong to the same class. However, while this assumption is true in many relevant situations, there are important real-world scenarios that violate this assumption, and this has spurred research into improving GNNs for these cases. In this work, we propose Evolving Computation Graphs (ECGs), a novel method for enhancing GNNs on heterophilic datasets. Our approach builds on prior theoretical insights linking node degree, high homophily, and inter vs intra-class embedding similarity by rewiring the GNNs' computation graph towards adding edges that connect nodes that are likely to be in the same class. We utilise weaker classifiers to identify these edges, ultimately improving GNN performance on non-homophilic data as a result. We evaluate ECGs on a diverse set of recently-proposed heterophilous datasets a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#25915;&#20987;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#21487;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12941</link><description>&lt;p&gt;
&#40065;&#26834;&#35821;&#20041;&#20998;&#21106;&#65306;&#24378;&#40065;&#26834;&#24615;&#25915;&#20987;&#21644;&#24555;&#36895;&#35757;&#32451;&#40065;&#26834;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models. (arXiv:2306.12941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#25915;&#20987;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#21487;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#37327;&#30340;&#24037;&#20316;&#24050;&#32463;&#38598;&#20013;&#22312;&#35774;&#35745;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19978;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#23384;&#22312;&#29992;&#20110;&#25915;&#20987;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#20998;&#21106;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#35780;&#20272;&#21327;&#35758;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#39640;&#20272;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#33267;&#20170;&#26368;&#25104;&#21151;&#30340;&#33719;&#24471;&#40065;&#26834;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#35201;&#23398;&#20064;&#30340;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#27604;&#22270;&#20687;&#20998;&#31867;&#26356;&#39640;&#30340;&#35745;&#31639;&#37327;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#40065;&#26834;ImageNet&#20998;&#31867;&#22120;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a large amount of work has focused on designing adversarial attacks against image classifiers, only a few methods exist to attack semantic segmentation models. We show that attacking segmentation models presents task-specific challenges, for which we propose novel solutions. Our final evaluation protocol outperforms existing methods, and shows that those can overestimate the robustness of the models. Additionally, so far adversarial training, the most successful way for obtaining robust image classifiers, could not be successfully applied to semantic segmentation. We argue that this is because the task to be learned is more challenging, and requires significantly higher computational effort than for image classification. As a remedy, we show that by taking advantage of recent advances in robust ImageNet classifiers, one can train adversarially robust segmentation models at limited computational cost by fine-tuning robust backbones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12929</link><description>&lt;p&gt;
&#21487;&#37327;&#21270;Transformer&#65306;&#36890;&#36807;&#24110;&#21161;&#27880;&#24847;&#21147;&#22836;&#8220;&#20160;&#20040;&#20063;&#19981;&#20570;&#8221;&#21435;&#38500;&#31163;&#32676;&#20540;
&lt;/p&gt;
&lt;p&gt;
Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;Transformer&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#30001;&#20110;&#20854;&#35268;&#27169;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#65292;&#20294;&#36825;&#26159;&#20197;&#26497;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;&#37327;&#21270;&#26159;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26102;&#38388;&#21644;&#23384;&#20648;&#22120;&#28040;&#32791;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;transformer&#27169;&#22411;&#24448;&#24448;&#23398;&#20064;&#21040;&#20854;&#28608;&#27963;&#20013;&#30340;&#24378;&#31163;&#32676;&#20540;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37327;&#21270;&#12290;&#20026;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#31163;&#32676;&#20540;&#30340;&#23384;&#22312;&#38656;&#35201;&#23558;&#28608;&#27963;&#32622;&#20110;&#26356;&#39640;&#30340;&#27604;&#29305;&#23485;&#24230;&#25110;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#23383;&#26684;&#24335;&#65292;&#36827;&#34892;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#20854;&#20182;&#21464;&#36890;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24378;&#31163;&#32676;&#20540;&#19982;&#29305;&#23450;&#27880;&#24847;&#22836;&#34892;&#20026;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#22836;&#35797;&#22270;&#23398;&#20064;&#8220;&#26080;&#25805;&#20316;&#8221;&#25110;&#20165;&#20165;&#26159;&#37096;&#20998;&#27531;&#24046;&#26356;&#26032;&#12290;&#20026;&#20102;&#23454;&#29616;&#27880;&#24847;&#21147;&#22836;&#20013;&#38656;&#35201;&#30340;&#31934;&#30830;&#38646;&#20301;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#25945;&#25480;&#27880;&#24847;&#21147;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#30340;&#37327;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#37327;&#21270;&#29978;&#33267;&#26159;&#24378;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#20840;&#23616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21327;&#35843;&#34892;&#21160;&#65292;&#23454;&#29616;&#26356;&#24555;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12926</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#29366;&#24577;&#39044;&#27979;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#20840;&#23616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21327;&#35843;&#34892;&#21160;&#65292;&#23454;&#29616;&#26356;&#24555;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#25511;&#21046;&#21333;&#20010;&#26426;&#22120;&#20154;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23558;DRL&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#32676;&#20307;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38750;&#38745;&#24577;&#24615;&#65292;&#21363;&#24403;&#20004;&#20010;&#25110;&#26356;&#22810;&#26426;&#22120;&#20154;&#21516;&#26102;&#26356;&#26032;&#20010;&#20307;&#25110;&#20849;&#20139;&#25919;&#31574;&#26102;&#65292;&#20250;&#36827;&#20837;&#19968;&#20010;&#30456;&#20114;&#20381;&#23384;&#30340;&#22521;&#35757;&#36807;&#31243;&#65292;&#24182;&#19988;&#19981;&#20445;&#35777;&#25910;&#25947;&#12290;&#20811;&#26381;&#38750;&#38745;&#24577;&#24615;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#65292;&#20363;&#22914;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;/&#25110;&#34892;&#21160;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#28040;&#38500;&#20840;&#23616;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#20449;&#24687;&#20307;&#30340;&#20840;&#23616;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#25551;&#36848;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#20197;&#38598;&#20307;&#36816;&#36755;&#20026;&#27979;&#35797;&#22330;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#22810;&#26234;&#33021;&#20307;&#22521;&#35757;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#20154;&#19981;&#20132;&#25442;&#20449;&#24687;&#65292;&#24182;&#19988;&#34987;&#35757;&#32451;&#20381;&#38752;&#36890;&#36807;&#25512;&#65288;push&#65289;&#21644;&#25289;&#65288;pull&#65289;&#29289;&#20307;&#36827;&#34892;&#38544;&#24335;&#36890;&#20449;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#20154;&#24444;&#27492;&#20849;&#20139;&#29366;&#24577;&#39044;&#27979;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#21327;&#35843;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20849;&#20139;&#39044;&#27979;&#21487;&#20197;&#20351;&#26234;&#33021;&#20307;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#38656;&#35201;&#26356;&#23569;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#30028;&#38754;&#65292;&#21487;&#20197;&#24110;&#21161;&#19987;&#19994;&#39046;&#22495;&#30340;&#19987;&#23478;&#24555;&#36895;&#36816;&#34892;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#25968;&#25454;&#26032;&#31867;&#21035;&#21457;&#29616;&#31639;&#27861;&#65292;&#29983;&#25104;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12919</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#30028;&#38754;&#30340;&#34920;&#26684;&#25968;&#25454;&#26032;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interactive Interface for Novel Class Discovery in Tabular Data. (arXiv:2306.12919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12919
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#30028;&#38754;&#65292;&#21487;&#20197;&#24110;&#21161;&#19987;&#19994;&#39046;&#22495;&#30340;&#19987;&#23478;&#24555;&#36895;&#36816;&#34892;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#25968;&#25454;&#26032;&#31867;&#21035;&#21457;&#29616;&#31639;&#27861;&#65292;&#29983;&#25104;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#31867;&#21035;&#21457;&#29616;(Novel Class Discovery, NCD) &#26159;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21512;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#26631;&#35760;&#30340;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21512;&#36827;&#34892;&#23547;&#25214;&#26032;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#30340; NCD &#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#65292;&#32780;&#34920;&#26684;&#25968;&#25454;&#21017;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#20026;&#20102;&#35299;&#37322;&#32858;&#31867;&#25110; NCD &#31639;&#27861;&#30340;&#32467;&#26524;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#38656;&#35201;&#29702;&#35299;&#34920;&#26684;&#25968;&#25454;&#30340;&#39046;&#22495;&#21644;&#24212;&#29992;&#29305;&#23450;&#23646;&#24615;&#65292;&#36825;&#39033;&#20219;&#21153;&#24448;&#24448;&#21482;&#33021;&#30001;&#19987;&#19994;&#39046;&#22495;&#30340;&#19987;&#23478;&#26469;&#23436;&#25104;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#20132;&#20114;&#30028;&#38754;&#20801;&#35768;&#19987;&#19994;&#39046;&#22495;&#30340;&#19987;&#23478;&#36731;&#26494;&#36816;&#34892;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454; NCD &#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#30340;&#25968;&#25454;&#31185;&#23398;&#30693;&#35782;&#65292;&#20063;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel Class Discovery (NCD) is the problem of trying to discover novel classes in an unlabeled set, given a labeled set of different but related classes. The majority of NCD methods proposed so far only deal with image data, despite tabular data being among the most widely used type of data in practical applications. To interpret the results of clustering or NCD algorithms, data scientists need to understand the domain- and application-specific attributes of tabular data. This task is difficult and can often only be performed by a domain expert. Therefore, this interface allows a domain expert to easily run state-of-the-art algorithms for NCD in tabular data. With minimal knowledge in data science, interpretable results can be generated.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CAD&#24341;&#25806;&#21644;&#28145;&#24230;&#23398;&#20064;&#29289;&#29702;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#31471;&#21040;&#31471;&#22320;&#35780;&#20272;&#35774;&#35745;&#36845;&#20195;&#24182;&#36827;&#34892;&#22810;&#30446;&#26631;&#33337;&#22411;&#20248;&#21270;&#65292;&#20854;&#20013;&#34164;&#21547;&#30528;&#33021;&#22815;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#27969;&#39044;&#27979;&#30340;DLP&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12915</link><description>&lt;p&gt;
&#22522;&#20110;CAD&#24341;&#25806;&#30340;&#28145;&#24230;&#23398;&#20064;&#29289;&#29702;&#30340;&#22810;&#30446;&#26631;&#33337;&#22411;&#20248;&#21270;&#19982;&#19977;&#32500;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Hull Form Optimization with CAD Engine-based Deep Learning Physics for 3D Flow Prediction. (arXiv:2306.12915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CAD&#24341;&#25806;&#21644;&#28145;&#24230;&#23398;&#20064;&#29289;&#29702;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#31471;&#21040;&#31471;&#22320;&#35780;&#20272;&#35774;&#35745;&#36845;&#20195;&#24182;&#36827;&#34892;&#22810;&#30446;&#26631;&#33337;&#22411;&#20248;&#21270;&#65292;&#20854;&#20013;&#34164;&#21547;&#30528;&#33021;&#22815;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#27969;&#39044;&#27979;&#30340;DLP&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Deep Learning Physics Optimization (DLPO)&#26694;&#26550;&#26469;&#24314;&#31435;&#26460;&#20234;&#26031;&#22561;&#27979;&#35797;&#26696;&#20363;&#65288;DTC&#65289;&#38598;&#35013;&#31665;&#33337;&#30340;&#24418;&#29366;&#20248;&#21270;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24212;&#29992;&#65306;&#65288;1&#65289;&#25935;&#24863;&#24615;&#20998;&#26512;&#20197;&#26816;&#27979;&#26368;&#26377;&#21069;&#36884;&#30340;&#22522;&#30784;&#33337;&#20307;&#24418;&#29366;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22810;&#30446;&#26631;&#20248;&#21270;&#20197;&#37327;&#21270;&#26368;&#20339;&#33337;&#20307;&#24418;&#24335;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;DLPO&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#31471;&#21040;&#31471;&#22320;&#35780;&#20272;&#35774;&#35745;&#36845;&#20195;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;Extrality&#30340;Deep Learning Physics&#65288;DLP&#65289;&#27169;&#22411;&#19982;CAD&#24341;&#25806;&#21644;&#20248;&#21270;&#22120;&#32806;&#21512;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DLP&#27169;&#22411;&#26159;&#36890;&#36807;&#26469;&#33258;RANS&#27169;&#25311;&#30340;&#23436;&#25972;&#19977;&#32500;&#20307;&#31215;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#32780;&#39640;&#36136;&#37327;&#30340;&#23454;&#26102;&#19977;&#32500;&#27969;&#39044;&#27979;&#65292;&#20351;&#20854;&#25104;&#20026;&#30456;&#23545;&#20110;&#27700;&#21160;&#21147;&#25928;&#29575;&#36827;&#34892;&#26032;&#38598;&#35013;&#31665;&#33337;&#35774;&#35745;&#20248;&#21270;&#30340;&#20248;&#31168;&#35780;&#20272;&#22120;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#33337;&#20307;&#34920;&#38754;&#30340;&#39640;&#31934;&#24230;&#31215;&#20998;&#26469;&#24674;&#22797;&#20316;&#29992;&#22312;&#33337;&#19978;&#30340;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a built-in Deep Learning Physics Optimization (DLPO) framework to set up a shape optimization study of the Duisburg Test Case (DTC) container vessel. We present two different applications: (1) sensitivity analysis to detect the most promising generic basis hull shapes, and (2) multi-objective optimization to quantify the trade-off between optimal hull forms. DLPO framework allows for the evaluation of design iterations automatically in an end-to-end manner. We achieved these results by coupling Extrality's Deep Learning Physics (DLP) model to a CAD engine and an optimizer. Our proposed DLP model is trained on full 3D volume data coming from RANS simulations, and it can provide accurate and high-quality 3D flow predictions in real-time, which makes it a good evaluator to perform optimization of new container vessel designs w.r.t the hydrodynamic efficiency. In particular, it is able to recover the forces acting on the vessel by integration on the hull surface wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20445;&#38505;&#19994;&#20381;&#36182;&#20010;&#20154;&#25935;&#24863;&#29305;&#24449;&#39044;&#27979;&#39118;&#38505;&#23481;&#26131;&#36896;&#25104;&#27495;&#35270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20351;&#29992;Wasserstein&#37325;&#24515;&#32531;&#35299;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12912</link><description>&lt;p&gt;
&#20351;&#29992;Wasserstein&#37325;&#24515;&#32531;&#35299;&#20445;&#38505;&#20013;&#30340;&#27495;&#35270;
&lt;/p&gt;
&lt;p&gt;
Mitigating Discrimination in Insurance with Wasserstein Barycenters. (arXiv:2306.12912v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20445;&#38505;&#19994;&#20381;&#36182;&#20010;&#20154;&#25935;&#24863;&#29305;&#24449;&#39044;&#27979;&#39118;&#38505;&#23481;&#26131;&#36896;&#25104;&#27495;&#35270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20351;&#29992;Wasserstein&#37325;&#24515;&#32531;&#35299;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#38505;&#19994;&#20005;&#37325;&#20381;&#36182;&#20110;&#26681;&#25454;&#28508;&#22312;&#23458;&#25143;&#30340;&#29305;&#24449;&#39044;&#27979;&#39118;&#38505;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#27169;&#22411;&#24456;&#24120;&#35265;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#25351;&#20986;&#65292;&#36825;&#31181;&#20570;&#27861;&#20250;&#22240;&#22522;&#20110;&#25935;&#24863;&#29305;&#24449;&#65288;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#32780;&#20135;&#29983;&#27495;&#35270;&#12290;&#37492;&#20110;&#36825;&#31181;&#27495;&#35270;&#36890;&#24120;&#21487;&#20197;&#24402;&#22240;&#20110;&#21382;&#21490;&#25968;&#25454;&#20559;&#35265;&#65292;&#22240;&#27492;&#28040;&#38500;&#25110;&#33267;&#23569;&#32531;&#35299;&#27495;&#35270;&#26159;&#21487;&#21462;&#30340;&#12290;&#38543;&#30528;&#20174;&#26356;&#20256;&#32479;&#30340;&#27169;&#22411;&#36716;&#21521;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#65292;&#23545;&#26356;&#22823;&#30340;&#32531;&#35299;&#21628;&#22768;&#20063;&#22312;&#22686;&#21152;&#65292;&#22240;&#20026;&#20165;&#20165;&#25490;&#38500;&#20215;&#26684;&#36807;&#31243;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#34987;&#35777;&#26126;&#26159;&#26080;&#25928;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20026;&#20160;&#20040;&#39044;&#27979;&#22312;&#34892;&#19994;&#20869;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#32416;&#27491;&#20559;&#35265;&#24182;&#19981;&#20687;&#31616;&#21333;&#22320;&#35782;&#21035;&#25935;&#24863;&#21464;&#37327;&#37027;&#20040;&#30452;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#32553;&#25918;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#21644;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The insurance industry is heavily reliant on predictions of risks based on characteristics of potential customers. Although the use of said models is common, researchers have long pointed out that such practices perpetuate discrimination based on sensitive features such as gender or race. Given that such discrimination can often be attributed to historical data biases, an elimination or at least mitigation is desirable. With the shift from more traditional models to machine-learning based predictions, calls for greater mitigation have grown anew, as simply excluding sensitive variables in the pricing process can be shown to be ineffective. In this article, we first investigate why predictions are a necessity within the industry and why correcting biases is not as straightforward as simply identifying a sensitive variable. We then propose to ease the biases through the use of Wasserstein barycenters instead of simple scaling. To demonstrate the effects and effectiveness of the approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#20301;&#26694;&#26550;&#65292;&#22312;&#24322;&#26500;&#38598;&#32676;&#19978;&#21551;&#29992;&#35757;&#32451;&#21644;&#25512;&#29702;&#24037;&#20316;&#27969;&#31243;&#12290;&#37319;&#29992;SmartSim&#37096;&#32626;&#25968;&#25454;&#24211;&#65292;&#23384;&#20648;&#25968;&#25454;&#21644;ML&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#25991;&#20214;&#31995;&#32479;&#38382;&#39064;&#12290;&#22312;Polaris&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#23637;&#31034;&#20102;&#23436;&#32654;&#30340;&#25193;&#23637;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#20013;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#39564;&#35777;&#20102;&#26694;&#26550;&#24320;&#38144;&#30340;&#21487;&#24573;&#30053;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12900</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#21512;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21407;&#20301;&#26694;&#26550;&#21450;&#20854;&#22312;CFD&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
In Situ Framework for Coupling Simulation and Machine Learning with Application to CFD. (arXiv:2306.12900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#20301;&#26694;&#26550;&#65292;&#22312;&#24322;&#26500;&#38598;&#32676;&#19978;&#21551;&#29992;&#35757;&#32451;&#21644;&#25512;&#29702;&#24037;&#20316;&#27969;&#31243;&#12290;&#37319;&#29992;SmartSim&#37096;&#32626;&#25968;&#25454;&#24211;&#65292;&#23384;&#20648;&#25968;&#25454;&#21644;ML&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#25991;&#20214;&#31995;&#32479;&#38382;&#39064;&#12290;&#22312;Polaris&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#23637;&#31034;&#20102;&#23436;&#32654;&#30340;&#25193;&#23637;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#20013;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#39564;&#35777;&#20102;&#26694;&#26550;&#24320;&#38144;&#30340;&#21487;&#24573;&#30053;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#20419;&#36827;&#27969;&#20307;&#21160;&#21147;&#23398;&#35745;&#31639;&#26041;&#38754;&#26377;&#35768;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#27169;&#25311;&#30340;&#22686;&#38271;&#65292;&#20026;&#20256;&#32479;&#30340;&#31163;&#32447;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20250;&#36896;&#25104;I/O&#21644;&#23384;&#20648;&#29942;&#39048;&#12290;&#27492;&#22806;&#65292;&#22312;&#36816;&#34892;&#26102;&#25191;&#34892;&#25512;&#29702;&#38656;&#35201;&#23558;ML&#26694;&#26550;&#24211;&#19982;&#27169;&#25311;&#20195;&#30721;&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#32806;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#31616;&#21270;&#36825;&#31181;&#32806;&#21512;&#24182;&#22312;&#24322;&#26500;&#38598;&#32676;&#19978;&#21551;&#29992;&#21407;&#20301;&#35757;&#32451;&#21644;&#25512;&#29702;&#24037;&#20316;&#27969;&#31243;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#21033;&#29992;SmartSim&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#37096;&#32626;&#20102;&#19968;&#20010;&#25968;&#25454;&#24211;&#23558;&#25968;&#25454;&#21644;ML&#27169;&#22411;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#25991;&#20214;&#31995;&#32479;&#38382;&#39064;&#12290;&#22312;Polaris&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#25968;&#25454;&#24211;&#37096;&#32626;&#22312;&#26032;&#20301;&#32622;&#20174;&#32780;&#21462;&#24471;&#25104;&#20493;&#22686;&#38271;&#30340;&#25968;&#25454;&#20256;&#36755;&#21644;&#25512;&#26029;&#25104;&#26412;&#30340;&#23436;&#32654;&#25193;&#23637;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#20013;&#23454;&#26102;&#30740;&#31350;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35757;&#32451;&#65292;&#26174;&#31034;&#20986;&#26694;&#26550;&#24320;&#38144;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#21487;&#24573;&#30053;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen many successful applications of machine learning (ML) to facilitate fluid dynamic computations. As simulations grow, generating new training datasets for traditional offline learning creates I/O and storage bottlenecks. Additionally, performing inference at runtime requires non-trivial coupling of ML framework libraries with simulation codes. This work offers a solution to both limitations by simplifying this coupling and enabling in situ training and inference workflows on heterogeneous clusters. Leveraging SmartSim, the presented framework deploys a database to store data and ML models in memory, thus circumventing the file system. On the Polaris supercomputer, we demonstrate perfect scaling efficiency to the full machine size of the data transfer and inference costs thanks to a novel co-located deployment of the database. Moreover, we train an autoencoder in situ from a turbulent flow simulation, showing that the framework overhead is negligible relative to a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12898</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;
&lt;/p&gt;
&lt;p&gt;
Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots. (arXiv:2306.12898v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32452;&#35013;&#30340;InAs / GaAs&#37327;&#23376;&#28857;&#65288;QDs&#65289;&#20855;&#26377;&#29992;&#20110;&#24320;&#21457;&#21508;&#31181;&#20809;&#30005;&#23376;&#22120;&#20214;&#30340;&#26497;&#39640;&#20215;&#20540;&#12290;&#24314;&#31435;&#29305;&#23450;&#23494;&#24230;&#30340;QDs&#30340;&#36807;&#31243;&#21442;&#25968;&#26159;&#19968;&#20010;&#22810;&#32500;&#20248;&#21270;&#25361;&#25112;&#65292;&#36890;&#24120;&#36890;&#36807;&#32791;&#26102;&#21644;&#36845;&#20195;&#30340;&#35797;&#38169;&#26469;&#35299;&#20915;&#12290;&#22312;&#27492;&#65292;&#20316;&#32773;&#20351;&#29992;&#22522;&#20110;3D ResNet&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#19987;&#38376;&#35757;&#32451;RHEED&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#34920;&#38754;&#24418;&#35980;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable for developing various optoelectronic devices such as QD lasers and single photon sources. The applications strongly rely on the density and quality of these dots, which has motivated studies of the growth process control to realize high-quality epi-wafers and devices. Establishing the process parameters in molecular beam epitaxy (MBE) for a specific density of QDs is a multidimensional optimization challenge, usually addressed through time-consuming and iterative trial-and-error. Meanwhile, reflective high-energy electron diffraction (RHEED) has been widely used to capture a wealth of growth information in situ. However, it still faces the challenges of extracting information from noisy and overlapping images. Here, based on 3D ResNet, we developed a machine learning (ML) model specially designed for training RHEED videos instead of static images and providing real-time feedback on surface morphologies for pro
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.12873</link><description>&lt;p&gt;
FuXi: &#19968;&#20010;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12873
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;0.25&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;10&#22825;&#22825;&#27668;&#39044;&#25253;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#27604;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;(ECMWF)&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;(HRES)&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#19982;ECMWF&#38598;&#21512;&#24179;&#22343;(EM)&#30456;&#24403;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32531;&#35299;&#39044;&#25253;&#35823;&#24046;&#30340;&#31215;&#32047;&#23545;&#20110;&#26377;&#25928;&#30340;&#38271;&#26399;&#39044;&#25253;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20943;&#23569;&#31215;&#32047;&#35823;&#24046;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#33258;&#22238;&#24402;&#22810;&#26102;&#38388;&#27493;&#38271;&#25439;&#22833;&#65292;&#20294;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21457;&#29616;&#26080;&#27861;&#22312;&#30701;&#21644;&#38271;&#23548;&#20986;&#26102;&#38388;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuXi&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#20998;&#36776;&#29575;&#20026;0.25&#24230;&#12289;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;6&#23567;&#26102;&#30340;15&#22825;&#20840;&#29699;&#39044;&#27979;&#12290;FuXi&#22522;&#20110;&#32423;&#32852;&#38598;&#21512;&#27169;&#22411;&#24320;&#21457;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#20943;&#23569;&#20102;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#12290;&#20351;&#29992;&#31354;&#27668;&#28201;&#24230;&#65292;&#27604;&#28287;&#24230;&#21644;&#20301;&#21183;&#39640;&#24230;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#21644;&#24322;&#24120;&#30456;&#20851;&#31995;&#25968;(ACC)&#35780;&#20272;&#20102;FuXi&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;ECMWF HRES&#30456;&#27604;&#65292;FuXi&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#31215;&#32047;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#30340;&#39118;&#22122;&#22768;&#38477;&#22122;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#24471;&#39118;&#22122;&#22768;&#38477;&#22122;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21644;&#32431;&#39044;&#27979;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12867</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#30340;&#39118;&#22122;&#22768;&#38477;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model. (arXiv:2306.12867v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#30340;&#39118;&#22122;&#22768;&#38477;&#22122;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#24471;&#39118;&#22122;&#22768;&#38477;&#22122;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21644;&#32431;&#39044;&#27979;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21333;&#36890;&#36947;&#39118;&#22122;&#38899;&#38477;&#20302;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#25105;&#20204;&#20808;&#21069;&#25552;&#20986;&#30340;&#32467;&#21512;&#39044;&#27979;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38750;&#21152;&#24615;&#30340;&#35821;&#38899;&#22122;&#22768;&#27169;&#22411;&#26469;&#35299;&#37322;&#33180;&#30340;&#38750;&#32447;&#24615;&#21464;&#24418;&#65292;&#36825;&#31181;&#21464;&#24418;&#26159;&#30001;&#39118;&#27969;&#21644;&#21487;&#33021;&#30340;&#21098;&#35009;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#22312;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39118;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#20197;&#21450;&#32431;&#39044;&#27979;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#25968;&#25454;&#29983;&#25104;&#33050;&#26412;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312;&#32447;&#19978;&#25214;&#21040;(https://uhh.de/inf-sp-storm-wind)&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a method for single-channel wind noise reduction using our previously proposed diffusion-based stochastic regeneration model combining predictive and generative modelling. We introduce a non-additive speech in noise model to account for the non-linear deformation of the membrane caused by the wind flow and possible clipping. We show that our stochastic regeneration model outperforms other neural-network-based wind noise reduction methods as well as purely predictive and generative models, on a dataset using simulated and real-recorded wind noise. We further show that the proposed method generalizes well by testing on an unseen dataset with real-recorded wind noise. Audio samples, data generation scripts and code for the proposed methods can be found online (https://uhh.de/inf-sp-storm-wind).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#20869;&#22312;&#22870;&#21169;&#26469;&#35299;&#20915;&#35270;&#35273;&#35266;&#23519;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#22312;Atari&#21644;Minecraft&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12860</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#39044;&#35757;&#32451;&#29366;&#24577;&#21040;&#30446;&#26631;Transformer&#35270;&#35273;&#35266;&#23519;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Visual Observation via Offline Pretrained State-to-Go Transformer. (arXiv:2306.12860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#20869;&#22312;&#22870;&#21169;&#26469;&#35299;&#20915;&#35270;&#35273;&#35266;&#23519;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#22312;Atari&#21644;Minecraft&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35266;&#23519;&#23398;&#20064;&#26088;&#22312;&#20165;&#20174;&#35270;&#35273;&#35266;&#23519;&#25968;&#25454;&#20013;&#24674;&#22797;&#31574;&#30053;&#65292;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#24341;&#20837;&#24182;&#31163;&#32447;&#39044;&#35757;&#32451;&#29366;&#24577;&#21040;&#30446;&#26631;(STG) Transformer&#20197;&#39044;&#27979;&#21644;&#21306;&#20998;&#28436;&#31034;&#30340;&#28508;&#22312;&#36716;&#25442;&#12290;&#38543;&#21518;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;STG Transformer&#20026;&#19979;&#28216;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20854;&#20013;&#20195;&#29702;&#20165;&#20174;&#20869;&#22312;&#22870;&#21169;&#20013;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;Atari&#21644;Minecraft&#19978;&#20248;&#20110;&#22522;&#32447;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21487;&#20197;&#36798;&#21040;&#19982;&#20174;&#29615;&#22659;&#22870;&#21169;&#23398;&#20064;&#30340;&#31574;&#30053;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from visual observation (LfVO), aiming at recovering policies from only visual observation data, is promising yet a challenging problem. Existing LfVO approaches either only adopt inefficient online learning schemes or require additional task-specific information like goal states, making them not suited for open-ended tasks. To address these issues, we propose a two-stage framework for learning from visual observation. In the first stage, we introduce and pretrain State-to-Go (STG) Transformer offline to predict and differentiate latent transitions of demonstrations. Subsequently, in the second stage, the STG Transformer provides intrinsic rewards for downstream reinforcement learning tasks where an agent learns merely from intrinsic rewards. Empirical results on Atari and Minecraft show that our proposed method outperforms baselines and in some tasks even achieves performance comparable to the policy learned from environmental rewards. These results shed light on the potentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#30340;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#32531;&#35299;&#19981;&#21516;&#29992;&#25143;&#32456;&#31471;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#25152;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12859</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#30340;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Federated Learning Method Based on Adaptive OPTICS Clustering. (arXiv:2306.12859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#30340;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#32531;&#35299;&#19981;&#21516;&#29992;&#25143;&#32456;&#31471;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#25152;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20849;&#20139;&#35745;&#31639;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#22312;&#21442;&#19982;&#35774;&#22791;&#19978;&#26412;&#22320;&#25191;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#24182;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#25104;&#20840;&#23616;&#27169;&#22411;&#26469;&#23398;&#20064;&#20849;&#20139;&#27169;&#22411;&#12290;&#32852;&#37030;&#23398;&#20064;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#25968;&#25454;&#22312;&#19981;&#21516;&#29992;&#25143;&#32456;&#31471;&#19978;&#30340;&#38750;&#29420;&#31435;&#21644;&#30456;&#21516;&#20998;&#24067;&#25152;&#23548;&#33268;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#30340;&#22686;&#24378;&#22411;&#32852;&#37030;&#32858;&#21512;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#23558;&#32858;&#31867;&#29615;&#22659;&#35270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#23545;&#21442;&#25968;&#25628;&#32034;&#26041;&#21521;&#30340;&#35843;&#25972;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#32858;&#31867;&#21442;&#25968;&#20197;&#36798;&#21040;&#26368;&#20339;&#32852;&#37030;&#32858;&#21512;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;OPTICS&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning technology, which realizes the balance between data privacy protection and data sharing computing. To protect data privacy, feder-ated learning learns shared models by locally executing distributed training on participating devices and aggregating local models into global models. There is a problem in federated learning, that is, the negative impact caused by the non-independent and identical distribu-tion of data across different user terminals. In order to alleviate this problem, this paper pro-poses a strengthened federation aggregation method based on adaptive OPTICS clustering. Specifically, this method perceives the clustering environment as a Markov decision process, and models the adjustment process of parameter search direction, so as to find the best clus-tering parameters to achieve the best federated aggregation method. The core contribution of this paper is to propose an adaptive OPTICS clustering algorithm for federated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20002;&#22833;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#23433;&#20840;&#26102;&#31354;&#25968;&#25454;&#39640;&#25928;&#21010;&#20998;&#26041;&#27861;(IFL-LSTP)&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#30830;&#20445;&#20998;&#24067;&#24335;&#23384;&#20648;&#30340;&#36127;&#36733;&#24179;&#34913;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#21010;&#20998;&#30340;&#26102;&#31354;&#25509;&#36817;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12857</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#20002;&#22833;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#23433;&#20840;&#26102;&#31354;&#25968;&#25454;&#39640;&#25928;&#21010;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Partitioning Method of Large-Scale Public Safety Spatio-Temporal Data based on Information Loss Constraints. (arXiv:2306.12857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20002;&#22833;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#23433;&#20840;&#26102;&#31354;&#25968;&#25454;&#39640;&#25928;&#21010;&#20998;&#26041;&#27861;(IFL-LSTP)&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#30830;&#20445;&#20998;&#24067;&#24335;&#23384;&#20648;&#30340;&#36127;&#36733;&#24179;&#34913;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#21010;&#20998;&#30340;&#26102;&#31354;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26102;&#31354;&#25968;&#25454;&#30340;&#23384;&#20648;&#12289;&#31649;&#29702;&#21644;&#24212;&#29992;&#22312;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#20844;&#20849;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#29420;&#29305;&#26102;&#31354;&#20998;&#24067;&#29305;&#24449;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#25968;&#25454;&#26102;&#31354;&#25509;&#36817;&#24230;&#21644;&#20998;&#24067;&#24335;&#23384;&#20648;&#36127;&#36733;&#24179;&#34913;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20002;&#22833;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#23433;&#20840;&#26102;&#31354;&#25968;&#25454;&#39640;&#25928;&#21010;&#20998;&#26041;&#27861;(IFL-LSTP)&#12290;&#35813;IFL-LSTP&#27169;&#22411;&#38024;&#23545;&#22823;&#35268;&#27169;&#26102;&#31354;&#28857;&#25968;&#25454;&#65292;&#23558;&#26102;&#31354;&#21010;&#20998;&#27169;&#22359;(STPM)&#21644;&#22270;&#21010;&#20998;&#27169;&#22359;(GPM)&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#25552;&#39640;&#21010;&#20998;&#25928;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#30830;&#20445;&#20998;&#24067;&#24335;&#23384;&#20648;&#30340;&#36127;&#36733;&#24179;&#34913;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#21010;&#20998;&#30340;&#26102;&#31354;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The storage, management, and application of massive spatio-temporal data are widely applied in various practical scenarios, including public safety. However, due to the unique spatio-temporal distribution characteristics of re-al-world data, most existing methods have limitations in terms of the spatio-temporal proximity of data and load balancing in distributed storage. There-fore, this paper proposes an efficient partitioning method of large-scale public safety spatio-temporal data based on information loss constraints (IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal point da-ta by combining the spatio-temporal partitioning module (STPM) with the graph partitioning module (GPM). This approach can significantly reduce the scale of data while maintaining the model's accuracy, in order to improve the partitioning efficiency. It can also ensure the load balancing of distributed storage while maintaining spatio-temporal proximity of the data partitioning res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MultiTASC&#65292;&#19968;&#31181;&#22810;&#31199;&#25143;&#24863;&#30693;&#35843;&#24230;&#31243;&#24207;&#65292;&#23427;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#35774;&#22791;&#30340;&#36716;&#21457;&#20915;&#31574;&#20989;&#25968;&#65292;&#20197;&#20248;&#21270;&#31995;&#32479;&#21534;&#21520;&#37327;&#21644;&#25552;&#39640;&#24310;&#36831;&#26381;&#21153;&#27700;&#24179;&#30446;&#26631;&#30340;&#28385;&#36275;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.12830</link><description>&lt;p&gt;
MultiTASC: &#38754;&#21521;&#28040;&#36153;&#32773;&#36793;&#32536;&#32423;&#32852; DNN &#25512;&#29702;&#30340;&#22810;&#31199;&#25143;&#24863;&#30693;&#35843;&#24230;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
MultiTASC: A Multi-Tenancy-Aware Scheduler for Cascaded DNN Inference at the Consumer Edge. (arXiv:2306.12830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MultiTASC&#65292;&#19968;&#31181;&#22810;&#31199;&#25143;&#24863;&#30693;&#35843;&#24230;&#31243;&#24207;&#65292;&#23427;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#35774;&#22791;&#30340;&#36716;&#21457;&#20915;&#31574;&#20989;&#25968;&#65292;&#20197;&#20248;&#21270;&#31995;&#32479;&#21534;&#21520;&#37327;&#21644;&#25552;&#39640;&#24310;&#36831;&#26381;&#21153;&#27700;&#24179;&#30446;&#26631;&#30340;&#28385;&#36275;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#31995;&#32479;&#30001;&#36731;&#37327;&#32423;&#27169;&#22411;&#21644;&#37325;&#22411;&#39640;&#31934;&#24230;&#27169;&#22411;&#32452;&#25104;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#31471;&#25918;&#32622;&#36731;&#22411;&#27169;&#22411;&#21644;&#22312;&#26381;&#21153;&#22120;&#19978;&#25918;&#32622;&#37325;&#22411;&#27169;&#22411;&#65292;&#32423;&#32852;&#27169;&#22411;&#26500;&#25104;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; MultiTASC&#65292;&#19968;&#31181;&#22810;&#31199;&#25143;&#24863;&#30693;&#35843;&#24230;&#31243;&#24207;&#65292;&#23427;&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#35774;&#22791;&#30340;&#36716;&#21457;&#20915;&#31574;&#20989;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#31995;&#32479;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#24310;&#36831;&#12290;&#36890;&#36807;&#26126;&#30830;&#32771;&#34385;&#35774;&#22791;&#24322;&#26500;&#24615;&#65292;&#25105;&#20204;&#30340;&#35843;&#24230;&#31243;&#24207;&#25913;&#21892;&#20102;&#24310;&#36831;&#26381;&#21153;&#27700;&#24179;&#30446;&#26631; (SLO) &#28385;&#36275;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascade systems comprise a two-model sequence, with a lightweight model processing all samples and a heavier, higher-accuracy model conditionally refining harder samples to improve accuracy. By placing the light model on the device side and the heavy model on a server, model cascades constitute a widely used distributed inference approach. With the rapid expansion of intelligent indoor environments, such as smart homes, the new setting of Multi-Device Cascade is emerging where multiple and diverse devices are to simultaneously use a shared heavy model on the same server, typically located within or close to the consumer environment. This work presents MultiTASC, a multi-tenancy-aware scheduler that adaptively controls the forwarding decision functions of the devices in order to maximize the system throughput, while sustaining high accuracy and low latency. By explicitly considering device heterogeneity, our scheduler improves the latency service-level objective (SLO) satisfaction rate 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#24377;&#24615;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20135;&#29983;&#20934;&#30830;&#30340;&#24377;&#24615;&#27169;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#24212;&#21464;&#33021;&#23494;&#24230;&#21644;&#30456;&#20851;&#30340;&#24377;&#24615;&#24120;&#25968;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#25299;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12818</link><description>&lt;p&gt;
StrainNet: &#20351;&#29992;SE(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#24377;&#24615;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
StrainNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks. (arXiv:2306.12818v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#24377;&#24615;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20135;&#29983;&#20934;&#30830;&#30340;&#24377;&#24615;&#27169;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#24212;&#21464;&#33021;&#23494;&#24230;&#21644;&#30456;&#20851;&#30340;&#24377;&#24615;&#24120;&#25968;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#25299;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26230;&#24577;&#22266;&#20307;&#30340;&#24377;&#24615;&#24615;&#36136;&#23545;&#20110;&#35745;&#31639;&#26448;&#26009;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20174;&#22836;&#31639;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30740;&#31350;&#20855;&#26377;&#22823;&#37327;&#21407;&#23376;&#30340;&#22797;&#26434;&#26448;&#26009;&#26102;&#22914;&#27492;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20351;&#29992;SE(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#39640;&#25928;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#30340;&#24377;&#24615;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#19982;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#30456;&#24403;&#20934;&#30830;&#30340;&#37325;&#35201;&#26631;&#37327;&#24377;&#24615;&#27169;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23545;&#31216;&#24863;&#30693;GNN&#27169;&#22411;&#36824;&#33021;&#22815;&#39044;&#27979;&#24212;&#21464;&#33021;&#23494;&#24230;(SED)&#21644;&#30456;&#20851;&#24377;&#24615;&#24120;&#25968;&#65292;&#36825;&#20123;&#26159;&#21463;&#26230;&#20307;&#23398;&#32676;&#24433;&#21709;&#26174;&#33879;&#30340;&#22522;&#26412;&#24352;&#37327;&#37327;&#12290;&#35813;&#27169;&#22411;&#19968;&#33268;&#21306;&#20998;SED&#24352;&#37327;&#30340;&#29420;&#31435;&#20803;&#32032;&#65292;&#31526;&#21512;&#26230;&#20307;&#32467;&#26500;&#30340;&#23545;&#31216;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#25299;&#23637;&#24615;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26230;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the elastic properties of crystalline solids is vital for computational materials science. However, traditional atomistic scale ab initio approaches are computationally intensive, especially for studying complex materials with a large number of atoms in a unit cell. We introduce a novel data-driven approach to efficiently predict the elastic properties of crystal structures using SE(3)-equivariant graph neural networks (GNNs). This approach yields important scalar elastic moduli with the accuracy comparable to recent data-driven studies. Importantly, our symmetry-aware GNNs model also enables the prediction of the strain energy density (SED) and the associated elastic constants, the fundamental tensorial quantities that are significantly influenced by a material's crystallographic group. The model consistently distinguishes independent elements of SED tensors, in accordance with the symmetry of the crystal structures. Finally, our deep learning model possesses mea
&lt;/p&gt;</description></item><item><title>XAI-TRIS&#25552;&#20379;&#20102;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12816</link><description>&lt;p&gt;
XAI-TRIS&#65306;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance. (arXiv:2306.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12816
&lt;/p&gt;
&lt;p&gt;
XAI-TRIS&#25552;&#20379;&#20102;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#21487;&#35299;&#37322;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#39640;&#24230;&#24341;&#29992;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#20915;&#31574;&#8220;&#21487;&#29702;&#35299;&#8221;&#32473;&#20154;&#31867;&#65292;&#20363;&#22914;&#36890;&#36807;&#23545;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#8220;&#37325;&#35201;&#24615;&#8221;&#35780;&#20998;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#27491;&#24335;&#30340;&#22522;&#30784;&#65292;&#20351;&#24471;&#26080;&#27861;&#20174;&#32473;&#23450;XAI&#26041;&#27861;&#30340;&#32467;&#26524;&#20013;&#23433;&#20840;&#22320;&#24471;&#20986;&#32467;&#35770;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20063;&#38459;&#30861;&#20102;XAI&#26041;&#27861;&#30340;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#35777;&#39564;&#35777;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#30446;&#21069;&#32570;&#20047;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#36890;&#24120;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#31181;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#24773;&#26223;&#21046;&#20316;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#36890;&#36807;&#35774;&#35745;&#24050;&#30693;&#37325;&#35201;&#30340;&#31867;&#26465;&#20214;&#29305;&#24449;&#65292;&#20316;&#20026;&#22320;&#38754;&#23454;&#20917;&#35299;&#37322;&#12290;&#21033;&#29992;&#26032;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#19978;&#27979;&#35797;&#20102;&#24191;&#27867;&#30340;XAI&#26041;&#27861;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;XAI&#26041;&#27861;&#30340;&#24456;&#22810;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35746;&#21333;&#31807;&#27169;&#25311;&#20013;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#23545;&#36755;&#20837;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#21450;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#25552;&#39640;&#20102;CGAN&#30340;&#36924;&#30495;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12806</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#22120;&#22312;&#38480;&#20215;&#35746;&#21333;&#31807;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65306;&#21487;&#35299;&#37322;&#24615;&#12289;&#25361;&#25112;&#21644;&#24378;&#20581;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness. (arXiv:2306.12806v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35746;&#21333;&#31807;&#27169;&#25311;&#20013;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#23545;&#36755;&#20837;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#21450;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#25552;&#39640;&#20102;CGAN&#30340;&#36924;&#30495;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38480;&#20215;&#35746;&#21333;&#31807;&#26159;&#19968;&#31181;&#22522;&#26412;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#24066;&#22330;&#26426;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35746;&#21333;&#31807;&#27169;&#25311;&#20013;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22240;&#20026;&#20854;&#33021;&#22815;&#23545;&#20132;&#26131;&#20195;&#29702;&#30340;&#23384;&#22312;&#20570;&#20986;&#21453;&#24212;&#32780;&#21463;&#21040;&#20851;&#27880;&#65292;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;&#22238;&#27979;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#26469;&#33258;Coletta&#31561;&#20154;&#65288;2022&#65289;&#30340;&#26368;&#20808;&#36827;&#30340;CGAN&#65292;&#25506;&#32034;&#20102;&#23427;&#23545;&#36755;&#20837;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#65292;&#24378;&#35843;&#20102;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#29305;&#24449;&#21450;&#20854;&#26426;&#21046;&#36827;&#34892;&#20102;&#8220;&#23545;&#25239;&#25915;&#20987;&#8221;&#12290;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#35748;&#35782;&#26469;&#25552;&#39640;CGAN&#30340;&#36924;&#30495;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limit order books are a fundamental and widespread market mechanism. This paper investigates the use of conditional generative models for order book simulation. For developing a trading agent, this approach has drawn recent attention as an alternative to traditional backtesting due to its ability to react to the presence of the trading agent. Using a state-of-the-art CGAN (from Coletta et al. (2022)), we explore its dependence upon input features, which highlights both strengths and weaknesses. To do this, we use "adversarial attacks" on the model's features and its mechanism. We then show how these insights can be used to improve the CGAN, both in terms of its realism and robustness. We finish by laying out a roadmap for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#20855;&#26377;&#23616;&#37096;&#21487;&#21464;&#27979;&#37327;&#23610;&#24230;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#65288;GSD&#65289;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#21644;&#19981;&#31934;&#30830;&#27010;&#29575;&#27169;&#22411;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#38750;&#26631;&#20934;&#31354;&#38388;&#20013;&#22914;&#20309;&#27491;&#30830;&#21033;&#29992;&#20840;&#37096;&#20449;&#24687;&#26469;&#36827;&#34892;&#27604;&#36739;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12803</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#21487;&#21464;&#27979;&#37327;&#23610;&#24230;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#40065;&#26834;&#32479;&#35745;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement. (arXiv:2306.12803v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#20855;&#26377;&#23616;&#37096;&#21487;&#21464;&#27979;&#37327;&#23610;&#24230;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#65288;GSD&#65289;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#21644;&#19981;&#31934;&#30830;&#27010;&#29575;&#27169;&#22411;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#38750;&#26631;&#20934;&#31354;&#38388;&#20013;&#22914;&#20309;&#27491;&#30830;&#21033;&#29992;&#20840;&#37096;&#20449;&#24687;&#26469;&#36827;&#34892;&#27604;&#36739;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#21487;&#21464;&#27979;&#37327;&#23610;&#24230;&#30340;&#31354;&#38388;&#65292;&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#30456;&#24403;&#26222;&#36941;&#30340;&#65292;&#27604;&#22914;&#35828;&#65292;&#20855;&#26377;&#19981;&#21516;&#32553;&#25918;&#32500;&#24230;&#30340;&#22810;&#32500;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#27491;&#30830;&#22320;&#21033;&#29992;&#36825;&#20123;&#31354;&#38388;&#20013;&#32534;&#30721;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#20173;&#28982;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#21464;&#37327;&#26399;&#26395;&#30340;&#65288;&#38598;&#21512;&#65289;&#20559;&#24207;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#38543;&#26426;&#21464;&#37327;&#26144;&#23556;&#21040;&#36825;&#20123;&#38750;&#26631;&#20934;&#31354;&#38388;&#20013;&#12290;&#24403;&#27809;&#26377;&#25110;&#23436;&#20840;&#30340;&#22522;&#25968;&#32467;&#26500;&#26102;&#65292;&#36825;&#20010;&#20559;&#24207;&#20851;&#31995;&#21253;&#21547;&#38543;&#26426;&#20248;&#21183;&#21644;&#26399;&#26395;&#39034;&#24207;&#20316;&#20026;&#26497;&#31471;&#24773;&#20917;&#12290;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#23548;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#65288;GSD&#65289;&#39034;&#24207;&#30340;&#65288;&#27491;&#21017;&#21270;&#30340;&#65289;&#32479;&#35745;&#26816;&#39564;&#65292;&#24182;&#36890;&#36807;&#19981;&#31934;&#30830;&#27010;&#29575;&#27169;&#22411;&#20351;&#20854;&#26356;&#20026;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#29992;&#22810;&#32500;&#36139;&#22256;&#24230;&#37327;&#12289;&#37329;&#34701;&#21644;&#21307;&#23398;&#25968;&#25454;&#36827;&#34892;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spaces with locally varying scale of measurement, like multidimensional structures with differently scaled dimensions, are pretty common in statistics and machine learning. Nevertheless, it is still understood as an open question how to exploit the entire information encoded in them properly. We address this problem by considering an order based on (sets of) expectations of random variables mapping into such non-standard spaces. This order contains stochastic dominance and expectation order as extreme cases when no, or respectively perfect, cardinal structure is given. We derive a (regularized) statistical test for our proposed generalized stochastic dominance (GSD) order, operationalize it by linear optimization, and robustify it by imprecise probability models. Our findings are illustrated with data from multidimensional poverty measurement, finance, and medicine.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12802</link><description>&lt;p&gt;
Otter-Knowledge&#65306;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#21033;&#29992;&#22823;&#37327;&#30340;&#34507;&#30333;&#36136;&#25110;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#33719;&#24471;&#33647;&#29289;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#22914;&#39044;&#27979;&#33647;&#29289;&#21644;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#27169;&#24577;&#30340;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#24207;&#21015;&#25110;SMILES&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;7&#20010;&#20844;&#20849;&#26469;&#28304;&#30340;&#39044;&#22788;&#29702;&#21644;&#25972;&#21512;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;30M&#20010;&#19977;&#20803;&#32452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;Therapeutic Data Commons (TDC)&#22522;&#20934;&#27979;&#35797;&#20013;&#24615;&#33021;&#25253;&#21578;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#25512;&#33616;&#31995;&#32479;&#65292;&#23558;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#32467;&#21512;&#25104;&#19968;&#20010;&#36229;&#22270;&#25490;&#21517;&#26694;&#26550;&#65292;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#36229;&#22270;&#25490;&#21517;&#24314;&#27169;&#38598;&#25104;&#25512;&#33616;&#31995;&#32479;&#30340;&#12290;&#36229;&#22270;&#21487;&#20197;&#24314;&#27169;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.12800</link><description>&lt;p&gt;
HypeRS&#65306;&#26500;&#24314;&#22522;&#20110;&#36229;&#22270;&#39537;&#21160;&#30340;&#38598;&#25104;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
HypeRS: Building a Hypergraph-driven ensemble Recommender System. (arXiv:2306.12800v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#25512;&#33616;&#31995;&#32479;&#65292;&#23558;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#32467;&#21512;&#25104;&#19968;&#20010;&#36229;&#22270;&#25490;&#21517;&#26694;&#26550;&#65292;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#36229;&#22270;&#25490;&#21517;&#24314;&#27169;&#38598;&#25104;&#25512;&#33616;&#31995;&#32479;&#30340;&#12290;&#36229;&#22270;&#21487;&#20197;&#24314;&#27169;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#39044;&#27979;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#20559;&#22909;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#25512;&#33616;&#31995;&#32479;&#65292;&#23558;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#32467;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#36229;&#22270;&#25490;&#21517;&#26694;&#26550;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#20351;&#29992;&#36229;&#22270;&#25490;&#21517;&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#38598;&#25104;&#12290;&#36229;&#22270;&#26159;&#22270;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#39640;&#38454;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#30340;&#25512;&#33616;&#31995;&#32479;&#20998;&#37197;&#19981;&#21516;&#30340;&#36229;&#36793;&#26435;&#37325;&#26469;&#21306;&#20998;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#23454;&#38469;&#21644;&#39044;&#27979;&#36830;&#25509;&#65292;&#24182;&#22312;&#30005;&#24433;&#12289;&#38899;&#20048;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are designed to predict user preferences over collections of items. These systems process users' previous interactions to decide which items should be ranked higher to satisfy their desires. An ensemble recommender system can achieve great recommendation performance by effectively combining the decisions generated by individual models. In this paper, we propose a novel ensemble recommender system that combines predictions made by different models into a unified hypergraph ranking framework. This is the first time that hypergraph ranking has been employed to model an ensemble of recommender systems. Hypergraphs are generalizations of graphs where multiple vertices can be connected via hyperedges, efficiently modeling high-order relations. We differentiate real and predicted connections between users and items by assigning different hyperedge weights to individual recommender systems. We perform experiments using four datasets from the fields of movie, music and news 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#39564;&#27491;&#21017;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#36895;&#24230;&#27169;&#22411;&#32500;&#24230;&#30340;&#21516;&#26102;&#36866;&#24212;&#20110;&#22320;&#38663;&#35266;&#27979;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#35266;&#27979;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#21453;&#28436;&#12290;</title><link>http://arxiv.org/abs/2306.12776</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#39564;&#27491;&#21017;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
A prior regularized full waveform inversion using generative diffusion models. (arXiv:2306.12776v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#39564;&#27491;&#21017;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#36895;&#24230;&#27169;&#22411;&#32500;&#24230;&#30340;&#21516;&#26102;&#36866;&#24212;&#20110;&#22320;&#38663;&#35266;&#27979;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#35266;&#27979;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#21453;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#27874;&#24418;&#21453;&#28436;&#65288;FWI&#65289;&#20855;&#26377;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#22320;&#19979;&#27169;&#22411;&#20272;&#35745;&#30340;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#35266;&#27979;&#38480;&#21046;&#65288;&#20363;&#22914;&#21306;&#22495;&#22122;&#22768;&#12289;&#26377;&#38480;&#30340;&#38663;&#28304;&#25110;&#25509;&#25910;&#22120;&#20197;&#21450;&#24102;&#38480;&#25968;&#25454;&#65289;&#65292;&#24456;&#38590;&#20351;&#29992;FWI&#24471;&#21040;&#25152;&#38656;&#30340;&#39640;&#20998;&#36776;&#29575;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#27491;&#21017;&#21270;FWI&#30340;&#26032;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#20808;&#21069;&#30340;&#36895;&#24230;&#27169;&#22411;&#20998;&#24067;&#19978;&#39044;&#20808;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#20998;&#24067;&#20195;&#34920;&#20102;&#25105;&#20204;&#23545;&#22320;&#19979;&#30340;&#39044;&#26399;&#65292;&#24182;&#23558;FWI&#24182;&#20837;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#23558;&#20854;&#36866;&#24212;&#20110;&#22320;&#38663;&#35266;&#27979;&#12290;&#25193;&#25955;&#27169;&#22411;&#20043;&#25152;&#20197;&#29420;&#29305;&#36866;&#21512;&#36825;&#31181;&#23454;&#29616;&#65292;&#26159;&#22240;&#20026;&#20854;&#29983;&#25104;&#36807;&#31243;&#20445;&#30041;&#20102;&#36895;&#24230;&#27169;&#22411;&#30340;&#24418;&#24335;&#21644;&#23610;&#23544;&#12290;&#25968;&#20540;&#23454;&#20363;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;FWI&#12290;&#21363;&#20351;&#22312;&#21482;&#26377;&#23569;&#37327;&#35266;&#27979;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full waveform inversion (FWI) has the potential to provide high-resolution subsurface model estimations. However, due to limitations in observation, e.g., regional noise, limited shots or receivers, and band-limited data, it is hard to obtain the desired high-resolution model with FWI. To address this challenge, we propose a new paradigm for FWI regularized by generative diffusion models. Specifically, we pre-train a diffusion model in a fully unsupervised manner on a prior velocity model distribution that represents our expectations of the subsurface and then adapt it to the seismic observations by incorporating the FWI into the sampling process of the generative diffusion models. What makes diffusion models uniquely appropriate for such an implementation is that the generative process retains the form and dimensions of the velocity model. Numerical examples demonstrate that our method can outperform the conventional FWI with only negligible additional computational cost. Even in case
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2306.12774</link><description>&lt;p&gt;
&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#32431;&#25506;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#23384;&#22312;&#32447;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#19968;&#23450;&#32622;&#20449;&#24230;&#19979;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#19982;&#26631;&#20934;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#26159;&#21487;&#33021;&#22312;&#22810;&#20010;&#33218;&#20043;&#38388;&#36827;&#34892;&#28151;&#21512;&#12290;&#36825;&#31181;&#24773;&#20917;&#25913;&#21464;&#20102;&#38382;&#39064;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#19979;&#30028;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#27492;&#35774;&#32622;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#22522;&#20110;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#26041;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#21338;&#24328;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#32422;&#26463;&#22914;&#20309;&#25913;&#21464;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#35299;&#20915;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#22312;&#26102;&#38388;&#20559;&#31227;&#19979;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#33258;&#21160;&#36866;&#24212;&#32593;&#32476;&#20013;&#19981;&#26029;&#28436;&#21270;&#30340;&#27010;&#24565;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2306.12768</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#24847;&#35782;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#20559;&#31227;&#19979;&#30340;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Concept-aware clustering for decentralized deep learning under temporal shift. (arXiv:2306.12768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#35299;&#20915;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#22312;&#26102;&#38388;&#20559;&#31227;&#19979;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#33258;&#21160;&#36866;&#24212;&#32593;&#32476;&#20013;&#19981;&#26029;&#28436;&#21270;&#30340;&#27010;&#24565;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#12289;&#21487;&#33021;&#22240;&#20026;&#26102;&#38388;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26102;&#38388;&#21464;&#21270;&#21364;&#19968;&#30452;&#27809;&#26377;&#24471;&#21040;&#20851;&#27880;&#12290;&#22312;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#21160;&#24577;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#21644;&#36866;&#24212;&#32593;&#32476;&#20013;&#19981;&#26029;&#28436;&#21270;&#30340;&#27010;&#24565;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20808;&#21069;&#30340;&#30693;&#35782;&#25110;&#27010;&#24565;&#25968;&#37327;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20248;&#20110;&#20197;&#21069;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized deep learning requires dealing with non-iid data across clients, which may also change over time due to temporal shifts. While non-iid data has been extensively studied in distributed settings, temporal shifts have received no attention. To the best of our knowledge, we are first with tackling the novel and challenging problem of decentralized learning with non-iid and dynamic data. We propose a novel algorithm that can automatically discover and adapt to the evolving concepts in the network, without any prior knowledge or estimation of the number of concepts. We evaluate our algorithm on standard benchmark datasets and demonstrate that it outperforms previous methods for decentralized learning.
&lt;/p&gt;</description></item><item><title>Blended-NeRF&#26159;&#19968;&#31181;&#40065;&#26834;&#32780;&#28789;&#27963;&#30340;&#32534;&#36753;NeRF&#22330;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#33258;&#28982;&#24615;&#19982;&#19968;&#33268;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#25110;&#22270;&#20687;&#34917;&#19969;&#30340;&#29289;&#20307;&#21512;&#25104;&#24182;&#28151;&#21512;&#21040;&#21407;&#22987;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.12760</link><description>&lt;p&gt;
&#28151;&#21512;&#24335;&#31070;&#32463;&#36752;&#23556;&#22330;&#65306;&#38646;&#26679;&#26412;&#29289;&#20307;&#29983;&#25104;&#19982;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields. (arXiv:2306.12760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12760
&lt;/p&gt;
&lt;p&gt;
Blended-NeRF&#26159;&#19968;&#31181;&#40065;&#26834;&#32780;&#28789;&#27963;&#30340;&#32534;&#36753;NeRF&#22330;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#33258;&#28982;&#24615;&#19982;&#19968;&#33268;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#25110;&#22270;&#20687;&#34917;&#19969;&#30340;&#29289;&#20307;&#21512;&#25104;&#24182;&#28151;&#21512;&#21040;&#21407;&#22987;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;3D&#22330;&#26223;&#20013;&#32534;&#36753;&#23616;&#37096;&#21306;&#22495;&#25110;&#29305;&#23450;&#29289;&#20307;&#24182;&#28151;&#21512;&#21040;&#24050;&#26377;&#30340;NeRF&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#22330;&#26223;&#34920;&#31034;&#30340;&#38544;&#24335;&#23646;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Blended-NeRF&#65292;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#25110;&#22270;&#20687;&#34917;&#19969;&#20197;&#21450;3D ROI&#21253;&#22260;&#30418;&#30340;NeRF&#22330;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#30340;&#32534;&#36753;&#30340;&#40065;&#26834;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;&#26469;&#24341;&#23548;&#21512;&#25104;&#26397;&#21521;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#25110;&#22270;&#20687;&#34917;&#19969;&#65292;&#36824;&#21033;&#29992;&#19968;&#20010;&#24050;&#23384;&#22312;&#30340;NeRF&#22330;&#26223;&#19978;&#21021;&#22987;&#21270;&#30340;3D MLP&#27169;&#22411;&#26469;&#29983;&#25104;&#29289;&#20307;&#24182;&#23558;&#20854;&#28151;&#21512;&#21040;&#21407;&#22987;&#22330;&#26223;&#20013;&#30340;&#25351;&#23450;&#21306;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;3D ROI&#30418;&#23616;&#37096;&#21270;&#20197;&#23454;&#29616;&#23616;&#37096;&#32534;&#36753;&#65292;&#24182;&#21033;&#29992;&#26032;&#39062;&#30340;&#20307;&#31215;&#28151;&#21512;&#25216;&#26415;&#23558;&#20869;&#37096;&#21512;&#25104;&#20869;&#23481;&#26080;&#32541;&#28151;&#21512;&#21040;&#29616;&#26377;&#22330;&#26223;&#20013;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#32780;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Editing a local region or a specific object in a 3D scene represented by a NeRF is challenging, mainly due to the implicit nature of the scene representation. Consistently blending a new realistic object into the scene adds an additional level of difficulty. We present Blended-NeRF, a robust and flexible framework for editing a specific region of interest in an existing NeRF scene, based on text prompts or image patches, along with a 3D ROI box. Our method leverages a pretrained language-image model to steer the synthesis towards a user-provided text prompt or image patch, along with a 3D MLP model initialized on an existing NeRF scene to generate the object and blend it into a specified region in the original scene. We allow local editing by localizing a 3D ROI box in the input scene, and seamlessly blend the content synthesized inside the ROI with the existing scene using a novel volumetric blending technique. To obtain natural looking and view-consistent results, we leverage existin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#23450;&#20041;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#34913;&#37327;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#36739;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#22330;&#26223;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#38024;&#23545;&#36896;&#25104;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.12756</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;:&#22522;&#20110;&#36229;&#20986;&#20998;&#24067;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective. (arXiv:2306.12756v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#23450;&#20041;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#34913;&#37327;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#36739;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#22330;&#26223;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#38024;&#23545;&#36896;&#25104;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#26631;&#35782;&#31526;&#26469;&#26816;&#32034;&#25991;&#26723;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#26469;&#24320;&#21457;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21364;&#24471;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#24403;&#19968;&#20010;&#26032;&#30340;&#26816;&#32034;&#33539;&#24335;&#36827;&#20837;&#21040;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26102;&#65292;&#34913;&#37327;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21363;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22914;&#20309;&#27867;&#21270;&#21040;&#26032;&#30340;&#20998;&#24067;&#20013;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#26816;&#32034;&#38382;&#39064;&#30340;&#19977;&#20010;&#26041;&#38754;&#23450;&#20041;OOD&#40065;&#26834;&#24615;&#65306;1&#65289;&#26597;&#35810;&#21464;&#21270;&#65307;2&#65289;&#26410;&#30693;&#30340;&#26597;&#35810;&#31867;&#22411;&#65307;3&#65289;&#26410;&#30693;&#20219;&#21153;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20960;&#20010;&#20195;&#34920;&#24615;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22312;OOD&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#27604;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;OOD&#22330;&#26223;&#20013;&#26356;&#26126;&#26174;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36896;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23427;&#20204;OOD&#27867;&#21270;&#24615;&#33021;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, we have witnessed generative retrieval increasingly gaining attention in the information retrieval (IR) field, which retrieves documents by directly generating their identifiers. So far, much effort has been devoted to developing effective generative retrieval models. There has been less attention paid to the robustness perspective. When a new retrieval paradigm enters into the real-world application, it is also critical to measure the out-of-distribution (OOD) generalization, i.e., how would generative retrieval models generalize to new distributions. To answer this question, firstly, we define OOD robustness from three perspectives in retrieval problems: 1) The query variations; 2) The unforeseen query types; and 3) The unforeseen tasks. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of several representative generative retrieval models against dense retrieval models. The empirical results indicate that the OOD robustness of generative re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#39046;&#22495;&#31163;&#32447;RL&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#32447;&#19979;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25903;&#25345;&#32422;&#26463;&#30446;&#26631;&#35299;&#20915;&#20102;OOD&#29366;&#24577;&#25805;&#20316;&#21644;OOD&#36716;&#25442;&#21160;&#24577;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.12755</link><description>&lt;p&gt;
&#36229;&#36234;OOD&#29366;&#24577;&#34892;&#21160;&#65306;&#25903;&#25345;&#20132;&#21449;&#39046;&#22495;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning. (arXiv:2306.12755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12755
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#39046;&#22495;&#31163;&#32447;RL&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#32447;&#19979;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25903;&#25345;&#32422;&#26463;&#30446;&#26631;&#35299;&#20915;&#20102;OOD&#29366;&#24577;&#25805;&#20316;&#21644;OOD&#36716;&#25442;&#21160;&#24577;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20165;&#20351;&#29992;&#39044;&#20808;&#25910;&#38598;&#19988;&#22266;&#23450;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#31574;&#30053;&#12290;&#23613;&#31649;&#36991;&#20813;&#20102;RL&#20013;&#32791;&#26102;&#30340;&#22312;&#32447;&#20132;&#20114;&#65292;&#20294;&#23427;&#23545;&#20110;OOD&#29366;&#24577;&#25805;&#20316;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35757;&#32451;&#20013;&#23384;&#22312;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20132;&#21449;&#39046;&#22495;&#31163;&#32447;RL&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36328;&#39046;&#22495;&#31163;&#32447;&#25968;&#25454;&#30340;OOD&#36716;&#25442;&#21160;&#24577;&#38382;&#39064;&#65292;&#24182;&#26399;&#26395;&#20854;&#25552;&#39640;&#32447;&#19979;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to learn a policy using only pre-collected and fixed data. Although avoiding the time-consuming online interactions in RL, it poses challenges for out-of-distribution (OOD) state actions and often suffers from data inefficiency for training. Despite many efforts being devoted to addressing OOD state actions, the latter (data inefficiency) receives little attention in offline RL. To address this, this paper proposes the cross-domain offline RL, which assumes offline data incorporate additional source-domain data from varying transition dynamics (environments), and expects it to contribute to the offline data efficiency. To do so, we identify a new challenge of OOD transition dynamics, beyond the common OOD state actions issue, when utilizing cross-domain offline data. Then, we propose our method BOSA, which employs two support-constrained objectives to address the above OOD issues. Through extensive experiments in the cross-domain offline RL sett
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#35299;&#20915;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;Adam&#30340;&#36895;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21333;&#35843;&#32447;&#25628;&#32034;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#32467;&#21512;&#21516;&#27493;&#30340;Polyak&#21021;&#22987;&#21270;&#27493;&#20240;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12747</link><description>&lt;p&gt;
&#19981;&#35201;&#22826;&#21333;&#35843;&#65306;&#25918;&#23485;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#32447;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models. (arXiv:2306.12747v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#35299;&#20915;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;Adam&#30340;&#36895;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21333;&#35843;&#32447;&#25628;&#32034;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#32467;&#21512;&#21516;&#27493;&#30340;Polyak&#21021;&#22987;&#21270;&#27493;&#20240;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#32447;&#25628;&#32034;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29616;&#20195;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;Adam&#30340;&#36895;&#24230;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#65288;&#23567;&#25209;&#37327;&#65289;&#30446;&#26631;&#20989;&#25968;&#30340;&#21333;&#35843;&#20943;&#23569;&#65292;&#29616;&#26377;&#30340;&#32447;&#25628;&#32034;&#21487;&#33021;&#20250;&#37319;&#21462;&#27604;&#24517;&#35201;&#26356;&#23567;&#30340;&#27493;&#39588;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#21333;&#35843;&#32447;&#25628;&#32034;&#26041;&#27861;&#26469;&#25918;&#23485;&#36825;&#20010;&#26465;&#20214;&#65292;&#24182;&#21487;&#33021;&#25509;&#21463;&#26356;&#22823;&#30340;&#27493;&#38271;&#12290;&#23613;&#31649;&#32570;&#20047;&#21333;&#35843;&#36882;&#20943;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#19982;&#21333;&#35843;&#24773;&#20917;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38750;&#21333;&#35843;&#26041;&#27861;&#22312;SGD / Adam&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#29978;&#33267;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#21333;&#35843;&#32447;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;POlyak NOnmonotone&#38543;&#26426;&#65288;PoNoS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#21333;&#35843;&#32447;&#25628;&#32034;&#19982;Polyak&#21021;&#22987;&#27493;&#38271;&#32467;&#21512;&#32780;&#24471;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#32622;&#25216;&#26415;&#65292;&#22312;&#22823;&#22810;&#25968;&#36845;&#20195;&#20013;&#23558;&#22238;&#28335;&#30340;&#25968;&#37327;&#20943;&#23569;&#21040;&#38646;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#36739;&#22823;&#30340;&#21021;&#22987;s&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that line search methods can speed up Stochastic Gradient Descent (SGD) and Adam in modern over-parameterized settings. However, existing line searches may take steps that are smaller than necessary since they require a monotone decrease of the (mini-)batch objective function. We explore nonmonotone line search methods to relax this condition and possibly accept larger step sizes. Despite the lack of a monotonic decrease, we prove the same fast rates of convergence as in the monotone case. Our experiments show that nonmonotone methods improve the speed of convergence and generalization properties of SGD/Adam even beyond the previous monotone line searches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtained by combining a nonmonotone line search with a Polyak initial step size. Furthermore, we develop a new resetting technique that in the majority of the iterations reduces the amount of backtracks to zero while still maintaining a large initial s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MID&#30340;&#26032;&#30340;&#24471;&#20998;&#26041;&#26696;&#65292;&#36890;&#36807;&#26377;&#25928;&#32500;&#25345;&#19981;&#21516;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#36843;&#20351;&#27169;&#22411;&#27880;&#24847;&#21040;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#33410;&#28857;&#38477;&#37319;&#26679;&#22270;&#27744;&#21270;&#25216;&#26415;&#30340;&#22270;&#23618;&#27425;&#34920;&#31034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12726</link><description>&lt;p&gt;
&#25506;&#32034;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#22810;&#26679;&#24615;&#29992;&#20110;&#33410;&#28857;&#38477;&#37319;&#26679;&#22270;&#27744;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Exploring Node-feature and Graph-structure Diversities for Node Drop Graph Pooling. (arXiv:2306.12726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MID&#30340;&#26032;&#30340;&#24471;&#20998;&#26041;&#26696;&#65292;&#36890;&#36807;&#26377;&#25928;&#32500;&#25345;&#19981;&#21516;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#36843;&#20351;&#27169;&#22411;&#27880;&#24847;&#21040;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#33410;&#28857;&#38477;&#37319;&#26679;&#22270;&#27744;&#21270;&#25216;&#26415;&#30340;&#22270;&#23618;&#27425;&#34920;&#31034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27744;&#21270;&#25805;&#20316;&#23545;&#20110;&#26377;&#25928;&#30340;&#22270;&#23618;&#27425;&#34920;&#31034;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#33410;&#28857;&#38477;&#37319;&#26679;&#27744;&#21270;&#24050;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#30340;&#22270;&#27744;&#21270;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33410;&#28857;&#38477;&#37319;&#26679;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#20445;&#30041;&#21069;k&#20010;&#33410;&#28857;&#65292;&#24573;&#30053;&#20102;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#23548;&#33268;&#23376;&#20248;&#30340;&#22270;&#23618;&#27425;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#24471;&#20998;&#26041;&#26696;&#65292;&#24182;&#31216;&#20043;&#20026;MID&#65292;&#23427;&#30001;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#25805;&#20316;&#30340;&#22810;&#32500;&#24471;&#20998;&#31354;&#38388;&#32452;&#25104;&#65292;&#21363;flipscore&#21644;Dropscore&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22810;&#32500;&#24471;&#20998;&#31354;&#38388;&#36890;&#36807;&#22810;&#20010;&#26631;&#20934;&#25551;&#36848;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#65307;flipscore&#40723;&#21169;&#32500;&#25252;&#19981;&#21516;&#30340;&#33410;&#28857;&#29305;&#24449;&#65307;Dropscore&#21017;&#36843;&#20351;&#27169;&#22411;&#27880;&#24847;&#21040;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#32780;&#19981;&#26159;&#20572;&#30041;&#22312;&#26174;&#33879;&#30340;&#26412;&#22320;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
A pooling operation is essential for effective graph-level representation learning, where the node drop pooling has become one mainstream graph pooling technology. However, current node drop pooling methods usually keep the top-k nodes according to their significance scores, which ignore the graph diversity in terms of the node features and the graph structures, thus resulting in suboptimal graph-level representations. To address the aforementioned issue, we propose a novel plug-and-play score scheme and refer to it as MID, which consists of a \textbf{M}ultidimensional score space with two operations, \textit{i.e.}, fl\textbf{I}pscore and \textbf{D}ropscore. Specifically, the multidimensional score space depicts the significance of nodes through multiple criteria; the flipscore encourages the maintenance of dissimilar node features; and the dropscore forces the model to notice diverse graph structures instead of being stuck in significant local structures. To evaluate the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65288;SSL&#27169;&#22411;&#65289;&#36827;&#34892;&#27468;&#21809;&#22768;&#38899;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#33258;&#30417;&#30563;&#21069;&#31471;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;SSL&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#24120;&#35268;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12714</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#21069;&#31471;&#23454;&#29616;&#27468;&#21809;&#22768;&#38899;&#33258;&#21160;&#29702;&#35299;&#20219;&#21153;&#65306;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies. (arXiv:2306.12714v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65288;SSL&#27169;&#22411;&#65289;&#36827;&#34892;&#27468;&#21809;&#22768;&#38899;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#33258;&#30417;&#30563;&#21069;&#31471;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;SSL&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#24120;&#35268;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23545;&#33258;&#21160;&#27468;&#21809;&#22768;&#38899;&#29702;&#35299;&#20219;&#21153;&#65292;&#22914;&#27468;&#25163;&#35782;&#21035;&#12289;&#27468;&#22768;&#36716;&#24405;&#21644;&#27468;&#21809;&#25216;&#24039;&#20998;&#31867;&#31561;&#26041;&#38754;&#20855;&#26377;&#34920;&#24449;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#20016;&#23500;&#20154;&#22768;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21457;&#25381;&#33391;&#22909;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#20173;&#28982;&#26159;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65288;SSL&#27169;&#22411;&#65289;&#22312;&#35821;&#38899;&#22788;&#29702;&#21644;&#38899;&#20048;&#20998;&#31867;&#39046;&#22495;&#32463;&#36807;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#20197;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#20063;&#33021;&#23454;&#29616;&#19982;&#24120;&#35268;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;SSL&#27169;&#22411;&#22312;&#21508;&#31181;&#27468;&#21809;&#22768;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#27604;&#36739;&#19977;&#20010;&#19981;&#21516;&#20219;&#21153;&#65288;&#21363;&#27468;&#25163;&#35782;&#21035;&#12289;&#27468;&#22768;&#36716;&#24405;&#21644;&#27468;&#21809;&#25216;&#24039;&#20998;&#31867;&#65289;&#30340;SSL&#27169;&#22411;&#21644;&#24120;&#35268;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SSL&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#22312;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#21069;&#31471;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#35828;&#26126;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#25552;&#39640;&#27468;&#21809;&#22768;&#38899;&#29702;&#35299;&#20219;&#21153;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic singing voice understanding tasks, such as singer identification, singing voice transcription, and singing technique classification, benefit from data-driven approaches that utilize deep learning techniques. These approaches work well even under the rich diversity of vocal and noisy samples owing to their representation ability. However, the limited availability of labeled data remains a significant obstacle to achieving satisfactory performance. In recent years, self-supervised learning models (SSL models) have been trained using large amounts of unlabeled data in the field of speech processing and music classification. By fine-tuning these models for the target tasks, comparable performance to conventional supervised learning can be achieved with limited training data. Therefore, in this paper, we investigate the effectiveness of SSL models for various singing voice recognition tasks. We report the results of experiments comparing SSL models for three different tasks (i.e.,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;&#20013;&#20998;&#25903;&#22240;&#23376;&#30340;&#26368;&#20248;&#21462;&#20540;&#38382;&#39064;&#65292;&#22522;&#20110;&#38548;&#31163;&#25928;&#29575;&#25552;&#20986;&#21019;&#26032;&#31639;&#27861;OptIForest&#65292;&#35813;&#31639;&#27861;&#32467;&#26500;&#31616;&#27905;&#12289;&#26816;&#27979;&#24615;&#33021;&#20248;&#31168;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.12703</link><description>&lt;p&gt;
OptIForest: &#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26368;&#20248;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
OptIForest: Optimal Isolation Forest for Anomaly Detection. (arXiv:2306.12703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;&#20013;&#20998;&#25903;&#22240;&#23376;&#30340;&#26368;&#20248;&#21462;&#20540;&#38382;&#39064;&#65292;&#22522;&#20110;&#38548;&#31163;&#25928;&#29575;&#25552;&#20986;&#21019;&#26032;&#31639;&#27861;OptIForest&#65292;&#35813;&#31639;&#27861;&#32467;&#26500;&#31616;&#27905;&#12289;&#26816;&#27979;&#24615;&#33021;&#20248;&#31168;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35832;&#22810;&#39046;&#22495;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#35832;&#22914;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#20837;&#20405;&#26816;&#27979;&#12289;&#37329;&#34701;&#39118;&#38505;&#30417;&#25511;&#12289;&#20154;&#31867;&#20581;&#24247;&#30417;&#27979;&#31561;&#12290;&#26681;&#25454;&#38548;&#31163;&#26862;&#26519;&#26426;&#21046;&#25552;&#20986;&#30340;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20854;&#31616;&#27905;&#12289;&#26377;&#25928;&#12289;&#39640;&#25928;&#32780;&#22791;&#21463;&#38738;&#30544;&#65292;&#20363;&#22914;&#38024;&#23545;&#23454;&#38469;&#37096;&#32626;&#65292;iForest&#26159;&#26368;&#24120;&#29992;&#30340;&#26816;&#27979;&#22120;&#20043;&#19968;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#38548;&#31163;&#26862;&#26519;&#37319;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#65292;&#20294;&#26694;&#26550;LSHiForest&#24050;&#32463;&#35777;&#26126;&#20102;&#22810;&#21449;&#38548;&#31163;&#26641;&#32467;&#26500;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23578;&#26080;&#29702;&#35770;&#24037;&#20316;&#22238;&#31572;&#20851;&#20110;&#38548;&#31163;&#26862;&#26519;&#30340;&#26368;&#20248;&#26641;&#32467;&#26500;&#30340;&#26681;&#26412;&#21644;&#23454;&#36341;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#20309;&#31181;&#20998;&#25903;&#22240;&#23376;&#30340;&#38548;&#31163;&#26641;&#32467;&#26500;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#38548;&#31163;&#25928;&#29575;&#29702;&#35770;&#26469;&#35299;&#31572;&#35813;&#38382;&#39064;&#65292;&#36827;&#32780;&#30830;&#23450;&#20102;&#19968;&#20010;&#38548;&#31163;&#26641;&#30340;&#26368;&#20248;&#20998;&#25903;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection plays an increasingly important role in various fields for critical tasks such as intrusion detection in cybersecurity, financial risk detection, and human health monitoring. A variety of anomaly detection methods have been proposed, and a category based on the isolation forest mechanism stands out due to its simplicity, effectiveness, and efficiency, e.g., iForest is often employed as a state-of-the-art detector for real deployment. While the majority of isolation forests use the binary structure, a framework LSHiForest has demonstrated that the multi-fork isolation tree structure can lead to better detection performance. However, there is no theoretical work answering the fundamentally and practically important question on the optimal tree structure for an isolation forest with respect to the branching factor. In this paper, we establish a theory on isolation efficiency to answer the question and determine the optimal branching factor for an isolation tree. Based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#22686;&#38271;&#26041;&#27861;&#65292;&#37319;&#29992;&#21160;&#24577;&#31283;&#23450;&#21270;&#30340;&#21442;&#25968;&#21270;&#26041;&#26696;&#21644;&#23398;&#20064;&#29575;&#36866;&#24212;&#26426;&#21046;&#65292;&#33410;&#32422;&#35745;&#31639;&#39044;&#31639;&#30340;&#21516;&#26102;&#65292;&#22312;&#20934;&#30830;&#24615;&#19978;&#20063;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12700</link><description>&lt;p&gt;
&#21033;&#29992;&#26041;&#24046;&#20256;&#36882;&#21644;&#23398;&#20064;&#29575;&#35843;&#33410;&#36890;&#36807;&#36882;&#22686;&#25104;&#38271;&#31070;&#32463;&#32593;&#32476;&#30340;&#21152;&#36895;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation. (arXiv:2306.12700v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#22686;&#38271;&#26041;&#27861;&#65292;&#37319;&#29992;&#21160;&#24577;&#31283;&#23450;&#21270;&#30340;&#21442;&#25968;&#21270;&#26041;&#26696;&#21644;&#23398;&#20064;&#29575;&#36866;&#24212;&#26426;&#21046;&#65292;&#33410;&#32422;&#35745;&#31639;&#39044;&#31639;&#30340;&#21516;&#26102;&#65292;&#22312;&#20934;&#30830;&#24615;&#19978;&#20063;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#22686;&#38271;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#30340;&#22686;&#38271;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21160;&#24577;&#31283;&#23450;&#21270;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#26799;&#24230;&#32553;&#25918;&#30340;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#24182;&#20445;&#25345;&#32593;&#32476;&#25512;&#29702;&#21151;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#19981;&#21516;&#29983;&#38271;&#38454;&#27573;&#30340;&#23376;&#32593;&#32476;&#20998;&#24067;&#19981;&#22343;&#34913;&#32780;&#23548;&#33268;&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29575;&#36866;&#24212;&#26426;&#21046;&#65292;&#20197;&#37325;&#26032;&#24179;&#34913;&#36825;&#20123;&#29420;&#31435;&#23376;&#32452;&#20214;&#30340;&#26799;&#24230;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33410;&#32422;&#21407;&#22987;&#35745;&#31639;&#39044;&#31639;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#35757;&#32451;&#22823;&#22411;&#22266;&#23450;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an approach to efficiently grow neural networks, within which parameterization and optimization strategies are designed by considering their effects on the training dynamics. Unlike existing growing methods, which follow simple replication heuristics or utilize auxiliary gradient-based local optimization, we craft a parameterization scheme which dynamically stabilizes weight, activation, and gradient scaling as the architecture evolves, and maintains the inference functionality of the network. To address the optimization difficulty resulting from imbalanced training effort distributed to subnetworks fading in at different growth phases, we propose a learning rate adaption mechanism that rebalances the gradient contribution of these separate subcomponents. Experimental results show that our method achieves comparable or better accuracy than training large fixed-size models, while saving a substantial portion of the original computation budget for training. We demonstrate that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#31934;&#31616;&#32534;&#30721;&#22120;&#30340;&#20998;&#23618;DNN&#20998;&#21106;&#35745;&#31639;&#26041;&#27861;&#65292;&#22312;&#25968;&#37327;&#23439;&#22823;&#30340;&#25968;&#25454;&#20256;&#36755;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#25968;&#25454;&#37327;&#30340;&#38477;&#20302;&#65292;&#24182;&#20197;&#26368;&#23567;&#30340;&#24320;&#38144;&#21644;&#26102;&#38388;&#35843;&#25972;&#35745;&#31639;&#36127;&#36733;&#21644;&#20256;&#36755;&#25968;&#25454;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.12691</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#31934;&#31616;&#32534;&#30721;&#22120;&#30340;&#20998;&#23618;DNN&#22312;&#24102;&#23485;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#28789;&#27963;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Slimmable Encoders for Flexible Split DNNs in Bandwidth and Resource Constrained IoT Systems. (arXiv:2306.12691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#31934;&#31616;&#32534;&#30721;&#22120;&#30340;&#20998;&#23618;DNN&#20998;&#21106;&#35745;&#31639;&#26041;&#27861;&#65292;&#22312;&#25968;&#37327;&#23439;&#22823;&#30340;&#25968;&#25454;&#20256;&#36755;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#25968;&#25454;&#37327;&#30340;&#38477;&#20302;&#65292;&#24182;&#20197;&#26368;&#23567;&#30340;&#24320;&#38144;&#21644;&#26102;&#38388;&#35843;&#25972;&#35745;&#31639;&#36127;&#36733;&#21644;&#20256;&#36755;&#25968;&#25454;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20363;&#22914;&#33021;&#37327;&#65292;&#21516;&#26102;&#20063;&#23545;&#30828;&#20214;&#33021;&#21147;&#25552;&#20986;&#35201;&#27714;&#12290;&#22312;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#30340;&#26041;&#27861;&#20013;&#65292;&#27169;&#22411;&#30340;&#25191;&#34892;&#34987;&#21368;&#36733;&#21040;&#20301;&#20110;5G&#22522;&#30784;&#35774;&#26045;&#36793;&#32536;&#30340;&#35745;&#31639;&#33021;&#21147;&#35774;&#22791;&#19978;&#12290;&#21518;&#19968;&#31867;&#26041;&#27861;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#38656;&#35201;&#22312;&#24102;&#23485;&#21463;&#38480;&#19988;&#26102;&#21464;&#30340;&#26080;&#32447;&#38142;&#36335;&#19978;&#20256;&#36755;&#20449;&#24687;&#20016;&#23500;&#30340;&#20449;&#21495;&#12290;&#26368;&#36817;&#30340;&#20998;&#21106;&#35745;&#31639;&#33539;&#20363;&#35797;&#22270;&#36890;&#36807;&#20998;&#21457;DNN&#27169;&#22411;&#30340;&#25191;&#34892;&#21040;&#31995;&#32479;&#30340;&#21508;&#20010;&#23618;&#19978;&#26469;&#35299;&#20915;&#36825;&#20010;&#20725;&#23616;&#65292;&#20197;&#20943;&#23569;&#35201;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#65292;&#24182;&#19988;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#26045;&#21152;&#26368;&#23567;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#31934;&#31616;&#38598;&#21512;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#20998;&#21106;&#35745;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#33021;&#22815;&#20197;&#26368;&#23567;&#30340;&#24320;&#38144;&#21644;&#26102;&#38388;&#23454;&#29616;&#23454;&#26102;&#22320;&#35843;&#25972;&#35745;&#31639;&#36127;&#36733;&#21644;&#20256;&#36755;&#25968;&#25454;&#22823;&#23567;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The execution of large deep neural networks (DNN) at mobile edge devices requires considerable consumption of critical resources, such as energy, while imposing demands on hardware capabilities. In approaches based on edge computing the execution of the models is offloaded to a compute-capable device positioned at the edge of 5G infrastructures. The main issue of the latter class of approaches is the need to transport information-rich signals over wireless links with limited and time-varying capacity. The recent split computing paradigm attempts to resolve this impasse by distributing the execution of DNN models across the layers of the systems to reduce the amount of data to be transmitted while imposing minimal computing load on mobile devices. In this context, we propose a novel split computing approach based on slimmable ensemble encoders. The key advantage of our design is the ability to adapt computational load and transmitted data size in real-time with minimal overhead and time
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#37327;&#23376;&#35745;&#31639;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#39046;&#22495;&#65292;&#21363;&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65292;&#23427;&#26377;&#26395;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26356;&#24378;&#30340;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#12290;&#20294;&#22312;&#23454;&#29616;&#31283;&#20581;&#30340;&#23454;&#38469;QAML&#24037;&#20855;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.12688</link><description>&lt;p&gt;
&#37327;&#23376;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards quantum enhanced adversarial robustness in machine learning. (arXiv:2306.12688v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#37327;&#23376;&#35745;&#31639;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#39046;&#22495;&#65292;&#21363;&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65292;&#23427;&#26377;&#26395;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26356;&#24378;&#30340;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#12290;&#20294;&#22312;&#23454;&#29616;&#31283;&#20581;&#30340;&#23454;&#38469;QAML&#24037;&#20855;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#29305;&#24449;&#26816;&#27979;&#31561;&#25968;&#25454;&#39537;&#21160;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#28982;&#32780;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#8212;&#8212;&#21363;&#23545;&#20110;&#34987;&#31713;&#25913;&#20197;&#27450;&#39575;&#31639;&#27861;&#30340;&#36755;&#20837;&#26679;&#26412;&#8212;&#8212;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#37327;&#23376;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#26377;&#21487;&#33021;&#25552;&#20379;&#19981;&#20165;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;&#30830;&#23454;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;&#37327;&#23376;&#26426;&#26800;&#29616;&#35937;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25512;&#21160;&#20102;&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65288;QAML&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#26032;&#30340;&#37327;&#23376;&#20248;&#21183;&#26469;&#28304;&#12290;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#22312;&#26500;&#24314;&#31283;&#20581;&#30340;&#23454;&#38469;QAML&#24037;&#20855;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;QAML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#20851;&#38190;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21487;&#33021;&#30830;&#23450;&#23454;&#29616;QA ML&#36335;&#32447;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are powerful tools for data driven tasks such as image classification and feature detection, however their vulnerability to adversarial examples - input samples manipulated to fool the algorithm remains a serious challenge. The integration of machine learning with quantum computing has the potential to yield tools offering not only better accuracy and computational efficiency, but also superior robustness against adversarial attacks. Indeed, recent work has employed quantum mechanical phenomena to defend against adversarial attacks, spurring the rapid development of the field of quantum adversarial machine learning (QAML) and potentially yielding a new source of quantum advantage. Despite promising early results, there remain challenges towards building robust real-world QAML tools. In this review we discuss recent progress in QAML and identify key challenges. We also suggest future research directions which could determine the route to practicality for QA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#26041;&#27861;SEEK&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#39044;&#27979;&#65292;&#36890;&#36807;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30456;&#20851;&#30340;&#20849;&#20139;&#35821;&#20041;&#26041;&#38754;&#29983;&#25104;&#19968;&#20010;&#22810;&#26041;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#21644;&#22522;&#22240;-&#30142;&#30149;&#20851;&#32852;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.12687</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#39044;&#27979;&#20013;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explainable Representations for Relation Prediction in Knowledge Graphs. (arXiv:2306.12687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#26041;&#27861;SEEK&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#39044;&#27979;&#65292;&#36890;&#36807;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30456;&#20851;&#30340;&#20849;&#20139;&#35821;&#20041;&#26041;&#38754;&#29983;&#25104;&#19968;&#20010;&#22810;&#26041;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#21644;&#22522;&#22240;-&#30142;&#30149;&#20851;&#32852;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20197;&#26412;&#20307;&#35770;&#25903;&#25345;&#30340;&#35821;&#20041;&#20016;&#23500;&#32467;&#26500;&#34920;&#31034;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#36825;&#20010;&#25968;&#25454;&#36890;&#24120;&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#23427;&#21487;&#20135;&#29983;&#23454;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20445;&#30041;&#32467;&#26500;&#21644;&#26412;&#22320;&#22270;&#37051;&#22495;&#29305;&#24615;&#65292;&#20294;&#29306;&#29298;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#38142;&#25509;&#25110;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#65292;&#20102;&#35299;&#21738;&#20123;&#20855;&#20307;&#29305;&#24449;&#26356;&#22909;&#22320;&#35299;&#37322;&#20851;&#31995;&#23545;&#20110;&#25903;&#25345;&#22797;&#26434;&#25110;&#20851;&#38190;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEEK&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25903;&#25345;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#39044;&#27979;&#30340;&#34920;&#31034;&#12290;&#23427;&#22522;&#20110;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30456;&#20851;&#30340;&#20849;&#20139;&#35821;&#20041;&#26041;&#38754;&#65288;&#21363;&#23376;&#22270;&#65289;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#29983;&#25104;&#19968;&#20010;&#22810;&#26041;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;SEEK&#35780;&#20272;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#20013;&#39640;&#24230;&#22797;&#26434;&#30340;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#65306;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#21644;&#22522;&#22240;-&#30142;&#30149;&#20851;&#32852;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent real-world entities and their relations in a semantically-rich structure supported by ontologies. Exploring this data with machine learning methods often relies on knowledge graph embeddings, which produce latent representations of entities that preserve structural and local graph neighbourhood properties, but sacrifice explainability. However, in tasks such as link or relation prediction, understanding which specific features better explain a relation is crucial to support complex or critical applications.  We propose SEEK, a novel approach for explainable representations to support relation prediction in knowledge graphs. It is based on identifying relevant shared semantic aspects (i.e., subgraphs) between entities and learning representations for each subgraph, producing a multi-faceted and explainable representation.  We evaluate SEEK on two real-world highly complex relation prediction tasks: protein-protein interaction prediction and gene-disease associ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29256;&#26412;&#30340;&#31163;&#32676;&#20540;&#40065;&#26834;&#25289;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#24178;&#20928;&#30340;&#26679;&#26412;&#24182;&#20272;&#35745;&#20855;&#26377;&#27491;&#30830;&#25903;&#25345;&#30340;&#31232;&#30095;&#22238;&#24402;&#21521;&#37327;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20984;&#26494;&#24347;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12678</link><description>&lt;p&gt;
&#21033;&#29992;&#20984;&#24615;&#30340;&#31163;&#32676;&#20540;&#40065;&#26834;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Outlier-robust Estimation of a Sparse Linear Model Using Invexity. (arXiv:2306.12678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29256;&#26412;&#30340;&#31163;&#32676;&#20540;&#40065;&#26834;&#25289;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#24178;&#20928;&#30340;&#26679;&#26412;&#24182;&#20272;&#35745;&#20855;&#26377;&#27491;&#30830;&#25903;&#25345;&#30340;&#31232;&#30095;&#22238;&#24402;&#21521;&#37327;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20984;&#26494;&#24347;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32676;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20855;&#26377;&#27491;&#30830;&#25903;&#25345;&#30340;&#31232;&#30095;&#22238;&#24402;&#21521;&#37327;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32676;&#20540;&#40065;&#26834;&#25289;&#32034;&#30340;&#32452;&#21512;&#29256;&#26412;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#24178;&#20928;&#30340;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#24178;&#20928;&#26679;&#26412;&#36827;&#34892;&#33391;&#22909;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#20026;&#32452;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20984;&#26494;&#24347;&#65292;&#24182;&#20026;&#27492;&#26494;&#24347;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#26631;&#20934;&#30340;&#25289;&#32034;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study problem of estimating a sparse regression vector with correct support in the presence of outlier samples. The inconsistency of lasso-type methods is well known in this scenario. We propose a combinatorial version of outlier-robust lasso which also identifies clean samples. Subsequently, we use these clean samples to make a good estimation. We also provide a novel invex relaxation for the combinatorial problem and provide provable theoretical guarantees for this relaxation. Finally, we conduct experiments to validate our theory and compare our results against standard lasso.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#20174;&#39044;&#35757;&#32451;&#22270;&#20687;&#34920;&#31034;&#20013;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#26469;&#20998;&#31163;&#26680;&#24515;&#29305;&#24449;&#12289;&#34394;&#20551;&#29305;&#24449;&#21644;&#20854;&#20182;&#29305;&#24449;&#12290;&#36827;&#19968;&#27493;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#20004;&#31181;&#34394;&#20551;&#29305;&#24449;&#21435;&#38500;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12673</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22270;&#20687;&#34920;&#31034;&#20013;&#34394;&#20551;&#29305;&#24449;&#30340;&#35782;&#21035;&#21644;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Identifying and Disentangling Spurious Features in Pretrained Image Representations. (arXiv:2306.12673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#20174;&#39044;&#35757;&#32451;&#22270;&#20687;&#34920;&#31034;&#20013;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#26469;&#20998;&#31163;&#26680;&#24515;&#29305;&#24449;&#12289;&#34394;&#20551;&#29305;&#24449;&#21644;&#20854;&#20182;&#29305;&#24449;&#12290;&#36827;&#19968;&#27493;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#20004;&#31181;&#34394;&#20551;&#29305;&#24449;&#21435;&#38500;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#20013;&#20351;&#29992;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#24403;&#36825;&#20123;&#30456;&#20851;&#24615;&#19981;&#25104;&#31435;&#26102;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22266;&#23450;&#39044;&#35757;&#32451;&#34920;&#31034;&#24182;&#35757;&#32451;&#19968;&#20010;&#19981;&#20351;&#29992;&#34394;&#20551;&#29305;&#24449;&#30340;&#20998;&#31867;&#22836;&#12290;&#25105;&#20204;&#30740;&#31350;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#22914;&#20309;&#34920;&#31034;&#34394;&#20551;&#29305;&#24449;&#65292;&#24182;&#25506;&#32034;&#21435;&#38500;&#34394;&#20551;&#29305;&#24449;&#20449;&#24687;&#30340;&#31574;&#30053;&#12290;&#32771;&#34385;&#27700;&#40479;&#25968;&#25454;&#38598;&#21644;&#19968;&#20123;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#30693;&#36947;&#34394;&#20551;&#29305;&#24449;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#30001;&#20110;&#34920;&#31034;&#32416;&#32544;&#65292;&#20854;&#21435;&#38500;&#20063;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#34920;&#31034;&#20998;&#20026;&#26680;&#24515;&#65292;&#34394;&#20551;&#21644;&#20854;&#20182;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#34394;&#20551;&#29305;&#24449;&#21435;&#38500;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#32534;&#30721;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;&#26368;&#24046;&#32452;&#20934;&#30830;&#24230;&#34913;&#37327;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks employ spurious correlations in their predictions, resulting in decreased performance when these correlations do not hold. Recent works suggest fixing pretrained representations and training a classification head that does not use spurious features. We investigate how spurious features are represented in pretrained representations and explore strategies for removing information about spurious features. Considering the Waterbirds dataset and a few pretrained representations, we find that even with full knowledge of spurious features, their removal is not straightforward due to entangled representation. To address this, we propose a linear autoencoder training method to separate the representation into core, spurious, and other features. We propose two effective spurious feature removal approaches that are applied to the encoding and significantly improve classification performance measured by worst group accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#23558;&#23569;&#37327;&#30417;&#30563;&#24335;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25968;&#25454;&#65292;&#24182;&#29992;&#27492;&#26041;&#27861;&#23545;&#36890;&#29992;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.12659</link><description>&lt;p&gt;
Instruct-FinGPT: &#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26222;&#36866;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. (arXiv:2306.12659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#23558;&#23569;&#37327;&#30417;&#30563;&#24335;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25968;&#25454;&#65292;&#24182;&#29992;&#27492;&#26041;&#27861;&#23545;&#36890;&#29992;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#21457;&#29616;&#37329;&#34701;&#25991;&#31456;&#12289;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#27934;&#23519;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22609;&#36896;&#25105;&#20204;&#23545;&#24066;&#22330;&#36208;&#21521;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#38754;&#20855;&#26377;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#20934;&#30830;&#35299;&#35835;&#25968;&#23383;&#20540;&#24182;&#25235;&#20303;&#37329;&#34701;&#32972;&#26223;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#37329;&#34701;&#24773;&#24863;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#23569;&#37327;&#30340;&#30417;&#30563;&#24335;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25968;&#25454;&#65292;&#24182;&#29992;&#27492;&#26041;&#27861;&#23545;&#36890;&#29992;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24335;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#20197;&#21450;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#65292;&#22914;ChatGPT&#21644;LLaMAs&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23383;&#29702;&#35299;&#21644;&#32972;&#26223;&#29702;&#35299;&#26159;&#20851;&#38190;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21452;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#25311;&#21512;&#20540;&#36845;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12658</link><description>&lt;p&gt;
&#25311;&#21512;&#20540;&#36845;&#20195;&#26041;&#27861;&#27714;&#35299;&#36866;&#24212;&#32467;&#26500;&#21452;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fitted Value Iteration Methods for Bicausal Optimal Transport. (arXiv:2306.12658v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21452;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#25311;&#21512;&#20540;&#36845;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25311;&#21512;&#20540;&#36845;&#20195;&#26041;&#27861;(FVI)&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36866;&#24212;&#32467;&#26500;&#30340;&#21452;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;(OT)&#12290;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;FVI&#37319;&#29992;&#20989;&#25968;&#31867;&#29992;&#20110;&#36817;&#20284;&#21452;&#22240;&#26524;OT&#20013;&#30340;&#20540;&#20989;&#25968;&#12290;&#22312;&#21487;&#38598;&#20013;&#26465;&#20214;&#21644;&#36817;&#20284;&#23436;&#22791;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#65288;&#23616;&#37096;&#65289;Rademacher&#22797;&#26434;&#24230;&#35777;&#26126;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36866;&#24403;&#32467;&#26500;&#65292;&#28385;&#36275;&#26679;&#26412;&#22797;&#26434;&#24230;&#35777;&#26126;&#25152;&#38656;&#30340;&#20851;&#38190;&#20551;&#35774;&#26465;&#20214;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;FVI&#22312;&#26102;&#38388;&#36328;&#24230;&#22686;&#21152;&#26102;&#20248;&#20110;&#32447;&#24615;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;Sinkhorn&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#21487;&#25509;&#21463;&#31934;&#24230;&#30340;&#21516;&#26102;&#20855;&#26377;&#24456;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a fitted value iteration (FVI) method to compute bicausal optimal transport (OT) where couplings have an adapted structure. Based on the dynamic programming formulation, FVI adopts a function class to approximate the value functions in bicausal OT. Under the concentrability condition and approximate completeness assumption, we prove the sample complexity using (local) Rademacher complexity. Furthermore, we demonstrate that multilayer neural networks with appropriate structures satisfy the crucial assumptions required in sample complexity proofs. Numerical experiments reveal that FVI outperforms linear programming and adapted Sinkhorn methods in scalability as the time horizon increases, while still maintaining acceptable accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12646</link><description>&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#21644;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learnability and Algorithm for Continual Learning. (arXiv:2306.12646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#35774;&#32622;&#12290;CIL &#23398;&#20064;&#20102;&#19968;&#31995;&#21015;&#30001;&#19981;&#21516;&#27010;&#24565;&#25110;&#31867;&#21035;&#32452;&#25104;&#30340;&#20219;&#21153;&#24207;&#21015;&#12290;&#20219;&#20309;&#26102;&#20505;&#65292;&#37117;&#23558;&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#25110;&#20998;&#31867;&#21040;&#30446;&#21069;&#20026;&#27490;&#23398;&#20064;&#30340;&#20219;&#20309;&#31867;&#30340;&#27979;&#35797;&#23454;&#20363;&#65292;&#32780;&#19981;&#38656;&#35201;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#34429;&#28982;&#35768;&#22810;&#25216;&#26415;&#24050;&#25552;&#20986;&#29992;&#20110; CIL&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#26159;&#32463;&#39564;&#24615;&#30340;&#12290;&#26368;&#36817;&#26174;&#31034;&#20102;&#65292;&#24378;&#30340; CIL &#31995;&#32479;&#38656;&#35201;&#24378;&#30340;&#20219;&#21153;&#20869;&#39044;&#27979; (WP) &#21644;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#24378;&#30340;&#20998;&#24067;&#22806; (OOD) &#26816;&#27979;&#12290;&#28982;&#32780;&#65292;CIL &#26159;&#21542;&#30495;&#27491;&#21487;&#23398;&#20064;&#20173;&#26410;&#30693;&#12290;&#26412;&#25991;&#34920;&#26126;&#20102; CIL &#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#22522;&#20110;&#27492;&#65292;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; CIL &#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the challenging continual learning (CL) setting of Class Incremental Learning (CIL). CIL learns a sequence of tasks consisting of disjoint sets of concepts or classes. At any time, a single model is built that can be applied to predict/classify test instances of any classes learned thus far without providing any task related information for each test instance. Although many techniques have been proposed for CIL, they are mostly empirical. It has been shown recently that a strong CIL system needs a strong within-task prediction (WP) and a strong out-of-distribution (OOD) detection for each task. However, it is still not known whether CIL is actually learnable. This paper shows that CIL is learnable. Based on the theory, a new CIL algorithm is also proposed. Experimental results demonstrate its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#36807;&#24230;&#24179;&#28369;&#21644;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.12640</link><description>&lt;p&gt;
&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Addressing the Limitations of Graph Neural Networks. (arXiv:2306.12640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#36807;&#24230;&#24179;&#28369;&#21644;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#20851;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#36807;&#24230;&#24179;&#28369;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#35201;&#25506;&#32034;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report gives a summary of two problems about graph convolutional networks (GCNs): over-smoothing and heterophily challenges, and outlines future directions to explore.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12627</link><description>&lt;p&gt;
&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#22604;&#32553;&#27491;&#21017;&#21270;&#33258;&#32534;&#30721;&#22120;&#65306;&#20013;&#24515;&#30340;&#40657;&#27934;
&lt;/p&gt;
&lt;p&gt;
Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#24050;&#24191;&#27867;&#29992;&#20110;&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#24320;&#21457;&#20013;&#12290;&#23427;&#20204;&#30340;&#24212;&#29992;&#21069;&#25552;&#26159;&#22312;&#27491;&#24120;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#21518;&#65292;&#24322;&#24120;&#36755;&#20837;&#23558;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#24471;&#27491;&#24120;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#26377;&#20102;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#33258;&#32534;&#30721;&#22120;&#21487;&#20197;&#19968;&#23450;&#31243;&#24230;&#19978;&#27867;&#21270;&#21040;&#27491;&#24120;&#31867;&#20043;&#22806;&#65292;&#24182;&#22312;&#19968;&#20123;&#24322;&#24120;&#26679;&#26412;&#19978;&#23454;&#29616;&#36739;&#23567;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#20026;&#20102;&#25913;&#21892;&#24615;&#33021;&#65292;&#21508;&#31181;&#25216;&#26415;&#25552;&#20986;&#20102;&#20854;&#20182;&#32452;&#20214;&#21644;&#26356;&#22797;&#26434;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#19981;&#26159;&#28155;&#21152;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12289;&#28041;&#21450;&#35745;&#31639;&#21644;&#32321;&#29712;&#30340;&#35757;&#32451;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35843;&#33410;&#34920;&#31034;&#30340;&#33539;&#25968;&#65292;&#29992;&#19968;&#20010;&#35745;&#31639;&#31616;&#21333;&#30340;&#39033;&#26469;&#34917;&#20805;&#37325;&#26500;&#25439;&#22833;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#26368;&#23567;&#21270;&#20102;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;&#21644;&#38468;&#21152;&#20449;&#24687;&#30340;&#25509;&#36817;&#20851;&#31995;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#36890;&#20449;&#37327;&#21363;&#21487;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12625</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning through Importance Sampling. (arXiv:2306.12625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;&#21644;&#38468;&#21152;&#20449;&#24687;&#30340;&#25509;&#36817;&#20851;&#31995;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#36890;&#20449;&#37327;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#31471;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#26159;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#37325;&#35201;&#29942;&#39048;&#12290;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#27604;&#29305;&#29575;-&#20934;&#30830;&#24615;&#25240;&#34935;&#8212;&#8212;&#20854;&#20013;&#23458;&#25143;&#31471;n&#21457;&#36865;&#26469;&#33258;&#20165;&#20026;&#35813;&#23458;&#25143;&#31471;&#30340;&#27010;&#29575;&#20998;&#24067;q&#966;&#65288;n&#65289;&#30340;&#26679;&#26412;&#65292;&#26381;&#21153;&#22120;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#20272;&#35745;&#23458;&#25143;&#31471;&#20998;&#24067;&#30340;&#24179;&#22343;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;FL&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20855;&#26377;&#39044;&#25968;&#25454;&#20998;&#24067;p&#952;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#35813;&#20998;&#24067;&#19982;&#23458;&#25143;&#31471;&#20998;&#24067;q&#966;&#65288;n&#65289;&#22312;Kullback-Leibler&#65288;KL&#65289;&#21457;&#25955;&#26041;&#38754;&#25509;&#36817;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;q&#966;&#65288;n)&#19982;&#38468;&#21152;&#20449;&#24687;p&#952;&#20043;&#38388;&#30340;&#36825;&#31181;&#25509;&#36817;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#38656;&#35201;&#22823;&#32422;Dkl&#65288;q&#966;&#65288;n&#65289;|| p&#952;&#65289;&#20301;&#30340;&#36890;&#20449;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high communication cost of sending model updates from the clients to the server is a significant bottleneck for scalable federated learning (FL). Among existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been achieved using stochastic compression methods -- in which the client $n$ sends a sample from a client-only probability distribution $q_{\phi^{(n)}}$, and the server estimates the mean of the clients' distributions using these samples. However, such methods do not take full advantage of the FL setup where the server, throughout the training process, has side information in the form of a pre-data distribution $p_{\theta}$ that is close to the client's distribution $q_{\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit this closeness between the clients' distributions $q_{\phi^{(n)}}$'s and the side information $p_{\theta}$ at the server, and propose a framework that requires approximately $D_{KL}(q_{\phi^{(n)}}|| p_{\theta})$ bits of com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.12619</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22914;&#26524;&#23558;CIL&#23450;&#24335;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#26631;&#31614;&#29983;&#25104;&#38382;&#39064;&#65292;&#21017;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;CF&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CIL&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#20102;&#35789;&#27719;&#34920;&#30340;&#31232;&#30095;&#24615;&#20197;&#20415;&#20110;&#29983;&#25104;&#65292;&#24182;&#20351;&#29992;&#26631;&#31614;&#35821;&#20041;&#21019;&#24314;&#20266;&#37325;&#25773;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VAG&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#22823;&#24133;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
&lt;/p&gt;</description></item><item><title>RobustNeuralNetworks.jl&#26159;&#19968;&#20010;&#29992;Julia&#32534;&#20889;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21253;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#40065;&#26834;&#24615;&#32422;&#26463;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2306.12612</link><description>&lt;p&gt;
RobustNeuralNetworks.jl&#65306;&#24102;&#26377;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
RobustNeuralNetworks.jl: a Package for Machine Learning and Data-Driven Control with Certified Robustness. (arXiv:2306.12612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12612
&lt;/p&gt;
&lt;p&gt;
RobustNeuralNetworks.jl&#26159;&#19968;&#20010;&#29992;Julia&#32534;&#20889;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21253;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#40065;&#26834;&#24615;&#32422;&#26463;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23545;&#20110;&#24494;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#20986;&#29616;&#24847;&#22806;&#25110;&#33030;&#24369;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RobustNeuralNetworks.jl&#65306;&#19968;&#20010;Julia&#21253;&#65292;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33258;&#28982;&#22320;&#28385;&#36275;&#19968;&#32452;&#29992;&#25143;&#23450;&#20041;&#30340;&#40065;&#26834;&#24615;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#21253;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;Recurrent Equilibrium Network&#65288;REN&#65289;&#21644;Lipschitz-Bounded Deep Network&#65288;LBDN&#65289;&#27169;&#22411;&#31867;&#65292;&#24182;&#26088;&#22312;&#30452;&#25509;&#19982;Julia&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#21253;Flux.jl&#25509;&#21475;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27169;&#22411;&#21442;&#25968;&#21270;&#32972;&#21518;&#30340;&#29702;&#35770;&#65292;&#27010;&#36848;&#20102;&#35813;&#21253;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25945;&#31243;&#65292;&#28436;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#38750;&#32447;&#24615;&#29366;&#24577;&#35266;&#27979;&#22120;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are typically sensitive to small input perturbations, leading to unexpected or brittle behaviour. We present RobustNeuralNetworks.jl: a Julia package for neural network models that are constructed to naturally satisfy a set of user-defined robustness constraints. The package is based on the recently proposed Recurrent Equilibrium Network (REN) and Lipschitz-Bounded Deep Network (LBDN) model classes, and is designed to interface directly with Julia's most widely-used machine learning package, Flux.jl. We discuss the theory behind our model parameterization, give an overview of the package, and provide a tutorial demonstrating its use in image classification, reinforcement learning, and nonlinear state-observer design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24120;&#25968;&#20869;&#23384;&#27880;&#24847;&#21147;&#22359;&#65288;CMAB&#65289;&#65292;&#29992;&#20110;&#22312;&#24120;&#25968;&#20869;&#23384;&#20013;&#35745;&#31639;&#36755;&#20986;&#24182;&#25191;&#34892;&#26356;&#26032;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12599</link><description>&lt;p&gt;
&#24120;&#25968;&#20869;&#23384;&#27880;&#24847;&#21147;&#22359;
&lt;/p&gt;
&lt;p&gt;
Constant Memory Attention Block. (arXiv:2306.12599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24120;&#25968;&#20869;&#23384;&#27880;&#24847;&#21147;&#22359;&#65288;CMAB&#65289;&#65292;&#29992;&#20110;&#22312;&#24120;&#25968;&#20869;&#23384;&#20013;&#35745;&#31639;&#36755;&#20986;&#24182;&#25191;&#34892;&#26356;&#26032;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22522;&#30784;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#20381;&#36182;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#26377;&#25928;&#25429;&#33719;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36755;&#20837;/&#25968;&#25454;&#28857;&#25968;&#37327;&#26041;&#38754;&#38656;&#35201;&#32447;&#24615;&#25110;&#20108;&#27425;&#20869;&#23384;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20302;&#35745;&#31639;&#39046;&#22495;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24120;&#25968;&#20869;&#23384;&#27880;&#24847;&#21147;&#22359;&#65288;CMAB&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#27880;&#24847;&#21147;&#22359;&#65292;&#20854;&#22312;&#24120;&#25968;&#20869;&#23384;&#20013;&#35745;&#31639;&#20854;&#36755;&#20986;&#65292;&#24182;&#22312;&#24120;&#25968;&#35745;&#31639;&#20013;&#25191;&#34892;&#26356;&#26032;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;CMAB&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#36807;&#31243;&#21644;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20869;&#23384;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern foundation model architectures rely on attention mechanisms to effectively capture context. However, these methods require linear or quadratic memory in terms of the number of inputs/datapoints, limiting their applicability in low-compute domains. In this work, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that computes its output in constant memory and performs updates in constant computation. Highlighting CMABs efficacy, we introduce methods for Neural Processes and Temporal Point Processes. Empirically, we show our proposed methods achieve results competitive with state-of-the-art while being significantly more memory efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#24490;&#29615;&#24037;&#20316;&#27969;&#65292;&#29992;&#20110;&#33258;&#28982;&#28798;&#23475;&#21518;&#24555;&#36895;&#35757;&#32451;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;2023&#24180;&#23494;&#35199;&#35199;&#27604;&#24030;&#28378;&#21160;&#21449;&#21475;&#40857;&#21367;&#39118;&#20107;&#20214;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.12589</link><description>&lt;p&gt;
&#24555;&#36895;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#24037;&#20316;&#27969;&#65306;&#38024;&#23545;2023&#24180;&#23494;&#35199;&#35199;&#27604;&#24030;&#28378;&#21160;&#21449;&#21475;&#40857;&#21367;&#39118;&#20107;&#20214;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event. (arXiv:2306.12589v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#24490;&#29615;&#24037;&#20316;&#27969;&#65292;&#29992;&#20110;&#33258;&#28982;&#28798;&#23475;&#21518;&#24555;&#36895;&#35757;&#32451;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;2023&#24180;&#23494;&#35199;&#35199;&#27604;&#24030;&#28378;&#21160;&#21449;&#21475;&#40857;&#21367;&#39118;&#20107;&#20214;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#28798;&#23475;&#21518;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#23545;&#20110;&#25351;&#23548;&#21644;&#20248;&#21270;&#31532;&#19968;&#24212;&#31572;&#32773;&#30340;&#21162;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#28798;&#23475;&#25439;&#20260;&#12289;&#21355;&#26143;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#32570;&#20047;&#24191;&#27867;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#31561;&#38382;&#39064;&#65292;&#20197;&#33258;&#21160;&#21270;&#26041;&#24335;&#25191;&#34892;&#27492;&#31867;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#24182;&#19981;&#23481;&#26131;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#24490;&#29615;&#24037;&#20316;&#27969;&#65292;&#29992;&#20110;&#22312;&#33258;&#28982;&#28798;&#23475;&#21518;&#24555;&#36895;&#35757;&#32451;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#27169;&#22411;&#12290;&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#27492;&#24037;&#20316;&#27969;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#24037;&#20316;&#27969;&#26159;&#19982;&#32654;&#22269;&#32418;&#21313;&#23383;&#20250;&#21512;&#20316;&#25191;&#34892;&#30340;&#65292;&#38024;&#23545;2023&#24180;3&#26376;&#23494;&#35199;&#35199;&#27604;&#24030;&#28378;&#21160;&#21449;&#21475;&#40857;&#21367;&#39118;&#20107;&#20214;&#12290;&#26681;&#25454;&#21518;&#28798;&#24773;&#25910;&#38598;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#20154;&#22312;&#24490;&#29615;&#27169;&#22411;&#36807;&#31243;&#30340;&#36755;&#20986;&#22312;&#21463;&#25439;&#24314;&#31569;&#26041;&#38754;&#23454;&#29616;&#20102;0.86&#30340;&#31934;&#24230;&#21644;0.80&#30340;&#21484;&#22238;&#29575;&#12290;&#36825;&#20010;&#24037;&#20316;&#27969;&#30340;&#31471;&#21040;&#31471;&#23454;&#29616;&#26102;&#38388;&#19981;&#21040;2&#20010;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid and accurate building damage assessments from high-resolution satellite imagery following a natural disaster is essential to inform and optimize first responder efforts. However, performing such building damage assessments in an automated manner is non-trivial due to the challenges posed by variations in disaster-specific damage, diversity in satellite imagery, and the dearth of extensive, labeled datasets. To circumvent these issues, this paper introduces a human-in-the-loop workflow for rapidly training building damage assessment models after a natural disaster. This article details a case study using this workflow, executed in partnership with the American Red Cross during a tornado event in Rolling Fork, Mississippi in March, 2023. The output from our human-in-the-loop modeling process achieved a precision of 0.86 and recall of 0.80 for damaged buildings when compared to ground truth data collected post-disaster. This workflow was implemented end-to-end in under 2 hours per s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#31070;&#32463;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20284;&#28982;&#20989;&#25968;&#19981;&#21487;&#35745;&#31639;&#20294;&#21487;&#20197;&#36890;&#36807;&#21069;&#21521;&#27169;&#25311;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#26368;&#20248;&#27010;&#29575;&#25512;&#26029;&#65292;&#30528;&#37325;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#20945;&#30340;&#21442;&#25968;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2306.12584</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#31070;&#32463;&#27169;&#25311;&#30340;&#20107;&#20214;&#38598;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Neural Simulation-Based Inference Over Event Ensembles. (arXiv:2306.12584v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#31070;&#32463;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20284;&#28982;&#20989;&#25968;&#19981;&#21487;&#35745;&#31639;&#20294;&#21487;&#20197;&#36890;&#36807;&#21069;&#21521;&#27169;&#25311;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#26368;&#20248;&#27010;&#29575;&#25512;&#26029;&#65292;&#30528;&#37325;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#20945;&#30340;&#21442;&#25968;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20107;&#20214;&#38598;&#26159;&#24120;&#35265;&#30340;&#35266;&#27979;&#20540;&#38598;&#21512;&#65292;&#23427;&#20204;&#20849;&#21516;&#32422;&#26463;&#20102;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#23618;&#32423;&#32467;&#26500;&#65292;&#20854;&#20013;&#8220;&#23616;&#37096;&#8221;&#21442;&#25968;&#24433;&#21709;&#21333;&#20010;&#20107;&#20214;&#65292;&#8220;&#20840;&#23616;&#8221;&#21442;&#25968;&#24433;&#21709;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#19981;&#21487;&#35745;&#31639;&#20294;&#21487;&#20197;&#36890;&#36807;&#21069;&#21521;&#27169;&#25311;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#26368;&#20248;&#27010;&#29575;&#25512;&#26029;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20284;&#28982;&#20989;&#25968;&#65288;&#27604;&#65289;&#25110;&#21518;&#39564;&#27010;&#29575;&#30340;&#31070;&#32463;&#20272;&#35745;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#26126;&#30830;&#32771;&#34385;&#27169;&#22411;&#23618;&#32423;&#32467;&#26500;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#20945;&#30340;&#21442;&#25968;&#32422;&#26463;&#12290;&#25105;&#20204;&#20197;&#29289;&#29702;&#31185;&#23398;&#20026;&#20363;&#30740;&#31350;&#20102;&#26412;&#25991;&#35752;&#35770;&#30340;&#20869;&#23481;&#65292;&#30528;&#37325;&#20110;&#31890;&#23376;&#29289;&#29702;&#23398;&#65288;&#31890;&#23376;&#23545;&#25758;&#26426;&#25968;&#25454;&#65289;&#21644;&#22825;&#20307;&#29289;&#29702;&#23398;&#65288;&#24378;&#24341;&#21147;&#36879;&#38236;&#35266;&#27979;&#65289;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where "local" parameters impact individual events and "global" parameters influence the entire dataset. We introduce practical approaches for optimal dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via forward modeling. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics (particle collider data) and astrophysics (strong gravitational lensing observations).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#22238;&#24402;&#20013;&#23558;&#29983;&#25104;&#25968;&#25454;&#19982;&#23545;&#25239;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12582</link><description>&lt;p&gt;
&#29983;&#25104;&#25968;&#25454;&#22312;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#65306;&#19968;&#39033;&#28176;&#36817;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training with Generated Data in High-Dimensional Regression: An Asymptotic Study. (arXiv:2306.12582v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#22238;&#24402;&#20013;&#23558;&#29983;&#25104;&#25968;&#25454;&#19982;&#23545;&#25239;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#65288;&#20363;&#22914;\cite{carmon2019unlabeled,gowal2021improving,xing2022artificial}&#65289;&#34920;&#26126;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#21152;&#20837;&#24102;&#20266;&#26631;&#31614;&#30340;&#39069;&#22806;&#30495;&#23454;&#25110;&#29983;&#25104;&#25968;&#25454;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#22312;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#23545;&#35813;&#26041;&#27861;&#30340;&#28176;&#36817;&#34892;&#20026;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#22312;&#26080;&#23725;&#35757;&#32451;&#20013;&#23384;&#22312;&#21452;&#23792;&#29616;&#35937;&#65292;&#20294;&#22312;&#36866;&#24403;&#30340;$\mathcal{L}_2$&#27491;&#21017;&#21270;&#20013;&#65292;&#20004;&#38454;&#27573;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#30340;&#24555;&#36895;&#20132;&#21449;&#39564;&#35777;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, studies such as \cite{carmon2019unlabeled,gowal2021improving,xing2022artificial} have demonstrated that incorporating additional real or generated data with pseudo-labels can enhance adversarial training through a two-stage training approach. In this paper, we perform a theoretical analysis of the asymptotic behavior of this method in high-dimensional linear regression. While a double-descent phenomenon can be observed in ridgeless training, with an appropriate $\mathcal{L}_2$ regularization, the two-stage adversarial training achieves a better performance. Finally, we derive a shortcut cross-validation formula specifically tailored for the two-stage training method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#37327;&#21270;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31227;&#38500;&#29983;&#32974;&#26356;&#26032;&#24555;&#36895;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#23567;&#21270;&#30340;&#27515;&#21333;&#20803;&#65292;&#24182;&#20026;&#28418;&#31227;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#29992;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.12574</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#31227;&#38500;&#29983;&#32974;&#26356;&#26032;&#27969;&#25968;&#25454;&#22312;&#32447;&#37327;&#21270;&#30340;&#39640;&#25928;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An efficient and straightforward online quantization method for a data stream through remove-birth updating. (arXiv:2306.12574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#37327;&#21270;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31227;&#38500;&#29983;&#32974;&#26356;&#26032;&#24555;&#36895;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#23567;&#21270;&#30340;&#27515;&#21333;&#20803;&#65292;&#24182;&#20026;&#28418;&#31227;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#29992;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#35774;&#22791;&#30340;&#22686;&#38271;&#27491;&#22312;&#21019;&#36896;&#20986;&#22823;&#37327;&#25968;&#25454;&#65292;&#21363;&#25152;&#35859;&#30340;&#22823;&#25968;&#25454;&#65292;&#24182;&#23545;&#26377;&#25928;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#36825;&#20123;&#25968;&#25454;&#19981;&#26029;&#22320;&#20135;&#29983;&#65292;&#24418;&#25104;&#20102;&#21160;&#24577;&#27969;&#25968;&#25454;&#12290;&#27969;&#25968;&#25454;&#30340;&#29305;&#24449;&#21487;&#33021;&#20250;&#21160;&#24577;&#21464;&#21270;&#65292;&#36825;&#31181;&#21464;&#21270;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#12290;&#22240;&#27492;&#65292;&#22788;&#29702;&#27969;&#25968;&#25454;&#30340;&#26041;&#27861;&#24517;&#39035;&#22312;&#21160;&#24577;&#36866;&#24212;&#36825;&#20123;&#21464;&#21270;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#32553;&#20943;&#23427;&#20204;&#30340;&#20307;&#31215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#27010;&#24565;&#28418;&#31227;&#22312;&#32447;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31227;&#38500;&#29983;&#32974;&#26356;&#26032;&#35782;&#21035;&#24182;&#26367;&#25442;&#27010;&#29575;&#20302;&#30340;&#21333;&#20803;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#20135;&#29983;&#26368;&#23567;&#21270;&#30340;&#27515;&#21333;&#20803;&#12290;&#26412;&#30740;&#31350;&#36824;&#34920;&#26126;&#19968;&#20123;&#36890;&#36807;&#25152;&#25552;&#20986;&#26041;&#27861;&#35745;&#31639;&#20986;&#26469;&#30340;&#24230;&#37327;&#25351;&#26631;&#23545;&#20110;&#28418;&#31227;&#26816;&#27979;&#23558;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of network-connected devices is creating an explosion of data, known as big data, and posing significant challenges to efficient data analysis. This data is generated continuously, creating a dynamic flow known as a data stream. The characteristics of a data stream may change dynamically, and this change is known as concept drift. Consequently, a method for handling data streams must efficiently reduce their volume while dynamically adapting to these changing characteristics. This paper proposes a simple online vector quantization method for concept drift. The proposed method identifies and replaces units with low win probability through remove-birth updating, thus achieving a rapid adaptation to concept drift. Furthermore, the results of this study show that the proposed method can generate minimal dead units even in the presence of concept drift. This study also suggests that some metrics calculated from the proposed method will be helpful for drift detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#39044;&#27979;&#25439;&#22833;&#30340;&#36741;&#21161;&#30417;&#30563;&#26041;&#24335;&#65292;&#23637;&#31034;&#20102;&#22312;&#28436;&#31034;&#25968;&#37327;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#20196;&#24314;&#27169;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12554</link><description>&lt;p&gt;
&#36890;&#36807;&#25351;&#20196;&#39044;&#27979;&#25552;&#39640;&#38271;&#26399;&#27169;&#20223;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving Long-Horizon Imitation Through Instruction Prediction. (arXiv:2306.12554v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#39044;&#27979;&#25439;&#22833;&#30340;&#36741;&#21161;&#30417;&#30563;&#26041;&#24335;&#65292;&#23637;&#31034;&#20102;&#22312;&#28436;&#31034;&#25968;&#37327;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#20196;&#24314;&#27169;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#38271;&#26399;&#35745;&#21010;&#21450;&#20854;&#32452;&#21512;&#24615;&#36136;&#23545;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#26469;&#35828;&#26159;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#36807;&#24230;&#25311;&#21512;&#25233;&#21046;&#20102;&#27867;&#21270;&#65292;&#24182;&#19988;&#32047;&#31215;&#35823;&#24046;&#25439;&#23475;&#20102;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#36890;&#24120;&#26410;&#20351;&#29992;&#30340;&#36741;&#21161;&#30417;&#30563;&#26041;&#24335;&#65306;&#35821;&#35328;&#12290;&#21463;&#26368;&#36817;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#25351;&#20196;&#39044;&#27979;&#25439;&#22833;&#26469;&#35757;&#32451;&#20195;&#29702;&#65292;&#20197;&#40723;&#21169;&#23398;&#20064;&#22312;&#39640;&#23618;&#27425;&#19978;&#25805;&#20316;&#30340;&#20855;&#26377;&#26102;&#38388;&#25193;&#23637;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;BabyAI&#21644;Crafter&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25351;&#20196;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#35268;&#21010;&#29615;&#22659;&#20013;&#21463;&#38480;&#30340;&#28436;&#31034;&#25968;&#37327;&#26102;&#30340;&#34920;&#29616;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25351;&#20196;&#24314;&#27169;&#23545;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#26368;&#20026;&#37325;&#35201;&#65292;&#32780;&#22312;&#38656;&#35201;&#31616;&#21333;&#35745;&#21010;&#30340;&#29615;&#22659;&#20013;&#21017;&#21487;&#20197;&#29702;&#35299;&#22320;&#33719;&#24471;&#26356;&#23567;&#30340;&#25910;&#30410;&#12290;&#26356;&#22810;&#32454;&#33410;&#21644;&#20195;&#30721;&#21487;&#22312;[https://github.com/facebookresearch/long-term-fairness]&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex, long-horizon planning and its combinatorial nature pose steep challenges for learning-based agents. Difficulties in such settings are exacerbated in low data regimes where over-fitting stifles generalization and compounding errors hurt accuracy. In this work, we explore the use of an often unused source of auxiliary supervision: language. Inspired by recent advances in transformer-based models, we train agents with an instruction prediction loss that encourages learning temporally extended representations that operate at a high level of abstraction. Concretely, we demonstrate that instruction modeling significantly improves performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter benchmarks. In further analysis we find that instruction modeling is most important for tasks that require complex reasoning, while understandably offering smaller gains in environments that require simple plans. More details and code can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#26102;&#38388;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#65292;&#21457;&#29616;&#27491;&#25351;&#25968;&#30340;&#33034;&#32447;&#23558;&#36755;&#20837;&#31354;&#38388;&#20998;&#25104;&#19981;&#21516;&#21306;&#22495;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.12548</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#26102;&#38388;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Finite-time Lyapunov exponents of deep neural networks. (arXiv:2306.12548v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#26102;&#38388;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#65292;&#21457;&#29616;&#27491;&#25351;&#25968;&#30340;&#33034;&#32447;&#23558;&#36755;&#20837;&#31354;&#38388;&#20998;&#25104;&#19981;&#21516;&#21306;&#22495;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35745;&#31639;&#20102;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#22914;&#20309;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#25506;&#32034;&#28145;&#24230;&#32593;&#32476;&#19982;&#21160;&#21147;&#31995;&#32479;&#20043;&#38388;&#30340;&#31867;&#27604;&#65292;&#20854;&#20013;&#23616;&#37096;&#25200;&#21160;&#30340;&#22686;&#38271;&#25110;&#34928;&#20943;&#30001;&#26377;&#38480;&#26102;&#38388;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#26469;&#25551;&#36848;&#12290;&#25105;&#20204;&#26174;&#31034;&#26368;&#22823;&#25351;&#25968;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#24418;&#25104;&#20960;&#20309;&#32467;&#26500;&#65292;&#31867;&#20284;&#20110;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#30456;&#24178;&#32467;&#26500;&#12290;&#22823;&#27491;&#25351;&#25968;&#30340;&#33034;&#32447;&#23558;&#36755;&#20837;&#31354;&#38388;&#20998;&#25104;&#32593;&#32476;&#23558;&#20854;&#19982;&#19981;&#21516;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#19981;&#21516;&#21306;&#22495;&#12290;&#36825;&#20123;&#33034;&#32447;&#21487;&#35270;&#21270;&#28145;&#24230;&#32593;&#32476;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#26500;&#24314;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#25581;&#31034;&#20102;&#20854;&#23398;&#20064;&#33021;&#21147;&#32972;&#21518;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compute how small input perturbations affect the output of deep neural networks, exploring an analogy between deep networks and dynamical systems, where the growth or decay of local perturbations is characterised by finite-time Lyapunov exponents. We show that the maximal exponent forms geometrical structures in input space, akin to coherent structures in dynamical systems. Ridges of large positive exponents divide input space into different regions that the network associates with different classes. These ridges visualise the geometry that deep networks construct in input space, shedding light on the fundamental mechanisms underlying their learning capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#26041;&#27861;MGxTransformer&#65292;&#32467;&#21512;&#20102;VPTR&#21644;&#22810;&#37325;&#32593;&#26684;&#26550;&#26500;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#27169;&#25311;&#32467;&#26524;&#26356;&#20026;&#20934;&#30830;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.12545</link><description>&lt;p&gt;
&#31070;&#32463;&#22810;&#37325;&#32593;&#26684;&#20869;&#23384;&#29992;&#20110;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Neural Multigrid Memory For Computational Fluid Dynamics. (arXiv:2306.12545v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#26041;&#27861;MGxTransformer&#65292;&#32467;&#21512;&#20102;VPTR&#21644;&#22810;&#37325;&#32593;&#26684;&#26550;&#26500;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#27169;&#25311;&#32467;&#26524;&#26356;&#20026;&#20934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#21253;&#25324;&#39134;&#26426;&#21644;&#33337;&#33334;&#35774;&#35745;&#12289;&#24037;&#19994;&#27969;&#31243;&#20248;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25968;&#25454;&#39537;&#21160;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#35270;&#39057;&#39044;&#27979;&#21464;&#25442;&#22120;&#65288;VPTR&#65289;&#65288;Ye &amp; Bilodeau, 2022&#65289;&#21644;&#22810;&#37325;&#32593;&#26684;&#26550;&#26500;&#65288;MgConv&#65292;MgResnet&#65289;&#65288;Ke&#31561;&#65292;2017&#65289;&#30340;&#20248;&#28857;&#12290;VPTR&#25797;&#38271;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#22788;&#29702;&#22823;&#22411;&#36755;&#20837;&#25968;&#25454;&#65292;&#26159;&#28237;&#27969;&#27969;&#21160;&#39044;&#27979;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22810;&#37325;&#32593;&#26684;&#26550;&#26500;&#21033;&#29992;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#32593;&#26684;&#26469;&#25429;&#25417;&#28237;&#27969;&#27969;&#21160;&#30340;&#22810;&#23610;&#24230;&#29305;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#27169;&#25311;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;MGxTransformer&#22312;&#20934;&#30830;&#39044;&#27979;&#36895;&#24230;&#12289;&#28201;&#24230;&#21644;&#28237;&#27969;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulent flow simulation plays a crucial role in various applications, including aircraft and ship design, industrial process optimization, and weather prediction. In this paper, we propose an advanced data-driven method for simulating turbulent flow, representing a significant improvement over existing approaches.  Our methodology combines the strengths of Video Prediction Transformer (VPTR) (Ye &amp; Bilodeau, 2022) and Multigrid Architecture (MgConv, MgResnet) (Ke et al., 2017). VPTR excels in capturing complex spatiotemporal dependencies and handling large input data, making it a promising choice for turbulent flow prediction. Meanwhile, Multigrid Architecture utilizes multiple grids with different resolutions to capture the multiscale nature of turbulent flows, resulting in more accurate and efficient simulations.  Through our experiments, we demonstrate the effectiveness of our proposed approach, named MGxTransformer, in accurately predicting velocity, temperature, and turbulence in
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#20984;&#20248;&#21270;&#38656;&#35201;&#22312;&#20869;&#23384;&#21644;&#26597;&#35810;&#20043;&#38388;&#26435;&#34913;&#65292;&#20351;&#29992; $\tilde{O}(d^2)$ &#27604;&#29305;&#20869;&#23384;&#21644; $\tilde{O}(d)$ &#27425;&#26597;&#35810;&#30340;&#21106;&#24179;&#38754;&#26041;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12534</link><description>&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#20869;&#23384;&#21644;&#26597;&#35810;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Memory-Query Tradeoffs for Randomized Convex Optimization. (arXiv:2306.12534v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12534
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#38656;&#35201;&#22312;&#20869;&#23384;&#21644;&#26597;&#35810;&#20043;&#38388;&#26435;&#34913;&#65292;&#20351;&#29992; $\tilde{O}(d^2)$ &#27604;&#29305;&#20869;&#23384;&#21644; $\tilde{O}(d)$ &#27425;&#26597;&#35810;&#30340;&#21106;&#24179;&#38754;&#26041;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21333;&#20301;&#29699;&#19978;&#26368;&#23567;&#21270;&#19968;&#20010; $d$ &#32500;&#12289;$1$-Lipschitz &#20984;&#20989;&#25968;&#65292;&#24517;&#39035;&#20351;&#29992; $\Omega(d^{2-\delta})$ &#27604;&#29305;&#30340;&#20869;&#23384;&#25110;&#32773;&#36827;&#34892; $\Omega(d^{1+\delta/6-o(1)})$ &#27425;&#26597;&#35810;&#65292;&#20854;&#20013; $\delta\in (0,1)$ &#26159;&#20219;&#24847;&#24120;&#25968;&#65292;&#31934;&#24230; $\epsilon$ &#22312; $d$ &#20013;&#26159;&#20934;&#22810;&#39033;&#24335;&#23567;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#65292;&#20351;&#29992; $\tilde{O}(d^2)$ &#27604;&#29305;&#20869;&#23384;&#21644; $\tilde{O}(d)$ &#27425;&#26597;&#35810;&#30340;&#21106;&#24179;&#38754;&#26041;&#27861;&#65292;&#22312;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#20013;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#65292;&#32780;&#23545;&#20110;&#20984;&#20248;&#21270;&#65292;&#38656;&#35201;&#20108;&#27425;&#20869;&#23384;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#26597;&#35810;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that any randomized first-order algorithm which minimizes a $d$-dimensional, $1$-Lipschitz convex function over the unit ball must either use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$ queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$ is quasipolynomially small in $d$. Our result implies that cutting plane methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries, are Pareto-optimal among randomized first-order algorithms, and quadratic memory is required to achieve optimal query complexity for convex optimization.
&lt;/p&gt;</description></item><item><title>SIDDMs&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#24555;&#36895;&#25910;&#25947;&#19988;&#19968;&#23450;&#31243;&#24230;&#19978;&#20445;&#35777;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.12511</link><description>&lt;p&gt;
&#21322;&#38544;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;SIDDMs&#65289;
&lt;/p&gt;
&lt;p&gt;
Semi-Implicit Denoising Diffusion Models (SIDDMs). (arXiv:2306.12511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12511
&lt;/p&gt;
&lt;p&gt;
SIDDMs&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#24555;&#36895;&#25910;&#25947;&#19988;&#19968;&#23450;&#31243;&#24230;&#19978;&#20445;&#35777;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#37327;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#24555;&#36895;&#37319;&#26679;&#32780;&#19981;&#29306;&#29298;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#65288;&#22914; DDPMS&#65289;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#65292;&#20016;&#23500;&#22810;&#26679;&#30340;&#26679;&#26412;&#65292;&#20294;&#21463;&#36845;&#20195;&#27493;&#39588;&#25968;&#37327;&#30340;&#22266;&#26377;&#38480;&#21046;&#32780;&#36895;&#24230;&#36739;&#24930;&#12290;Denoising Diffusion Generative Adversarial Networks (DDGAN) &#35797;&#22270;&#36890;&#36807;&#38598;&#25104; GAN &#27169;&#22411;&#29992;&#20110;&#25193;&#25955;&#36807;&#31243;&#30340;&#36739;&#22823;&#36339;&#36291;&#26469;&#35268;&#36991;&#27492;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;DDGAN &#36935;&#21040;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#38544;&#24335;&#21644;&#26174;&#24335;&#22240;&#23376;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21033;&#29992;&#38544;&#24335;&#27169;&#22411;&#26469;&#21305;&#37197;&#22024;&#26434;&#25968;&#25454;&#30340;&#36793;&#32536;&#20998;&#24067;&#21644;&#21069;&#21521;&#25193;&#25955;&#30340;&#26174;&#24335;&#26465;&#20214;&#20998;&#24067;&#12290;&#36825;&#31181;&#32452;&#21512;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#21305;&#37197;&#32852;&#21512;&#21435;&#22122;&#20998;&#24067;&#12290;&#19982; DDPMS &#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21322;&#38544;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;SIDDMs&#65289;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25152;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#37319;&#29992;U-Net&#21644;pretrained SAM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#38024;&#23545;&#20083;&#33146;&#36229;&#22768;&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#65292;&#36827;&#34892;&#32959;&#30244;&#21306;&#22495;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;U-Net&#27169;&#22411;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#33391;&#24615;&#21644;&#24694;&#24615;&#32959;&#30244;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.12510</link><description>&lt;p&gt;
&#22522;&#20110;U-Net&#21644;Segment Anything Model&#30340;&#20083;&#33146;&#32959;&#30244;&#22312;&#36229;&#22768;&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#20013;&#30340;&#23545;&#27604;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images. (arXiv:2306.12510v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12510
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#37319;&#29992;U-Net&#21644;pretrained SAM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#38024;&#23545;&#20083;&#33146;&#36229;&#22768;&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#65292;&#36827;&#34892;&#32959;&#30244;&#21306;&#22495;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;U-Net&#27169;&#22411;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#33391;&#24615;&#21644;&#24694;&#24615;&#32959;&#30244;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#22312;&#20083;&#33146;&#36229;&#22768;&#65288;BUS&#65289;&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#20013;&#35782;&#21035;&#21644;&#25551;&#32472;&#32959;&#30244;&#21306;&#22495;&#30340;&#31639;&#27861;&#12290;&#35813;&#25216;&#26415;&#37319;&#29992;&#20102;&#20004;&#31181;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;U-Net&#21644;pretrained SAM&#65292;&#29992;&#20110;&#32959;&#30244;&#20998;&#21106;&#12290;U-Net&#27169;&#22411;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#21033;&#29992;&#20854;&#28145;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;pretrained SAM&#26550;&#26500;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#25429;&#25417;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#24182;&#29983;&#25104;&#20998;&#21106;&#32467;&#26524;&#12290;&#35780;&#20272;&#22312;&#19981;&#21516;&#33391;&#24615;&#21644;&#24694;&#24615;&#32959;&#30244;&#30340;&#27880;&#37322;&#32959;&#30244;&#21306;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;U-Net&#27169;&#22411;&#33021;&#22815;&#22312;BUS&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#20013;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#20998;&#21106;&#20083;&#33146;&#32959;&#30244;&#65292;&#19988;&#20248;&#20110;pretrained SAM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, the main objective is to develop an algorithm capable of identifying and delineating tumor regions in breast ultrasound (BUS) and mammographic images. The technique employs two advanced deep learning architectures, namely U-Net and pretrained SAM, for tumor segmentation. The U-Net model is specifically designed for medical image segmentation and leverages its deep convolutional neural network framework to extract meaningful features from input images. On the other hand, the pretrained SAM architecture incorporates a mechanism to capture spatial dependencies and generate segmentation results. Evaluation is conducted on a diverse dataset containing annotated tumor regions in BUS and mammographic images, covering both benign and malignant tumors. This dataset enables a comprehensive assessment of the algorithm's performance across different tumor types. Results demonstrate that the U-Net model outperforms the pretrained SAM architecture in accurately identifying and segment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65288;DLN&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;&#30340;&#35821;&#35328;&#27169;&#22411;&#23618;&#65288;LLMs&#65289;&#65292;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#35757;&#32451;&#65292;&#20351;&#24471;DLN-2&#30340;&#24615;&#33021;&#29978;&#33267;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.12509</link><description>&lt;p&gt;
&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65306;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;LLM&#30340;&#25552;&#31034;&#23618;
&lt;/p&gt;
&lt;p&gt;
Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65288;DLN&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;&#30340;&#35821;&#35328;&#27169;&#22411;&#23618;&#65288;LLMs&#65289;&#65292;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#35757;&#32451;&#65292;&#20351;&#24471;DLN-2&#30340;&#24615;&#33021;&#29978;&#33267;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35270;&#20026;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#8220;&#35821;&#35328;&#23618;&#8221;&#65292;&#20854;&#20013;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#26159;&#27599;&#20010;&#23618;&#30340;&#33258;&#28982;&#35821;&#35328;&#8220;&#25552;&#31034;&#8221;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#36825;&#26679;&#30340;&#23618;&#21472;&#21152;&#22312;&#19968;&#36215;&#65292;&#23558;&#19968;&#20010;&#23618;&#30340;&#36755;&#20986;&#39304;&#36865;&#21040;&#19979;&#19968;&#20010;&#23618;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22534;&#21472;&#30340;&#32467;&#26500;&#31216;&#20026;&#8220;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#8221;&#65288;DLN&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#26377;&#25928;&#22320;&#38024;&#23545;&#21333;&#23618;&#35821;&#35328;&#32593;&#32476;&#65288;DLN-1&#65289;&#25191;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#35757;&#32451;2&#23618;DLNs&#65288;DLN-2&#65289;&#65292;&#20854;&#20013;&#24517;&#39035;&#23398;&#20064;&#20004;&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#31532;&#19968;&#23618;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#65292;&#38656;&#35201;&#36827;&#34892;&#36793;&#32536;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#25552;&#31034;&#35757;&#32451;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;DLN-2&#27604;&#21333;&#23618;&#36798;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#21363;&#20351;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;LLM&#26356;&#23567;&#19988;&#26356;&#24369;&#65292;&#20063;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;DLN&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65306;https://github.com/microsoft/deep-language-networks&#12290;
&lt;/p&gt;
&lt;p&gt;
We view large language models (LLMs) as stochastic \emph{language layers} in a network, where the learnable parameters are the natural language \emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LIME&#26041;&#27861;&#25506;&#32034;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19981;&#20339;&#24615;&#33021;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#30340;&#36133;&#34880;&#30151;&#26816;&#27979;&#20013;&#12290;&#36890;&#36807;&#20998;&#26512;&#38169;&#35823;&#20998;&#31867;&#23454;&#20363;&#65292;&#30830;&#23450;&#36896;&#25104;&#27169;&#22411;&#24615;&#33021;&#24046;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#24615;&#33021;&#19981;&#20339;&#30340;&#21306;&#22495;&#65292;&#24182;&#35745;&#31639;&#20986;&#20854;&#38169;&#35823;&#29575;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#35880;&#24910;&#30340;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#38477;&#20302;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.12507</link><description>&lt;p&gt;
&#22522;&#20110;LIME&#30340;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;&#40657;&#21283;&#23376;&#30340;&#24615;&#33021;&#19981;&#20339;&#21306;&#22495;&#65306;&#20197;&#36133;&#34880;&#30151;&#26816;&#27979;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Investigating Poor Performance Regions of Black Boxes: LIME-based Exploration in Sepsis Detection. (arXiv:2306.12507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LIME&#26041;&#27861;&#25506;&#32034;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19981;&#20339;&#24615;&#33021;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#30340;&#36133;&#34880;&#30151;&#26816;&#27979;&#20013;&#12290;&#36890;&#36807;&#20998;&#26512;&#38169;&#35823;&#20998;&#31867;&#23454;&#20363;&#65292;&#30830;&#23450;&#36896;&#25104;&#27169;&#22411;&#24615;&#33021;&#24046;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#24615;&#33021;&#19981;&#20339;&#30340;&#21306;&#22495;&#65292;&#24182;&#35745;&#31639;&#20986;&#20854;&#38169;&#35823;&#29575;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#35880;&#24910;&#30340;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#38477;&#20302;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#35299;&#37322;&#65288;LIME&#65289;&#26469;&#25552;&#20379;&#35299;&#37322;&#24615;&#22320;&#25551;&#36848;&#39640;&#39118;&#38505;&#36133;&#34880;&#30151;&#26816;&#27979;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#12290;&#36890;&#36807;&#20998;&#26512;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#35782;&#21035;&#36129;&#29486;&#20110;&#27425;&#20248;&#24615;&#33021;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#20998;&#26512;&#25581;&#31034;&#20986;&#20998;&#31867;&#22120;&#24615;&#33021;&#19981;&#20339;&#30340;&#21306;&#22495;&#65292;&#20174;&#32780;&#35745;&#31639;&#20986;&#36825;&#20123;&#21306;&#22495;&#20869;&#30340;&#38169;&#35823;&#29575;&#65292;&#36825;&#23545;&#20110;&#22312;&#36133;&#34880;&#30151;&#26816;&#27979;&#31561;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#19979;&#36827;&#34892;&#35880;&#24910;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20351;&#29992;eICU&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;&#20998;&#31867;&#22120;&#24615;&#33021;&#19981;&#20339;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#24182;&#22312;&#20851;&#38190;&#26102;&#21051;&#38477;&#20302;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting machine learning models remains a challenge, hindering their adoption in clinical settings. This paper proposes leveraging Local Interpretable Model-Agnostic Explanations (LIME) to provide interpretable descriptions of black box classification models in high-stakes sepsis detection. By analyzing misclassified instances, significant features contributing to suboptimal performance are identified. The analysis reveals regions where the classifier performs poorly, allowing the calculation of error rates within these regions. This knowledge is crucial for cautious decision-making in sepsis detection and other critical applications. The proposed approach is demonstrated using the eICU dataset, effectively identifying and visualizing regions where the classifier underperforms. By enhancing interpretability, our method promotes the adoption of machine learning models in clinical practice, empowering informed decision-making and mitigating risks in critical scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#26377;Shuffled SGD&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21407;&#22987;-&#23545;&#20598;&#35270;&#35282;&#21644;&#25913;&#36827;&#30028;&#38480;&#65292;&#26088;&#22312;&#35299;&#20915;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.12498</link><description>&lt;p&gt;
&#24102;&#26377;Shuffled SGD&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65306;&#21407;&#22987;-&#23545;&#20598;&#35270;&#35282;&#21644;&#25913;&#36827;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds. (arXiv:2306.12498v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#26377;Shuffled SGD&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21407;&#22987;-&#23545;&#20598;&#35270;&#35282;&#21644;&#25913;&#36827;&#30028;&#38480;&#65292;&#26088;&#22312;&#35299;&#20915;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#26222;&#36941;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#19982;&#27599;&#20010;&#26102;&#26399;&#20174;&#25968;&#25454;&#38598;&#20013;&#26080;&#26367;&#25442;&#38543;&#26426;&#25277;&#26679;&#21644;&#19982;&#65288;&#21487;&#33021;&#30340;&#65289;&#37325;&#25490;&#32451;&#30340;&#32463;&#39564;&#24815;&#20363;&#30456;&#21453;&#65292;SGD&#30340;&#29702;&#35770;&#23545;&#24212;&#36890;&#24120;&#20381;&#36182;&#20110;&#24102;&#26367;&#25442;&#30340;&#25277;&#26679;&#20551;&#35774;&#12290;&#20165;&#26368;&#36817;&#25165;&#20998;&#26512;&#20102;&#37319;&#29992;&#26080;&#26367;&#25442;&#25277;&#26679;&#30340;Shuffled SGD&#12290;&#23545;&#20110;&#20855;&#26377;$n$&#20010;&#32452;&#20214;&#21644;&#23545;&#20110;&#27599;&#20010;&#32452;&#20214;&#20989;&#25968;$L$-&#24179;&#28369;&#24615;&#20551;&#35774;&#30340;&#20984;&#26377;&#38480;&#21644;&#38382;&#39064;&#65292;&#22312;&#36275;&#22815;&#23567;&#30340;&#27493;&#38271;&#65288;$\mathcal{O}(\frac{1}{nL})$&#65289;&#19979;&#65292;&#23384;&#22312;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30028;&#38480;&#20284;&#20046;&#36807;&#20110;&#24754;&#35266; - &#23454;&#38469;&#19978;&#65292;&#39044;&#27979;&#30340;&#24615;&#33021;&#36890;&#24120;&#19981;&#27604;&#20840;&#26799;&#24230;&#19979;&#38477;&#26356;&#22909; - &#24182;&#19988;&#19982;&#32463;&#39564;&#35266;&#23519;&#19981;&#31526;&#12290;&#20026;&#20102;&#32553;&#23567;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#23558;&#28966;&#28857;&#20174;&#19968;&#33324;&#26377;&#38480;&#21644;&#38382;&#39064;&#38598;&#20013;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets without replacement and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of sampling with replacement. It is only very recently that SGD with sampling without replacement -- shuffled SGD -- has been analyzed. For convex finite sum problems with $n$ components and under the $L$-smoothness assumption for each component function, there are matching upper and lower bounds, under sufficiently small -- $\mathcal{O}(\frac{1}{nL})$ -- step sizes. Yet those bounds appear too pessimistic -- in fact, the predicted performance is generally no better than for full gradient descent -- and do not agree with the empirical observations. In this work, to narrow the gap between the theory and practice of shuffled SGD, we sharpen the focus from general finite sum problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31934;&#20934;&#31934;&#31070;&#21307;&#23398;&#39046;&#22495;&#30340;&#21313;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#30495;&#23454;&#19990;&#30028;&#20154;&#32676;&#36827;&#34892;&#30740;&#31350;&#12289;&#23545;&#20020;&#24202;&#32467;&#26524;&#23450;&#20041;&#30340;&#29616;&#23454;&#32771;&#34385;&#12289;&#32771;&#34385;&#27835;&#30103;&#30456;&#20851;&#22240;&#32032;&#20197;&#21450;&#20844;&#24179;&#24615;&#31561;&#65292;&#25552;&#20986;&#20102;&#23558;&#37325;&#28857;&#20174;&#22238;&#39038;&#30740;&#31350;&#36716;&#31227;&#21040;&#36828;&#26223;&#24615;&#23454;&#26045;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.12462</link><description>&lt;p&gt;
&#31934;&#20934;&#31934;&#31070;&#21307;&#23398;&#65306;&#39044;&#27979;&#30340;&#39044;&#27979;(arXiv:2306.12462v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
Precision psychiatry: predicting predictability. (arXiv:2306.12462v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31934;&#20934;&#31934;&#31070;&#21307;&#23398;&#39046;&#22495;&#30340;&#21313;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#30495;&#23454;&#19990;&#30028;&#20154;&#32676;&#36827;&#34892;&#30740;&#31350;&#12289;&#23545;&#20020;&#24202;&#32467;&#26524;&#23450;&#20041;&#30340;&#29616;&#23454;&#32771;&#34385;&#12289;&#32771;&#34385;&#27835;&#30103;&#30456;&#20851;&#22240;&#32032;&#20197;&#21450;&#20844;&#24179;&#24615;&#31561;&#65292;&#25552;&#20986;&#20102;&#23558;&#37325;&#28857;&#20174;&#22238;&#39038;&#30740;&#31350;&#36716;&#31227;&#21040;&#36828;&#26223;&#24615;&#23454;&#26045;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#31934;&#31070;&#21307;&#23398;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#20026;&#31934;&#31070;&#20581;&#24247;&#25252;&#29702;&#25552;&#20379;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;&#22810;&#20803;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21019;&#24314;&#22522;&#20110;&#20020;&#24202;&#25968;&#25454;(&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#30151;&#29366;&#35780;&#20272;&#12289;&#22522;&#22240;&#20449;&#24687;&#21644;&#33041;&#25104;&#20687;)&#30340;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#12290;&#34429;&#28982;&#25216;&#26415;&#21019;&#26032;&#21463;&#21040;&#20102;&#24456;&#22823;&#30340;&#37325;&#35270;&#65292;&#20294;&#30001;&#20110;&#31934;&#31070;&#20581;&#24247;&#30340;&#22797;&#26434;&#21644;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#21151;&#23454;&#26045;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#31934;&#20934;&#31934;&#31070;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#21313;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#38656;&#23545;&#30495;&#23454;&#19990;&#30028;&#20154;&#32676;&#36827;&#34892;&#30740;&#31350;&#20197;&#21450;&#23545;&#20020;&#24202;&#32467;&#26524;&#23450;&#20041;&#36827;&#34892;&#29616;&#23454;&#32771;&#34385;&#65292;&#38656;&#32771;&#34385;&#27835;&#30103;&#30456;&#20851;&#22240;&#32032;&#65292;&#22914;&#23433;&#24944;&#21058;&#25928;&#24212;&#21644;&#19981;&#36981;&#20174;&#22788;&#26041;&#12290;&#20844;&#24179;&#24615;&#12289;&#19982;&#29616;&#34892;&#23454;&#36341;&#30340;&#21069;&#30651;&#24615;&#39564;&#35777;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#23454;&#26045;&#30740;&#31350;&#31561;&#20854;&#20182;&#20851;&#38190;&#38382;&#39064;&#30446;&#21069;&#34987;&#20302;&#20272;&#20102;&#12290;&#25552;&#20986;&#20102;&#23558;&#37325;&#28857;&#20174;&#22238;&#39038;&#30740;&#31350;&#36716;&#31227;&#21040;&#36828;&#26223;&#24615;&#23454;&#26045;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precision psychiatry is an ermerging field that aims to provide individualized approaches to mental health care. Multivariate analysis and machine learning are used to create outcome prediction models based on clinical data such as demographics, symptom assessments, genetic information, and brain imaging. While much emphasis has been placed on technical innovation, the complex and varied nature of mental health presents significant challenges to the successful implementation of these models. From this perspective, I review ten challenges in the field of precision psychiatry, including the need for studies on real-world populations and realistic clinical outcome definitions, consideration of treatment-related factors such as placebo effects and non-adherence to prescriptions. Fairness, prospective validation in comparison to current practice and implementation studies of prediction models are other key issues that are currently understudied. A shift is proposed from retrospective studie
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#19968;&#31181;&#28145;&#24230;&#21160;&#24577;&#27969;&#34892;&#30149;&#23398;&#30340;&#27169;&#22411;&#65292;&#23558;&#27969;&#34892;&#30149;&#23398;&#26041;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;COVID-19&#22312;&#22810;&#32423;&#21306;&#22495;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#21487;&#35270;&#21270;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.12457</link><description>&lt;p&gt;
&#22810;&#32423;&#21306;&#22495;COVID-19&#28145;&#24230;&#21160;&#24577;&#27969;&#34892;&#30149;&#23398;&#24314;&#27169;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in Multi-level Districts. (arXiv:2306.12457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#19968;&#31181;&#28145;&#24230;&#21160;&#24577;&#27969;&#34892;&#30149;&#23398;&#30340;&#27169;&#22411;&#65292;&#23558;&#27969;&#34892;&#30149;&#23398;&#26041;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;COVID-19&#22312;&#22810;&#32423;&#21306;&#22495;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#21487;&#35270;&#21270;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;COVID-19&#24050;&#32463;&#20840;&#29699;&#34067;&#24310;&#24182;&#23545;&#25972;&#20010;&#19990;&#30028;&#20135;&#29983;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#12290;&#27169;&#25311;COVID-19&#20256;&#25773;&#29366;&#20917;&#23545;&#20102;&#35299;&#24403;&#21069;&#29366;&#24577;&#21644;&#21046;&#23450;&#24178;&#39044;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;SEIR&#27169;&#22411;&#30340;&#27969;&#34892;&#30149;&#23398;&#26041;&#31243;&#27169;&#25311;&#30142;&#30149;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#19981;&#33021;&#31934;&#30830;&#22320;&#25311;&#21512;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#30001;&#20110;&#19981;&#21516;&#30340;&#24773;&#20917;&#65292;&#22914;&#31038;&#20132;&#36317;&#31163;&#25919;&#31574;&#21644;&#24178;&#39044;&#31574;&#30053;&#31561;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#25311;&#21512;&#24615;&#33021;&#65292;&#20294;&#26080;&#27861;&#21487;&#35270;&#21270;&#26426;&#21046;&#12290;&#26041;&#27861;&#65306;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21160;&#24577;&#27969;&#34892;&#30149;&#23398;&#65288;DDE&#65289;&#26041;&#27861;&#65292;&#23558;&#27969;&#34892;&#30149;&#23398;&#26041;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#39640;&#31934;&#24230;&#21644;&#21487;&#35270;&#21270;&#12290; DDE&#21253;&#21547;&#28145;&#24230;&#32593;&#32476;&#20197;&#36866;&#24212;&#25928;&#24212;&#20989;&#25968;&#65292;&#20197;&#22522;&#20110;&#31070;&#32463;ODE&#26041;&#27861;&#35299;&#20915;&#21464;&#37327;&#26041;&#31243;&#26469;&#27169;&#25311;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#65292;&#30830;&#20445;&#25311;&#21512;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Objective: COVID-19 has spread worldwide and made a huge influence across the world. Modeling the infectious spread situation of COVID-19 is essential to understand the current condition and to formulate intervention measurements. Epidemiological equations based on the SEIR model simulate disease development. The traditional parameter estimation method to solve SEIR equations could not precisely fit real-world data due to different situations, such as social distancing policies and intervention strategies. Additionally, learning-based models achieve outstanding fitting performance, but cannot visualize mechanisms. Methods: Thus, we propose a deep dynamic epidemiological (DDE) method that combines epidemiological equations and deep-learning advantages to obtain high accuracy and visualization. The DDE contains deep networks to fit the effect function to simulate the ever-changing situations based on the neural ODE method in solving variants' equations, ensuring the fitting performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DVAE.CIV &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#20174;&#24102;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#20998;&#35299;&#26465;&#20214; IV &#21644;&#20854;&#26465;&#20214;&#38598;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.12453</link><description>&lt;p&gt;
&#23398;&#20064;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#34920;&#31034;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Conditional Instrumental Variable Representation for Causal Effect Estimation. (arXiv:2306.12453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DVAE.CIV &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#20174;&#24102;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#20998;&#35299;&#26465;&#20214; IV &#21644;&#20854;&#26465;&#20214;&#38598;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#27835;&#30103;&#23545;&#24863;&#20852;&#36259;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#32463;&#24120;&#21463;&#21040;&#28151;&#28102;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#20102;&#27835;&#30103;&#21644;&#32467;&#26524;&#12290; &#24037;&#20855;&#21464;&#37327; (IV) &#26041;&#27861;&#26159;&#28040;&#38500;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#28151;&#28102;&#20559;&#24046;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110; IV &#30340;&#20272;&#35745;&#22120;&#38656;&#35201;&#26377;&#19968;&#20010;&#34987;&#25552;&#21517;&#30340; IV&#65292;&#32780;&#23545;&#20110;&#26465;&#20214; IV (CIV) &#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38656;&#35201;&#20854;&#30456;&#24212;&#30340;&#26465;&#20214;&#38598;&#12290; &#22522;&#20110;&#36825;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DVAE.CIV &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#20174;&#20855;&#26377;&#28151;&#28102;&#22240;&#32032;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#20998;&#35299; CIV &#30340;&#34920;&#31034;&#21644;&#20854;&#26465;&#20214;&#38598;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#21644;&#23545;&#28151;&#28102;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental challenges in causal inference is to estimate the causal effect of a treatment on its outcome of interest from observational data. However, causal effect estimation often suffers from the impacts of confounding bias caused by unmeasured confounders that affect both the treatment and the outcome. The instrumental variable (IV) approach is a powerful way to eliminate the confounding bias from latent confounders. However, the existing IV-based estimators require a nominated IV, and for a conditional IV (CIV) the corresponding conditioning set too, for causal effect estimation. This limits the application of IV-based estimators. In this paper, by leveraging the advantage of disentangled representation learning, we propose a novel method, named DVAE.CIV, for learning and disentangling the representations of CIV and the representations of its conditioning set for causal effect estimations from data with latent confounders. Extensive experimental results on both synthet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#22810;&#39033;&#36164;&#20135;&#27874;&#21160;&#29575;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#21450;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#26368;&#20248;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.12446</link><description>&lt;p&gt;
&#27604;&#36739;&#22810;&#20803;&#25968;&#25454;&#19979;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27874;&#21160;&#29575;&#39044;&#27979;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Comparing deep learning models for volatility prediction using multivariate data. (arXiv:2306.12446v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#22810;&#39033;&#36164;&#20135;&#27874;&#21160;&#29575;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#21450;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#26368;&#20248;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20351;&#29992;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#27874;&#21160;&#29575;&#30340;&#25928;&#26524;&#65292;&#20174;&#31616;&#21333;&#27973;&#23618;&#30340;&#27169;&#22411;&#21040;&#26356;&#28145;&#23618;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#22825;&#30495;&#39044;&#27979;&#21644;&#32463;&#20856;GARCH&#27169;&#22411;&#30340;&#21464;&#21270;&#30456;&#27604;&#36739;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;GARCH&#27169;&#22411;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#39044;&#27979;&#20102;&#20116;&#31181;&#36164;&#20135;&#65288;&#21363;S\&amp;P500&#12289;&#32435;&#26031;&#36798;&#20811;100&#12289;&#40644;&#37329;&#12289;&#30333;&#38134;&#21644;&#30707;&#27833;&#65289;&#30340;&#27874;&#21160;&#29575;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#32988;&#36807;&#20102;&#32463;&#20856;&#30340;&#26041;&#27861;&#21644;&#27973;&#23618;&#32593;&#32476;&#12290;&#36825;&#20123;&#23454;&#39564;&#34987;&#37325;&#22797;&#36827;&#34892;&#65292;&#24182;&#19988;&#31454;&#20105;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#65292;&#22240;&#27492;&#40723;&#21169;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims at comparing several deep learning-based forecasters in the task of volatility prediction using multivariate data, proceeding from simpler or shallower to deeper and more complex models and compare them to the naive prediction and variations of classical GARCH models. Specifically, the volatility of five assets (i.e., S\&amp;P500, NASDAQ100, gold, silver, and oil) was predicted with the GARCH models, Multi-Layer Perceptrons, recurrent neural networks, Temporal Convolutional Networks, and the Temporal Fusion Transformer. In most cases the Temporal Fusion Transformer followed by variants of Temporal Convolutional Network outperformed classical approaches and shallow networks. These experiments were repeated, and the difference between competing models was shown to be statistically significant, therefore encouraging their use in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#35828;&#35805;&#20154;&#39564;&#35777;&#25216;&#26415;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#65292;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#12289;&#38899;&#39057;&#36136;&#37327;&#26631;&#20934;&#21644;AD&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;ASV&#24615;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.12444</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#35828;&#35805;&#20154;&#39564;&#35777;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20020;&#24202;&#35797;&#39564;&#20013;&#30340;&#34920;&#29616;&#21463;&#21738;&#20123;&#22240;&#32032;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials. (arXiv:2306.12444v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#35828;&#35805;&#20154;&#39564;&#35777;&#25216;&#26415;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#65292;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#12289;&#38899;&#39057;&#36136;&#37327;&#26631;&#20934;&#21644;AD&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;ASV&#24615;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#26816;&#27979;&#37325;&#22797;&#30340;&#24739;&#32773;&#21442;&#19982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#37325;&#22797;&#30340;&#24739;&#32773;&#21487;&#33021;&#20250;&#30772;&#22351;&#35797;&#39564;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#23548;&#33268;&#37325;&#22823;&#30340;&#20581;&#24247;&#21644;&#36130;&#21153;&#39118;&#38505;&#12290;&#24320;&#21457;&#20934;&#30830;&#30340;&#33258;&#21160;&#21270;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;ASV&#65289;&#27169;&#22411;&#23545;&#20110;&#39564;&#35777;&#24050;&#27880;&#20876;&#20010;&#20307;&#30340;&#36523;&#20221;&#24182;&#21435;&#38500;&#37325;&#22797;&#39033;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#25968;&#25454;&#30340;&#22823;&#23567;&#21644;&#36136;&#37327;&#20250;&#24433;&#21709;ASV&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#23545;&#20110;&#24433;&#21709;&#20020;&#24202;&#29615;&#22659;&#20013;ASV&#33021;&#21147;&#30340;&#22240;&#32032;&#30340;&#35843;&#26597;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20174;&#22810;&#20010;&#35828;&#35805;&#20219;&#21153;&#20013;&#33719;&#21462;&#30340;659&#20010;&#20855;&#26377;&#19981;&#21516;AD&#24863;&#26579;&#31243;&#24230;&#30340;&#21442;&#19982;&#32773;&#30340;&#35821;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25506;&#35752;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#12289;&#38899;&#39057;&#36136;&#37327;&#26631;&#20934;&#21644;AD&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;ASV&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ASV&#30340;&#24615;&#33021;&#65306;1&#65289;&#22312;&#30007;&#24615;&#35828;&#35805;&#32773;&#19978;&#30053;&#20248;&#20110;&#22899;&#24615;&#35828;&#35805;&#32773;&#65307;2&#65289;
&lt;/p&gt;
&lt;p&gt;
Detecting duplicate patient participation in clinical trials is a major challenge because repeated patients can undermine the credibility and accuracy of the trial's findings and result in significant health and financial risks. Developing accurate automated speaker verification (ASV) models is crucial to verify the identity of enrolled individuals and remove duplicates, but the size and quality of data influence ASV performance. However, there has been limited investigation into the factors that can affect ASV capabilities in clinical environments. In this paper, we bridge the gap by conducting analysis of how participant demographic characteristics, audio quality criteria, and severity level of Alzheimer's disease (AD) impact the performance of ASV utilizing a dataset of speech recordings from 659 participants with varying levels of AD, obtained through multiple speech tasks. Our results indicate that ASV performance: 1) is slightly better on male speakers than on female speakers; 2)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#39062;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;DEPAC&#65292;&#35813;&#25968;&#25454;&#38598;&#26631;&#35760;&#20102;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#26631;&#20934;&#31579;&#26597;&#24037;&#20855;&#19978;&#30340;&#38376;&#27099;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#25163;&#24037;&#31579;&#36873;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20154;&#31867;&#35821;&#38899;&#20013;&#30340;&#31934;&#31070;&#30142;&#30149;&#36857;&#35937;&#12290;&#35813;&#30740;&#31350;&#20026;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#20449;&#24687;&#20016;&#23500;&#19988;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2306.12443</link><description>&lt;p&gt;
DEPAC&#65306;&#19968;&#20221;&#38024;&#23545;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#26816;&#27979;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
DEPAC: a Corpus for Depression and Anxiety Detection from Speech. (arXiv:2306.12443v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#39062;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;DEPAC&#65292;&#35813;&#25968;&#25454;&#38598;&#26631;&#35760;&#20102;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#26631;&#20934;&#31579;&#26597;&#24037;&#20855;&#19978;&#30340;&#38376;&#27099;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#25163;&#24037;&#31579;&#36873;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20154;&#31867;&#35821;&#38899;&#20013;&#30340;&#31934;&#31070;&#30142;&#30149;&#36857;&#35937;&#12290;&#35813;&#30740;&#31350;&#20026;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#20449;&#24687;&#20016;&#23500;&#19988;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#22256;&#25200;&#65292;&#27604;&#22914;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#65292;&#23545;&#20840;&#29699;&#30142;&#30149;&#36127;&#25285;&#30340;&#36129;&#29486;&#26368;&#22823;&#12290;&#21463;&#21040;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#26032;&#25216;&#26415;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#38556;&#30861;&#30340;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#21487;&#20197;&#20026;&#21463;&#24433;&#21709;&#30340;&#20154;&#20204;&#20943;&#23569;&#30171;&#33510;&#12290;&#36825;&#31181;&#31995;&#32479;&#30340;&#24320;&#21457;&#38656;&#35201;&#20449;&#24687;&#20016;&#23500;&#19988;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#39062;&#30340;&#24515;&#29702;&#22256;&#25200;&#20998;&#26512;&#38899;&#39057;&#25968;&#25454;&#38598;DEPAC&#65292;&#22522;&#20110;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#26631;&#20934;&#31579;&#26597;&#24037;&#20855;&#19978;&#30340;&#24050;&#24314;&#31435;&#38376;&#27099;&#36827;&#34892;&#26631;&#35760;&#12290;&#36825;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#21253;&#25324;&#27599;&#20010;&#20010;&#20307;&#30340;&#22810;&#20010;&#35821;&#38899;&#20219;&#21153;&#20197;&#21450;&#30456;&#20851;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#38598;&#65292;&#21253;&#25324;&#25163;&#24037;&#31579;&#36873;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#22312;&#20154;&#31867;&#35821;&#38899;&#20013;&#35782;&#21035;&#31934;&#31070;&#30142;&#30149;&#36857;&#35937;&#26041;&#38754;&#21457;&#25381;&#20102;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#22522;&#32447;&#30340;&#24615;&#33021;&#26469;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#38899;&#39057;&#35821;&#26009;&#24211;&#21644;&#29305;&#24449;&#38598;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis systems of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labeled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#35789;&#32423;&#21035;&#20851;&#31995;&#22270;(TRG)&#65292;&#29992;&#20110;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;TRG&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#25945;&#24072;&#27169;&#22411;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#25439;&#22833;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.12442</link><description>&lt;p&gt;
&#22522;&#20110;&#35789;&#32423;&#21035;&#20851;&#31995;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation via Token-level Relationship Graph. (arXiv:2306.12442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#35789;&#32423;&#21035;&#20851;&#31995;&#22270;(TRG)&#65292;&#29992;&#20110;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;TRG&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#25945;&#24072;&#27169;&#22411;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#25439;&#22833;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#20256;&#36882;&#30340;&#30495;&#27491;&#28508;&#21147;&#36824;&#27809;&#26377;&#34987;&#20805;&#20998;&#25366;&#25496;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#33976;&#39311;&#21333;&#20010;&#20449;&#24687;&#25110;&#23454;&#20363;&#32423;&#21035;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#35789;&#32423;&#21035;&#20851;&#31995;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#21463;&#21040;&#38271;&#23614;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#35789;&#32423;&#21035;&#20851;&#31995;&#22270;(TRG)&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35789;&#32423;&#21035;&#30340;&#20851;&#31995;&#30693;&#35782;&#26469;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;TRG&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#25945;&#24072;&#27169;&#22411;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33976;&#39311;&#32467;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#19979;&#25991;&#25439;&#22833;&#30340;&#35789;&#32423;&#21035;&#19978;&#19979;&#25991;&#25439;&#22833;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a powerful technique for transferring knowledge from a pre-trained teacher model to a student model. However, the true potential of knowledge transfer has not been fully explored. Existing approaches primarily focus on distilling individual information or instance-level relationships, overlooking the valuable information embedded in token-level relationships, which may be particularly affected by the long-tail effects. To address the above limitations, we propose a novel method called Knowledge Distillation with Token-level Relationship Graph (TRG) that leverages the token-wise relational knowledge to enhance the performance of knowledge distillation. By employing TRG, the student model can effectively emulate higher-level semantic information from the teacher model, resulting in improved distillation results. To further enhance the learning process, we introduce a token-wise contextual loss called contextual loss, which encourages the student model to capture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;&#20102;&#29983;&#25104;&#20020;&#24202;&#21512;&#29702;&#30340;&#21307;&#23398;&#22270;&#20687;&#32780;&#24341;&#20837;&#30149;&#29702;&#23398;&#23478;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12438</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#23558;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#19982;&#20020;&#24202;&#30693;&#35782;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback. (arXiv:2306.12438v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;&#20102;&#29983;&#25104;&#20020;&#24202;&#21512;&#29702;&#30340;&#21307;&#23398;&#22270;&#20687;&#32780;&#24341;&#20837;&#30149;&#29702;&#23398;&#23478;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#25429;&#33719;&#21307;&#23398;&#22270;&#20687;&#20013;&#24494;&#22937;&#20020;&#24202;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#20419;&#36827;&#20020;&#24202;&#25968;&#25454;&#20849;&#20139;&#12289;&#22686;&#24378;&#32597;&#35265;&#30149;&#25968;&#25454;&#38598;&#12289;&#24182;&#22312;&#35268;&#27169;&#19978;&#39640;&#25928;&#22320;&#21512;&#25104;&#26631;&#27880;&#21307;&#23398;&#22270;&#20687;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#30340;&#36136;&#37327;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#34429;&#28982;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#21512;&#25104;&#35270;&#35273;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#20294;&#36825;&#20123;&#22270;&#20687;&#30340;&#20020;&#24202;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#36136;&#30097;&#12290;&#39046;&#22495;&#19981;&#21487;&#30693;&#20998;&#25968;&#65292;&#22914;FID&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#26080;&#27861;&#34701;&#20837;&#20020;&#24202;&#30693;&#35782;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#35780;&#20272;&#20020;&#24202;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#27169;&#22411;&#21487;&#33021;&#20197;&#35768;&#22810;&#19981;&#21487;&#39044;&#27979;&#30340;&#26041;&#24335;&#26080;&#27861;&#21512;&#25104;&#20020;&#24202;&#21512;&#29702;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#38590;&#20197;&#39044;&#35265;&#28508;&#22312;&#30340;&#22833;&#36133;&#24182;&#25163;&#21160;&#35774;&#35745;&#35780;&#20998;&#20197;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30149;&#29702;&#23398;&#23478;&#21442;&#19982;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20020;&#24202;&#21512;&#29702;&#30340;&#21307;&#23398;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models capable of capturing nuanced clinical features in medical images hold great promise for facilitating clinical data sharing, enhancing rare disease datasets, and efficiently synthesizing annotated medical images at scale. Despite their potential, assessing the quality of synthetic medical images remains a challenge. While modern generative models can synthesize visually-realistic medical images, the clinical validity of these images may be called into question. Domain-agnostic scores, such as FID score, precision, and recall, cannot incorporate clinical knowledge and are, therefore, not suitable for assessing clinical sensibility. Additionally, there are numerous unpredictable ways in which generative models may fail to synthesize clinically plausible images, making it challenging to anticipate potential failures and manually design scores for their detection. To address these challenges, this paper introduces a pathologist-in-the-loop framework for generating clinical
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPSTAN&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#26001;&#28857;&#27969;&#34892;&#30149;&#23398;&#30693;&#35782;&#34701;&#20837;&#26102;&#31354;&#27169;&#22411;&#24182;&#33258;&#36866;&#24212;&#22320;&#23450;&#20041;&#26001;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#39640;&#27969;&#34892;&#30149;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#20803;&#32676;&#20307;&#29702;&#35770;&#23558;&#26001;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#32435;&#20837;&#27169;&#22411;&#65292;&#20026;&#26500;&#24314;&#22810;&#26001;&#28857;&#30693;&#35782;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.12436</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#32676;&#20307;&#21644;&#26102;&#31354;&#20851;&#27880;&#32593;&#32476;&#30340;&#27969;&#34892;&#30149;&#39044;&#27979;&#27169;&#22411;(MPSTAN)
&lt;/p&gt;
&lt;p&gt;
MPSTAN: Metapopulation-based Spatio-Temporal Attention Network for Epidemic Forecasting. (arXiv:2306.12436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12436
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPSTAN&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#26001;&#28857;&#27969;&#34892;&#30149;&#23398;&#30693;&#35782;&#34701;&#20837;&#26102;&#31354;&#27169;&#22411;&#24182;&#33258;&#36866;&#24212;&#22320;&#23450;&#20041;&#26001;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#39640;&#27969;&#34892;&#30149;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#20803;&#32676;&#20307;&#29702;&#35770;&#23558;&#26001;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#32435;&#20837;&#27169;&#22411;&#65292;&#20026;&#26500;&#24314;&#22810;&#26001;&#28857;&#30693;&#35782;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27969;&#34892;&#30149;&#39044;&#27979;&#23545;&#25919;&#24220;&#21046;&#23450;&#26377;&#25928;&#30340;&#38450;&#25511;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#26102;&#31354;&#27169;&#22411;&#19981;&#33021;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#39044;&#27979;&#22810;&#26679;&#21270;&#27969;&#34892;&#36235;&#21183;&#30340;&#24120;&#35268;&#26694;&#26550;&#12290;&#23558;&#20256;&#26579;&#30149;&#39046;&#22495;&#30693;&#35782;&#20174;&#21333;&#20010;&#26001;&#28857;&#21040;&#22810;&#20010;&#26001;&#28857;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#26395;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#21333;&#20010;&#26001;&#28857;&#30693;&#35782;&#24573;&#35270;&#20102;&#26001;&#28857;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;&#26500;&#24314;&#22810;&#26001;&#28857;&#30693;&#35782;&#22312;&#32570;&#20047;&#20154;&#21475;&#27969;&#21160;&#24615;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20803;&#32676;&#20307;&#21644;&#26102;&#31354;&#20851;&#27880;&#32593;&#32476;(MPSTAN)&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#23558;&#22810;&#26001;&#28857;&#27969;&#34892;&#30149;&#23398;&#30693;&#35782;&#34701;&#20837;&#26102;&#31354;&#27169;&#22411;&#24182;&#33258;&#36866;&#24212;&#22320;&#23450;&#20041;&#26001;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#39640;&#27969;&#34892;&#30149;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20803;&#32676;&#20307;&#29702;&#35770;&#23558;&#26001;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#32435;&#20837;&#27169;&#22411;&#65292;&#20026;&#26500;&#24314;&#22810;&#26001;&#28857;&#30693;&#35782;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate epidemic forecasting plays a vital role for governments in developing effective prevention measures for suppressing epidemics. Most of the present spatio-temporal models cannot provide a general framework for stable, and accurate forecasting of epidemics with diverse evolution trends. Incorporating epidemiological domain knowledge ranging from single-patch to multi-patch into neural networks is expected to improve forecasting accuracy. However, relying solely on single-patch knowledge neglects inter-patch interactions, while constructing multi-patch knowledge is challenging without population mobility data. To address the aforementioned problems, we propose a novel hybrid model called Metapopulation-based Spatio-Temporal Attention Network (MPSTAN). This model aims to improve the accuracy of epidemic forecasting by incorporating multi-patch epidemiological knowledge into a spatio-temporal model and adaptively defining inter-patch interactions. Moreover, we incorporate inter-pat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;T1&#38745;&#24687;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;(TRS-MRI)&#25195;&#25551;&#35782;&#21035;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#26377;&#25928;&#22320;&#21306;&#20998;&#20986;&#24739;&#26377;OCD&#21644;&#20854;&#20182;&#31934;&#31070;&#38556;&#30861;&#30340;&#24739;&#32773;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;</title><link>http://arxiv.org/abs/2306.12435</link><description>&lt;p&gt;
&#24212;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;T1&#38745;&#24687;&#24577;&#30913;&#20849;&#25391;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#65292;&#24110;&#21161;&#35786;&#26029;&#24378;&#36843;&#30151;
&lt;/p&gt;
&lt;p&gt;
Modeling T1 Resting-State MRI Variants Using Convolutional Neural Networks in Diagnosis of OCD. (arXiv:2306.12435v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;T1&#38745;&#24687;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;(TRS-MRI)&#25195;&#25551;&#35782;&#21035;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#26377;&#25928;&#22320;&#21306;&#20998;&#20986;&#24739;&#26377;OCD&#21644;&#20854;&#20182;&#31934;&#31070;&#38556;&#30861;&#30340;&#24739;&#32773;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#36843;&#30151;&#26159;&#19968;&#31181;&#39640;&#24230;&#20196;&#20154;&#30171;&#33510;&#30340;&#30142;&#30149;&#65292;&#20854;&#24120;&#24120;&#19982;&#21069;&#39069;&#30382;&#23618;&#21644;&#20195;&#35874;&#22411;&#35895;&#27688;&#37240;&#21463;&#20307;5(mGluR5)&#26377;&#20851;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#27979;&#37327;&#23567;&#40736;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#30340;&#20998;&#24067;&#23481;&#31215;&#27604;&#65292;&#21457;&#29616;&#35813;&#21463;&#20307;&#20449;&#21495;&#27700;&#24179;&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#26356;&#22810;&#23454;&#35777;&#25968;&#25454;&#65292;&#23578;&#26080;&#27861;&#23436;&#20840;&#39564;&#35777;mGluR5&#30340;&#21442;&#19982;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#24739;&#26377;&#31934;&#31070;&#20998;&#35010;&#30151;&#12289;&#25233;&#37057;&#30151;&#21644;&#24378;&#36843;&#30151;&#24739;&#32773;&#30340;T1&#38745;&#24687;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;(TRS-MRI)&#25195;&#25551;&#65292;&#20197;&#22238;&#31572;&#26377;&#20851;OCD&#30340;&#22240;&#26524;&#22240;&#32032;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36825;&#20123;&#30142;&#30149;&#20043;&#38388;&#30340;&#20132;&#21449;&#27604;&#36739;&#65292;&#23547;&#25214;&#29305;&#23450;&#30142;&#30149;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#21306;&#20998;&#24739;&#26377;OCD&#21644;&#20854;&#20182;&#31934;&#31070;&#38556;&#30861;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#35782;&#21035;&#24739;&#26377;OCD &#30340;&#24739;&#32773;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#35786;&#26029;OCD&#30340;&#24037;&#20855;&#65292;&#20026;&#20256;&#32479;&#35786;&#26029;&#27969;&#31243;&#25552;&#20379;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obsessive-compulsive disorder (OCD) presents itself as a highly debilitating disorder. The disorder has common associations with the prefrontal cortex and the glutamate receptor known as Metabotropic Glutamate Receptor 5 (mGluR5). This receptor has been observed to demonstrate higher levels of signaling from positron emission tomography scans measured by its distribution volume ratios in mice. Despite this evidence, studies are unable to fully verify the involvement of mGluR5 as more empirical data is needed. Computational modeling methods were used as a means of validation for previous hypotheses involving mGluR5. The inadequacies in relation to the causal factor of OCD were answered by utilizing T1 resting-state magnetic resonance imaging (TRS-MRI) scans of patients suffering from schizophrenia, major depressive disorder, and obsessive-compulsive disorder. Because comorbid cases often occur within these disorders, cross-comparative abilities become necessary to find distinctive chara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#20813;&#30123;&#33639;&#20809;&#24187;&#28783;&#29255;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#24037;&#20855;&#24110;&#21161;&#21307;&#29983;&#26089;&#26399;&#26816;&#27979;&#24322;&#24120;&#65292;&#35299;&#20915;&#20813;&#30123;&#30142;&#30149;&#26041;&#38754;&#30340;&#21307;&#30103;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12432</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35299;&#35835;&#20813;&#30123;&#33639;&#20809;&#24187;&#28783;&#29255;&#65306;&#25239;&#26680;&#25239;&#20307;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interpretation of immunofluorescence slides by deep learning techniques: anti-nuclear antibodies case study. (arXiv:2306.12432v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#20813;&#30123;&#33639;&#20809;&#24187;&#28783;&#29255;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#24037;&#20855;&#24110;&#21161;&#21307;&#29983;&#26089;&#26399;&#26816;&#27979;&#24322;&#24120;&#65292;&#35299;&#20915;&#20813;&#30123;&#30142;&#30149;&#26041;&#38754;&#30340;&#21307;&#30103;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#30142;&#30149;&#25968;&#37327;&#21644;&#20005;&#37325;&#31243;&#24230;&#37117;&#22312;&#19981;&#26029;&#22686;&#21152;&#65292;&#32780;&#20813;&#30123;&#30142;&#30149;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#20540;&#24471;&#20851;&#27880;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;2017&#24180;&#30340;&#25968;&#25454;&#26174;&#31034;&#65292;&#20813;&#30123;&#30142;&#30149;&#24433;&#21709;&#30528;&#20840;&#29699;8&#65285;&#30340;&#20154;&#21475;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26368;&#26032;&#30340;&#20813;&#30123;&#30142;&#30149;&#21307;&#30103;&#35299;&#20915;&#26041;&#26696;&#30340;&#32508;&#36848;&#65292;&#20391;&#37325;&#20110;&#21033;&#29992;&#29616;&#20195;&#35299;&#20915;&#26041;&#26696;&#22914;&#28145;&#24230;&#23398;&#20064;&#26469;&#22312;&#26089;&#26399;&#26816;&#27979;&#24322;&#24120;&#65292;&#20026;&#20581;&#24247;&#21307;&#29983;&#25552;&#20379;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#31561;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#22312;&#31361;&#23612;&#26031;&#24635;&#20891;&#20107;&#21307;&#38498;&#30340;&#20813;&#30123;&#23398;&#37096;&#38376;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#20182;&#20204;&#35748;&#20026;&#23427;&#26159;&#19968;&#20010;&#38750;&#24120;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, diseases are increasing in numbers and severity by the hour. Immunity diseases, affecting 8\% of the world population in 2017 according to the World Health Organization (WHO), is a field in medicine worth attention due to the high rate of disease occurrence classified under this category. This work presents an up-to-date review of state-of-the-art immune diseases healthcare solutions. We focus on tackling the issue with modern solutions such as Deep Learning to detect anomalies in the early stages hence providing health practitioners with efficient tools. We rely on advanced deep learning techniques such as Convolutional Neural Networks (CNN) to fulfill our objective of providing an efficient tool while providing a proficient analysis of this solution. The proposed solution was tested and evaluated by the immunology department in the Principal Military Hospital of Instruction of Tunis, which considered it a very helpful tool.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20108;&#27425;&#22411;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;&#20449;&#24687;&#35770;&#35282;&#24230;&#25552;&#20379;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#21487;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.12383</link><description>&lt;p&gt;
&#20108;&#27425;&#22411;&#36172;&#33218;&#26426;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65306;Hessian&#30456;&#20851;&#24615;&#30028;&#38480;&#21644;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20108;&#27425;&#22411;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;&#20449;&#24687;&#35770;&#35282;&#24230;&#25552;&#20379;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#21487;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#20013;&#65292;&#20102;&#35299;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#26159;&#19968;&#20010;&#23454;&#38469;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#22522;&#26412;&#24773;&#20917;&#65292;&#21363;&#30446;&#26631;&#20989;&#25968;&#26159;&#20108;&#27425;&#22411;&#30340;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31532;&#19968;&#20010;&#32039;&#23494;&#21051;&#30011;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20855;&#26377;&#21452;&#37325;&#24615;&#36136;&#12290;&#39318;&#20808;&#65292;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;&#20998;&#37197;&#30340;&#27010;&#24565;&#26469;&#25429;&#25417;&#25628;&#32034;&#31639;&#27861;&#21644;&#30446;&#26631;&#20989;&#25968;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#35777;&#26126;&#20102;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#32039;&#23494;&#19979;&#30028;&#12290;&#36890;&#36807;&#35299;&#20915;&#26368;&#20248;&#33021;&#37327;&#35889;&#65292;&#24471;&#21040;&#20102;&#37197;&#22871;&#30340;&#19978;&#38480;&#12290;&#20854;&#27425;&#65292;&#31639;&#27861;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#23545;&#20110;&#37325;&#23614;&#22122;&#22768;&#20998;&#24067;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23545;&#29616;&#20195;BDL&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12306</link><description>&lt;p&gt;
&#36229;&#36234;&#28145;&#24230;&#38598;&#25104;&#8212;&#8212;&#22522;&#20110;&#20998;&#24067;&#20559;&#31227;&#19979;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift. (arXiv:2306.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23545;&#29616;&#20195;BDL&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#23454;&#29616;&#22312;&#20998;&#24067;&#20559;&#31227;&#25968;&#25454;&#19978;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#39044;&#27979;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22823;&#35268;&#27169;&#35843;&#26597;&#65292;&#20197;&#31995;&#32479;&#26041;&#24335;&#35780;&#20272;&#26368;&#36817;&#30340; SOTA &#26041;&#27861;&#22312;&#22810;&#26679;&#12289;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#28165;&#26224;&#20102;&#35299;BDL&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#20917;&#65292;&#25105;&#20204;&#22312;&#26469;&#33258;WILDS&#38598;&#21512;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#29616;&#20195;BDL&#31639;&#27861;&#65292;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26657;&#20934;&#33021;&#21147;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#22823;&#22411;&#65292;&#21367;&#31215;&#21644;&#22522;&#20110; transformer &#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#19978;&#30340;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#31526;&#21495;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#29256;&#26412;&#65292;&#25581;&#31034;&#20986;&#26041;&#27861;&#26159;&#36807;&#24230;&#33258;&#20449;&#36824;&#26159;&#20302;&#25391;&#24133;&#65292;&#36827;&#19968;&#27493;&#28145;&#20837;&#30740;&#31350;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20102;&#39318;&#27425;&#31995;&#32479;&#35780;&#20272;BDL&#22312;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#34920;&#29616;&#65292;&#20570;&#20102;&#26356;&#22810;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Corrector&#25805;&#20316;&#31526;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#26696;&#23545;&#20110;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#36824;&#22312;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/2306.12047</link><description>&lt;p&gt;
&#20351;&#29992;Corrector&#25805;&#20316;&#31526;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Corrector Operator to Enhance Accuracy and Reliability of Neural Operator Surrogates of Nonlinear Variational Boundary-Value Problems. (arXiv:2306.12047v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Corrector&#25805;&#20316;&#31526;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#38750;&#32447;&#24615;&#21464;&#20998;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#20934;&#30830;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#26696;&#23545;&#20110;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#36824;&#22312;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31867;&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#31639;&#31526;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#24335;&#12290;&#31070;&#32463;&#31639;&#23376;&#26377;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#12289;&#25104;&#26412;-&#20934;&#30830;&#24230;&#26435;&#34913;&#21644;&#38750;&#24179;&#20961;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#12290;&#31070;&#32463;&#31639;&#23376;&#20934;&#30830;&#24230;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#25512;&#29702;&#12289;&#20248;&#21270;&#21644;&#25511;&#21046;&#31561;&#21518;&#32493;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#21464;&#20998;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#32473;&#20986;&#20102;&#31070;&#32463;&#31639;&#23376;&#39044;&#27979;&#32467;&#26524;&#30340;&#26657;&#27491;&#20540;&#12290;&#19982;&#26657;&#27491;&#38382;&#39064;&#30456;&#20851;&#30340;&#31639;&#23376;&#31216;&#20026;&#26657;&#27491;&#31639;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#26041;&#26696;&#23545;&#37319;&#29992;PCANet&#22411;&#31070;&#32463;&#31639;&#23376;&#30340;&#20108;&#32500;&#38750;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20351;&#29992;&#26657;&#27491;&#26041;&#27861;&#23545;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#26657;&#27491;&#26102;&#65292;&#36924;&#36817;&#30340;&#20934;&#30830;&#24230;&#36817;&#20046;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#38750;&#32447;&#24615;d&#20043;&#19978;&#30340;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#20063;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on developing methods for approximating the solution operators of a class of parametric partial differential equations via neural operators. Neural operators have several challenges, including the issue of generating appropriate training data, cost-accuracy trade-offs, and nontrivial hyperparameter tuning. The unpredictability of the accuracy of neural operators impacts their applications in downstream problems of inference, optimization, and control. A framework is proposed based on the linear variational problem that gives the correction to the prediction furnished by neural operators. The operator associated with the corrector problem is referred to as the corrector operator. Numerical results involving a nonlinear diffusion model in two dimensions with PCANet-type neural operators show almost two orders of increase in the accuracy of approximations when neural operators are corrected using the proposed scheme. Further, topology optimization involving a nonlinear d
&lt;/p&gt;</description></item><item><title>AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11971</link><description>&lt;p&gt;
AdCraft&#65306;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20248;&#21270;&#30340;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11971
&lt;/p&gt;
&lt;p&gt;
AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#8212;&#8212; AdCraft&#65292;&#20854;&#20855;&#26377;&#38543;&#26426;&#21644;&#38750;&#38745;&#24577;&#29305;&#24615;&#12290;&#35813;&#29615;&#22659;&#27169;&#25311;&#20102;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20013;&#20986;&#20215;&#21644;&#39044;&#31639;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;SEM&#26159;&#19968;&#31181;&#21033;&#29992;&#20184;&#36153;&#24191;&#21578;&#26469;&#22686;&#21152;&#32593;&#31449;&#22312;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#19978;&#30340;&#21487;&#35265;&#24615;&#30340;&#25968;&#23383;&#33829;&#38144;&#25216;&#26415;&#12290;SEM&#24191;&#21578;&#27963;&#21160;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#21253;&#25324;&#20851;&#38190;&#23383;&#36873;&#25321;&#12289;&#24191;&#21578;&#35774;&#35745;&#12289;&#20986;&#20215;&#31649;&#29702;&#12289;&#39044;&#31639;&#35843;&#25972;&#21644;&#34920;&#29616;&#30417;&#25511;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20248;&#21270;SEM&#24191;&#21578;&#25237;&#25918;&#27963;&#21160;&#30340;&#28508;&#22312;&#31574;&#30053;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#25110;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#21487;&#23450;&#21046;&#29615;&#22659;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#35780;&#20272;&#21644;&#25552;&#39640;&#19982;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20184;&#20986;&#36825;&#20123;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;AdCraft&#29615;&#22659;&#19979;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.11768</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#21033;&#29992;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#20960;&#20309;&#32467;&#26500;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#29289;&#29702;&#21270;&#23398;&#24314;&#27169;&#21644;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#20256;&#32479;&#26041;&#27861;&#36153;&#26102;&#36153;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#25972;&#21512;&#19977;&#32500;&#20960;&#20309;&#25968;&#25454;&#65292;&#21152;&#19978;&#31867;&#20284;AlphaFold&#30340;&#24037;&#20855;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#19977;&#32500;&#32467;&#26500;&#39044;&#27979;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#20174;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#20027;&#27969;&#20219;&#21153;&#12289;&#24120;&#29992;&#30340;3D&#34507;&#30333;&#36136;&#34920;&#31034;&#21644;&#39044;&#27979;/&#29983;&#25104;&#27169;&#22411;&#20837;&#25163;&#65292;&#28982;&#21518;&#35814;&#32454;&#20171;&#32461;&#27599;&#20010;&#20219;&#21153;&#30340;&#22238;&#39038;&#65288;&#20363;&#22914;&#32467;&#21512;&#20301;&#28857;&#39044;&#27979;&#12289;&#32467;&#21512;&#26500;&#35937;&#29983;&#25104;&#12289;\emph{de novo} &#20998;&#23376;&#35774;&#35745;&#31561;&#65289;&#65292;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
&lt;/p&gt;</description></item><item><title>FDINet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#29305;&#24449;&#22833;&#30495;&#25351;&#25968;&#26469;&#20445;&#25252;DNN&#27169;&#22411;&#20813;&#21463;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#24182;&#21033;&#29992;FDI&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#20998;&#24067;&#24335;&#25552;&#21462;&#25915;&#20987;&#20013;&#30340;&#21246;&#32467;&#25932;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.11338</link><description>&lt;p&gt;
FDINet&#65306;&#21033;&#29992;&#29305;&#24449;&#22833;&#30495;&#25351;&#25968;&#20445;&#25252; DNN &#27169;&#22411;&#20813;&#21463;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FDINet: Protecting against DNN Model Extraction via Feature Distortion Index. (arXiv:2306.11338v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11338
&lt;/p&gt;
&lt;p&gt;
FDINet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#29305;&#24449;&#22833;&#30495;&#25351;&#25968;&#26469;&#20445;&#25252;DNN&#27169;&#22411;&#20813;&#21463;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#24182;&#21033;&#29992;FDI&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#20998;&#24067;&#24335;&#25552;&#21462;&#25915;&#20987;&#20013;&#30340;&#21246;&#32467;&#25932;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#24179;&#21488;&#30001;&#20110;&#20854;&#26131;&#29992;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#24555;&#36895;&#24320;&#21457;&#33021;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102; MLaaS &#20013;&#22522;&#20110;&#20113;&#30340;&#27169;&#22411;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; FDINET&#65292;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#29305;&#24449;&#20998;&#24067;&#30340;&#26032;&#39062;&#38450;&#24481;&#26426;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#25163;&#30340;&#26597;&#35810;&#30340;&#29305;&#24449;&#20998;&#24067;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#26597;&#35810;&#30340;&#29305;&#24449;&#20998;&#24067;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#19981;&#21516;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#22833;&#30495;&#25351;&#25968;&#65288;FDI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24230;&#37327;&#35774;&#35745;&#65292;&#29992;&#20110;&#23450;&#37327;&#27979;&#37327;&#25509;&#25910;&#21040;&#30340;&#26597;&#35810;&#30340;&#29305;&#24449;&#20998;&#24067;&#20559;&#24046;&#12290;&#25152;&#25552;&#20986;&#30340; FDINET &#21033;&#29992; FDI &#35757;&#32451;&#19968;&#20010;&#20108;&#36827;&#21046;&#26816;&#27979;&#22120;&#65292;&#24182;&#21033;&#29992; FDI &#30456;&#20284;&#24615;&#35782;&#21035;&#20998;&#24067;&#24335;&#25552;&#21462;&#25915;&#20987;&#20013;&#30340;&#21246;&#32467;&#25932;&#20154;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272; FDINET &#23545;&#25239;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINET, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary's queries, we reveal that the feature distribution of these queries deviates from that of the model's training set. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINET utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINET against
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.11207</link><description>&lt;p&gt;
Quilt-1M: &#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#21152;&#36895;&#20351;&#24471;&#22312;&#32447;&#22270;&#20687;&#21644;&#25991;&#23383;&#25968;&#25454;&#22823;&#37327;&#28044;&#29616;&#65292;&#20294;&#21307;&#23398;&#39046;&#22495;&#65288;&#29305;&#21035;&#26159;&#30284;&#30151;&#32452;&#32455;&#23398;&#65289;&#31867;&#20284;&#30340;&#25968;&#25454;&#21364;&#24456;&#31232;&#23569;&#65292;&#36825;&#38459;&#30861;&#20102;&#21307;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#21033;&#29992;YouTube&#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#65292;&#20174;&#20013;&#36873;&#25321;&#20102; 1,087 &#23567;&#26102;&#30340;&#21307;&#23398;&#32452;&#32455;&#23398;&#35270;&#39057;&#65292;&#20197;&#27492;&#33258;&#21160;&#31579;&#36873;&#20986;&#20849;&#21253;&#21547; 768,826 &#20010;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#23383;&#23545;&#30340; Quilt &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20248;&#21270;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11086</link><description>&lt;p&gt;
&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#21464;&#20998;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing variational quantum state diagonalization using reinforcement learning techniques. (arXiv:2306.11086v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20248;&#21270;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21457;&#23637;&#23545;&#20110; NISQ &#35745;&#31639;&#26426;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#31639;&#27861;&#38656;&#35201;&#30701;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#36825;&#31181;&#30005;&#36335;&#26356;&#26131;&#20110;&#22312;&#36817;&#26399;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#20063;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#21035;&#26377;&#36259;&#30340;&#31639;&#27861;&#26159;&#25152;&#35859;&#30340;&#21464;&#20998;&#23545;&#35282;&#21270;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#31639;&#27861;&#23376;&#20363;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22788;&#29702;&#20197;&#37327;&#23376;&#29366;&#24577;&#32534;&#30721;&#30340;&#25968;&#25454;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#36776;&#37327;&#23376;&#24577;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#31995;&#32479;&#30340;&#32416;&#32544;&#24615;&#36136;&#65292;&#25110;&#32773;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22312;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#20219;&#21153;&#20013;&#25152;&#38656;&#30005;&#36335;&#38750;&#24120;&#27973;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#30005;&#36335;&#28145;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of variational quantum algorithms is crucial for the application of NISQ computers. Such algorithms require short quantum circuits, which are more amenable to implementation on near-term hardware, and many such methods have been developed. One of particular interest is the so-called the variational diagonalization method, which constitutes an important algorithmic subroutine, and it can be used directly for working with data encoded in quantum states. In particular, it can be applied to discern the features of quantum states, such as entanglement properties of a system, or in quantum machine learning algorithms. In this work, we tackle the problem of designing a very shallow quantum circuit, required in the quantum state diagonalization task, by utilizing reinforcement learning. To achieve this, we utilize a novel encoding method that can be used to tackle the problem of circuit depth optimization using a reinforcement learning approach. We demonstrate that our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.10359</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#39537;&#21160;Foley&#38899;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foley&#38899;&#25928;&#29983;&#25104;&#26088;&#22312;&#20026;&#22810;&#23186;&#20307;&#20869;&#23481;&#29983;&#25104;&#32972;&#26223;&#38899;&#25928;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#24320;&#21457;&#38598;&#20316;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#25968;&#23383;&#25110;one-hot&#21521;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#37197;&#23545;&#65288;CLAP&#65289;&#25216;&#26415;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26469;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#20043;&#21518;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#38899;&#39057;&#29255;&#27573;&#24182;&#36873;&#25321;&#26368;&#20339;&#29255;&#27573;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#29983;&#25104;&#30340;&#27874;&#24418;&#65292;&#26368;&#20339;&#29255;&#27573;&#26159;&#26681;&#25454;&#23884;&#20837;&#20043;&#38388;&#30456;&#20284;&#24615;&#24471;&#20998;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#8221;&#30340;&#26032;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#21644;&#20219;&#21153;&#36866;&#24212;&#27169;&#22359;&#20197;&#21450;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#23569;&#26679;&#26412;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10234</link><description>&lt;p&gt;
&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Few-shot Learning. (arXiv:2306.10234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#8221;&#30340;&#26032;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#21644;&#20219;&#21153;&#36866;&#24212;&#27169;&#22359;&#20197;&#21450;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#23569;&#26679;&#26412;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#22312;&#19981;&#20132;&#25442;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#65292;&#32780;&#20107;&#23454;&#19978;&#26576;&#20123;&#23458;&#25143;&#31471;&#21487;&#33021;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#21363;&#23569;&#26679;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#22312;&#36825;&#20123;&#23458;&#25143;&#31471;&#19978;&#36935;&#21040;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#21644;&#20219;&#21153;&#36866;&#24212;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#23569;&#26679;&#26412;&#23458;&#25143;&#31471;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#21160;&#24577;&#22320;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables multiple clients to collaboratively learn a machine learning model without exchanging their own local data. In this way, the server can exploit the computational power of all clients and train the model on a larger set of data samples among all clients. Although such a mechanism is proven to be effective in various fields, existing works generally assume that each client preserves sufficient data for training. In practice, however, certain clients may only contain a limited number of samples (i.e., few-shot samples). For example, the available photo data taken by a specific user with a new mobile device is relatively rare. In this scenario, existing FL efforts typically encounter a significant performance drop on these clients. Therefore, it is urgent to develop a few-shot model that can generalize to clients with limited data under the FL scenario. In this paper, we refer to this novel problem as federated few-shot learning. Nevertheless, the problem re
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RANS-PINN&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06034</link><description>&lt;p&gt;
&#22522;&#20110;RANS-PINN&#30340;&#27169;&#25311;&#20195;&#29702;&#39044;&#27979;&#28237;&#27969;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows. (arXiv:2306.06034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RANS-PINN&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20026;&#24314;&#31435;&#30001;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#20102;&#26694;&#26550;&#12290; PINN&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20250;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#27491;&#21017;&#21270;&#39033;&#26469;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#30001;&#20110;&#27169;&#25311;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#21160;&#24577;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#65292;PINN&#24050;&#32463;&#22312;&#23398;&#20064;&#30001;Navier-Stokes&#26041;&#31243;&#25511;&#21046;&#30340;&#28082;&#20307;&#27969;&#21160;&#38382;&#39064;&#30340;&#21442;&#25968;&#20195;&#29702;&#26041;&#38754;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RANS-PINN&#65292;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;PINN&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65288;&#21363;&#36895;&#24230;&#21644;&#21387;&#21147;&#65289;&#12290;&#20026;&#20102;&#32771;&#34385;&#28237;&#27969;&#24341;&#20837;&#30340;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;RANS-PINN&#37319;&#29992;&#22522;&#20110;&#38647;&#35834;&#24179;&#22343;Navier-Stokes&#65288;RANS&#65289;&#30340;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#30830;&#20445;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#26377;&#25928;&#21021;&#22987;&#21270;&#21644;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) provide a framework to build surrogate models for dynamical systems governed by differential equations. During the learning process, PINNs incorporate a physics-based regularization term within the loss function to enhance generalization performance. Since simulating dynamics controlled by partial differential equations (PDEs) can be computationally expensive, PINNs have gained popularity in learning parametric surrogates for fluid flow problems governed by Navier-Stokes equations. In this work, we introduce RANS-PINN, a modified PINN framework, to predict flow fields (i.e., velocity and pressure) in high Reynolds number turbulent flow regime. To account for the additional complexity introduced by turbulence, RANS-PINN employs a 2-equation eddy viscosity model based on a Reynolds-averaged Navier-Stokes (RANS) formulation. Furthermore, we adopt a novel training approach that ensures effective initialization and balance among the various component
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;CounTS&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#20026;&#21487;&#35299;&#37322;&#24615;&#24314;&#27169;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.06024</link><description>&lt;p&gt;
&#21487;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Self-Interpretable Time Series Prediction with Counterfactual Explanations. (arXiv:2306.06024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;CounTS&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#20026;&#21487;&#35299;&#37322;&#24615;&#24314;&#27169;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20110;&#20687;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24605;&#36335;&#65292;&#26088;&#22312;&#24320;&#21457;&#20986;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#34987;&#31216;&#20026;Counterfactual Time Series&#65288;CounTS&#65289;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#26102;&#38388;&#24207;&#21015;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26102;&#38388;&#24207;&#21015;&#32465;&#26550;&#12289;&#34892;&#21160;&#21644;&#39044;&#27979;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#33258;&#25105;&#35299;&#37322;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#29305;&#23450;&#24352;&#37327;&#20998;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;Mixed-TD&#65292;&#37319;&#29992;&#28151;&#21512;&#26041;&#24335;&#30340;SVD&#21644;CPD&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#26144;&#23556;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21487;&#29992;&#29255;&#19978;&#23384;&#20648;&#22120;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.05021</link><description>&lt;p&gt;
Mixed-TD: &#22522;&#20110;&#23618;&#29305;&#23450;&#24352;&#37327;&#20998;&#35299;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition. (arXiv:2306.05021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#29305;&#23450;&#24352;&#37327;&#20998;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;Mixed-TD&#65292;&#37319;&#29992;&#28151;&#21512;&#26041;&#24335;&#30340;SVD&#21644;CPD&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#26144;&#23556;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21487;&#29992;&#29255;&#19978;&#23384;&#20648;&#22120;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30456;&#24403;&#22810;&#26679;&#21270;&#65292;&#20174;VGG&#21040;ResNet&#65292;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#21464;&#25442;&#22120;&#12290;&#20026;&#20102;&#35774;&#35745;&#25928;&#29575;&#39640;&#30340;&#21152;&#36895;&#22120;&#65292;&#35768;&#22810;&#24037;&#20316;&#37319;&#29992;&#20102;&#22522;&#20110;&#25968;&#25454;&#27969;&#30340;&#12289;&#23618;&#38388;&#27969;&#27700;&#32447;&#32467;&#26500;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#38024;&#23545;&#27599;&#19968;&#23618;&#36827;&#34892;&#20102;&#33258;&#23450;&#20041;&#30828;&#20214;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#36229;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#21040;&#27492;&#31867;&#25968;&#25454;&#27969;&#20307;&#31995;&#32467;&#26500;&#21152;&#36895;&#22120;&#19978;&#36890;&#24120;&#21463;&#21487;&#29992;&#29255;&#19978;&#20869;&#23384;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#39044;&#21152;&#36733;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21040;&#29255;&#19978;&#20197;&#26368;&#22823;&#21270;&#31995;&#32479;&#24615;&#33021;&#26159;&#29702;&#24819;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32593;&#32476;&#36890;&#24120;&#20250;&#36890;&#36807;&#20462;&#21098;&#12289;&#37327;&#21270;&#21644;&#24352;&#37327;&#20998;&#35299;&#31561;&#26041;&#27861;&#36827;&#34892;&#21387;&#32553;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;CNN&#26144;&#23556;&#21040;FPGA&#19978;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;Mixed-TD&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#28151;&#21512;&#26041;&#24335;&#30340;&#23618;&#29305;&#23450;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#21644;&#20856;&#22411;&#22810;&#39033;&#24335;&#20998;&#35299;&#65288;CPD&#65289;&#65292;&#22312;&#20445;&#25345;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#12290;Mixed-TD&#26694;&#26550;&#37319;&#29992;&#21160;&#24577;&#26144;&#23556;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#22320;&#21033;&#29992;&#21487;&#29992;&#30340;&#29255;&#19978;&#23384;&#20648;&#22120;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Mixed-TD&#26694;&#26550;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#21608;&#26399;&#26041;&#38754;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#21407;&#22987;&#26410;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Network designs are quite diverse, from VGG-style to ResNet-style, and from Convolutional Neural Networks to Transformers. Towards the design of efficient accelerators, many works have adopted a dataflow-based, inter-layer pipelined architecture, with a customised hardware towards each layer, achieving ultra high throughput and low latency. The deployment of neural networks to such dataflow architecture accelerators is usually hindered by the available on-chip memory as it is desirable to preload the weights of neural networks on-chip to maximise the system performance. To address this, networks are usually compressed before the deployment through methods such as pruning, quantization and tensor decomposition. In this paper, a framework for mapping CNNs onto FPGAs based on a novel tensor decomposition method called Mixed-TD is proposed. The proposed method applies layer-specific Singular Value Decomposition (SVD) and Canonical Polyadic Decomposition (CPD) in a mixed manner, achi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31163;&#25955;&#25193;&#25955;&#26680;&#22914;&#20309;&#24433;&#21709;&#22270;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#36873;&#25321;&#27491;&#30830;&#30340;&#25910;&#25947;&#20808;&#39564;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.02957</link><description>&lt;p&gt;
&#31163;&#25955;&#22270;&#25193;&#25955;&#20013;&#19981;&#21516;&#25910;&#25947;&#20808;&#39564;&#30340;&#22797;&#26434;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Complex Preferences for Different Convergent Priors in Discrete Graph Diffusion. (arXiv:2306.02957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31163;&#25955;&#25193;&#25955;&#26680;&#22914;&#20309;&#24433;&#21709;&#22270;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#36873;&#25321;&#27491;&#30830;&#30340;&#25910;&#25947;&#20808;&#39564;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#22312;&#29983;&#25104;&#35768;&#22810;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#35270;&#39057;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#22522;&#30784;&#25193;&#25955;&#36807;&#31243;&#21644;&#26368;&#32456;&#25910;&#25947;&#20808;&#39564;&#22914;&#20309;&#24433;&#21709;&#29983;&#25104;&#30340;&#24615;&#33021;&#36827;&#34892;&#30340;&#30740;&#31350;&#26377;&#38480;&#65307;&#27492;&#30740;&#31350;&#20063;&#20165;&#38480;&#20110;&#36830;&#32493;&#25968;&#25454;&#31867;&#22411;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#26694;&#26550;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#31163;&#25955;&#25193;&#25955;&#26680;&#65288;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#20808;&#39564;&#20998;&#24067;&#65289;&#22914;&#20309;&#24433;&#21709;&#22270;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#26680;&#31995;&#21015;&#20844;&#24335;&#65292;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#20271;&#21162;&#21033;&#20808;&#39564;&#65292;&#24182;&#30740;&#31350;&#36825;&#20123;&#19981;&#21516;&#30340;&#26680;&#23545;&#29983;&#25104;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#22270;&#30340;&#36136;&#37327;&#23545;&#20351;&#29992;&#30340;&#20808;&#39564;&#24456;&#25935;&#24863;&#65292;&#26368;&#20248;&#36873;&#25321;&#19981;&#33021;&#29992;&#26126;&#26174;&#30340;&#32479;&#35745;&#25968;&#25454;&#25110;&#25351;&#26631;&#26469;&#35299;&#37322;&#65292;&#36825;&#25361;&#25112;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#30452;&#35273;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31163;&#25955;&#25968;&#25454;&#19978;&#65292;&#36873;&#25321;&#27491;&#30830;&#30340;&#25910;&#25947;&#20808;&#39564;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved state-of-the-art performance in generating many different kinds of data, including images, text, and videos. Despite their success, there has been limited research on how the underlying diffusion process and the final convergent prior can affect generative performance; this research has also been limited to continuous data types and a score-based diffusion framework. To fill this gap, we explore how different discrete diffusion kernels (which converge to different prior distributions) affect the performance of diffusion models for graphs. To this end, we developed a novel formulation of a family of discrete diffusion kernels which are easily adjustable to converge to different Bernoulli priors, and we study the effect of these different kernels on generative performance. We show that the quality of generated graphs is sensitive to the prior used, and that the optimal choice cannot be explained by obvious statistics or metrics, which challenges the intuiti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20195;&#25968;&#35780;&#20272;&#22120;&#26469;&#20272;&#35745;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;&#31532;&#20108;&#31181;&#35780;&#20272;&#22120;&#30340;&#27491;&#30830;&#24615;&#34987;&#20445;&#35777;&#12290;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#65292;&#32531;&#35299;&#20102;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#65292;&#24182;&#36890;&#36807;&#25628;&#32034;&#26469;&#23547;&#25214;&#20960;&#20046;&#26080;&#35823;&#24046;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2306.01726</link><description>&lt;p&gt;
&#35780;&#20272;&#26377;&#22122;&#22768;&#21028;&#21035;&#22120;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#27969;&#24335;&#31639;&#27861; -- &#20108;&#20803;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification. (arXiv:2306.01726v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20195;&#25968;&#35780;&#20272;&#22120;&#26469;&#20272;&#35745;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;&#31532;&#20108;&#31181;&#35780;&#20272;&#22120;&#30340;&#27491;&#30830;&#24615;&#34987;&#20445;&#35777;&#12290;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#65292;&#32531;&#35299;&#20102;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#65292;&#24182;&#36890;&#36807;&#25628;&#32034;&#26469;&#23547;&#25214;&#20960;&#20046;&#26080;&#35823;&#24046;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#35780;&#20272;&#20316;&#20026;&#27969;&#24335;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;: &#32473;&#23450;&#19968;&#20010;&#20998;&#31867;&#22120;&#20915;&#31574;&#30340;&#25968;&#25454;&#33609;&#22270;&#65292;&#20272;&#35745;&#26631;&#31614;&#30340;&#30495;&#23454;&#27969;&#34892;&#24230;&#20197;&#21450;&#27599;&#20010;&#20998;&#31867;&#22120;&#23545;&#23427;&#20204;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#20004;&#31181;&#23436;&#20840;&#20195;&#25968;&#21270;&#30340;&#35780;&#20272;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20004;&#31181;&#35780;&#20272;&#22120;&#37117;&#22522;&#20110;&#20998;&#31867;&#22120;&#20135;&#29983;&#29420;&#31435;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#22810;&#25968;&#25237;&#31080;&#30340;&#12290;&#32780;&#31532;&#20108;&#31181;&#21017;&#26159;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#24182;&#34987;&#20445;&#35777;&#26159;&#27491;&#30830;&#30340;&#12290;&#20294;&#26159;&#22914;&#20309;&#30830;&#20445;&#20998;&#31867;&#22120;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#27979;&#35797;&#20013;&#26159;&#29420;&#31435;&#30340;&#21602;&#65311;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#26469;&#32531;&#35299;&#36825;&#20010;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#25968;&#25925;&#38556;&#27169;&#24335;&#26469;&#25298;&#32477;&#22826;&#30456;&#20851;&#30340;&#35780;&#20272;&#38598;&#21512;&#65292;&#20351;&#29992; \texttt{adult}&#65292;\texttt{mushroom} &#21644; \texttt{two-norm} &#25968;&#25454;&#38598;&#23545;&#19968;&#32452;&#20960;&#20046;&#26080;&#35823;&#24046;&#19977;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#35777;&#25628;&#32034;&#12290;&#36825;&#20123;&#25628;&#32034;&#36890;&#36807;&#26500;&#24314;&#35780;&#20272;&#31354;&#38388;&#20013;&#30340;&#34920;&#38754;&#26469;&#36827;&#34892;&#31934;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming task: given a data sketch of the decisions by an ensemble, estimate the true prevalence of the labels as well as each classifier's accuracy on them. Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that the classifiers make independent errors. The first is based on majority voting. The second, the main contribution of the paper, is guaranteed to be correct. But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates. A search for nearly error independent trios is empirically carried out on the \texttt{adult}, \texttt{mushroom}, and \texttt{two-norm} datasets by using the algebraic failure modes to reject evaluation ensembles as too correlated. The searches are refined by constructing a surface in evaluation spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#21644;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00684</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#27969;&#37319;&#26679;&#24179;&#34913;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#21644;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411; (EBM) &#26159;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#26410;&#26631;&#20934;&#21270;&#23545;&#25968;&#23494;&#24230;&#30340;&#22810;&#21151;&#33021;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;EBM &#38750;&#24120;&#28789;&#27963;&#65292;&#20294;&#32570;&#20047;&#27169;&#22411;&#30340;&#35268;&#33539;&#21270;&#24120;&#37327;&#65292;&#20351;&#27169;&#22411;&#30340;&#20284;&#28982;&#20989;&#25968;&#35745;&#31639;&#19981;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36817;&#20284;&#37319;&#26679;&#22120;&#21644;&#21464;&#20998;&#25512;&#29702;&#25216;&#26415;&#26469;&#20272;&#35745;&#20284;&#28982;&#20989;&#25968;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#29983;&#25104;&#26679;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#20272;&#35745;&#23494;&#24230;&#30340;&#32479;&#35745;&#31934;&#24230;&#65292;&#20363;&#22914;&#30830;&#23450;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#31867;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#21364;&#20184;&#20986;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24402;&#19968;&#21270;&#27969; (NF)&#65292;&#36825;&#31181;&#27169;&#22411;&#26368;&#36817;&#34987;&#25552;&#20986;&#20197;&#20415;&#20110;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558; NF &#25311;&#21512;&#21040; EBM &#19978;&#65292;&#20197;&#20415; NF &#36741;&#21161;&#19979;&#30340;&#37319;&#26679;&#26041;&#26696;&#33021;&#22815;&#22987;&#32456;&#20026; EBM &#25552;&#20379;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#26368;&#32456;&#25552;&#39640;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479; EBM &#35757;&#32451;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#21644;&#26356;&#22909;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#31181;&#21270;&#21512;&#29289;&#36129;&#29486;&#23545;&#31895;&#31961;BVOC&#25490;&#25918;&#22320;&#22270;&#36827;&#34892;&#36229;&#20998;&#36776;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36229;&#20998;&#36776;&#29575;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14180</link><description>&lt;p&gt;
&#21033;&#29992;&#21270;&#21512;&#29289;&#20114;&#36830;&#30340;&#22810;&#31181;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#30340;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Multi-BVOC Super-Resolution Exploiting Compounds Inter-Connection. (arXiv:2305.14180v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#31181;&#21270;&#21512;&#29289;&#36129;&#29486;&#23545;&#31895;&#31961;BVOC&#25490;&#25918;&#22320;&#22270;&#36827;&#34892;&#36229;&#20998;&#36776;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36229;&#20998;&#36776;&#29575;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38470;&#22320;&#29983;&#24577;&#31995;&#32479;&#25490;&#25918;&#21040;&#22320;&#29699;&#22823;&#27668;&#20013;&#30340;&#29983;&#29289;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;BVOC&#65289;&#26159;&#22823;&#27668;&#21270;&#23398;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#30001;&#20110;&#27979;&#37327;&#30340;&#31232;&#32570;&#24615;&#65292;&#21487;&#38752;&#30340;BVOC&#25490;&#25918;&#22320;&#22270;&#21487;&#20197;&#25552;&#20379;&#26356;&#23494;&#38598;&#30340;&#25968;&#25454;&#65292;&#20197;&#20379;&#22823;&#27668;&#21270;&#23398;&#12289;&#27668;&#20505;&#21644;&#31354;&#27668;&#36136;&#37327;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#21516;&#21270;&#21512;&#29289;&#36129;&#29486;&#21516;&#26102;&#36229;&#20998;&#36776;&#31895;&#31961;BVOC&#25490;&#25918;&#22320;&#22270;&#30340;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20934;&#30830;&#22320;&#35843;&#26597;&#20960;&#31181;BVOC&#29289;&#31181;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21457;&#29616;&#30340;&#30456;&#20284;&#24615;&#24314;&#31435;&#20102;&#19968;&#20010;Multi-Image Super-Resolution&#65288;MISR&#65289;&#31995;&#32479;&#65292;&#20854;&#20013;&#19982;&#19981;&#21516;&#21270;&#21512;&#29289;&#30456;&#20851;&#32852;&#30340;&#22810;&#20010;&#25490;&#25918;&#22320;&#22270;&#34987;&#38598;&#25104;&#20197;&#25552;&#39640;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#37197;&#32622;&#30340;&#29289;&#31181;&#21644;&#21512;&#24182;BVOC&#25968;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;BVOC&#20851;&#31995;&#32435;&#20837;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#39640;SR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biogenic Volatile Organic Compounds (BVOCs) emitted from the terrestrial ecosystem into the Earth's atmosphere are an important component of atmospheric chemistry. Due to the scarcity of measurement, a reliable enhancement of BVOCs emission maps can aid in providing denser data for atmospheric chemical, climate, and air quality models. In this work, we propose a strategy to super-resolve coarse BVOC emission maps by simultaneously exploiting the contributions of different compounds. To this purpose, we first accurately investigate the spatial inter-connections between several BVOC species. Then, we exploit the found similarities to build a Multi-Image Super-Resolution (MISR) system, in which a number of emission maps associated with diverse compounds are aggregated to boost Super-Resolution (SR) performance. We compare different configurations regarding the species and the number of joined BVOCs. Our experimental results show that incorporating BVOCs' relationship into the process can 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11554</link><description>&lt;p&gt;
ToolkenGPT&#65306;&#36890;&#36807;&#24037;&#20855;&#23884;&#20837;&#25193;&#20805;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#29992;&#24037;&#20855;&#28436;&#31034;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#26082;&#36153;&#26102;&#21448;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#24037;&#20855;&#38598;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#20363;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21482;&#20801;&#35768;&#28436;&#31034;&#20960;&#27425;&#65292;&#23548;&#33268;&#23545;&#24037;&#20855;&#30340;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#22823;&#37327;&#24037;&#20855;&#21487;&#20379;&#36873;&#25321;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#23436;&#20840;&#26080;&#27861;&#27491;&#24120;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{ToolkenGPT}$&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;$\underline{&#24037;&#20855;}$&#34920;&#31034;&#20026;&#19968;&#20010;$\underline{token}$&#65288;$\textit{toolken}$&#65289;&#65292;&#24182;&#20026;&#20854;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#24471;&#24037;&#20855;&#35843;&#29992;&#19982;&#29983;&#25104;&#24120;&#35268;&#21333;&#35789;&#26631;&#35760;&#30340;&#26041;&#24335;&#30456;&#21516;&#12290;&#19968;&#26086;&#35302;&#21457;&#20102;toolken&#65292;LLM&#34987;&#25552;&#31034;&#23436;&#25104;&#24037;&#20855;&#25191;&#34892;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;ToolkenGPT&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;1&#65289;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#20805;LLM&#19982;&#22806;&#37096;&#24037;&#20855;&#30340;&#20132;&#20114;&#65292;2&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;3&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
&lt;/p&gt;</description></item><item><title>CosmoPower-JAX&#20351;&#29992;&#21487;&#24494;&#30340;&#23431;&#23449;&#27169;&#25311;&#22120;&#36827;&#34892;&#39640;&#32500;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20854;&#37319;&#29992;JAX&#30340;&#29305;&#24615;&#20197;&#21450;GPU&#25216;&#26415;&#21152;&#36895;&#21442;&#25968;&#20272;&#35745;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#20934;&#30830;&#30340;&#21442;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.06347</link><description>&lt;p&gt;
CosmoPower-JAX:&#21033;&#29992;&#21487;&#24494;&#30340;&#23431;&#23449;&#27169;&#25311;&#22120;&#36827;&#34892;&#39640;&#32500;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators. (arXiv:2305.06347v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06347
&lt;/p&gt;
&lt;p&gt;
CosmoPower-JAX&#20351;&#29992;&#21487;&#24494;&#30340;&#23431;&#23449;&#27169;&#25311;&#22120;&#36827;&#34892;&#39640;&#32500;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20854;&#37319;&#29992;JAX&#30340;&#29305;&#24615;&#20197;&#21450;GPU&#25216;&#26415;&#21152;&#36895;&#21442;&#25968;&#20272;&#35745;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#20934;&#30830;&#30340;&#21442;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CosmoPower-JAX&#65292;&#36825;&#26159;CosmoPower&#26694;&#26550;&#30340;&#19968;&#20010;&#22522;&#20110;JAX&#30340;&#23454;&#29616;&#65292;&#36890;&#36807;&#26500;&#24314;&#23431;&#23449;&#21151;&#29575;&#35889;&#30340;&#31070;&#32463;&#20223;&#30495;&#22120;&#26469;&#21152;&#36895;&#23431;&#23449;&#23398;&#25512;&#26029;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;JAX&#30340;&#33258;&#21160;&#24494;&#20998;&#12289;&#25209;&#37327;&#35780;&#20272;&#21644;&#21363;&#26102;&#32534;&#35793;&#29305;&#24615;&#65292;&#24182;&#22312;&#22270;&#24418;&#22788;&#29702;&#22120;&#65288;GPU&#65289;&#19978;&#36816;&#34892;&#25512;&#26029;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#37319;&#26679;&#25216;&#26415;&#23558;&#21442;&#25968;&#20272;&#35745;&#21152;&#36895;&#25968;&#20493;&#12290;&#36825;&#20123;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#22320;&#25506;&#32034;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#65292;&#20363;&#22914;&#29992;&#20110;&#19979;&#19968;&#20195;&#23431;&#23449;&#23398;&#35843;&#26597;&#20998;&#26512;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CosmoPower-JAX&#22312;&#20004;&#20010;&#27169;&#25311;&#30340;&#31532;&#22235;&#38454;&#27573;&#37197;&#32622;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#19968;&#20010;&#36827;&#34892;37&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#23431;&#23449;&#21098;&#20999;&#20998;&#26512;&#30340;&#21333;&#20010;&#35843;&#26597;&#12290;&#25105;&#20204;&#20351;&#29992;CosmoPower-JAX&#21644;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599;&#21462;&#26679;&#22120;&#27966;&#29983;&#30340;&#31561;&#39640;&#32447;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#19982;&#26410;&#20351;&#29992;&#20223;&#30495;&#20284;&#28982;&#30340;&#23884;&#22871;&#21462;&#26679;&#22120;&#27966;&#29983;&#30340;&#31561;&#39640;&#32447;&#30456;&#31526;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23431;&#23449;&#21098;&#20999;&#21644;&#26143;&#31995;&#32858;&#31867;&#30340;&#32852;&#21512;&#20998;&#26512;&#65292;&#23558;&#21442;&#25968;&#31354;&#38388;&#22686;&#21152;&#21040;167&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#23454;&#29616;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20960;&#20998;&#38047;&#32780;&#19981;&#26159;&#20960;&#22825;&#20869;&#20135;&#29983;&#21518;&#39564;&#21644;&#21442;&#25968;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CosmoPower-JAX, a JAX-based implementation of the CosmoPower framework, which accelerates cosmological inference by building neural emulators of cosmological power spectra. We show how, using the automatic differentiation, batch evaluation and just-in-time compilation features of JAX, and running the inference pipeline on graphics processing units (GPUs), parameter estimation can be accelerated by orders of magnitude with advanced gradient-based sampling techniques. These can be used to efficiently explore high-dimensional parameter spaces, such as those needed for the analysis of next-generation cosmological surveys. We showcase the accuracy and computational efficiency of CosmoPower-JAX on two simulated Stage IV configurations. We first consider a single survey performing a cosmic shear analysis totalling 37 model parameters. We validate the contours derived with CosmoPower-JAX and a Hamiltonian Monte Carlo sampler against those derived with a nested sampler and without em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#20998;&#31867;&#26641;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#35775;&#38382;&#22823;&#37096;&#20998;&#26469;&#33258;&#20998;&#24067; $P_{X&#65292;Y}$ &#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#21482;&#33021;&#33719;&#24471;&#26469;&#33258;&#25317;&#26377;&#19981;&#21516; $X$-&#36793;&#32536;&#30340;&#30446;&#26631;&#20998;&#24067; $Q_{X&#65292;Y}$ &#30340;&#23569;&#37327;&#25968;&#25454;&#12290;&#20351;&#29992;&#30340;&#20248;&#21270;&#26631;&#20934;&#26159;&#19968;&#20010;&#20851;&#20110;&#20998;&#24067; $P_{X} \to Q_{X}$ &#30340; \emph{&#24179;&#22343;&#24046;&#24322;}&#65292;&#35813;&#26631;&#20934;&#21487;&#20197;&#26174;&#33879;&#25918;&#23485;&#26368;&#36817;&#25552;&#20986;&#30340; \emph{&#36716;&#31227;&#25351;&#25968;}&#65292;&#26368;&#32456;&#21487;&#20197;&#24471;&#21040;&#26368;&#20248;&#30340;&#21098;&#26525;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04335</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#20998;&#31867;&#26641;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Classification Tree Pruning Under Covariate Shift. (arXiv:2305.04335v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#20998;&#31867;&#26641;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#35775;&#38382;&#22823;&#37096;&#20998;&#26469;&#33258;&#20998;&#24067; $P_{X&#65292;Y}$ &#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#21482;&#33021;&#33719;&#24471;&#26469;&#33258;&#25317;&#26377;&#19981;&#21516; $X$-&#36793;&#32536;&#30340;&#30446;&#26631;&#20998;&#24067; $Q_{X&#65292;Y}$ &#30340;&#23569;&#37327;&#25968;&#25454;&#12290;&#20351;&#29992;&#30340;&#20248;&#21270;&#26631;&#20934;&#26159;&#19968;&#20010;&#20851;&#20110;&#20998;&#24067; $P_{X} \to Q_{X}$ &#30340; \emph{&#24179;&#22343;&#24046;&#24322;}&#65292;&#35813;&#26631;&#20934;&#21487;&#20197;&#26174;&#33879;&#25918;&#23485;&#26368;&#36817;&#25552;&#20986;&#30340; \emph{&#36716;&#31227;&#25351;&#25968;}&#65292;&#26368;&#32456;&#21487;&#20197;&#24471;&#21040;&#26368;&#20248;&#30340;&#21098;&#26525;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#22343;&#21248;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#23376;&#26641;&#20197;&#24179;&#34913;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#20998;&#31867;&#26641;&#21098;&#26525;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#30340;&#26368;&#20248;&#21098;&#26525;&#30340;&#39640;&#25928;&#31243;&#24207;&#65292;&#35813;&#31243;&#24207;&#21487;&#20197;&#35775;&#38382;&#22823;&#37096;&#20998;&#26469;&#33258;&#20998;&#24067; $P_{X&#65292;Y}$ &#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#21482;&#33021;&#33719;&#24471;&#26469;&#33258;&#25317;&#26377;&#19981;&#21516; $X$-&#36793;&#32536;&#30340;&#30446;&#26631;&#20998;&#24067; $Q_{X&#65292;Y}$ &#30340;&#23569;&#37327;&#25968;&#25454;&#12290;&#22312;&#22522;&#26412;&#20132;&#21449;&#39564;&#35777;&#21644;&#20854;&#20182;&#36827;&#34892;&#24809;&#32602;&#30340;&#21464;&#20307;&#65292;&#22914;&#22522;&#20110;&#20449;&#24687;&#24230;&#37327;&#30340;&#21098;&#26525;&#26041;&#27861;&#38750;&#24120;&#19981;&#29702;&#24819;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26368;&#20248;&#21098;&#26525;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#30340;&#20248;&#21270;&#26631;&#20934;&#26159;&#19968;&#20010;&#20851;&#20110;&#20998;&#24067; $P_{X} \to Q_{X}$ &#30340; \emph{&#24179;&#22343;&#24046;&#24322;}&#65288;&#22312; $X$ &#31354;&#38388;&#19978;&#24179;&#22343;&#65289;&#65292;&#35813;&#26631;&#20934;&#21487;&#20197;&#26174;&#33879;&#25918;&#23485;&#26368;&#36817;&#25552;&#20986;&#30340; \emph{&#36716;&#31227;&#25351;&#25968;} &#36825;&#19968;&#32479;&#35745;&#23398;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#34987;&#35777;&#26126;&#33021;&#22815;&#32039;&#23494;&#22320;&#25429;&#25417;&#36825;&#31181;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#20998;&#31867;&#30340;&#26497;&#38480;&#38480;&#21046;&#12290;&#25105;&#20204;&#25918;&#23485;&#30340;&#26631;&#20934;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#20998;&#24067;&#20043;&#38388;&#30340;\emph{&#30456;&#23545;&#32500;&#24230;}&#24230;&#37327;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#20449;&#24687;&#30340;&#29616;&#26377;&#24230;&#37327;&#27010;&#24565;&#65292;&#20363;&#22914;&#38389;&#21487;&#22827;&#26031;&#22522;&#21644;R&#233;nyi&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of \emph{pruning} a classification tree, that is, selecting a suitable subtree that balances bias and variance, in common situations with inhomogeneous training data. Namely, assuming access to mostly data from a distribution $P_{X, Y}$, but little data from a desired distribution $Q_{X, Y}$ with different $X$-marginals, we present the first efficient procedure for optimal pruning in such situations, when cross-validation and other penalized variants are grossly inadequate. Optimality is derived with respect to a notion of \emph{average discrepancy} $P_{X} \to Q_{X}$ (averaged over $X$ space) which significantly relaxes a recent notion -- termed \emph{transfer-exponent} -- shown to tightly capture the limits of classification under such a distribution shift. Our relaxed notion can be viewed as a measure of \emph{relative dimension} between distributions, as it relates to existing notions of information such as the Minkowski and Renyi dimensions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20102;&#19968;&#31181;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312; DA &#33021;&#28304;&#24066;&#22330;&#19978;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#33258;&#21160;&#36827;&#34892;&#20132;&#26131;&#12290;</title><link>http://arxiv.org/abs/2303.16266</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#20132;&#26131;&#31574;&#30053;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning for optimization of energy trading strategy. (arXiv:2303.16266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20102;&#19968;&#31181;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312; DA &#33021;&#28304;&#24066;&#22330;&#19978;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#33258;&#21160;&#36827;&#34892;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#33021;&#28304;&#26469;&#33258;&#22823;&#37327;&#23567;&#22411;&#29983;&#20135;&#32773;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#36825;&#20123;&#26469;&#28304;&#30340;&#25928;&#29575;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20063;&#26159;&#38543;&#26426;&#30340;&#65292;&#21152;&#21095;&#20102;&#33021;&#28304;&#24066;&#22330;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#22269;&#23478;&#65292;&#36825;&#31181;&#24179;&#34913;&#26159;&#22312;&#39044;&#27979;&#26085;&#65288;DA&#65289;&#33021;&#28304;&#24066;&#22330;&#19978;&#23436;&#25104;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#22312;DA&#33021;&#28304;&#24066;&#22330;&#19978;&#30340;&#33258;&#21160;&#21270;&#20132;&#26131;&#12290;&#25105;&#20204;&#23558;&#27492;&#27963;&#21160;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35268;&#33539;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#20248;&#21270;&#21363;&#29992;&#31574;&#30053;&#12290;&#25105;&#20204;&#21512;&#25104;&#21442;&#25968;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#19968;&#20010;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#26469;&#33258;&#29615;&#22659;&#30340;&#21487;&#29992;&#20449;&#24687;&#26469;&#24433;&#21709;&#26410;&#26469;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing part of energy is produced from renewable sources by a large number of small producers. The efficiency of these sources is volatile and, to some extent, random, exacerbating the energy market balance problem. In many countries, that balancing is performed on day-ahead (DA) energy markets. In this paper, we consider automated trading on a DA energy market by a medium size prosumer. We model this activity as a Markov Decision Process and formalize a framework in which a ready-to-use strategy can be optimized with real-life data. We synthesize parametric trading strategies and optimize them with an evolutionary algorithm. We also use state-of-the-art reinforcement learning algorithms to optimize a black-box trading strategy fed with available information from the environment that can impact future prices.
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06053
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#25429;&#33719;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20687;&#24490;&#29615;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36825;&#26679;&#30340;&#39640;&#23481;&#37327;&#32467;&#26500;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25193;&#23637;&#23427;&#20204;&#65292;&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#24207;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#12290; TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#31616;&#21333;&#26131;&#34892;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#19968;&#20010;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20986;&#38750;&#24120;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.04091</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23454;&#29616;&#35270;&#35273;&#25277;&#35937;&#21644;&#25512;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#23616;&#38480;&#24212;&#29992;&#20013;&#24050;&#32463;&#36798;&#21040;&#20102;&#20154;&#31867;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#23637;&#29616;&#26356;&#24191;&#27867;&#21644;&#26356;&#28789;&#27963;&#30340;&#26234;&#33021;&#12290;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#26088;&#22312;&#35780;&#20272;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#65292;&#29992;&#20110;&#26292;&#21147;&#35299;&#20915;ARC&#20013;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;ARC&#38382;&#39064;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.00028</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#36830;&#32493;&#21644;&#31163;&#25955;&#31354;&#38388;&#30340;&#22238;&#24402;&#20256;&#24863;&#22120;&#25918;&#32622;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#26696;&#65292;&#29992;&#20110;&#30417;&#27979;&#28201;&#24230;&#12289;&#38477;&#27700;&#31561;&#31354;&#38388;&#65288;&#25110;&#26102;&#31354;&#65289;&#30456;&#20851;&#29616;&#35937;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#24050;&#30693;&#20869;&#26680;&#20989;&#25968;&#21442;&#25968;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#21040;&#29615;&#22659;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#26410;&#26631;&#35760;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#30340;&#35825;&#23548;&#28857;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#12290;&#20351;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#32423;&#21035;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#12290;&#22312;&#20505;&#36873;&#20256;&#24863;&#22120;&#25918;&#32622;&#28857;&#38598;&#21512;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#36138;&#23146;&#39034;&#24207;&#36873;&#25321;&#31639;&#27861;&#26469;&#25214;&#21040;&#36739;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
&lt;/p&gt;</description></item><item><title>IGB&#26159;&#19968;&#20010;&#30740;&#31350;&#25968;&#25454;&#38598;&#24037;&#20855;&#65292;&#21253;&#21547;&#21516;&#36136;&#21644;&#24322;&#36136;&#24615;&#23398;&#26415;&#22270;&#24418;&#65292;&#35268;&#27169;&#24040;&#22823;&#65292;&#24182;&#25552;&#20379;&#24037;&#20855;&#29992;&#20110;&#29983;&#25104;&#19981;&#21516;&#29305;&#24615;&#30340;&#21512;&#25104;&#22270;&#24418;&#65292;&#20026;GNN&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#35299;&#20915;&#20844;&#20849;&#22270;&#24418;&#25968;&#25454;&#38598;&#22312;&#26631;&#35760;&#12289;&#29305;&#24449;&#12289;&#24322;&#36136;&#24615;&#21644;&#22823;&#23567;&#26041;&#38754;&#24046;&#36317;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2302.13522</link><description>&lt;p&gt;
IGB: &#38024;&#23545;&#20844;&#20849;&#22270;&#24418;&#25968;&#25454;&#38598;&#22312;&#26631;&#35760;&#12289;&#29305;&#24449;&#12289;&#24322;&#36136;&#24615;&#21644;&#22823;&#23567;&#26041;&#38754;&#30340;&#24046;&#36317;&#20026;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research. (arXiv:2302.13522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13522
&lt;/p&gt;
&lt;p&gt;
IGB&#26159;&#19968;&#20010;&#30740;&#31350;&#25968;&#25454;&#38598;&#24037;&#20855;&#65292;&#21253;&#21547;&#21516;&#36136;&#21644;&#24322;&#36136;&#24615;&#23398;&#26415;&#22270;&#24418;&#65292;&#35268;&#27169;&#24040;&#22823;&#65292;&#24182;&#25552;&#20379;&#24037;&#20855;&#29992;&#20110;&#29983;&#25104;&#19981;&#21516;&#29305;&#24615;&#30340;&#21512;&#25104;&#22270;&#24418;&#65292;&#20026;GNN&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#35299;&#20915;&#20844;&#20849;&#22270;&#24418;&#25968;&#25454;&#38598;&#22312;&#26631;&#35760;&#12289;&#29305;&#24449;&#12289;&#24322;&#36136;&#24615;&#21644;&#22823;&#23567;&#26041;&#38754;&#24046;&#36317;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#31070;&#32463;&#32593;&#32476; (GNNs) &#24050;&#32463;&#23637;&#31034;&#20102;&#23545;&#20110;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294; GNN &#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#22823;&#35268;&#27169;&#28789;&#27963;&#30340;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#20844;&#20849;GNN&#25968;&#25454;&#38598;&#37117;&#30456;&#23545;&#36739;&#23567;&#65292;&#36825;&#38480;&#21046;&#20102; GNN &#30340;&#25512;&#24191;&#21040;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#24456;&#23569;&#26377;&#22823;&#22411;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#25552;&#20379;&#20016;&#23500;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#30830;&#23450; GNN &#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#30340;&#20302;&#20934;&#30830;&#24615;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#36824;&#26159;&#27169;&#22411;&#26080;&#27861;&#25512;&#24191;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451; GNN &#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#20197;&#20415;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;&#22240;&#32032;&#23545; GNN &#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20234;&#21033;&#35834;&#20234;&#22270;&#24418;&#22522;&#20934; (IGB)&#65292;&#36825;&#26159;&#19968;&#20010;&#30740;&#31350;&#25968;&#25454;&#38598;&#24037;&#20855;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#39640;&#31934;&#24230;&#22320;&#35757;&#32451;&#12289;&#23457;&#26597;&#21644;&#31995;&#32479;&#22320;&#35780;&#20272;GNN&#27169;&#22411;&#12290;IGB &#21253;&#25324;&#21516;&#36136;&#21644;&#24322;&#36136;&#24615;&#23398;&#26415;&#22270;&#24418;&#65292;&#35268;&#27169;&#24040;&#22823;&#65292;&#24182;&#19988;&#21487;&#20197;&#26631;&#35760;&#21644;&#25805;&#20316;&#65292;&#20197;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#21512;&#25104;&#22270;&#24418;&#30340;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#21508;&#31181;&#22270;&#24418;&#29305;&#24615;&#23545; GNN &#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#20234;&#21033;&#35834;&#20234;&#22270;&#24418;&#22522;&#20934;&#23558;&#20026; GNN &#30740;&#31350;&#22242;&#20307;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#22270;&#24418;&#25968;&#25454;&#38598;&#22312;&#26631;&#35760;&#12289;&#29305;&#24449;&#12289;&#24322;&#36136;&#24615;&#21644;&#22823;&#23567;&#26041;&#38754;&#30340;&#24046;&#36317;&#65292;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown high potential for a variety of real-world, challenging applications, but one of the major obstacles in GNN research is the lack of large-scale flexible datasets. Most existing public datasets for GNNs are relatively small, which limits the ability of GNNs to generalize to unseen data. The few existing large-scale graph datasets provide very limited labeled data. This makes it difficult to determine if the GNN model's low accuracy for unseen data is inherently due to insufficient training data or if the model failed to generalize. Additionally, datasets used to train GNNs need to offer flexibility to enable a thorough study of the impact of various factors while training GNN models.  In this work, we introduce the Illinois Graph Benchmark (IGB), a research dataset tool that the developers can use to train, scrutinize and systematically evaluate GNN models with high fidelity. IGB includes both homogeneous and heterogeneous academic graphs of enorm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#35774;&#35745;&#30340;&#38543;&#26426;&#26679;&#26412;&#30340;&#32447;&#24615;&#22238;&#24402;&#30340;&#23545;&#31216;&#28151;&#21512;&#65292;&#36890;&#36807;&#20998;&#26512;EM&#31639;&#27861;&#30340;&#24207;&#21015;&#25910;&#25947;&#24615;&#21644;&#26497;&#38480;&#20540;&#65292;&#24471;&#20986;&#20102;$\ell_\infty$&#33539;&#25968;&#21644;$\ell_2$&#33539;&#25968;&#20013;&#30340;&#20272;&#35745;&#23574;&#38160;&#24230;&#12290;&#30740;&#31350;&#34920;&#26126;EM&#31639;&#27861;&#21487;&#20197;&#23637;&#29616;&#20986;&#22810;&#20010;&#29420;&#29305;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2302.10066</link><description>&lt;p&gt;
&#23398;&#20064;&#25104;&#23545;&#24046;&#20998;&#28151;&#21512;&#30340;EM&#31639;&#27861;&#30340;&#23574;&#38160;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharp analysis of EM for learning mixtures of pairwise differences. (arXiv:2302.10066v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#35774;&#35745;&#30340;&#38543;&#26426;&#26679;&#26412;&#30340;&#32447;&#24615;&#22238;&#24402;&#30340;&#23545;&#31216;&#28151;&#21512;&#65292;&#36890;&#36807;&#20998;&#26512;EM&#31639;&#27861;&#30340;&#24207;&#21015;&#25910;&#25947;&#24615;&#21644;&#26497;&#38480;&#20540;&#65292;&#24471;&#20986;&#20102;$\ell_\infty$&#33539;&#25968;&#21644;$\ell_2$&#33539;&#25968;&#20013;&#30340;&#20272;&#35745;&#23574;&#38160;&#24230;&#12290;&#30740;&#31350;&#34920;&#26126;EM&#31639;&#27861;&#21487;&#20197;&#23637;&#29616;&#20986;&#22810;&#20010;&#29420;&#29305;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#35774;&#35745;&#30340;&#38543;&#26426;&#26679;&#26412;&#30340;&#32447;&#24615;&#22238;&#24402;&#30340;&#23545;&#31216;&#28151;&#21512;&#65292;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#30340;&#22122;&#22768;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#20540;&#21608;&#22260;&#23616;&#37096;&#20998;&#26512;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#36215;&#23427;&#30340;&#24207;&#21015;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#20174;&#32780;&#20026;&#36845;&#20195;&#20272;&#35745;&#35823;&#24046;&#25552;&#20379;&#19968;&#20010;$\ell_\infty$-&#33539;&#25968;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;EM&#24207;&#21015;&#30340;&#26497;&#38480;&#23454;&#29616;&#20102;&#22312;$\ell_2$-&#33539;&#25968;&#20013;&#30340;&#20272;&#35745;&#23574;&#38160;&#24230;&#65292;&#21305;&#37197;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#24120;&#25968;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#27169;&#25311;&#35770;&#35777;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#25910;&#25947;&#30340;&#38382;&#39064;&#26356;&#20026;&#24494;&#22937;&#65292;&#36890;&#24120;&#19981;&#20250;&#21457;&#29983;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21327;&#21464;&#37327;&#20998;&#24067;&#34987;&#36866;&#24403;&#22320;&#32467;&#26500;&#21270;&#26102;&#65292;EM&#31639;&#27861;&#21487;&#20197;&#34920;&#29616;&#20986;&#20960;&#20010;&#29420;&#29305;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a symmetric mixture of linear regressions with random samples from the pairwise comparison design, which can be seen as a noisy version of a type of Euclidean distance geometry problem. We analyze the expectation-maximization (EM) algorithm locally around the ground truth and establish that the sequence converges linearly, providing an $\ell_\infty$-norm guarantee on the estimation error of the iterates. Furthermore, we show that the limit of the EM sequence achieves the sharp rate of estimation in the $\ell_2$-norm, matching the information-theoretically optimal constant. We also argue through simulation that convergence from a random initialization is much more delicate in this setting, and does not appear to occur in general. Our results show that the EM algorithm can exhibit several unique behaviors when the covariate distribution is suitably structured.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21333;&#26102;&#38388;&#35268;&#27169;&#30340;&#31639;&#27861;&#65306;Prox-DASA&#21644;Prox-DASA-GT&#65292;&#23427;&#20204;&#21487;&#20197;&#29992;&#24120;&#37327;&#25209;&#37327;&#22823;&#23567;&#25214;&#21040;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#30340;$\epsilon$-&#38745;&#27490;&#28857;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#22823;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#22797;&#26434;&#30340;&#25805;&#20316;&#25110;&#26356;&#24378;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2302.09766</link><description>&lt;p&gt;
&#19968;&#31181;&#21333;&#26679;&#26412;&#21435;&#20013;&#24515;&#21270;&#36817;&#31471;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#38543;&#26426;&#22797;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization. (arXiv:2302.09766v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21333;&#26102;&#38388;&#35268;&#27169;&#30340;&#31639;&#27861;&#65306;Prox-DASA&#21644;Prox-DASA-GT&#65292;&#23427;&#20204;&#21487;&#20197;&#29992;&#24120;&#37327;&#25209;&#37327;&#22823;&#23567;&#25214;&#21040;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#30340;$\epsilon$-&#38745;&#27490;&#28857;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#22823;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#22797;&#26434;&#30340;&#25805;&#20316;&#25110;&#26356;&#24378;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;$n$&#20010;&#20195;&#29702;&#20849;&#21516;&#20248;&#21270;&#30001;&#20809;&#28369;&#39033;&#21644;&#38750;&#20809;&#28369;&#20984;&#39033;&#30456;&#21152;&#30340;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#21333;&#26102;&#38388;&#35268;&#27169;&#31639;&#27861;&#65306;Prox-DASA&#21644;Prox-DASA-GT&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#24120;&#37327;&#25209;&#37327;&#22823;&#23567;&#65288;&#21363;$\mathcal{O}(1)$&#65289;&#22312;$\mathcal{O}(n^{-1}\epsilon^{-2})$&#27425;&#36845;&#20195;&#20013;&#25214;&#21040;$\epsilon$-&#38745;&#27490;&#28857;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19981;&#38656;&#35201;&#22823;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#22797;&#26434;&#30340;&#27599;&#27425;&#36845;&#20195;&#25805;&#20316;&#65288;&#22914;&#21452;&#37325;&#24490;&#29615;&#65289;&#25110;&#26356;&#24378;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21487;&#27604;&#25311;&#30340;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#65292;&#36825;&#20123;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/xuxingc/ProxDASA&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on decentralized stochastic non-convex optimization, where $n$ agents work together to optimize a composite objective function which is a sum of a smooth term and a non-smooth convex term. To solve this problem, we propose two single-time scale algorithms: Prox-DASA and Prox-DASA-GT. These algorithms can find $\epsilon$-stationary points in $\mathcal{O}(n^{-1}\epsilon^{-2})$ iterations using constant batch sizes (i.e., $\mathcal{O}(1)$). Unlike prior work, our algorithms achieve comparable complexity without requiring large batch sizes, more complex per-iteration operations (such as double loops), or stronger assumptions. Our theoretical findings are supported by extensive numerical experiments, which demonstrate the superiority of our algorithms over previous approaches. Our code is available at https://github.com/xuxingc/ProxDASA.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29420;&#31435;&#32447;&#24615;Markov&#21338;&#24328;&#27169;&#22411;&#65292;&#38024;&#23545;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22823;&#37327;&#20195;&#29702;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#26032;&#31639;&#27861;&#20197;&#23398;&#20064;Markov&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#21644;Markov&#30456;&#20851;&#22343;&#34913;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;Markov&#21338;&#24328;&#20989;&#25968;&#36924;&#36817;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#24182;&#21462;&#24471;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.03673</link><description>&lt;p&gt;
&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#20013;&#25171;&#30772;&#22810;&#26234;&#20307;&#30340;&#35781;&#21650;&#65306;&#24102;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;Markov&#21338;&#24328;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation. (arXiv:2302.03673v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29420;&#31435;&#32447;&#24615;Markov&#21338;&#24328;&#27169;&#22411;&#65292;&#38024;&#23545;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22823;&#37327;&#20195;&#29702;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#26032;&#31639;&#27861;&#20197;&#23398;&#20064;Markov&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#21644;Markov&#30456;&#20851;&#22343;&#34913;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;Markov&#21338;&#24328;&#20989;&#25968;&#36924;&#36817;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#24182;&#21462;&#24471;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#8212;&#8212;&#29420;&#31435;&#32447;&#24615;Markov&#21338;&#24328;&#65292;&#29992;&#20110;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22823;&#37327;&#20195;&#29702;&#30340;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#26159;&#19968;&#31867;&#24102;&#26377;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;Markov&#21338;&#24328;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#33258;&#24049;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#29992;&#20110;&#34987;&#20854;&#20182;&#29609;&#23478;&#30340;&#31574;&#30053;&#36793;&#32536;&#21270;&#30340;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#23398;&#20064;Markov&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#21644;Markov&#30456;&#20851;&#22343;&#34913;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#20165;&#19982;&#27599;&#20010;&#20195;&#29702;&#33258;&#24049;&#30340;&#20989;&#25968;&#31867;&#22797;&#26434;&#24230;&#25104;&#22810;&#39033;&#24335;&#27604;&#20363;&#65292;&#20174;&#32780;&#25171;&#30772;&#20102;&#22810;&#26234;&#20307;&#30340;&#35781;&#21650;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#20989;&#25968;&#36924;&#36817;&#30340;Markov&#21338;&#24328;&#30340;&#30740;&#31350;&#65292;&#22312;&#29305;&#21270;&#20110;&#26631;&#20934;&#34920;&#26684;&#29366;&#20917;&#30340;Markov&#21338;&#24328;&#35774;&#32622;&#26102;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#20250;&#38543;&#30528;\emph{&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;}&#30340;&#22823;&#23567;&#25104;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#35813;&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;&#22312;&#20195;&#29702;&#30340;&#25968;&#37327;&#19978;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#21019;&#26032;&#65306;(1) &#21033;&#29992;&#31574;&#30053;&#37325;&#25918;&#26469;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65307;(2) &#21033;&#29992;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26469;&#33719;&#24471;&#35745;&#31639;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#32479;&#35745;&#19978;&#30340;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with a large state space and a large number of agents. This is a class of Markov games with independent linear function approximation, where each agent has its own function approximation for the state-action value functions that are marginalized by other players' policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#21442;&#25968;&#30340;&#28789;&#27963;&#20808;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#26367;&#20195;&#27169;&#22411;&#30340;&#27169;&#25311;&#24471;&#20986;&#65292;&#35813;&#26041;&#27861;&#22312;&#20272;&#35745;&#31995;&#32479;&#21457;&#32946;&#21442;&#25968;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.02522</link><description>&lt;p&gt;
&#21464;&#20998;&#36125;&#21494;&#26031;&#31995;&#32479;&#21457;&#32946;&#21442;&#25968;&#25512;&#26029;&#20013;&#30340;&#20808;&#39564;&#23494;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prior Density Learning in Variational Bayesian Phylogenetic Parameters Inference. (arXiv:2302.02522v2 [q-bio.PE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#21442;&#25968;&#30340;&#28789;&#27963;&#20808;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#26367;&#20195;&#27169;&#22411;&#30340;&#27169;&#25311;&#24471;&#20986;&#65292;&#35813;&#26041;&#27861;&#22312;&#20272;&#35745;&#31995;&#32479;&#21457;&#32946;&#21442;&#25968;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#22312;&#36125;&#21494;&#26031;&#20272;&#35745;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36335;&#24452;&#12290;&#36825;&#20123;&#36827;&#27493;&#20351;&#24471;&#21464;&#20998;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#25104;&#20026;&#36924;&#36817;&#31995;&#32479;&#21457;&#32946;&#21518;&#39564;&#30340;&#33945;&#29305;&#21345;&#32599;&#39532;&#23572;&#31185;&#22827;&#38142;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#20043;&#19968;&#26159;&#36890;&#36807;&#22266;&#23450;&#20998;&#24067;&#26469;&#24314;&#27169;&#20808;&#39564;&#65292;&#22914;&#26524;&#23427;&#20204;&#36828;&#31163;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#20559;&#20506;&#21518;&#39564;&#36924;&#36817;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21644;&#23454;&#26045;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#23427;&#20204;&#30340;&#21442;&#25968;&#65292;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#26469;&#25918;&#26494;&#20808;&#39564;&#23494;&#24230;&#30340;&#20005;&#26684;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#26367;&#20195;&#27169;&#22411;&#19979;&#30340;&#25903;&#38271;&#24230;&#21644;&#36827;&#21270;&#21442;&#25968;&#20272;&#35745;&#12290;&#25152;&#36827;&#34892;&#30340;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#20986;&#36825;&#31181;&#26041;&#27861;&#22312;&#20272;&#35745;&#25903;&#38271;&#24230;&#21644;&#36827;&#21270;&#27169;&#22411;&#21442;&#25968;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#23427;&#20204;&#36824;&#34920;&#26126;&#28789;&#27963;&#30340;&#20808;&#39564;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advances in variational inference are providing promising paths in Bayesian estimation problems. These advances make variational phylogenetic inference an alternative approach to Markov Chain Monte Carlo methods for approximating the phylogenetic posterior. However, one of the main drawbacks of such approaches is the modelling of the prior through fixed distributions, which could bias the posterior approximation if they are distant from the current data distribution. In this paper, we propose an approach and an implementation framework to relax the rigidity of the prior densities by learning their parameters using a gradient-based method and a neural network-based parameterization. We applied this approach for branch lengths and evolutionary parameters estimation under several Markov chain substitution models. The results of performed simulations show that the approach is powerful in estimating branch lengths and evolutionary model parameters. They also show that a flexible prior m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36136;&#30097;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#21367;&#31215;&#26680;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#32447;&#24615;&#32452;&#21512;&#26041;&#27861;&#65292;&#20174;&#38543;&#26426;&#21367;&#31215;&#26680;&#20013;&#21019;&#24314;&#20986;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#32593;&#32476;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11360</link><description>&lt;p&gt;
&#32447;&#24615;&#32452;&#21512;&#30340;&#23041;&#21147;&#65306;&#38543;&#26426;&#21367;&#31215;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Power of Linear Combinations: Learning with Random Convolutions. (arXiv:2301.11360v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36136;&#30097;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#21367;&#31215;&#26680;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#32447;&#24615;&#32452;&#21512;&#26041;&#27861;&#65292;&#20174;&#38543;&#26426;&#21367;&#31215;&#26680;&#20013;&#21019;&#24314;&#20986;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#32593;&#32476;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#21367;&#31215;&#26680;&#22823;&#23567;&#26469;&#20445;&#25345;&#19982;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#65288;&#22914;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#65289;&#30340;&#31454;&#20105;&#21147;&#65292;&#23548;&#33268;&#26377;&#22823;&#37327;&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#22788;&#29702;&#12290;&#26412;&#25991;&#36136;&#30097;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#21367;&#31215;&#26680;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24456;&#22810;&#24403;&#20195;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29978;&#33267;&#22312;&#19981;&#26356;&#26032;&#21021;&#22987;&#21270;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#24773;&#20917;&#19979;&#23601;&#21487;&#20197;&#36798;&#21040;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#23454;&#38469;&#19978;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#32452;&#21512;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#38543;&#26426;&#21367;&#31215;&#26680;&#32452;&#21512;&#25104;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#32593;&#32476;&#36816;&#31639;&#31526;&#65292;&#20854;&#36890;&#36807;&#39640;&#25928;&#30340; $1 \times 1$ &#21367;&#31215;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#32452;&#21512;&#21487;&#20197;&#38544;&#24335;&#22320;&#27491;&#21017;&#21270;&#32467;&#26524;&#36816;&#31639;&#31526;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#65292;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the traditional paradigm of convolutional neural networks (CNNs), modern CNNs manage to keep pace with more recent, for example transformer-based, models by not only increasing model depth and width but also the kernel size. This results in large amounts of learnable model parameters that need to be handled during training. While following the convolutional paradigm with the according spatial inductive bias, we question the significance of \emph{learned} convolution filters. In fact, our findings demonstrate that many contemporary CNN architectures can achieve high test accuracies without ever updating randomly initialized (spatial) convolution filters. Instead, simple linear combinations (implemented through efficient $1\times 1$ convolutions) suffice to effectively recombine even random filters into expressive network operators. Furthermore, these combinations of random filters can implicitly regularize the resulting operations, mitigating overfitting and enhancing overall 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#22312;&#25307;&#21215;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#26102;&#20986;&#29616;&#30340;&#25968;&#25454;&#21487;&#20449;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08563</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#24863;&#30693;&#29575;&#23398;&#20064;&#30340;CMAB&#26041;&#26696;&#65292;&#36890;&#36807;&#21487;&#20449;&#25968;&#25454;&#25910;&#38598;&#22312;&#20154;&#32676;&#20013;&#25239;&#20987;COVID-19
&lt;/p&gt;
&lt;p&gt;
A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd. (arXiv:2301.08563v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#22312;&#25307;&#21215;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#26102;&#20986;&#29616;&#30340;&#25968;&#25454;&#21487;&#20449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#65292;&#25307;&#21215;&#21487;&#20449;&#21644;&#39640;&#36136;&#37327;&#30340;&#24037;&#20316;&#32773;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20551;&#35774;&#24037;&#20316;&#32773;&#30340;&#33021;&#21147;&#26159;&#20107;&#20808;&#24050;&#30693;&#30340;&#65292;&#35201;&#20040;&#20551;&#35774;&#24179;&#21488;&#19968;&#26086;&#25509;&#25910;&#21040;&#20182;&#20204;&#25910;&#38598;&#30340;&#25968;&#25454;&#23601;&#30693;&#36947;&#20182;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#38469;&#19978;&#65292;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#25910;&#20837;&#65292;&#35768;&#22810;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#19981;&#35802;&#23454;&#22320;&#25191;&#34892;&#20854;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#21521;&#24179;&#21488;&#25253;&#21578;&#34394;&#20551;&#25968;&#25454;&#65292;&#36825;&#34987;&#31216;&#20026;&#34394;&#20551;&#25968;&#25454;&#25915;&#20987;&#12290;&#23545;&#20110;&#24179;&#21488;&#26469;&#35828;&#65292;&#35780;&#20272;&#25152;&#25910;&#21040;&#30340;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#21313;&#20998;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#21322;&#30417;&#30563;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#65288;SCMABA&#65289;&#30340;&#28608;&#21169;&#26426;&#21046;&#26469;&#35299;&#20915;MCS&#20013;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#30340;&#25307;&#32856;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24037;&#20316;&#32773;&#25307;&#21215;&#24314;&#27169;&#20026;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26469;&#20998;&#31163;&#25506;&#32034;&#21644;&#24320;&#21457;&#65292;&#23558;&#24050;&#25307;&#21215;&#30340;&#24037;&#20316;&#32773;&#30340;&#24863;&#30693;&#29575;&#35270;&#20026;&#8220;&#33218;&#8220;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recruitment of trustworthy and high-quality workers is an important research issue for MCS. Previous studies either assume that the qualities of workers are known in advance, or assume that the platform knows the qualities of workers once it receives their collected data. In reality, to reduce costs and thus maximize revenue, many strategic workers do not perform their sensing tasks honestly and report fake data to the platform, which is called False data attacks. And it is very hard for the platform to evaluate the authenticity of the received data. In this paper, an incentive mechanism named Semi-supervision based Combinatorial Multi-Armed Bandit reverse Auction (SCMABA) is proposed to solve the recruitment problem of multiple unknown and strategic workers in MCS. First, we model the worker recruitment as a multi-armed bandit reverse auction problem and design an UCB-based algorithm to separate the exploration and exploitation, regarding the Sensing Rates (SRs) of recruited worke
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#23545;&#20195;&#20215;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#65292;&#20195;&#20215;&#20989;&#25968;&#36234;&#36235;&#20110;&#38598;&#20013;&#22312;&#19968;&#20010;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#21487;&#35266;&#23519;&#37327;&#21644;&#20351;&#29992;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#30340;&#20540;&#19978;&#12290;</title><link>http://arxiv.org/abs/2301.06883</link><description>&lt;p&gt;
&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#23545;&#20110;&#37327;&#23376;&#20195;&#20215;&#20989;&#25968;&#38598;&#20013;&#24615;&#30340;&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The quantum cost function concentration dependency on the parametrization expressivity. (arXiv:2301.06883v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#23545;&#20195;&#20215;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#65292;&#20195;&#20215;&#20989;&#25968;&#36234;&#36235;&#20110;&#38598;&#20013;&#22312;&#19968;&#20010;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#21487;&#35266;&#23519;&#37327;&#21644;&#20351;&#29992;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#30340;&#20540;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25105;&#20204;&#30446;&#21069;&#22788;&#20110;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#30340;&#37327;&#23376;&#35774;&#22791;&#26102;&#20195;&#65292;&#20294;&#26159;&#20154;&#20204;&#27491;&#22312;&#36827;&#34892;&#22810;&#39033;&#30740;&#31350;&#65292;&#26088;&#22312;&#25226;&#26426;&#22120;&#23398;&#20064;&#24341;&#20837;&#21040;&#37327;&#23376;&#39046;&#22495;&#12290;&#30446;&#21069;&#65292;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#26159;&#26500;&#24314;&#36825;&#31181;&#27169;&#22411;&#30340;&#20027;&#35201;&#31574;&#30053;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#25105;&#20204;&#20173;&#19981;&#30693;&#36947;&#21019;&#24314;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#26368;&#23567;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#22914;&#20309;&#24433;&#21709;&#20195;&#20215;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#65292;&#20195;&#20215;&#20989;&#25968;&#23601;&#36234;&#20542;&#21521;&#20110;&#38598;&#20013;&#20110;&#19968;&#20010;&#20540;&#65292;&#36825;&#20010;&#20540;&#26082;&#21462;&#20915;&#20110;&#25152;&#36873;&#25321;&#30340;&#21487;&#35266;&#23519;&#37327;&#65292;&#20063;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24471;&#20986;&#20102;&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#21644;&#20195;&#20215;&#20989;&#25968;&#22343;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#21442;&#25968;&#34920;&#36798;&#33021;&#21147;&#19982;&#20195;&#20215;&#20989;&#25968;&#30340;&#26041;&#24046;&#30456;&#20851;&#32852;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#25968;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we are currently in the era of noisy intermediate scale quantum devices, several studies are being conducted with the aim of bringing machine learning to the quantum domain. Currently, quantum variational circuits are one of the main strategies used to build such models. However, despite its widespread use, we still do not know what are the minimum resources needed to create a quantum machine learning model. In this article, we analyze how the expressiveness of the parametrization affects the cost function. We analytically show that the more expressive the parametrization is, the more the cost function will tend to concentrate around a value that depends both on the chosen observable and on the number of qubits used. For this, we initially obtain a relationship between the expressiveness of the parametrization and the mean value of the cost function. Afterwards, we relate the expressivity of the parametrization with the variance of the cost function. Finally, we show some nume
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.10535</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#24212;&#29992;&#24191;&#27867;&#65292;&#21253;&#25324;&#31185;&#23398;&#12289;&#24037;&#31243;&#12289;&#37329;&#34701;&#21644;&#26085;&#24120;&#29983;&#27963;&#12290;&#21457;&#23637;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#21644;&#35777;&#26126;&#23450;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#20132;&#21449;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20869;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#65292;&#36890;&#36807;&#32467;&#21512;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01793</link><description>&lt;p&gt;
\{kappa}HGCN: &#36890;&#36807;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#23398;&#20064;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20869;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#65292;&#36890;&#36807;&#32467;&#21512;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#29366;&#32467;&#26500;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#21644;&#24130;&#24459;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#30456;&#27604;&#20110;&#24179;&#22374;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#26354;&#38754;&#21452;&#26354;&#31354;&#38388;&#25552;&#20379;&#20102;&#26356;&#26131;&#22788;&#29702;&#21644;&#23884;&#20837;&#30340;&#31354;&#38388;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23637;&#29616;&#38544;&#21547;&#26641;&#29366;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#26641;&#29366;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#23637;&#31034;&#20986;&#26641;&#29366;&#12289;&#24179;&#22374;&#21644;&#22278;&#24418;&#21306;&#22495;&#30340;&#24322;&#36136;&#32452;&#25104;&#12290;&#23558;&#36825;&#26679;&#24322;&#36136;&#30340;&#32467;&#26500;&#30452;&#25509;&#23884;&#20837;&#19968;&#20010;&#21516;&#36136;&#21270;&#30340;&#23884;&#20837;&#31354;&#38388;&#65288;&#21363;&#21452;&#26354;&#31354;&#38388;&#65289;&#24517;&#28982;&#23548;&#33268;&#37325;&#22823;&#22833;&#30495;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#32570;&#28857;&#65292;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#21452;&#26354;&#31354;&#38388;&#30340;&#26354;&#29575;&#65292;&#20197;&#23454;&#29616;&#28789;&#27963;&#20934;&#30830;&#22320;&#24314;&#27169;&#26641;&#29366;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in real-world applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvatur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#65292;&#26469;&#20248;&#21270;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#20219;&#21153;&#25928;&#29575;&#39640;&#30340;&#21516;&#26102;&#65292;&#20063;&#33021;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.15359</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20132;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#20027;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning. (arXiv:2211.15359v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#65292;&#26469;&#20248;&#21270;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#20219;&#21153;&#25928;&#29575;&#39640;&#30340;&#21516;&#26102;&#65292;&#20063;&#33021;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#30340;&#19979;&#19968;&#20010;&#27493;&#39588;&#26159;&#20174;&#26049;&#35266;&#32773;&#30340;&#35282;&#33394;&#20013;&#35299;&#33073;&#20986;&#26469;&#65292;&#21464;&#24471;&#26356;&#21152;&#20027;&#21160;&#12290;&#26126;&#30830;&#23450;&#20041;&#30340;&#20027;&#21160;&#34892;&#20026;&#21487;&#20197;&#25913;&#21892;&#20154;&#26426;&#21512;&#20316;&#65292;&#22240;&#20026;&#20195;&#29702;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#31215;&#26497;&#30340;&#35282;&#33394;&#24182;&#35299;&#38500;&#20102;&#29992;&#25143;&#30340;&#36131;&#20219;&#12290;&#28982;&#32780;&#65292;&#20027;&#21160;&#24615;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22240;&#20026;&#25191;&#34892;&#19981;&#24403;&#30340;&#39044;&#38450;&#24615;&#34892;&#21160;&#21487;&#33021;&#19981;&#20165;&#23545;&#20219;&#21153;&#32467;&#26524;&#20135;&#29983;&#30772;&#22351;&#24615;&#24433;&#21709;&#65292;&#32780;&#19988;&#36824;&#20250;&#23545;&#19982;&#29992;&#25143;&#30340;&#20851;&#31995;&#20135;&#29983;&#24433;&#21709;&#12290;&#20026;&#20102;&#35774;&#35745;&#21512;&#36866;&#30340;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#37117;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#12290;&#36825;&#37324;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20248;&#21270;&#20027;&#21160;&#34892;&#20026;&#65292;&#20351;&#20854;&#20219;&#21153;&#23548;&#21521;&#8212;&#8212;&#36825;&#24847;&#21619;&#30528;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#8212;&#8212;&#21516;&#26102;&#22312;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#26102;&#20063;&#20855;&#26377;&#31038;&#20132;&#25928;&#30410;&#12290;&#23558;&#36825;&#20004;&#20010;&#26041;&#38754;&#21253;&#21547;&#22312;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20027;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#26356;&#21152;&#25104;&#21151;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both social as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for more successful human-mach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PhAST&#26041;&#27861;&#26469;&#24555;&#36895;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#20652;&#21270;&#21058;&#26469;&#39537;&#21160;&#30005;&#21270;&#23398;&#21453;&#24212;, &#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20307;&#31995;&#32467;&#26500;, &#21487;&#20197;&#22686;&#21152;&#35745;&#31639;&#25928;&#29575;&#21644;&#31934;&#24230;</title><link>http://arxiv.org/abs/2211.12020</link><description>&lt;p&gt;
PhAST&#65306;&#29289;&#29702;&#24863;&#30693;&#12289;&#21487;&#25193;&#23637;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;GNN&#22312;&#21152;&#36895;&#20652;&#21270;&#21058;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design. (arXiv:2211.12020v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PhAST&#26041;&#27861;&#26469;&#24555;&#36895;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#20652;&#21270;&#21058;&#26469;&#39537;&#21160;&#30005;&#21270;&#23398;&#21453;&#24212;, &#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20307;&#31995;&#32467;&#26500;, &#21487;&#20197;&#22686;&#21152;&#35745;&#31639;&#25928;&#29575;&#21644;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#27668;&#20505;&#21361;&#26426;&#38656;&#35201;&#24555;&#36895;&#21521;&#20302;&#30899;&#33021;&#28304;&#36716;&#21464;&#12290;&#20652;&#21270;&#21058;&#26448;&#26009;&#22312;&#35768;&#22810;&#24037;&#19994;&#36807;&#31243;&#20013;&#30340;&#30005;&#21270;&#23398;&#21453;&#24212;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22914;&#21487;&#20877;&#29983;&#33021;&#28304;&#20648;&#23384;&#21644;&#30005;&#33655;&#21512;&#25104;&#12290;&#20026;&#20102;&#20943;&#23569;&#22312;&#36825;&#20123;&#36807;&#31243;&#20013;&#28040;&#32791;&#30340;&#33021;&#37327;&#65292;&#25105;&#20204;&#24517;&#39035;&#24555;&#36895;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#20652;&#21270;&#21058;&#26469;&#39537;&#21160;&#30005;&#21270;&#23398;&#21453;&#24212;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26377;&#28508;&#21147;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#39640;&#25928;&#22320;&#27169;&#25311;&#26448;&#26009;&#30340;&#24615;&#36136;&#65292;&#20174;&#32780;&#21152;&#36895;&#30005;&#20652;&#21270;&#21058;&#30340;&#35774;&#35745;&#12290;&#20026;&#27492;&#65292;Open Catalyst Project OC20&#25968;&#25454;&#38598;&#24050;&#32463;&#34987;&#26500;&#24314;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24050;&#32463;&#22312;OC20&#19978;&#35757;&#32451;&#30340;&#29616;&#26377;ML&#27169;&#22411;&#20173;&#28982;&#26080;&#27861;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#35201;&#27714;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#21019;&#26032;&#65292;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#22686;&#21152;&#35745;&#31639;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#22312;&#22270;&#32763;&#35793;&#23618;&#12289;&#22270;&#27880;&#24847;&#21147;&#23618;&#21644;&#27744;&#21270;&#23618;&#20013;&#25552;&#20986;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;Physical Attribute Scaling Transformer (PhAST)&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PhAST&#22312;&#29983;&#25104;&#20934;&#30830;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#20960;&#20010;&#30456;&#20851;&#24212;&#29992;&#65292;&#21253;&#25324;&#30005;&#20652;&#21270;&#21058;&#30340;&#21457;&#29616;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mitigating the climate crisis requires a rapid transition towards lower carbon energy. Catalyst materials play a crucial role in the electrochemical reactions involved in a great number of industrial processes key to this transition, such as renewable energy storage and electrofuel synthesis. To reduce the amount of energy spent on such processes, we must quickly discover more efficient catalysts to drive the electrochemical reactions. Machine learning (ML) holds the potential to efficiently model the properties of materials from large amounts of data, and thus to accelerate electrocatalyst design. The Open Catalyst Project OC20 data set was constructed to that end. However, most existing ML models trained on OC20 are still neither scalable nor accurate enough for practical applications. Here, we propose several task-specific innovations, applicable to most architectures, which increase both computational efficiency and accuracy. In particular, we propose improvements in (1) the graph 
&lt;/p&gt;</description></item><item><title>scikit-fda&#26159;&#19968;&#20010;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#24182;&#20110;scikit-learn&#20860;&#23481;&#65292;&#37319;&#29992;&#19977;&#26465;&#27454;BSD&#35768;&#21487;&#35777;&#21457;&#24067;&#65292;&#23545;FDA&#31038;&#21306;&#36129;&#29486;&#24320;&#25918;&#12290;</title><link>http://arxiv.org/abs/2211.02566</link><description>&lt;p&gt;
scikit-fda&#65306;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
scikit-fda: A Python Package for Functional Data Analysis. (arXiv:2211.02566v2 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02566
&lt;/p&gt;
&lt;p&gt;
scikit-fda&#26159;&#19968;&#20010;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#24182;&#20110;scikit-learn&#20860;&#23481;&#65292;&#37319;&#29992;&#19977;&#26465;&#27454;BSD&#35768;&#21487;&#35777;&#21457;&#24067;&#65292;&#23545;FDA&#31038;&#21306;&#36129;&#29486;&#24320;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;scikit-fda&#26159;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#65288;FDA&#65289;&#30340;Python&#36719;&#20214;&#21253;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#30340;&#34920;&#31034;&#12289;&#39044;&#22788;&#29702;&#21644;&#25506;&#32034;&#24615;&#20998;&#26512;&#12290;&#35813;&#24211;&#24314;&#31435;&#22312;Python&#31185;&#23398;&#35745;&#31639;&#29983;&#24577;&#31995;&#32479;&#20043;&#19978;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#20102;scikit-learn&#24212;&#29992;&#31243;&#24207;&#25509;&#21475;&#65292;&#20197;&#21033;&#29992;&#35813;&#36719;&#20214;&#21253;&#25552;&#20379;&#30340;&#26426;&#22120;&#23398;&#20064;&#21151;&#33021;&#65306;&#21253;&#25324;&#31649;&#36947;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#12290;&#36825;&#20010;scikit-fda&#36719;&#20214;&#21253;&#24050;&#32463;&#20197;&#19977;&#26465;&#27454;BSD&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#21457;&#24067;&#20026;&#33258;&#30001;&#21644;&#24320;&#28304;&#36719;&#20214;&#65292;&#24182;&#23545;FDA&#31038;&#21306;&#30340;&#36129;&#29486;&#25345;&#24320;&#25918;&#24577;&#24230;&#12290;&#35813;&#24211;&#30340;&#24191;&#27867;&#25991;&#26723;&#21253;&#25324;&#36880;&#27493;&#25945;&#31243;&#21644;&#35814;&#32454;&#30340;&#20351;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The library scikit-fda is a Python package for Functional Data Analysis (FDA). It provides a comprehensive set of tools for representation, preprocessing, and exploratory analysis of functional data. The library is built upon and integrated in Python's scientific ecosystem. In particular, it conforms to the scikit-learn application programming interface so as to take advantage of the functionality for machine learning provided by this package: pipelines, model selection, and hyperparameter tuning, among others. The scikit-fda package has been released as free and open-source software under a 3-Clause BSD license and is open to contributions from the FDA community. The library's extensive documentation includes step-by-step tutorials and detailed examples of use.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#35774;&#32622;&#65292;&#21033;&#29992;&#26377;&#26631;&#35760;&#30340;&#36712;&#36857;&#25968;&#25454;&#21644;&#26080;&#21160;&#20316;&#30340;&#36712;&#36857;&#25968;&#25454;&#35757;&#32451;&#21453;&#21160;&#21147;&#23398;&#27169;&#22411;&#20197;&#33719;&#21462;&#20195;&#29702;&#26631;&#31614;&#65292;&#26368;&#32456;&#20351;&#29992;&#20219;&#20309;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#23454;&#29616;&#39640;&#25104;&#21151;&#29575;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.06518</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#21160;&#20316;&#36712;&#36857;&#30340;&#21322;&#30417;&#30563;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories. (arXiv:2210.06518v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#35774;&#32622;&#65292;&#21033;&#29992;&#26377;&#26631;&#35760;&#30340;&#36712;&#36857;&#25968;&#25454;&#21644;&#26080;&#21160;&#20316;&#30340;&#36712;&#36857;&#25968;&#25454;&#35757;&#32451;&#21453;&#21160;&#21147;&#23398;&#27169;&#22411;&#20197;&#33719;&#21462;&#20195;&#29702;&#26631;&#31614;&#65292;&#26368;&#32456;&#20351;&#29992;&#20219;&#20309;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#23454;&#29616;&#39640;&#25104;&#21151;&#29575;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#26234;&#33021;&#20307;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19981;&#21516;&#22823;&#23567;&#12289;&#36136;&#37327;&#21644;&#27979;&#37327;&#31867;&#22411;&#30340;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#12289;&#23454;&#38469;&#19978;&#21463;&#21040;&#21551;&#21457;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#26469;&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36825;&#31181;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#20004;&#20010;&#36712;&#36857;&#38598;&#65306;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#29366;&#24577;&#12289;&#34892;&#20026;&#21644;&#22870;&#21169;&#19977;&#20803;&#32452;&#30340;&#26631;&#35760;&#36712;&#36857;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#20165;&#21253;&#21547;&#29366;&#24577;&#21644;&#22870;&#21169;&#20449;&#24687;&#30340;&#26410;&#26631;&#35760;&#36712;&#36857;&#38598;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20803;&#31639;&#27861;&#27969;&#27700;&#32447;&#65292;&#35813;&#31639;&#27861;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#23398;&#20064;&#21453;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20195;&#29702;&#26631;&#31614;&#65292;&#28982;&#21518;&#23558;&#20219;&#20309;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#30495;&#23454;&#21644;&#20195;&#29702;&#26631;&#35760;&#36712;&#36857;&#12290;&#32463;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#27969;&#27700;&#32447;&#38750;&#24120;&#25104;&#21151;&#8212;&#8212;&#22312;&#20960;&#20010;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#26576;&#20123;&#31163;&#32447;RL&#31639;&#27861;&#21487;&#20197;&#19982;&#22312;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#65292;&#21363;&#20351;&#21518;&#32773;&#25317;&#26377;&#26356;&#22810;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural agents can effectively learn from multiple data sources that differ in size, quality, and types of measurements. We study this heterogeneity in the context of offline reinforcement learning (RL) by introducing a new, practically motivated semi-supervised setting. Here, an agent has access to two sets of trajectories: labelled trajectories containing state, action and reward triplets at every timestep, along with unlabelled trajectories that contain only state and reward information. For this setting, we develop and study a simple meta-algorithmic pipeline that learns an inverse dynamics model on the labelled data to obtain proxy-labels for the unlabelled data, followed by the use of any offline RL algorithm on the true and proxy-labelled trajectories. Empirically, we find this simple pipeline to be highly successful -- on several D4RL benchmarks~\cite{fu2020d4rl}, certain offline RL algorithms can match the performance of variants trained on a fully labelled dataset even when w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EETG&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#36136;&#37327;-&#22810;&#26679;&#24615;&#8221;&#31639;&#27861;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#19994;&#21270;&#36816;&#21160;&#20808;&#39564;&#30693;&#35782;&#65292;&#33021;&#22815;&#24110;&#21161;&#22235;&#36275;&#26426;&#22120;&#20154;&#25104;&#21151;&#22320;&#31359;&#36234;&#21508;&#31181;&#29615;&#22659;&#65292;&#27604;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.04819</link><description>&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#22810;&#26679;&#30340;&#29615;&#22659;&#36712;&#36857;&#29983;&#25104;&#22120;&#20808;&#39564;&#30693;&#35782;&#65292;&#39640;&#25928;&#23398;&#20064;&#36816;&#21160;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors. (arXiv:2210.04819v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EETG&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#36136;&#37327;-&#22810;&#26679;&#24615;&#8221;&#31639;&#27861;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#19994;&#21270;&#36816;&#21160;&#20808;&#39564;&#30693;&#35782;&#65292;&#33021;&#22815;&#24110;&#21161;&#22235;&#36275;&#26426;&#22120;&#20154;&#25104;&#21151;&#22320;&#31359;&#36234;&#21508;&#31181;&#29615;&#22659;&#65292;&#27604;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#21508;&#31181;&#19981;&#35268;&#21017;&#22320;&#24418;&#30340;&#40065;&#26834;&#36816;&#21160;&#25511;&#21046;&#22120;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20197;&#36712;&#36857;&#29983;&#25104;&#22120;&#65288;TG&#65289;&#24418;&#24335;&#21152;&#20837;&#33391;&#22909;&#30340;&#36816;&#21160;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#36816;&#21160;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20219;&#21153;/&#29615;&#22659;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#22312;&#21333;&#20010;&#33391;&#22909;&#30340;TG&#20013;&#23450;&#20041;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#35843;&#25972;&#65292;&#21516;&#26102;&#36824;&#26377;&#21487;&#33021;&#38477;&#20302;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;EETG&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#8220;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;Quality-Diversity&#65289;&#8221;&#31639;&#27861;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#19994;&#21270;&#36816;&#21160;&#20808;&#39564;&#30693;&#35782;&#65292;&#21516;&#26102;&#22312;Policies Modulating TG&#65288;PMTG&#65289;&#26694;&#26550;&#20869;&#32500;&#25345;&#21333;&#19968;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EETG&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#25104;&#21151;&#22320;&#31359;&#36234;&#21508;&#31181;&#29615;&#22659;&#65292;&#22914;&#26012;&#22369;&#12289;&#21488;&#38454;&#12289;&#23822;&#23702;&#22320;&#24418;&#21644;&#24179;&#34913;&#27178;&#26438;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EETG&#27604;&#20351;&#29992;&#21333;&#20010;TG&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven learning based methods have recently been particularly successful at learning robust locomotion controllers for a variety of unstructured terrains. Prior work has shown that incorporating good locomotion priors in the form of trajectory generators (TGs) is effective at efficiently learning complex locomotion skills. However, defining a good, single TG as tasks/environments become increasingly more complex remains a challenging problem as it requires extensive tuning and risks reducing the effectiveness of the prior. In this paper, we present Evolved Environmental Trajectory Generators (EETG), a method that learns a diverse set of specialised locomotion priors using Quality-Diversity algorithms while maintaining a single policy within the Policies Modulating TG (PMTG) architecture. The results demonstrate that EETG enables a quadruped robot to successfully traverse a wide range of environments, such as slopes, stairs, rough terrain, and balance beams. Our experiments show th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;12&#31181;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#22312;5&#20010;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#37329;&#34701;&#26426;&#26500;&#20934;&#30830;&#24615;&#21644;&#28508;&#22312;&#30408;&#21033;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#20102;&#26368;&#25104;&#21151;&#21644;&#26368;&#19981;&#25104;&#21151;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.07912</link><description>&lt;p&gt;
&#20844;&#24179;&#20449;&#29992;&#35780;&#20998;&#30340;&#31639;&#27861;&#20915;&#31574;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic decision making methods for fair credit scoring. (arXiv:2209.07912v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;12&#31181;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#22312;5&#20010;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#37329;&#34701;&#26426;&#26500;&#20934;&#30830;&#24615;&#21644;&#28508;&#22312;&#30408;&#21033;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#20102;&#26368;&#25104;&#21151;&#21644;&#26368;&#19981;&#25104;&#21151;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35780;&#20272;&#36151;&#27454;&#30003;&#35831;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#20102;&#24456;&#38271;&#19968;&#27573;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#33258;&#21160;&#20915;&#31574;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#23545;&#26576;&#20123;&#32676;&#20307;&#25110;&#20010;&#20154;&#36827;&#34892;&#19981;&#24179;&#31561;&#23545;&#24453;&#65292;&#36827;&#32780;&#23548;&#33268;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;12&#31181;&#20027;&#35201;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#22312;5&#31181;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#19979;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#35780;&#20272;&#23427;&#20204;&#23545;&#20110;&#37329;&#34701;&#26426;&#26500;&#30340;&#20934;&#30830;&#24615;&#21644;&#28508;&#22312;&#30408;&#21033;&#33021;&#21147;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24050;&#32463;&#30830;&#23450;&#20102;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26368;&#25104;&#21151;&#21644;&#26368;&#19981;&#25104;&#21151;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#24357;&#21512;&#23454;&#39564;&#26426;&#22120;&#23398;&#20064;&#19982;&#20854;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of machine learning in evaluating the creditworthiness of loan applicants has been demonstrated for a long time. However, there is concern that the use of automated decision-making processes may result in unequal treatment of groups or individuals, potentially leading to discriminatory outcomes. This paper seeks to address this issue by evaluating the effectiveness of 12 leading bias mitigation methods across 5 different fairness metrics, as well as assessing their accuracy and potential profitability for financial institutions. Through our analysis, we have identified the challenges associated with achieving fairness while maintaining accuracy and profitabiliy, and have highlighted both the most successful and least successful mitigation methods. Ultimately, our research serves to bridge the gap between experimental machine learning and its practical applications in the finance industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;PERLA&#8221;&#30340;&#22686;&#24378;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#23558;&#26234;&#33021;&#20307;&#30340;&#32852;&#21512;&#31574;&#30053;&#37319;&#26679;&#25216;&#26415;&#24341;&#20837;&#35780;&#35770;&#23478;&#20013;&#65292;&#20943;&#23569;&#20102;&#32852;&#21512;&#21160;&#20316;&#21333;&#19968;&#26679;&#26412;&#36896;&#25104;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#20351;&#24471;CT-DE MARL&#31639;&#27861;&#30340;&#23398;&#20064;&#26356;&#21152;&#31283;&#23450;&#19988;&#26356;&#24555;&#36895;&#12290;</title><link>http://arxiv.org/abs/2209.01054</link><description>&lt;p&gt;
&#20351;&#29992;&#35780;&#20272;&#22120;&#26041;&#24046;&#38477;&#20302;&#25511;&#21046;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Taming Multi-Agent Reinforcement Learning with Estimator Variance Reduction. (arXiv:2209.01054v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;PERLA&#8221;&#30340;&#22686;&#24378;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#23558;&#26234;&#33021;&#20307;&#30340;&#32852;&#21512;&#31574;&#30053;&#37319;&#26679;&#25216;&#26415;&#24341;&#20837;&#35780;&#35770;&#23478;&#20013;&#65292;&#20943;&#23569;&#20102;&#32852;&#21512;&#21160;&#20316;&#21333;&#19968;&#26679;&#26412;&#36896;&#25104;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#20351;&#24471;CT-DE MARL&#31639;&#27861;&#30340;&#23398;&#20064;&#26356;&#21152;&#31283;&#23450;&#19988;&#26356;&#24555;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CT-DE&#65289;&#26159;&#35768;&#22810;&#39046;&#20808;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#23545;&#32852;&#21512;&#21160;&#20316;&#30340;&#21333;&#20010;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#65292;&#22240;&#27492;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#38543;&#30528;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25506;&#32034;&#21644;&#26356;&#26032;&#31574;&#30053;&#65292;&#36825;&#20123;&#21333;&#20010;&#26679;&#26412;&#21487;&#33021;&#36739;&#24046;&#22320;&#20195;&#34920;&#31995;&#32479;&#20195;&#29702;&#30340;&#23454;&#38469;&#32852;&#21512;&#31574;&#30053;&#65292;&#23548;&#33268;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#39640;&#65292;&#38459;&#30861;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#36866;&#29992;&#20110;&#20219;&#20309;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;Performance Enhancing Reinforcement Learning Apparatus&#65288;PERLA&#65289;&#22312;&#20195;&#29702;&#35757;&#32451;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#20195;&#29702;&#30340;&#32852;&#21512;&#31574;&#30053;&#37319;&#26679;&#25216;&#26415;&#21040;&#35780;&#35770;&#23478;&#20013;&#12290;&#36825;&#23558;&#23548;&#33268;TD&#26356;&#26032;&#26356;&#25509;&#36817;&#24403;&#21069;&#32852;&#21512;&#31574;&#30053;&#30340;&#30495;&#27491;&#26399;&#26395;&#20540;&#65292;&#32780;&#19981;&#26159;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#23545;&#32852;&#21512;&#21160;&#20316;&#30340;&#21333;&#20010;&#26679;&#26412;&#30340;&#20272;&#35745;&#20540;&#12290;&#36825;&#20135;&#29983;&#20102;&#26041;&#24046;&#20943;&#23569;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#20174;&#32780;&#20026;CT-DE MARL&#31639;&#27861;&#25552;&#20379;&#26356;&#31283;&#23450;&#21644;&#26356;&#24555;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralised training with decentralised execution (CT-DE) serves as the foundation of many leading multi-agent reinforcement learning (MARL) algorithms. Despite its popularity, it suffers from a critical drawback due to its reliance on learning from a single sample of the joint-action at a given state. As agents explore and update their policies during training, these single samples may poorly represent the actual joint-policy of the system of agents leading to high variance gradient estimates that hinder learning. To address this problem, we propose an enhancement tool that accommodates any actor-critic MARL method. Our framework, Performance Enhancing Reinforcement Learning Apparatus (PERLA), introduces a sampling technique of the agents' joint-policy into the critics while the agents train. This leads to TD updates that closely approximate the true expected value under the current joint-policy rather than estimates from a single sample of the joint-action at a given state. This prod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#35782;&#21035;&#20986;&#32933;&#32982;&#22899;&#24615;&#30340;&#33041;&#32593;&#32476;&#65292;&#34920;&#26126;&#32933;&#32982;&#22823;&#33041;&#30340;&#29305;&#24449;&#26159;&#20855;&#26377;&#21151;&#33021;&#38556;&#30861;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2208.14007</link><description>&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#28304;&#23450;&#20301;&#33041;&#30005;&#22270;&#29305;&#24449;&#65292;&#25214;&#21040;&#32933;&#32982;&#30151;&#30340;&#31070;&#32463;&#26631;&#24535;
&lt;/p&gt;
&lt;p&gt;
Finding neural signatures for obesity through feature selection on source-localized EEG. (arXiv:2208.14007v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#35782;&#21035;&#20986;&#32933;&#32982;&#22899;&#24615;&#30340;&#33041;&#32593;&#32476;&#65292;&#34920;&#26126;&#32933;&#32982;&#22823;&#33041;&#30340;&#29305;&#24449;&#26159;&#20855;&#26377;&#21151;&#33021;&#38556;&#30861;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32933;&#32982;&#26159;&#29616;&#20195;&#31038;&#20250;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#19982;&#26174;&#33879;&#38477;&#20302;&#30340;&#29983;&#27963;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#30446;&#21069;&#65292;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#25506;&#32034;&#32933;&#32982;&#30456;&#20851;&#30340;&#31070;&#32463;&#23398;&#35777;&#25454;&#30340;&#30740;&#31350;&#23616;&#38480;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20174;&#33041;&#30005;&#22270;&#25968;&#25454;&#27966;&#29983;&#30340; alpha &#27874;&#27573;&#21151;&#33021;&#36830;&#25509;&#29305;&#24449;&#26469;&#35782;&#21035;&#32933;&#32982;&#22899;&#24615;&#30340;&#33041;&#32593;&#32476;&#12290;&#33719;&#24471;&#20102;&#24635;&#20307;&#20998;&#31867;&#20934;&#30830;&#24230;&#20026; 0.937&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32933;&#32982;&#22823;&#33041;&#30340;&#29305;&#24449;&#26159;&#20855;&#26377;&#21151;&#33021;&#38556;&#30861;&#30340;&#32593;&#32476;&#65292;&#20854;&#20013;&#36127;&#36131;&#22788;&#29702;&#33258;&#25105;&#21442;&#29031;&#20449;&#24687;&#21644;&#29615;&#22659;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21306;&#22495;&#21463;&#25439;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obesity is a serious issue in the modern society and is often associated to significantly reduced quality of life. Current research conducted to explore obesity-related neurological evidences using electroencephalography (EEG) data are limited to traditional approaches. In this study, we developed a novel machine learning model to identify brain networks of obese females using alpha band functional connectivity features derived from EEG data. An overall classification accuracy of 0.937 is achieved. Our finding suggests that the obese brain is characterized by a dysfunctional network in which the areas that responsible for processing self-referential information and environmental context information are impaired.
&lt;/p&gt;</description></item><item><title>SNAP &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24102;&#27602;&#32032;&#29305;&#24449;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;&#65292;&#30456;&#27604;&#21516;&#31867;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#27602;&#32032;&#25915;&#20987;&#65292;&#24182;&#33719;&#24471;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2208.12348</link><description>&lt;p&gt;
SNAP: &#24102;&#27602;&#32032;&#30340;&#39640;&#25928;&#31169;&#23494;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
SNAP: Efficient Extraction of Private Properties with Poisoning. (arXiv:2208.12348v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12348
&lt;/p&gt;
&lt;p&gt;
SNAP &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24102;&#27602;&#32032;&#29305;&#24449;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;&#65292;&#30456;&#27604;&#21516;&#31867;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#27602;&#32032;&#25915;&#20987;&#65292;&#24182;&#33719;&#24471;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#25512;&#29702;&#25915;&#20987;&#20801;&#35768;&#23545;&#25163;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#36825;&#20123;&#25915;&#20987;&#23545;&#20110;&#20849;&#20139;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#20855;&#26377;&#38544;&#31169;&#24433;&#21709;&#12290;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#25512;&#29702;&#25915;&#20987;&#23384;&#22312;&#19968;&#20123;&#24050;&#26377;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#20381;&#36182;&#20110;&#25915;&#20987;&#32773;&#35757;&#32451;&#22823;&#37327;&#30340;&#38452;&#24433;&#27169;&#22411;&#65292;&#20174;&#32780;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#27602;&#32032;&#25915;&#20987;&#24182;&#26597;&#35810;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#30340;&#29305;&#24449;&#25512;&#29702;&#25915;&#20987;&#35774;&#32622;&#12290;&#22312;&#23545;&#27602;&#32032;&#19979;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#29702;&#35770;&#20998;&#26512;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#29305;&#24449;&#25512;&#29702;&#25915;&#20987;&#65292;SNAP&#65292;&#23427;&#27604; Mahloujifar et al. &#30340;&#26368;&#26032;&#27602;&#32032;&#25915;&#20987;&#29305;&#24449;&#25512;&#29702;&#25915;&#20987;&#33719;&#24471;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#27602;&#32032;&#25915;&#20987;&#12290;&#20363;&#22914;&#65292;&#22312;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#38598;&#19978;&#65292;SNAP &#25915;&#20987;&#25104;&#21151;&#29575;&#21487;&#36798;&#21040; 34&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners sharing their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed, but they all rely on the attacker training a large number of shadow models, which induces a large computational overhead.  In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model confidences under poisoning, we design an efficient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. For example, on the Census dataset, SNAP achieves 34% high
&lt;/p&gt;</description></item><item><title>Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#65292;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2208.06868</link><description>&lt;p&gt;
Frouros: &#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#28418;&#31227;&#26816;&#27979;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
Frouros: A Python library for drift detection in machine learning systems. (arXiv:2208.06868v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06868
&lt;/p&gt;
&lt;p&gt;
Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#65292;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Frouros&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#33021;&#22815;&#26816;&#27979;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#28418;&#31227;&#12290;&#23427;&#25552;&#20379;&#20102;&#20256;&#32479;&#21644;&#26368;&#36817;&#31639;&#27861;&#30340;&#32452;&#21512;&#26469;&#26816;&#27979;&#27010;&#24565;&#21644;&#25968;&#25454;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#30446;&#26631;&#26159;&#20351;&#23427;&#19982;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#65292;&#24182;&#36731;&#26494;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#35813;&#24211;&#36981;&#24490;&#19968;&#31995;&#21015;&#26368;&#20339;&#24320;&#21457;&#21644;&#25345;&#32493;&#38598;&#25104;&#23454;&#36341;&#65292;&#20197;&#30830;&#20445;&#26131;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/IFCA/frouros&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frouros is an open-source Python library capable of detecting drift in machine learning systems. It provides a combination of classical and more recent algorithms for drift detection: both concept and data drift. We have designed it with the objective of making it compatible with any machine learning framework and easily adaptable to real-world use cases. The library is developed following a set of best development and continuous integration practices to ensure ease of maintenance and extensibility. The source code is available at https://github.com/IFCA/frouros.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#23545;&#31216;&#20989;&#25968;&#27169;&#22411;&#30340;&#20998;&#31163;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26082;&#21487;&#36890;&#36807;Jastrow&#24418;&#24335;&#26377;&#25928;&#34920;&#31034;&#30340;&#21453;&#23545;&#31216;&#20989;&#25968;&#65292;&#21364;&#26080;&#27861;&#29992;Slater&#34892;&#21015;&#24335;&#36827;&#34892;&#36924;&#36817;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2208.03264</link><description>&lt;p&gt;
&#36208;&#21521;&#21453;&#23545;&#31216;&#31070;&#32463;&#36817;&#20284;&#20989;&#25968;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Towards Antisymmetric Neural Ansatz Separation. (arXiv:2208.03264v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#23545;&#31216;&#20989;&#25968;&#27169;&#22411;&#30340;&#20998;&#31163;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26082;&#21487;&#36890;&#36807;Jastrow&#24418;&#24335;&#26377;&#25928;&#34920;&#31034;&#30340;&#21453;&#23545;&#31216;&#20989;&#25968;&#65292;&#21364;&#26080;&#27861;&#29992;Slater&#34892;&#21015;&#24335;&#36827;&#34892;&#36924;&#36817;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#21453;&#23545;&#31216;&#20989;&#25968;&#27169;&#22411;&#65288;&#25110;\emph{Ans\"atze}&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#21363;&#24418;&#24335;&#20026;$f(x_{\sigma(1)},\ldots,x_{\sigma(N)}) = \text{sign}(\sigma)f(x_1,\ldots,x_N)$&#30340;&#20989;&#25968;$f$&#65292;&#20854;&#20013;$\sigma$&#26159;&#20219;&#24847;&#25490;&#21015;&#12290;&#23427;&#20204;&#20986;&#29616;&#22312;&#37327;&#23376;&#21270;&#23398;&#30340;&#32972;&#26223;&#19979;&#65292;&#26159;&#36153;&#31859;&#23376;&#31995;&#32479;&#30340;&#27874;&#20989;&#25968;&#30340;&#22522;&#26412;&#24314;&#27169;&#24037;&#20855;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#24120;&#35265;&#30340;&#21453;&#23545;&#31216;Ans\"atze&#65306;&#21033;&#29992;&#34892;&#21015;&#24335;&#20132;&#26367;&#32467;&#26500;&#30340;Slater&#34920;&#31034;&#21644;&#23558;Slater&#34892;&#21015;&#24335;&#22686;&#21152;&#19968;&#20010;&#20219;&#24847;&#23545;&#31216;&#20989;&#25968;&#30340;Jastrow&#36817;&#20284;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#22312;Jastrow&#24418;&#24335;&#19979;&#21487;&#20197;&#34987;&#26377;&#25928;&#34920;&#31034;&#30340;$N$&#32500;&#21453;&#23545;&#31216;&#20989;&#25968;&#65292;&#20294;&#21487;&#20197;&#35777;&#26126;&#38500;&#38750;&#26377;&#25351;&#25968;&#32423;&#30340;&#65288;$N^2$&#65289;&#65292;&#21542;&#21017;&#23427;&#19981;&#33021;&#29992;Slater&#34892;&#21015;&#24335;&#36827;&#34892;&#36924;&#36817;&#12290;&#36825;&#20195;&#34920;&#20102;&#36825;&#20004;&#20010;Ans\"atze&#20043;&#38388;&#30340;&#31532;&#19968;&#20010;&#26126;&#30830;&#30340;&#23450;&#37327;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study separations between two fundamental models (or \emph{Ans\"atze}) of antisymmetric functions, that is, functions $f$ of the form $f(x_{\sigma(1)}, \ldots, x_{\sigma(N)}) = \text{sign}(\sigma)f(x_1, \ldots, x_N)$, where $\sigma$ is any permutation. These arise in the context of quantum chemistry, and are the basic modeling tool for wavefunctions of Fermionic systems. Specifically, we consider two popular antisymmetric Ans\"atze: the Slater representation, which leverages the alternating structure of determinants, and the Jastrow ansatz, which augments Slater determinants with a product by an arbitrary symmetric function. We construct an antisymmetric function in $N$ dimensions that can be efficiently expressed in Jastrow form, yet provably cannot be approximated by Slater determinants unless there are exponentially (in $N^2$) many terms. This represents the first explicit quantitative separation between these two Ans\"atze.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;VL-CheckList&#65292;&#20351;&#29992;&#29289;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19971;&#31181;&#27969;&#34892;&#30340;VLP&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2207.00221</link><description>&lt;p&gt;
VL-CheckList: &#20351;&#29992;&#29289;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;VL-CheckList&#65292;&#20351;&#29992;&#29289;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19971;&#31181;&#27969;&#34892;&#30340;VLP&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#20419;&#36827;&#20102;&#35768;&#22810;&#36328;&#27169;&#24577;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#26159;&#36890;&#36807;&#27604;&#36739;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#30340;&#19979;&#28216;&#20219;&#21153;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#20379;&#24456;&#23569;&#20851;&#20110;&#27599;&#31181;VLP&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#30340;&#20449;&#24687;&#65292;&#26356;&#19981;&#29992;&#35828;&#20026;&#31038;&#21306;&#22312;&#26410;&#26469;&#22914;&#20309;&#25913;&#36827;&#31995;&#32479;&#25552;&#20379;&#35265;&#35299;&#20102;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27979;&#35797;CheckList&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VL-CheckList&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20102;&#35299;VLP&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;VLP&#27169;&#22411;&#30340;&#22270;&#20687;-&#25991;&#26412;&#33021;&#21147;&#20998;&#20026;&#19977;&#31867;&#65306;&#29289;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#36827;&#19968;&#27493;&#20998;&#35299;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#36890;&#36807;&#35813;&#26694;&#26550;&#23545;&#19971;&#31181;&#26368;&#36817;&#27969;&#34892;&#30340;VLP&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#20998;&#26512;&#12290;&#32467;&#26524;&#36890;&#36807;&#25581;&#31034;&#27604;&#36739;&#27169;&#22411;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#26469;&#30830;&#35748;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Pretraining (VLP) models have recently successfully facilitated many cross-modal downstream tasks. Most existing works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method, let alone provides insights on how the community can improve the systems in the future. Inspired by the CheckList for testing natural language processing, we exploit VL-CheckList, a novel framework to understand the capabilities of VLP models. The proposed method divides the image-texting ability of a VLP model into three categories: objects, attributes, and relations, and uses a novel taxonomy to further break down these three aspects. We conduct comprehensive studies to analyze seven recently popular VLP models via the proposed framework. Results confirm the effectiveness of the proposed method by revealing fine-grained differences among the compared mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22270;&#27744;&#21270;&#26041;&#27861;&#30340;&#24191;&#27867;&#22238;&#39038;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#24211;&#30340;&#27010;&#36848;&#65292;&#23637;&#31034;&#20102;&#36825;&#20010;&#20805;&#28385;&#24076;&#26395;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#29616;&#26377;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2204.07321</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#27744;&#21270;&#65306;&#36827;&#23637;&#65292;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities. (arXiv:2204.07321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22270;&#27744;&#21270;&#26041;&#27861;&#30340;&#24191;&#27867;&#22238;&#39038;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#24211;&#30340;&#27010;&#36848;&#65292;&#23637;&#31034;&#20102;&#36825;&#20010;&#20805;&#28385;&#24076;&#26395;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#29616;&#26377;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#35768;&#22810;&#22270;&#32423;&#20219;&#21153;&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#20363;&#22914;&#22270;&#20998;&#31867;&#21644;&#22270;&#29983;&#25104;&#12290;&#20316;&#20026;&#26550;&#26500;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22270;&#27744;&#21270;&#23545;&#20110;&#33719;&#24471;&#25972;&#20307;&#30340;&#22270;&#32423;&#34920;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#20805;&#28385;&#24076;&#26395;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24456;&#23569;&#26377;&#20154;&#21162;&#21147;&#31995;&#32479;&#22320;&#24635;&#32467;&#36825;&#20123;&#24037;&#20316;&#12290;&#20026;&#20102;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#22880;&#23450;&#22522;&#30784;&#65292;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#25552;&#20379;&#26368;&#36817;&#20851;&#20110;&#22270;&#27744;&#21270;&#30340;&#26041;&#27861;&#30340;&#24191;&#27867;&#22238;&#39038;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1&#65289;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#29616;&#26377;&#22270;&#27744;&#21270;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#20102;&#25968;&#23398;&#27010;&#36848;&#65307;2&#65289;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#22270;&#27744;&#21270;&#30456;&#20851;&#30340;&#24211;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#19979;&#28216;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24320;&#28304;&#23454;&#29616;&#65307;3&#65289;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#20010;&#32508;&#36848;&#21487;&#20197;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have emerged as a leading architecture for many graph-level tasks, such as graph classification and graph generation. As an essential component of the architecture, graph pooling is indispensable for obtaining a holistic graph-level representation of the whole graph. Although a great variety of methods have been proposed in this promising and fast-developing research field, to the best of our knowledge, little effort has been made to systematically summarize these works. To set the stage for the development of future works, in this paper, we attempt to fill this gap by providing a broad review of recent methods for graph pooling. Specifically, 1) we first propose a taxonomy of existing graph pooling methods with a mathematical summary for each category; 2) then, we provide an overview of the libraries related to graph pooling, including the commonly used datasets, model architectures for downstream tasks, and open-source implementations; 3) next, we further outlin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30740;&#31350;&#20102;&#19968;&#32500;&#27178;&#22330;&#37327;&#23376;&#20234;&#36763;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#32570;&#38519;&#65292;&#21457;&#29616;&#28608;&#21457;&#33021;&#19982;&#32570;&#38519;&#25968;&#37327;&#25104;&#27604;&#20363;&#20851;&#31995;&#65292;&#24182;&#24314;&#31435;&#20102;&#32570;&#38519;&#25968;&#37327;&#30340;&#21069;&#19977;&#20010;&#32047;&#31215;&#37327;&#20043;&#38388;&#30340;&#26222;&#36866;&#24130;&#24459;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2204.06769</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#37327;&#23376;&#30456;&#21464;&#20013;&#30340;&#25299;&#25169;&#32570;&#38519;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning topological defects formation with neural networks in a quantum phase transition. (arXiv:2204.06769v2 [cond-mat.dis-nn] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30740;&#31350;&#20102;&#19968;&#32500;&#27178;&#22330;&#37327;&#23376;&#20234;&#36763;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#32570;&#38519;&#65292;&#21457;&#29616;&#28608;&#21457;&#33021;&#19982;&#32570;&#38519;&#25968;&#37327;&#25104;&#27604;&#20363;&#20851;&#31995;&#65292;&#24182;&#24314;&#31435;&#20102;&#32570;&#38519;&#25968;&#37327;&#30340;&#21069;&#19977;&#20010;&#32047;&#31215;&#37327;&#20043;&#38388;&#30340;&#26222;&#36866;&#24130;&#24459;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#27714;&#35299;&#22797;&#26434;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#26041;&#38754;&#25317;&#26377;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#38750;&#24179;&#34913;&#36807;&#31243;&#26102;&#65292;&#21253;&#25324;&#37327;&#23376;&#30456;&#21464;&#20013;&#30340;&#20851;&#38190;&#21160;&#21147;&#23398;&#65292;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30740;&#31350;&#20102;&#19968;&#32500;&#27178;&#22330;&#37327;&#23376;&#20234;&#36763;&#27169;&#22411;&#20013;&#30340;&#25299;&#25169;&#32570;&#38519;&#30340;&#26102;&#38388;&#28436;&#21270;&#12289;&#26222;&#36866;&#32479;&#35745;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#31995;&#32479;&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28608;&#21457;&#33021;&#28385;&#36275;&#24130;&#24459;&#20851;&#31995;&#65292;&#34920;&#26126;&#28608;&#21457;&#33021;&#19982;&#32570;&#38519;&#25968;&#37327;&#25104;&#27604;&#20363;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#32570;&#38519;&#25968;&#37327;&#30340;&#21069;&#19977;&#20010;&#32047;&#31215;&#37327;&#20043;&#38388;&#30340;&#26222;&#36866;&#24130;&#24459;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks possess formidable representational power, rendering them invaluable in solving complex quantum many-body systems. While they excel at analyzing static solutions, nonequilibrium processes, including critical dynamics during a quantum phase transition, pose a greater challenge for neural networks. To address this, we utilize neural networks and machine learning algorithms to investigate the time evolutions, universal statistics, and correlations of topological defects in a one-dimensional transverse-field quantum Ising model. Specifically, our analysis involves computing the energy of the system during a quantum phase transition following a linear quench of the transverse magnetic field strength. The excitation energies satisfy a power-law relation to the quench rate, indicating a proportional relationship between the excitation energy and the kink numbers. Moreover, we establish a universal power-law relationship between the first three cumulants of the kink numbers and
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23458;&#35266;&#21021;&#27493;&#23450;&#20041;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#26041;&#27861;&#30340;&#34892;&#20026;&#24341;&#36215;&#30340;&#38169;&#35823;&#35299;&#35835;&#12290;</title><link>http://arxiv.org/abs/2111.07473</link><description>&lt;p&gt;
&#21033;&#29992;&#25233;&#21046;&#21464;&#37327;&#30340;&#32447;&#24615;&#22522;&#30784;&#25968;&#25454;&#23457;&#26597;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)
&lt;/p&gt;
&lt;p&gt;
Scrutinizing XAI using linear ground-truth data with suppressor variables. (arXiv:2111.07473v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07473
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23458;&#35266;&#21021;&#27493;&#23450;&#20041;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#26041;&#27861;&#30340;&#34892;&#20026;&#24341;&#36215;&#30340;&#38169;&#35823;&#35299;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#36234;&#26469;&#36234;&#24120;&#29992;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;ML&#27169;&#22411;(&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;)&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#40657;&#21283;&#23376;&#65292;&#22240;&#27492;&#24050;&#32463;&#24320;&#21457;&#20986;&#22823;&#37327;&#31243;&#24207;&#26469;&#38416;&#26126;&#20854;&#20869;&#37096;&#36816;&#20316;&#21644;&#39044;&#27979;&#26041;&#24335;&#65292;&#23450;&#20041;&#20102;"&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;"(XAI)&#39046;&#22495;&#12290;&#26174;&#33879;&#24615;&#26041;&#27861;&#26681;&#25454;&#26576;&#31181;"&#37325;&#35201;&#24615;"&#24230;&#37327;&#23545;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#25490;&#24207;&#12290;&#30001;&#20110;&#36804;&#20170;&#20026;&#27490;&#32570;&#20047;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#38590;&#39564;&#35777;&#12290;&#24050;&#32463;&#35777;&#23454;&#65292;&#19968;&#20123;&#26174;&#33879;&#24615;&#26041;&#27861;&#21487;&#20197;&#31361;&#20986;&#26174;&#31034;&#19982;&#39044;&#27979;&#30446;&#26631;&#27809;&#26377;&#32479;&#35745;&#32852;&#31995;&#30340;&#29305;&#24449;(&#25233;&#21046;&#21464;&#37327;)&#12290;&#20026;&#20102;&#36991;&#20813;&#30001;&#20110;&#36825;&#31181;&#34892;&#20026;&#24341;&#36215;&#30340;&#38169;&#35823;&#35299;&#35835;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30830;&#20445;&#27492;&#31867;&#32852;&#31995;&#23384;&#22312;&#26159;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#23458;&#35266;&#21021;&#27493;&#23450;&#20041;&#12290;&#25105;&#20204;&#31934;&#24515;&#21046;&#20316;&#20102;&#19968;&#20010;&#22522;&#30784;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25152;&#26377;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#37117;&#26159;&#26126;&#30830;&#23450;&#20041;&#30340;&#21644;&#32447;&#24615;&#30340;&#65292;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is increasingly often used to inform high-stakes decisions. As complex ML models (e.g., deep neural networks) are often considered black boxes, a wealth of procedures has been developed to shed light on their inner workings and the ways in which their predictions come about, defining the field of 'explainable AI' (XAI). Saliency methods rank input features according to some measure of 'importance'. Such methods are difficult to validate since a formal definition of feature importance is, thus far, lacking. It has been demonstrated that some saliency methods can highlight features that have no statistical association with the prediction target (suppressor variables). To avoid misinterpretations due to such behavior, we propose the actual presence of such an association as a necessary condition and objective preliminary definition for feature importance. We carefully crafted a ground-truth dataset in which all statistical dependencies are well-defined and linear, se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24809;&#32602;&#39033;&#30340;&#20998;&#24067;&#24335;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#24314;&#31435;&#32479;&#35745;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#26368;&#20248;&#35299;&#22312;$\ell_2$-&#25439;&#22833;&#20013;&#20197;&#25509;&#36817;&#26368;&#20248;&#26368;&#23567;&#29575;&#33719;&#24471;&#12290;</title><link>http://arxiv.org/abs/2111.06530</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#39033;&#30340;&#20998;&#24067;&#24335;&#31232;&#30095;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Distributed Sparse Regression via Penalization. (arXiv:2111.06530v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24809;&#32602;&#39033;&#30340;&#20998;&#24067;&#24335;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#24314;&#31435;&#32479;&#35745;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#26368;&#20248;&#35299;&#22312;$\ell_2$-&#25439;&#22833;&#20013;&#20197;&#25509;&#36817;&#26368;&#20248;&#26368;&#23567;&#29575;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27169;&#25311;&#20026;&#26080;&#20013;&#24515;&#33410;&#28857;&#30340;&#26080;&#21521;&#22270;&#30340;&#20195;&#29702;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#12290;&#20272;&#35745;&#38382;&#39064;&#34987;&#21046;&#23450;&#20026;&#23558;&#23616;&#37096;LASSO&#25439;&#22833;&#20989;&#25968;&#30340;&#24635;&#21644;&#19982;&#20849;&#35782;&#32422;&#26463;&#30340;&#20108;&#27425;&#24809;&#32602;&#30456;&#32467;&#21512;&#30340;&#26368;&#23567;&#21270;--&#21518;&#32773;&#23545;&#20110;&#33719;&#24471;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#24809;&#32602;&#39033;&#30340;&#20849;&#35782;&#26041;&#27861;&#22312;&#20248;&#21270;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#23427;&#20204;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#20445;&#35777;&#36824;&#19981;&#28165;&#26970;&#12290; &#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#36825;&#20010;&#24320;&#25918;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#65306;&#22312;&#21512;&#36866;&#30340;&#24809;&#32602;&#21442;&#25968;&#36873;&#25321;&#19979;&#65292;&#24809;&#32602;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#22312;$\ell_2$- &#25439;&#22833;&#20013;&#20197;&#25509;&#36817;&#26368;&#20248;&#26368;&#23567;&#29575; $\mathcal{O}(s \log d/N)$ &#33719;&#24471; &#65292;&#20854;&#20013; $s$&#26159;&#31232;&#30095;&#20540;&#65292;$d$&#26159;&#29615;&#22659;&#32500;&#25968;&#65292;$N$&#26159;&#32593;&#32476;&#20013;&#30340;&#24635;&#26679;&#26412;&#22823;&#23567;--&#36825;&#19982;&#38598;&#20013;&#24335;&#30340;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sparse linear regression over a network of agents, modeled as an undirected graph (with no centralized node). The estimation problem is formulated as the minimization of the sum of the local LASSO loss functions plus a quadratic penalty of the consensus constraint -- the latter being instrumental to obtain distributed solution methods. While penalty-based consensus methods have been extensively studied in the optimization literature, their statistical and computational guarantees in the high dimensional setting remain unclear. This work provides an answer to this open problem. Our contribution is two-fold. First, we establish statistical consistency of the estimator: under a suitable choice of the penalty parameter, the optimal solution of the penalized problem achieves near optimal minimax rate $\mathcal{O}(s \log d/N)$ in $\ell_2$-loss, where $s$ is the sparsity value, $d$ is the ambient dimension, and $N$ is the total sample size in the network -- this matches centralized s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;CounterNet&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#21644;&#30456;&#24212;&#30340;&#22240;&#26524;&#35299;&#37322;&#29983;&#25104;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;CounterNet&#36890;&#36807;&#19968;&#36215;&#35757;&#32451;ML&#27169;&#22411;&#21644;&#29983;&#25104;CF&#35299;&#37322;&#26469;&#25552;&#20379;&#39044;&#27979;&#24863;&#30693;&#30340;CF&#35299;&#37322;&#65292;&#24182;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2109.07557</link><description>&lt;p&gt;
CounterNet&#65306;&#20855;&#26377;&#39044;&#27979;&#24863;&#30693;&#22240;&#26524;&#35299;&#37322;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations. (arXiv:2109.07557v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;CounterNet&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#21644;&#30456;&#24212;&#30340;&#22240;&#26524;&#35299;&#37322;&#29983;&#25104;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;CounterNet&#36890;&#36807;&#19968;&#36215;&#35757;&#32451;ML&#27169;&#22411;&#21644;&#29983;&#25104;CF&#35299;&#37322;&#26469;&#25552;&#20379;&#39044;&#27979;&#24863;&#30693;&#30340;CF&#35299;&#37322;&#65292;&#24182;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;CounterNet&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#21644;&#30456;&#24212;&#30340;&#22240;&#26524;&#35299;&#37322;&#29983;&#25104;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#22240;&#26524;&#35299;&#37322;&#25552;&#20379;&#20102;&#23545;&#29031;&#26696;&#20363;&#65292;&#21363;&#35797;&#22270;&#25214;&#21040;&#26368;&#23567;&#30340;&#20462;&#25913;&#29305;&#24449;&#20540;&#30340;&#23454;&#20363;&#65292;&#23558;ML&#27169;&#22411;&#23545;&#35813;&#23454;&#20363;&#30340;&#39044;&#27979;&#25913;&#21464;&#20026;&#39044;&#23450;&#20041;&#36755;&#20986;&#12290;&#20043;&#21069;&#29983;&#25104;&#22240;&#26524;&#35299;&#37322;&#30340;&#25216;&#26415;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#23427;&#20204;&#37117;&#26159;&#38024;&#23545;&#19987;&#26377;ML&#27169;&#22411;&#30340;&#21518;&#32493;&#26041;&#27861;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#29983;&#25104;&#36807;&#31243;&#32570;&#20047;&#23545;ML&#27169;&#22411;&#30340;&#35757;&#32451;&#30340;&#21551;&#31034;&#65292;&#36825;&#23548;&#33268;&#20102;&#27169;&#22411;&#39044;&#27979;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#38169;&#20301;&#65307;&#22823;&#22810;&#25968;&#25216;&#26415;&#20381;&#36182;&#20110;&#35299;&#20915;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#25968;&#25454;&#28857;&#30340;&#20998;&#31163;&#26102;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;CF&#35299;&#37322;(&#36825;&#23545;&#20182;&#20204;&#30340;&#36816;&#34892;&#26102;&#38388;&#26377;&#36127;&#38754;&#24433;&#21709;)&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31471;&#21040;&#31471;&#26694;&#26550;CounterNet&#20570;&#20986;&#20102;&#26032;&#36129;&#29486;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#19982;&#29983;&#25104;CF&#35299;&#37322;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#19968;&#36215;&#35757;&#32451;ML&#27169;&#22411;&#21644;&#29983;&#25104;CF&#35299;&#37322;&#65292;CounterNet&#35299;&#20915;&#20102;&#20043;&#21069;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#21644;&#35299;&#37322;&#26356;&#22909;&#23545;&#40784;&#30340;&#39044;&#27979;&#24863;&#30693;&#30340;CF&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;CounterNet&#36991;&#20813;&#20102;&#23545;&#27599;&#20010;&#36755;&#20837;&#25968;&#25454;&#28857;&#35299;&#20915;&#20998;&#31163;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents CounterNet, a novel end-to-end learning framework which integrates Machine Learning (ML) model training and the generation of corresponding counterfactual (CF) explanations into a single end-to-end pipeline. Counterfactual explanations offer a contrastive case, i.e., they attempt to find the smallest modification to the feature values of an instance that changes the prediction of the ML model on that instance to a predefined output. Prior techniques for generating CF explanations suffer from two major limitations: (i) all of them are post-hoc methods designed for use with proprietary ML models -- as a result, their procedure for generating CF explanations is uninformed by the training of the ML model, which leads to misalignment between model predictions and explanations; and (ii) most of them rely on solving separate time-intensive optimization problems to find CF explanations for each input data point (which negatively impacts their runtime). This work makes a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#38544;&#31169;&#25193;&#22823;&#32467;&#26524;&#30340;&#30740;&#31350;&#65292;&#23545;&#26356;&#20026;&#22797;&#26434;&#12289;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25277;&#26679;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#26696;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#25439;&#23475;&#65307;&#25991;&#31456;&#20998;&#26512;&#20102;&#32858;&#31867;&#25277;&#26679;&#21644;&#20998;&#23618;&#25277;&#26679;&#33539;&#20363;&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2007.12674</link><description>&lt;p&gt;
&#25511;&#21046;&#25277;&#26679;&#26041;&#26696;&#20013;&#30340;&#38544;&#31169;&#25439;&#22833;: &#20998;&#23618;&#25277;&#26679;&#21644;&#32858;&#31867;&#25277;&#26679;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling Privacy Loss in Sampling Schemes: an Analysis of Stratified and Cluster Sampling. (arXiv:2007.12674v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.12674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#38544;&#31169;&#25193;&#22823;&#32467;&#26524;&#30340;&#30740;&#31350;&#65292;&#23545;&#26356;&#20026;&#22797;&#26434;&#12289;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25277;&#26679;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#26696;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#25439;&#23475;&#65307;&#25991;&#31456;&#20998;&#26512;&#20102;&#32858;&#31867;&#25277;&#26679;&#21644;&#20998;&#23618;&#25277;&#26679;&#33539;&#20363;&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#26679;&#26041;&#26696;&#26159;&#32479;&#35745;&#23398;&#12289;&#35843;&#26597;&#35774;&#35745;&#21644;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#22312;&#24046;&#20998;&#38544;&#31169;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26524;&#26159;&#65292;&#23545;&#20110;&#19968;&#20010;&#20154;&#32676;&#30340;&#31616;&#21333;&#38543;&#26426;&#26679;&#26412;&#36816;&#34892;&#30340;&#24046;&#24322;&#31169;&#23494;&#26426;&#21046;&#27604;&#23545;&#25972;&#20010;&#20154;&#32676;&#36816;&#34892;&#30456;&#21516;&#31639;&#27861;&#25552;&#20379;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25277;&#26679;&#35774;&#35745;&#24448;&#24448;&#27604;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#25152;&#28041;&#21450;&#21040;&#30340;&#31616;&#21333;&#30340;&#29420;&#31435;&#20110;&#25968;&#25454;&#30340;&#25277;&#26679;&#26041;&#26696;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38544;&#31169;&#25193;&#22823;&#32467;&#26524;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25277;&#26679;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20123;&#25277;&#26679;&#26041;&#26696;&#19981;&#20165;&#36890;&#24120;&#26080;&#27861;&#25193;&#22823;&#38544;&#31169;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#25439;&#23475;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27969;&#34892;&#30340;&#32858;&#31867;&#25277;&#26679;&#21644;&#20998;&#23618;&#25277;&#26679;&#33539;&#20363;&#30340;&#38544;&#31169;&#24433;&#21709;&#65292;&#21516;&#26102;&#20026;&#26356;&#26222;&#36941;&#30340;&#25277;&#26679;&#35774;&#35745;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling schemes are fundamental tools in statistics, survey design, and algorithm design. A fundamental result in differential privacy is that a differentially private mechanism run on a simple random sample of a population provides stronger privacy guarantees than the same algorithm run on the entire population. However, in practice, sampling designs are often more complex than the simple, data-independent sampling schemes that are addressed in prior work. In this work, we extend the study of privacy amplification results to more complex, data-dependent sampling schemes. We find that not only do these sampling schemes often fail to amplify privacy, they can actually result in privacy degradation. We analyze the privacy implications of the pervasive cluster sampling and stratified sampling paradigms, as well as provide some insight into the study of more general sampling designs.
&lt;/p&gt;</description></item></channel></rss>