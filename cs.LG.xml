<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#35805;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#25913;&#21892;&#20302;&#36136;&#37327;&#35821;&#38899;&#35782;&#21035;&#32467;&#26524;&#23548;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#24212;&#29992;&#30340;&#22833;&#36133;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#23558;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#36798;&#21040;19.2%&#12290;</title><link>http://arxiv.org/abs/2401.02417</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20316;&#20026;&#33258;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#20652;&#21270;&#21058;
&lt;/p&gt;
&lt;p&gt;
Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition. (arXiv:2401.02417v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#35805;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#25913;&#21892;&#20302;&#36136;&#37327;&#35821;&#38899;&#35782;&#21035;&#32467;&#26524;&#23548;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#24212;&#29992;&#30340;&#22833;&#36133;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#23558;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#36798;&#21040;19.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#35789;&#38169;&#35823;&#29575;&#19981;&#26029;&#19979;&#38477;&#65292;&#20294;&#22522;&#20110;ASR&#31995;&#32479;&#26500;&#24314;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#24212;&#29992;&#20173;&#28982;&#24402;&#22240;&#20110;&#20302;&#36136;&#37327;&#30340;&#35821;&#38899;&#35782;&#21035;&#32467;&#26524;&#23548;&#33268;&#30340;&#22823;&#37327;&#22833;&#36133;&#12290;&#29616;&#26377;&#30340;&#21161;&#25163;&#31995;&#32479;&#25910;&#38598;&#20102;&#22823;&#37327;&#36825;&#20123;&#22833;&#36133;&#30340;&#20132;&#20114;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#20174;&#36825;&#20123;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#21363;&#20351;&#26159;&#31163;&#32447;&#23398;&#20064;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CLC&#65306;&#23545;&#35805;&#23545;&#27604;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#24494;&#35843;&#65292;&#21033;&#29992;&#22833;&#36133;&#23545;&#35805;&#20013;&#23481;&#26131;&#26816;&#27979;&#21040;&#30340;&#20154;&#24037;&#30165;&#36857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;CLC&#31995;&#21015;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;ASR&#27169;&#22411;&#22312;OD3&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#30340;&#22823;&#35268;&#27169;&#21322;&#21512;&#25104;&#20803;&#25968;&#25454;&#38598;&#65292;&#23545;&#35805;&#26159;&#20197;&#20219;&#21153;&#20026;&#23548;&#21521;&#30340;&#65292;&#25552;&#21319;&#24133;&#24230;&#39640;&#36798;19.2%&#12290;&#36825;&#20123;&#22686;&#30410;&#20063;&#21487;&#20197;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CLC&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While word error rates of automatic speech recognition (ASR) systems have consistently fallen, natural language understanding (NLU) applications built on top of ASR systems still attribute significant numbers of failures to low-quality speech recognition results. Existing assistant systems collect large numbers of these unsuccessful interactions, but these systems usually fail to learn from these interactions, even in an offline fashion. In this work, we introduce CLC: Contrastive Learning for Conversations, a family of methods for contrastive fine-tuning of models in a self-supervised fashion, making use of easily detectable artifacts in unsuccessful conversations with assistants. We demonstrate that our CLC family of approaches can improve the performance of ASR models on OD3, a new public large-scale semi-synthetic meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains transfer to real-world systems as well, where we show that CLC can help to improve performance 
&lt;/p&gt;</description></item><item><title>ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.02416</link><description>&lt;p&gt;
ODIN: &#19968;&#20010;&#29992;&#20110;2D&#21644;3D&#24863;&#30693;&#30340;&#21333;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02416
&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#20808;&#36827;&#27169;&#22411;&#22312;&#20687;ScanNet&#36825;&#26679;&#30340;&#24403;&#20195;3D&#24863;&#30693;&#22522;&#20934;&#19978;&#20351;&#29992;&#24182;&#26631;&#35760;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;3D&#28857;&#20113;&#65292;&#35813;&#28857;&#20113;&#26159;&#36890;&#36807;&#23545;&#24863;&#30693;&#21040;&#30340;&#22810;&#35270;&#35282;RGB-D&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#33719;&#24471;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#25918;&#24323;&#20102;&#22823;&#35268;&#27169;&#30340;2D&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32988;&#36807;&#23558;&#23039;&#24577;RGB-D&#22810;&#35270;&#35282;&#22270;&#20687;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28040;&#32791;&#23039;&#24577;&#22270;&#20687;&#21644;&#21518;&#22788;&#29702;&#30340;3D&#28857;&#20113;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21152;&#21095;&#20102;2D&#21644;3D&#24863;&#30693;&#38656;&#35201;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#36825;&#20010;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;ODIN&#65288;Omni-Dimensional INstance segmentation&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20132;&#26367;&#30340;2D&#35270;&#22270;&#20869;&#21644;3D&#35270;&#22270;&#38388;&#20449;&#24687;&#34701;&#21512;&#26469;&#21306;&#20998;2D&#21644;3D&#29305;&#24449;&#25805;&#20316;&#65292;&#21033;&#29992;&#28041;&#21450;&#30340;&#20196;&#29260;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;2D&#34917;&#19969;&#20196;&#29260;&#21644;3D&#22352;&#26631;&#30340;&#20687;&#32032;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#21270;&#30340;&#20998;&#20301;&#25968;&#26469;&#20272;&#35745;&#21518;&#39564;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#23450;&#20041;&#36125;&#21494;&#26031;&#21487;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#35780;&#20272;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#21487;&#20197;&#38598;&#25104;&#21518;&#22788;&#29702;&#25193;&#23637;&#27493;&#39588;&#20197;&#20445;&#35777;&#21518;&#39564;&#20272;&#35745;&#30340;&#26080;&#20559;&#24615;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.02413</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation-Based Inference with Quantile Regression. (arXiv:2401.02413v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02413
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#21270;&#30340;&#20998;&#20301;&#25968;&#26469;&#20272;&#35745;&#21518;&#39564;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#23450;&#20041;&#36125;&#21494;&#26031;&#21487;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#35780;&#20272;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#21487;&#20197;&#38598;&#25104;&#21518;&#22788;&#29702;&#25193;&#23637;&#27493;&#39588;&#20197;&#20445;&#35777;&#21518;&#39564;&#20272;&#35745;&#30340;&#26080;&#20559;&#24615;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#27169;&#25311;&#25512;&#26029;&#65288;Simulation-Based Inference&#65292;SBI&#65289;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#20998;&#20301;&#25968;&#20272;&#35745;&#65288;Neural Quantile Estimation&#65292;NQE&#65289;&#12290;NQE&#36890;&#36807;&#33258;&#22238;&#24402;&#26041;&#24335;&#23398;&#20064;&#27599;&#20010;&#21518;&#39564;&#32500;&#24230;&#30340;&#21333;&#19968;&#32500;&#24230;&#20998;&#20301;&#25968;&#65292;&#20197;&#25968;&#25454;&#21644;&#20043;&#21069;&#30340;&#21518;&#39564;&#32500;&#24230;&#20026;&#26465;&#20214;&#12290;&#21518;&#39564;&#26679;&#26412;&#36890;&#36807;&#20351;&#29992;&#21333;&#35843;&#19977;&#27425;&#22467;&#23572;&#31859;&#29305;&#26679;&#26465;&#25554;&#20540;&#39044;&#27979;&#20998;&#20301;&#25968;&#36827;&#34892;&#33719;&#21462;&#65292;&#24182;&#23545;&#23614;&#37096;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#36827;&#34892;&#20102;&#29305;&#27530;&#22788;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#23616;&#37096;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#65288;CDF&#65289;&#30340;&#36125;&#21494;&#26031;&#21487;&#20449;&#21306;&#38388;&#30340;&#26367;&#20195;&#23450;&#20041;&#65292;&#20854;&#35780;&#20272;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#26368;&#39640;&#21518;&#39564;&#23494;&#24230;&#21306;&#22495;&#65288;HPDR&#65289;&#24555;&#24471;&#22810;&#12290;&#22312;&#27169;&#25311;&#39044;&#31639;&#26377;&#38480;&#21644;/&#25110;&#24050;&#30693;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23558;&#21518;&#22788;&#29702;&#25193;&#23637;&#27493;&#39588;&#38598;&#25104;&#21040;NQE&#20013;&#65292;&#20197;&#30830;&#20445;&#21518;&#39564;&#20272;&#35745;&#30340;&#26080;&#20559;&#24615;&#65292;&#19988;&#38468;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;NQE&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neural Quantile Estimation (NQE), a novel Simulation-Based Inference (SBI) method based on conditional quantile regression. NQE autoregressively learns individual one dimensional quantiles for each posterior dimension, conditioned on the data and previous posterior dimensions. Posterior samples are obtained by interpolating the predicted quantiles using monotonic cubic Hermite spline, with specific treatment for the tail behavior and multi-modal distributions. We introduce an alternative definition for the Bayesian credible region using the local Cumulative Density Function (CDF), offering substantially faster evaluation than the traditional Highest Posterior Density Region (HPDR). In case of limited simulation budget and/or known model misspecification, a post-processing broadening step can be integrated into NQE to ensure the unbiasedness of the posterior estimation with negligible additional computational cost. We demonstrate that the proposed NQE method achieves state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CALM&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#26356;&#20855;&#20307;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#21487;&#20197;&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;&#27169;&#22411;&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#24182;&#19988;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02412</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#30340;LLMs&#65306;&#36890;&#36807;&#32452;&#21512;&#25193;&#23637;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
LLM Augmented LLMs: Expanding Capabilities through Composition. (arXiv:2401.02412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CALM&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#26356;&#20855;&#20307;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#21487;&#20197;&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;&#27169;&#22411;&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#24182;&#19988;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#38750;&#24179;&#20961;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#25972;&#20307;&#32467;&#26500;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#22686;&#24378;&#25110;&#36171;&#20104;&#26032;&#30340;&#25216;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20854;&#36866;&#24212;&#33021;&#21147;&#65292;&#27491;&#22312;&#35757;&#32451;&#22810;&#20010;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#27169;&#22411;&#23454;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#19982;&#26356;&#20855;&#20307;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#23454;&#29992;&#30340;&#32452;&#21512;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;&#21151;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CALM -&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#27169;&#22411;-&#65292;&#23427;&#24341;&#20837;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#20197;&#32452;&#21512;&#23427;&#20204;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#30340;&#26174;&#33879;&#29305;&#28857;&#21253;&#25324;&#65306;(i)&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;LLMs&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;LLMs&#30340;&#35268;&#27169;&#65292;(ii)&#20445;&#25345;&#29616;&#26377;&#27169;&#22411;&#26435;&#37325;&#19981;&#21464;&#65292;&#20174;&#32780;&#20445;&#30041;&#29616;&#26377;&#21151;&#33021;&#65292;(iii)&#24212;&#29992;&#26032;&#21151;&#33021;&#21482;&#38656;&#35201;&#23545;&#22686;&#21152;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Appli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#20307;&#31215;&#28210;&#26579;&#25193;&#23637;&#21040;&#26412;&#22320;2D&#22270;&#20687;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;3D GANs&#26080;&#27861;&#35299;&#20915;2D&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;3D&#20960;&#20309;&#22270;&#24418;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02411</link><description>&lt;p&gt;
&#20320;&#25152;&#35265;&#21363;&#25152;GAN: &#22312;3D GAN&#20013;&#20026;&#39640;&#20445;&#30495;&#24230;&#20960;&#20309;&#22270;&#24418;&#28210;&#26579;&#27599;&#20010;&#20687;&#32032;
&lt;/p&gt;
&lt;p&gt;
What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs. (arXiv:2401.02411v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#20307;&#31215;&#28210;&#26579;&#25193;&#23637;&#21040;&#26412;&#22320;2D&#22270;&#20687;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;3D GANs&#26080;&#27861;&#35299;&#20915;2D&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;3D&#20960;&#20309;&#22270;&#24418;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#22312;&#36890;&#36807;&#31070;&#32463;&#20307;&#31215;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#38598;&#21512;&#20013;&#23398;&#20064;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#22270;&#20687;&#21644;&#22330;&#26223;3D&#20960;&#20309;&#22270;&#24418;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20307;&#31215;&#28210;&#26579;&#20013;&#23494;&#38598;&#37319;&#26679;&#23548;&#33268;&#30340;&#26174;&#33879;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#36843;&#20351;3D GANs&#37319;&#29992;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#25110;&#37319;&#29992;&#20302;&#20998;&#36776;&#29575;&#28210;&#26579;&#19982;&#21518;&#22788;&#29702;&#30340;2D&#36229;&#20998;&#36776;&#29575;&#65292;&#36825;&#25439;&#23475;&#20102;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#21644;&#35299;&#20915;&#20960;&#20309;&#22270;&#24418;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;3D GANs&#23578;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;2D&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;3D&#20960;&#20309;&#22270;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#20307;&#31215;&#28210;&#26579;&#25193;&#23637;&#21040;&#26412;&#22320;2D&#22270;&#20687;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#25216;&#26415;&#65292;&#22240;&#27492;&#33021;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#32454;&#33410;&#35299;&#20915;&#32454;&#31890;&#24230;&#30340;3D&#20960;&#20309;&#22270;&#24418;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#37319;&#26679;&#22120;&#26469;&#21152;&#36895;3D GAN&#35757;&#32451;&#20013;&#30340;&#31070;&#32463;&#28210;&#26579;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#28145;&#24230;&#37319;&#26679;&#27425;&#25968;&#39640;&#36798;5&#20493;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26126;&#30830;&#22320;&#8220;&#28210;&#26579;&#27599;&#20010;&#20687;&#32032;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering. Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry. Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images. In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail. Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples. This enables us to explicitly "render every pixel" o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#20108;&#32500;&#28201;&#24230;&#22330;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#36755;&#20837;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#26102;&#28201;&#24230;&#25968;&#25454;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19979;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#12289;&#27785;&#31215;&#27169;&#24335;&#21644;&#24037;&#33402;&#30340;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2401.02403</link><description>&lt;p&gt;
&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#23454;&#26102;&#20108;&#32500;&#28201;&#24230;&#22330;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks. (arXiv:2401.02403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#20108;&#32500;&#28201;&#24230;&#22330;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#36755;&#20837;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#26102;&#28201;&#24230;&#25968;&#25454;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19979;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#12289;&#27785;&#31215;&#27169;&#24335;&#21644;&#24037;&#33402;&#30340;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#28201;&#24230;&#22330;&#23545;&#20110;&#38450;&#27490;&#36807;&#28909;&#12289;&#35843;&#25972;&#24037;&#33402;&#21442;&#25968;&#21644;&#30830;&#20445;&#24037;&#33402;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#65292;&#20294;&#24448;&#24448;&#32791;&#26102;&#19988;&#19981;&#36866;&#29992;&#20110;&#36845;&#20195;&#35774;&#35745;&#22330;&#26223;&#20013;&#30340;&#23454;&#26102;&#39044;&#27979;&#21644;&#22312;&#32447;&#25511;&#21046;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#20869;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#28201;&#24230;&#22330;&#39044;&#27979;&#30340;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#36755;&#20837;&#12289;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;(ConvLSTM)&#26550;&#26500;&#12290;&#21033;&#29992;&#24037;&#33402;&#36807;&#31243;&#20013;&#30340;&#23454;&#26102;&#28201;&#24230;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19979;&#21508;&#31181;&#20960;&#20309;&#24418;&#29366;&#12289;&#27785;&#31215;&#27169;&#24335;&#21644;&#24037;&#33402;&#30340;&#20108;&#32500;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the temperature field in metal additive manufacturing (AM) processes is critical to preventing overheating, adjusting process parameters, and ensuring process stability. While physics-based computational models offer precision, they are often time-consuming and unsuitable for real-time predictions and online control in iterative design scenarios. Conversely, machine learning models rely heavily on high-quality datasets, which can be costly and challenging to obtain within the metal AM domain. Our work addresses this by introducing a physics-informed neural network framework specifically designed for temperature field prediction in metal AM. This framework incorporates a physics-informed input, physics-informed loss function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture. Utilizing real-time temperature data from the process, our model predicts 2D temperature fields for future timestamps across diverse geometries, deposition patterns, and proce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#32593;&#32476;&#25552;&#20379;&#19981;&#38656;&#35201;&#25968;&#20540;&#27714;&#35299;PDE&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.02398</link><description>&lt;p&gt;
&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic data for neural operators. (arXiv:2401.02398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#32593;&#32476;&#25552;&#20379;&#19981;&#38656;&#35201;&#25968;&#20540;&#27714;&#35299;PDE&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33719;&#21462;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#25968;&#20540;&#35299;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#36229;&#20986;&#20102;&#24403;&#21069;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#31639;&#23376;&#37117;&#23384;&#22312;&#21516;&#26679;&#30340;&#38382;&#39064;&#65306;&#35757;&#32451;&#32593;&#32476;&#25152;&#38656;&#30340;&#25968;&#25454;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#22914;&#26377;&#38480;&#24046;&#20998;&#25110;&#26377;&#38480;&#20803;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#30340;&#20989;&#25968;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#25968;&#20540;&#27714;&#35299;PDE&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65306;&#25105;&#20204;&#20174;&#24050;&#30693;&#35299;&#20301;&#20110;&#30340;&#32463;&#20856;&#29702;&#35770;&#35299;&#31354;&#38388;&#65288;&#20363;&#22914;$H_0^1(\Omega)$&#65289;&#20013;&#25277;&#21462;&#22823;&#37327;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#8220;&#38543;&#26426;&#20989;&#25968;&#8221;$u_j$&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#38543;&#26426;&#35299;&#26041;&#26696;&#20195;&#20837;&#26041;&#31243;&#24182;&#33719;&#24471;&#30456;&#24212;&#30340;&#21491;&#20391;&#20989;&#25968;$f_j$&#65292;&#23558;$(f_j, u_j)_{j=1}^N$&#20316;&#20026;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous developments in the recent literature show the promising potential of deep learning in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers. However, data-driven neural operators all suffer from the same problem: the data needed to train a network depends on classical numerical solvers such as finite difference or finite element, among others. In this paper, we propose a new approach to generating synthetic functional training data that does not require solving a PDE numerically. The way we do this is simple: we draw a large number $N$ of independent and identically distributed `random functions' $u_j$ from the underlying solution space (e.g., $H_0^1(\Omega)$) in which we know the solution lies according to classical theory. We then plug each such random candidate solution into the equation and get a corresponding right-hand side function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21442;&#25968;&#21270;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#22312;&#24322;&#36136;&#22266;&#20307;&#20013;&#30740;&#31350;&#20102;&#31283;&#24577;&#28909;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29420;&#31435;&#20110;&#26377;&#38480;&#20803;&#26041;&#27861;&#31561;&#32463;&#20856;&#27714;&#35299;&#22120;&#65292;&#24182;&#20351;&#29992;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#23450;&#20041;&#65292;&#38477;&#20302;&#20102;&#23548;&#25968;&#38454;&#25968;&#21644;&#33258;&#21160;&#24494;&#20998;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.02363</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#25805;&#20316;&#23398;&#20064;&#21644;&#26377;&#38480;&#20803;&#26041;&#27861;&#30340;&#34701;&#21512;&#65292;&#29992;&#20110;&#21442;&#25968;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Integration of physics-informed operator learning and finite element method for parametric learning of partial differential equations. (arXiv:2401.02363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21442;&#25968;&#21270;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#22312;&#24322;&#36136;&#22266;&#20307;&#20013;&#30740;&#31350;&#20102;&#31283;&#24577;&#28909;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29420;&#31435;&#20110;&#26377;&#38480;&#20803;&#26041;&#27861;&#31561;&#32463;&#20856;&#27714;&#35299;&#22120;&#65292;&#24182;&#20351;&#29992;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#23450;&#20041;&#65292;&#38477;&#20302;&#20102;&#23548;&#25968;&#38454;&#25968;&#21644;&#33258;&#21160;&#24494;&#20998;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#21442;&#25968;&#21270;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#37325;&#28857;&#26159;&#30740;&#31350;&#20855;&#26377;&#26174;&#33879;&#30456;&#20301;&#23545;&#27604;&#30340;&#24322;&#36136;&#22266;&#20307;&#20013;&#30340;&#31283;&#24577;&#28909;&#26041;&#31243;&#12290;&#31867;&#20284;&#30340;&#26041;&#31243;&#22312;&#21270;&#23398;&#25193;&#25955;&#12289;&#38745;&#30005;&#23398;&#21644;&#36798;&#35199;&#27969;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#23384;&#22312;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#22797;&#26434;&#28909;&#23548;&#29575;&#20998;&#24067;&#19982;&#28201;&#24230;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#22312;&#22266;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#24494;&#32467;&#26500;&#20013;&#30340;&#28909;&#27969;&#20998;&#37327;&#12290;&#19968;&#20010;&#29420;&#29305;&#30340;&#26041;&#38754;&#26159;&#25105;&#20204;&#22312;&#25968;&#25454;&#19978;&#29420;&#31435;&#20110;&#26377;&#38480;&#20803;&#26041;&#27861;&#31561;&#32463;&#20856;&#27714;&#35299;&#22120;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#36129;&#29486;&#22312;&#20110;&#25105;&#20204;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#39062;&#23450;&#20041;&#65292;&#22522;&#20110;&#31163;&#25955;&#21270;&#30340;&#24369;&#24418;&#24335;&#25511;&#21046;&#26041;&#31243;&#12290;&#36825;&#19981;&#20165;&#38477;&#20302;&#20102;&#25152;&#38656;&#23548;&#25968;&#30340;&#38454;&#25968;&#65292;&#36824;&#28040;&#38500;&#20102;&#22312;&#26500;&#24314;&#25439;&#22833;&#39033;&#20013;&#30340;&#33258;&#21160;&#24494;&#20998;&#38656;&#27714;&#65292;&#25509;&#21463;&#20102;&#28508;&#22312;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method that employs physics-informed deep learning techniques for parametrically solving partial differential equations. The focus is on the steady-state heat equations within heterogeneous solids exhibiting significant phase contrast. Similar equations manifest in diverse applications like chemical diffusion, electrostatics, and Darcy flow. The neural network aims to establish the link between the complex thermal conductivity profiles and temperature distributions, as well as heat flux components within the microstructure, under fixed boundary conditions. A distinctive aspect is our independence from classical solvers like finite element methods for data. A noteworthy contribution lies in our novel approach to defining the loss function, based on the discretized weak form of the governing equation. This not only reduces the required order of derivatives but also eliminates the need for automatic differentiation in the construction of loss terms, accepting potential numeri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#20102;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.02349</link><description>&lt;p&gt;
&#20998;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24615;&#33021;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey Analyzing Generalization in Deep Reinforcement Learning. (arXiv:2401.02349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#20102;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#39640;&#32500;&#29366;&#24577;&#25110;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25104;&#21151;&#21644;&#20851;&#27880;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30446;&#21069;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#27491;&#22312;&#34987;&#24212;&#29992;&#65292;&#20174;&#21307;&#30103;&#24212;&#29992;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65292;&#20294;&#20851;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#26377;&#35768;&#22810;&#24453;&#35299;&#31572;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36935;&#21040;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23545;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#20013;&#30340;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#20026;&#24403;&#21069;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#25552;&#20379;&#19968;&#20010;&#31616;&#27905;&#31995;&#32479;&#30340;&#32479;&#19968;&#20998;&#26512;&#65292;&#24182;&#26377;&#21161;&#20110;&#26500;&#24314;&#20581;&#22766;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities. Furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions. We believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#29305;&#24449;&#29983;&#25104;&#22120;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;MSDA-TF&#65289;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#23398;&#31185;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#20445;&#30041;&#27973;&#23618;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#39057;&#35889;&#30340;&#33041;&#30005;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25552;&#21462;&#20854;&#20013;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.02344</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#29305;&#24449;&#29983;&#25104;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#29420;&#31435;&#20110;&#23398;&#31185;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition. (arXiv:2401.02344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#29305;&#24449;&#29983;&#25104;&#22120;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;MSDA-TF&#65289;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#23398;&#31185;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#20445;&#30041;&#27973;&#23618;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#39057;&#35889;&#30340;&#33041;&#30005;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25552;&#21462;&#20854;&#20013;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#22312;&#36890;&#36807;&#33041;&#30005;&#20449;&#21495;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20010;&#20307;&#20043;&#38388;&#30340;&#33041;&#20449;&#21495;&#27169;&#24335;&#21464;&#21270;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#19981;&#21516;&#23398;&#31185;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#38754;&#20020;&#19982;&#19981;&#20805;&#20998;&#30340;&#29305;&#24449;&#34920;&#31034;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#21487;&#33021;&#24573;&#35270;&#28304;&#20027;&#20307;&#26412;&#36523;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#20107;&#23454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#29305;&#24449;&#29983;&#25104;&#22120;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;MSDA-TF&#65289;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#29305;&#24449;&#29983;&#25104;&#22120;&#20445;&#30041;&#21367;&#31215;&#23618;&#20197;&#25429;&#25417;&#27973;&#23618;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#39057;&#35889;&#30340;&#33041;&#30005;&#25968;&#25454;&#34920;&#31034;&#65292;&#32780;&#33258;&#27880;&#24847;&#26426;&#21046;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#12290;&#22312;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#30456;&#20851;&#24615;&#23558;&#28304;&#20027;&#20307;&#20998;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning-based algorithms have demonstrated excellent performance in automated emotion recognition via electroencephalogram (EEG) signals, variations across brain signal patterns of individuals can diminish the model's effectiveness when applied across different subjects. While transfer learning techniques have exhibited promising outcomes, they still encounter challenges related to inadequate feature representations and may overlook the fact that source subjects themselves can possess distinct characteristics. In this work, we propose a multi-source domain adaptation approach with a transformer-based feature generator (MSDA-TF) designed to leverage information from multiple sources. The proposed feature generator retains convolutional layers to capture shallow spatial, temporal, and spectral EEG data representations, while self-attention mechanisms extract global dependencies within these features. During the adaptation process, we group the source subjects based on corr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#21151;&#29575;&#36861;&#36394;&#30340;&#36867;&#36920;&#30828;&#20214;&#26408;&#39532;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#22122;&#22768;&#28151;&#28102;&#20391;&#20449;&#36947;&#20998;&#26512;&#65292;&#20351;&#24471;&#30828;&#20214;&#26408;&#39532;&#33021;&#22815;&#32469;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02342</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#21151;&#29575;&#36861;&#36394;&#30340;&#36867;&#36920;&#30828;&#20214;&#26408;&#39532;
&lt;/p&gt;
&lt;p&gt;
Evasive Hardware Trojan through Adversarial Power Trace. (arXiv:2401.02342v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#21151;&#29575;&#36861;&#36394;&#30340;&#36867;&#36920;&#30828;&#20214;&#26408;&#39532;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#22122;&#22768;&#28151;&#28102;&#20391;&#20449;&#36947;&#20998;&#26512;&#65292;&#20351;&#24471;&#30828;&#20214;&#26408;&#39532;&#33021;&#22815;&#32469;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#20379;&#24212;&#38142;&#30340;&#20840;&#29699;&#21270;&#20351;&#24471;IC&#26131;&#21463;&#21040;&#30828;&#20214;&#26408;&#39532;&#65288;HT&#65289;&#30340;&#23041;&#32961;&#12290;&#38754;&#23545;&#36825;&#31181;&#23041;&#32961;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#38750;&#20837;&#20405;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#33021;&#22312;&#40644;&#37329;&#33455;&#29255;&#26080;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#26816;&#27979;HT&#12290;&#26412;&#25991;&#36136;&#30097;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#26469;&#26816;&#27979;HT&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;HT&#28151;&#28102;&#65288;HTO&#65289;&#26041;&#27861;&#65292;&#20197;&#20801;&#35768;HT&#32469;&#36807;&#36825;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#22312;&#30005;&#36335;&#30340;&#19968;&#37096;&#20998;&#21644;HT&#19968;&#36215;&#35774;&#35745;&#21644;&#23454;&#29616;&#23545;&#25239;&#24615;&#22122;&#22768;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#27169;&#25311;&#23545;&#25239;&#24615;&#36857;&#32447;&#26469;&#29702;&#35770;&#19978;&#35823;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;ASIC&#21644;FPGA&#30340;HTO&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;TrustHub&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;HTO&#21487;&#20197;&#22312;ASIC&#35774;&#35745;&#20013;&#21482;&#20351;&#29992;&#19968;&#20010;&#26230;&#20307;&#31649;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The globalization of the Integrated Circuit (IC) supply chain, driven by time-to-market and cost considerations, has made ICs vulnerable to hardware Trojans (HTs). Against this threat, a promising approach is to use Machine Learning (ML)-based side-channel analysis, which has the advantage of being a non-intrusive method, along with efficiently detecting HTs under golden chip-free settings. In this paper, we question the trustworthiness of ML-based HT detection via side-channel analysis. We introduce a HT obfuscation (HTO) approach to allow HTs to bypass this detection method. Rather than theoretically misleading the model by simulated adversarial traces, a key aspect of our approach is the design and implementation of adversarial noise as part of the circuitry, alongside the HT. We detail HTO methodologies for ASICs and FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly, we found that HTO can be implemented with only a single transistor for ASIC designs to genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02333</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#21462;&#65306;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#20197;&#23454;&#29616;&#39640;&#25928;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104; (RAG) &#26550;&#26500;&#22312;&#20174;&#21508;&#31181;&#25991;&#20214;&#20013;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21253;&#21547;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340; PDF &#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558; PDF &#23384;&#20648;&#22312;&#26816;&#32034;&#25968;&#25454;&#24211;&#20013;&#65292;&#24182;&#21333;&#29420;&#25552;&#21462;&#34920;&#26684;&#20869;&#23481;&#12290;&#25552;&#21462;&#30340;&#34920;&#26684;&#32463;&#36807;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#22788;&#29702;&#65292;&#23558;&#26631;&#39064;&#19982;&#30456;&#24212;&#30340;&#20540;&#36830;&#25509;&#36215;&#26469;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#20016;&#23500;&#25968;&#25454;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340; Llama-2-chat &#35821;&#35328;&#27169;&#22411;&#22312; RAG &#26550;&#26500;&#20013;&#36827;&#34892;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#27425;&#24615;&#25552;&#31034;&#20351;&#29992; ChatGPT 3.5 API &#22686;&#24378;&#34920;&#26684;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182; PDF &#25991;&#20214;&#19968;&#36215;&#36755;&#20837;&#26816;&#32034;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02329</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#23569;&#25968;&#32676;&#20307;&#37117;&#26159;&#24179;&#31561;&#30340;: &#31354;&#31867;&#21035;&#24863;&#30693;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#34920;&#29616;&#20026;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#31867;&#21035;&#24179;&#34913;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#26412;&#22320;&#31867;&#21035;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#25968;&#31867;&#21035;&#20013;&#30001;&#20110;&#36807;&#25311;&#21512;&#26412;&#22320;&#19981;&#24179;&#34913;&#25968;&#25454;&#32780;&#23548;&#33268;&#20934;&#30830;&#24615;&#36739;&#24046;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedED&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31354;&#31867;&#21035;&#33976;&#39311;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#20013;&#20445;&#30041;&#20102;&#19982;&#31354;&#31867;&#21035;&#30456;&#20851;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36923;&#36753;&#25233;&#21046;&#30452;&#25509;&#38459;&#26029;&#20102;&#39044;&#27979;&#32467;&#26524;&#20013;&#23545;&#31354;&#31867;&#21035;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20998;&#20301;&#25968;Huber&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#25429;&#25417;&#22122;&#22768;&#24182;&#35843;&#25972;&#21442;&#25968;&#26469;&#22686;&#24378;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#35777;&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02325</link><description>&lt;p&gt;
&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#35299;&#37322;&#21442;&#25968;&#35843;&#25972;&#30340;&#40065;&#26834;&#20998;&#20301;&#25968;Huber&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning. (arXiv:2401.02325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02325
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20998;&#20301;&#25968;Huber&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#25429;&#25417;&#22122;&#22768;&#24182;&#35843;&#25972;&#21442;&#25968;&#26469;&#22686;&#24378;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#35777;&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#20301;&#25968;Huber&#25439;&#22833;&#20989;&#25968;&#26469;&#20272;&#35745;&#22238;&#25253;&#20998;&#24067;&#65292;&#35813;&#20989;&#25968;&#20174;&#39640;&#26031;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#35745;&#31639;&#20013;&#20135;&#29983;&#65292;&#25429;&#25417;&#21040;&#24403;&#21069;&#21644;&#30446;&#26631;&#20998;&#20301;&#25968;&#20540;&#20013;&#30340;&#22122;&#22768;&#12290;&#19982;&#32463;&#20856;&#30340;&#20998;&#20301;&#25968;Huber&#25439;&#22833;&#30456;&#27604;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#20102;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#36817;&#20284;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#25968;&#37327;&#26469;&#35843;&#25972;&#21442;&#25968;&#12290;&#23454;&#35777;&#27979;&#35797;&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#30340;&#24120;&#35265;&#24212;&#29992;Atari&#28216;&#25103;&#21644;&#26368;&#36817;&#30340;&#23545;&#20914;&#31574;&#30053;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning (RL) estimates return distribution mainly by learning quantile values via minimizing the quantile Huber loss function, entailing a threshold parameter often selected heuristically or via hyperparameter search, which may not generalize well and can be suboptimal. This paper introduces a generalized quantile Huber loss function derived from Wasserstein distance (WD) calculation between Gaussian distributions, capturing noise in predicted (current) and target (Bellman-updated) quantile values. Compared to the classical quantile Huber loss, this innovative loss function enhances robustness against outliers. Notably, the classical Huber loss function can be seen as an approximation of our proposed loss, enabling parameter adjustment by approximating the amount of noise in the data during the learning process. Empirical tests on Atari games, a common application in distributional RL, and a recent hedging strategy using distributional RL, validate the eff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACOL&#30340;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#32972;&#26223;&#36172;&#21338;&#26426;&#31649;&#29702;&#24178;&#25200;&#65292;&#24182;&#20998;&#37197;mmWave&#27874;&#26463;&#20197;&#26381;&#21153;&#36710;&#36742;&#12290;</title><link>http://arxiv.org/abs/2401.02323</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#22312;&#27627;&#31859;&#27874;&#36710;&#32852;&#32593;&#36890;&#20449;&#20013;&#30340;&#24178;&#25200;&#24863;&#30693;&#27874;&#26463;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Context Learning Strategy for Interference-Aware Beam Allocation in mmWave Vehicular Communications. (arXiv:2401.02323v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACOL&#30340;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#32972;&#26223;&#36172;&#21338;&#26426;&#31649;&#29702;&#24178;&#25200;&#65292;&#24182;&#20998;&#37197;mmWave&#27874;&#26463;&#20197;&#26381;&#21153;&#36710;&#36742;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#34987;&#35748;&#20026;&#26159;5G&#21450;&#20854;&#21518;&#32493;&#32593;&#32476;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#24378;&#20449;&#36947;&#24102;&#23485;&#21644;&#32593;&#32476;&#23481;&#37327;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#35752;&#35770;&#20102;&#23558;mmWave&#24212;&#29992;&#20110;&#36710;&#32852;&#32593;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#23558;mmWave&#24212;&#29992;&#20110;&#36710;&#32852;&#32593;&#36890;&#20449;&#38754;&#20020;&#30528;&#39640;&#31227;&#21160;&#33410;&#28857;&#21644;mmWave&#27874;&#26463;&#29421;&#31364;&#35206;&#30422;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#23494;&#38598;&#32593;&#32476;&#20013;&#30340;&#39640;&#31227;&#21160;&#24615;&#65292;&#37325;&#21472;&#30340;&#27874;&#26463;&#21487;&#33021;&#20250;&#24341;&#36215;&#24378;&#24178;&#25200;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#24178;&#25200;&#25511;&#21046;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26234;&#33021;&#20307;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;MACOL&#65289;&#30340;&#26032;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#32972;&#26223;&#36172;&#21338;&#26426;&#26469;&#31649;&#29702;&#24178;&#25200;&#65292;&#21516;&#26102;&#20998;&#37197;mmWave&#27874;&#26463;&#20197;&#26381;&#21153;&#36710;&#36742;&#12290;
&lt;/p&gt;
&lt;p&gt;
Millimeter wave (mmWave) has been recognized as one of key technologies for 5G and beyond networks due to its potential to enhance channel bandwidth and network capacity. The use of mmWave for various applications including vehicular communications has been extensively discussed. However, applying mmWave to vehicular communications faces challenges of high mobility nodes and narrow coverage along the mmWave beams. Due to high mobility in dense networks, overlapping beams can cause strong interference which leads to performance degradation. As a remedy, beam switching capability in mmWave can be utilized. Then, frequent beam switching and cell change become inevitable to manage interference, which increase computational and signalling complexity. In order to deal with the complexity in interference control, we develop a new strategy called Multi-Agent Context Learning (MACOL), which utilizes Contextual Bandit to manage interference while allocating mmWave beams to serve vehicles in the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#65292;&#35813;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;PDE&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;PINNs&#20013;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#19981;&#40065;&#26834;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02300</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Robust Physics Informed Neural Networks. (arXiv:2401.02300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02300
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#65292;&#35813;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;PDE&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;PINNs&#20013;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#19981;&#40065;&#26834;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#29256;&#26412;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#12290;&#26631;&#20934;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#30001;PDE&#25551;&#36848;&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#12290;&#35813;&#32593;&#32476;&#22312;&#30001;&#29289;&#29702;&#22495;&#21644;&#36793;&#30028;&#38543;&#26426;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;PINNs&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35299;&#20915;&#30001;PDE&#21644;&#36793;&#30028;&#26465;&#20214;&#25551;&#36848;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#20256;&#32479;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;PDE&#30340;&#24378;&#27531;&#24046;&#12290;&#36825;&#31181;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#23545;&#30495;&#23454;&#35823;&#24046;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#21487;&#33021;&#30456;&#24046;&#24456;&#22823;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#22256;&#38590;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#30693;&#36947;&#31934;&#30830;&#35299;&#65292;&#25105;&#20204;&#23601;&#19981;&#33021;&#20272;&#35745;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#24050;&#32463;&#20197;&#25152;&#38656;&#30340;&#31934;&#24230;&#25910;&#25947;&#21040;&#35299;&#12290;&#36825;&#22312;&#25105;&#20204;&#19981;&#30693;&#36947;&#31934;&#30830;&#35299;&#26102;&#23588;&#20854;&#27491;&#30830;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce a Robust version of the Physics-Informed Neural Networks (RPINNs) to approximate the Partial Differential Equations (PDEs) solution. Standard Physics Informed Neural Networks (PINN) takes into account the governing physical laws described by PDE during the learning process. The network is trained on a data set that consists of randomly selected points in the physical domain and its boundary. PINNs have been successfully applied to solve various problems described by PDEs with boundary conditions. The loss function in traditional PINNs is based on the strong residuals of the PDEs. This loss function in PINNs is generally not robust with respect to the true error. The loss function in PINNs can be far from the true error, which makes the training process more difficult. In particular, we do not know if the training process has already converged to the solution with the required accuracy. This is especially true if we do not know the exact solution, so we cannot estimate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20984;&#20985;&#35268;&#21010;&#35757;&#32451;&#21333;&#23618;&#24418;&#24577;&#24863;&#30693;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;K-DDCCP&#31639;&#27861;&#32467;&#21512;SLMP&#27169;&#22411;&#21644;WDCCP&#31639;&#27861;&#26469;&#35299;&#20915;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02296</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20985;&#35268;&#21010;&#35757;&#32451;&#21333;&#23618;&#24418;&#24577;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Training Single-Layer Morphological Perceptron Using Convex-Concave Programming. (arXiv:2401.02296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20984;&#20985;&#35268;&#21010;&#35757;&#32451;&#21333;&#23618;&#24418;&#24577;&#24863;&#30693;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;K-DDCCP&#31639;&#27861;&#32467;&#21512;SLMP&#27169;&#22411;&#21644;WDCCP&#31639;&#27861;&#26469;&#35299;&#20915;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#20984;&#20985;&#35268;&#21010;&#65288;DCCP&#65289;&#35757;&#32451;&#21333;&#23618;&#24418;&#24577;&#24863;&#30693;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;K-DDCCP&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;Ritter&#21644;Urcid&#25552;&#20986;&#30340;&#29616;&#26377;&#21333;&#23618;&#24418;&#24577;&#24863;&#30693;&#22120;&#65288;SLMP&#65289;&#27169;&#22411;&#19982;Charisopoulos&#21644;Maragos&#25552;&#20986;&#30340;&#21152;&#26435;&#20984;&#20985;&#35268;&#21010;&#65288;WDCCP&#65289;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#31639;&#27861;&#21033;&#29992;&#20984;&#20985;&#35268;&#21010;&#36807;&#31243;&#65288;DCCP&#65289;&#24182;&#23558;&#20108;&#20998;&#31867;&#38382;&#39064;&#24314;&#27169;&#20026;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32422;&#26463;&#34987;&#34920;&#31034;&#20026;&#20984;&#20989;&#25968;&#20043;&#24046;&#65292;&#20174;&#32780;&#21487;&#20197;&#24212;&#29992;DCCP&#36719;&#20214;&#21253;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;K-DDCCP&#31639;&#27861;&#22312;&#35299;&#20915;&#20108;&#20998;&#31867;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;SLMP&#27169;&#22411;&#33021;&#21147;&#30340;&#31639;&#27861;&#65292;&#20026;&#24418;&#24577;&#23398;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the training of a single-layer morphological perceptron using disciplined convex-concave programming (DCCP). We introduce an algorithm referred to as K-DDCCP, which combines the existing single-layer morphological perceptron (SLMP) model proposed by Ritter and Urcid with the weighted disciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and Maragos. The proposed training algorithm leverages the disciplined convex-concave procedure (DCCP) and formulates a non-convex optimization problem for binary classification. To tackle this problem, the constraints are expressed as differences of convex functions, enabling the application of the DCCP package. The experimental results confirm the effectiveness of the K-DDCCP algorithm in solving binary classification problems. Overall, this work contributes to the field of morphological neural networks by proposing an algorithm that extends the capabilities of the SLMP model.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;Power-Link&#36890;&#36807;&#24341;&#20837;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25512;&#21160;&#20102;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.02290</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Path-based Explanation for Knowledge Graph Completion. (arXiv:2401.02290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02290
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;Power-Link&#36890;&#36807;&#24341;&#20837;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25512;&#21160;&#20102;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20132;&#20114;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#35299;&#37322;&#21364;&#27809;&#26377;&#24471;&#21040;&#24517;&#35201;&#30340;&#20851;&#27880;&#12290;&#23545;&#22522;&#20110;GNN&#30340;KGC&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#36866;&#24403;&#35299;&#37322;&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;KGC&#35299;&#37322;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23454;&#20363;/&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#65292;&#36335;&#24452;&#21487;&#20197;&#25552;&#20379;&#26356;&#21451;&#22909;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#23545;&#29983;&#25104;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Power-Link&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25506;&#32034;&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#20351;&#24471;&#21487;&#20197;&#20197;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#29983;&#25104;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#26469;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26631;&#35760;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#20154;&#24037;&#19987;&#23478;&#26816;&#26597;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20986;&#32780;&#19981;&#26159;&#25972;&#20010;DNN&#30340;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.02283</link><description>&lt;p&gt;
DEM: &#33322;&#31354;&#33322;&#22825;&#20013;&#29992;&#20110;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace. (arXiv:2401.02283v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#26469;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26631;&#35760;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#20154;&#24037;&#19987;&#23478;&#26816;&#26597;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20986;&#32780;&#19981;&#26159;&#25972;&#20010;DNN&#30340;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#33322;&#22825;&#39046;&#22495;&#30340;&#36719;&#20214;&#24320;&#21457;&#35201;&#27714;&#36981;&#24490;&#20005;&#26684;&#12289;&#39640;&#36136;&#37327;&#30340;&#26631;&#20934;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#21830;&#29992;&#36719;&#20214;&#30340;&#30417;&#31649;&#25351;&#21335;&#65288;&#20363;&#22914;ARP-4754&#21644;DO-178&#65289;&#65292;&#20294;&#36825;&#20123;&#25351;&#21335;&#24182;&#19981;&#36866;&#29992;&#20110;&#20855;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#32452;&#20214;&#30340;&#36719;&#20214;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20351;&#33322;&#31354;&#33322;&#22825;&#31995;&#32479;&#21463;&#30410;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38761;&#21629;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#29992;&#20110;DNN&#30340;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#33021;&#22815;&#26631;&#35760;DNN&#36755;&#20986;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#30340;&#20851;&#38190;&#20248;&#21183;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#19987;&#23478;&#26816;&#26597;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;DNN&#23545;&#20854;&#20182;&#38468;&#36817;&#36755;&#20837;&#30340;&#39044;&#27979;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#26816;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21453;&#65292;&#21518;&#32773;&#36890;&#24120;&#35797;&#22270;&#23545;&#25972;&#20010;DNN&#36827;&#34892;&#35748;&#35777;&#65292;&#32780;&#38750;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software development in the aerospace domain requires adhering to strict, high-quality standards. While there exist regulatory guidelines for commercial software in this domain (e.g., ARP-4754 and DO-178), these do not apply to software with deep neural network (DNN) components. Consequently, it is unclear how to allow aerospace systems to benefit from the deep learning revolution. Our work here seeks to address this challenge with a novel, output-centric approach for DNN certification. Our method employs statistical verification techniques, and has the key advantage of being able to flag specific inputs for which the DNN's output may be unreliable - so that they may be later inspected by a human expert. To achieve this, our method conducts a statistical analysis of the DNN's predictions for other, nearby inputs, in order to detect inconsistencies. This is in contrast to existing techniques, which typically attempt to certify the entire DNN, as opposed to individual outputs. Our method
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40060;&#31867;&#20998;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35782;&#21035;&#20445;&#25252;&#40060;&#31867;&#29289;&#31181;&#65292;&#24182;&#22312;&#26377;&#38480;&#30828;&#20214;&#19978;&#36816;&#34892;&#12290;&#35813;&#27169;&#22411;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#27700;&#22495;&#30340;&#40060;&#31867;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23545;&#25429;&#33719;&#30340;&#40060;&#31867;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#32473;&#20986;&#26159;&#21542;&#21487;&#39135;&#29992;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.02278</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#40060;&#31867;&#20998;&#31867;&#27169;&#22411;&#22312;&#21487;&#25345;&#32493;&#28023;&#27915;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;: &#21360;&#24230;&#23612;&#35199;&#20122;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case. (arXiv:2401.02278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02278
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40060;&#31867;&#20998;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35782;&#21035;&#20445;&#25252;&#40060;&#31867;&#29289;&#31181;&#65292;&#24182;&#22312;&#26377;&#38480;&#30828;&#20214;&#19978;&#36816;&#34892;&#12290;&#35813;&#27169;&#22411;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#27700;&#22495;&#30340;&#40060;&#31867;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23545;&#25429;&#33719;&#30340;&#40060;&#31867;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#32473;&#20986;&#26159;&#21542;&#21487;&#39135;&#29992;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28023;&#20135;&#21697;&#30340;&#24040;&#22823;&#38656;&#27714;&#23548;&#33268;&#20102;&#23545;&#28023;&#27915;&#36164;&#28304;&#30340;&#36807;&#24230;&#24320;&#21457;&#21644;&#19968;&#20123;&#29289;&#31181;&#28626;&#20020;&#28781;&#32477;&#12290;&#22312;&#21487;&#25345;&#32493;&#28023;&#27915;&#21457;&#23637;&#20013;&#65292;&#36807;&#24230;&#25429;&#25438;&#26159;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#25252;&#28023;&#27915;&#36164;&#28304;&#21644;&#21487;&#25345;&#32493;&#25429;&#25438;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#40060;&#31867;&#20998;&#31867;&#25216;&#26415;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#21463;&#20445;&#25252;&#30340;&#40060;&#31867;&#29289;&#31181;&#12290;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;MobileNet&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#20998;&#31867;&#22120;M-MobileNet&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30828;&#20214;&#19978;&#36816;&#34892;&#12290;&#20316;&#20026;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;37,462&#24352;&#21360;&#24230;&#23612;&#35199;&#20122;&#32676;&#23707;&#27700;&#22495;&#40060;&#31867;&#22270;&#29255;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23558;&#25429;&#33719;&#30340;&#40060;&#31867;&#30340;&#22270;&#20687;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#29289;&#31181;&#65292;&#24182;&#32473;&#20986;&#26159;&#21542;&#21487;&#39135;&#29992;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#20462;&#25913;&#30340;MobileNet&#27169;&#22411;&#20165;&#20351;&#29992;&#20102;50%&#30340;&#39030;&#23618;&#21442;&#25968;&#65292;&#32422;42%&#30340;GTX 860M&#21033;&#29992;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enormous demand for seafood products has led to exploitation of marine resources and near-extinction of some species. In particular, overfishing is one the main issues in sustainable marine development. In alignment with the protection of marine resources and sustainable fishing, this study proposes to advance fish classification techniques that support identifying protected fish species using state-of-the-art machine learning. We use a custom modification of the MobileNet model to design a lightweight classifier called M-MobileNet that is capable of running on limited hardware. As part of the study, we compiled a labeled dataset of 37,462 images of fish found in the waters of the Indonesian archipelago. The proposed model is trained on the dataset to classify images of the captured fish into their species and give recommendations on whether they are consumable or not. Our modified MobileNet model uses only 50\% of the top layer parameters with about 42% GTX 860M utility and achiev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#12290;&#36825;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.02277</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#21644;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation Theorem for Vector- and Hypercomplex-Valued Neural Networks. (arXiv:2401.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#12290;&#36825;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#34920;&#26126;&#65292;&#20855;&#26377;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#20219;&#24847;&#25152;&#38656;&#30340;&#31934;&#24230;&#36924;&#36817;&#32039;&#38598;&#19978;&#30340;&#36830;&#32493;&#20989;&#25968;&#12290;&#35813;&#23450;&#29702;&#25903;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23454;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20123;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;&#22797;&#25968;&#12289;&#22235;&#20803;&#25968;&#12289;&#22235;&#20803;&#25968;&#30690;&#37327;&#21644;Clifford&#20540;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#35813;&#23450;&#29702;&#22343;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#22312;&#20855;&#26377;&#38468;&#21152;&#20195;&#25968;&#25110;&#20960;&#20309;&#24615;&#36136;&#30340;&#20195;&#25968;&#19978;&#23450;&#20041;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25193;&#23637;&#21040;&#20102;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#20316;&#20026;&#29305;&#27530;&#23454;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#24182;&#38416;&#36848;&#20102;&#22312;&#36825;&#31181;&#20195;&#25968;&#19978;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#37492;&#20110;&#22797;&#26434;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#24322;&#36136;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#32570;&#22833;&#20540;&#21450;&#20854;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#20851;&#27880;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25554;&#34917;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02258</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#28145;&#24230;&#20851;&#27880;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#36136;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation. (arXiv:2401.02258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02258
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#37492;&#20110;&#22797;&#26434;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#24322;&#36136;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#32570;&#22833;&#20540;&#21450;&#20854;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#20851;&#27880;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25554;&#34917;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26222;&#36941;&#23384;&#22312;&#32570;&#22833;&#65292;&#32473;&#21487;&#38752;&#30340;&#19979;&#28216;&#20998;&#26512;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#23613;&#31649;&#36882;&#24402;&#32593;&#32476;&#25554;&#34917;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#19981;&#33021;&#25193;&#23637;&#21040;&#21487;&#20197;&#32531;&#35299;&#22797;&#26434;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#30340;&#28145;&#24230;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25554;&#34917;&#36824;&#23384;&#22312;&#20272;&#35745;&#22320;&#38754;&#30495;&#20540;&#20559;&#24046;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#23545;&#25554;&#34917;&#20540;&#30340;&#32622;&#20449;&#24230;&#22987;&#32456;&#26159;&#26410;&#34987;&#27979;&#37327;&#30340;&#25110;&#20174;&#27169;&#22411;&#36755;&#20986;&#21518;&#35745;&#31639;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DEep Attention Recurrent Imputation (DEARI)&#65292;&#23427;&#22312;&#24322;&#36136;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#21516;&#26102;&#20272;&#35745;&#32570;&#22833;&#20540;&#21450;&#20854;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#32852;&#21512;&#34920;&#31034;&#29305;&#24449;&#30456;&#20851;&#24615;&#21644;&#26102;&#24207;&#21160;&#24577;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#26377;&#25928;&#30340;&#27531;&#24046;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#25554;&#34917;&#24615;&#33021;&#21644;&#31283;&#23450;&#25910;&#25947;&#24615;&#30340;&#28145;&#24230;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#33258;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#26469;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missingness is ubiquitous in multivariate time series and poses an obstacle to reliable downstream analysis. Although recurrent network imputation achieved the SOTA, existing models do not scale to deep architectures that can potentially alleviate issues arising in complex data. Moreover, imputation carries the risk of biased estimations of the ground truth. Yet, confidence in the imputed values is always unmeasured or computed post hoc from model output. We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates missing values and their associated uncertainty in heterogeneous multivariate time series. By jointly representing feature-wise correlations and temporal dynamics, we adopt a self attention mechanism, along with an effective residual component, to achieve a deep recurrent neural network with good imputation performance and stable convergence. We also leverage self-supervised metric learning to boost performance by optimizing sample similarity. Finally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#65292;&#37319;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;CaSSLe&#21644;&#21322;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;Kaizen&#26469;&#24179;&#34913;&#34920;&#24449;&#23398;&#20064;&#21644;&#19979;&#28216;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#26696;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#35757;&#32451;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#20445;&#30041;&#21644;&#23450;&#21046;&#21270;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02255</link><description>&lt;p&gt;
&#24179;&#34913;&#36830;&#32493;&#23398;&#20064;&#21644;&#24494;&#35843;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Balancing Continual Learning and Fine-tuning for Human Activity Recognition. (arXiv:2401.02255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#65292;&#37319;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;CaSSLe&#21644;&#21322;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;Kaizen&#26469;&#24179;&#34913;&#34920;&#24449;&#23398;&#20064;&#21644;&#19979;&#28216;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#26696;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#35757;&#32451;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#20445;&#30041;&#21644;&#23450;&#21046;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;Human Activity Recognition&#65292;HAR&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#23545;&#20154;&#31867;&#34892;&#20026;&#36827;&#34892;&#22522;&#26412;&#30340;&#29702;&#35299;&#12290;&#30001;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#21160;&#24577;&#24615;&#65292;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#20026;HAR&#31995;&#32479;&#25552;&#20379;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#30340;&#23450;&#21046;&#21270;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25910;&#38598;&#26631;&#35760;&#25968;&#25454;&#30340;&#22256;&#38590;&#24615;&#65292;&#29616;&#26377;&#30340;&#30528;&#37325;&#20110;&#26377;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#24212;&#29992;&#33539;&#22260;&#26377;&#38480;&#65292;&#32780;&#26080;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20165;&#22788;&#29702;&#34920;&#24449;&#23398;&#20064;&#65292;&#24182;&#23558;&#20998;&#31867;&#22120;&#35757;&#32451;&#25512;&#36831;&#21040;&#31245;&#21518;&#30340;&#38454;&#27573;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#36830;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;CaSSLe&#21644;&#21322;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;Kaizen&#24212;&#29992;&#20110;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;HAR&#20219;&#21153;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#33258;&#36866;&#24212;&#25913;&#36827;&#12290;&#36825;&#20123;&#26041;&#26696;&#37325;&#26032;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#20445;&#30041;&#30693;&#35782;&#65292;&#24182;&#19988;Kaizen&#23558;&#20854;&#19982;&#33258;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#34920;&#24449;&#23398;&#20064;&#21644;&#19979;&#28216;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable-based Human Activity Recognition (HAR) is a key task in human-centric machine learning due to its fundamental understanding of human behaviours. Due to the dynamic nature of human behaviours, continual learning promises HAR systems that are tailored to users' needs. However, because of the difficulty in collecting labelled data with wearable sensors, existing approaches that focus on supervised continual learning have limited applicability, while unsupervised continual learning methods only handle representation learning while delaying classifier training to a later stage. This work explores the adoption and adaptation of CaSSLe, a continual self-supervised learning model, and Kaizen, a semi-supervised continual learning model that balances representation learning and down-stream classification, for the task of wearable-based HAR. These schemes re-purpose contrastive learning for knowledge retention and, Kaizen combines that with self-training in a unified scheme that can leve
&lt;/p&gt;</description></item><item><title>L3Cube-IndicNews&#26159;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#31995;&#30340;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30701;&#26631;&#39064;&#12289;&#38271;&#25991;&#26723;&#21644;&#38271;&#27573;&#33853;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#23427;&#25552;&#20379;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#25991;&#31456;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02254</link><description>&lt;p&gt;
L3Cube-IndicNews&#65306;&#21360;&#24230;&#35821;&#31995;&#26032;&#38395;&#30701;&#25991;&#21644;&#38271;&#25991;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages. (arXiv:2401.02254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02254
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicNews&#26159;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#31995;&#30340;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30701;&#26631;&#39064;&#12289;&#38271;&#25991;&#26723;&#21644;&#38271;&#27573;&#33853;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#23427;&#25552;&#20379;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#25991;&#31456;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;L3Cube-IndicNews&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#20026;&#21360;&#24230;&#22320;&#21306;&#30340;&#21508;&#22823;&#26041;&#35328;&#35821;&#35328;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20851;&#27880;&#26032;&#38395;&#26631;&#39064;&#21644;&#25991;&#31456;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#24230;&#35821;&#35328;&#19978;&#65292;&#21253;&#25324;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#21345;&#32435;&#36798;&#35821;&#12289;&#22885;&#37324;&#20122;&#35821;&#12289;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#21644;&#26049;&#36974;&#26222;&#35821;&#12290;&#27599;&#20010;&#26032;&#38395;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;L3Cube-IndicNews&#25552;&#20379;&#20102;3&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#25991;&#26723;&#38271;&#24230;&#36827;&#34892;&#20998;&#31867;&#65306;&#30701;&#26631;&#39064;&#20998;&#31867;&#65288;SHC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#26032;&#38395;&#26631;&#39064;&#21644;&#26032;&#38395;&#31867;&#21035;&#65292;&#38271;&#25991;&#26723;&#20998;&#31867;&#65288;LDC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#25972;&#20010;&#26032;&#38395;&#25991;&#31456;&#21644;&#26032;&#38395;&#31867;&#21035;&#65292;&#38271;&#27573;&#33853;&#20998;&#31867;&#65288;LPC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#26032;&#38395;&#30340;&#23376;&#25991;&#31456;&#21644;&#26032;&#38395;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;3&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#20445;&#25345;&#20102;&#19968;&#33268;&#30340;&#26631;&#31614;&#65292;&#20197;&#36827;&#34892;&#28145;&#20837;&#30340;&#22522;&#20110;&#38271;&#24230;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#25351;&#26631;&#23545;&#27599;&#20010;&#21360;&#24230;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles. We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi. Each of these news datasets comprises 10 or more classes of news articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category. We maintain consistent labeling across all 3 datasets for in-depth length-based analysis. We evaluate each of these Indic language datasets using 4 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31163;&#32447;&#35268;&#33539;&#21270;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20197;&#21033;&#29992;&#31163;&#32447;&#36712;&#36857;&#25968;&#25454;&#35757;&#32451;&#22810;&#30446;&#26631;&#25919;&#31574;&#12290;&#22312;&#38754;&#23545;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#28436;&#31034;&#38382;&#39064;&#26102;&#65292;&#25552;&#20986;&#20102;&#36807;&#28388;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#20559;&#22909;&#26465;&#20214;&#21270;&#26631;&#37327;&#21270;&#26356;&#26032;&#19982;&#25919;&#31574;&#35268;&#33539;&#21270;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.02244</link><description>&lt;p&gt;
&#25919;&#31574;&#35268;&#33539;&#21270;&#30340;&#31163;&#32447;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Policy-regularized Offline Multi-objective Reinforcement Learning. (arXiv:2401.02244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31163;&#32447;&#35268;&#33539;&#21270;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20197;&#21033;&#29992;&#31163;&#32447;&#36712;&#36857;&#25968;&#25454;&#35757;&#32451;&#22810;&#30446;&#26631;&#25919;&#31574;&#12290;&#22312;&#38754;&#23545;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#28436;&#31034;&#38382;&#39064;&#26102;&#65292;&#25552;&#20986;&#20102;&#36807;&#28388;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#20559;&#22909;&#26465;&#20214;&#21270;&#26631;&#37327;&#21270;&#26356;&#26032;&#19982;&#25919;&#31574;&#35268;&#33539;&#21270;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#20165;&#20351;&#29992;&#31163;&#32447;&#36712;&#36857;&#25968;&#25454;&#26469;&#35757;&#32451;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#25919;&#31574;&#12290;&#25105;&#20204;&#23558;&#24191;&#27867;&#37319;&#29992;&#30340;&#29992;&#20110;&#21333;&#30446;&#26631;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#31163;&#32447;&#35268;&#33539;&#21270;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#35774;&#32622;&#65292;&#20197;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#31163;&#32447;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#65292;&#21363;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#28436;&#31034;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#36817;&#20284;&#34892;&#20026;&#20559;&#22909;&#26469;&#36807;&#28388;&#20986;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#28436;&#31034;&#65292;&#21644;2&#65289;&#37319;&#29992;&#20855;&#26377;&#39640;&#31574;&#30053;&#34920;&#36798;&#33021;&#21147;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20559;&#22909;&#26465;&#20214;&#21270;&#26631;&#37327;&#21270;&#26356;&#26032;&#26041;&#27861;&#34701;&#20837;&#21040;&#25919;&#31574;&#35268;&#33539;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20197;&#20351;&#29992;&#21333;&#20010;&#31574;&#30053;&#32593;&#32476;&#21516;&#26102;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#20174;&#32780;&#20943;&#23569;&#20026;&#21508;&#31181;&#20559;&#22909;&#35757;&#32451;&#22823;&#37327;&#20010;&#20307;&#31574;&#30053;&#25152;&#20135;&#29983;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#26435;&#37325;&#35843;&#25972;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to utilize only offline trajectory data to train a policy for multi-objective RL. We extend the offline policy-regularized method, a widely-adopted approach for single-objective offline RL problems, into the multi-objective setting in order to achieve the above goal. However, such methods face a new challenge in offline MORL settings, namely the preference-inconsistent demonstration problem. We propose two solutions to this problem: 1) filtering out preference-inconsistent demonstrations via approximating behavior preferences, and 2) adopting regularization techniques with high policy expressiveness. Moreover, we integrate the preference-conditioned scalarized update method into policy-regularized offline RL, in order to simultaneously learn a set of policies using a single policy network, thus reducing the computational cost induced by the training of a large number of individual policies for various preferences. Finally, we introduce Regularization Weight Adapta
&lt;/p&gt;</description></item><item><title>U-Mixer&#26159;&#19968;&#31181;&#32467;&#21512;Unet&#21644;Mixer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#26657;&#27491;&#26041;&#27861;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#38750;&#31283;&#24577;&#25361;&#25112;&#65292;&#25429;&#25417;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21512;&#24182;&#29305;&#24449;&#20197;&#33719;&#21462;&#20840;&#38754;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.02236</link><description>&lt;p&gt;
U-Mixer: &#19968;&#31181;&#24102;&#26377;&#31283;&#23450;&#24615;&#26657;&#27491;&#30340;Unet-Mixer&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting. (arXiv:2401.02236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02236
&lt;/p&gt;
&lt;p&gt;
U-Mixer&#26159;&#19968;&#31181;&#32467;&#21512;Unet&#21644;Mixer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#26657;&#27491;&#26041;&#27861;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#38750;&#31283;&#24577;&#25361;&#25112;&#65292;&#25429;&#25417;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21512;&#24182;&#29305;&#24449;&#20197;&#33719;&#21462;&#20840;&#38754;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30001;&#20110;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#25110;&#19981;&#35268;&#21017;&#27874;&#21160;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#21576;&#29616;&#38750;&#31283;&#24577;&#12290;&#36825;&#38459;&#30861;&#20102;&#36890;&#36807;&#28145;&#23618;&#27425;&#30340;&#31283;&#23450;&#29305;&#24449;&#20256;&#25773;&#65292;&#30772;&#22351;&#20102;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21464;&#24471;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#22312;&#25429;&#25417;&#28508;&#22312;&#27169;&#24335;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;&#21517;&#20026;U-Mixer&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#38750;&#31283;&#24577;&#25361;&#25112;&#12290;&#36890;&#36807;&#32467;&#21512;Unet&#21644;Mixer&#65292;U-Mixer&#26377;&#25928;&#22320;&#25429;&#25417;&#19981;&#21516;&#22359;&#21644;&#36890;&#36947;&#20043;&#38388;&#30340;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#36890;&#36947;&#20043;&#38388;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21512;&#24182;&#20302;&#23618;&#21644;&#39640;&#23618;&#29305;&#24449;&#20197;&#33719;&#21462;&#20840;&#38754;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#20854;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31283;&#23450;&#24615;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#24046;&#24322;&#26469;&#26126;&#30830;&#24674;&#22797;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is a crucial task in various domains. Caused by factors such as trends, seasonality, or irregular fluctuations, time series often exhibits non-stationary. It obstructs stable feature propagation through deep layers, disrupts feature distributions, and complicates learning data distribution changes. As a result, many existing models struggle to capture the underlying patterns, leading to degraded forecasting performance. In this study, we tackle the challenge of non-stationarity in time series forecasting with our proposed framework called U-Mixer. By combining Unet and Mixer, U-Mixer effectively captures local temporal dependencies between different patches and channels separately to avoid the influence of distribution variations among channels, and merge low- and high-levels features to obtain comprehensive data representations. The key contribution is a novel stationarity correction method, explicitly restoring data distribution by constraining the difference 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.02225</link><description>&lt;p&gt;
&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trajectory-Oriented Policy Optimization with Sparse Rewards. (arXiv:2401.02225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#31232;&#30095;&#22870;&#21169;&#36890;&#24120;&#21482;&#34920;&#31034;&#20219;&#21153;&#26159;&#21542;&#37096;&#20998;&#25110;&#23436;&#20840;&#23436;&#25104;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#20195;&#29702;&#33719;&#24471;&#26377;&#29992;&#21453;&#39304;&#20043;&#21069;&#24517;&#39035;&#25191;&#34892;&#35768;&#22810;&#25506;&#32034;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;DRL&#31639;&#27861;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#23398;&#20064;&#21487;&#34892;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#24555;&#36895;&#21644;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#35270;&#20026;&#25351;&#23548;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#20351;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#19982;&#31163;&#32447;&#31034;&#33539;&#30456;&#21305;&#37197;&#30340;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;(MMD)&#30340;&#36712;&#36857;&#36317;&#31163;&#65292;&#24182;&#23558;&#31574;&#30053;&#20248;&#21270;&#24314;&#27169;&#20026;&#19968;&#20010;&#21463;&#36317;&#31163;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) remains challenging in tasks with sparse rewards. These sparse rewards often only indicate whether the task is partially or fully completed, meaning that many exploration actions must be performed before the agent obtains useful feedback. Hence, most existing DRL algorithms fail to learn feasible policies within a reasonable time frame. To overcome this problem, we develop an approach that exploits offline demonstration trajectories for faster and more efficient online RL in sparse reward settings. Our key insight is that by regarding offline demonstration trajectories as guidance, instead of imitating them, our method learns a policy whose state-action visitation marginal distribution matches that of offline demonstrations. Specifically, we introduce a novel trajectory distance based on maximum mean discrepancy (MMD) and formulate policy optimization as a distance-constrained optimization problem. Then, we show that this distance-constrained optimizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#21464;&#37327;t&#20998;&#24067;&#30340;&#40065;&#26834;&#21452;&#32447;&#24615;&#22240;&#23376;&#20998;&#26512;&#65288;tbfa&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37325;&#23614;&#25110;&#21463;&#27745;&#26579;&#30340;&#30697;&#38453;&#25968;&#25454;&#20013;&#21516;&#26102;&#25552;&#21462;&#34892;&#21644;&#21015;&#21464;&#37327;&#30340;&#20844;&#20849;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.02203</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#21464;&#37327;t&#20998;&#24067;&#30340;&#40065;&#26834;&#21452;&#32447;&#24615;&#22240;&#23376;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Robust bilinear factor analysis based on the matrix-variate $t$ distribution. (arXiv:2401.02203v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#21464;&#37327;t&#20998;&#24067;&#30340;&#40065;&#26834;&#21452;&#32447;&#24615;&#22240;&#23376;&#20998;&#26512;&#65288;tbfa&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37325;&#23614;&#25110;&#21463;&#27745;&#26579;&#30340;&#30697;&#38453;&#25968;&#25454;&#20013;&#21516;&#26102;&#25552;&#21462;&#34892;&#21644;&#21015;&#21464;&#37327;&#30340;&#20844;&#20849;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#20803;t&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#26512;&#65288;tfa&#65289;&#26159;&#19968;&#31181;&#22312;&#37325;&#23614;&#25110;&#21463;&#27745;&#26579;&#25968;&#25454;&#19978;&#25552;&#21462;&#20844;&#20849;&#22240;&#23376;&#30340;&#26377;&#29992;&#30340;&#40065;&#26834;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;tfa&#21482;&#36866;&#29992;&#20110;&#21521;&#37327;&#25968;&#25454;&#12290;&#24403;tfa&#24212;&#29992;&#20110;&#30697;&#38453;&#25968;&#25454;&#26102;&#65292;&#36890;&#24120;&#20250;&#20808;&#23558;&#30697;&#38453;&#35266;&#27979;&#21521;&#37327;&#21270;&#12290;&#36825;&#24102;&#26469;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;i&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#30697;&#38453;&#32467;&#26500;&#34987;&#30772;&#22351;&#65292;&#65288;ii&#65289;&#40065;&#26834;&#24615;&#21487;&#33021;&#20002;&#22833;&#65292;&#22240;&#20026;&#21521;&#37327;&#21270;&#30340;&#30697;&#38453;&#25968;&#25454;&#36890;&#24120;&#20250;&#23548;&#33268;&#36739;&#39640;&#30340;&#25968;&#25454;&#32500;&#24230;&#65292;&#36825;&#23481;&#26131;&#23548;&#33268;tfa&#30340;&#23849;&#28291;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#30697;&#38453;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#30697;&#38453;&#21464;&#37327;t&#20998;&#24067;&#30340;&#21452;&#32447;&#24615;&#22240;&#23376;&#20998;&#26512;&#65288;tbfa&#65289;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23427;&#33021;&#22815;&#21516;&#26102;&#23545;&#24863;&#20852;&#36259;&#30340;&#34892;&#21644;&#21015;&#21464;&#37327;&#20174;&#37325;&#23614;&#25110;&#21463;&#27745;&#26579;&#30340;&#30697;&#38453;&#25968;&#25454;&#20013;&#25552;&#21462;&#20844;&#20849;&#22240;&#23376;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#27714;&#35299;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factor Analysis based on multivariate $t$ distribution ($t$fa) is a useful robust tool for extracting common factors on heavy-tailed or contaminated data. However, $t$fa is only applicable to vector data. When $t$fa is applied to matrix data, it is common to first vectorize the matrix observations. This introduces two challenges for $t$fa: (i) the inherent matrix structure of the data is broken, and (ii) robustness may be lost, as vectorized matrix data typically results in a high data dimension, which could easily lead to the breakdown of $t$fa. To address these issues, starting from the intrinsic matrix structure of matrix data, a novel robust factor analysis model, namely bilinear factor analysis built on the matrix-variate $t$ distribution ($t$bfa), is proposed in this paper. The novelty is that it is capable to simultaneously extract common factors for both row and column variables of interest on heavy-tailed or contaminated matrix data. Two efficient algorithms for maximum likeli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#20998;&#31867;&#23454;&#26102;&#30340;&#36710;&#36733;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20197;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#27700;&#24179;&#21644;&#24773;&#22659;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.02199</link><description>&lt;p&gt;
LADRI: &#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#39118;&#38505;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System. (arXiv:2401.02199v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#20998;&#31867;&#23454;&#26102;&#30340;&#36710;&#36733;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20197;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#27700;&#24179;&#21644;&#24773;&#22659;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#30340;&#28436;&#36827;&#65292;&#26234;&#33021;&#20132;&#36890;&#30340;&#35270;&#37326;&#19981;&#26029;&#25193;&#22823;&#65292;&#30830;&#20445;&#26497;&#20854;&#23433;&#20840;&#21464;&#24471;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#20026;&#36843;&#20999;&#12290;&#20256;&#32479;&#30340;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#20154;&#24037;&#39550;&#39542;&#30340;&#36710;&#36742;&#65292;&#26080;&#27861;&#20805;&#20998;&#36866;&#24212;ADS&#22810;&#26041;&#38754;&#12289;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#23454;&#26102;&#21160;&#24577;&#39118;&#38505;&#35780;&#20272;&#65288;DRA&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#31361;&#30772;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26680;&#24515;&#37096;&#20998;&#8212;&#8212;ANN&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#20998;&#31867;&#23454;&#26102;&#30340;&#36710;&#36733;&#20256;&#24863;&#22120;&#65288;OBS&#65289;&#25968;&#25454;&#26469;&#32454;&#33268;&#22320;&#20998;&#26512;&#21644;&#20998;&#31867;&#39118;&#38505;&#32500;&#24230;&#12290;&#36825;&#31181;&#20197;&#23398;&#20064;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#19981;&#20165;&#25552;&#21319;&#20102;ADS&#30340;&#24773;&#22659;&#24847;&#35782;&#65292;&#36824;&#20016;&#23500;&#20102;&#20854;&#23545;&#21363;&#26102;&#36816;&#34892;&#29615;&#22659;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#20998;&#26512;OBS&#25968;&#25454;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20854;&#24403;&#21069;&#30340;&#39118;&#38505;&#37197;&#32622;&#25991;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20056;&#23458;&#21644;&#26356;&#24191;&#27867;&#26053;&#36884;&#20013;&#30340;&#23433;&#20840;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the horizon of intelligent transportation expands with the evolution of Automated Driving Systems (ADS), ensuring paramount safety becomes more imperative than ever. Traditional risk assessment methodologies, primarily crafted for human-driven vehicles, grapple to adequately adapt to the multifaceted, evolving environments of ADS. This paper introduces a framework for real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of Artificial Neural Networks (ANNs).  Our proposed solution transcends these limitations, drawing upon ANNs, a cornerstone of deep learning, to meticulously analyze and categorize risk dimensions using real-time On-board Sensor (OBS) data. This learning-centric approach not only elevates the ADS's situational awareness but also enriches its understanding of immediate operational contexts. By dissecting OBS data, the system is empowered to pinpoint its current risk profile, thereby enhancing safety prospects for onboard passengers and the broader tr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#24635;&#32467;&#20102;NODE21&#25361;&#25112;&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#20102;&#21512;&#25104;&#29983;&#25104;&#30340;&#32467;&#33410;&#35757;&#32451;&#22270;&#20687;&#23545;&#26816;&#27979;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.02192</link><description>&lt;p&gt;
&#33016;&#37096;X&#23556;&#32447;&#19978;&#32467;&#33410;&#30340;&#26816;&#27979;&#19982;&#29983;&#25104;: NODE21&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Nodule detection and generation on chest X-rays: NODE21 Challenge. (arXiv:2401.02192v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02192
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#24635;&#32467;&#20102;NODE21&#25361;&#25112;&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#20102;&#21512;&#25104;&#29983;&#25104;&#30340;&#32467;&#33410;&#35757;&#32451;&#22270;&#20687;&#23545;&#26816;&#27979;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#32467;&#33410;&#21487;&#33021;&#26159;&#32954;&#30284;&#30340;&#26089;&#26399;&#34920;&#29616;&#65292;&#26159;&#30007;&#24615;&#21644;&#22899;&#24615;&#20013;&#23548;&#33268;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#23454;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#20013;&#26816;&#27979;&#32954;&#32467;&#33410;&#26041;&#38754;&#33719;&#24471;&#39640;&#24615;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#40644;&#37329;&#26631;&#20934;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#38459;&#27490;&#20102;&#23545;&#35813;&#20219;&#21153;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;&#19968;&#20010;&#20844;&#20849;&#30740;&#31350;&#25361;&#25112;&#65292;NODE21&#65292;&#26088;&#22312;&#26816;&#27979;&#19982;&#29983;&#25104;&#33016;&#37096;X&#23556;&#32447;&#20013;&#30340;&#32954;&#32467;&#33410;&#12290;&#26816;&#27979;&#37096;&#20998;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#33410;&#26816;&#27979;&#31995;&#32479;&#65292;&#29983;&#25104;&#37096;&#20998;&#30830;&#23450;&#20102;&#32467;&#33410;&#29983;&#25104;&#31639;&#27861;&#22312;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#24182;&#25913;&#21892;&#26816;&#27979;&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;NODE21&#25361;&#25112;&#30340;&#32467;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39069;&#22806;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;&#21512;&#25104;&#29983;&#25104;&#30340;&#32467;&#33410;&#35757;&#32451;&#22270;&#20687;&#23545;&#26816;&#27979;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary nodules may be an early manifestation of lung cancer, the leading cause of cancer-related deaths among both men and women. Numerous studies have established that deep learning methods can yield high-performance levels in the detection of lung nodules in chest X-rays. However, the lack of gold-standard public datasets slows down the progression of the research and prevents benchmarking of methods for this task. To address this, we organized a public research challenge, NODE21, aimed at the detection and generation of lung nodules in chest X-rays. While the detection track assesses state-of-the-art nodule detection systems, the generation track determines the utility of nodule generation algorithms to augment training data and hence improve the performance of the detection systems. This paper summarizes the results of the NODE21 challenge and performs extensive additional experiments to examine the impact of the synthetically generated nodule training images on the detection al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#20844;&#24179;&#24615;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;FairGridSearch&#65292;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#32452;&#21512;&#24182;&#25512;&#33616;&#26368;&#20339;&#32452;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#20934;&#30830;&#24230;&#21644;&#20844;&#24179;&#24615;&#24230;&#37327;&#23545;&#27169;&#22411;&#35780;&#20272;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.02183</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#22686;&#24378;&#27169;&#22411;&#27604;&#36739;&#30340;&#26694;&#26550;: FairGridSearch
&lt;/p&gt;
&lt;p&gt;
FairGridSearch: A Framework to Compare Fairness-Enhancing Models. (arXiv:2401.02183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#20844;&#24179;&#24615;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;FairGridSearch&#65292;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#32452;&#21512;&#24182;&#25512;&#33616;&#26368;&#20339;&#32452;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#20934;&#30830;&#24230;&#21644;&#20844;&#24179;&#24615;&#24230;&#37327;&#23545;&#27169;&#22411;&#35780;&#20272;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#20915;&#31574;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#22797;&#21046;&#25110;&#29978;&#33267;&#25918;&#22823;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#23384;&#22312;&#21508;&#31181;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#21644;&#22522;&#26412;&#20272;&#35745;&#22120;&#65292;&#20294;&#36873;&#25321;&#29305;&#23450;&#24212;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#20108;&#20803;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27604;&#36739;&#20844;&#24179;&#24615;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;FairGridSearch&#12290;FairGridSearch&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#32452;&#21512;&#65292;&#24182;&#25512;&#33616;&#26368;&#20339;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#23558;FairGridSearch&#24212;&#29992;&#20110;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;(&#25104;&#24180;&#20154;&#12289;COMPAS&#21644;&#24503;&#22269;&#20449;&#29992;)&#65292;&#24182;&#20998;&#26512;&#24230;&#37327;&#36873;&#25321;&#12289;&#22522;&#26412;&#20272;&#35745;&#22120;&#36873;&#25321;&#21644;&#20998;&#31867;&#38408;&#20540;&#23545;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#36873;&#25321;&#21512;&#36866;&#30340;&#20934;&#30830;&#24230;&#21644;&#20844;&#24179;&#24615;&#24230;&#37327;&#23545;&#27169;&#22411;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;&#22522;&#26412;&#20272;&#35745;&#22120;&#21644;&#20998;&#31867;&#38408;&#20540;&#20063;&#20250;&#23545;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are increasingly used in critical decision-making applications. However, these models are susceptible to replicating or even amplifying bias present in real-world data. While there are various bias mitigation methods and base estimators in the literature, selecting the optimal model for a specific application remains challenging.  This paper focuses on binary classification and proposes FairGridSearch, a novel framework for comparing fairness-enhancing models. FairGridSearch enables experimentation with different model parameter combinations and recommends the best one. The study applies FairGridSearch to three popular datasets (Adult, COMPAS, and German Credit) and analyzes the impacts of metric selection, base estimator choice, and classification threshold on model fairness.  The results highlight the significance of selecting appropriate accuracy and fairness metrics for model evaluation. Additionally, different base estimators and classification threshold va
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26550;&#26500;&#26469;&#35299;&#20915;&#36328;&#24179;&#21488;&#25968;&#25454;&#20013;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;&#32422;&#26463;&#26465;&#20214;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02154</link><description>&lt;p&gt;
&#36328;&#24179;&#21488;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Disentangle Estimation of Causal Effects from Cross-Silo Data. (arXiv:2401.02154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02154
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26550;&#26500;&#26469;&#35299;&#20915;&#36328;&#24179;&#21488;&#25968;&#25454;&#20013;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;&#32422;&#26463;&#26465;&#20214;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#33647;&#29289;&#30740;&#21457;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#20272;&#35745;&#19981;&#21516;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#25968;&#25454;&#29305;&#24449;&#21487;&#33021;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#24179;&#21488;&#19978;&#65292;&#24182;&#19988;&#22312;&#21508;&#26041;&#20043;&#38388;&#20445;&#25345;&#31169;&#23494;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30452;&#25509;&#20449;&#24687;&#20132;&#27969;&#12290;&#36825;&#21453;&#36807;&#26469;&#21487;&#33021;&#23548;&#33268;&#23616;&#37096;&#22240;&#26524;&#25928;&#24212;&#30340;&#20272;&#35745;&#23384;&#22312;&#20559;&#24046;&#65292;&#20381;&#36182;&#20110;&#20165;&#23376;&#38598;&#21327;&#21464;&#37327;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#32806;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#20849;&#20139;&#21644;&#31169;&#26377;&#20998;&#25903;&#30340;&#32452;&#21512;&#20419;&#36827;&#27169;&#22411;&#21442;&#25968;&#30340;&#26080;&#32541;&#36328;&#24179;&#21488;&#20256;&#36882;&#65292;&#20016;&#23500;&#22240;&#26524;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#23616;&#32422;&#26463;&#26465;&#20214;&#26469;&#26377;&#25928;&#20943;&#36731;&#21508;&#20010;&#32570;&#22833;&#22495;&#20869;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25105;&#20204;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26032;&#30340;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects among different events is of great importance to critical fields such as drug development. Nevertheless, the data features associated with events may be distributed across various silos and remain private within respective parties, impeding direct information exchange between them. This, in turn, can result in biased estimations of local causal effects, which rely on the characteristics of only a subset of the covariates. To tackle this challenge, we introduce an innovative disentangle architecture designed to facilitate the seamless cross-silo transmission of model parameters, enriched with causal mechanisms, through a combination of shared and private branches. Besides, we introduce global constraints into the equation to effectively mitigate bias within the various missing domains, thereby elevating the accuracy of our causal effect estimation. Extensive experiments conducted on new semi-synthetic datasets show that our method outperforms state-of-the-art b
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23545;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36798;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#33021;&#21147;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;TDL&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#32508;&#36848;&#23545;GNN4TDL&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#28436;&#21270;&#39046;&#22495;&#30340;&#27934;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.02143</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#24102;&#26377;&#20998;&#31867;&#21644;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions. (arXiv:2401.02143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23545;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36798;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#33021;&#21147;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;TDL&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#32508;&#36848;&#23545;GNN4TDL&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#28436;&#21270;&#39046;&#22495;&#30340;&#27934;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#65288;TDL&#65289;&#30340;&#39046;&#22495;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#35813;&#32508;&#36848;&#31361;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;TDL&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65306;&#25968;&#25454;&#23454;&#20363;&#21644;&#29305;&#24449;&#20540;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#30340;&#34920;&#36848;&#19981;&#36275;&#12290;GNN&#20197;&#20854;&#22825;&#28982;&#33021;&#21147;&#26469;&#27169;&#25311;&#34920;&#26684;&#25968;&#25454;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#21508;&#31181;TDL&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20852;&#36259;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#35774;&#35745;&#21644;&#23454;&#29616;GNN&#29992;&#20110;TDL&#65288;GNN4TDL&#65289;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#23427;&#21253;&#25324;&#23545;&#22522;&#30784;&#38382;&#39064;&#30340;&#35814;&#32454;&#30740;&#31350;&#21644;&#22522;&#20110;GNN&#30340;TDL&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#20026;&#20854;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#27880;&#26500;&#24314;&#22270;&#32467;&#26500;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#20840;&#38754;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural Networks (GNNs), a domain where deep learning-based approaches have increasingly shown superior performance in both classification and regression tasks compared to traditional methods. The survey highlights a critical gap in deep neural TDL methods: the underrepresentation of latent correlations among data instances and feature values. GNNs, with their innate capability to model intricate relationships and interactions between diverse elements of tabular data, have garnered significant interest and application across various TDL domains. Our survey provides a systematic review of the methods involved in designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed investigation into the foundational aspects and an overview of GNN-based TDL methods, offering insights into their evolving landscape. We present a comprehensive taxonomy focused on constructing graph structures and representation learn
&lt;/p&gt;</description></item><item><title>PosCUDA&#26159;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#26080;&#27861;&#23398;&#20064;&#30340;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#20301;&#32622;&#21367;&#31215;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23567;&#38899;&#39057;&#22359;&#19978;&#20351;&#29992;&#25353;&#31867;&#21035;&#30340;&#21367;&#31215;&#21644;&#31169;&#38053;&#25511;&#21046;&#22359;&#30340;&#20301;&#32622;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#30340;&#26080;&#27861;&#23398;&#20064;&#65292;&#24182;&#20445;&#25345;&#38899;&#39057;&#25968;&#25454;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.02135</link><description>&lt;p&gt;
PosCUDA: &#29992;&#20110;&#26080;&#27861;&#23398;&#20064;&#30340;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#20301;&#32622;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
PosCUDA: Position based Convolution for Unlearnable Audio Datasets. (arXiv:2401.02135v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02135
&lt;/p&gt;
&lt;p&gt;
PosCUDA&#26159;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#26080;&#27861;&#23398;&#20064;&#30340;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#20301;&#32622;&#21367;&#31215;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23567;&#38899;&#39057;&#22359;&#19978;&#20351;&#29992;&#25353;&#31867;&#21035;&#30340;&#21367;&#31215;&#21644;&#31169;&#38053;&#25511;&#21046;&#22359;&#30340;&#20301;&#32622;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#30340;&#26080;&#27861;&#23398;&#20064;&#65292;&#24182;&#20445;&#25345;&#38899;&#39057;&#25968;&#25454;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#24178;&#20928;&#30340;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36991;&#20813;&#26114;&#36149;&#30340;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#20016;&#23500;&#30340;&#25968;&#25454;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#22312;&#26410;&#32463;&#25480;&#26435;&#30340;&#24773;&#20917;&#19979;&#28389;&#29992;&#20010;&#20154;&#25968;&#25454;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#37325;&#22823;&#38544;&#31169;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914;CUDA&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#28155;&#21152;&#25353;&#31867;&#21035;&#27169;&#31946;&#22788;&#29702;&#26469;&#20351;&#25968;&#25454;&#38598;&#26080;&#27861;&#23398;&#20064;&#65292;&#21363;&#27169;&#22411;&#27704;&#36828;&#26080;&#27861;&#20351;&#29992;&#24050;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#38477;&#20302;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20351;&#20854;&#23545;&#23454;&#38469;&#24212;&#29992;&#26080;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PosCUDA&#65292;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#26080;&#27861;&#23398;&#20064;&#30340;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#20301;&#32622;&#21367;&#31215;&#12290;PosCUDA&#22312;&#23567;&#38899;&#39057;&#22359;&#19978;&#20351;&#29992;&#25353;&#31867;&#21035;&#30340;&#21367;&#31215;&#12290;&#22359;&#30340;&#20301;&#32622;&#22522;&#20110;&#27599;&#20010;&#31867;&#21035;&#30340;&#31169;&#38053;&#65292;&#22240;&#27492;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20301;&#32622;&#27169;&#31946;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PosCUDA&#21487;&#20197;&#23454;&#29616;&#26080;&#27861;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#32500;&#25345;&#38899;&#39057;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models require large amounts of clean data to acheive good performance. To avoid the cost of expensive data acquisition, researchers use the abundant data available on the internet. This raises significant privacy concerns on the potential misuse of personal data for model training without authorisation. Recent works such as CUDA propose solutions to this problem by adding class-wise blurs to make datasets unlearnable, i.e a model can never use the acquired dataset for learning. However these methods often reduce the quality of the data making it useless for practical applications. We introduce PosCUDA, a position based convolution for creating unlearnable audio datasets. PosCUDA uses class-wise convolutions on small patches of audio. The location of the patches are based on a private key for each class, hence the model learns the relations between positional blurs and labels, while failing to generalize. We empirically show that PosCUDA can achieve unlearnability while m
&lt;/p&gt;</description></item><item><title>ACP-ESM&#26159;&#19968;&#20010;&#20351;&#29992;&#20197;&#34507;&#30333;&#36136;&#20026;&#23548;&#21521;&#30340;Transformer&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#25239;&#30284;&#32957;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#25239;&#30284;&#32957;&#30340;&#31283;&#23450;&#24615;&#65292;&#25913;&#21892;&#36873;&#25321;&#24615;&#65292;&#24182;&#25552;&#39640;&#23545;&#30284;&#32454;&#32990;&#30340;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02124</link><description>&lt;p&gt;
ACP-ESM:&#19968;&#31181;&#20351;&#29992;&#20197;&#34507;&#30333;&#36136;&#20026;&#23548;&#21521;&#30340;Transformer&#26041;&#27861;&#36827;&#34892;&#25239;&#30284;&#32957;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach. (arXiv:2401.02124v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02124
&lt;/p&gt;
&lt;p&gt;
ACP-ESM&#26159;&#19968;&#20010;&#20351;&#29992;&#20197;&#34507;&#30333;&#36136;&#20026;&#23548;&#21521;&#30340;Transformer&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#25239;&#30284;&#32957;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#25239;&#30284;&#32957;&#30340;&#31283;&#23450;&#24615;&#65292;&#25913;&#21892;&#36873;&#25321;&#24615;&#65292;&#24182;&#25552;&#39640;&#23545;&#30284;&#32454;&#32990;&#30340;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#30284;&#32957;(ACPs)&#26159;&#19968;&#31867;&#22312;&#30284;&#30151;&#30740;&#31350;&#21644;&#27835;&#30103;&#39046;&#22495;&#24341;&#36215;&#37325;&#35270;&#30340;&#20998;&#23376;&#12290; ACPs&#26159;&#30001;&#27688;&#22522;&#37240;&#26500;&#25104;&#30340;&#30701;&#38142;&#65292;&#26159;&#34507;&#30333;&#36136;&#30340;&#26500;&#24314;&#22522;&#22359;&#65292;&#24182;&#20855;&#26377;&#36873;&#25321;&#24615;&#38774;&#21521;&#21644;&#26432;&#27515;&#30284;&#32454;&#32990;&#30340;&#33021;&#21147;&#12290; ACPs&#20043;&#25152;&#20197;&#20855;&#26377;&#38024;&#23545;&#30284;&#32454;&#32990;&#30340;&#36873;&#25321;&#24615;&#65292;&#24448;&#24448;&#24402;&#22240;&#20110;&#30284;&#32454;&#32990;&#34920;&#38754;&#29305;&#24615;&#19982;&#27491;&#24120;&#32454;&#32990;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;ACP&#34987;&#35748;&#20026;&#26159;&#30284;&#30151;&#27835;&#30103;&#30340;&#28508;&#22312;&#20505;&#36873;&#33647;&#29289;&#12290;ACPs&#21487;&#20197;&#21333;&#29420;&#20351;&#29992;&#25110;&#19982;&#21270;&#30103;&#21644;&#25918;&#30103;&#31561;&#27835;&#30103;&#26041;&#24335;&#32852;&#21512;&#20351;&#29992;&#12290;&#34429;&#28982;ACP&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30284;&#30151;&#27835;&#30103;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#20811;&#26381;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#20248;&#21270;&#20854;&#31283;&#23450;&#24615;&#65292;&#25913;&#21892;&#36873;&#25321;&#24615;&#65292;&#25552;&#39640;&#23545;&#30284;&#32454;&#32990;&#30340;&#20256;&#36882;&#24615;&#65292;&#24182;&#19988;&#24212;&#23545;&#32957;&#24207;&#21015;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, develo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31227;&#21160;ALOHA&#31995;&#32479;&#65292;&#29992;&#20110;&#23398;&#20064;&#21452;&#25163;&#31227;&#21160;&#25805;&#20316;&#12290;&#36890;&#36807;&#20302;&#25104;&#26412;&#30340;&#36828;&#31243;&#25805;&#20316;&#21644;&#25972;&#20307;&#36523;&#20307;&#25511;&#21046;&#65292;&#31995;&#32479;&#33021;&#22815;&#23436;&#25104;&#22797;&#26434;&#30340;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25552;&#39640;&#25104;&#21151;&#29575;&#36798;&#21040;90%&#12290;</title><link>http://arxiv.org/abs/2401.02117</link><description>&lt;p&gt;
&#31227;&#21160;ALOHA&#65306;&#20302;&#25104;&#26412;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#23398;&#20064;&#21452;&#25163;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation. (arXiv:2401.02117v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31227;&#21160;ALOHA&#31995;&#32479;&#65292;&#29992;&#20110;&#23398;&#20064;&#21452;&#25163;&#31227;&#21160;&#25805;&#20316;&#12290;&#36890;&#36807;&#20302;&#25104;&#26412;&#30340;&#36828;&#31243;&#25805;&#20316;&#21644;&#25972;&#20307;&#36523;&#20307;&#25511;&#21046;&#65292;&#31995;&#32479;&#33021;&#22815;&#23436;&#25104;&#22797;&#26434;&#30340;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25552;&#39640;&#25104;&#21151;&#29575;&#36798;&#21040;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#28436;&#31034;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30340;&#32467;&#26524;&#38598;&#20013;&#22312;&#26700;&#38754;&#25805;&#20316;&#19978;&#65292;&#32570;&#20047;&#23545;&#20110;&#36890;&#24120;&#26377;&#29992;&#20219;&#21153;&#25152;&#38656;&#35201;&#30340;&#31227;&#21160;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#20223;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#21452;&#25163;&#25805;&#20316;&#19988;&#38656;&#35201;&#20840;&#36523;&#25511;&#21046;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#31227;&#21160;ALOHA&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#21644;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;&#12290;&#23427;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#31227;&#21160;&#24213;&#30424;&#21644;&#19968;&#20010;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#25509;&#21475;&#26469;&#22686;&#24378;ALOHA&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36890;&#36807;&#31227;&#21160;ALOHA&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#30417;&#30563;&#24335;&#34892;&#20026;&#20811;&#38534;&#65292;&#24182;&#21457;&#29616;&#19982;&#29616;&#26377;&#30340;&#38745;&#24577;ALOHA&#25968;&#25454;&#38598;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;50&#27425;&#28436;&#31034;&#65292;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#23558;&#25104;&#21151;&#29575;&#25552;&#39640;90%&#65292;&#20351;&#24471;&#31227;&#21160;ALOHA&#33021;&#22815;&#33258;&#20027;&#23436;&#25104;&#22797;&#26434;&#30340;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#28818;&#34430;&#21644;&#19978;&#33756;&#65292;&#24182;&#25171;&#24320;&#19968;&#20010;&#21452;&#38376;&#22721;&#26588;&#23384;&#25918;&#37325;&#22411;&#28921;&#39274;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooki
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#24449;&#20102;&#38217;&#38156;&#30898;&#21270;&#29289;&#65288;CZT&#65289;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#22312;&#35782;&#21035;&#21508;&#31181;&#32452;&#32455;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#25216;&#26415;&#26469;&#20811;&#26381;&#20256;&#32479;CT&#25506;&#27979;&#22120;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.02106</link><description>&lt;p&gt;
&#38024;&#23545;&#36719;&#32452;&#32455;&#25104;&#20687;&#30340;&#38217;&#38156;&#30898;&#21270;&#29289;(CZT)&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Cadmium Zinc Telluride (CZT) photon counting detector Characterisation for soft tissue imaging. (arXiv:2401.02106v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#24449;&#20102;&#38217;&#38156;&#30898;&#21270;&#29289;&#65288;CZT&#65289;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#22312;&#35782;&#21035;&#21508;&#31181;&#32452;&#32455;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#25216;&#26415;&#26469;&#20811;&#26381;&#20256;&#32479;CT&#25506;&#27979;&#22120;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20809;&#23376;&#35745;&#25968;&#26816;&#27979;&#25216;&#26415;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;X&#23556;&#32447;&#25104;&#20687;&#30740;&#31350;&#20852;&#36259;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#25195;&#25551;&#20202;&#21487;&#20197;&#20174;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#20013;&#21463;&#30410;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#26377;&#28508;&#21147;&#20811;&#26381;&#20256;&#32479;CT&#25506;&#27979;&#22120;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#30740;&#31350;&#22312;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#20013;&#24212;&#29992;&#21322;&#23548;&#20307;&#25506;&#27979;&#26448;&#26009;&#29992;&#20110;&#26816;&#27979;&#36719;&#32452;&#32455;&#23545;&#27604;&#24230;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#25935;&#24230;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#34920;&#24449;&#38217;&#38156;&#30898;&#21270;&#29289;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#22312;&#35782;&#21035;&#21508;&#31181;&#32452;&#32455;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;X&#23556;&#32447;&#31649;&#30005;&#21387;&#21644;&#30005;&#27969;&#20998;&#21035;&#35774;&#32622;&#20026;25 keV&#12289;35 keV&#21644;0.5 mA&#12289;1.0 mA&#65292;&#35780;&#20272;&#20102;CZT&#25506;&#27979;&#22120;&#30340;&#26368;&#20339;&#24103;&#36895;&#29575;&#65288;FPS&#65289;&#65292;&#36890;&#36807;&#20445;&#25345;&#26368;&#20339;FPS&#22266;&#23450;&#65292;&#23558;&#25506;&#27979;&#22120;&#33021;&#37327;&#38408;&#20540;&#20174;15 keV&#21040;35 keV&#35774;&#32622;&#20026;&#23567;&#27493;&#65292;&#23558;X&#23556;&#32447;&#31649;&#30340;&#30005;&#27969;&#35774;&#32622;&#20026;0.1 mA&#21040;1.0 mA&#30340;&#33539;&#22260;&#65292;&#20197;&#25214;&#21040;&#30005;&#21387;&#21644;&#30005;&#27969;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of photon counting detection technology has resulted in significant X-ray imaging research interest in recent years. Computed Tomography (CT) scanners can benefit from photon-counting detectors, which are new technology with the potential to overcome key limitations of conventional CT detectors. Researchers are still studying the effectiveness and sensitivity of semiconductor detector materials in photon counting detectors for detecting soft tissue contrasts. This study aimed to characterize the performance of the Cadmium Zinc Telluride photon counting detector in identifying various tissues. An optimal frame rate per second (FPS) of CZT detector was evaluated by setting the X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA respectively by keeping the optimum FPS fixed, the detector energy thresholds were set in small steps from 15 keV to 35 keV and the Currents were set for X-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between voltage and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#27969;&#27700;&#32447;&#24182;&#34892;&#20307;&#30340;&#20869;&#23384;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BPipe&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;BPipe&#22312;GPT-3&#27169;&#22411;&#19978;&#26377;&#25928;&#65292;&#20294;&#22312;LLaMA&#35757;&#32451;&#20013;&#24182;&#26410;&#33719;&#24471;&#30456;&#20284;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;BPipe&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#24615;&#33021;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;BPipe&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02088</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#20869;&#23384;&#24179;&#34913;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#20307;&#65306;BPipe
&lt;/p&gt;
&lt;p&gt;
Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe. (arXiv:2401.02088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#27969;&#27700;&#32447;&#24182;&#34892;&#20307;&#30340;&#20869;&#23384;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BPipe&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;BPipe&#22312;GPT-3&#27169;&#22411;&#19978;&#26377;&#25928;&#65292;&#20294;&#22312;LLaMA&#35757;&#32451;&#20013;&#24182;&#26410;&#33719;&#24471;&#30456;&#20284;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;BPipe&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#24615;&#33021;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;BPipe&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#27700;&#32447;&#24182;&#34892;&#20307;&#26159;&#35757;&#32451;&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#20869;&#23384;&#28040;&#32791;&#19978;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#20869;&#23384;&#21033;&#29992;&#19981;&#20805;&#20998;&#12290;BPipe&#25216;&#26415;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;GPT-3&#27169;&#22411;&#19978;&#35777;&#26126;&#20102;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;LLaMA&#35757;&#32451;&#20013;&#26410;&#33719;&#24471;&#31867;&#20284;&#30340;&#22909;&#22788;&#12290;&#27492;&#22806;&#65292;&#22312;&#24212;&#29992;flash attention&#26102;&#65292;BPipe&#22312;GPT-3&#35757;&#32451;&#20013;&#21482;&#24102;&#26469;&#24494;&#19981;&#36275;&#36947;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;BPipe&#22312;GPT-3&#21644;LLaMA&#19978;&#24615;&#33021;&#24046;&#24322;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;BPipe&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pipeline parallelism is an essential technique in the training of large-scale Transformer models. However, it suffers from imbalanced memory consumption, leading to insufficient memory utilization. The BPipe technique was proposed to address this issue and has proven effective in the GPT-3 model. Nevertheless, our experiments have not yielded similar benefits for LLaMA training. Additionally, BPipe only yields negligible benefits for GPT-3 training when applying flash attention. We analyze the underlying causes of the divergent performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel method to estimate the performance of BPipe.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#22270;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#35270;&#22270;&#21644;&#22270;&#27169;&#24335;&#26469;&#35299;&#37322;&#29305;&#23450;&#31867;&#21035;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02086</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
View-based Explanations for Graph Neural Networks. (arXiv:2401.02086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02086
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#22270;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#35270;&#22270;&#21644;&#22270;&#27169;&#24335;&#26469;&#35299;&#37322;&#29305;&#23450;&#31867;&#21035;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#35299;&#37322;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22270;&#20998;&#31867;&#31561;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26088;&#22312;&#29702;&#35299;GNNs&#30340;&#25972;&#20307;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#33021;&#36820;&#22238;&#38590;&#20197;&#35775;&#38382;&#25110;&#30452;&#25509;&#26597;&#35810;&#30340;&#35299;&#37322;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;GVEX&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#35299;&#37322;&#30340;&#22270;&#35270;&#22270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#23618;&#30340;&#35299;&#37322;&#32467;&#26500;&#65292;&#31216;&#20026;&#35299;&#37322;&#35270;&#22270;&#12290;&#35299;&#37322;&#35270;&#22270;&#21253;&#25324;&#19968;&#32452;&#22270;&#27169;&#24335;&#21644;&#19968;&#32452;&#35825;&#21457;&#30340;&#35299;&#37322;&#23376;&#22270;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#22270;&#21644;&#30001;&#22522;&#20110;GNN&#30340;&#20998;&#31867;&#22120;M&#20998;&#37197;&#30340;&#29305;&#23450;&#31867;&#21035;&#26631;&#31614;l&#30340;&#25968;&#25454;&#24211;G&#65292;&#23427;&#31616;&#27905;&#22320;&#25551;&#36848;&#20102;&#26368;&#22909;&#35299;&#37322;&#20026;&#20160;&#20040;l&#30001;M&#20998;&#37197;&#30340;G&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#35745;&#31639;GNN&#35299;&#37322;&#30340;&#26368;&#20339;&#35299;&#37322;&#35270;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;&#931;^2_P&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating explanations for graph neural networks (GNNs) has been studied to understand their behavior in analytical tasks such as graph classification. Existing approaches aim to understand the overall results of GNNs rather than providing explanations for specific class labels of interest, and may return explanation structures that are hard to access, nor directly queryable.  We propose GVEX, a novel paradigm that generates Graph Views for EXplanation. (1) We design a two-tier explanation structure called explanation views. An explanation view consists of a set of graph patterns and a set of induced explanation subgraphs. Given a database G of multiple graphs and a specific class label l assigned by a GNN-based classifier M, it concisely describes the fraction of G that best explains why l is assigned by M. (2) We propose quality measures and formulate an optimization problem to compute optimal explanation views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3) 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#29983;&#25104;&#22120;&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#20174;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#24191;&#20041;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#25552;&#39640;&#37319;&#26679;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#22797;&#26434;&#20998;&#24067;&#20989;&#25968;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02080</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#29983;&#25104;&#22120;&#29992;&#20110;&#39640;&#25928;&#37319;&#26679;Boltzmann&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Energy based diffusion generator for efficient sampling of Boltzmann distributions. (arXiv:2401.02080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02080
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#29983;&#25104;&#22120;&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#20174;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#24191;&#20041;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#25552;&#39640;&#37319;&#26679;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#22797;&#26434;&#20998;&#24067;&#20989;&#25968;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#29983;&#25104;&#22120;&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#20174;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#37319;&#26679;&#27169;&#22411;&#37319;&#29992;&#31867;&#20284;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#65292;&#21033;&#29992;&#35299;&#30721;&#22120;&#23558;&#26469;&#33258;&#31616;&#21333;&#20998;&#24067;&#30340;&#28508;&#22312;&#21464;&#37327;&#36716;&#25442;&#20026;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#12290;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#22797;&#26434;&#20998;&#24067;&#30340;&#24378;&#22823;&#24314;&#27169;&#33021;&#21147;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#29983;&#25104;&#26679;&#26412;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#20934;&#30830;&#21464;&#20998;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#20041;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#35299;&#30721;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22797;&#26434;&#20998;&#24067;&#20989;&#25968;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel sampler called the energy based diffusion generator for generating samples from arbitrary target distributions. The sampling model employs a structure similar to a variational autoencoder, utilizing a decoder to transform latent variables from a simple distribution into random variables approximating the target distribution, and we design an encoder based on the diffusion model. Leveraging the powerful modeling capacity of the diffusion model for complex distributions, we can obtain an accurate variational estimate of the Kullback-Leibler divergence between the distributions of the generated samples and the target. Moreover, we propose a decoder based on generalized Hamiltonian dynamics to further enhance sampling performance. Through empirical evaluation, we demonstrate the effectiveness of our method across various complex distribution functions, showcasing its superiority compared to existing methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21746;&#23398;&#25991;&#29486;&#30340;&#26032;&#30340;&#20449;&#20219;&#26694;&#26550;$\mathcal{U}$-&#20449;&#20219;&#24230;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#20449;&#20219;&#24615;&#65292;&#35813;&#26694;&#26550;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#20811;&#26381;&#20559;&#35265;&#21644;&#35823;&#23548;&#24615;&#20449;&#20219;&#35780;&#20272;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.02062</link><description>&lt;p&gt;
U-&#20449;&#20219;&#27169;&#22411; - &#20915;&#31574;&#20013;&#30340;&#21487;&#38752;&#24615;&#12289;&#33021;&#21147;&#21644;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
U-Trustworthy Models.Reliability, Competence, and Confidence in Decision-Making. (arXiv:2401.02062v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21746;&#23398;&#25991;&#29486;&#30340;&#26032;&#30340;&#20449;&#20219;&#26694;&#26550;$\mathcal{U}$-&#20449;&#20219;&#24230;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#20449;&#20219;&#24615;&#65292;&#35813;&#26694;&#26550;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#20811;&#26381;&#20559;&#35265;&#21644;&#35823;&#23548;&#24615;&#20449;&#20219;&#35780;&#20272;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#39044;&#27979;&#27169;&#22411;&#20013;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#65292;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#36234;&#26469;&#36234;&#20851;&#27880;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#20449;&#20219;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#20449;&#20219;&#30340;AI&#25991;&#29486;&#20381;&#36182;&#20110;&#27010;&#29575;&#26694;&#26550;&#21644;&#26657;&#20934;&#20316;&#20026;&#20449;&#20219;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31163;&#24320;&#20102;&#36825;&#20010;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21746;&#23398;&#25991;&#29486;&#20013;&#20851;&#20110;&#20449;&#20219;&#30340;&#21551;&#31034;&#30340;&#26032;&#30340;&#20449;&#20219;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#25968;&#23398;&#23450;&#20041;&#65292;&#31216;&#20026;$\mathcal{U}$-&#20449;&#20219;&#24230;&#65292;&#19987;&#38376;&#20026;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#30340;&#20219;&#21153;&#23376;&#38598;&#37327;&#36523;&#23450;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#27169;&#22411;&#30340;$\mathcal{U}$-&#20449;&#20219;&#24230;&#21462;&#20915;&#20110;&#23427;&#22312;&#36825;&#20010;&#20219;&#21153;&#23376;&#38598;&#20013;&#26368;&#22823;&#21270;&#36125;&#21494;&#26031;&#25928;&#29992;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#32452;&#32467;&#26524;&#25361;&#25112;&#20102;&#27010;&#29575;&#26694;&#26550;&#65292;&#36890;&#36807;&#23637;&#31034;&#23427;&#21487;&#33021;&#20559;&#21521;&#19981;&#22826;&#21487;&#20449;&#30340;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#35823;&#23548;&#24615;&#20449;&#20219;&#35780;&#20272;&#30340;&#39118;&#38505;&#12290;&#22312;$\mathcal{U}$-&#20449;&#20219;&#24230;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#20449;&#20219;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing concerns regarding bias and discrimination in predictive models, the AI community has increasingly focused on assessing AI system trustworthiness. Conventionally, trustworthy AI literature relies on the probabilistic framework and calibration as prerequisites for trustworthiness. In this work, we depart from this viewpoint by proposing a novel trust framework inspired by the philosophy literature on trust. We present a precise mathematical definition of trustworthiness, termed $\mathcal{U}$-trustworthiness, specifically tailored for a subset of tasks aimed at maximizing a utility function. We argue that a model's $\mathcal{U}$-trustworthiness is contingent upon its ability to maximize Bayes utility within this task subset. Our first set of results challenges the probabilistic framework by demonstrating its potential to favor less trustworthy models and introduce the risk of misleading trustworthiness assessments. Within the context of $\mathcal{U}$-trustworthiness, we prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#21449;&#29109;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#25240;&#21472;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#25240;&#21472;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#31867;&#22343;&#20540;&#30340;&#20960;&#20309;&#29305;&#24615;&#20250;&#21457;&#29983;&#20559;&#31227;&#12290;</title><link>http://arxiv.org/abs/2401.02058</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#32422;&#26463;ReLU&#29305;&#24449;&#27169;&#22411;&#36827;&#34892;&#20132;&#21449;&#29109;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#31070;&#32463;&#25240;&#21472;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model. (arXiv:2401.02058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#21449;&#29109;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#25240;&#21472;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#25240;&#21472;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#31867;&#22343;&#20540;&#30340;&#20960;&#20309;&#29305;&#24615;&#20250;&#21457;&#29983;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#33539;&#24335;&#21253;&#25324;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#23558;&#35757;&#32451;&#25439;&#22833;&#20540;&#25512;&#21521;&#38646;&#65292;&#21363;&#20351;&#35757;&#32451;&#35823;&#24046;&#24050;&#32463;&#28040;&#22833;&#12290;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#35266;&#23519;&#21040;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#25240;&#21472;&#21040;&#23427;&#20204;&#30340;&#31867;&#22343;&#20540;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22343;&#20540;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31616;&#21333;&#20856;&#22411;&#31561;&#35282;&#32039;&#26694;&#65288;ETF&#65289;&#30340;&#39030;&#28857;&#12290;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#25240;&#21472;&#65288;NC&#65289;&#12290;&#20026;&#20102;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#26469;&#35777;&#26126;NC&#20986;&#29616;&#22312;&#35757;&#32451;&#38382;&#39064;&#30340;&#20840;&#23616;&#35299;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#26102;&#65292;&#19968;&#20123;NC&#29305;&#24615;&#23558;&#19981;&#20877;&#25104;&#31435;&#12290;&#20363;&#22914;&#65292;&#24403;&#25439;&#22833;&#25910;&#25947;&#26102;&#65292;&#31867;&#22343;&#20540;&#20960;&#20309;&#20250;&#20559;&#31163;&#31616;&#21333;&#20856;&#22411;&#31561;&#35282;&#32039;&#26694;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;NC&#25512;&#24191;&#21040;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#26080;&#32422;&#26463;ReLU&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#35757;&#32451;&#38382;&#39064;&#30340;&#20840;&#23616;&#35299;&#20013;&#65292;&#24403;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#26102;&#65292;NC&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#23545;&#20110;&#31867;&#22343;&#20540;&#30340;&#20960;&#20309;&#29305;&#24615;&#20250;&#21457;&#29983;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current paradigm of training deep neural networks for classification tasks includes minimizing the empirical risk that pushes the training loss value towards zero, even after the training error has been vanished. In this terminal phase of training, it has been observed that the last-layer features collapse to their class-means and these class-means converge to the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural Collapse (NC). To theoretically understand this phenomenon, recent works employ a simplified unconstrained feature model to prove that NC emerges at the global solutions of the training problem. However, when the training dataset is class-imbalanced, some NC properties will no longer be true. For example, the class-means geometry will skew away from the simplex ETF when the loss converges. In this paper, we generalize NC to imbalanced regime for cross-entropy loss under the unconstrained ReLU feature model. We prove that, while the wi
&lt;/p&gt;</description></item><item><title>Spikformer V2&#26159;&#19968;&#31181;&#22522;&#20110;SNNs&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#20986;&#33033;&#20914;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#33033;&#20914;Transformer&#26469;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#22270;&#20687;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.02020</link><description>&lt;p&gt;
Spikformer V2&#65306;&#36890;&#36807;SNN Ticket&#22312;ImageNet&#19978;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;
&lt;/p&gt;
&lt;p&gt;
Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket. (arXiv:2401.02020v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02020
&lt;/p&gt;
&lt;p&gt;
Spikformer V2&#26159;&#19968;&#31181;&#22522;&#20110;SNNs&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#20986;&#33033;&#20914;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#33033;&#20914;Transformer&#26469;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#22270;&#20687;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22240;&#20854;&#29983;&#29289;&#23398;&#21512;&#29702;&#30340;&#32467;&#26500;&#32780;&#38395;&#21517;&#65292;&#20294;&#20854;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#32467;&#26500;&#30340;&#39640;&#24615;&#33021;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#29616;&#26377;&#30340;SNNs&#20013;&#32570;&#22833;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#33258;&#27880;&#24847;&#33021;&#21147;&#21644;SNNs&#30340;&#29983;&#29289;&#29305;&#24615;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33033;&#20914;&#33258;&#27880;&#24847;&#65288;SSA&#65289;&#21644;&#33033;&#20914;Transformer&#65288;Spikformer&#65289;&#12290;SSA&#26426;&#21046;&#28040;&#38500;&#20102;&#23545;softmax&#30340;&#38656;&#27714;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#33033;&#20914;&#30340;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#25429;&#33719;&#31232;&#30095;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#36825;&#31181;&#26080;&#20056;&#27861;&#30340;&#31232;&#30095;&#35745;&#31639;&#20351;&#24471;SSA&#39640;&#25928;&#19988;&#33410;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#33033;&#20914;&#21367;&#31215;&#24178;&#32454;&#32990;&#65288;SCS&#65289;&#21644;&#34917;&#20805;&#21367;&#31215;&#23618;&#26469;&#22686;&#24378;Spikformer&#30340;&#26550;&#26500;&#12290;&#21152;&#19978;SCS&#30340;Spikformer&#34987;&#31216;&#20026;Spikformer V2&#12290;&#20026;&#20102;&#35757;&#32451;&#26356;&#22823;&#26356;&#28145;&#30340;Spikformer V2&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#25506;+
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs), known for their biologically plausible architecture, face the challenge of limited performance. The self-attention mechanism, which is the cornerstone of the high-performance Transformer and also a biologically inspired structure, is absent in existing SNNs. To this end, we explore the potential of leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA) and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for softmax and captures the sparse visual feature employing spike-based Query, Key, and Value. This sparse computation without multiplication makes SSA efficient and energy-saving. Further, we develop a Spiking Convolutional Stem (SCS) with supplementary convolutional layers to enhance the architecture of Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer V2. To train larger and deeper Spikformer V2, we introduce a pioneering explorat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Generative&#26041;&#27861;&#24212;&#29992;&#20110;&#31163;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#23558;&#20248;&#21270;&#38382;&#39064;&#35270;&#20026;&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#21644;&#20248;&#21270;&#26410;&#30693;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#31034;&#20363;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.02019</link><description>&lt;p&gt;
&#20174;&#20989;&#25968;&#21040;&#20998;&#24067;&#24314;&#27169;&#65306;&#19968;&#31181;PAC-Generative&#26041;&#27861;&#24212;&#29992;&#20110;&#31163;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Function to Distribution Modeling: A PAC-Generative Approach to Offline Optimization. (arXiv:2401.02019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Generative&#26041;&#27861;&#24212;&#29992;&#20110;&#31163;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#23558;&#20248;&#21270;&#38382;&#39064;&#35270;&#20026;&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#21644;&#20248;&#21270;&#26410;&#30693;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#31034;&#20363;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#31163;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#38500;&#20102;&#19968;&#32452;&#8220;&#31163;&#32447;&#8221;&#25968;&#25454;&#31034;&#20363;&#22806;&#26159;&#26410;&#30693;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31163;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#23398;&#20064;&#26410;&#30693;&#30446;&#26631;&#20989;&#25968;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#28982;&#21518;&#24212;&#29992;&#29616;&#26377;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#34429;&#28982;&#24314;&#27169;&#26410;&#30693;&#30446;&#26631;&#20989;&#25968;&#30340;&#24605;&#24819;&#30452;&#35266;&#19988;&#21560;&#24341;&#20154;&#65292;&#20294;&#20174;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#26681;&#25454;&#20248;&#21270;&#30446;&#26631;&#35843;&#25972;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#20063;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#19981;&#26159;&#23398;&#20064;&#28982;&#21518;&#20248;&#21270;&#26410;&#30693;&#30446;&#26631;&#20989;&#25968;&#65292;&#32780;&#26159;&#20174;&#19968;&#20010;&#26356;&#30452;&#25509;&#20294;&#19981;&#22826;&#30452;&#35266;&#30340;&#35282;&#24230;&#30475;&#24453;&#20248;&#21270;&#65292;&#21363;&#23558;&#20248;&#21270;&#35270;&#20026;&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#31034;&#20363;&#20013;&#23398;&#20064;&#19968;&#20010;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#8220;&#37325;&#26032;&#21152;&#26435;&#8221;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of offline optimization, where the objective function is unknown except for a collection of ``offline" data examples. While recent years have seen a flurry of work on applying various machine learning techniques to the offline optimization problem, the majority of these work focused on learning a surrogate of the unknown objective function and then applying existing optimization algorithms. While the idea of modeling the unknown objective function is intuitive and appealing, from the learning point of view it also makes it very difficult to tune the objective of the learner according to the objective of optimization. Instead of learning and then optimizing the unknown objective function, in this paper we take on a less intuitive but more direct view that optimization can be thought of as a process of sampling from a generative model. To learn an effective generative model from the offline data examples, we consider the standard technique of ``re-weighti
&lt;/p&gt;</description></item><item><title>SwitchTab&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#28508;&#22312;&#20381;&#36182;&#20851;&#31995;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#26469;&#35299;&#32806;&#25968;&#25454;&#23545;&#20013;&#30340;&#20114;&#30456;&#20851;&#32852;&#21644;&#26174;&#33879;&#29305;&#24449;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#23884;&#20837;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#65292;SwitchTab&#22312;&#32454;&#35843;&#19979;&#31471;&#21040;&#31471;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02013</link><description>&lt;p&gt;
SwitchTab: &#20999;&#25442;&#24335;&#33258;&#32534;&#30721;&#22120;&#22312;&#34920;&#26684;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwitchTab: Switched Autoencoders Are Effective Tabular Learners. (arXiv:2401.02013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02013
&lt;/p&gt;
&lt;p&gt;
SwitchTab&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#28508;&#22312;&#20381;&#36182;&#20851;&#31995;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#26469;&#35299;&#32806;&#25968;&#25454;&#23545;&#20013;&#30340;&#20114;&#30456;&#20851;&#32852;&#21644;&#26174;&#33879;&#29305;&#24449;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#23884;&#20837;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#65292;SwitchTab&#22312;&#32454;&#35843;&#19979;&#31471;&#21040;&#31471;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#25968;&#25454;&#26679;&#26412;&#21576;&#29616;&#26126;&#30830;&#30340;&#31354;&#38388;&#25110;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#19981;&#22826;&#26126;&#26174;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;SwitchTab&#65292;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#20381;&#36182;&#20851;&#31995;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;SwitchTab&#21033;&#29992;&#19968;&#20010;&#38750;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#26469;&#35299;&#32806;&#25968;&#25454;&#23545;&#20013;&#30340;&#20114;&#30456;&#20851;&#32852;&#21644;&#26174;&#33879;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#36827;&#32780;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#39564;&#35777;SwitchTab&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#28041;&#21450;&#34920;&#26684;&#25968;&#25454;&#30340;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#32454;&#35843;&#19979;&#31471;&#21040;&#31471;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning methods have achieved significant success in computer vision and natural language processing, where data samples exhibit explicit spatial or semantic dependencies. However, applying these methods to tabular data is challenging due to the less pronounced dependencies among data samples. In this paper, we address this limitation by introducing SwitchTab, a novel self-supervised method specifically designed to capture latent dependencies in tabular data. SwitchTab leverages an asymmetric encoder-decoder framework to decouple mutual and salient features among data pairs, resulting in more representative embeddings. These embeddings, in turn, contribute to better decision boundaries and lead to improved results in downstream tasks. To validate the effectiveness of SwitchTab, we conduct extensive experiments across various domains involving tabular data. The results showcase superior performance in end-to-end prediction tasks with fine-tuning. Moreover
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20108;&#38454;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22240;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#20559;&#35265;&#32780;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#32431;&#19968;&#38454;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.02012</link><description>&lt;p&gt;
&#24555;&#36895;&#32780;&#20844;&#24179;: &#39640;&#25928;&#30340;&#20108;&#38454;&#40065;&#26834;&#20248;&#21270;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fast &amp; Fair: Efficient Second-Order Robust Optimization for Fairness in Machine Learning. (arXiv:2401.02012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02012
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20108;&#38454;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22240;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#20559;&#35265;&#32780;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#32431;&#19968;&#38454;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#25506;&#35752;&#20102;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#24320;&#21457;&#26356;&#20844;&#24179;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#20197;&#20943;&#36731;&#20854;&#24050;&#30693;&#23384;&#22312;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;DNN&#23545;&#20110;&#19982;&#31181;&#26063;&#21644;&#24615;&#21035;&#31561;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#20559;&#35265;&#24456;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25913;&#21464;&#29983;&#27963;&#30340;&#32467;&#26524;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#36910;&#25429;&#23244;&#30097;&#20154;&#30340;&#20154;&#33080;&#35782;&#21035;&#36719;&#20214;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20223;&#23556;&#32447;&#24615;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#25913;&#21892;&#20844;&#24179;&#24615;&#12290;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#32431;&#19968;&#38454;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project explores adversarial training techniques to develop fairer Deep Neural Networks (DNNs) to mitigate the inherent bias they are known to exhibit. DNNs are susceptible to inheriting bias with respect to sensitive attributes such as race and gender, which can lead to life-altering outcomes (e.g., demographic bias in facial recognition software used to arrest a suspect). We propose a robust optimization problem, which we demonstrate can improve fairness in several datasets, both synthetic and real-world, using an affine linear model. Leveraging second order information, we are able to find a solution to our optimization problem more efficiently than a purely first order method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#22810;&#20219;&#21153;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#30340;&#38543;&#26426;&#38142;&#36335;&#25925;&#38556;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#32570;&#22833;&#20915;&#31574;&#30340;&#20581;&#22766;&#20998;&#25955;&#38797;&#28857;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#36798;&#21040;&#20102;O(sqrt(T))&#30340;regret&#21644;O(T^fra)&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.02011</link><description>&lt;p&gt;
&#20998;&#25955;&#22810;&#20219;&#21153;&#22312;&#32447;&#20984;&#20248;&#21270;&#22312;&#38543;&#26426;&#38142;&#36335;&#22833;&#25928;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Task Online Convex Optimization Under Random Link Failures. (arXiv:2401.02011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#22810;&#20219;&#21153;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#30340;&#38543;&#26426;&#38142;&#36335;&#25925;&#38556;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#32570;&#22833;&#20915;&#31574;&#30340;&#20581;&#22766;&#20998;&#25955;&#38797;&#28857;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#36798;&#21040;&#20102;O(sqrt(T))&#30340;regret&#21644;O(T^fra)&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#30340;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#37051;&#23621;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;&#30001;&#20110;&#32593;&#32476;&#25317;&#22622;&#12289;&#30828;&#20214;/&#36719;&#20214;&#38382;&#39064;&#12289;&#36890;&#20449;&#20013;&#26029;&#21644;&#20854;&#20182;&#22240;&#32032;&#65292;&#20256;&#36755;&#25925;&#38556;&#21487;&#33021;&#21457;&#29983;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20998;&#25955;&#22810;&#20219;&#21153;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#30340;&#38543;&#26426;&#38142;&#36335;&#25925;&#38556;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#25104;&#23545;&#32422;&#26463;&#30456;&#20114;&#32806;&#21512;&#12290;&#34429;&#28982;&#22312;&#32422;&#26463;&#20248;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20256;&#32479;&#30340;&#38797;&#28857;&#31639;&#27861;&#22312;&#36825;&#37324;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#65292;&#22240;&#20026;&#23384;&#22312;&#38543;&#26426;&#20002;&#21253;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20581;&#22766;&#30340;&#20998;&#25955;&#38797;&#28857;&#31639;&#27861;&#65292;&#20197;&#23545;&#25239;&#20855;&#26377;&#24322;&#36136;&#27010;&#29575;&#30340;&#38543;&#26426;&#38142;&#36335;&#22833;&#25928;&#65292;&#36890;&#36807;&#29992;&#37051;&#23621;&#30340;&#26368;&#26032;&#25509;&#25910;&#21040;&#30340;&#20540;&#26367;&#20195;&#32570;&#22833;&#20915;&#31574;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#35880;&#24910;&#22320;&#38480;&#21046;&#30001;&#36825;&#31181;&#26367;&#20195;&#20135;&#29983;&#30340;&#32047;&#31215;&#20559;&#24046;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;O(sqrt(T))&#30340;regret&#21644;O(T^fra)&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized optimization methods often entail information exchange between neighbors. Transmission failures can happen due to network congestion, hardware/software issues, communication outage, and other factors. In this paper, we investigate the random link failure problem in decentralized multi-task online convex optimization, where agents have individual decisions that are coupled with each other via pairwise constraints. Although widely used in constrained optimization, conventional saddle-point algorithms are not directly applicable here because of random packet dropping. To address this issue, we develop a robust decentralized saddle-point algorithm against random link failures with heterogeneous probabilities by replacing the missing decisions of neighbors with their latest received values. Then, by judiciously bounding the accumulated deviation stemming from this replacement, we first establish that our algorithm achieves $\mathcal{O}(\sqrt{T})$ regret and $\mathcal{O}(T^\fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#31526;&#21512;&#25512;&#29702;&#26469;&#35299;&#20915;&#31185;&#23398;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#21453;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#24212;&#29992;&#24191;&#27867;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02008</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;&#20195;&#29702;&#24314;&#27169;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#35774;&#35745;&#20248;&#21270;&#65292;&#24182;&#24212;&#29992;&#20110;&#22797;&#21512;&#26448;&#26009;&#24494;&#32467;&#26500;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Two-Stage Surrogate Modeling for Data-Driven Design Optimization with Application to Composite Microstructure Generation. (arXiv:2401.02008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#31526;&#21512;&#25512;&#29702;&#26469;&#35299;&#20915;&#31185;&#23398;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#21453;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#24212;&#29992;&#24191;&#27867;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20004;&#38454;&#27573;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#21453;&#38382;&#39064;&#12290;&#22312;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#31532;&#19968;&#38454;&#27573;&#65292;&#19968;&#31181;&#31216;&#20026;&#8220;&#23398;&#20064;&#32773;&#8221;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#20986;&#19968;&#32452;&#22312;&#36755;&#20837;&#35774;&#35745;&#31354;&#38388;&#20013;&#65292;&#20854;&#39044;&#27979;&#36755;&#20986;&#19982;&#26399;&#26395;&#32467;&#26524;&#23494;&#20999;&#19968;&#33268;&#30340;&#20505;&#36873;&#39033;&#12290;&#38543;&#21518;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#37319;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;&#20195;&#29702;&#27169;&#22411;&#20316;&#20026;&#8220;&#35780;&#20272;&#32773;&#8221;&#65292;&#26469;&#35780;&#20272;&#22312;&#31532;&#19968;&#38454;&#27573;&#29983;&#25104;&#30340;&#32553;&#20943;&#20505;&#36873;&#31354;&#38388;&#12290;&#36825;&#20010;&#35780;&#20272;&#36807;&#31243;&#36890;&#36807;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#27700;&#24179;&#26469;&#28040;&#38500;&#19981;&#20934;&#30830;&#21644;&#19981;&#30830;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26694;&#26550;&#30340;&#29420;&#29305;&#36129;&#29486;&#22312;&#20110;&#38598;&#25104;&#20102;&#31526;&#21512;&#25512;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#38454;&#27573;&#21453;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#24037;&#31243;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel two-stage machine learning-based surrogate modeling framework to address inverse problems in scientific and engineering fields. In the first stage of the proposed framework, a machine learning model termed the "learner" identifies a limited set of candidates within the input design space whose predicted outputs closely align with desired outcomes. Subsequently, in the second stage, a separate surrogate model, functioning as an "evaluator," is employed to assess the reduced candidate space generated in the first stage. This evaluation process eliminates inaccurate and uncertain solutions, guided by a user-defined coverage level. The framework's distinctive contribution is the integration of conformal inference, providing a versatile and efficient approach that can be widely applicable. To demonstrate the effectiveness of the proposed framework compared to conventional single-stage inverse problems, we conduct several benchmark tests and investigate an engin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20511;&#21161;&#27010;&#29575;&#35745;&#31639;&#26426;&#30340;&#22343;&#22330;&#36741;&#21161;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#30828;&#20214;&#21644;&#31232;&#30095;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#20004;&#31867;&#22343;&#22330;&#29702;&#35770;&#36741;&#21161;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35757;&#32451;&#28145;&#24230;&#19988;&#26080;&#38480;&#21046;&#30340;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#21487;&#35757;&#32451;&#24615;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.01996</link><description>&lt;p&gt;
&#20511;&#21161;&#27010;&#29575;&#35745;&#31639;&#26426;&#30340;&#22343;&#22330;&#36741;&#21161;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Assisted Deep Boltzmann Learning with Probabilistic Computers. (arXiv:2401.01996v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20511;&#21161;&#27010;&#29575;&#35745;&#31639;&#26426;&#30340;&#22343;&#22330;&#36741;&#21161;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#30828;&#20214;&#21644;&#31232;&#30095;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#20004;&#31867;&#22343;&#22330;&#29702;&#35770;&#36741;&#21161;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35757;&#32451;&#28145;&#24230;&#19988;&#26080;&#38480;&#21046;&#30340;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#21487;&#35757;&#32451;&#24615;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20316;&#20026;&#21551;&#21457;&#20110;&#29289;&#29702;&#23398;&#30340;&#12289;&#22522;&#20110;&#33021;&#37327;&#30340;&#21644;&#29983;&#25104;&#24615;&#36136;&#30340;&#27169;&#22411;&#65292;&#26222;&#36890;&#29627;&#23572;&#20857;&#26364;&#26426;&#34987;&#35748;&#20026;&#38590;&#20197;&#35757;&#32451;&#12290;&#36825;&#31181;&#35266;&#28857;&#23548;&#33268;&#20102;&#23545;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#31616;&#21270;&#65292;&#22914;&#38480;&#21046;&#23618;&#20869;&#36830;&#25509;&#25110;&#36880;&#23618;&#35757;&#32451;&#30340;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#12290;&#32780;&#26368;&#36817;&#21457;&#23637;&#30340;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#30828;&#20214;&#65288;&#20855;&#20307;&#22320;&#35828;&#26159;&#20855;&#26377;&#27010;&#29575;&#20301;&#30340;&#27010;&#29575;&#35745;&#31639;&#26426;&#65289;&#21487;&#33021;&#25913;&#21464;&#23545;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#21487;&#35757;&#32451;&#24615;&#30340;&#35748;&#30693;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#21313;&#20159;&#20159;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#29575;&#30340;&#27010;&#29575;&#35745;&#31639;&#26426;&#22312;&#26368;&#21021;&#29992;&#20110;D-Wave&#30340;&#21407;&#26377;&#31232;&#30095;&#32593;&#32476;&#19978;&#35757;&#32451;&#28145;&#24230;&#19988;&#26080;&#38480;&#21046;&#30340;&#29627;&#23572;&#20857;&#26364;&#26426;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#27010;&#29575;&#35745;&#31639;&#26426;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31867;&#22343;&#22330;&#29702;&#35770;&#36741;&#21161;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;xMFTs&#65288;x = Naive&#21644;Hierarchical&#65289;&#12290;xMFTs&#29992;&#20110;&#20272;&#35745;&#23545;&#27604;&#25955;&#24230;&#27491;&#30456;&#20301;&#20013;&#30340;&#24179;&#22343;&#20540;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their appeal as physics-inspired, energy-based and generative nature, general Boltzmann Machines (BM) are considered intractable to train. This belief led to simplified models of BMs with restricted intralayer connections or layer-by-layer training of deep BMs. Recent developments in domain-specific hardware -- specifically probabilistic computers (p-computer) with probabilistic bits (p-bit) -- may change established wisdom on the tractability of deep BMs. In this paper, we show that deep and unrestricted BMs can be trained using p-computers generating hundreds of billions of Markov Chain Monte Carlo (MCMC) samples per second, on sparse networks developed originally for use in D-Wave's annealers. To maximize the efficiency of learning the p-computer, we introduce two families of Mean-Field Theory assisted learning algorithms, or xMFTs (x = Naive and Hierarchical). The xMFTs are used to estimate the averages and correlations during the positive phase of the contrastive divergenc
&lt;/p&gt;</description></item><item><title>GPS-SSL&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24230;&#37327;&#31354;&#38388;&#24182;&#21033;&#29992;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#23545;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;Cifar10&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01990</link><description>&lt;p&gt;
GPS-SSL: &#24341;&#23548;&#27491;&#26679;&#26412;&#37319;&#26679;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01990
&lt;/p&gt;
&lt;p&gt;
GPS-SSL&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24230;&#37327;&#31354;&#38388;&#24182;&#21033;&#29992;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#23545;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;Cifar10&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#23548;&#27491;&#26679;&#26412;&#37319;&#26679;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;GPS-SSL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27491;&#26679;&#26412;&#36873;&#25321;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;SSL&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#29983;&#25104;&#27491;&#26679;&#26412;&#65292;&#24182;&#23558;&#20808;&#39564;&#30693;&#35782;&#32467;&#21512;&#36827;&#21435;&#65292;&#20294;&#26159;&#38169;&#35823;&#25110;&#32773;&#36807;&#24369;&#30340;DA&#20250;&#20005;&#37325;&#38477;&#20302;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;GPS-SSL&#21017;&#25552;&#20986;&#35774;&#35745;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#20351;&#24471;&#27431;&#27663;&#36317;&#31163;&#25104;&#20026;&#35821;&#20041;&#20851;&#31995;&#30340;&#26377;&#24847;&#20041;&#30340;&#26367;&#20195;&#12290;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#37117;&#21487;&#20197;&#29420;&#31435;&#22320;&#23884;&#20837;&#21040;&#36825;&#20010;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#32780;&#19981;&#21463;&#25152;&#20351;&#29992;&#30340;DA&#24433;&#21709;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;GPS-SSL&#36866;&#29992;&#20110;&#20219;&#20309;SSL&#26041;&#27861;&#65292;&#22914;SimCLR&#25110;BYOL&#12290;GPS-SSL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#20943;&#23569;&#20102;&#23450;&#21046;&#24378;DA&#30340;&#21387;&#21147;&#12290;&#20363;&#22914;&#65292;GPS-SSL&#22312;Cifar10&#19978;&#20351;&#29992;&#24369;DA&#36798;&#21040;&#20102;85.58&#65285;&#65292;&#32780;&#22522;&#20934;&#20540;&#21482;&#36798;&#21040;&#20102;37.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a general method to inject a priori knowledge into Self-Supervised Learning (SSL) positive samples selection. Current SSL methods leverage Data-Augmentations (DA) for generating positive samples and incorporate prior knowledge - an incorrect, or too weak DA will drastically reduce the quality of the learned representation. GPS-SSL proposes instead to design a metric space where Euclidean distances become a meaningful proxy for semantic relationship. In that space, it is now possible to generate positive samples from nearest neighbor sampling. Any prior knowledge can now be embedded into that metric space independently from the employed DA. From its simplicity, GPS-SSL is applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches 85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We therefore move a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#19982;&#31034;&#20363;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.01987</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#21147;&#21644;&#23545;&#25239;&#35757;&#32451;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning of Multivariate Time Series using Attention and Adversarial Training. (arXiv:2401.01987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#19982;&#31034;&#20363;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#24320;&#21457;&#20986;&#23545;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#21482;&#26377;&#22312;&#27492;&#20445;&#35777;&#19979;&#65292;&#26041;&#27861;&#25165;&#33021;&#21512;&#27861;&#22320;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#65292;&#20363;&#22914;&#65292;&#23545;&#25239;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25110;&#20026;&#40657;&#30418;&#20915;&#31574;&#31995;&#32479;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#24418;&#25104;&#31283;&#23450;&#30340;&#34920;&#31034;&#21644;&#29983;&#25104;&#36924;&#30495;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#26524;&#12290;&#34429;&#28982;&#35768;&#22810;&#24212;&#29992;&#38598;&#20013;&#20110;&#29983;&#25104;&#22270;&#20687;&#25968;&#25454;&#65292;&#20294;&#22312;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22810;&#21464;&#37327;&#20449;&#21495;&#26041;&#38754;&#65292;&#20184;&#20986;&#30340;&#21162;&#21147;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#29983;&#25104;&#20154;&#24037;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#12290;&#36890;&#36807;t-SNE&#21487;&#35270;&#21270;&#12289;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#21644;&#29109;&#24471;&#20998;&#23545;&#34920;&#31034;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#20449;&#21495;&#19982;&#31034;&#20363;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#36739;&#39640;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#24120;&#35268;&#30340;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
A critical factor in trustworthy machine learning is to develop robust representations of the training data. Only under this guarantee methods are legitimate to artificially generate data, for example, to counteract imbalanced datasets or provide counterfactual explanations for blackbox decision-making systems. In recent years, Generative Adversarial Networks (GANs) have shown considerable results in forming stable representations and generating realistic data. While many applications focus on generating image data, less effort has been made in generating time series data, especially multivariate signals. In this work, a Transformer-based autoencoder is proposed that is regularized using an adversarial training scheme to generate artificial multivariate time series signals. The representation is evaluated using t-SNE visualizations, Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the generated signals exhibit higher similarity to an exemplary dataset than using
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#21487;&#20197;&#27604;&#36739;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#22312;&#32771;&#34385;&#26597;&#35810;&#28857;&#21644;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20960;&#20309;&#29305;&#24615;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01981</link><description>&lt;p&gt;
&#36229;&#36234;&#36951;&#25022;&#65306;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20960;&#20309;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Beyond Regrets: Geometric Metrics for Bayesian Optimization. (arXiv:2401.01981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#21487;&#20197;&#27604;&#36739;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#22312;&#32771;&#34385;&#26597;&#35810;&#28857;&#21644;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20960;&#20309;&#29305;&#24615;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#38024;&#23545;&#40657;&#30418;&#23376;&#30446;&#26631;&#20989;&#25968;&#30340;&#21407;&#21017;&#24615;&#20248;&#21270;&#31574;&#30053;&#12290;&#23427;&#22312;&#31185;&#23398;&#21457;&#29616;&#21644;&#23454;&#39564;&#35774;&#35745;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#36890;&#24120;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24615;&#33021;&#26159;&#36890;&#36807;&#22522;&#20110;&#36951;&#25022;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#30340;&#65292;&#22914;&#30636;&#26102;&#36951;&#25022;&#12289;&#31616;&#21333;&#36951;&#25022;&#21644;&#32047;&#31215;&#36951;&#25022;&#12290;&#36825;&#20123;&#24230;&#37327;&#20165;&#20381;&#36182;&#20110;&#20989;&#25968;&#35780;&#20272;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#32771;&#34385;&#26597;&#35810;&#28857;&#21644;&#20840;&#23616;&#35299;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#20063;&#19981;&#32771;&#34385;&#26597;&#35810;&#28857;&#26412;&#36523;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#20204;&#19981;&#33021;&#21306;&#20998;&#26159;&#21542;&#25104;&#21151;&#25214;&#21040;&#20102;&#22810;&#20010;&#20840;&#23616;&#35299;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#19981;&#33021;&#35780;&#20272;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#32473;&#23450;&#25628;&#32034;&#31354;&#38388;&#20013;&#21033;&#29992;&#21644;&#25506;&#32034;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#21363;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;&#24179;&#22343;&#24230;&#21644;&#24179;&#22343;&#36317;&#31163;&#12290;&#36825;&#20123;&#24230;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#32771;&#34385;&#26597;&#35810;&#28857;&#21644;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a principled optimization strategy for a black-box objective function. It shows its effectiveness in a wide variety of real-world applications such as scientific discovery and experimental design. In general, the performance of Bayesian optimization is assessed by regret-based metrics such as instantaneous, simple, and cumulative regrets. These metrics only rely on function evaluations, so that they do not consider geometric relationships between query points and global solutions, or query points themselves. Notably, they cannot discriminate if multiple global solutions are successfully found. Moreover, they do not evaluate Bayesian optimization's abilities to exploit and explore a search space given. To tackle these issues, we propose four new geometric metrics, i.e., precision, recall, average degree, and average distance. These metrics allow us to compare Bayesian optimization algorithms considering the geometry of both query points and global optima, or que
&lt;/p&gt;</description></item><item><title>Tailor&#26159;&#19968;&#20010;&#38024;&#23545;&#39640;&#31471;&#26102;&#23578;&#24066;&#22330;&#30340;&#23610;&#23544;&#24314;&#35758;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#38544;&#24335;&#21644;&#26174;&#24335;&#29992;&#25143;&#20449;&#21495;&#65292;&#37319;&#29992;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23610;&#23544;&#24314;&#35758;&#12290;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21152;&#36141;&#29289;&#36710;&#30340;&#20132;&#20114;&#22686;&#21152;&#20102;&#29992;&#25143;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.01978</link><description>&lt;p&gt;
Tailor: &#39640;&#31471;&#26102;&#23578;&#24066;&#22330;&#30340;&#23610;&#23544;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Tailor: Size Recommendations for High-End Fashion Marketplaces. (arXiv:2401.01978v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01978
&lt;/p&gt;
&lt;p&gt;
Tailor&#26159;&#19968;&#20010;&#38024;&#23545;&#39640;&#31471;&#26102;&#23578;&#24066;&#22330;&#30340;&#23610;&#23544;&#24314;&#35758;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#38544;&#24335;&#21644;&#26174;&#24335;&#29992;&#25143;&#20449;&#21495;&#65292;&#37319;&#29992;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23610;&#23544;&#24314;&#35758;&#12290;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21152;&#36141;&#29289;&#36710;&#30340;&#20132;&#20114;&#22686;&#21152;&#20102;&#29992;&#25143;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#21644;&#21160;&#24577;&#30340;&#39640;&#31471;&#26102;&#23578;&#24066;&#22330;&#20013;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#23610;&#23544;&#24314;&#35758;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#28385;&#36275;&#39038;&#23458;&#22312;&#36825;&#26041;&#38754;&#30340;&#26399;&#26395;&#19981;&#20165;&#23545;&#30830;&#20445;&#20182;&#20204;&#30340;&#28385;&#24847;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#20063;&#22312;&#20419;&#36827;&#39038;&#23458;&#20445;&#25345;&#65292;&#36825;&#26159;&#20219;&#20309;&#26102;&#23578;&#38646;&#21806;&#21830;&#25104;&#21151;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#38544;&#24335;&#65288;&#21152;&#36141;&#29289;&#36710;&#65289;&#21644;&#26174;&#24335;&#65288;&#36864;&#36135;&#21407;&#22240;&#65289;&#29992;&#25143;&#20449;&#21495;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65306;&#19968;&#20010;&#37319;&#29992;LSTM&#23545;&#29992;&#25143;&#20449;&#21495;&#36827;&#34892;&#32534;&#30721;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#27604;SFNet&#25552;&#39640;&#20102;45.7%&#12290;&#36890;&#36807;&#20351;&#29992;&#21152;&#36141;&#29289;&#36710;&#30340;&#20132;&#20114;&#65292;&#19982;&#20165;&#20351;&#29992;&#35746;&#21333;&#30456;&#27604;&#65292;&#25105;&#20204;&#23558;&#29992;&#25143;&#35206;&#30422;&#33539;&#22260;&#22686;&#21152;&#20102;24.5%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#23454;&#39564;&#20197;&#27979;&#37327;&#27169;&#22411;&#30340;&#24310;&#36831;&#24615;&#33021;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#23454;&#26102;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-changing and dynamic realm of high-end fashion marketplaces, providing accurate and personalized size recommendations has become a critical aspect. Meeting customer expectations in this regard is not only crucial for ensuring their satisfaction but also plays a pivotal role in driving customer retention, which is a key metric for the success of any fashion retailer. We propose a novel sequence classification approach to address this problem, integrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our approach comprises two distinct models: one employs LSTMs to encode the user signals, while the other leverages an Attention mechanism. Our best model outperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions we increase the user coverage by 24.5% when compared with only using Orders. Moreover, we evaluate the models' usability in real-time recommendation scenarios by conducting experiments to measure their latency performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#25277;&#35937;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20363;&#31243;&#20197;&#21450;&#21033;&#29992;&#23569;&#37327;&#30340;nun</title><link>http://arxiv.org/abs/2401.01974</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#27491;&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65306;&#20197;LLMs&#20026;&#31243;&#24207;&#21592;
&lt;/p&gt;
&lt;p&gt;
Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#25277;&#35937;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20363;&#31243;&#20197;&#21450;&#21033;&#29992;&#23569;&#37327;&#30340;nun
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25512;&#29702;&#20027;&#35201;&#37319;&#29992;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#25512;&#29702;&#12289;&#27867;&#21270;&#12289;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#25512;&#29702;&#20197;&#21450;&#35745;&#25968;&#26041;&#38754;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#25511;&#21046;&#22120;&#36827;&#34892;&#35270;&#35273;&#25512;&#29702;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35843;&#24230;&#19968;&#32452;(&#35270;&#35273;)&#24037;&#20855;&#26469;&#35299;&#20915;&#23376;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#12289;&#35270;&#35273; grounding &#21644;&#35270;&#39057;&#30340;&#26102;&#38388;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#24403;&#21069;&#24418;&#24335;&#19979;&#20005;&#37325;&#20381;&#36182;&#20110;&#22312;&#25552;&#31034;&#20013;&#38024;&#23545;&#20855;&#20307;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#20154;&#24037;&#35774;&#35745;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#36825;&#38656;&#35201;&#39640;&#25216;&#33021;&#31243;&#24207;&#21592;&#25237;&#20837;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#30340;&#20363;&#31243;&#65292;&#24182;&#21033;&#29992;&#23569;&#37327;&#30340;nu
&lt;/p&gt;
&lt;p&gt;
Visual reasoning is dominated by end-to-end neural networks scaled to billions of model parameters and training examples. However, even the largest models struggle with compositional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting. Visual reasoning with large language models (LLMs) as controllers can, in principle, address these limitations by decomposing the task and solving subtasks by orchestrating a set of (visual) tools. Recently, these models achieved great performance on tasks such as compositional visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in their current form, these models heavily rely on human engineering of in-context examples in the prompt, which are often dataset- and task-specific and require significant labor by highly skilled programmers. In this work, we present a framework that mitigates these issues by introducing spatially and temporally abstract routines and by leveraging a small nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01951</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#33021;&#21542;&#20165;&#29983;&#25104;&#36924;&#30495;&#30340;&#25163;&#37096;&#22270;&#20687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#36798;&#21313;&#24180;&#20043;&#20037;&#65292;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19968;&#30452;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#20363;&#22914;&#20154;&#25163;&#21644;&#25163;&#25351;&#20013;&#25152;&#23384;&#22312;&#30340;&#29305;&#24449;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#23384;&#22312;&#12290;&#34429;&#28982;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21644;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#25351;&#21521;&#20102;&#24213;&#23618;&#32467;&#26500;&#30340;&#26681;&#26412;&#32570;&#38519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#20854;&#20013;&#21253;&#21547;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#65292;&#26469;&#23637;&#31034;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;GAN&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative $n$-dimensional Cartesian coordinate system. We show that this drastically improves quality of hand and face images generated by GANs and Variational AutoEncoders (VAE).
&lt;/p&gt;</description></item><item><title>&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#65292;Generative AI&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#30528;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#23433;&#20840;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.01923</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;: &#35270;&#37326;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01923
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#65292;Generative AI&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#30528;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#23433;&#20840;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#24863;&#30693;&#12289;&#32593;&#32476;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#22914;&#26234;&#33021;&#25163;&#26426;&#12289;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#26234;&#33021;&#38899;&#31665;&#21644;&#23478;&#24237;&#26426;&#22120;&#20154;&#65292;&#24050;&#32463;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative AI&#65289;&#30340;&#36827;&#23637;&#65292;&#22914;GPT&#12289;LLaMA&#12289;DALL-E&#21644;&#31283;&#23450;&#25193;&#25955;&#31561;&#65292;&#32473;&#29289;&#32852;&#32593;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#25105;&#20204;&#23545;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#24102;&#26469;&#30340;&#22909;&#22788;&#30340;&#30475;&#27861;&#21644;&#24895;&#26223;&#65292;&#24182;&#35752;&#35770;&#20102;Generative AI&#22312;&#29289;&#32852;&#32593;&#30456;&#20851;&#39046;&#22495;&#30340;&#19968;&#20123;&#37325;&#35201;&#24212;&#29992;&#12290;&#20805;&#20998;&#21033;&#29992;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#26368;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;Generative AI&#27169;&#22411;&#30340;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#21368;&#36733;&#12289;&#35774;&#22791;&#31471;&#24494;&#35843;&#12289;&#32852;&#37030;&#23398;&#20064;&#12289;&#23433;&#20840;&#20197;&#21450;&#24320;&#21457;&#24037;&#20855;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#24046;&#36317;&#20197;&#21450;&#20351;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#23454;&#29616;&#30340;&#26377;&#24076;&#26395;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#33021;&#22815;&#28608;&#21457;&#26032;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with sensing, networking, and computing capabilities, Internet of Things (IoT) such as smartphones, wearables, smart speakers, and household robots have been seamlessly weaved into our daily lives. Recent advancements in Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold immense promise to push IoT to the next level. In this article, we share our vision and views on the benefits that Generative AI brings to IoT, and discuss some of the most important applications of Generative AI in IoT-related domains. Fully harnessing Generative AI in IoT is a complex challenge. We identify some of the most critical challenges including high resource demands of the Generative AI models, prompt engineering, on-device inference, offloading, on-device fine-tuning, federated learning, security, as well as development tools and benchmarks, and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;&#26410;&#25351;&#23450;&#30340;&#35270;&#35282;&#23398;&#20064;&#32452;&#21512;&#24615;&#22330;&#26223;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#28508;&#22312;&#34920;&#31034;&#20998;&#20026;&#19982;&#35270;&#35282;&#26080;&#20851;&#30340;&#37096;&#20998;&#21644;&#19982;&#35270;&#35282;&#30456;&#20851;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.01922</link><description>&lt;p&gt;
&#20174;&#22810;&#20010;&#26410;&#25351;&#23450;&#35270;&#35282;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Object-Centric Learning from Multiple Unspecified Viewpoints. (arXiv:2401.01922v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;&#26410;&#25351;&#23450;&#30340;&#35270;&#35282;&#23398;&#20064;&#32452;&#21512;&#24615;&#22330;&#26223;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#28508;&#22312;&#34920;&#31034;&#20998;&#20026;&#19982;&#35270;&#35282;&#26080;&#20851;&#30340;&#37096;&#20998;&#21644;&#19982;&#35270;&#35282;&#30456;&#20851;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#38750;&#24120;&#20016;&#23500;&#65292;&#19981;&#20165;&#22240;&#20026;&#23545;&#35937;&#21644;&#32972;&#26223;&#26377;&#26080;&#38480;&#21487;&#33021;&#30340;&#32452;&#21512;&#65292;&#32780;&#19988;&#22240;&#20026;&#21516;&#19968;&#22330;&#26223;&#30340;&#35266;&#23519;&#22312;&#35270;&#35282;&#21464;&#21270;&#26102;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#22312;&#35266;&#23519;&#22810;&#20010;&#35270;&#35282;&#30340;&#22810;&#29289;&#20307;&#35270;&#35273;&#22330;&#26223;&#26102;&#65292;&#20154;&#31867;&#21487;&#20197;&#20174;&#27599;&#20010;&#35270;&#35282;&#23545;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#24615;&#24863;&#30693;&#65292;&#21516;&#26102;&#23454;&#29616;&#25152;&#35859;&#30340;&#8220;&#23545;&#35937;&#24658;&#24120;&#24615;&#8221;&#65292;&#23613;&#31649;&#20855;&#20307;&#30340;&#35270;&#35282;&#26159;&#26410;&#30693;&#30340;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#31867;&#22312;&#31227;&#21160;&#36807;&#31243;&#20013;&#35782;&#21035;&#30456;&#21516;&#23545;&#35937;&#21644;&#39640;&#25928;&#23398;&#20064;&#35270;&#35273;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#35774;&#35745;&#20855;&#22791;&#31867;&#20284;&#33021;&#21147;&#30340;&#27169;&#22411;&#20855;&#26377;&#24456;&#22823;&#30340;&#21560;&#24341;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22810;&#20010;&#26410;&#25351;&#23450;&#30340;&#35270;&#35282;&#23398;&#20064;&#32452;&#21512;&#24615;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#28508;&#22312;&#34920;&#31034;&#20998;&#20026;&#19982;&#35270;&#35282;&#26080;&#20851;&#30340;&#37096;&#20998;&#21644;&#19982;&#35270;&#35282;&#30456;&#20851;&#30340;&#37096;&#20998;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual scenes are extremely diverse, not only because there are infinite possible combinations of objects and backgrounds but also because the observations of the same scene may vary greatly with the change of viewpoints. When observing a multi-object visual scene from multiple viewpoints, humans can perceive the scene compositionally from each viewpoint while achieving the so-called ``object constancy'' across different viewpoints, even though the exact viewpoints are untold. This ability is essential for humans to identify the same object while moving and to learn from vision efficiently. It is intriguing to design models that have a similar ability. In this paper, we consider a novel problem of learning compositional scene representations from multiple unspecified (i.e., unknown and unrelated) viewpoints without using any supervision and propose a deep generative model which separates latent representations into a viewpoint-independent part and a viewpoint-dependent part to solve th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25193;&#23637;&#20102;AstroLLaMA&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#30340;LLaMA-2&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#24067;&#20102;&#24102;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;</title><link>http://arxiv.org/abs/2401.01916</link><description>&lt;p&gt;
AstroLLaMA-Chat: &#20351;&#29992;&#23545;&#35805;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#25193;&#23637;AstroLLaMA
&lt;/p&gt;
&lt;p&gt;
AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01916
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25193;&#23637;&#20102;AstroLLaMA&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#30340;LLaMA-2&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#24067;&#20102;&#24102;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#22686;&#24378;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;7B&#21442;&#25968;&#30340;LLaMA-2&#27169;&#22411;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#19968;&#32452;&#32463;&#36807;&#31579;&#36873;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#20171;&#32461;&#21644;&#32467;&#35770;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#20687;GPT-4&#36825;&#26679;&#30340;&#36890;&#29992;LLMs&#22312;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#22238;&#31572;&#22330;&#26223;&#20013;&#30001;&#20110;&#26356;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26377;&#38480;&#36164;&#28304;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19987;&#38376;&#20027;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AstroLLaMA&#30340;&#25193;&#23637;&#65306;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23545;7B LLaMA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#26368;&#32456;&#21457;&#24067;&#20102;&#36866;&#29992;&#20110;&#31038;&#21306;&#20351;&#29992;&#30340;&#20855;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;&#20840;&#38754;&#30340;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#24182;&#23558;&#22312;&#21363;&#23558;&#21457;&#24067;&#30340;&#23436;&#25972;&#35770;&#25991;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;&#27169;&#22411;AstroLLaMA-Chat&#29616;&#24050;&#22312;...
&lt;/p&gt;
&lt;p&gt;
We explore the potential of enhancing LLM performance in astronomy-focused question-answering through targeted, continual pre-training. By employing a compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of astronomy corpus -- comprising abstracts, introductions, and conclusions -- we achieve notable improvements in specialized topic comprehension. While general LLMs like GPT-4 outperform in broader question-answering scenarios due to superior reasoning capabilities, our findings suggest that continual pre-training with limited resources can still enhance model performance on specialized topics. Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset, culminating in the release of the chat-enabled AstroLLaMA for community use. Comprehensive quantitative benchmarking is currently in progress and will be detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now available at
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#23567;&#26102;&#38388;&#27493;&#38271;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SSNN&#65289;&#65292;&#36890;&#36807;&#23558;SNN&#20998;&#25104;&#22810;&#20010;&#38454;&#27573;&#65292;&#36880;&#27493;&#32553;&#23567;&#26102;&#38388;&#27493;&#38271;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#30340;&#31070;&#32463;&#24418;&#24577;&#23545;&#35937;&#35782;&#21035;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#20998;&#31867;&#22120;&#35299;&#20915;&#20102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01912</link><description>&lt;p&gt;
&#32553;&#23567;&#26102;&#38388;&#27493;&#38271;&#65306;&#23454;&#29616;&#20855;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#24310;&#36831;&#31070;&#32463;&#24418;&#24577;&#23545;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network. (arXiv:2401.01912v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#23567;&#26102;&#38388;&#27493;&#38271;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SSNN&#65289;&#65292;&#36890;&#36807;&#23558;SNN&#20998;&#25104;&#22810;&#20010;&#38454;&#27573;&#65292;&#36880;&#27493;&#32553;&#23567;&#26102;&#38388;&#27493;&#38271;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#30340;&#31070;&#32463;&#24418;&#24577;&#23545;&#35937;&#35782;&#21035;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#20998;&#31867;&#22120;&#35299;&#20915;&#20102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30340;&#31070;&#32463;&#24418;&#24577;&#23545;&#35937;&#35782;&#21035;&#26159;&#20302;&#21151;&#32791;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SNNs&#22312;&#35782;&#21035;&#31070;&#32463;&#24418;&#24577;&#23545;&#35937;&#26102;&#23384;&#22312;&#26174;&#33879;&#30340;&#24310;&#36831;&#38382;&#39064;&#65292;&#38656;&#35201;&#20351;&#29992;10&#21040;40&#20010;&#25110;&#26356;&#22810;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#22312;&#20302;&#24310;&#36831;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;SNNs&#30340;&#24615;&#33021;&#20005;&#37325;&#38477;&#20302;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32553;&#23567;SNN&#65288;SSNN&#65289;&#26469;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#31070;&#32463;&#24418;&#24577;&#23545;&#35937;&#35782;&#21035;&#65292;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;SNNs&#20998;&#25104;&#22810;&#20010;&#38454;&#27573;&#65292;&#36880;&#27493;&#32553;&#23567;&#26102;&#38388;&#27493;&#38271;&#26469;&#20943;&#23569;&#25512;&#26029;&#24310;&#36831;&#12290;&#22312;&#26102;&#38388;&#27493;&#38271;&#32553;&#23567;&#36807;&#31243;&#20013;&#65292;&#26102;&#38388;&#21464;&#25442;&#22120;&#24179;&#28369;&#22320;&#36716;&#25442;&#26102;&#38388;&#23610;&#24230;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#20445;&#30041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;SNN&#28155;&#21152;&#22810;&#20010;&#26089;&#26399;&#20998;&#31867;&#22120;&#65292;&#20197;&#20943;&#36731;&#26367;&#20195;&#26799;&#24230;&#21644;&#30495;&#23454;&#26799;&#24230;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the pe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#20013;&#38750;&#37197;&#23545;&#35757;&#32451;&#24341;&#21457;&#30340;&#26631;&#31614;&#24046;&#24322;&#38382;&#39064;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20854;&#23545;&#21307;&#23398;FM&#20379;&#24212;&#38142;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.01911</link><description>&lt;p&gt;
&#23545;&#38750;&#37197;&#23545;&#30340;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#65306;MedCLIP&#30340;&#35797;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP. (arXiv:2401.01911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#20013;&#38750;&#37197;&#23545;&#35757;&#32451;&#24341;&#21457;&#30340;&#26631;&#31614;&#24046;&#24322;&#38382;&#39064;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20854;&#23545;&#21307;&#23398;FM&#20379;&#24212;&#38142;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#24050;&#32463;&#30830;&#31435;&#20102;&#20854;&#20316;&#20026;&#22522;&#30707;&#24615;&#36827;&#23637;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#20174;&#22823;&#37327;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#32780;&#21448;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#29031;&#23398;&#20064;&#21307;&#23398;FM MedCLIP&#37319;&#29992;&#20102;&#38750;&#37197;&#23545;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#12290;&#34429;&#28982;&#21307;&#23398;&#39046;&#22495;&#32463;&#24120;&#37319;&#29992;&#38750;&#37197;&#23545;&#35757;&#32451;&#26469;&#22686;&#24378;&#25968;&#25454;&#65292;&#20294;&#19982;&#35813;&#26041;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#23433;&#20840;&#38382;&#39064;&#30340;&#25506;&#32034;&#24182;&#26410;&#36319;&#19978;&#20854;&#23454;&#38469;&#20351;&#29992;&#30340;&#27493;&#20240;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38750;&#37197;&#23545;&#35757;&#32451;&#20013;&#22266;&#26377;&#30340;&#22686;&#24378;&#33021;&#21147;&#20063;&#34920;&#26126;&#24494;&#23567;&#30340;&#26631;&#31614;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#37325;&#35201;&#30340;&#27169;&#22411;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26631;&#31614;&#24046;&#24322;&#26694;&#26550;&#21270;&#20026;&#21518;&#38376;&#25915;&#20987;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#20998;&#26512;&#20854;&#23545;&#25972;&#20010;FM&#20379;&#24212;&#38142;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20027;&#35201;&#22260;&#32469;&#23545;&#21307;&#23398;FMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, foundation models (FMs) have solidified their role as cornerstone advancements in the deep learning domain. By extracting intricate patterns from vast datasets, these models consistently achieve state-of-the-art results across a spectrum of downstream tasks, all without necessitating extensive computational resources. Notably, MedCLIP, a vision-language contrastive learning-based medical FM, has been designed using unpaired image-text training. While the medical domain has often adopted unpaired training to amplify data, the exploration of potential security concerns linked to this approach hasn't kept pace with its practical usage. Notably, the augmentation capabilities inherent in unpaired training also indicate that minor label discrepancies can result in significant model deviations. In this study, we frame this label discrepancy as a backdoor attack problem. We further analyze its impact on medical FMs throughout the FM supply chain. Our evaluation primarily revol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#31890;&#23376;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#22312;ALICE&#23454;&#39564;&#20013;&#30001;&#20110;&#19981;&#21516;&#25506;&#27979;&#22120;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#32780;&#20135;&#29983;&#30340;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01905</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#31890;&#23376;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Machine-learning-based particle identification with missing data. (arXiv:2401.01905v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#31890;&#23376;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#22312;ALICE&#23454;&#39564;&#20013;&#30001;&#20110;&#19981;&#21516;&#25506;&#27979;&#22120;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#32780;&#20135;&#29983;&#30340;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;CERN&#30340;Large Hadron Collider&#30340;ALICE&#23454;&#39564;&#33539;&#22260;&#20869;&#36827;&#34892;&#31890;&#23376;&#37492;&#21035;&#65288;PID&#65289;&#12290;&#37492;&#23450;LHC&#25552;&#20379;&#30340;&#36229;&#30456;&#23545;&#35770;&#30896;&#25758;&#20135;&#29289;&#26159;ALICE&#30340;&#20851;&#38190;&#30446;&#26631;&#20043;&#19968;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;PID&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#36873;&#25321;&#65292;&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#29702;&#35770;&#27169;&#25311;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#25552;&#39640;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#27491;&#30830;&#30340;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#23376;&#25506;&#27979;&#22120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26816;&#27979;&#25216;&#26415;&#65292;&#20197;&#21450;&#26377;&#38480;&#30340;&#25506;&#27979;&#22120;&#25928;&#29575;&#21644;&#25509;&#21463;&#24230;&#65292;&#20135;&#29983;&#30340;&#31890;&#23376;&#24182;&#19981;&#24635;&#26159;&#22312;ALICE&#30340;&#25152;&#26377;&#32452;&#20214;&#20013;&#20135;&#29983;&#20449;&#21495;&#12290;&#36825;&#23548;&#33268;&#25968;&#25454;&#20013;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26080;&#27861;&#35757;&#32451;&#36825;&#20123;&#31034;&#20363;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36339;&#36807;&#20102;&#25968;&#25454;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#31181;&#36866;&#29992;&#20110;PID&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a novel method for Particle Identification (PID) within the scope of the ALICE experiment at the Large Hadron Collider at CERN. Identifying products of ultrarelativisitc collisions delivered by the LHC is one of the crucial objectives of ALICE. Typically employed PID methods rely on hand-crafted selections, which compare experimental data to theoretical simulations. To improve the performance of the baseline methods, novel approaches use machine learning models that learn the proper assignment in a classification task. However, because of the various detection techniques used by different subdetectors, as well as the limited detector efficiency and acceptance, produced particles do not always yield signals in all of the ALICE components. This results in data with missing values. Machine learning techniques cannot be trained with such examples, so a significant part of the data is skipped during training. In this work, we propose the first method for PID that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22768;&#35465;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33041;&#30005;&#20449;&#21495;&#20998;&#31867;&#20013;&#38450;&#24481;&#23433;&#20840;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#21644;&#24341;&#20837;&#22768;&#35465;&#26426;&#21046;&#26469;&#20445;&#25252;&#38544;&#31169;&#21644;&#32531;&#35299;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.01896</link><description>&lt;p&gt;
&#22522;&#20110;&#22768;&#35465;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26469;&#20943;&#36731;&#33041;&#30005;&#20449;&#21495;&#20998;&#31867;&#20013;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Reputation-Based Federated Learning Defense to Mitigate Threats in EEG Signal Classification. (arXiv:2401.01896v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22768;&#35465;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33041;&#30005;&#20449;&#21495;&#20998;&#31867;&#20013;&#38450;&#24481;&#23433;&#20840;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#21644;&#24341;&#20837;&#22768;&#35465;&#26426;&#21046;&#26469;&#20445;&#25252;&#38544;&#31169;&#21644;&#32531;&#35299;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22768;&#35465;&#30340;&#23041;&#32961;&#32531;&#35299;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#38450;&#24481;&#30005;&#33041;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20998;&#31867;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#12290;&#34429;&#28982;&#30001;&#20110;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#65288;BCI&#65289;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#33041;&#30005;&#20449;&#21495;&#20998;&#26512;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#30001;&#20110;&#20998;&#24067;&#24335;&#30340;EEG&#25968;&#25454;&#30340;&#24615;&#36136;&#20197;&#21450;&#30456;&#20851;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#26159;&#24456;&#38590;&#20026;EEG&#20998;&#26512;&#21019;&#24314;&#39640;&#25928;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26694;&#26550;&#21033;&#29992;&#20102;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#19982;&#26469;&#33258;&#20998;&#25955;&#28304;&#30340;&#26412;&#22320;&#21270;&#25968;&#25454;&#30340;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#22768;&#35465;&#30340;&#26426;&#21046;&#26469;&#32531;&#35299;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#21644;&#35782;&#21035;&#21463;&#25439;&#21442;&#19982;&#32773;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#22768;&#35465;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26694;&#26550;&#30340;&#25928;&#29575;&#65292;&#22522;&#20110;&#25968;&#25454;&#35757;&#32451;&#30340;&#39118;&#38505;&#32423;&#21035;&#36827;&#34892;&#20102;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a reputation-based threat mitigation framework that defends potential security threats in electroencephalogram (EEG) signal classification during model aggregation of Federated Learning. While EEG signal analysis has attracted attention because of the emergence of brain-computer interface (BCI) technology, it is difficult to create efficient learning models for EEG analysis because of the distributed nature of EEG data and related privacy and security concerns. To address these challenges, the proposed defending framework leverages the Federated Learning paradigm to preserve privacy by collaborative model training with localized data from dispersed sources and introduces a reputation-based mechanism to mitigate the influence of data poisoning attacks and identify compromised participants. To assess the efficiency of the proposed reputation-based federated learning defense framework, data poisoning attacks based on the risk level of training data derived by Explainab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#23431;&#23449;&#23548;&#21521;&#30340;&#21327;&#21516;&#28145;&#24230;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#26816;&#27979;&#21644;&#20572;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;CDL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24369;&#28857;&#65292;&#20445;&#25252;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#21644;&#26412;&#22320;&#25935;&#24863;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.01895</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#20803;&#23431;&#23449;&#23548;&#21521;&#30340;&#21327;&#21516;&#28145;&#24230;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#26816;&#27979;&#21644;&#20572;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Robust Adversary Detection-Deactivation Method for Metaverse-oriented Collaborative Deep Learning. (arXiv:2401.01895v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#23431;&#23449;&#23548;&#21521;&#30340;&#21327;&#21516;&#28145;&#24230;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#26816;&#27979;&#21644;&#20572;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;CDL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24369;&#28857;&#65292;&#20445;&#25252;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#21644;&#26412;&#22320;&#25935;&#24863;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#27491;&#36235;&#21521;&#20110;&#21019;&#24314;&#19968;&#20010;&#21487;&#20197;&#23558;&#29616;&#23454;&#19990;&#30028;&#36716;&#31227;&#21040;&#22823;&#37327;&#23454;&#26102;&#20114;&#21160;&#25903;&#25345;&#30340;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#25968;&#23383;&#24773;&#22659;&#12290;&#39044;&#35757;&#32451;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#27491;&#22312;&#23637;&#31034;&#23427;&#20204;&#22312;&#24110;&#21161;&#20803;&#23431;&#23449;&#23454;&#29616;&#20248;&#31168;&#21709;&#24212;&#26102;&#30340;&#19981;&#26029;&#22686;&#24378;&#33021;&#21147;&#65292;&#32780;&#29616;&#22312;&#65292;&#35768;&#22810;&#22823;&#22411;&#27169;&#22411;&#37117;&#26159;&#36890;&#36807;&#22810;&#20010;&#21442;&#19982;&#32773;&#21327;&#21516;&#35757;&#32451;&#30340;&#65292;&#36825;&#31181;&#26041;&#24335;&#34987;&#31216;&#20026;&#21327;&#21516;&#28145;&#24230;&#23398;&#20064;&#65288;CDL&#65289;&#12290;&#28982;&#32780;&#65292;CDL&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#20960;&#20010;&#23433;&#20840;&#24369;&#28857;&#65292;&#21487;&#33021;&#23545;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#25110;&#20010;&#20307;&#23454;&#20307;&#25317;&#26377;&#30340;&#26412;&#22320;&#25935;&#24863;&#25968;&#25454;&#38598;&#36896;&#25104;&#33268;&#21629;&#25915;&#20987;&#12290;&#22312;CDL&#20013;&#65292;&#24694;&#24847;&#21442;&#19982;&#32773;&#21487;&#20197;&#38544;&#34255;&#22312;&#20027;&#35201;&#26080;&#36764;&#32773;&#20013;&#65292;&#24182;&#24708;&#26080;&#22768;&#24687;&#22320;&#19978;&#20256;&#27450;&#39575;&#24615;&#21442;&#25968;&#20197;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#25110;&#32773;&#20182;&#20204;&#21487;&#20197;&#28389;&#29992;&#19979;&#36733;&#30340;&#21442;&#25968;&#26500;&#24314;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20197;&#38750;&#27861;&#33719;&#21462;&#20182;&#20154;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaverse is trending to create a digital circumstance that can transfer the real world to an online platform supported by large quantities of real-time interactions. Pre-trained Artificial Intelligence (AI) models are demonstrating their increasing capability in aiding the metaverse to achieve an excellent response with negligible delay, and nowadays, many large models are collaboratively trained by various participants in a manner named collaborative deep learning (CDL). However, several security weaknesses can threaten the safety of the CDL training process, which might result in fatal attacks to either the pre-trained large model or the local sensitive data sets possessed by an individual entity. In CDL, malicious participants can hide within the major innocent and silently uploads deceptive parameters to degenerate the model performance, or they can abuse the downloaded parameters to construct a Generative Adversarial Network (GAN) to acquire the private information of others ille
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#25442;&#25968;&#20540;&#36890;&#37327;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.01783</link><description>&lt;p&gt;
&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#36924;&#36817;&#25968;&#20540;&#36890;&#37327;&#30340;&#36229;&#27874;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#25442;&#25968;&#20540;&#36890;&#37327;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#29992;&#20110;&#25968;&#20540;&#35299;PDE&#65292;&#26368;&#36817;&#21457;&#23637;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22914;PINN&#21644;&#31070;&#32463;&#31639;&#23376;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#32570;&#28857;&#65292;&#26377;&#24456;&#22810;&#31181;&#31867;&#22411;&#30340;&#30740;&#31350;&#23558;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#25968;&#20540;&#26041;&#26696;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#65292;&#23558;&#25968;&#20540;&#26041;&#26696;&#20013;&#30340;&#25968;&#20540;&#36890;&#37327;&#26367;&#25442;&#20026;&#31070;&#32463;&#31639;&#23376;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#21463;&#25968;&#20540;&#26041;&#26696;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;FNO&#36924;&#36817;&#25968;&#20540;&#36890;&#37327;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19982;&#21407;&#22987;&#26041;&#27861;&#30340;&#27604;&#36739;&#20855;&#26377;&#25968;&#20540;&#26041;&#26696;&#21644;FNO&#30340;&#20248;&#21183;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical numerical schemes exist for solving PDEs numerically, and recently, neural network-based methods have been developed. However, methodologies using neural networks, such as PINN and neural operators, lack robustness and generalization power. To compensate for such drawbacks, there are many types of research combining classical numerical schemes and machine learning methods by replacing a small portion of the numerical schemes with neural networks. In this work, we focus on hyperbolic conservation laws and replace numerical fluxes in the numerical schemes by neural operator. For this, we construct losses that are motivated by numerical schemes for conservation laws and approximate numerical flux by FNO. Through experiments, we show that our methodology has advantages of both numerical schemes and FNO by comparing with original methods. For instance, we demonstrate our method gains robustness, resolution invariance property, and feasibility of a data-driven method. Our method es
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#37329;&#34701;&#20132;&#26131;&#19978;&#19979;&#25991;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#21345;&#29255;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.01641</link><description>&lt;p&gt;
&#26397;&#30528;&#22522;&#20110;&#20132;&#26131;&#24207;&#21015;&#30340;&#22522;&#30784;&#37319;&#36141;&#27169;&#22411;: &#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#33258;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#37329;&#34701;&#20132;&#26131;&#19978;&#19979;&#25991;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#21345;&#29255;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#29616;&#20195;&#37329;&#34701;&#31995;&#32479;&#20013;&#65292;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;&#21644;&#30041;&#23384;&#39044;&#27979;&#31561;&#29992;&#20363;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#22522;&#20110;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;&#25163;&#21160;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#30446;&#21069;&#65292;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#23558;&#20854;&#24212;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#33719;&#21462;&#37329;&#34701;&#20132;&#26131;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21253;&#21547;51&#20159;&#31508;&#20132;&#26131;&#30340;180&#20010;&#21457;&#21345;&#38134;&#34892;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20445;&#30041;&#25968;&#25454;&#38598;&#19978;&#30340;&#21345;&#29255;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#12290;&#23884;&#20837;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly impro
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#22312;LLM&#30340;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#37117;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.00625</link><description>&lt;p&gt;
&#36229;&#36234;&#25928;&#29575;&#65306;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models. (arXiv:2401.00625v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#22312;LLM&#30340;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#37117;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#36805;&#29467;&#21457;&#23637;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#27493;&#65292;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#24102;&#26469;&#20102;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#21644;&#36130;&#21153;&#36164;&#28304;&#39640;&#28040;&#32791;&#31561;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#23457;&#26597;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#22686;&#24378;LLMs&#36164;&#28304;&#25928;&#29575;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#26681;&#25454;&#20248;&#21270;&#37325;&#28857;&#23545;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65306;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;LLM&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#65288;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#12289;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#31995;&#32479;&#35774;&#35745;&#65289;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#35843;&#30740;&#36890;&#36807;&#29305;&#23450;&#36164;&#28304;&#31867;&#22411;&#24341;&#20837;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#36164;&#28304;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between var
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#23398;&#35745;&#31639;&#30340;&#29289;&#29702;&#30456;&#20851;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#35299;&#20915;&#39640;&#32500;PDE&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#36229;&#20302;&#24310;&#36831;&#65292;&#36890;&#36807;&#36991;&#20813;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#21644;&#20351;&#29992;&#24352;&#37327;&#21387;&#32553;&#26041;&#27861;&#26469;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00413</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#21270;&#12289;&#26080;&#21453;&#21521;&#20256;&#25773;&#30340;&#20809;&#23398;PINN&#35757;&#32451;&#23454;&#29616;&#23454;&#26102;FJ/MAC PDE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Real-Time FJ/MAC PDE Solvers via Tensorized, Back-Propagation-Free Optical PINN Training. (arXiv:2401.00413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#23398;&#35745;&#31639;&#30340;&#29289;&#29702;&#30456;&#20851;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#35299;&#20915;&#39640;&#32500;PDE&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#36229;&#20302;&#24310;&#36831;&#65292;&#36890;&#36807;&#36991;&#20813;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#21644;&#20351;&#29992;&#24352;&#37327;&#21387;&#32553;&#26041;&#27861;&#26469;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#20540;&#35299;&#21508;&#31181;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#26102;&#38388;&#12289;&#33021;&#37327;&#28040;&#32791;&#21644;&#30828;&#20214;&#36164;&#28304;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#22330;&#26223;&#65288;&#22914;&#33258;&#20027;&#31995;&#32479;&#12289;&#36229;&#38899;&#36895;&#27969;&#21160;&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#28304;&#39044;&#31639;&#21644;&#38656;&#35201;&#36817;&#23454;&#26102;&#21709;&#24212;&#12290;&#21033;&#29992;&#20809;&#23398;&#35745;&#31639;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#33455;&#29255;&#19978;&#30340;&#29289;&#29702;&#30456;&#20851;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#20351;&#29992;fJ/MAC&#20809;&#23376;&#21151;&#32791;&#21644;&#36229;&#20302;&#24310;&#36831;&#35299;&#20915;&#39640;&#32500;PDE&#12290;&#23613;&#31649;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36229;&#39640;&#36895;&#24230;&#65292;&#20294;&#22312;&#20809;&#23398;&#33455;&#29255;&#19978;&#35757;&#32451;PINN&#26159;&#22256;&#38590;&#30340;&#65292;&#21407;&#22240;&#26159;&#65288;1&#65289;&#20809;&#23376;&#35774;&#22791;&#30340;&#23610;&#23544;&#36739;&#22823;&#65292;&#65288;2&#65289;&#32570;&#20047;&#21487;&#25193;&#23637;&#30340;&#20809;&#23398;&#23384;&#20648;&#35774;&#22791;&#26469;&#23384;&#20648;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#23454;&#38469;&#30340;&#20809;&#23398;PINN&#35757;&#32451;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#36991;&#20813;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#24352;&#37327;&#21387;&#32553;&#26041;&#27861;&#26469;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations (PDEs) numerically often requires huge computing time, energy cost, and hardware resources in practical applications. This has limited their applications in many scenarios (e.g., autonomous systems, supersonic flows) that have a limited energy budget and require near real-time response. Leveraging optical computing, this paper develops an on-chip training framework for physics-informed neural networks (PINNs), aiming to solve high-dimensional PDEs with fJ/MAC photonic power consumption and ultra-low latency. Despite the ultra-high speed of optical neural networks, training a PINN on an optical chip is hard due to (1) the large size of photonic devices, and (2) the lack of scalable optical memory devices to store the intermediate results of back-propagation (BP). To enable realistic optical PINN training, this paper presents a scalable method to avoid the BP process. We also employ a tensor-compressed approach to improve the convergence and scalabi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.16713</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26465;&#20214;&#33258;&#27880;&#24847;&#21147;&#25554;&#34917;&#65288;CSAI&#65289;&#27169;&#22411;&#20197;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32454;&#33410;&#30340;&#26465;&#20214;&#38544;&#34255;&#29366;&#24577;&#21021;&#22987;&#21270;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25554;&#34917;&#25216;&#26415;&#19981;&#21516;&#65292;&#23427;&#29305;&#21035;&#38024;&#23545;&#21307;&#30103;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;CSAI&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data. This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets. By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.16430</link><description>&lt;p&gt;
&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20174;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#23398;&#20064;&#65292;&#39318;&#20808;&#25311;&#21512;&#20559;&#22909;&#20998;&#25968;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;RLHF&#30340;&#22788;&#29702;&#36807;&#31243;&#22797;&#26434;&#12289;&#32791;&#26102;&#19988;&#19981;&#31283;&#23450;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#20351;&#29992;&#31163;&#31574;&#30053;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;DPO&#20351;&#29992;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#21644;&#23545;&#25968;&#25439;&#22833;&#65292;&#23548;&#33268;&#22312;&#20559;&#22909;&#25509;&#36817;&#30830;&#23450;&#24615;&#26102;&#24573;&#30053;&#20102;KL&#27491;&#21017;&#21270;&#39033;&#32780;&#36807;&#24230;&#25311;&#21512;&#20559;&#22909;&#25968;&#25454;&#12290;IPO&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26681;&#26597;&#25214;&#30340;&#25104;&#23545;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#26469;&#35299;&#20915;&#24573;&#30053;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#65292;&#24182;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20294;&#26159;IPO&#30340;&#25104;&#23545;&#25439;&#22833;&#20173;&#28982;&#26080;&#27861;&#20351;KL&#27491;&#21017;&#21270;&#29983;&#25928;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20559;&#22909;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#24207;&#21015;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#29992;&#20110;&#29983;&#25104;&#27835;&#30103;&#32957;&#12290;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#25429;&#25417;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.15665</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#32957;&#29983;&#25104;&#30340;&#22810;&#27169;&#24335;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Multi-Modal Contrastive Diffusion Model for Therapeutic Peptide Generation. (arXiv:2312.15665v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#24207;&#21015;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#29992;&#20110;&#29983;&#25104;&#27835;&#30103;&#32957;&#12290;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#25429;&#25417;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#32957;&#20195;&#34920;&#19968;&#31867;&#23545;&#27835;&#30103;&#20154;&#31867;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#30340;&#33647;&#29289;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#29983;&#25104;&#27835;&#30103;&#32957;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20165;&#21033;&#29992;&#24207;&#21015;&#25110;&#32467;&#26500;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#65288;MMCD&#65289;&#65292;&#23558;&#24207;&#21015;&#21644;&#32467;&#26500;&#27169;&#24577;&#34701;&#21512;&#22312;&#19968;&#20010;&#25193;&#25955;&#26694;&#26550;&#20013;&#65292;&#20849;&#21516;&#29983;&#25104;&#26032;&#30340;&#32957;&#24207;&#21015;&#21644;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MMCD&#20998;&#21035;&#26500;&#24314;&#20102;&#24207;&#21015;&#27169;&#24577;&#21644;&#32467;&#26500;&#27169;&#24577;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#27599;&#20010;&#25193;&#25955;&#26102;&#38388;&#27493;&#20013;&#36827;&#34892;&#21306;&#38388;&#23545;&#27604;&#21644;&#20869;&#23545;&#27604;&#65292;&#26088;&#22312;&#25429;&#25417;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#24182;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#21306;&#38388;&#23545;&#27604;&#36890;&#36807;&#26368;&#22823;&#21270;&#23884;&#20837;&#30340;&#19968;&#33268;&#24615;&#26469;&#23545;&#40784;&#32957;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#65292;&#32780;&#20869;&#23545;&#27604;&#21017;&#36890;&#36807;&#21306;&#20998;&#23884;&#20837;&#26469;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Therapeutic peptides represent a unique class of pharmaceutical agents crucial for the treatment of human diseases. Recently, deep generative models have exhibited remarkable potential for generating therapeutic peptides, but they only utilize sequence or structure information alone, which hinders the performance in generation. In this study, we propose a Multi-Modal Contrastive Diffusion model (MMCD), fusing both sequence and structure modalities in a diffusion framework to co-generate novel peptide sequences and structures. Specifically, MMCD constructs the sequence-modal and structure-modal diffusion models, respectively, and devises a multi-modal contrastive learning strategy with intercontrastive and intra-contrastive in each diffusion timestep, aiming to capture the consistency between two modalities and boost model performance. The inter-contrastive aligns sequences and structures of peptides by maximizing the agreement of their embeddings, while the intra-contrastive differenti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#23041;&#32961;&#65292;&#30740;&#31350;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#21407;&#22987;&#30446;&#26631;&#26032;&#38395;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#24341;&#20837;&#27745;&#26579;&#25968;&#25454;&#26469;&#25805;&#32437;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2312.15228</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25968;&#25454;&#27745;&#26579;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65306;&#22914;&#20309;&#20351;&#27169;&#22411;&#22312;&#19981;&#20462;&#25913;&#30446;&#26631;&#26032;&#38395;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#38169;&#35823;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It. (arXiv:2312.15228v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#23041;&#32961;&#65292;&#30740;&#31350;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#21407;&#22987;&#30446;&#26631;&#26032;&#38395;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#24341;&#20837;&#27745;&#26579;&#25968;&#25454;&#26469;&#25805;&#32437;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#21407;&#22987;&#30446;&#26631;&#26032;&#38395;&#30340;&#24773;&#20917;&#19979;&#30772;&#22351;&#22312;&#32447;&#23398;&#20064;&#26816;&#27979;&#22120;&#23545;&#29305;&#23450;&#26032;&#38395;&#20869;&#23481;&#30340;&#24615;&#33021;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#23436;&#20840;&#25511;&#21046;&#25152;&#26377;&#20449;&#24687;&#65292;&#36825;&#31181;&#24773;&#20917;&#30830;&#23454;&#21487;&#33021;&#21457;&#29983;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#21487;&#33021;&#36890;&#36807;&#23558;&#27745;&#26579;&#25968;&#25454;&#24341;&#20837;&#35757;&#32451;&#25968;&#25454;&#26469;&#25805;&#32437;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#22797;&#26434;&#24615;&#21644;&#25915;&#20987;&#31867;&#22411;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#23545;&#27492;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#21508;&#19981;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake news detection models are critical to countering disinformation but can be manipulated through adversarial attacks. In this position paper, we analyze how an attacker can compromise the performance of an online learning detector on specific news content without being able to manipulate the original target news. In some contexts, such as social networks, where the attacker cannot exert complete control over all the information, this scenario can indeed be quite plausible. Therefore, we show how an attacker could potentially introduce poisoning data into the training data to manipulate the behavior of an online learning method. Our initial findings reveal varying susceptibility of logistic regression models based on complexity and attack type.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#30340;&#27010;&#29575;&#24314;&#27169;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#27599;&#20010;&#20107;&#20214;&#19982;&#19968;&#32452;&#39033;&#30446;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#12290;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#20219;&#20309;&#24378;&#24230;&#20026;&#22522;&#30784;&#30340;&#36882;&#24402;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#20851;&#20110;&#24207;&#21015;&#21382;&#21490;&#26465;&#20214;&#19979;&#30340;&#27010;&#29575;&#26597;&#35810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15045</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#30340;&#27010;&#29575;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Modeling for Sequences of Sets in Continuous-Time. (arXiv:2312.15045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#30340;&#27010;&#29575;&#24314;&#27169;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#27599;&#20010;&#20107;&#20214;&#19982;&#19968;&#32452;&#39033;&#30446;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#12290;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#20219;&#20309;&#24378;&#24230;&#20026;&#22522;&#30784;&#30340;&#36882;&#24402;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#20851;&#20110;&#24207;&#21015;&#21382;&#21490;&#26465;&#20214;&#19979;&#30340;&#27010;&#29575;&#26597;&#35810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#30340;&#32479;&#35745;&#21442;&#25968;&#27169;&#22411;&#24037;&#20855;&#31665;&#20013;&#65292;&#31070;&#32463;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#34917;&#20805;&#12290;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#27599;&#20010;&#20107;&#20214;&#19982;&#21333;&#20010;&#39033;&#30446;&#65288;&#21333;&#20010;&#20107;&#20214;&#31867;&#22411;&#25110;&#8220;&#26631;&#35760;&#8221;&#65289;&#30456;&#20851;&#32852;&#30340;&#24207;&#21015;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#27599;&#20010;&#20107;&#20214;&#19982;&#19968;&#32452;&#39033;&#30446;&#30456;&#20851;&#32852;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#38598;&#21512;&#25968;&#20540;&#25968;&#25454;&#24314;&#27169;&#26694;&#26550;&#65292;&#19982;&#20219;&#20309;&#22522;&#20110;&#24378;&#24230;&#30340;&#36882;&#24402;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#20860;&#23481;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#25512;&#29702;&#26041;&#27861;&#65292;&#21487;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22238;&#31572;&#35832;&#22914;&#8220;&#22312;&#32771;&#34385;&#24207;&#21015;&#21382;&#21490;&#30340;&#26465;&#20214;&#19979;&#65292;&#39033;&#30446;A&#22312;&#39033;&#30446;B&#20043;&#21069;&#35266;&#23519;&#21040;&#30340;&#27010;&#29575;&#8221;&#31561;&#27010;&#29575;&#26597;&#35810;&#38382;&#39064;&#12290;&#30001;&#20110;&#38382;&#39064;&#35774;&#32622;&#30340;&#36830;&#32493;&#26102;&#38388;&#24615;&#36136;&#21644;&#27599;&#20010;&#20107;&#20214;&#30340;&#28508;&#22312;&#32467;&#26524;&#31354;&#38388;&#30340;&#32452;&#21512;&#26497;&#22823;&#65292;&#23545;&#20110;&#31070;&#32463;&#27169;&#22411;&#26469;&#35828;&#65292;&#35745;&#31639;&#36825;&#20123;&#26597;&#35810;&#30340;&#31934;&#30830;&#31572;&#26696;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural marked temporal point processes have been a valuable addition to the existing toolbox of statistical parametric models for continuous-time event data. These models are useful for sequences where each event is associated with a single item (a single type of event or a "mark") -- but such models are not suited for the practical situation where each event is associated with a set of items. In this work, we develop a general framework for modeling set-valued data in continuous-time, compatible with any intensity-based recurrent neural point process model. In addition, we develop inference methods that can use such models to answer probabilistic queries such as "the probability of item $A$ being observed before item $B$," conditioned on sequence history. Computing exact answers for such queries is generally intractable for neural models due to both the continuous-time nature of the problem setting and the combinatorially-large space of potential outcomes for each event. To address th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31561;&#21464;&#24615;&#30340;&#24187;&#35273;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#25104;&#22240;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#31243;&#24230;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#20989;&#25968;&#12290;&#36890;&#36807;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#22312;&#33719;&#24471;&#31561;&#21464;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#34920;&#26126;&#26576;&#20123;&#31867;&#22411;&#30340;&#31561;&#21464;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#31038;&#20132;&#20851;&#31995;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2312.14504</link><description>&lt;p&gt;
&#22522;&#20110;&#31561;&#21464;&#24615;&#30340;&#24187;&#35273;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Hallucinations based on Equivariance. (arXiv:2312.14504v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31561;&#21464;&#24615;&#30340;&#24187;&#35273;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#25104;&#22240;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#31243;&#24230;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#20989;&#25968;&#12290;&#36890;&#36807;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#22312;&#33719;&#24471;&#31561;&#21464;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#34920;&#26126;&#26576;&#20123;&#31867;&#22411;&#30340;&#31561;&#21464;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#31038;&#20132;&#20851;&#31995;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#33719;&#21462;&#21019;&#24314;&#23545;&#24187;&#35273;&#20813;&#30123;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#24448;&#24448;&#24402;&#22240;&#20110;&#23545;&#29616;&#23454;&#31038;&#20132;&#20851;&#31995;&#30340;&#35823;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20551;&#35774;&#33021;&#22815;&#24443;&#24213;&#25484;&#25569;&#25152;&#26377;&#36825;&#20123;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#19981;&#20250;&#20986;&#29616;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#25552;&#20986;&#20102;&#26576;&#20123;&#31867;&#22411;&#30340;&#31561;&#21464;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#21644;&#29702;&#35299;&#36825;&#20123;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#31243;&#24230;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#34913;&#37327;&#20102;&#23427;&#20204;&#33719;&#24471;&#31561;&#21464;&#24615;&#30340;&#31243;&#24230;&#12290;&#21033;&#29992;&#36825;&#20010;&#25351;&#26631;&#65292;&#25105;&#27979;&#35797;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#33719;&#24471;&#23383;&#31526;&#32423;&#31561;&#21464;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#24341;&#20837;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;T5&#65288;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#21464;&#21387;&#22120;&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#29702;&#35299;&#32463;&#36807;&#25490;&#21015;&#30340;&#36755;&#20837;&#25991;&#26412;&#32780;&#26080;&#38656;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to acquire knowledge for creating very large language models that are immune to hallucinations. Hallucinations in contemporary large language models are often attributed to a misunderstanding of real-world social relationships. Therefore, I hypothesize that very large language models capable of thoroughly grasping all these relationships will be free from hallucinations. Additionally, I propose that certain types of equivariant language models are adept at learning and understanding these relationships. Building on this, I have developed a specialized cross-entropy error function to create a hallucination scale for language models, which measures their extent of equivariance acquisition. Utilizing this scale, I tested language models for their ability to acquire character-level equivariance. In particular, I introduce and employ a novel technique based on T5 (Text To Text Transfer Transformer) that efficiently understands permuted input texts without the need for explic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#29305;&#24449;&#30340;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#39057;&#35889;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#19981;&#26029;&#23398;&#20064;&#20197;&#20998;&#31867;&#27700;&#22768;&#20449;&#21495;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#20851;&#31995;&#21644;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2312.13143</link><description>&lt;p&gt;
&#22522;&#20110;&#26174;&#33879;&#29305;&#24449;&#30340;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Underwater Acoustic Signal Recognition Based on Salient Feature. (arXiv:2312.13143v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#29305;&#24449;&#30340;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#39057;&#35889;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#19981;&#26029;&#23398;&#20064;&#20197;&#20998;&#31867;&#27700;&#22768;&#20449;&#21495;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#20851;&#31995;&#21644;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22797;&#26434;&#29615;&#22659;&#20013;&#27700;&#22768;&#20449;&#21495;&#30340;&#35782;&#21035;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#20027;&#27969;&#30340;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;&#20027;&#35201;&#20381;&#36182;&#20110;&#26102;&#39057;&#20998;&#26512;&#26469;&#25552;&#21462;&#39057;&#35889;&#29305;&#24449;&#65292;&#22312;&#35813;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35782;&#21035;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#19987;&#23478;&#31995;&#32479;&#65292;&#38754;&#20020;&#30528;&#30693;&#35782;&#24211;&#21463;&#38480;&#21644;&#22788;&#29702;&#22797;&#26434;&#20851;&#31995;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38480;&#21046;&#28304;&#20110;&#35268;&#21017;&#25110;&#25512;&#29702;&#24341;&#25806;&#30340;&#22797;&#26434;&#24615;&#21644;&#32500;&#25252;&#22256;&#38590;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#39057;&#35889;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#19981;&#26029;&#23398;&#20064;&#20197;&#20998;&#31867;&#27700;&#22768;&#20449;&#21495;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#25277;&#35937;&#29305;&#24449;&#65292;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of technology, the recognition of underwater acoustic signals in complex environments has become increasingly crucial. Currently, mainstream underwater acoustic signal recognition relies primarily on time-frequency analysis to extract spectral features, finding widespread applications in the field. However, existing recognition methods heavily depend on expert systems, facing limitations such as restricted knowledge bases and challenges in handling complex relationships. These limitations stem from the complexity and maintenance difficulties associated with rules or inference engines. Recognizing the potential advantages of deep learning in handling intricate relationships, this paper proposes a method utilizing neural networks for underwater acoustic signal recognition. The proposed approach involves continual learning of features extracted from spectra for the classification of underwater acoustic signals. Deep learning models can automatically learn abstra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLP-Net&#30340;&#36229;&#36731;&#37327;&#32423;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#32593;&#32476;&#65292;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#26426;&#21046;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#39640;&#35745;&#31639;&#36895;&#24230;&#12290;&#19982;&#24120;&#35265;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#19981;&#21516;&#65292;&#20351;&#29992;&#29305;&#24449;&#36866;&#24212;&#27169;&#22359;&#23454;&#29616;&#22810;&#23610;&#24230;&#20449;&#24687;&#35299;&#30721;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#22312;&#20934;&#30830;&#24230;&#21644;Dice&#31995;&#25968;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.12789</link><description>&lt;p&gt;
SLP-Net&#65306;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#30340;&#39640;&#25928;&#36731;&#37327;&#32423;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SLP-Net:An efficient lightweight network for segmentation of skin lesions. (arXiv:2312.12789v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLP-Net&#30340;&#36229;&#36731;&#37327;&#32423;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#32593;&#32476;&#65292;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#26426;&#21046;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#39640;&#35745;&#31639;&#36895;&#24230;&#12290;&#19982;&#24120;&#35265;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#19981;&#21516;&#65292;&#20351;&#29992;&#29305;&#24449;&#36866;&#24212;&#27169;&#22359;&#23454;&#29616;&#22810;&#23610;&#24230;&#20449;&#24687;&#35299;&#30721;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#22312;&#20934;&#30830;&#24230;&#21644;Dice&#31995;&#25968;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#40657;&#32032;&#30244;&#30340;&#21450;&#26102;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24110;&#21161;&#21307;&#29983;&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#30149;&#21464;&#21306;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#25216;&#26415;&#65292;&#21517;&#20026;SLP-Net&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;P(SNP)&#31995;&#32479;&#26426;&#21046;&#30340;&#36229;&#36731;&#37327;&#32423;&#20998;&#21106;&#32593;&#32476;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#20998;&#21106;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#24573;&#35270;&#20102;&#39640;&#30828;&#20214;&#25104;&#26412;&#12290;&#30456;&#21453;&#65292;SLP-Net&#20855;&#26377;&#38750;&#24120;&#23569;&#30340;&#21442;&#25968;&#21644;&#39640;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#27809;&#26377;&#24120;&#35265;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#24449;&#36866;&#24212;&#27169;&#22359;&#26469;&#26367;&#20195;&#35299;&#30721;&#22120;&#65292;&#24182;&#23454;&#29616;&#22810;&#23610;&#24230;&#20449;&#24687;&#35299;&#30721;&#12290;&#22312;ISIC2018&#25361;&#25112;&#36187;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Acc&#21644;DSC&#19978;&#20855;&#26377;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#32780;&#22312;PH2&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt treatment for melanoma is crucial. To assist physicians in identifying lesion areas precisely in a quick manner, we propose a novel skin lesion segmentation technique namely SLP-Net, an ultra-lightweight segmentation network based on the spiking neural P(SNP) systems type mechanism. Most existing convolutional neural networks achieve high segmentation accuracy while neglecting the high hardware cost. SLP-Net, on the contrary, has a very small number of parameters and a high computation speed. We design a lightweight multi-scale feature extractor without the usual encoder-decoder structure. Rather than a decoder, a feature adaptation module is designed to replace it and implement multi-scale information decoding. Experiments at the ISIC2018 challenge demonstrate that the proposed model has the highest Acc and DSC among the state-of-the-art methods, while experiments on the PH2 dataset also demonstrate a favorable generalization ability. Finally, we compare the computational compl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.12728</link><description>&lt;p&gt;
Lookahead:&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#26080;&#25439;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#38382;&#31572;&#12289;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;&#25903;&#20184;&#23453;&#36825;&#26679;&#20026;&#25968;&#21313;&#20159;&#29992;&#25143;&#25552;&#20379;&#37325;&#35201;&#37329;&#34701;&#20135;&#21697;&#30340;&#38656;&#35201;&#20934;&#30830;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25903;&#20184;&#23453;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#23558;LLMs&#19982;&#26368;&#20934;&#30830;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#30495;&#23454;&#20135;&#21697;&#26469;&#35828;&#65292;LLMs&#30340;&#25512;&#29702;&#36895;&#24230;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#23454;&#39564;&#24615;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;RAG&#31995;&#32479;&#30340;&#36895;&#24230;&#22823;&#24133;&#25552;&#21319;&#21644;&#25104;&#26412;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#26080;&#25439;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#12290;&#22312;&#20256;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#20196;&#29260;&#37117;&#30001;LLMs&#25353;&#39034;&#24207;&#29983;&#25104;&#65292;&#23548;&#33268;&#30340;&#26102;&#38388;&#28040;&#32791;&#19982;&#29983;&#25104;&#30340;&#20196;&#29260;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation (RAG) system that grounds LLMs on the most accurate and up-to-date information. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.  Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our RAG system, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26377;&#19968;&#23450;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2312.11671</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language-Model Agents on Realistic Autonomous Tasks. (arXiv:2312.11671v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11671
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26377;&#19968;&#23450;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#37326;&#22806;&#33719;&#21462;&#36164;&#28304;&#12289;&#22797;&#21046;&#33258;&#36523;&#21644;&#36866;&#24212;&#26032;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#33021;&#21147;&#20026;"&#33258;&#20027;&#22797;&#21046;&#21644;&#36866;&#24212;"&#25110;&#32773;ARA&#12290;&#25105;&#20204;&#35748;&#20026;&#20855;&#22791;ARA&#33021;&#21147;&#30340;&#31995;&#32479;&#21487;&#33021;&#20855;&#26377;&#24191;&#27867;&#32780;&#38590;&#20197;&#39044;&#27979;&#30340;&#21518;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#34913;&#37327;&#21644;&#39044;&#27979;ARA&#33021;&#21147;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#30456;&#20851;&#30340;&#23433;&#20840;&#12289;&#30417;&#27979;&#21644;&#23545;&#40784;&#25514;&#26045;&#12290;&#27492;&#22806;&#65292;&#19968;&#26086;&#31995;&#32479;&#20855;&#22791;ARA&#33021;&#21147;&#65292;&#23545;&#31995;&#32479;&#33021;&#21147;&#30340;&#38480;&#21046;&#21487;&#33021;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#31616;&#21333;&#30340;&#31034;&#20363;&#20195;&#29702;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20801;&#35768;&#20854;&#22312;&#19990;&#30028;&#20013;&#37319;&#21462;&#34892;&#21160;&#30340;&#24037;&#20855;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#20195;&#29702;&#22312;&#19982;ARA&#30456;&#20851;&#30340;12&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#20219;&#21153;&#21015;&#34920;&#20013;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23613;&#31649;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20063;&#26377;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#35780;&#20272;&#36824;&#27809;&#26377;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as "autonomous replication and adaptation" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult.  We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GNN&#31038;&#21306;&#26816;&#27979;&#20013;&#19981;&#21516;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#22312;&#32771;&#34385;&#38543;&#26426;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#20102;&#31639;&#27861;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#24573;&#35270;&#36229;&#21442;&#25968;&#35843;&#26597;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2312.09015</link><description>&lt;p&gt;
GNN&#23398;&#20064;&#35780;&#20272;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#37327;&#21270;GNN&#31038;&#21306;&#26816;&#27979;&#20013;&#38543;&#26426;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in GNN Learning Evaluations: A Comparison Between Measures for Quantifying Randomness in GNN Community Detection. (arXiv:2312.09015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GNN&#31038;&#21306;&#26816;&#27979;&#20013;&#19981;&#21516;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#22312;&#32771;&#34385;&#38543;&#26426;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#20102;&#31639;&#27861;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#24573;&#35270;&#36229;&#21442;&#25968;&#35843;&#26597;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
(1) &#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#22312;&#26080;&#30417;&#30563;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#22686;&#24378;&#33021;&#21147;&#24402;&#22240;&#20110;&#23427;&#20204;&#33021;&#22815;&#32534;&#30721;&#22270;&#30340;&#36830;&#25509;&#21644;&#29305;&#24449;&#20449;&#24687;&#31354;&#38388;&#12290;&#28508;&#22312;&#31038;&#21306;&#30340;&#35782;&#21035;&#22312;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#37117;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#30001;&#20110;&#24433;&#21709;GNN&#35780;&#20272;&#30340;&#20915;&#31574;&#20247;&#22810;&#65292;&#24403;&#21069;&#23454;&#38469;&#24615;&#33021;&#22522;&#20934;&#20196;&#20154;&#22256;&#24785;&#12290;(2) &#27604;&#36739;&#20102;&#19977;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#31639;&#27861;&#25490;&#21517;&#22312;&#23384;&#22312;&#38543;&#26426;&#24615;&#26102;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#40664;&#35748;&#36229;&#21442;&#25968;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#21644;&#24615;&#33021;&#36136;&#37327;&#36827;&#34892;&#35780;&#20272;&#12290;(3) &#32467;&#26524;&#27604;&#36739;&#20102;&#20351;&#29992;&#40664;&#35748;&#36229;&#21442;&#25968;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21457;&#29616;&#22312;&#24573;&#35270;&#36229;&#21442;&#25968;&#35843;&#26597;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#25351;&#26631;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#25490;&#21517;&#20013;&#30340;&#24182;&#21015;&#21517;&#27425;&#21487;&#33021;&#20250;&#22823;&#22823;&#25913;&#21464;&#37327;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
(1) The enhanced capability of Graph Neural Networks (GNNs) in unsupervised community detection of clustered nodes is attributed to their capacity to encode both the connectivity and feature information spaces of graphs. The identification of latent communities holds practical significance in various domains, from social networks to genomics. Current real-world performance benchmarks are perplexing due to the multitude of decisions influencing GNN evaluations for this task. (2) Three metrics are compared to assess the consistency of algorithm rankings in the presence of randomness. The consistency and quality of performance between the results under a hyperparameter optimisation with the default hyperparameters is evaluated. (3) The results compare hyperparameter optimisation with default hyperparameters, revealing a significant performance loss when neglecting hyperparameter investigation. A comparison of metrics indicates that ties in ranks can substantially alter the quantification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;Shannon&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;KL&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#12290;</title><link>http://arxiv.org/abs/2312.01520</link><description>&lt;p&gt;
Bayesian&#32593;&#32476;&#30340;&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#65306;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39640;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;Shannon&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;KL&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#22270;&#32467;&#26500;&#21487;&#20197;&#22788;&#29702;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#31232;&#30095;&#30340;&#19968;&#31995;&#21015;&#36739;&#23567;&#38382;&#39064;&#65292;&#36825;&#26159;Judea Pearl&#30340;&#22240;&#26524;&#24615;&#30340;&#22522;&#30784;&#65292;&#20063;&#20915;&#23450;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#22914;&#20309;&#22312;&#26368;&#24120;&#35265;&#30340;&#20998;&#24067;&#20551;&#35774;&#19979;&#35745;&#31639;BNs&#30340;Shannon&#29109;&#21644;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;BNs&#30340;&#22270;&#32467;&#26500;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#24182;&#29992;&#19968;&#25972;&#22871;&#25968;&#20540;&#31034;&#20363;&#35828;&#26126;&#20102;&#23427;&#20204;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#23558;&#39640;&#26031;BNs&#30340;KL&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian networks (BNs) are a foundational model in machine learning and causal inference. Their graphical structure can handle high-dimensional problems, divide them into a sparse collection of smaller ones, underlies Judea Pearl's causality, and determines their explainability and interpretability. Despite their popularity, there are almost no resources in the literature on how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for BNs under their most common distributional assumptions. In this paper, we provide computationally efficient algorithms for both by leveraging BNs' graphical structure, and we illustrate them with a complete set of numerical examples. In the process, we show it is possible to reduce the computational complexity of KL from cubic to quadratic for Gaussian BNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#24418;&#21464;&#21644;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#24515;&#33039;&#34180;&#22721;&#32467;&#26500;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#29289;&#29702;&#27169;&#25311;&#65292;&#20943;&#23569;&#21518;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.20065</link><description>&lt;p&gt;
LinFlo-Net: &#19968;&#31181;&#29983;&#25104;&#24515;&#33039;&#27169;&#25311;&#32593;&#26684;&#30340;&#20004;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart. (arXiv:2310.20065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#24418;&#21464;&#21644;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#24515;&#33039;&#34180;&#22721;&#32467;&#26500;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#29289;&#29702;&#27169;&#25311;&#65292;&#20943;&#23569;&#21518;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24739;&#32773;&#25104;&#20687;&#25968;&#25454;&#20013;&#33258;&#21160;&#29983;&#25104;&#20154;&#31867;&#24515;&#33039;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#24182;&#19988;&#29305;&#21035;&#24378;&#35843;&#20854;&#33021;&#22815;&#29983;&#25104;&#34180;&#22721;&#24515;&#33039;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#19968;&#20010;&#27169;&#26495;&#32593;&#26684;&#24418;&#21464;&#21040;&#32473;&#23450;&#22270;&#20687;&#30340;&#24515;&#33039;&#32467;&#26500;&#19978;&#12290;&#19982;&#20197;&#21069;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#26368;&#23567;&#21270;&#32593;&#26684;&#33258;&#31359;&#36879;&#65292;&#36825;&#36890;&#24120;&#21457;&#29983;&#22312;&#24418;&#21464;&#30340;&#34920;&#38754;&#32593;&#26684;&#20043;&#38388;&#30340;&#23567;&#36317;&#31163;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20004;&#38454;&#27573;&#30340;&#24418;&#21464;&#36807;&#31243;&#21644;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#21160;&#21147;&#23398;&#23548;&#20986;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#24809;&#32602;&#34920;&#38754;&#25509;&#35302;&#21644;&#30456;&#20114;&#28183;&#36879;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20135;&#29983;&#19981;&#33258;&#30456;&#20132;&#30340;&#32593;&#26684;&#12290;&#20135;&#29983;&#30340;&#32593;&#26684;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#25311;&#20013;&#21487;&#30452;&#25509;&#20351;&#29992;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#21518;&#22788;&#29702;&#21644;&#28165;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning model to automatically generate computer models of the human heart from patient imaging data with an emphasis on its capability to generate thin-walled cardiac structures. Our method works by deforming a template mesh to fit the cardiac structures to the given image. Compared with prior deep learning methods that adopted this approach, our framework is designed to minimize mesh self-penetration, which typically arises when deforming surface meshes separated by small distances. We achieve this by using a two-stage diffeomorphic deformation process along with a novel loss function derived from the kinematics of motion that penalizes surface contact and interpenetration. Our model demonstrates comparable accuracy with state-of-the-art methods while additionally producing meshes free of self-intersections. The resultant meshes are readily usable in physics based simulation, minimizing the need for post-processing and cleanup.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20449;&#21518;&#38376;&#26816;&#27979;&#22120;&#65288;CBD&#65289;&#65292;&#23427;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35843;&#33410;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#26696;&#65292;&#21363;&#23616;&#37096;&#20027;&#23548;&#27010;&#29575;&#32479;&#35745;&#12290;CBD&#33021;&#22815;&#25552;&#20379;&#23545;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#25512;&#26029;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#25915;&#20987;&#20445;&#35777;&#21487;&#26816;&#27979;&#30340;&#26465;&#20214;&#21644;&#20551;&#38451;&#24615;&#29575;&#30340;&#27010;&#29575;&#19978;&#30028;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20855;&#26377;&#26356;&#39640;&#40065;&#26834;&#24615;&#30340;&#35302;&#21457;&#22120;&#21644;&#26356;&#23567;&#25200;&#21160;&#24133;&#24230;&#30340;&#25915;&#20987;&#26356;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2310.17498</link><description>&lt;p&gt;
CBD: &#22522;&#20110;&#23616;&#37096;&#20027;&#23548;&#27010;&#29575;&#30340;&#21487;&#20449;&#21518;&#38376;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
CBD: A Certified Backdoor Detector Based on Local Dominant Probability. (arXiv:2310.17498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20449;&#21518;&#38376;&#26816;&#27979;&#22120;&#65288;CBD&#65289;&#65292;&#23427;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35843;&#33410;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#26696;&#65292;&#21363;&#23616;&#37096;&#20027;&#23548;&#27010;&#29575;&#32479;&#35745;&#12290;CBD&#33021;&#22815;&#25552;&#20379;&#23545;&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#25512;&#26029;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#25915;&#20987;&#20445;&#35777;&#21487;&#26816;&#27979;&#30340;&#26465;&#20214;&#21644;&#20551;&#38451;&#24615;&#29575;&#30340;&#27010;&#29575;&#19978;&#30028;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20855;&#26377;&#26356;&#39640;&#40065;&#26834;&#24615;&#30340;&#35302;&#21457;&#22120;&#21644;&#26356;&#23567;&#25200;&#21160;&#24133;&#24230;&#30340;&#25915;&#20987;&#26356;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#24120;&#35265;&#23041;&#32961;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#23884;&#20837;&#20102;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#23558;&#34987;&#21518;&#38376;&#27169;&#22411;&#35823;&#20998;&#31867;&#20026;&#23545;&#25239;&#30446;&#26631;&#65292;&#32780;&#27809;&#26377;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#23558;&#34987;&#27491;&#30830;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20449;&#21518;&#38376;&#26816;&#27979;&#22120;&#65288;CBD&#65289;&#65292;&#23427;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#23616;&#37096;&#20027;&#23548;&#27010;&#29575;&#32479;&#35745;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35843;&#33410;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#26696;&#12290;&#23545;&#20110;&#21463;&#26816;&#27979;&#30340;&#20219;&#20309;&#20998;&#31867;&#22120;&#65292;CBD&#25552;&#20379;&#20102;1&#65289;&#26816;&#27979;&#25512;&#26029;&#32467;&#26524;&#65292;2&#65289;&#25915;&#20987;&#22312;&#21516;&#19968;&#20998;&#31867;&#22495;&#19979;&#20445;&#35777;&#21487;&#26816;&#27979;&#30340;&#26465;&#20214;&#65292;&#24182;&#19988;3&#65289;&#20551;&#38451;&#24615;&#29575;&#30340;&#27010;&#29575;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#39640;&#40065;&#26834;&#24615;&#30340;&#35302;&#21457;&#22120;&#12289;&#26356;&#23567;&#25200;&#21160;&#24133;&#24230;&#30340;&#25915;&#20987;&#26356;&#26377;&#21487;&#33021;&#34987;&#26377;&#20445;&#35777;&#22320;&#26816;&#27979;&#20986;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#32771;&#34385;&#21508;&#31181;&#21518;&#38376;&#31867;&#22411;&#30340;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first certified backdoor detector (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic local dominant probability. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#22909;&#12289;&#26356;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20272;&#35745;&#30340;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#39640;&#26031;&#21327;&#26041;&#24046;&#30340;&#35889;&#35823;&#24046;&#21644;&#26377;&#30028;$k$&#38454;&#30697;&#30340;&#37325;&#23614;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.06289</link><description>&lt;p&gt;
&#26356;&#22909;&#12289;&#26356;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20272;&#35745;&#30340;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Better and Simpler Lower Bounds for Differentially Private Statistical Estimation. (arXiv:2310.06289v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#22909;&#12289;&#26356;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20272;&#35745;&#30340;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#39640;&#26031;&#21327;&#26041;&#24046;&#30340;&#35889;&#35823;&#24046;&#21644;&#26377;&#30028;$k$&#38454;&#30697;&#30340;&#37325;&#23614;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#20004;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#39640;&#32500;&#31169;&#26377;&#20272;&#35745;&#20219;&#21153;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#19979;&#30028;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#29992;&#20110;&#20272;&#35745;&#39640;&#26031;&#21327;&#26041;&#24046;&#30340;&#35889;&#35823;&#24046;$\alpha$&#65292;&#20219;&#24847;$\alpha \le O(1)$&#65292;&#38656;&#35201;$\tilde{\Omega} \left(\frac{d^{3/2}}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$&#20010;&#26679;&#26412;&#65292;&#36825;&#26159;&#32039;&#20945;&#30340;&#65292;&#20165;&#24046;&#23545;&#25968;&#22240;&#23376;&#12290;&#36825;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;$\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$&#26102;&#24314;&#31435;&#30340;&#32467;&#26524;&#35201;&#22909;&#65292;&#24182;&#19988;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#31616;&#21333;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#26377;&#30028;$k$&#38454;&#30697;&#30340;&#37325;&#23614;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#65292;&#38656;&#35201;$\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$&#20010;&#26679;&#26412;&#12290;&#36825;&#19982;&#24050;&#30693;&#30340;&#19978;&#30028;&#30456;&#21563;&#21512;&#65292;&#24182;&#25913;&#36827;&#20102;&#27492;&#38382;&#39064;&#30340;&#24050;&#30693;&#26368;&#20339;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#20165;&#36866;&#29992;&#20110;&#32431;&#31929;&#30340;&#24046;&#20998;&#38544;&#31169;&#25110;$k = 2$&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36981;&#24490;&#25351;&#32441;&#26041;&#24335;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide improved lower bounds for two well-known high-dimensional private estimation tasks. First, we prove that for estimating the covariance of a Gaussian up to spectral error $\alpha$ with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d^{3/2}}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ samples for any $\alpha \le O(1)$, which is tight up to logarithmic factors. This improves over previous work which established this for $\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$, and is also simpler than previous work. Next, we prove that for estimating the mean of a heavy-tailed distribution with bounded $k$th moments with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ samples. This matches known upper bounds and improves over the best known lower bound for this problem, which only hold for pure differential privacy, or when $k = 2$. Our techniques follow the method of fingerpr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#24179;&#28369;&#35299;&#37322;&#21644;&#33258;&#21160;&#24494;&#20998;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#26465;&#20214;&#20998;&#25903;&#30340;&#31243;&#24207;&#65292;&#24182;&#25104;&#21151;&#35745;&#31639;&#20986;&#24179;&#28369;&#31243;&#24207;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#20998;&#25903;&#31243;&#24207;&#21442;&#25968;&#21512;&#25104;&#21644;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.03585</link><description>&lt;p&gt;
&#36328;&#26465;&#20214;&#20998;&#25903;&#30340;&#33258;&#21160;&#24494;&#20998;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Smoothing Methods for Automatic Differentiation Across Conditional Branches. (arXiv:2310.03585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#24179;&#28369;&#35299;&#37322;&#21644;&#33258;&#21160;&#24494;&#20998;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#26465;&#20214;&#20998;&#25903;&#30340;&#31243;&#24207;&#65292;&#24182;&#25104;&#21151;&#35745;&#31639;&#20986;&#24179;&#28369;&#31243;&#24207;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#20998;&#25903;&#31243;&#24207;&#21442;&#25968;&#21512;&#25104;&#21644;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26465;&#20214;&#20998;&#25903;&#24341;&#20837;&#30340;&#19981;&#36830;&#32493;&#24615;&#30340;&#31243;&#24207;&#23545;&#20551;&#23450;&#30446;&#26631;&#20989;&#25968;&#21709;&#24212;&#26354;&#38754;&#20855;&#26377;&#19968;&#23450;&#24179;&#28369;&#24615;&#30340;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#24179;&#28369;&#35299;&#37322;&#65288;SI&#65289;&#26159;&#19968;&#31181;&#25277;&#35937;&#35299;&#37322;&#24418;&#24335;&#65292;&#23427;&#20197;&#39640;&#26031;&#26680;&#36817;&#20284;&#31243;&#24207;&#36755;&#20986;&#30340;&#21367;&#31215;&#65292;&#20174;&#32780;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#24179;&#28369;&#20854;&#36755;&#20986;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;SI&#19982;&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#24179;&#28369;&#31243;&#24207;&#30340;&#26799;&#24230;&#12290;&#19982;&#22312;&#24120;&#35268;&#31243;&#24207;&#25191;&#34892;&#20013;&#36827;&#34892;&#30340;&#33258;&#21160;&#24494;&#20998;&#19981;&#21516;&#65292;&#36825;&#20123;&#26799;&#24230;&#36824;&#25429;&#25417;&#20102;&#26367;&#20195;&#25511;&#21046;&#27969;&#36335;&#24452;&#30340;&#24433;&#21709;&#12290;SI&#19982;AD&#30340;&#32452;&#21512;&#20351;&#24471;&#25903;&#25345;&#22522;&#20110;&#26799;&#24230;&#30340;&#20998;&#25903;&#31243;&#24207;&#21442;&#25968;&#21512;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#20363;&#22914;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#23545;&#20223;&#30495;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#25110;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#21512;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;SI&#20013;&#20026;&#21487;&#34892;&#24615;&#32780;&#36827;&#34892;&#30340;&#36817;&#20284;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programs involving discontinuities introduced by control flow constructs such as conditional branches pose challenges to mathematical optimization methods that assume a degree of smoothness in the objective function's response surface. Smooth interpretation (SI) is a form of abstract interpretation that approximates the convolution of a program's output with a Gaussian kernel, thus smoothing its output in a principled manner. Here, we combine SI with automatic differentiation (AD) to efficiently compute gradients of smoothed programs. In contrast to AD across a regular program execution, these gradients also capture the effects of alternative control flow paths. The combination of SI with AD enables the direct gradient-based parameter synthesis for branching programs, allowing for instance the calibration of simulation models or their combination with neural network models in machine learning pipelines. We detail the effects of the approximations made for tractability in SI and propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2309.15216</link><description>&lt;p&gt;
&#20351;&#29992;CodeBERT&#21644;Random Forest Regressor&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;C&#32534;&#31243;&#20316;&#19994;
&lt;/p&gt;
&lt;p&gt;
Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#35780;&#20998;&#32534;&#31243;&#20316;&#19994;&#22240;&#22797;&#26434;&#24615;&#21644;&#20027;&#35266;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#31616;&#21270;&#20102;&#20219;&#21153;&#12290;&#23427;&#23458;&#35266;&#22320;&#35780;&#20272;&#20195;&#30721;&#36136;&#37327;&#65292;&#26816;&#27979;&#38169;&#35823;&#65292;&#24182;&#20934;&#30830;&#22320;&#20998;&#37197;&#20998;&#25968;&#65292;&#20943;&#36731;&#20102;&#25945;&#24072;&#30340;&#36127;&#25285;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#39640;&#25928;&#21644;&#20844;&#24179;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22238;&#24402;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#12290;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#65292;&#23558;&#25991;&#26412;&#20195;&#30721;&#36755;&#20837;&#36716;&#25442;&#20026;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#21521;&#37327;&#36755;&#20837;&#21040;&#20960;&#20010;&#27169;&#22411;&#20013;&#12290;&#27979;&#35797;&#32467;&#26524;&#35777;&#26126;&#20102;&#24314;&#35758;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#20026;1.89&#12290;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#21608;&#26399;&#24615;&#30340;Wavelet-Fourier&#21464;&#25442;&#32593;&#32476;&#65288;WFTNet&#65289;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#22312;&#21033;&#29992;&#20613;&#37324;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#26102;&#39057;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#21608;&#26399;&#37325;&#35201;&#24615;&#21152;&#26435;&#31995;&#25968;&#65288;PWC&#65289;&#26469;&#24179;&#34913;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;WFTNet&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.11319</link><description>&lt;p&gt;
WFTNet&#65306;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#21608;&#26399;&#24615;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting. (arXiv:2309.11319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#21608;&#26399;&#24615;&#30340;Wavelet-Fourier&#21464;&#25442;&#32593;&#32476;&#65288;WFTNet&#65289;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#22312;&#21033;&#29992;&#20613;&#37324;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#26102;&#39057;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#21608;&#26399;&#37325;&#35201;&#24615;&#21152;&#26435;&#31995;&#25968;&#65288;PWC&#65289;&#26469;&#24179;&#34913;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;WFTNet&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#23581;&#35797;&#21033;&#29992;&#39057;&#29575;&#21644;&#21608;&#26399;&#24615;&#20449;&#24687;&#36827;&#34892;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37117;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#32454;&#31890;&#24230;&#21644;&#23616;&#37096;&#39057;&#29575;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Wavelet-Fourier Transform Network&#65288;WFTNet&#65289;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;WFTNet&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#23567;&#27874;&#21464;&#25442;&#20174;&#20449;&#21495;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#26102;&#39057;&#20449;&#24687;&#65292;&#20854;&#20013;&#20613;&#37324;&#21494;&#21464;&#25442;&#25429;&#25417;&#20840;&#23616;&#21608;&#26399;&#27169;&#24335;&#65292;&#32780;&#23567;&#27874;&#21464;&#25442;&#25429;&#25417;&#23616;&#37096;&#21608;&#26399;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21608;&#26399;&#37325;&#35201;&#24615;&#21152;&#26435;&#31995;&#25968;&#65288;PWC&#65289;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;WFTNet&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent CNN and Transformer-based models tried to utilize frequency and periodicity information for long-term time series forecasting. However, most existing work is based on Fourier transform, which cannot capture fine-grained and local frequency structure. In this paper, we propose a Wavelet-Fourier Transform Network (WFTNet) for long-term time series forecasting. WFTNet utilizes both Fourier and wavelet transforms to extract comprehensive temporal-frequency information from the signal, where Fourier transform captures the global periodic patterns and wavelet transform captures the local ones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to adaptively balance the importance of global and local frequency patterns. Extensive experiments on various time series datasets show that WFTNet consistently outperforms other state-of-the-art baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#21644;&#32454;&#33410;&#31934;&#28860;&#26469;&#26174;&#33879;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15256</link><description>&lt;p&gt;
&#35753;&#22768;&#38899;&#23384;&#22312;&#65306;&#20174;&#26080;&#22768;&#35270;&#39057;&#20013;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#21644;&#32454;&#33410;&#31934;&#28860;&#26469;&#26174;&#33879;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20165;&#36890;&#36807;&#21767;&#36816;&#21160;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#20063;&#34987;&#31216;&#20026;&#21767;&#35821;&#36716;&#35821;&#38899;&#12290;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30001;&#20110;&#21516;&#24418;&#24322;&#38899;&#21644;&#22810;&#26679;&#21270;&#35821;&#38899;&#21464;&#21270;&#32780;&#36896;&#25104;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#65292;&#23548;&#33268;&#21457;&#38899;&#38169;&#35823;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#35821;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#32531;&#35299;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#65288;1&#65289;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#26469;&#28040;&#38500;&#21516;&#24418;&#24322;&#38899;&#65292;&#21644;&#65288;2&#65289;&#22768;&#23398;&#21464;&#24322;&#20449;&#24687;&#26469;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;&#35821;&#38899;&#39118;&#26684;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#27969;&#30340;&#21518;&#22788;&#29702;&#32593;&#32476;&#65292;&#25429;&#25417;&#21644;&#31934;&#28860;&#25152;&#29983;&#25104;&#35821;&#38899;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#30495;&#23454;&#20154;&#31867;&#35821;&#38899;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this work is to reconstruct high quality speech from lip motions alone, a task also known as lip-to-speech. A key challenge of lip-to-speech systems is the one-to-many mapping caused by (1) the existence of homophenes and (2) multiple speech variations, resulting in a mispronounced and over-smoothed speech. In this paper, we propose a novel lip-to-speech system that significantly improves the generation quality by alleviating the one-to-many mapping problem from multiple perspectives. Specifically, we incorporate (1) self-supervised speech representations to disambiguate homophenes, and (2) acoustic variance information to model diverse speech styles. Additionally, to better solve the aforementioned problem, we employ a flow based post-net which captures and refines the details of the generated speech. We perform extensive experiments and demonstrate that our method achieves the generation quality close to that of real human utterance, outperforming existing methods in term
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12517</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#22870;&#21169;&#65292;&#36824;&#26377;&#32422;&#26463;&#65306;&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#20351;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20855;&#26377;&#33258;&#28982;&#21160;&#20316;&#39118;&#26684;&#21644;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#20986;&#33394;&#25511;&#21046;&#22120;&#26159;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#32780;&#24320;&#21457;&#30340;&#65292;&#35813;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#35774;&#35745;&#22823;&#37327;&#22870;&#21169;&#39033;&#24182;&#30830;&#23450;&#21512;&#36866;&#30340;&#22870;&#21169;&#31995;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21516;&#26102;&#21253;&#21547;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#35753;&#24037;&#31243;&#24072;&#33021;&#22815;&#36866;&#24403;&#22320;&#21453;&#26144;&#20182;&#20204;&#23545;&#32422;&#26463;&#30340;&#24847;&#22270;&#24182;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#22788;&#29702;&#23427;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#24418;&#24577;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#20960;&#20010;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
&lt;/p&gt;</description></item><item><title>GIT-Mol&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#22788;&#29702;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;GIT-Former&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;GIT-Mol&#22312;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.06911</link><description>&lt;p&gt;
GIT-Mol&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#31185;&#23398;&#20013;&#30340;&#22270;&#20687;&#65292;&#22270;&#24418;&#21644;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06911
&lt;/p&gt;
&lt;p&gt;
GIT-Mol&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#22788;&#29702;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;GIT-Former&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;GIT-Mol&#22312;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#22788;&#29702;&#20998;&#23376;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20026;&#20998;&#23376;&#31185;&#23398;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20855;&#26377;&#22797;&#26434;&#20998;&#23376;&#32467;&#26500;&#25110;&#22270;&#20687;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GIT-Mol&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#20998;&#23376;&#25968;&#25454;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIT-Former&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23558;&#25152;&#26377;&#27169;&#24577;&#23545;&#40784;&#21040;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#24615;&#36136;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;5%-10%&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#65292;&#24182;&#22312;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;20.2%&#12290;&#36890;&#36807;&#20219;&#24847;&#21040;&#35821;&#35328;&#30340;&#20998;&#23376;&#32763;&#35793;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#28508;&#21147;&#36827;&#34892;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#21644;&#28210;&#26579;NeRF&#23545;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#28145;&#24230;&#22330;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21160;&#24577;&#38452;&#24433;&#24182;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2308.04669</link><description>&lt;p&gt;
&#24555;&#36895;NeRF&#21512;&#25104;&#21644;&#28210;&#26579;&#30340;&#36890;&#29992;&#38544;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#21644;&#28210;&#26579;NeRF&#23545;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#28145;&#24230;&#22330;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21160;&#24577;&#38452;&#24433;&#24182;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21508;&#31181;&#31070;&#32463;&#36752;&#23556;&#22330;&#26041;&#27861;&#22312;&#39640;&#28210;&#26579;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21152;&#36895;&#26041;&#27861;&#19987;&#38376;&#21270;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#21508;&#31181;&#38544;&#24335;&#26041;&#27861;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;NeRF&#20316;&#21697;&#36827;&#34892;&#23454;&#26102;&#21512;&#25104;&#12290;&#30001;&#20110;NeRF&#20381;&#36182;&#20110;&#27839;&#20809;&#32447;&#37319;&#26679;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#19968;&#33324;&#24615;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#27969;&#27700;&#32447;&#26469;&#24555;&#36895;&#21512;&#25104;NeRF&#23545;&#35937;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#20351;&#24471;&#21160;&#24577;&#38452;&#24433;&#21487;&#20197;&#20351;&#29992;&#35299;&#26512;&#20809;&#28304;&#22312;&#23545;&#35937;&#20869;&#37096;&#25110;&#23545;&#35937;&#20043;&#38388;&#36827;&#34892;&#25237;&#23556;&#65292;&#21516;&#26102;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;&#20027;&#35201;&#22320;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#28145;&#24230;&#22330;&#65288;NeDF&#65289;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#20809;&#32447;&#21644;&#38544;&#24335;&#34920;&#38754;&#20043;&#38388;&#30340;&#30452;&#25509;&#30456;&#20132;&#35745;&#31639;&#26469;&#24555;&#36895;&#30830;&#23450;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#20132;&#28857;&#31070;&#32463;&#32593;&#32476;&#26469;&#21152;&#36895;&#26597;&#35810;NeRF&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a variety of Neural radiance fields methods have garnered remarkable success in high render speed. However, current accelerating methods is specialized and not compatible for various implicit method, which prevent a real-time composition over different kinds of NeRF works. Since NeRF relies on sampling along rays, it's possible to provide a guidance generally. We propose a general implicit pipeline to rapidly compose NeRF objects. This new method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration inste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;Robusta&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02535</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#40065;&#26834;&#35821;&#20041;&#20998;&#21106;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;Robusta&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21644;&#22270;&#20687;&#21040;&#26631;&#31614;&#20998;&#21106;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;Robusta&#65292;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19979;&#28216;&#20998;&#21106;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#12289;&#20998;&#24067;&#21464;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#21644;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11586</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#21644;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25913;&#36827;&#26041;&#27861;&#21253;&#25324;&#22810;&#22270;&#31471;&#21475;&#32534;&#21495;&#12289;&#20010;&#20307;ID&#21644;&#21453;&#21521;&#28040;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20960;&#20046;&#21487;&#20197;&#24471;&#21040;&#23436;&#32654;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#26816;&#27979;&#27927;&#38065;&#20132;&#26131;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;GNN&#30340;&#23569;&#25968;&#31867;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#39640;&#36798;30%&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#26641;&#21644;GNN&#30340;&#22522;&#20934;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#25552;&#21319;&#20102;&#19977;&#20010;&#26631;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyses a set of simple adaptations that transform standard message-passing Graph Neural Networks (GNN) into provably powerful directed multigraph neural networks. The adaptations include multigraph port numbering, ego IDs, and reverse message passing. We prove that the combination of these theoretically enables the detection of any directed subgraph pattern. To validate the effectiveness of our proposed adaptations in practice, we conduct experiments on synthetic subgraph detection tasks, which demonstrate outstanding performance with almost perfect results. Moreover, we apply our proposed adaptations to two financial crime analysis tasks. We observe dramatic improvements in detecting money laundering transactions, improving the minority-class F1 score of a standard message-passing GNN by up to 30%, and closely matching or outperforming tree-based and GNN baselines. Similarly impressive results are observed on a real-world phishing detection dataset, boosting three standar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10759</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#22270;&#34920;&#31034;&#31616;&#21270;&#21644;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#34920;&#31034;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#32534;&#30721;&#22120;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#27880;&#24847;&#21147;&#21487;&#20197;&#25429;&#25417;&#21040;&#37051;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#23545;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#23567;&#22411;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32487;&#25215;&#20102;Transformer&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#28145;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#26469;&#37319;&#29992;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#20110;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#20063;&#33021;&#22312;&#33410;&#28857;&#25968;&#37327;&#20174;&#21315;&#32423;&#21040;&#21313;&#20159;&#32423;&#30340;&#33539;&#22260;&#20869;&#24102;&#26469;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#40723;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20854;&#20013;&#20840;&#23616;&#27880;&#24847;&#21147;&#26159;&#19968;&#20010;&#38459;&#30861;&#21487;&#25193;&#23637;&#24615;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#26696;&#31216;&#20026;&#31616;&#21270;&#22270;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04502</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#38752;&#21644;&#39640;&#24615;&#33021;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21363;&#20415;&#26159;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20063;&#20250;&#21253;&#21547;&#38169;&#35823;&#65292;&#26356;&#19981;&#29992;&#35828;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20102;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#25968;&#25454;&#21435;&#22122;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#24322;&#24120;&#20540;&#24182;&#36827;&#34892;&#27704;&#20037;&#24615;&#21435;&#38500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#36807;&#24230;&#25110;&#32773;&#27424;&#24230;&#36807;&#28388;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65288;AGRA&#65289;&#65292;&#19981;&#21516;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#28165;&#27927;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#32452;&#26679;&#26412;&#30340;&#32047;&#31215;&#26799;&#24230;&#21644;&#21333;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#22312;&#24403;&#21069;&#26356;&#26032;&#26102;&#20445;&#30041;&#23545;&#24212;&#30340;&#26679;&#26412;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AGRA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20840;&#38754;&#30340;&#32467;&#26524;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#65292;&#37319;&#29992;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26469;&#35745;&#31639;&#27169;&#22411;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#24182;&#19982;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.00876</link><description>&lt;p&gt;
&#22312;&#31526;&#21512;&#24615;&#39044;&#27979;&#20013;&#37327;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Deep Learning Model Uncertainty in Conformal Prediction. (arXiv:2306.00876v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#65292;&#37319;&#29992;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26694;&#26550;&#26469;&#35745;&#31639;&#27169;&#22411;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#24182;&#19982;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#20013;&#65292;&#31934;&#30830;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#21487;&#38752;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30340;&#32972;&#26223;&#19979;&#12290;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#32622;&#20449;&#27700;&#24179;&#26469;&#34920;&#31034;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#36827;&#34892;&#21333;&#20010;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#31526;&#21512;&#24615;&#39044;&#27979;&#20013;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#20173;&#28982;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#39046;&#22495;&#65292;&#36824;&#27809;&#26377;&#23436;&#20840;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#21450;&#20854;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#26041;&#27861;&#26469;&#37327;&#21270;&#31526;&#21512;&#24615;&#39044;&#27979;&#20013;&#20135;&#29983;&#30340;&#39044;&#27979;&#38598;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20026;&#35745;&#31639;&#24471;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#35748;&#35777;&#36793;&#30028;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36890;&#36807;CP&#27979;&#37327;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19982;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65288;&#22914;&#36125;&#21494;&#26031;&#26041;&#27861;&#12289;MC-Dropout&#21644;DeepEnsemble&#65289;&#36827;&#34892;&#27604;&#36739;.
&lt;/p&gt;
&lt;p&gt;
Precise estimation of predictive uncertainty in deep neural networks is a critical requirement for reliable decision-making in machine learning and statistical modeling, particularly in the context of medical AI. Conformal Prediction (CP) has emerged as a promising framework for representing the model uncertainty by providing well-calibrated confidence levels for individual predictions. However, the quantification of model uncertainty in conformal prediction remains an active research area, yet to be fully addressed. In this paper, we explore state-of-the-art CP methodologies and their theoretical foundations. We propose a probabilistic approach in quantifying the model uncertainty derived from the produced prediction sets in conformal prediction and provide certified boundaries for the computed uncertainty. By doing so, we allow model uncertainty measured by CP to be compared by other uncertainty quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and Evidentia
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17033</link><description>&lt;p&gt;
&#12298;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;2023&#65306;&#20851;&#27880;&#20799;&#31185;&#65288;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs&#65289;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#32959;&#30244;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#26368;&#24120;&#35265;&#21407;&#22240;&#12290;&#20799;&#31461;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#30340;&#20116;&#24180;&#29983;&#23384;&#29575;&#19981;&#21040;20&#65285;&#12290;&#30001;&#20110;&#32597;&#35265;&#65292;&#23545;&#36825;&#20123;&#23454;&#20307;&#30340;&#35786;&#26029;&#36890;&#24120;&#20250;&#24310;&#36831;&#65292;&#20854;&#27835;&#30103;&#20027;&#35201;&#22522;&#20110;&#21382;&#21490;&#27835;&#30103;&#29702;&#24565;&#65292;&#24182;&#19988;&#20020;&#24202;&#35797;&#39564;&#38656;&#35201;&#22810;&#26426;&#26500;&#21512;&#20316;&#12290;MICCAI&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#26159;&#19968;&#20010;&#37324;&#31243;&#30865;&#24335;&#30340;&#31038;&#21306;&#22522;&#20934;&#20107;&#20214;&#65292;&#24050;&#32463;&#25104;&#21151;&#21019;&#24314;&#36164;&#28304;12&#24180;&#65292;&#29992;&#20110;&#25104;&#20154;&#33014;&#36136;&#30244;&#30340;&#20998;&#21106;&#21644;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023&#25361;&#25112;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#20799;&#31461;&#33041;&#32959;&#30244;&#30340;BraTS&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#20010;&#22269;&#38469;&#21512;&#20316;&#32452;&#32455;&#19987;&#27880;&#20110;&#20799;&#31185;&#31070;&#32463;&#32959;&#30244;&#21644;&#20020;&#24202;&#35797;&#39564;&#30340;&#25968;&#25454;&#12290;BraTS-PEDs 2023&#25361;&#25112;&#20391;&#37325;&#20110;&#35780;&#20272;&#29992;&#20110;&#20799;&#31461;&#33041;&#33014;&#36136;&#30244;&#30340;&#20307;&#31215;&#20998;&#21106;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain gli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;MC&#26041;&#27861;&#19982;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.06432</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#39118;&#38505;&#27010;&#29575;&#20272;&#35745;&#30340;&#21487;&#25512;&#24191;&#12289;&#29289;&#29702;&#23398;&#22522;&#30784;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalizable Physics-informed Learning Framework for Risk Probability Estimation. (arXiv:2305.06432v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;MC&#26041;&#27861;&#19982;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#23545;&#20110;&#35768;&#22810;&#38543;&#26426;&#23433;&#20840;&#25511;&#21046;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#21644;&#26410;&#30693;&#25110;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#35745;&#31639;&#36825;&#20123;&#39118;&#38505;&#27010;&#29575;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#28385;&#36275;&#26576;&#20123;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#20107;&#23454;&#65292;&#35813;&#26041;&#31243;&#34920;&#24449;&#20102;&#27010;&#29575;&#20043;&#38388;&#30340;&#37051;&#36817;&#20851;&#31995;&#65292;&#20197;&#23558;MC&#26041;&#27861;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#29305;&#23450;&#35757;&#32451;&#37197;&#32622;&#19979;&#32473;&#20986;&#20272;&#35745;&#35823;&#24046;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#30693;&#21306;&#22495;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#65292;&#30456;&#27604;MC&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23427;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate estimates of long-term risk probabilities and their gradients are critical for many stochastic safe control methods. However, computing such risk probabilities in real-time and in unseen or changing environments is challenging. Monte Carlo (MC) methods cannot accurately evaluate the probabilities and their gradients as an infinitesimal devisor can amplify the sampling noise. In this paper, we develop an efficient method to evaluate the probabilities of long-term risk and their gradients. The proposed method exploits the fact that long-term risk probability satisfies certain partial differential equations (PDEs), which characterize the neighboring relations between the probabilities, to integrate MC methods and physics-informed neural networks. We provide theoretical guarantees of the estimation error given certain choices of training configurations. Numerical results show the proposed method has better sample efficiency, generalizes well to unseen regions, and can adapt to sys
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;&#65292;&#21487;&#20197;&#23545;&#20195;&#29702;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Shapley&#20540;&#21644;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#24310;&#36831;&#20840;&#23616;&#22238;&#25253;&#30340;&#22797;&#26434;&#20851;&#31995;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.07520</link><description>&lt;p&gt;
STAS: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning. (arXiv:2304.07520v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07520
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;&#65292;&#21487;&#20197;&#23545;&#20195;&#29702;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Shapley&#20540;&#21644;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#24310;&#36831;&#20840;&#23616;&#22238;&#25253;&#30340;&#22797;&#26434;&#20851;&#31995;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#26377;&#25928;&#30340;&#33539;&#20363;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#36171;&#20449;&#29992;&#20540;&#65292;&#21363;&#36890;&#36807;&#20195;&#29702;&#30340;&#36129;&#29486;&#26469;&#32473;&#20195;&#29702;&#36171;&#20449;&#29992;&#20540;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#38544;&#24335;&#22320;&#20998;&#35299;&#32852;&#21512;&#20215;&#20540;&#20989;&#25968;&#25110;&#26174;&#24335;&#22320;&#35745;&#31639;&#25152;&#26377;&#20195;&#29702;&#30340;&#25903;&#20184;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#21482;&#26377;&#22312;&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20840;&#23616;&#22870;&#21169;&#21482;&#33021;&#22312;&#21608;&#26399;&#32467;&#26463;&#26102;&#26174;&#31034;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#12290;&#23427;&#20204;&#32570;&#20047;&#23545;&#24310;&#36831;&#20840;&#23616;&#22870;&#21169;&#22312;&#26102;&#38388;&#32500;&#24230;&#20013;&#22797;&#26434;&#20851;&#31995;&#30340;&#24314;&#27169;&#21151;&#33021;&#65292;&#24182;&#19988;&#21463;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31354;&#38388;&#26102;&#38388;&#20851;&#27880;&#19982; Shapley&#65288;STAS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22238;&#25253;&#20998;&#35299;&#65307;STAS &#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#23398;&#20064;&#20449;&#29992;&#20998;&#37197;&#12290;&#23427;&#39318;&#20808;&#23558;&#20840;&#23616;&#22238;&#25253;&#20998;&#35299;&#22238;&#21040;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;&#28982;&#21518;&#20351;&#29992;Shapley&#20540;&#26469;&#35780;&#20272;&#21327;&#20316;MARL&#20013;&#27599;&#20010;&#20195;&#29702;&#30340;&#36129;&#29486;&#12290; STAS &#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31354;&#38388; - &#26102;&#38388;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#25429;&#33719;&#24310;&#36831;&#20840;&#23616;&#22870;&#21169;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#20013;&#65292;STAS &#33021;&#22815;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized Training with Decentralized Execution (CTDE) has been proven to be an effective paradigm in cooperative multi-agent reinforcement learning (MARL). One of the major challenges is yet credit assignment, which aims to credit agents by their contributions. Prior studies focus on either implicitly decomposing the joint value function or explicitly computing the payoff distribution of all agents. However, in episodic reinforcement learning settings where global rewards can only be revealed at the end of the episode, existing methods usually fail to work. They lack the functionality of modeling complicated relations of the delayed global reward in the temporal dimension and suffer from large variance and bias. We propose a novel method named Spatial-Temporal Attention with Shapley (STAS) for return decomposition; STAS learns credit assignment in both the temporal and the spatial dimension. It first decomposes the global return back to each time step, then utilizes Shapley Value to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#20174;&#38750;&#20984;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#12290;&#31639;&#27861;&#35299;&#20915;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06815</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#30340;&#26694;&#26550;&#12289;&#31639;&#27861;&#21644;&#25910;&#25947;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee. (arXiv:2303.06815v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#20174;&#38750;&#20984;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#12290;&#31639;&#27861;&#35299;&#20915;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21387;&#32553;&#23545;&#20110;&#37096;&#32626;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#35745;&#31639;&#35774;&#22791;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#20851;&#27880;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65306;&#20302;&#31209;&#36924;&#36817;&#21644;&#26435;&#37325;&#35009;&#21098;&#65292;&#36825;&#20123;&#25216;&#26415;&#30446;&#21069;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#26435;&#37325;&#35009;&#21098;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24635;&#26159;&#20250;&#36973;&#21463;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#20174;&#38750;&#20984;&#20248;&#21270;&#30340;&#26032;&#35270;&#35282;&#35774;&#35745;&#20102;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22359;&#22352;&#26631;&#19979;&#38477;&#65288;BCD&#65289;&#31639;&#27861;NN-BCD&#26469;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#38381;&#24335;&#24418;&#24335;&#30340;&#39640;&#25928;&#36845;&#20195;&#26041;&#26696;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;Kurdyka-{\L}ojasiewicz (K{\L})&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model compression is a crucial part of deploying neural networks (NNs), especially when the memory and storage of computing devices are limited in many applications. This paper focuses on two model compression techniques: low-rank approximation and weight pruning in neural networks, which are very popular nowadays. However, training NN with low-rank approximation and weight pruning always suffers significant accuracy loss and convergence issues. In this paper, a holistic framework is proposed for model compression from a novel perspective of nonconvex optimization by designing an appropriate objective function. Then, we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the nonconvex optimization. One advantage of our algorithm is that an efficient iteration scheme can be derived with closed-form, which is gradient-free. Therefore, our algorithm will not suffer from vanishing/exploding gradient problems. Furthermore, with the Kurdyka-{\L}ojasiewicz (K{\L}) property o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.02468</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#30340;Lon-ea&#65306;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#28608;&#27963;&#20989;&#25968;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#23398;&#20064;&#19981;&#21516;&#24847;&#20219;&#21153;&#30340;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#23618;&#20013;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#36719;&#26631;&#31614;&#26469;&#37327;&#21270;&#19981;&#21516;&#24847;&#37327;&#12290;&#20026;&#20102;&#39044;&#27979;&#36719;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#39044;&#22788;&#29702;&#22120;&#21644;&#32534;&#30721;&#22120;&#65292;&#24182;&#25913;&#21464;&#36755;&#20986;&#23618;&#20013;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21442;&#25968;&#19981;&#21464;&#12290;&#28982;&#21518;&#23558;&#36719;&#26631;&#31614;&#29992;&#20110;&#30828;&#26631;&#31614;&#39044;&#27979;&#12290;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#21253;&#25324;sigmoid&#20989;&#25968;&#20197;&#21450;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#30340;&#38454;&#36291;&#20989;&#25968;&#21644;&#26412;&#25991;&#20013;&#39318;&#27425;&#20171;&#32461;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13991</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#23398;&#20064;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65306;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#30340;&#32763;&#35793;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65292;&#22810;&#39046;&#22495;&#28151;&#21512;&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#35268;&#33539;&#25552;&#21462;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#23545;&#39118;&#26684;&#65288;&#20363;&#22914;&#65292;&#26080;&#20449;&#24687;&#30340;&#32441;&#29702;&#65289;&#26377;&#24456;&#24378;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#23545;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#24418;&#29366;&#65289;&#30340;&#20559;&#22909;&#65292;&#36825;&#19982;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#20542;&#21521;&#20110;&#20174;CXR&#22270;&#20687;&#20013;&#23398;&#20064;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#22240;&#27492;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#20174;CXR&#22270;&#20687;&#36827;&#34892;&#30149;&#29702;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#27169;&#22411;&#24212;&#35813;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#65288;SRMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#36817;&#20284;&#19981;&#21487;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#38469;&#26469;&#33719;&#24471;&#26435;&#37325;&#30340;&#28857;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2302.10975</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improved uncertainty quantification for neural networks with Bayesian last layer. (arXiv:2302.10975v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#36817;&#20284;&#19981;&#21487;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#38469;&#26469;&#33719;&#24471;&#26435;&#37325;&#30340;&#28857;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#26041;&#38754;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#38480;&#21046;&#65292;&#22240;&#27492;&#24120;&#24120;&#20542;&#21521;&#20110;&#20351;&#29992;&#33021;&#22815;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#22914;&#39640;&#26031;&#36807;&#31243;&#25110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20551;&#35774;&#25152;&#26377;&#21442;&#25968;&#37117;&#26381;&#20174;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#20135;&#29983;&#20998;&#24067;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#25512;&#26029;&#36890;&#24120;&#26159;&#19981;&#21487;&#22788;&#29702;&#30340;&#65292;&#38656;&#35201;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36817;&#20284;&#26041;&#27861;&#26159;&#20855;&#26377;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20182;&#20204;&#20165;&#22312;&#26368;&#21518;&#19968;&#20010;&#32447;&#24615;&#23618;&#20013;&#20551;&#35774;&#20998;&#24067;&#26435;&#37325;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#39044;&#27979;&#12290;&#20855;&#26377;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#23398;&#20064;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#20026;&#20102;&#36817;&#20284;&#19981;&#21487;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#38469;&#26469;&#33719;&#24471;&#38500;&#26368;&#21518;&#19968;&#23618;&#20197;&#22806;&#30340;&#25152;&#26377;&#20998;&#24067;&#26435;&#37325;&#30340;&#28857;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is an essential task in machine learning - a task in which neural networks (NNs) have traditionally not excelled. This can be a limitation for safety-critical applications, where uncertainty-aware methods like Gaussian processes or Bayesian linear regression are often preferred. Bayesian neural networks are an approach to address this limitation. They assume probability distributions for all parameters and yield distributed predictions. However, training and inference are typically intractable and approximations must be employed. A promising approximation is NNs with Bayesian last layer (BLL). They assume distributed weights only in the last linear layer and yield a normally distributed prediction. NNs with BLL can be seen as a Bayesian linear regression model with learned nonlinear features. To approximate the intractable Bayesian neural network, point estimates of the distributed weights in all but the last layer should be obtained by maximizing the margina
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#25915;&#20987;&#29616;&#35937;&#30340;&#29983;&#21629;&#21608;&#26399;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#65292;&#25552;&#20379;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#21644;&#25968;&#23398;&#26694;&#26550;&#65292;&#24635;&#32467;&#25972;&#20307;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.09457</link><description>&lt;p&gt;
&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25915;&#20987;&#65306;&#20174;&#29983;&#21629;&#21608;&#26399;&#35282;&#24230;&#30340;&#31995;&#32479;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective. (arXiv:2302.09457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#25915;&#20987;&#29616;&#35937;&#30340;&#29983;&#21629;&#21608;&#26399;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#65292;&#25552;&#20379;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#21644;&#25968;&#23398;&#26694;&#26550;&#65292;&#24635;&#32467;&#25972;&#20307;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#65288;AML&#65289;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#29616;&#35937;&#65292;&#36825;&#20123;&#29616;&#35937;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#39044;&#27979;&#19981;&#19968;&#33268;&#25110;&#20986;&#20046;&#24847;&#26009;&#12290;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#33539;&#24335;&#26469;&#25506;&#32034;&#21457;&#29983;&#22312;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#21516;&#38454;&#27573;&#30340;&#23545;&#25239;&#29616;&#35937;&#65292;&#20363;&#22914;&#22312;&#39044;&#35757;&#32451;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#21457;&#29983;&#30340;&#21518;&#38376;&#25915;&#20987;&#65307;&#22312;&#21518;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#25512;&#26029;&#38454;&#27573;&#21457;&#29983;&#30340;&#26435;&#37325;&#25915;&#20987;&#65307;&#22312;&#25512;&#26029;&#38454;&#27573;&#21457;&#29983;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#23545;&#25239;&#33539;&#24335;&#26377;&#30528;&#20849;&#21516;&#30340;&#30446;&#26631;&#65292;&#20294;&#23427;&#20204;&#30340;&#21457;&#23637;&#20960;&#20046;&#26159;&#29420;&#31435;&#30340;&#65292;&#23545;&#20110;AML&#39046;&#22495;&#20173;&#28982;&#27809;&#26377;&#23436;&#25972;&#30340;&#25972;&#20307;&#35748;&#30693;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;AML&#31038;&#21306;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#31995;&#32479;&#22320;&#22238;&#39038;&#35813;&#39046;&#22495;&#30340;&#25972;&#20307;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;AML&#30340;&#36890;&#29992;&#23450;&#20041;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#28085;&#30422;&#29616;&#26377;&#30340;&#25915;&#20987;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial machine learning (AML) studies the adversarial phenomenon of machine learning, which may make inconsistent or unexpected predictions with humans. Some paradigms have been recently developed to explore this adversarial phenomenon occurring at different stages of a machine learning system, such as backdoor attack occurring at the pre-training, in-training and inference stage; weight attack occurring at the post-training, deployment and inference stage; adversarial attack occurring at the inference stage. However, although these adversarial paradigms share a common goal, their developments are almost independent, and there is still no big picture of AML. In this work, we aim to provide a unified perspective to the AML community to systematically review the overall progress of this field. We firstly provide a general definition about AML, and then propose a unified mathematical framework to covering existing attack paradigms. According to the proposed unified framework, we buil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36924;&#36817;&#27861;&#65292;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.09267</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#27861;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation Approaches to Group Distributionally Robust Optimization. (arXiv:2302.09267v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36924;&#36817;&#27861;&#65292;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#65292;&#30446;&#30340;&#26159;&#23398;&#20064;&#19968;&#20010;&#33021;&#22312;$m$&#20010;&#19981;&#21516;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;GDRO&#24314;&#27169;&#20026;&#38543;&#26426;&#20984;&#20985;&#38797;&#28857;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;$m$&#20010;&#26679;&#26412;&#30340;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#27861;(SMD)&#65292;&#33021;&#22815;&#23454;&#29616;$O(m(\log m)/\epsilon ^2)$&#20010;&#26679;&#26412;&#30340;&#22797;&#26434;&#24230;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#35299;&#65292;&#36825;&#19982;$\Omega(m/\epsilon ^2)$&#30340;&#19979;&#30028;&#24819;&#21305;&#37197;&#65292;&#38500;&#20102;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;GDRO&#26500;&#36896;&#20026;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#31616;&#21333;&#22320;&#25191;&#34892;SMD&#65292;&#21478;&#19968;&#20010;&#25191;&#34892;&#19968;&#31181;&#29992;&#20110;&#38750;&#26126;&#26174;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#21487;&#20197;&#20174;&#27599;&#20010;&#20998;&#24067;&#20013;&#32472;&#21046;&#30340;&#26679;&#26412;&#25968;&#37327;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates group distributionally robust optimization (GDRO), with the purpose to learn a model that performs well over $m$ different distributions. First, we formulate GDRO as a stochastic convex-concave saddle-point problem, and demonstrate that stochastic mirror descent (SMD), using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$ sample complexity for finding an $\epsilon$-optimal solution, which matches the $\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make use of techniques from online learning to reduce the number of samples required in each round from $m$ to $1$, keeping the same sample complexity. Specifically, we cast GDRO as a two-players game where one player simply performs SMD and the other executes an online algorithm for non-oblivious multi-armed bandits. Next, we consider a more practical scenario where the number of samples that can be drawn from each distribution is different, and propose a novel formulation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27010;&#25324;&#25216;&#26415;&#65292;&#22312;&#20445;&#30041;&#22270;&#24418;&#20851;&#38190;&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#35268;&#27169;&#12289;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#29616;&#20195;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#31561;&#12290;</title><link>http://arxiv.org/abs/2302.06114</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#24418;&#27010;&#25324;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Summarization with Graph Neural Networks. (arXiv:2302.06114v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27010;&#25324;&#25216;&#26415;&#65292;&#22312;&#20445;&#30041;&#22270;&#24418;&#20851;&#38190;&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#35268;&#27169;&#12289;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#29616;&#20195;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;&#26222;&#21450;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35745;&#31639;&#25361;&#25112;&#26292;&#38706;&#20986;&#26469;&#65292;&#38656;&#35201;&#25552;&#21462;&#12289;&#22788;&#29702;&#21644;&#35299;&#37322;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23547;&#25214;&#19968;&#31181;&#33021;&#22815;&#24635;&#32467;&#36825;&#20123;&#24191;&#38420;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20851;&#38190;&#29305;&#24449;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#12290;&#36807;&#21435;&#65292;&#22823;&#22810;&#25968;&#22270;&#24418;&#27010;&#25324;&#25216;&#26415;&#26088;&#22312;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25429;&#25417;&#22270;&#24418;&#30340;&#26368;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20170;&#22825;&#65292;&#29616;&#20195;&#22270;&#24418;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26356;&#21152;&#27969;&#34892;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#28145;&#24230;&#23398;&#20064;&#27010;&#25324;&#25216;&#26415;&#36827;&#23637;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#22238;&#39038;&#65292;&#21253;&#25324;&#24490;&#29615;GNNs&#12289;&#21367;&#31215;GNNs&#12289;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#21516;&#26102;&#36824;&#35752;&#35770;&#20102;&#19968;&#26465;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#20351;&#29992;&#22270;&#24418;&#24378;&#21270;&#23398;&#20064;&#26469;&#35780;&#20272;&#21644;&#25913;&#36827;&#22270;&#24418;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large-scale graphs become more widespread, more and more computational challenges with extracting, processing, and interpreting large graph data are being exposed. It is therefore natural to search for ways to summarize these expansive graphs while preserving their key characteristics. In the past, most graph summarization techniques sought to capture the most important part of a graph statistically. However, today, the high dimensionality and complexity of modern graph data are making deep learning techniques more popular. Hence, this paper presents a comprehensive survey of progress in deep learning summarization techniques that rely on graph neural networks (GNNs). Our investigation includes a review of the current state-of-the-art approaches, including recurrent GNNs, convolutional GNNs, graph autoencoders, and graph attention networks. A new burgeoning line of research is also discussed where graph reinforcement learning is being used to evaluate and improve the quality of grap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#20960;&#20309;&#26500;&#36896;&#20102;&#32447;&#24615;&#20960;&#20046;&#27431;&#20960;&#37324;&#24471;&#27969;&#24418;&#65292;&#36890;&#36807;&#24341;&#20837;&#20559;&#24494;&#20998;&#26041;&#31243;Ricci&#27969;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#26799;&#24230;&#26080;&#31351;&#25110;&#38646;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.03390</link><description>&lt;p&gt;
&#22312;Ricci&#27969;&#19979;&#23398;&#20064;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Discretized Neural Networks under Ricci Flow. (arXiv:2302.03390v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#20960;&#20309;&#26500;&#36896;&#20102;&#32447;&#24615;&#20960;&#20046;&#27431;&#20960;&#37324;&#24471;&#27969;&#24418;&#65292;&#36890;&#36807;&#24341;&#20837;&#20559;&#24494;&#20998;&#26041;&#31243;Ricci&#27969;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#26799;&#24230;&#26080;&#31351;&#25110;&#38646;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#30001;&#20302;&#31934;&#24230;&#26435;&#37325;&#21644;&#28608;&#27963;&#20989;&#25968;&#26500;&#25104;&#30340;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30001;&#20110;&#38750;&#21487;&#24494;&#20998;&#31163;&#25955;&#20989;&#25968;&#32780;&#36973;&#21463;&#26080;&#31351;&#25110;&#38646;&#26799;&#24230;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#27492;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25226; STE&#36817;&#20284;&#26799;&#24230;&#30475;&#20316;&#25972;&#20307;&#20559;&#24046;&#30340;&#24230;&#37327;&#25200;&#21160;&#65292;&#36890;&#36807;&#23545;&#20598;&#29702;&#35770;&#23558;&#20854;&#30475;&#20316;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#24230;&#37327;&#25200;&#21160;&#65292;&#24182;&#22312;&#20449;&#24687;&#20960;&#20309;&#30340;&#22522;&#30784;&#19978;&#20026; DNN&#26500;&#36896;&#20102;&#32447;&#24615;&#20960;&#20046;&#27431;&#20960;&#37324;&#24471;&#65288;LNE&#65289;&#27969;&#24418;&#20197;&#22788;&#29702;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider Discretized Neural Networks (DNNs) consisting of low-precision weights and activations, which suffer from either infinite or zero gradients due to the non-differentiable discrete function in the training process. In this case, most training-based DNNs employ the standard Straight-Through Estimator (STE) to approximate the gradient w.r.t. discrete values. However, the STE gives rise to the problem of gradient mismatch, due to the perturbations of the approximated gradient. To address this problem, this paper reveals that this mismatch can be viewed as a metric perturbation in a Riemannian manifold through the lens of duality theory. Further, on the basis of the information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold for DNNs as a background to deal with perturbations. By introducing a partial differential equation on metrics, i.e., the Ricci flow, we prove the dynamical stability and convergence of the LNE metric with the $L^2$-norm per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#23454;&#39564;&#12289;&#25968;&#20540;&#27169;&#25311;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25512;&#24191;&#31649;&#36947;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#21457;&#29616;&#20855;&#26377;&#26368;&#20339;&#21018;&#24230;-&#38887;&#24230;&#26435;&#34913;&#30340;&#24494;&#32467;&#26500;&#22797;&#21512;&#26448;&#26009;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23884;&#22871;&#24490;&#29615;&#30340;&#24314;&#35758;-&#39564;&#35777;&#24037;&#20316;&#27969;&#65292;&#25104;&#21151;&#24357;&#21512;&#20102;&#27169;&#25311;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#20197;&#39640;&#26679;&#26412;&#25928;&#29575;&#21457;&#29616;&#20102;&#21018;&#24230;&#21644;&#38887;&#24230;&#20860;&#20855;&#30340;&#26448;&#26009;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#24085;&#32047;&#25176;&#26368;&#20248;&#35774;&#35745;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20986;&#29616;&#26377;&#30340;&#22686;&#21152;&#38887;&#24615;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.01078</link><description>&lt;p&gt;
&#35745;&#31639;&#21457;&#29616;&#20855;&#26377;&#26368;&#20339;&#21018;&#24230;-&#38887;&#24230;&#26435;&#34913;&#30340;&#24494;&#32467;&#26500;&#22797;&#21512;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Computational Discovery of Microstructured Composites with Optimal Stiffness-Toughness Trade-Offs. (arXiv:2302.01078v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#23454;&#39564;&#12289;&#25968;&#20540;&#27169;&#25311;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25512;&#24191;&#31649;&#36947;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#21457;&#29616;&#20855;&#26377;&#26368;&#20339;&#21018;&#24230;-&#38887;&#24230;&#26435;&#34913;&#30340;&#24494;&#32467;&#26500;&#22797;&#21512;&#26448;&#26009;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23884;&#22871;&#24490;&#29615;&#30340;&#24314;&#35758;-&#39564;&#35777;&#24037;&#20316;&#27969;&#65292;&#25104;&#21151;&#24357;&#21512;&#20102;&#27169;&#25311;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#20197;&#39640;&#26679;&#26412;&#25928;&#29575;&#21457;&#29616;&#20102;&#21018;&#24230;&#21644;&#38887;&#24230;&#20860;&#20855;&#30340;&#26448;&#26009;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#24085;&#32047;&#25176;&#26368;&#20248;&#35774;&#35745;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20986;&#29616;&#26377;&#30340;&#22686;&#21152;&#38887;&#24615;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21018;&#24230;&#21644;&#38887;&#24230;&#20043;&#38388;&#30340;&#20914;&#31361;&#26159;&#24037;&#31243;&#26448;&#26009;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#28436;&#31034;&#20351;&#29992;&#25972;&#20010;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25968;&#25454;&#39640;&#25928;&#25506;&#32034;&#26469;&#31995;&#32479;&#22320;&#21457;&#29616;&#20855;&#26377;&#26368;&#20339;&#21018;&#24230;-&#38887;&#24230;&#26435;&#34913;&#30340;&#24494;&#32467;&#26500;&#22797;&#21512;&#26448;&#26009;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#23545;&#25972;&#20010;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25968;&#25454;&#39640;&#25928;&#25506;&#32034;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#30340;&#31649;&#36947;&#65292;&#23558;&#29289;&#29702;&#23454;&#39564;&#12289;&#25968;&#20540;&#27169;&#25311;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#39044;&#35774;&#26448;&#26009;&#35774;&#35745;&#19987;&#23478;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#26045;&#20102;&#19968;&#20010;&#23884;&#22871;&#24490;&#29615;&#30340;&#24314;&#35758;-&#39564;&#35777;&#24037;&#20316;&#27969;&#65292;&#20197;&#24357;&#21512;&#27169;&#25311;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#20197;&#39640;&#26679;&#26412;&#25928;&#29575;&#21457;&#29616;&#20102;&#26082;&#21018;&#21448;&#38887;&#30340;&#24494;&#32467;&#26500;&#22797;&#21512;&#26448;&#26009;&#12290;&#23545;&#24085;&#32047;&#25176;&#26368;&#20248;&#35774;&#35745;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#20986;&#29616;&#26377;&#30340;&#22686;&#21152;&#38887;&#24615;&#26426;&#21046;&#65292;&#32780;&#36825;&#20123;&#26426;&#21046;&#20197;&#21069;&#26159;&#36890;&#36807;&#35797;&#39564;&#21644;&#29983;&#29289;&#27169;&#20223;&#26469;&#21457;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conflict between stiffness and toughness is a fundamental problem in engineering materials design. However, the systematic discovery of microstructured composites with optimal stiffness-toughness trade-offs has never been demonstrated, hindered by the discrepancies between simulation and reality and the lack of data-efficient exploration of the entire Pareto front. We introduce a generalizable pipeline that integrates physical experiments, numerical simulations, and artificial neural networks to address both challenges. Without any prescribed expert knowledge of material design, our approach implements a nested-loop proposal-validation workflow to bridge the simulation-to-reality gap and discover microstructured composites that are stiff and tough with high sample efficiency. Further analysis of Pareto-optimal designs allows us to automatically identify existing toughness enhancement mechanisms, which were previously discovered through trial-and-error or biomimicry. On a broader sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;Shapley&#20540;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#65292;&#23427;&#20204;&#22522;&#20110;&#19968;&#31181;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23558;&#20854;&#19982;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#35265;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2302.00736</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#36793;&#38469;&#36129;&#29486;&#36817;&#20284;&#35745;&#31639;Shapley&#20540;
&lt;/p&gt;
&lt;p&gt;
Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;Shapley&#20540;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#65292;&#23427;&#20204;&#22522;&#20110;&#19968;&#31181;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23558;&#20854;&#19982;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#35265;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26159;&#20026;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#29609;&#23478;&#20998;&#37197;&#26377;&#24847;&#20041;&#30340;&#36129;&#29486;&#20540;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#65292;&#26368;&#36817;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;Shapley&#20540;&#30340;&#26377;&#24847;&#20041;&#24615;&#28304;&#20110;&#20165;&#26377;Shapley&#20540;&#28385;&#36275;&#30340;&#20844;&#29702;&#23646;&#24615;&#65292;&#28982;&#32780;&#65292;&#30830;&#20999;&#35745;&#31639;&#30340;&#20195;&#20215;&#26159;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#39640;&#25928;&#36817;&#20284;Shapley&#20540;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22260;&#32469;&#30528;&#29609;&#23478;&#30340;&#36793;&#38469;&#36129;&#29486;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#30340;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#21253;&#25324;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#29992;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#30340;&#23454;&#35777;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley value is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, which has recently been used intensively in explainable artificial intelligence. The meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley values, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contributions. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparin
&lt;/p&gt;</description></item><item><title>SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2301.11329</link><description>&lt;p&gt;
SynthMorph&#23454;&#29616;&#30340;&#32771;&#34385;&#35299;&#21078;&#32467;&#26500;&#21644;&#26080;&#20851;&#37319;&#38598;&#26041;&#27861;&#30340;&#32852;&#21512;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11329
&lt;/p&gt;
&lt;p&gt;
SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#23556;&#22270;&#20687;&#37197;&#20934;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#34429;&#28982;&#20256;&#32479;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#19968;&#23545;&#22270;&#20687;&#36827;&#34892;&#32791;&#26102;&#30340;&#20248;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23558;&#22270;&#20687;&#23545;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#25442;&#30340;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20010;&#20989;&#25968;&#26159;&#24555;&#36895;&#30340;&#65292;&#20294;&#25429;&#25417;&#22823;&#30340;&#21464;&#25442;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#19988;&#22914;&#26524;&#27979;&#35797;&#22270;&#20687;&#30340;&#29305;&#24449;&#20174;&#35757;&#32451;&#39046;&#22495;&#21464;&#21270;&#65292;&#22914;&#20998;&#36776;&#29575;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#20223;&#23556;&#26041;&#27861;&#26159;&#23545;&#35299;&#21078;&#32467;&#26500;&#26080;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#22914;&#26524;&#31639;&#27861;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#32467;&#26500;&#65292;&#37197;&#20934;&#20250;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;SynthMorph&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23427;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#36827;&#34892;&#25805;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#30340;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#22810;&#26679;&#21270;&#37319;&#38598;&#35268;&#33539;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#32771;&#34385;&#19981;&#21516;&#30340;&#35299;&#21078;&#29305;&#24449;&#21644;&#23398;&#20064;&#25269;&#21046;&#37319;&#38598;&#29305;&#23450;&#38480;&#21046;&#30340;&#21464;&#25442;&#12290;&#36890;&#36807;&#36825;&#20123;&#21019;&#26032;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affine image registration is a cornerstone of medical-image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to anatomy, meaning the registration will be inaccurate if algorithms consider all structures in the image.  We address these shortcomings with SynthMorph, an easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we leverage a strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics unseen at training. Second, we opti
&lt;/p&gt;</description></item><item><title>DeepTaster&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20351;&#29992;&#21463;&#23475;&#32773;&#25968;&#25454;&#38750;&#27861;&#26500;&#24314;&#23244;&#30097;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21363;&#20351;&#23244;&#30097;&#27169;&#22411;&#30340;&#26550;&#26500;&#19982;&#21463;&#23475;&#27169;&#22411;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2211.13535</link><description>&lt;p&gt;
DeepTaster: &#22522;&#20110;&#23545;&#25239;&#25200;&#21160;&#30340;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35782;&#21035;&#19987;&#26377;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks. (arXiv:2211.13535v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13535
&lt;/p&gt;
&lt;p&gt;
DeepTaster&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20351;&#29992;&#21463;&#23475;&#32773;&#25968;&#25454;&#38750;&#27861;&#26500;&#24314;&#23244;&#30097;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21363;&#20351;&#23244;&#30097;&#27169;&#22411;&#30340;&#26550;&#26500;&#19982;&#21463;&#23475;&#27169;&#22411;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#19968;&#20123;&#25152;&#26377;&#32773;&#38480;&#21046;&#26410;&#32463;&#35768;&#21487;&#30340;&#37325;&#26032;&#20998;&#21457;&#12290;&#23558;&#20445;&#23494;&#25968;&#25454;&#23884;&#20837;DNN&#30340;&#27700;&#21360;&#25216;&#26415;&#34987;&#29992;&#20110;&#20445;&#25252;&#25152;&#26377;&#26435;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#21463;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;DeepJudge&#34987;&#24341;&#20837;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#23244;&#30097;&#27169;&#22411;&#19982;&#21463;&#23475;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#34429;&#28982;DeepJudge&#22312;&#35299;&#20915;&#27700;&#21360;&#25216;&#26415;&#30340;&#19981;&#36275;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20027;&#35201;&#24212;&#29992;&#20110;&#23244;&#30097;&#27169;&#22411;&#25220;&#34989;&#21463;&#23475;&#27169;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DeepTaster&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#21463;&#23475;&#32773;&#25968;&#25454;&#38750;&#27861;&#26500;&#24314;&#23244;&#30097;&#27169;&#22411;&#30340;&#24773;&#20917;&#12290;DeepTaster&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#36825;&#31181;DNN&#27169;&#22411;&#30423;&#31363;&#25915;&#20987;&#65292;&#21363;&#20351;&#23244;&#30097;&#27169;&#22411;&#30340;&#26550;&#26500;&#19982;&#21463;&#23475;&#27169;&#22411;&#19981;&#21516;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;DeepTaster&#29983;&#25104;&#19968;&#20010;&#23545;&#21463;&#23475;&#27169;&#22411;&#20855;&#26377;&#40065;&#26834;&#24615;&#19988;&#21487;&#20197;&#19968;&#33268;&#24615;&#22320;&#35782;&#21035;&#30340;&#29420;&#29305;&#25351;&#32441;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) requires large datasets and powerful computing resources, which has led some owners to restrict redistribution without permission. Watermarking techniques that embed confidential data into DNNs have been used to protect ownership, but these can degrade model performance and are vulnerable to watermark removal attacks. Recently, DeepJudge was introduced as an alternative approach to measuring the similarity between a suspect and a victim model. While DeepJudge shows promise in addressing the shortcomings of watermarking, it primarily addresses situations where the suspect model copies the victim's architecture. In this study, we introduce DeepTaster, a novel DNN fingerprinting technique, to address scenarios where a victim's data is unlawfully used to build a suspect model. DeepTaster can effectively identify such DNN model theft attacks, even when the suspect model's architecture deviates from the victim's. To accomplish this, DeepTaster generates a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#25511;&#21046;&#24615;&#36136;&#65292;&#21457;&#29616;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#21487;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#30340;&#19979;&#28216;&#25193;&#25955;KSD&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;</title><link>http://arxiv.org/abs/2211.05408</link><description>&lt;p&gt;
&#29992;&#26680;&#26031;&#22374;&#31163;&#24046;&#25511;&#21046;&#30697;
&lt;/p&gt;
&lt;p&gt;
Controlling Moments with Kernel Stein Discrepancies. (arXiv:2211.05408v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#25511;&#21046;&#24615;&#36136;&#65292;&#21457;&#29616;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#21487;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#30340;&#19979;&#28216;&#25193;&#25955;KSD&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#29992;&#20110;&#34913;&#37327;&#20998;&#24067;&#36924;&#36817;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#30446;&#26631;&#23494;&#24230;&#20855;&#26377;&#19981;&#21487;&#35745;&#31639;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#26102;&#35745;&#31639;&#12290;&#26174;&#33879;&#30340;&#24212;&#29992;&#21253;&#25324;&#35786;&#26029;&#36817;&#20284;MCMC&#37319;&#26679;&#22120;&#21644;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#36866;&#37197;&#24230;&#26816;&#39564;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;KSD&#30340;&#25910;&#25947;&#25511;&#21046;&#24615;&#36136;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#29992;&#20110;&#24369;&#25910;&#25947;&#25511;&#21046;&#30340;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#19979;&#28216;&#25193;&#25955;KSD&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#12290;&#20316;&#20026;&#19968;&#20010;&#30452;&#25509;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#23545;&#20110;&#27599;&#20010;$q&gt;0$&#65292;&#31532;&#19968;&#32452;&#24050;&#30693;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q &gt; 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22788;&#29702;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22352;&#26631;&#36716;&#25442;&#65292;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#34920;&#31034;&#20026;&#20108;&#27425;&#31995;&#32479;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.00357</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#24191;&#20041;&#20108;&#27425;&#23884;&#20837;&#26469;&#22788;&#29702;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Generalized Quadratic Embeddings for Nonlinear Dynamics using Deep Learning. (arXiv:2211.00357v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22788;&#29702;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22352;&#26631;&#36716;&#25442;&#65292;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#34920;&#31034;&#20026;&#20108;&#27425;&#31995;&#32479;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#31243;&#35774;&#35745;&#36807;&#31243;&#36890;&#24120;&#20381;&#36182;&#20110;&#33021;&#22815;&#25551;&#36848;&#24213;&#23618;&#21160;&#24577;&#34892;&#20026;&#30340;&#25968;&#23398;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#24314;&#27169;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#22352;&#26631;&#36716;&#25442;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#19968;&#20010;&#24120;&#35265;&#30340;&#31616;&#21333;&#27169;&#22411;&#32467;&#26500;&#26469;&#34920;&#31034;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#24120;&#35265;&#31616;&#21333;&#27169;&#22411;&#30340;&#20248;&#28857;&#22312;&#20110;&#21487;&#20197;&#23558;&#20026;&#20854;&#24320;&#21457;&#30340;&#23450;&#21046;&#35774;&#35745;&#24037;&#20855;&#24212;&#29992;&#20110;&#30740;&#31350;&#21508;&#31181;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#26368;&#31616;&#21333;&#30340;&#24120;&#35265;&#27169;&#22411;&#21487;&#20197;&#35748;&#20026;&#26159;&#32447;&#24615;&#30340;&#65292;&#20294;&#32447;&#24615;&#31995;&#32479;&#24448;&#24448;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20108;&#27425;&#31995;&#32479;&#20316;&#20026;&#24120;&#35265;&#32467;&#26500;&#65292;&#21463;&#21040;&#21319;&#32500;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26681;&#25454;&#36825;&#19968;&#21407;&#29702;&#65292;&#20809;&#28369;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#21487;&#20197;&#22312;&#36866;&#24403;&#30340;&#22352;&#26631;&#20013;&#34920;&#31034;&#20026;&#20108;&#27425;&#31995;&#32479;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36817;&#20284;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#25214;&#21040;&#36825;&#20123;&#22352;&#26631;&#26159;&#22256;&#38590;&#30340;&#65292;&#25152;&#20197;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#23398;&#20064;&#36825;&#20123;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The engineering design process often relies on mathematical modeling that can describe the underlying dynamic behavior. In this work, we present a data-driven methodology for modeling the dynamics of nonlinear systems. To simplify this task, we aim to identify a coordinate transformation that allows us to represent the dynamics of nonlinear systems using a common, simple model structure. The advantage of a common simple model is that customized design tools developed for it can be applied to study a large variety of nonlinear systems. The simplest common model -- one can think of -- is linear, but linear systems often fall short in accurately capturing the complex dynamics of nonlinear systems. In this work, we propose using quadratic systems as the common structure, inspired by the lifting principle. According to this principle, smooth nonlinear systems can be expressed as quadratic systems in suitable coordinates without approximation errors. However, finding these coordinates solely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mixup&#24341;&#23548;&#30340;&#20248;&#21270;&#19982;&#36873;&#25321;&#25216;&#26415;&#65292;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#36890;&#36807;&#29983;&#25104;&#30446;&#26631;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#21644;&#26356;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#39564;&#35777;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.00652</link><description>&lt;p&gt;
&#36817;&#20284;&#20248;&#21270;&#21644;&#27169;&#22411;&#36873;&#25321;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#22522;&#20110;Mixup&#24341;&#23548;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution. (arXiv:2209.00652v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mixup&#24341;&#23548;&#30340;&#20248;&#21270;&#19982;&#36873;&#25321;&#25216;&#26415;&#65292;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#36890;&#36807;&#29983;&#25104;&#30446;&#26631;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#21644;&#26356;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#39564;&#35777;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#36890;&#24120;&#20250;&#21066;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#24037;&#20316;&#20851;&#27880;&#20110;&#39046;&#22495;&#27867;&#21270; (DG)&#65292;&#20854;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#19988;&#30446;&#26631;&#25968;&#25454;&#26159;&#26410;&#35265;&#36807;&#30340;&#12290;&#23613;&#31649;&#22312;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20004;&#20010;&#22522;&#26412;&#22240;&#32032;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#24573;&#35270;&#65306;1) &#38754;&#21521;&#27491;&#21017;&#21270;&#30446;&#26631;&#30340;&#20248;&#21270;&#65292;&#20197;&#21450;2) &#38024;&#23545;DG&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#22240;&#20026;&#26080;&#27861;&#21033;&#29992;&#26377;&#20851;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mixup&#24341;&#23548;&#30340;&#20248;&#21270;&#19982;&#36873;&#25321;&#25216;&#26415;&#65292;&#29992;&#20110;DG&#12290;&#23545;&#20110;&#20248;&#21270;&#65292;&#25105;&#20204;&#21033;&#29992;&#35843;&#25972;&#21518;&#30340;Mixup&#29983;&#25104;&#19968;&#20010;&#20998;&#24067;&#19982;&#30446;&#26631;&#39046;&#22495;&#26377;&#25351;&#23548;&#20316;&#29992;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;Pareto&#20248;&#21270;&#36827;&#34892;&#20248;&#21270;&#12290;&#23545;&#20110;&#27169;&#22411;&#36873;&#25321;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#26356;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#39564;&#35777;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#22909;&#22320;&#20195;&#34920;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#25105;&#20204;&#25552;&#35758;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distribution shifts between training and test data typically undermine the performance of models. In recent years, lots of work pays attention to domain generalization (DG) where distribution shifts exist, and target data are unseen. Despite the progress in algorithm design, two foundational factors have long been ignored: 1) the optimization for regularization-based objectives, and 2) the model selection for DG since no knowledge about the target domain can be utilized. In this paper, we propose Mixup guided optimization and selection techniques for DG. For optimization, we utilize an adapted Mixup to generate an out-of-distribution dataset that can guide the preference direction and optimize with Pareto optimization. For model selection, we generate a validation dataset with a closer distance to the target distribution, and thereby it can better represent the target data. We also present some theoretical insights behind our proposals. Comprehensive experiments demonstrate that ou
&lt;/p&gt;</description></item><item><title>ULF&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2204.06863</link><description>&lt;p&gt;
ULF: &#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision. (arXiv:2204.06863v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06863
&lt;/p&gt;
&lt;p&gt;
ULF&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#20195;&#26367;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#30340;&#32463;&#27982;&#26377;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#26631;&#31614;&#20989;&#25968;&#65288;LFs&#65289;&#23545;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#33258;&#21160;&#26631;&#27880;&#12290;&#26631;&#31614;&#20989;&#25968;&#26159;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20026;&#30456;&#20851;&#31867;&#21035;&#29983;&#25104;&#20154;&#24037;&#26631;&#31614;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;k&#25240;&#20132;&#21449;&#39564;&#35777;&#21407;&#29702;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#22122;&#22768;&#38477;&#20302;&#25216;&#26415;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ULF&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#26631;&#31614;&#20989;&#25968;&#26657;&#27491;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#25152;&#26377;&#26631;&#31614;&#20989;&#25968;&#20043;&#22806;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#32416;&#27491;&#29305;&#23450;&#20110;&#20445;&#30041;&#26631;&#31614;&#20989;&#25968;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#23545;&#24369;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ULF&#36890;&#36807;&#37325;&#26032;&#20272;&#35745;&#39640;&#21487;&#38752;&#24615;&#20132;&#21449;&#39564;&#35777;&#26679;&#26412;&#19978;&#30340;&#26631;&#31614;&#20989;&#25968;&#20998;&#37197;&#65292;&#26469;&#25913;&#36827;&#26631;&#31614;&#20989;&#25968;&#23545;&#31867;&#21035;&#30340;&#20998;&#37197;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#23454;&#20102;ULF&#22312;&#22686;&#24378;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold cross-validation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models trained on all but some LFs to identify and correct biases specific to the held-out LFs. Specifically, ULF refines the allocation of LFs to classes by re-estimating this assignment on highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF's effectiveness in enhancing WS learning without the need for manual labeling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#30340;&#24179;&#22343;&#20998;&#24067;&#20248;&#21270;&#24179;&#28369;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#20302;&#31209;&#26799;&#24230;&#19979;&#38477;&#65288;FedLRGD&#65289;&#31639;&#27861;&#26469;&#21033;&#29992;&#25968;&#25454;&#30340;&#24179;&#28369;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2201.01954</link><description>&lt;p&gt;
&#24179;&#22343;&#20998;&#24067;&#20248;&#21270;&#24179;&#28369;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Federated Optimization of Smooth Loss Functions. (arXiv:2201.01954v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#30340;&#24179;&#22343;&#20998;&#24067;&#20248;&#21270;&#24179;&#28369;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#20302;&#31209;&#26799;&#24230;&#19979;&#38477;&#65288;FedLRGD&#65289;&#31639;&#27861;&#26469;&#21033;&#29992;&#25968;&#25454;&#30340;&#24179;&#28369;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#20351;&#29992;&#23384;&#20648;&#22312;$m$&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#26368;&#23567;&#21270;ERM&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#24179;&#22343;&#65288;FedAve&#65289;&#31639;&#27861;&#26159;&#30830;&#23450;ERM&#38382;&#39064;&#30340;$\epsilon$&#36817;&#20284;&#35299;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#26631;&#20934;&#20248;&#21270;&#31639;&#27861;&#65292;FedAve&#30340;&#25910;&#25947;&#20998;&#26512;&#20165;&#20381;&#36182;&#20110;&#20248;&#21270;&#21442;&#25968;&#20013;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#36890;&#24120;&#20063;&#38750;&#24120;&#24179;&#28369;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#39069;&#22806;&#30340;&#24179;&#28369;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#20302;&#31209;&#26799;&#24230;&#19979;&#38477;&#65288;FedLRGD&#65289;&#31639;&#27861;&#12290;&#30001;&#20110;&#25968;&#25454;&#30340;&#24179;&#28369;&#24615;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#24341;&#20837;&#20102;&#36817;&#20284;&#30340;&#20302;&#31209;&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20102;&#20960;&#36718;&#36890;&#20449;&#65292;&#20197;&#23398;&#20064;&#26381;&#21153;&#22120;&#21487;&#20197;&#29992;&#26469;&#36817;&#20284;&#23458;&#25143;&#31471;&#26799;&#24230;&#30340;&#26435;&#37325;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26381;&#21153;&#22120;&#19978;&#35299;&#20915;ERM&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study empirical risk minimization (ERM) within a federated learning framework, where a central server minimizes an ERM objective function using training data that is stored across $m$ clients. In this setting, the Federated Averaging (FedAve) algorithm is the staple for determining $\epsilon$-approximate solutions to the ERM problem. Similar to standard optimization algorithms, the convergence analysis of FedAve only relies on smoothness of the loss function in the optimization parameter. However, loss functions are often very smooth in the training data too. To exploit this additional smoothness, we propose the Federated Low Rank Gradient Descent (FedLRGD) algorithm. Since smoothness in data induces an approximate low rank structure on the loss function, our method first performs a few rounds of communication between the server and clients to learn weights that the server can use to approximate clients' gradients. Then, our method solves the ERM problem at the server 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;FedValue&#65292;&#35813;&#26041;&#27861;&#26159;&#38544;&#31169;&#20445;&#25252;&#12289;&#38024;&#23545;&#20219;&#21153;&#32780;&#26080;&#38656;&#27169;&#22411;&#30340;&#12290;&#23427;&#21253;&#25324;&#25968;&#25454;&#20215;&#20540;&#24230;&#37327;MShapley-CMI&#21644;&#32852;&#21512;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#35780;&#20272;&#25968;&#25454;&#26041;&#25968;&#25454;&#20215;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.08364</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65306;&#19968;&#31181;&#26080;&#27169;&#22411;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data Valuation for Vertical Federated Learning: A Model-free and Privacy-preserving Method. (arXiv:2112.08364v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;FedValue&#65292;&#35813;&#26041;&#27861;&#26159;&#38544;&#31169;&#20445;&#25252;&#12289;&#38024;&#23545;&#20219;&#21153;&#32780;&#26080;&#38656;&#27169;&#22411;&#30340;&#12290;&#23427;&#21253;&#25324;&#25968;&#25454;&#20215;&#20540;&#24230;&#37327;MShapley-CMI&#21644;&#32852;&#21512;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#35780;&#20272;&#25968;&#25454;&#26041;&#25968;&#25454;&#20215;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#39044;&#27979;&#20998;&#26512;&#33539;&#24335;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#25968;&#25454;&#25552;&#20379;&#26041;&#65288;&#21363;&#25968;&#25454;&#26041;&#65289;&#30340;&#21512;&#20316;&#65292;&#22312;&#20998;&#25955;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#19979;&#65292;&#36171;&#20104;&#19968;&#20010;&#32452;&#32455;&#65288;&#21363;&#20219;&#21153;&#26041;&#65289;&#25552;&#39640;&#20854;&#39044;&#27979;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#23545;VFL&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#32570;&#20047;&#26377;&#25928;&#19988;&#23433;&#20840;&#30340;&#24037;&#20855;&#26469;&#35780;&#20272;&#25968;&#25454;&#26041;&#25317;&#26377;&#30340;&#25968;&#25454;&#20215;&#20540;&#65292;&#21046;&#32422;&#20102;VFL&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedValue&#65292;&#19968;&#31181;&#38024;&#23545;VFL&#30340;&#20445;&#25252;&#38544;&#31169;&#19988;&#38024;&#23545;&#20219;&#21153;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#30001;&#25968;&#25454;&#20215;&#20540;&#24230;&#37327;&#21644;&#32852;&#21512;&#35745;&#31639;&#26041;&#27861;&#32452;&#25104;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20215;&#20540;&#24230;&#37327;&#65292;&#21363;MShapley-CMI&#12290;&#35813;&#24230;&#37327;&#35780;&#20272;&#25968;&#25454;&#26041;&#23545;&#39044;&#27979;&#20998;&#26512;&#20219;&#21153;&#30340;&#36129;&#29486;&#65292;&#32780;&#26080;&#38656;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#20110;VFL&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#21512;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated learning (VFL) is a promising paradigm for predictive analytics, empowering an organization (i.e., task party) to enhance its predictive models through collaborations with multiple data suppliers (i.e., data parties) in a decentralized and privacy-preserving way. Despite the fast-growing interest in VFL, the lack of effective and secure tools for assessing the value of data owned by data parties hinders the application of VFL in business contexts. In response, we propose FedValue, a privacy-preserving, task-specific but model-free data valuation method for VFL, which consists of a data valuation metric and a federated computation method. Specifically, we first introduce a novel data valuation metric, namely MShapley-CMI. The metric evaluates a data party's contribution to a predictive analytics task without the need of executing a machine learning model, making it well-suited for real-world applications of VFL. Next, we develop an innovative federated computation met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20195;&#25968;&#24555;&#25463;&#35268;&#33539;&#19982;DP&#31639;&#27861;&#30340;&#22810;&#24577;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22312;&#21322;&#29615;&#19978;&#34920;&#31034;&#30340;&#25152;&#26377;&#32452;&#21512;&#38382;&#39064;&#65292;&#21253;&#25324;&#20248;&#21270;&#12289;&#36923;&#36753;&#25512;&#29702;&#12289;&#27169;&#31946;&#38598;&#12289;&#21487;&#24494;softmax&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2107.01752</link><description>&lt;p&gt;
&#20195;&#25968;&#24555;&#25463;&#34701;&#21512;&#30340;&#22810;&#24577;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Polymorphic dynamic programming by algebraic shortcut fusion. (arXiv:2107.01752v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20195;&#25968;&#24555;&#25463;&#35268;&#33539;&#19982;DP&#31639;&#27861;&#30340;&#22810;&#24577;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22312;&#21322;&#29615;&#19978;&#34920;&#31034;&#30340;&#25152;&#26377;&#32452;&#21512;&#38382;&#39064;&#65292;&#21253;&#25324;&#20248;&#21270;&#12289;&#36923;&#36753;&#25512;&#29702;&#12289;&#27169;&#31946;&#38598;&#12289;&#21487;&#24494;softmax&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#35268;&#21010;(DP)&#26159;&#19968;&#31181;&#24191;&#27867;&#36866;&#29992;&#30340;&#31639;&#27861;&#35774;&#35745;&#33539;&#24335;&#65292;&#29992;&#20110;&#39640;&#25928;&#32780;&#30830;&#20999;&#22320;&#35299;&#20915;&#19981;&#21487;&#36991;&#20813;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31639;&#27861;&#30340;&#35774;&#35745;&#36890;&#24120;&#20197;&#19981;&#27491;&#24335;&#30340;&#26041;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#24448;&#24448;&#38590;&#20197;&#27491;&#30830;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#20195;&#25968;&#24418;&#24335;&#20027;&#20041;&#65292;&#21487;&#29992;&#20110;&#20174;&#29616;&#26377;&#30340;DP&#31639;&#27861;&#25110;&#31616;&#21333;&#30340;&#21151;&#33021;&#36882;&#24402;&#20013;&#31995;&#32479;&#22320;&#25512;&#23548;&#20986;&#26032;&#30340;DP&#31639;&#27861;&#12290;&#36825;&#20123;&#25512;&#23548;&#20135;&#29983;&#30340;&#31639;&#27861;&#26159;&#21487;&#35777;&#26126;&#27491;&#30830;&#21644;&#22810;&#24577;&#30340;&#65292;&#22312;&#20219;&#20309;&#21322;&#29615;&#19978;&#37117;&#33021;&#24212;&#29992;&#20110;&#21487;&#29992;&#21322;&#29615;&#34920;&#31034;&#30340;&#32452;&#21512;&#38382;&#39064;&#30340;&#20840;&#37096;&#33539;&#22260;&#12290;&#36825;&#21253;&#25324;&#65306;&#20248;&#21270;&#12289;&#26368;&#20248;&#27010;&#29575;&#21644;&#32500;&#29305;&#27604;&#35793;&#30721;&#12289;&#27010;&#29575;&#36793;&#32536;&#21270;&#12289;&#36923;&#36753;&#25512;&#29702;&#12289;&#27169;&#31946;&#38598;&#12289;&#21487;&#24494;softmax&#20197;&#21450;&#20851;&#31995;&#21644;&#28335;&#28304;&#26597;&#35810;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#35768;&#22810;&#29616;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#24314;&#35774;&#24615;&#31639;&#27861;&#30340;&#24819;&#27861;&#22522;&#30784;&#20043;&#19978;&#65292;&#22522;&#20110;&#20195;&#25968;&#24555;&#25463;&#35268;&#33539;&#19982;DP&#31639;&#27861;&#30340;&#34701;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#35768;&#22810;&#23454;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21147;&#37327;&#21644;&#24191;&#27867;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic programming (DP) is a broadly applicable algorithmic design paradigm for the efficient, exact solution of otherwise intractable, combinatorial problems. However, the design of such algorithms is often presented informally in an ad-hoc manner, and as a result is often difficult to apply correctly. In this paper, we present a rigorous algebraic formalism for systematically deriving novel DP algorithms, either from existing DP algorithms or from simple functional recurrences. These derivations lead to algorithms which are provably correct and polymorphic over any semiring, which means that they can be applied to the full scope of combinatorial problems expressible in terms of semirings. This includes, for example: optimization, optimal probability and Viterbi decoding, probabilistic marginalization, logical inference, fuzzy sets, differentiable softmax, and relational and provenance queries. The approach, building on many ideas from the existing literature on constructive algorith
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#22411;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#35757;&#32451;&#26399;&#38388;&#27745;&#26579;&#20840;&#23616;&#27169;&#22411;&#23454;&#29616;&#38544;&#34109;&#36890;&#20449;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.10561</link><description>&lt;p&gt;
&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#38544;&#34109;&#20449;&#36947;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Covert Channel Attack to Federated Learning Systems. (arXiv:2104.10561v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.10561
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#22411;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#35757;&#32451;&#26399;&#38388;&#27745;&#26579;&#20840;&#23616;&#27169;&#22411;&#23454;&#29616;&#38544;&#34109;&#36890;&#20449;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#22312;&#20247;&#22810;&#36793;&#32536;&#23458;&#25143;&#31471;&#20043;&#38388;&#20998;&#24067;&#27169;&#22411;&#35757;&#32451;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;&#36825;&#20123;&#23458;&#25143;&#31471;&#21512;&#20316;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#27844;&#38706;&#20182;&#20204;&#30340;&#26412;&#22320;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20043;&#38388;&#20849;&#20139;&#65292;&#29992;&#20110;&#26412;&#22320;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#36716;&#21270;&#20026;&#38544;&#34109;&#20449;&#36947;&#65292;&#23454;&#29616;&#38544;&#31192;&#30340;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#65292;&#22312;&#32852;&#37030;&#35757;&#32451;&#26399;&#38388;&#65292;&#24694;&#24847;&#21457;&#36865;&#32773;&#21487;&#20197;&#36890;&#36807;&#25552;&#20132;&#19987;&#38376;&#26500;&#36896;&#30340;&#26679;&#26412;&#26469;&#27745;&#26579;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;&#27169;&#22411;&#27745;&#26579;&#23545;&#20854;&#20182;&#21442;&#19982;&#32773;&#24433;&#21709;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#20063;&#19981;&#20250;&#25913;&#21464;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#23427;&#21487;&#20197;&#34987;&#24694;&#24847;&#25509;&#25910;&#32773;&#35266;&#23519;&#21040;&#65292;&#24182;&#29992;&#20110;&#20256;&#36755;&#19968;&#20010;&#27604;&#29305;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) goes beyond traditional, centralized machine learning by distributing model training among a large collection of edge clients. These clients cooperatively train a global, e.g., cloud-hosted, model without disclosing their local, private training data. The global model is then shared among all the participants which use it for local predictions. In this paper, we put forward a novel attacker model aiming at turning FL systems into covert channels to implement a stealth communication infrastructure. The main intuition is that, during federated training, a malicious sender can poison the global model by submitting purposely crafted examples. Although the effect of the model poisoning is negligible to other participants, and does not alter the overall model performance, it can be observed by a malicious receiver and used to transmit a single bit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#31614;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#22122;&#22768;&#21644;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.14956</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#21450;&#20854;&#22312;&#24189;&#38376;&#34746;&#26438;&#33740;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.14956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#31614;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#22122;&#22768;&#21644;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#32570;&#20047;&#20934;&#30830;&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#22240;&#27492;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#39318;&#20808;&#23545;&#21487;&#33021;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23454;&#20363;&#36827;&#34892;&#19968;&#20123;&#32416;&#27491;&#65292;&#28982;&#21518;&#29992;&#32416;&#27491;&#20449;&#24687;&#26356;&#26032;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#65288;MHWSIA&#65289;&#31561;&#29305;&#23450;&#39046;&#22495;&#20013;&#65292;&#19987;&#23478;&#24448;&#24448;&#38590;&#20197;&#25110;&#29978;&#33267;&#26080;&#27861;&#25163;&#21160;&#23454;&#29616;&#26080;&#22122;&#22768;&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#23548;&#33268;&#26631;&#31614;&#23384;&#22312;&#22797;&#26434;&#22122;&#22768;&#12290;&#36825;&#31181;&#24773;&#20917;&#24341;&#21457;&#20102;&#20004;&#20010;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65306;1&#65289;&#30001;&#20110;&#26631;&#31614;&#20013;&#23384;&#22312;&#22797;&#26434;&#22122;&#22768;&#65292;&#20808;&#21069;&#26041;&#27861;&#32416;&#27491;&#21487;&#33021;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23454;&#20363;&#30340;&#26041;&#27861;&#23398;&#23384;&#22312;&#23616;&#38480;&#24615;&#65307;2&#65289;&#30001;&#20110;&#25910;&#38598;&#26080;&#22122;&#22768;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#38750;&#24120;&#22256;&#38590;&#65292;&#39564;&#35777;/&#27979;&#35797;&#30340;&#36866;&#24403;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#32531;&#35299;&#20197;&#19978;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is an important concern because of the lack of accurate ground-truth labels in plenty of real-world scenarios. In practice, various approaches for this concern first make some corrections corresponding to potentially noisy-labeled instances, and then update predictive model with information of the made corrections. However, in specific areas, such as medical histopathology whole slide image analysis (MHWSIA), it is often difficult or even impossible for experts to manually achieve the noisy-free ground-truth labels which leads to labels with complex noise. This situation raises two more difficult problems: 1) the methodology of approaches making corrections corresponding to potentially noisy-labeled instances has limitations due to the complex noise existing in labels; and 2) the appropriate evaluation strategy for validation/testing is unclear because of the great difficulty in collecting the noisy-free ground-truth labels. In this paper, we focus on allevia
&lt;/p&gt;</description></item></channel></rss>