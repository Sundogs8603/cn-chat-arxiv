<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08600</link><description>&lt;p&gt;
&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#29702;&#35299;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#22810;&#20041;&#24615;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#22312;&#22810;&#20010;&#35821;&#20041;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#28608;&#27963;&#12290;&#22810;&#20041;&#24615;&#20351;&#25105;&#20204;&#26080;&#27861;&#25214;&#21040;&#31616;&#27905;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#24037;&#20316;&#12290;&#22810;&#20041;&#24615;&#30340;&#19968;&#20010;&#29468;&#27979;&#21407;&#22240;&#26159;&#21472;&#21152;&#25928;&#24212;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#37197;&#32473;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#36807;&#23436;&#22791;&#26041;&#21521;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#20010;&#21035;&#31070;&#32463;&#20803;&#65292;&#34920;&#31034;&#26356;&#22810;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#36825;&#20123;&#26041;&#21521;&#65292;&#20197;&#37325;&#26500;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#12290;&#36825;&#20123;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#19968;&#32452;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#27604;&#20854;&#20182;&#26041;&#27861;&#37492;&#23450;&#20986;&#30340;&#26041;&#21521;&#26356;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#65292;&#35299;&#37322;&#24615;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#34913;&#37327;&#30340;&#12290;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#20363;&#22914;&#36890;&#36807;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Ablating these features enables precise model editing, for example, by remo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;&#20869;&#37096;&#32500;&#24230;&#20026;1&#30340;&#25513;&#27169;&#27880;&#24847;&#21147;&#22836;&#23454;&#29616;MLP&#31070;&#32463;&#20803;&#65292;&#21487;&#20197;&#23558;MLP&#21644;&#27880;&#24847;&#21147;Transformer&#36716;&#25442;&#20026;&#20165;&#27880;&#24847;&#21147;&#30340;Transformer&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#22836;&#21487;&#20197;&#20998;&#21035;&#25191;&#34892;MLP&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20854;&#26435;&#37325;&#30697;&#38453;&#20013;&#32534;&#30721;&#20219;&#24847;&#30340;&#25513;&#30721;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.08593</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#27880;&#24847;&#21147;&#30340;Transformer&#21644;&#20351;&#29992;&#27880;&#24847;&#21147;&#22836;&#23454;&#29616;MLPs
&lt;/p&gt;
&lt;p&gt;
Attention-Only Transformers and Implementing MLPs with Attention Heads. (arXiv:2309.08593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;&#20869;&#37096;&#32500;&#24230;&#20026;1&#30340;&#25513;&#27169;&#27880;&#24847;&#21147;&#22836;&#23454;&#29616;MLP&#31070;&#32463;&#20803;&#65292;&#21487;&#20197;&#23558;MLP&#21644;&#27880;&#24847;&#21147;Transformer&#36716;&#25442;&#20026;&#20165;&#27880;&#24847;&#21147;&#30340;Transformer&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#22836;&#21487;&#20197;&#20998;&#21035;&#25191;&#34892;MLP&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20854;&#26435;&#37325;&#30697;&#38453;&#20013;&#32534;&#30721;&#20219;&#24847;&#30340;&#25513;&#30721;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30001;&#27880;&#24847;&#21147;&#22836;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#20132;&#26367;&#32452;&#25104;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21482;&#35201;MLP&#30340;&#28608;&#27963;&#20989;&#25968;&#26469;&#33258;&#38480;&#21046;&#31867;&#65288;&#21253;&#25324;SiLU&#21644;&#25509;&#36817;&#30340;ReLU&#21644;GeLU&#65289;&#65292;&#23601;&#21487;&#20197;&#36890;&#36807;&#24102;&#26377;&#20869;&#37096;&#32500;&#24230;&#20026;1&#30340;&#25513;&#27169;&#27880;&#24847;&#21147;&#22836;&#26469;&#23454;&#29616;MLP&#31070;&#32463;&#20803;&#12290;&#36825;&#26679;&#23601;&#21487;&#20197;&#23558;MLP&#21644;&#27880;&#24847;&#21147;Transformer&#36716;&#25442;&#20026;&#20165;&#27880;&#24847;&#21147;&#30340;Transformer&#65292;&#20294;&#20195;&#20215;&#26159;&#22823;&#22823;&#22686;&#21152;&#20102;&#27880;&#24847;&#21147;&#22836;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#22836;&#21487;&#20197;&#20998;&#21035;&#25191;&#34892;MLP&#30340;&#32452;&#25104;&#37096;&#20998;&#65288;&#32447;&#24615;&#21464;&#25442;&#21644;&#28608;&#27963;&#20989;&#25968;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#22836;&#21487;&#20197;&#22312;&#20854;&#26435;&#37325;&#30697;&#38453;&#20013;&#32534;&#30721;&#20219;&#24847;&#30340;&#25513;&#30721;&#27169;&#24335;&#65292;&#24182;&#19988;&#36825;&#20010;&#36817;&#20284;&#35823;&#24046;&#21487;&#20197;&#20219;&#24847;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture is widely used in machine learning models and consists of two alternating sublayers: attention heads and MLPs. We prove that an MLP neuron can be implemented by a masked attention head with internal dimension 1 so long as the MLP's activation function comes from a restricted class including SiLU and close approximations of ReLU and GeLU. This allows one to convert an MLP-and-attention transformer into an attention-only transformer at the cost of greatly increasing the number of attention heads. We also prove that attention heads can perform the components of an MLP (linear transformations and activation functions) separately. Finally, we prove that attention heads can encode arbitrary masking patterns in their weight matrices to within arbitrarily small error.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;</title><link>http://arxiv.org/abs/2309.08589</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#26159;&#19968;&#31181;&#31574;&#30053;&#25913;&#36827;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08589
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#20196;&#20154;&#36190;&#21497;&#30340;&#26032;&#33021;&#21147;&#20196;&#19990;&#30028;&#20026;&#20043;&#24778;&#21497;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30446;&#21069;&#32570;&#20047;&#33258;&#25105;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#25509;&#21463;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SECToR&#65288;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#23454;&#29616;&#33258;&#25105;&#25945;&#32946;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#12290;&#21463;&#21040;&#20197;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;Silver&#31561;&#20154;&#65292;2017&#65289;&#21644;&#20154;&#31867;&#35748;&#30693;&#65288;Kahneman&#65292;2011&#65289;&#20013;&#30340;&#30456;&#20851;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;SECToR&#39318;&#20808;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#36880;&#28176;&#24605;&#32771;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;SECToR&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#29983;&#25104;&#30456;&#21516;&#30340;&#31572;&#26696;&#65292;&#36825;&#27425;&#19981;&#20877;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#12290;&#36890;&#36807;SECToR&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#23398;&#20250;&#20102;&#36827;&#34892;&#22810;&#36798;29&#20301;&#25968;&#23383;&#30340;&#21152;&#27861;&#36816;&#31639;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#36229;&#36807;6&#20301;&#25968;&#23383;&#30340;&#22522;&#20934;&#30495;&#23454;&#31034;&#20363;&#65292;&#20165;&#36890;&#36807;&#21021;&#22987;&#30340;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Large language models have astounded the world with fascinating new capabilities. However, they currently lack the ability to teach themselves new skills, relying instead on being trained on large amounts of human-generated data. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a proof-of-concept demonstration that language models can successfully teach themselves new skills using chain-of-thought reasoning. Inspired by previous work in both reinforcement learning (Silver et al., 2017) and human cognition (Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly think its way through problems. SECToR then fine-tunes the model to generate those same answers, this time without using chain-of-thought reasoning. Language models trained via SECToR autonomously learn to add up to 29-digit numbers without any access to any ground truth examples beyond an initial supervised fine-tuning phase consisting only of numbers with 6 or fewer digits. Our central hypot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#30340;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#36890;&#36807;&#31526;&#21495;&#35745;&#21010;&#12289;&#35270;&#39057;&#25193;&#25955;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08587</link><description>&lt;p&gt;
&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Compositional Foundation Models for Hierarchical Planning. (arXiv:2309.08587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#30340;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#36890;&#36807;&#31526;&#21495;&#35745;&#21010;&#12289;&#35270;&#39057;&#25193;&#25955;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#38656;&#35201;&#36827;&#34892;&#36328;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#30340;&#23618;&#27425;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#20998;&#21035;&#23545;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20849;&#21516;&#35299;&#20915;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22312;&#29615;&#22659;&#20013;&#25166;&#26681;&#30340;&#31526;&#21495;&#35745;&#21010;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#29983;&#25104;&#30340;&#35270;&#39057;&#35745;&#21010;&#36890;&#36807;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#19982;&#35270;&#35273;-&#21160;&#20316;&#25511;&#21046;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#22312;&#27492;&#23618;&#27425;&#32467;&#26500;&#20013;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#24378;&#21046;&#20445;&#25345;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustr
&lt;/p&gt;</description></item><item><title>&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#65292;&#29992;ReLU&#26367;&#25442;softmax&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#22312;&#35745;&#31639;&#24615;&#33021;&#19978;&#25509;&#36817;&#25110;&#21305;&#37197;softmax&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#38500;&#27861;&#21487;&#20197;&#32531;&#35299;&#31934;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08586</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#29992;ReLU&#26367;&#25442;softmax
&lt;/p&gt;
&lt;p&gt;
Replacing softmax with ReLU in Vision Transformers. (arXiv:2309.08586v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08586
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#65292;&#29992;ReLU&#26367;&#25442;softmax&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#22312;&#35745;&#31639;&#24615;&#33021;&#19978;&#25509;&#36817;&#25110;&#21305;&#37197;softmax&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#38500;&#27861;&#21487;&#20197;&#32531;&#35299;&#31934;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#24403;&#23558;&#27880;&#24847;&#21147;softmax&#26367;&#25442;&#20026;ReLU&#36825;&#26679;&#30340;&#36880;&#28857;&#28608;&#27963;&#26102;&#65292;&#31934;&#24230;&#20250;&#19979;&#38477;&#12290;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#36890;&#36807;&#24207;&#21015;&#38271;&#24230;&#38500;&#20197;&#27880;&#24847;&#21147;&#19979;&#38477;&#34987;&#32531;&#35299;&#12290;&#25105;&#20204;&#22312;ImageNet-21k&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23567;&#22411;&#21040;&#22823;&#22411;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#26041;&#38754;&#65292;ReLU&#27880;&#24847;&#21147;&#21487;&#20197;&#36798;&#21040;&#25110;&#21305;&#37197;softmax&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research observed accuracy degradation when replacing the attention softmax with a point-wise activation such as ReLU. In the context of vision transformers, we find that this degradation is mitigated when dividing by sequence length. Our experiments training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#20581;&#30340;&#31163;&#32447;&#27169;&#22411;&#23548;&#21521;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#20027;&#35266;&#27169;&#22411;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#21442;&#25968;&#21270;&#19987;&#23478;&#23545;&#29615;&#22659;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20808;&#39564;&#22320;&#35748;&#20026;&#19987;&#23478;&#23545;&#29615;&#22659;&#20855;&#26377;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#26102;&#65292;&#20272;&#35745;&#30340;&#31574;&#30053;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;IRL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08571</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Approach to Robust Inverse Reinforcement Learning. (arXiv:2309.08571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08571
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#20581;&#30340;&#31163;&#32447;&#27169;&#22411;&#23548;&#21521;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#20027;&#35266;&#27169;&#22411;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#21442;&#25968;&#21270;&#19987;&#23478;&#23545;&#29615;&#22659;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20808;&#39564;&#22320;&#35748;&#20026;&#19987;&#23478;&#23545;&#29615;&#22659;&#20855;&#26377;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#26102;&#65292;&#20272;&#35745;&#30340;&#31574;&#30053;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;IRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#31163;&#32447;&#27169;&#22411;&#23548;&#21521;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;(IRL)&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#20027;&#35266;&#27169;&#22411;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#21306;&#21035;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#27169;&#22411;&#23548;&#21521;&#30340;IRL&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31867;&#20808;&#39564;&#20998;&#24067;&#26469;&#21442;&#25968;&#21270;&#19987;&#23478;&#23545;&#29615;&#22659;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#24320;&#21457;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#20272;&#35745;&#19987;&#23478;&#22870;&#21169;&#21644;&#20027;&#35266;&#21160;&#24577;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#24403;&#20808;&#39564;&#22320;&#35748;&#20026;&#19987;&#23478;&#23545;&#29615;&#22659;&#20855;&#26377;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#26102;&#65292;&#20272;&#35745;&#30340;&#31574;&#30053;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;MuJoCo&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;IRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert's reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert's model of the environment is to develop efficient algorithms to estimate the expert's reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#20114;&#24335;&#35774;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#22320;&#35774;&#35745;&#38750;&#32447;&#24615;&#20809;&#23398;&#20998;&#23376;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#20998;&#23376;&#30340;&#20809;&#23398;&#24615;&#36136;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.08570</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#30340;&#38750;&#32447;&#24615;&#20809;&#23398;&#20998;&#23376;&#30340;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Network Driven, Interactive Design for Nonlinear Optical Molecules Based on Group Contribution Method. (arXiv:2309.08570v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#20114;&#24335;&#35774;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#22320;&#35774;&#35745;&#38750;&#32447;&#24615;&#20809;&#23398;&#20998;&#23376;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#20998;&#23376;&#30340;&#20809;&#23398;&#24615;&#36136;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;Lewis&#27169;&#22411;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#65288;LGC&#65289;- &#22810;&#38454;&#27573;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;msBNN&#65289;- &#36827;&#21270;&#31639;&#27861;&#65288;EA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#29702;&#35774;&#35745;D-Pi-A&#22411;&#26377;&#26426;&#23567;&#20998;&#23376;&#38750;&#32447;&#24615;&#20809;&#23398;&#26448;&#26009;&#12290;&#36890;&#36807;&#32467;&#21512;msBNN&#21644;&#26657;&#27491;&#30340;Lewis&#27169;&#22411;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#65288;cLGC&#65289;&#65292;&#21487;&#20197;&#20934;&#30830;&#39640;&#25928;&#22320;&#33719;&#24471;&#20998;&#23376;&#30340;&#19981;&#21516;&#20809;&#23398;&#24615;&#36136; - &#20165;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#20026;LGC&#35774;&#35745;&#30340;EA&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#32467;&#26500;&#25628;&#32034;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#35813;&#26694;&#26550;&#34920;&#29616;&#33391;&#22909;&#30340;&#36923;&#36753;&#21407;&#22240;&#12290;&#32771;&#34385;&#21040;&#36825;&#31181;&#29702;&#35770;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#32467;&#21512;&#20102;&#21270;&#23398;&#21407;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#24037;&#20855;&#65292;&#24456;&#21487;&#33021;&#34987;&#35777;&#26126;&#22312;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#20013;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#30456;&#20851;&#38382;&#39064;&#26102;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Lewis-mode group contribution method (LGC) -- multi-stage Bayesian neural network (msBNN) -- evolutionary algorithm (EA) framework is reported for rational design of D-Pi-A type organic small-molecule nonlinear optical materials is presented. Upon combination of msBNN and corrected Lewis-mode group contribution method (cLGC), different optical properties of molecules are afforded accurately and efficiently - by using only a small data set for training. Moreover, by employing the EA model designed specifically for LGC, structural search is well achievable. The logical origins of the well performance of the framework are discussed in detail. Considering that such a theory guided, machine learning framework combines chemical principles and data-driven tools, most likely, it will be proven efficient to solve molecular design related problems in wider fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#33410;&#28857;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#22312;&#33410;&#28857;&#32423;&#21035;&#23545;&#29305;&#24449;&#21644;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21270;&#25200;&#21160;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#39057;&#29575;&#20272;&#35745;&#21644;&#37325;&#26500;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25200;&#21160;&#25968;&#25454;&#20013;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08569</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65306;&#19968;&#31181;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Differential Privacy in Graph Neural Networks: a Reconstruction Approach. (arXiv:2309.08569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#33410;&#28857;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#22312;&#33410;&#28857;&#32423;&#21035;&#23545;&#29305;&#24449;&#21644;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21270;&#25200;&#21160;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#39057;&#29575;&#20272;&#35745;&#21644;&#37325;&#26500;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25200;&#21160;&#25968;&#25454;&#20013;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23545;&#24314;&#27169;&#22797;&#26434;&#22270;&#25968;&#25454;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;GNN&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#20007;&#22833;&#22826;&#22810;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#20026;&#29992;&#25143;&#25552;&#20379;&#33410;&#28857;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#65292;&#21363;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#22312;&#25968;&#25454;&#34987;&#38598;&#20013;&#26381;&#21153;&#22120;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#65292;&#23545;&#33410;&#28857;&#32423;&#21035;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#25968;&#25454;&#24212;&#29992;&#38543;&#26426;&#21270;&#26426;&#21046;&#36827;&#34892;&#25200;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#29305;&#24449;&#35774;&#32622;&#20013;&#24212;&#29992;&#38543;&#26426;&#21270;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#20005;&#26684;&#38544;&#31169;&#20445;&#35777;&#30340;LDP&#21327;&#35758;&#12290;&#22522;&#20110;&#38543;&#26426;&#21270;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#20013;&#30340;&#39057;&#29575;&#20272;&#35745;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#37325;&#26500;&#26041;&#27861;&#26469;&#36817;&#20284;&#20174;&#25200;&#21160;&#25968;&#25454;&#20013;&#24674;&#22797;&#29305;&#24449;&#21644;&#26631;&#31614;&#12290;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#36825;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#22270;&#32858;&#31867;&#20013;&#30340;&#39057;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks have achieved tremendous success in modeling complex graph data in a variety of applications. However, there are limited studies investigating privacy protection in GNNs. In this work, we propose a learning framework that can provide node privacy at the user level, while incurring low utility loss. We focus on a decentralized notion of Differential Privacy, namely Local Differential Privacy, and apply randomization mechanisms to perturb both feature and label data at the node level before the data is collected by a central server for model training. Specifically, we investigate the application of randomization mechanisms in high-dimensional feature settings and propose an LDP protocol with strict privacy guarantees. Based on frequency estimation in statistical analysis of randomized data, we develop reconstruction methods to approximate features and labels from perturbed data. We also formulate this learning framework to utilize frequency estimates of graph cluste
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaKWS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#36755;&#20986;&#20851;&#38190;&#35789;&#26465;&#20214;&#30340;&#24402;&#19968;&#21270;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22788;&#29702;&#21548;&#35273;&#36755;&#20837;&#65292;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08561</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#20363;&#24402;&#19968;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary Keyword-spotting with Adaptive Instance Normalization. (arXiv:2309.08561v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaKWS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#36755;&#20986;&#20851;&#38190;&#35789;&#26465;&#20214;&#30340;&#24402;&#19968;&#21270;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22788;&#29702;&#21548;&#35273;&#36755;&#20837;&#65292;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#30340;&#20851;&#38190;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#37325;&#28857;&#26159;&#35782;&#21035;&#21475;&#35821;&#20013;&#29992;&#25143;&#23450;&#20041;&#30340;&#20851;&#38190;&#35789;&#12290;&#20851;&#38190;&#35789;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23558;&#38899;&#39057;&#21644;&#20851;&#38190;&#35789;&#26144;&#23556;&#21040;&#19968;&#20010;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20197;&#33719;&#24471;&#19968;&#20123;&#30456;&#20851;&#24615;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaKWS&#65292;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20851;&#38190;&#35789;&#26816;&#27979;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#36755;&#20986;&#20851;&#38190;&#35789;&#26465;&#20214;&#24402;&#19968;&#21270;&#21442;&#25968;&#12290;&#36825;&#20123;&#21442;&#25968;&#29992;&#20110;&#22788;&#29702;&#21548;&#35273;&#36755;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#21270;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#26368;&#36817;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#21644;ASR&#22522;&#20934;&#30456;&#27604;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open vocabulary keyword spotting is a crucial and challenging task in automatic speech recognition (ASR) that focuses on detecting user-defined keywords within a spoken utterance. Keyword spotting methods commonly map the audio utterance and keyword into a joint embedding space to obtain some affinity score. In this work, we propose AdaKWS, a novel method for keyword spotting in which a text encoder is trained to output keyword-conditioned normalization parameters. These parameters are used to process the auditory input. We provide an extensive evaluation using challenging and diverse multi-lingual benchmarks and show significant improvements over recent keyword spotting and ASR baselines. Furthermore, we study the effectiveness of our approach on low-resource languages that were unseen during the training. The results demonstrate a substantial performance improvement compared to baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#65292;&#26469;&#20248;&#21270;&#21307;&#30103;&#36164;&#28304;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08560</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#19988;&#20844;&#24179;&#20998;&#37197;&#21307;&#30103;&#36164;&#28304;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources. (arXiv:2309.08560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#65292;&#26469;&#20248;&#21270;&#21307;&#30103;&#36164;&#28304;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#36991;&#20813;&#30340;&#37197;&#32473;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#36890;&#27668;&#26426;&#30340;&#20379;&#24212;&#36890;&#24120;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#32039;&#24613;&#24773;&#20917;&#25110;&#36164;&#28304;&#26377;&#38480;&#30340;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#22914;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#12290;&#30446;&#21069;&#65292;&#38024;&#23545;&#21307;&#30103;&#36164;&#28304;&#20998;&#37197;&#30340;&#21327;&#35758;&#24182;&#27809;&#26377;&#26222;&#36941;&#25509;&#21463;&#30340;&#26631;&#20934;&#65292;&#23548;&#33268;&#21508;&#22269;&#25919;&#24220;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#21644;&#22522;&#20110;&#21551;&#21457;&#24335;&#21327;&#35758;&#26469;&#20248;&#20808;&#32771;&#34385;&#24739;&#32773;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#37325;&#30151;&#25252;&#29702;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#20197;&#20844;&#24179;&#26377;&#25928;&#22320;&#37197;&#32473;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#20010;&#20307;&#24739;&#32773;&#30340;&#30149;&#24773;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#25972;&#21512;&#21040;&#37325;&#30151;&#25252;&#29702;&#36164;&#28304;&#20998;&#37197;&#20013;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#36807;&#24230;&#37197;&#32473;&#36164;&#28304;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scarcity of health care resources could result in the unavoidable consequence of rationing. For example, ventilators are often limited in supply, especially during public health emergencies or in resource-constrained health care settings, such as amid the pandemic of COVID-19. Currently, there is no universally accepted standard for health care resource allocation protocols, resulting in different governments prioritizing patients based on various criteria and heuristic-based protocols. In this study, we investigate the use of reinforcement learning for critical care resource allocation policy optimization to fairly and effectively ration resources. We propose a transformer-based deep Q-network to integrate the disease progression of individual patients and the interaction effects among patients during the critical care resource allocation. We aim to improve both fairness of allocation and overall patient outcomes. Our experiments demonstrate that our method significantly reduces exces
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08549</link><description>&lt;p&gt;
&#22522;&#20110;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#30340;&#35757;&#32451;&#26469;&#25269;&#24481;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#26469;&#38450;&#27490;&#26469;&#33258;&#19981;&#21487;&#20449;&#25968;&#25454;&#28304;&#30340;&#28508;&#22312;&#27745;&#26579;&#25915;&#20987;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#36825;&#32473;&#20102;&#25915;&#20987;&#32773;&#35768;&#22810;&#21487;&#21033;&#29992;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#26377;&#25928;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#22914;&#20960;&#31181;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#37027;&#26679;&#21521;&#25152;&#26377;&#31034;&#20363;&#28155;&#21152;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#30340;&#23454;&#38469;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#26368;&#26032;&#25915;&#20987;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;H
&lt;/p&gt;
&lt;p&gt;
While numerous defense methods have been proposed to prohibit potential poisoning attacks from untrusted data sources, most research works only defend against specific attacks, which leaves many avenues for an adversary to exploit. In this work, we propose an efficient and robust training approach to defend against data poisoning attacks based on influence functions, named Healthy Influential-Noise based Training. Using influence functions, we craft healthy noise that helps to harden the classification model against poisoning attacks without significantly affecting the generalization ability on test data. In addition, our method can perform effectively when only a subset of the training data is modified, instead of the current method of adding noise to all examples that has been used in several previous works. We conduct comprehensive evaluations over two image datasets with state-of-the-art poisoning attacks under different realistic attack scenarios. Our empirical results show that H
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26377;&#25928;&#22320;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08546</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Continual Learning with Bayesian Adaptive Moment Regularization. (arXiv:2309.08546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26377;&#25928;&#22320;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36861;&#27714;&#38271;&#26399;&#33258;&#20027;&#24615;&#65292;&#26426;&#22120;&#20154;&#20195;&#29702;&#24517;&#39035;&#19981;&#26029;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#24182;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#25345;&#32493;&#23398;&#20064;&#35797;&#22270;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#21363;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#23548;&#33268;&#27169;&#22411;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#20808;&#39564;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#26426;&#22120;&#20154;&#24212;&#29992;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#31354;&#38388;&#25928;&#29575;&#19978;&#24456;&#39640;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#20250;&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#29702;&#24819;&#30340;&#29305;&#24615;&#65292;&#20294;&#22522;&#20110;&#20808;&#39564;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22833;&#36133;&#65292;&#22240;&#27492;&#19982;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#28508;&#22312;&#24212;&#29992;&#26041;&#38754;&#26377;&#38480;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#65288;BAdam&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20808;&#39564;&#30340;&#26041;&#27861;&#65292;&#23427;&#26356;&#22909;&#22320;&#32422;&#26463;&#21442;&#25968;&#22686;&#38271;&#65292;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#20855;&#26377;&#19968;&#31995;&#21015;&#29702;&#24819;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pursuit of long-term autonomy mandates that robotic agents must continuously adapt to their changing environments and learn to solve new tasks. Continual learning seeks to overcome the challenge of catastrophic forgetting, where learning to solve new tasks causes a model to forget previously learnt information. Prior-based continual learning methods are appealing for robotic applications as they are space efficient and typically do not increase in computational complexity as the number of tasks grows. Despite these desirable properties, prior-based approaches typically fail on important benchmarks and consequently are limited in their potential applications compared to their memory-based counterparts. We introduce Bayesian adaptive moment regularization (BAdam), a novel prior-based method that better constrains parameter growth, leading to lower catastrophic forgetting. Our method boasts a range of desirable properties for robotic applications such as being lightweight and task lab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#39640;&#25928;&#31283;&#20581;&#30340;&#20256;&#24863;&#22120;&#37096;&#32626;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#26469;&#35774;&#35745;&#26368;&#23567;&#20256;&#24863;&#22120;&#38598;&#21512;&#20197;&#23454;&#29616;&#22810;&#35206;&#30422;&#32422;&#26463;&#65292;&#24182;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#31639;&#27861;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.08545</link><description>&lt;p&gt;
&#22797;&#26434;&#29615;&#22659;&#20013;&#39640;&#25928;&#31283;&#20581;&#30340;&#20256;&#24863;&#22120;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Efficient and robust Sensor Placement in Complex Environments. (arXiv:2309.08545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#39640;&#25928;&#31283;&#20581;&#30340;&#20256;&#24863;&#22120;&#37096;&#32626;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#26469;&#35774;&#35745;&#26368;&#23567;&#20256;&#24863;&#22120;&#38598;&#21512;&#20197;&#23454;&#29616;&#22810;&#35206;&#30422;&#32422;&#26463;&#65292;&#24182;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#31639;&#27861;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#39640;&#25928;&#26080;&#38459;&#30340;&#30417;&#35270;&#25110;&#36890;&#20449;&#38382;&#39064;&#12290;&#19968;&#26041;&#38754;&#65292;&#20154;&#20204;&#24076;&#26395;&#20351;&#29992;&#26368;&#23569;&#25968;&#37327;&#30340;&#20256;&#24863;&#22120;&#35206;&#30422;&#29615;&#22659;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#20256;&#24863;&#22120;&#25925;&#38556;&#25110;&#23545;&#25239;&#25915;&#20987;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#35774;&#35745;&#33021;&#22815;&#23454;&#29616;&#22810;&#35206;&#30422;&#32422;&#26463;&#30340;&#26368;&#23567;&#20256;&#24863;&#22120;&#38598;&#21512;&#30340;&#25361;&#25112;&#65292;&#21363;&#29615;&#22659;&#20013;&#30340;&#27599;&#20010;&#28857;&#37117;&#34987;&#19968;&#23450;&#25968;&#37327;&#30340;&#20256;&#24863;&#22120;&#35206;&#30422;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#21152;&#36895;&#36138;&#23146;&#31639;&#27861;&#20013;&#25152;&#21046;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#35780;&#20272;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25581;&#31034;&#20102;&#25968;&#25454;&#30340;&#20960;&#20309;&#23646;&#24615;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#32456;&#38454;&#27573;&#12290;&#36890;&#36807;&#32771;&#34385;&#36825;&#20123;&#23646;&#24615;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#36138;&#23146;&#31639;&#27861;&#21644;&#949;-&#36138;&#23146;&#31639;&#27861;&#29983;&#25104;&#25968;&#25454;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficient and unobstructed surveillance or communication in complex environments. On one hand, one wishes to use a minimal number of sensors to cover the environment. On the other hand, it is often important to consider solutions that are robust against sensor failure or adversarial attacks. This paper addresses these challenges of designing minimal sensor sets that achieve multi-coverage constraints -- every point in the environment is covered by a prescribed number of sensors. We propose a greedy algorithm to achieve the objective. Further, we explore deep learning techniques to accelerate the evaluation of the objective function formulated in the greedy algorithm. The training of the neural network reveals that the geometric properties of the data significantly impact the network's performance, particularly at the end stage. By taking into account these properties, we discuss the differences in using greedy and $\epsilon$-greedy algorithms to generate data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#21644;&#21482;&#26377;&#23569;&#37327;&#31867;&#21035;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20248;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08534</link><description>&lt;p&gt;
&#26397;&#30528;&#20351;&#29992;&#26356;&#23569;&#26631;&#27880;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Last-layer Retraining for Group Robustness with Fewer Annotations. (arXiv:2309.08534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#21644;&#21482;&#26377;&#23569;&#37327;&#31867;&#21035;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20248;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#23569;&#25968;&#32676;&#20307;&#19978;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#29305;&#24449;&#20877;&#36171;&#26435;(DFR)&#25216;&#26415;&#36890;&#36807;&#31616;&#21333;&#30340;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32676;&#20307;&#20445;&#25252;&#24615;&#33021;&#65292;&#20294;&#23427;&#38656;&#35201;&#20445;&#30041;&#32676;&#20307;&#21644;&#31867;&#21035;&#30340;&#26631;&#27880;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#32676;&#20307;&#24179;&#34913;&#30340;&#20877;&#36171;&#26435;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#65288;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#65289;&#65292;&#21482;&#26377;&#23569;&#37327;&#30340;&#31867;&#21035;&#26631;&#27880;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#20986;&#20154;&#24847;&#26009;&#22320;&#26377;&#25928;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21363;&#20351;&#20877;&#36171;&#26435;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#19968;&#23567;&#37096;&#20998;&#26368;&#24046;&#32676;&#20307;&#25968;&#25454;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#36890;&#36807;&#20445;&#30041;&#19968;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#26469;&#37325;&#26032;&#35757;&#32451;&#26368;&#21518;&#19968;&#23618;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#39069;&#22806;&#25968;&#25454;&#25110;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;ERM&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization (ERM) of neural networks is prone to over-reliance on spurious correlations and poor generalization on minority groups. The recent deep feature reweighting (DFR) technique achieves state-of-the-art group robustness via simple last-layer retraining, but it requires held-out group and class annotations to construct a group-balanced reweighting dataset. In this work, we examine this impractical requirement and find that last-layer retraining can be surprisingly effective with no group annotations (other than for model selection) and only a handful of class annotations. We first show that last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data. This implies a "free lunch" where holding out a subset of training data to retrain the last layer can substantially outperform ERM on the entire dataset with no additional data or annotations. To further improve group robustness, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21442;&#25968;&#31232;&#30095;&#24615;&#23545;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#23610;&#24230;&#34892;&#20026;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26435;&#37325;&#31232;&#30095;&#24615;&#12289;&#38750;&#38646;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#20043;&#38388;&#30340;&#23610;&#24230;&#23450;&#24459;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#30830;&#23450;&#23545;&#20110;&#32473;&#23450;&#30340;&#26377;&#25928;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#65292;&#25152;&#38656;&#30340;&#26368;&#20339;&#31232;&#30095;&#27700;&#24179;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25299;&#23637;&#20102;&#23545;&#19981;&#21516;&#31232;&#30095;&#32467;&#26500;&#21644;&#31574;&#30053;&#30340;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#26435;&#37325;&#31232;&#30095;&#24615;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.08520</link><description>&lt;p&gt;
&#38024;&#23545;&#31232;&#30095;&#36830;&#25509;&#27169;&#22411;&#30340;&#23610;&#24230;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Sparsely-Connected Foundation Models. (arXiv:2309.08520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21442;&#25968;&#31232;&#30095;&#24615;&#23545;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#23610;&#24230;&#34892;&#20026;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26435;&#37325;&#31232;&#30095;&#24615;&#12289;&#38750;&#38646;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#20043;&#38388;&#30340;&#23610;&#24230;&#23450;&#24459;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#30830;&#23450;&#23545;&#20110;&#32473;&#23450;&#30340;&#26377;&#25928;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#65292;&#25152;&#38656;&#30340;&#26368;&#20339;&#31232;&#30095;&#27700;&#24179;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25299;&#23637;&#20102;&#23545;&#19981;&#21516;&#31232;&#30095;&#32467;&#26500;&#21644;&#31574;&#30053;&#30340;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#26435;&#37325;&#31232;&#30095;&#24615;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#31232;&#30095;&#24615;&#23545;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;Transformer&#65288;&#21363;&#8220;&#22522;&#30784;&#27169;&#22411;&#8221;&#65289;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#23610;&#24230;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#20010;&#35774;&#23450;&#19979;&#65292;&#25105;&#20204;&#39318;&#27425;&#30830;&#23450;&#20102;&#25551;&#36848;&#26435;&#37325;&#31232;&#30095;&#24615;&#12289;&#38750;&#38646;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#23610;&#24230;&#23450;&#24459;&#65292;&#24182;&#22312;ViT/JFT-4B&#21644;T5/C4&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#23450;&#24459;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#8220;&#26368;&#20339;&#31232;&#30095;&#24615;&#8221;&#65292;&#21363;&#23545;&#20110;&#32473;&#23450;&#30340;&#26377;&#25928;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#65292;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#30340;&#31232;&#30095;&#27700;&#24179;&#12290;&#23545;&#20110;&#22266;&#23450;&#25968;&#37327;&#30340;&#38750;&#38646;&#21442;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#31232;&#30095;&#24615;&#38543;&#30528;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#30740;&#31350;&#33539;&#22260;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31232;&#30095;&#32467;&#26500;&#65288;&#22914;&#30828;&#20214;&#21451;&#22909;&#30340;n:m&#27169;&#24335;&#65289;&#21644;&#31574;&#30053;&#65288;&#22914;&#20174;&#39044;&#35757;&#32451;&#31264;&#23494;&#27169;&#22411;&#24320;&#22987;&#65289;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;&#26435;&#37325;&#31232;&#30095;&#24615;&#22312;&#19981;&#21516;&#21442;&#25968;&#21644;&#26465;&#20214;&#19979;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e., "foundation models"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data, which we validate empirically across model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the "optimal sparsity", the sparsity level which yields the best performance for a given effective model size and training budget. For a fixed number of non-zero parameters, we identify that the optimal sparsity increases with the amount of data used for training. We also extend our study to different sparsity structures (such as the hardware-friendly n:m pattern) and strategies (such as starting from a pretrained dense model). Our findings shed light on the power and limitations of weight sparsity across various parameter and c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#65292;&#25506;&#32034;&#20102;&#23427;&#19982;&#32463;&#20856;&#22270;&#20687;&#28388;&#27874;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#19982;&#25193;&#25955;&#21644;&#28183;&#36879;&#28388;&#27874;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.08511</link><description>&lt;p&gt;
&#24191;&#20041;&#27010;&#29575;&#25193;&#25955;&#23610;&#24230;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Generalised Probabilistic Diffusion Scale-Spaces. (arXiv:2309.08511v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#65292;&#25506;&#32034;&#20102;&#23427;&#19982;&#32463;&#20856;&#22270;&#20687;&#28388;&#27874;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#19982;&#25193;&#25955;&#21644;&#28183;&#36879;&#28388;&#27874;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#22312;&#20174;&#23398;&#20064;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#26032;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#21021;&#30001;&#29289;&#29702;&#23398;&#20013;&#30340;&#28418;&#31227;&#25193;&#25955;&#27010;&#24565;&#25512;&#21160;&#65292;&#23427;&#20204;&#36890;&#36807;&#27491;&#21521;&#36807;&#31243;&#24212;&#29992;&#22270;&#20687;&#25200;&#21160;&#65292;&#22914;&#22122;&#22768;&#21644;&#27169;&#31946;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30456;&#24212;&#30340;&#23398;&#20064;&#36870;&#36807;&#31243;&#29983;&#25104;&#22270;&#20687;&#65292;&#24182;&#21487;&#22312;&#38468;&#21152;&#20449;&#24687;&#26465;&#20214;&#19979;&#36827;&#34892;&#35843;&#25972;&#65292;&#20174;&#32780;&#23548;&#33268;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#23454;&#36341;&#23548;&#21521;&#30340;&#25193;&#23637;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29702;&#35770;&#32972;&#26223;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#65292;&#23588;&#20854;&#26159;&#19982;&#28418;&#31227;&#25193;&#25955;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#38416;&#26126;&#19982;&#32463;&#20856;&#22270;&#20687;&#28388;&#27874;&#30340;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#25193;&#25955;&#21644;&#28183;&#36879;&#28388;&#27874;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic diffusion models excel at sampling new images from learned distributions. Originally motivated by drift-diffusion concepts from physics, they apply image perturbations such as noise and blur in a forward process that results in a tractable probability distribution. A corresponding learned reverse process generates images and can be conditioned on side information, which leads to a wide variety of practical applications. Most of the research focus currently lies on practice-oriented extensions. In contrast, the theoretical background remains largely unexplored, in particular the relations to drift-diffusion. In order to shed light on these connections to classical image filtering, we propose a generalised scale-space theory for probabilistic diffusion models. Moreover, we show conceptual and empirical connections to diffusion and osmosis filters.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#28014;&#28216;&#29983;&#24577;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#26041;&#26696;&#65292;&#33021;&#21152;&#24555;&#20998;&#26512;&#36895;&#24230;&#12289;&#20943;&#23569;&#23454;&#39564;&#20559;&#24046;&#65292;&#24182;&#20419;&#36827;&#28014;&#28216;&#29983;&#24577;&#30456;&#20851;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08500</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#28014;&#28216;&#29983;&#24577;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep-learning-powered data analysis in plankton ecology. (arXiv:2309.08500v1 [physics.bio-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08500
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#28014;&#28216;&#29983;&#24577;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#26041;&#26696;&#65292;&#33021;&#21152;&#24555;&#20998;&#26512;&#36895;&#24230;&#12289;&#20943;&#23569;&#23454;&#39564;&#20559;&#24046;&#65292;&#24182;&#20419;&#36827;&#28014;&#28216;&#29983;&#24577;&#30456;&#20851;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#26045;&#20026;&#28014;&#28216;&#29983;&#24577;&#23398;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#20316;&#20026;&#26367;&#20195;&#20256;&#32479;&#26041;&#27861;&#30340;&#19968;&#31181;&#26032;&#36884;&#24452;&#65292;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#26041;&#26696;&#26469;&#30740;&#31350;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#28014;&#28216;&#29983;&#29289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#27010;&#36848;&#65292;&#21253;&#25324;&#28014;&#28216;&#26893;&#29289;&#21644;&#28014;&#28216;&#21160;&#29289;&#22270;&#20687;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#12289;&#35269;&#39135;&#21644;&#28216;&#21160;&#34892;&#20026;&#20998;&#26512;&#65292;&#20197;&#21450;&#29983;&#24577;&#24314;&#27169;&#12290;&#28145;&#24230;&#23398;&#20064;&#26377;&#26395;&#21152;&#24555;&#20998;&#26512;&#36895;&#24230;&#65292;&#20943;&#23569;&#20154;&#20026;&#23454;&#39564;&#20559;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#30456;&#20851;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#23610;&#24230;&#19978;&#33719;&#21462;&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#20877;&#29616;&#24615;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22914;&#20309;&#28436;&#21464;&#20197;&#20943;&#23569;&#19981;&#20934;&#30830;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#28014;&#28216;&#29983;&#24577;&#30740;&#31350;&#20013;&#29305;&#21035;&#21487;&#33021;&#25512;&#21160;&#21457;&#23637;&#30340;&#26426;&#20250;&#12290;&#36825;&#20123;&#20363;&#23376;&#38468;&#24102;&#20102;&#35814;&#32454;&#30340;&#25945;&#31243;&#21644;&#20195;&#30721;&#31034;&#20363;&#65292;&#20351;&#35835;&#32773;&#33021;&#22815;&#24212;&#29992;&#25152;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The implementation of deep learning algorithms has brought new perspectives to plankton ecology. Emerging as an alternative approach to established methods, deep learning offers objective schemes to investigate plankton organisms in diverse environments. We provide an overview of deep-learning-based methods including detection and classification of phytoand zooplankton images, foraging and swimming behaviour analysis, and finally ecological modelling. Deep learning has the potential to speed up the analysis and reduce the human experimental bias, thus enabling data acquisition at relevant temporal and spatial scales with improved reproducibility. We also discuss shortcomings and show how deep learning architectures have evolved to mitigate imprecise readouts. Finally, we suggest opportunities where deep learning is particularly likely to catalyze plankton research. The examples are accompanied by detailed tutorials and code samples that allow readers to apply the methods described in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2309.08499</link><description>&lt;p&gt;
P-ROCKET: &#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;ROCKET&#21644;MINIROCKET&#22240;&#20854;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;ROCKET&#21644;MINIROCKET&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#38543;&#26426;&#19968;&#32500;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#25429;&#25417;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;S-ROCKET&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21098;&#26525;&#20887;&#20313;&#30340;&#21367;&#31215;&#26680;&#12290;&#28982;&#32780;&#65292;&#36827;&#21270;&#31639;&#27861;&#26412;&#36523;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;S-ROCKET&#20013;&#35780;&#20272;&#21367;&#31215;&#26680;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#30452;&#25509;&#35780;&#20272;&#20855;&#26377;&#38750;&#26174;&#33879;&#24046;&#24322;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;S-ROCKET&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#36830;&#25509;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WEEND&#30340;&#35789;&#32423;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#21457;&#35328;&#20154;&#20998;&#31163;&#65292;&#24182;&#22312;2&#20010;&#21457;&#35328;&#20154;&#30340;&#30701;&#29255;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08489</link><description>&lt;p&gt;
&#26397;&#21521;&#35789;&#32423;&#31471;&#21040;&#31471;&#31070;&#32463;&#21457;&#35328;&#20154;&#20998;&#31163;&#19982;&#36741;&#21161;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Word-Level End-to-End Neural Speaker Diarization with Auxiliary Network. (arXiv:2309.08489v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WEEND&#30340;&#35789;&#32423;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#21457;&#35328;&#20154;&#20998;&#31163;&#65292;&#24182;&#22312;2&#20010;&#21457;&#35328;&#20154;&#30340;&#30701;&#29255;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26631;&#20934;&#30340;&#21457;&#35328;&#20154;&#20998;&#31163;&#35797;&#22270;&#22238;&#31572;&#8220;&#35841;&#22312;&#20160;&#20040;&#26102;&#20505;&#35828;&#20102;&#20160;&#20040;&#8221;&#65292;&#20294;&#29616;&#23454;&#20013;&#22823;&#22810;&#25968;&#30456;&#20851;&#24212;&#29992;&#26356;&#20851;&#24515;&#30830;&#23450;&#8220;&#35841;&#35828;&#20102;&#20160;&#20040;&#8221;&#12290;&#26080;&#35770;&#26159;&#20256;&#32479;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#36824;&#26159;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20998;&#31163;&#65288;EEND&#65289;&#65292;&#37117;&#38656;&#35201;&#19968;&#20010;&#39069;&#22806;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21644;&#19968;&#20010;&#21327;&#35843;&#31639;&#27861;&#26469;&#23558;&#35828;&#35805;&#32773;&#26631;&#31614;&#19982;&#35782;&#21035;&#30340;&#21333;&#35789;&#20851;&#32852;&#36215;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36741;&#21161;&#32593;&#32476;&#30340;&#35789;&#32423;&#31471;&#21040;&#31471;&#31070;&#32463;&#20998;&#31163;&#65288;WEEND&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#22312;&#30456;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#20013;&#25191;&#34892;&#31471;&#21040;&#31471;ASR&#21644;&#21457;&#35328;&#20154;&#20998;&#31163;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#35821;&#38899;&#34987;&#35782;&#21035;&#26102;&#65292;&#21516;&#26102;&#20026;&#27599;&#20010;&#35782;&#21035;&#30340;&#21333;&#35789;&#39044;&#27979;&#35828;&#35805;&#32773;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WEEND&#22312;&#25152;&#26377;&#20004;&#20010;&#21457;&#35328;&#20154;&#30340;&#30701;&#29255;&#22330;&#26223;&#19978;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#24191;&#21040;5&#20998;&#38047;&#30340;&#38899;&#39057;&#38271;&#24230;&#12290;&#23613;&#31649;&#22312;3&#20010;&#25110;&#26356;&#22810;&#21457;&#35328;&#20154;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#65292;&#19981;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While standard speaker diarization attempts to answer the question "who spoken when", most of relevant applications in reality are more interested in determining "who spoken what". Whether it is the conventional modularized approach or the more recent end-to-end neural diarization (EEND), an additional automatic speech recognition (ASR) model and an orchestration algorithm are required to associate the speaker labels with recognized words. In this paper, we propose Word-level End-to-End Neural Diarization (WEEND) with auxiliary network, a multi-task learning algorithm that performs end-to-end ASR and speaker diarization in the same neural architecture. That is, while speech is being recognized, speaker labels are predicted simultaneously for each recognized word. Experimental results demonstrate that WEEND outperforms the turn-based diarization baseline system on all 2-speaker short-form scenarios and has the capability to generalize to audio lengths of 5 minutes. Although 3+speaker co
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#65288;AHT&#65289;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#35774;&#35745;&#19968;&#20010;&#31574;&#30053;&#26469;&#22312;&#26377;&#38480;&#36890;&#20449;&#36890;&#36947;&#19978;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#65292;&#23558;&#36125;&#21494;&#26031;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.08477</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#30340;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Multi-Agent Reinforcement Learning for Decentralized Active Hypothesis Testing. (arXiv:2309.08477v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08477
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#65288;AHT&#65289;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#35774;&#35745;&#19968;&#20010;&#31574;&#30053;&#26469;&#22312;&#26377;&#38480;&#36890;&#20449;&#36890;&#36947;&#19978;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#65292;&#23558;&#36125;&#21494;&#26031;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20998;&#24067;&#24335;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#65288;AHT&#65289;&#38382;&#39064;&#30340;&#19968;&#20010;&#20998;&#24067;&#24335;&#24418;&#24335;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#20174;&#29615;&#22659;&#20013;&#25910;&#38598;&#21040;&#24102;&#22122;&#22768;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#20986;&#27491;&#30830;&#30340;&#20551;&#35774;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#36873;&#25321;&#19968;&#20010;&#37319;&#26679;&#21160;&#20316;&#65292;&#36825;&#20123;&#19981;&#21516;&#30340;&#21160;&#20316;&#20250;&#23548;&#33268;&#20174;&#19981;&#21516;&#20998;&#24067;&#20013;&#25277;&#21462;&#35266;&#27979;&#25968;&#25454;&#65292;&#27599;&#20010;&#20998;&#24067;&#19982;&#19968;&#20010;&#29305;&#23450;&#30340;&#20551;&#35774;&#30456;&#20851;&#32852;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#22312;&#26377;&#38480;&#36895;&#29575;&#30340;&#36890;&#20449;&#36890;&#36947;&#19978;&#36827;&#34892;&#28040;&#24687;&#20132;&#25442;&#26469;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#12290;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#23558;&#36125;&#21494;&#26031;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#36825;&#31181;&#39118;&#38505;&#21253;&#25324;&#37319;&#26679;&#25104;&#26412;&#21644;&#26234;&#33021;&#20307;&#22312;&#22768;&#26126;&#20551;&#35774;&#26102;&#20135;&#29983;&#30340;&#32852;&#21512;&#32456;&#31471;&#25104;&#26412;&#12290;&#22312;AHT&#38382;&#39064;&#20013;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#32467;&#26500;&#21270;&#31574;&#30053;&#36890;&#24120;&#22312;&#25968;&#23398;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#21363;&#20351;&#26159;&#22312;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#32972;&#26223;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#36716;&#21521;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
We consider a decentralized formulation of the active hypothesis testing (AHT) problem, where multiple agents gather noisy observations from the environment with the purpose of identifying the correct hypothesis. At each time step, agents have the option to select a sampling action. These different actions result in observations drawn from various distributions, each associated with a specific hypothesis. The agents collaborate to accomplish the task, where message exchanges between agents are allowed over a rate-limited communications channel. The objective is to devise a multi-agent policy that minimizes the Bayes risk. This risk comprises both the cost of sampling and the joint terminal cost incurred by the agents upon making a hypothesis declaration. Deriving optimal structured policies for AHT problems is generally mathematically intractable, even in the context of a single agent. As a result, recent efforts have turned to deep learning methodologies to address these problems, whi
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19981;&#20855;&#22791;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#23427;&#20204;&#22312;&#39044;&#27979;&#25216;&#33021;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08473</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the limitations of data-driven weather forecasting models. (arXiv:2309.08473v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08473
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19981;&#20855;&#22791;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#23427;&#20204;&#22312;&#39044;&#27979;&#25216;&#33021;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#21457;&#23637;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#36890;&#24120;&#22768;&#31216;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#21069;&#19968;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;Pangu-Weather&#30340;&#39044;&#27979;&#26041;&#38754;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#19982;&#24863;&#30693;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20027;&#35201;&#32467;&#35770;&#26159;Pangu-Weather&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;&#31867;&#20284;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#20855;&#22791;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#32780;&#23427;&#20204;&#22312;&#20256;&#32479;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#25216;&#33021;&#25351;&#26631;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;&#19982;&#20854;&#20182;&#24403;&#21069;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
As in many other areas of engineering and applied science, Machine Learning (ML) is having a profound impact in the domain of Weather and Climate Prediction. A very recent development in this area has been the emergence of fully data-driven ML prediction models which routinely claim superior performance to that of traditional physics-based models. In this work, we examine some aspects of the forecasts produced by an exemplar of the current generation of ML models, Pangu-Weather, with a focus on the fidelity and physical consistency of those forecasts and how these characteristics relate to perceived forecast performance. The main conclusion is that Pangu-Weather forecasts, and by extension those of similar ML models, do not have the fidelity and physical consistency of physics-based models and their advantage in accuracy on traditional deterministic metrics of forecast skill can be attributed, to a large extent, to these peculiarities. Similarly to other current post-processing technol
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.08460</link><description>&lt;p&gt;
&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
Explaining Search Result Stances to Opinionated People. (arXiv:2309.08460v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#22312;&#24418;&#25104;&#35266;&#28857;&#20043;&#21069;&#20351;&#29992;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#25214;&#21040;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#27700;&#24179;&#30340;&#23454;&#38469;&#20915;&#31574;&#12290;&#25628;&#32034;&#30340;&#35748;&#30693;&#21162;&#21147;&#21487;&#33021;&#20351;&#26377;&#35266;&#28857;&#30340;&#29992;&#25143;&#23481;&#26131;&#21463;&#21040;&#35748;&#30693;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#30830;&#35748;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#31435;&#22330;&#26631;&#31614;&#21450;&#20854;&#35299;&#37322;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#25105;&#20204;&#33258;&#21160;&#23545;&#19977;&#20010;&#20027;&#39064;&#65288;&#30693;&#35782;&#20135;&#26435;&#12289;&#26657;&#26381;&#21644;&#26080;&#31070;&#35770;&#65289;&#30340;&#25628;&#32034;&#32467;&#26524;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#65292;&#20998;&#20026;&#21453;&#23545;&#12289;&#20013;&#31435;&#21644;&#25903;&#25345;&#65292;&#24182;&#20026;&#36825;&#20123;&#26631;&#31614;&#29983;&#25104;&#35299;&#37322;&#12290;&#22312;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#20013;&#65288;N =203&#65289;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#20559;&#35265;&#65288;&#24179;&#34913; vs &#20559;&#35265;&#65289;&#21644;&#35299;&#37322;&#27700;&#24179;&#65288;&#32431;&#25991;&#26412;&#12289;&#20165;&#26631;&#31614;&#12289;&#26631;&#31614;&#21644;&#35299;&#37322;&#65289;&#26159;&#21542;&#20250;&#24433;&#21709;&#34987;&#28857;&#20987;&#30340;&#25628;&#32034;&#32467;&#26524;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#23548;&#33268;&#26356;&#22810;&#26679;&#21270;&#30340;&#25628;&#32034;&#32467;&#26524;&#28040;&#36153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24182;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
People use web search engines to find information before forming opinions, which can lead to practical decisions with different levels of impact. The cognitive effort of search can leave opinionated users vulnerable to cognitive biases, e.g., the confirmation bias. In this paper, we investigate whether stance labels and their explanations can help users consume more diverse search results. We automatically classify and label search results on three topics (i.e., intellectual property rights, school uniforms, and atheism) as against, neutral, and in favor, and generate explanations for these labels. In a user study (N =203), we then investigate whether search result stance bias (balanced vs biased) and the level of explanation (plain text, label only, label and explanation) influence the diversity of search results clicked. We find that stance labels and explanations lead to a more diverse search result consumption. However, we do not find evidence for systematic opinion change among us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08454</link><description>&lt;p&gt;
&#28151;&#21512;&#32534;&#30721;&#22120;&#25903;&#25345;&#36830;&#32493;&#35821;&#38899;&#20998;&#31163;&#29992;&#20110;&#20250;&#35758;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Mixture Encoder Supporting Continuous Speech Separation for Meeting Recognition. (arXiv:2309.08454v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#22788;&#29702;&#37325;&#21472;&#30340;&#35821;&#38899;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#23558;&#35821;&#38899;&#20998;&#31163;&#25104;&#26080;&#37325;&#21472;&#30340;&#27969;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;ASR&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22312;ASR&#27169;&#22411;&#20013;&#21253;&#21547;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#28151;&#21512;&#32534;&#30721;&#22120;&#21033;&#29992;&#21407;&#22987;&#37325;&#21472;&#30340;&#35821;&#38899;&#26469;&#20943;&#36731;&#35821;&#38899;&#20998;&#31163;&#24341;&#20837;&#30340;&#20266;&#24433;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20165;&#38024;&#23545;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#31163;&#22120;&#65288;&#21253;&#25324;&#24378;&#22823;&#30340;TF-GridNet&#27169;&#22411;&#65289;&#35780;&#20272;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36824;&#23637;&#31034;&#20102;TF-GridNet&#30340;&#24378;&#22823;&#20998;&#31163;&#33021;&#21147;&#65292;&#22823;&#22823;&#32553;&#23567;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-life applications of automatic speech recognition (ASR) require processing of overlapped speech. A commonmethod involves first separating the speech into overlap-free streams and then performing ASR on the resulting signals. Recently, the inclusion of a mixture encoder in the ASR model has been proposed. This mixture encoder leverages the original overlapped speech to mitigate the effect of artifacts introduced by the speech separation. Previously, however, the method only addressed two-speaker scenarios. In this work, we extend this approach to more natural meeting contexts featuring an arbitrary number of speakers and dynamic overlaps. We evaluate the performance using different speech separators, including the powerful TF-GridNet model. Our experiments show state-of-the-art performance on the LibriCSS dataset and highlight the advantages of the mixture encoder. Furthermore, they demonstrate the strong separation of TF-GridNet which largely closes the gap between previous m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#37319;&#26679;&#20998;&#35299;&#28508;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#20154;&#21475;&#32676;&#20307;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#20013;&#25968;&#25454;&#38598;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#20851;&#27880;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08442</link><description>&lt;p&gt;
&#26397;&#30528;&#36127;&#36131;&#20219;&#30340;&#20154;&#33080;&#25968;&#25454;&#38598;&#65306;&#23545;&#20174;&#20154;&#21475;&#32676;&#20307;&#20013;&#37319;&#26679;&#20154;&#33080;&#22270;&#20687;&#30340;&#20998;&#35299;&#28508;&#31354;&#38388;&#20998;&#24067;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Toward responsible face datasets: modeling the distribution of a disentangled latent space for sampling face images from demographic groups. (arXiv:2309.08442v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#37319;&#26679;&#20998;&#35299;&#28508;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#20154;&#21475;&#32676;&#20307;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#20013;&#25968;&#25454;&#38598;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#20851;&#27880;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20123;&#29616;&#20195;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#34987;&#26333;&#20809;&#20986;&#21487;&#33021;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#36827;&#34892;&#27495;&#35270;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#23545;&#24615;&#21035;&#21644;&#20986;&#36523;&#31561;&#21508;&#31181;&#38754;&#37096;&#23646;&#24615;&#30340;&#19981;&#20844;&#24179;&#20851;&#27880;&#12290;&#21407;&#22240;&#22312;&#20110;&#34987;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#24179;&#34913;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#37319;&#38598;&#19968;&#20010;&#21508;&#20010;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#37117;&#24179;&#34913;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#26696;&#65292;&#21363;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#24179;&#34913;&#24615;&#21644;&#21487;&#33021;&#26080;&#20559;&#35265;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#12289;&#27491;&#21017;&#21270;&#25110;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#21644;&#37319;&#26679;&#19968;&#20010;StyleGAN&#28508;&#31354;&#38388;&#30340;&#20998;&#35299;&#25237;&#24433;&#65292;&#20197;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#20154;&#21475;&#32676;&#20307;&#65288;&#20363;&#22914; $hispanic-female$&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21512;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#20154;&#21475;&#32676;&#20307;&#65292;&#19988;&#36825;&#20123;&#36523;&#20221;&#19982;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#36523;&#20221;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been exposed that some modern facial recognition systems could discriminate specific demographic groups and may lead to unfair attention with respect to various facial attributes such as gender and origin. The main reason are the biases inside datasets, unbalanced demographics, used to train theses models. Unfortunately, collecting a large-scale balanced dataset with respect to various demographics is impracticable.  In this paper, we investigate as an alternative the generation of a balanced and possibly bias-free synthetic dataset that could be used to train, to regularize or to evaluate deep learning-based facial recognition models. We propose to use a simple method for modeling and sampling a disentangled projection of a StyleGAN latent space to generate any combination of demographic groups (e.g. $hispanic-female$). Our experiments show that we can synthesis any combination of demographic groups effectively and the identities are different from the original traini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#24418;&#24577;&#20449;&#24687;&#65292;&#23558;&#32454;&#32990;&#23646;&#24615;&#29702;&#35299;&#24471;&#26356;&#20840;&#38754;&#65292;&#30456;&#36739;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;98.3&#65285;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#24471;&#21040;&#35777;&#26126;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08421</link><description>&lt;p&gt;
MIML: &#36890;&#36807;&#24494;&#27969;&#25511;&#31995;&#32479;&#20869;&#30340;&#26426;&#26800;&#29305;&#24615;&#23545;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#36827;&#34892;&#22810;&#37325;&#22270;&#20687;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems. (arXiv:2309.08421v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#24418;&#24577;&#20449;&#24687;&#65292;&#23558;&#32454;&#32990;&#23646;&#24615;&#29702;&#35299;&#24471;&#26356;&#20840;&#38754;&#65292;&#30456;&#36739;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;98.3&#65285;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#24471;&#21040;&#35777;&#26126;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#35760;&#32454;&#32990;&#20998;&#31867;&#26377;&#21161;&#20110;&#20026;&#36827;&#19968;&#27493;&#20351;&#29992;&#25110;&#26816;&#26597;&#25552;&#20379;&#21407;&#22987;&#32454;&#32990;&#65292;&#28982;&#32780;&#29616;&#26377;&#25216;&#26415;&#22312;&#29305;&#24322;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#24448;&#24448;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#35813;&#26550;&#26500;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#27599;&#20010;&#32454;&#32990;&#22266;&#26377;&#30340;&#24191;&#38420;&#19988;&#24120;&#24120;&#34987;&#20302;&#20272;&#30340;&#24418;&#24577;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#32454;&#32990;&#23646;&#24615;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#21033;&#29992;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36890;&#24120;&#34987;&#20002;&#24323;&#30340;&#24418;&#24577;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#32454;&#32990;&#20998;&#31867;&#31934;&#24230;&#36798;&#21040;&#20102;&#24778;&#20154;&#30340;98.3&#65285;&#65292;&#22823;&#22823;&#20248;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;MIML&#24050;&#34987;&#35777;&#26126;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#26377;&#25928;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label-free cell classification is advantageous for supplying pristine cells for further use or examination, yet existing techniques frequently fall short in terms of specificity and speed. In this study, we address these limitations through the development of a novel machine learning framework, Multiplex Image Machine Learning (MIML). This architecture uniquely combines label-free cell images with biomechanical property data, harnessing the vast, often underutilized morphological information intrinsic to each cell. By integrating both types of data, our model offers a more holistic understanding of the cellular properties, utilizing morphological information typically discarded in traditional machine learning models. This approach has led to a remarkable 98.3\% accuracy in cell classification, a substantial improvement over models that only consider a single data type. MIML has been proven effective in classifying white blood cells and tumor cells, with potential for broader applicatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24314;&#27169;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#23545;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#33616;&#25910;&#38598;&#39069;&#22806;&#30340;SPECT MPI&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08415</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;CRT&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification. (arXiv:2309.08415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24314;&#27169;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#23545;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#33616;&#25910;&#38598;&#39069;&#22806;&#30340;SPECT MPI&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#24739;&#32773;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#65288;CRT&#65289;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#25512;&#33616;&#22312;&#22522;&#32447;&#20020;&#24202;&#21464;&#37327;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#30340;&#29305;&#24449;&#19981;&#36275;&#26102;&#25910;&#38598;&#39069;&#22806;&#30340;&#21333;&#20809;&#23376;&#21457;&#23556;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#24515;&#32908;&#28748;&#27880;&#26174;&#20687;&#65288;SPECT MPI&#65289;&#21464;&#37327;&#12290;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#32435;&#20837;&#20102;218&#21517;&#25509;&#21463;&#38745;&#24687;&#38376;&#25511;SPECT MPI&#30340;&#24739;&#32773;&#12290;CRT&#21453;&#24212;&#34987;&#23450;&#20041;&#20026;6&#20010;&#26376;&#38543;&#35775;&#26102;&#24038;&#23460;&#23556;&#34880;&#20998;&#25968;&#65288;LVEF&#65289;&#22686;&#21152;&gt; 5%&#12290;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#38598;&#25104;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#32467;&#26524;&#12290;CRT&#30340;&#21453;&#24212;&#29575;&#20026;55.5%&#65288;n = 121&#65289;&#65292;&#25972;&#20307;&#30007;&#24615;&#21344;61.0%&#65288;n = 133&#65289;&#65292;&#24179;&#22343;&#24180;&#40836;62.0&#23681;&#65292;LVEF&#20026;27.7&#12290;&#35813;&#22810;&#38454;&#27573;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#38598;&#25104;&#27169;&#22411;2&#65288;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;SPECT&#25968;&#25454;&#65289;&#30456;&#20284;&#65292;AUC&#20998;&#21035;&#20026;0.75&#21644;0.77&#65292;&#20934;&#30830;&#24615;&#20998;&#21035;&#20026;0.71&#21644;...
&lt;/p&gt;
&lt;p&gt;
Aims. The purpose of this study is to create a multi-stage machine learning model to predict cardiac resynchronization therapy (CRT) response for heart failure (HF) patients. This model exploits uncertainty quantification to recommend additional collection of single-photon emission computed tomography myocardial perfusion imaging (SPECT MPI) variables if baseline clinical variables and features from electrocardiogram (ECG) are not sufficient. Methods. 218 patients who underwent rest-gated SPECT MPI were enrolled in this study. CRT response was defined as an increase in left ventricular ejection fraction (LVEF) &gt; 5% at a 6 month follow-up. A multi-stage ML model was created by combining two ensemble models. Results. The response rate for CRT was 55.5% (n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, and LVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (which utilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71 vs
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#30340;&#27010;&#24565;&#23558;&#39034;&#24207;&#28145;&#23618;&#26550;&#26500;&#26367;&#25442;&#20026;&#24182;&#34892;&#27973;&#23618;&#26550;&#26500;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#25130;&#26029;&#39640;&#38454;&#39033;&#30340;&#26041;&#24335;&#26469;&#24471;&#21040;&#23485;&#24191;&#23618;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.08414</link><description>&lt;p&gt;
&#20877;&#27425;&#20351;&#28145;&#23618;&#32593;&#32476;&#21464;&#27973;
&lt;/p&gt;
&lt;p&gt;
Make Deep Networks Shallow Again. (arXiv:2309.08414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08414
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#30340;&#27010;&#24565;&#23558;&#39034;&#24207;&#28145;&#23618;&#26550;&#26500;&#26367;&#25442;&#20026;&#24182;&#34892;&#27973;&#23618;&#26550;&#26500;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#25130;&#26029;&#39640;&#38454;&#39033;&#30340;&#26041;&#24335;&#26469;&#24471;&#21040;&#23485;&#24191;&#23618;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#24212;&#29992;&#20013;&#26377;&#30528;&#33391;&#22909;&#30340;&#25104;&#21151;&#35760;&#24405;&#65292;&#22240;&#27492;&#34987;&#35270;&#20026;&#26368;&#20339;&#26550;&#26500;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#20197;&#26469;&#23427;&#20204;&#30340;&#20027;&#35201;&#32570;&#28857;&#19968;&#30452;&#26159;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#23548;&#33268;&#25968;&#20540;&#20248;&#21270;&#31639;&#27861;&#26080;&#27861;&#25910;&#25947;&#12290;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#30340;&#27010;&#24565;&#65292;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;--&#22312;&#24120;&#35268;&#23618;&#30340;&#26049;&#36793;&#26500;&#24314;&#20102;&#19968;&#20010;&#24658;&#31561;&#26144;&#23556;&#12290;&#36825;&#20010;&#27010;&#24565;&#36866;&#29992;&#20110;&#20855;&#26377;&#30456;&#21516;&#32500;&#24230;&#30340;&#23618;&#22534;&#21472;&#65292;&#24182;&#19988;&#22823;&#22823;&#20943;&#36731;&#20102;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#27531;&#24046;&#36830;&#25509;&#23618;&#22534;&#21472;&#21487;&#20197;&#34920;&#36798;&#20026;&#31867;&#20284;&#27888;&#21202;&#23637;&#24320;&#30340;&#39033;&#30340;&#25193;&#23637;&#12290;&#36825;&#31181;&#23637;&#24320;&#26041;&#24335;&#25552;&#20986;&#20102;&#25130;&#26029;&#39640;&#38454;&#39033;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#30001;&#25152;&#26377;&#21021;&#22987;&#22534;&#21472;&#23618;&#20197;&#24182;&#34892;&#26041;&#24335;&#32452;&#25104;&#30340;&#21333;&#20010;&#23485;&#24191;&#23618;&#30340;&#32467;&#26500;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23558;&#39034;&#24207;&#28145;&#23618;&#26550;&#26500;&#26367;&#25442;&#20026;&#24182;&#34892;&#27973;&#23618;&#26550;&#26500;&#12290;&#22312;&#36825;&#19968;&#29702;&#35770;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24615;&#33021;&#21487;&#33021;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have a good success record and are thus viewed as the best architecture choice for complex applications. Their main shortcoming has been, for a long time, the vanishing gradient which prevented the numerical optimization algorithms from acceptable convergence. A breakthrough has been achieved by the concept of residual connections -- an identity mapping parallel to a conventional layer. This concept is applicable to stacks of layers of the same dimension and substantially alleviates the vanishing gradient problem. A stack of residual connection layers can be expressed as an expansion of terms similar to the Taylor expansion. This expansion suggests the possibility of truncating the higher-order terms and receiving an architecture consisting of a single broad layer composed of all initially stacked layers in parallel. In other words, a sequential deep architecture is substituted by a parallel shallow one. Prompted by this theory, we investigated the performance capa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32422;&#26463;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#26696;COSMO&#65292;&#29992;&#20110;&#38750;&#29615;&#32467;&#26500;&#23398;&#20064;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#21487;&#24494;&#36817;&#20284;&#30340;&#26041;&#21521;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#20248;&#20808;&#21521;&#37327;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#26041;&#21521;&#30697;&#38453;&#21644;&#30456;&#24212;&#30340;&#38750;&#29615;&#37051;&#25509;&#30697;&#38453;&#65292;&#32780;&#26080;&#38656;&#22312;&#20219;&#20309;&#27493;&#39588;&#20013;&#35780;&#20272;&#38750;&#29615;&#24615;&#12290;&#23613;&#31649;&#27809;&#26377;&#26174;&#24335;&#32422;&#26463;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;COSMO&#22987;&#32456;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#29615;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#28176;&#36817;&#24555;&#36895;&#65292;&#32780;&#19988;&#27604;&#20854;&#20182;&#26377;&#32422;&#26463;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.08406</link><description>&lt;p&gt;
&#19981;&#21463;&#38480;&#30340;&#24179;&#28369;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constraint-Free Structure Learning with Smooth Acyclic Orientations. (arXiv:2309.08406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32422;&#26463;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#26696;COSMO&#65292;&#29992;&#20110;&#38750;&#29615;&#32467;&#26500;&#23398;&#20064;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#21487;&#24494;&#36817;&#20284;&#30340;&#26041;&#21521;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#20248;&#20808;&#21521;&#37327;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#26041;&#21521;&#30697;&#38453;&#21644;&#30456;&#24212;&#30340;&#38750;&#29615;&#37051;&#25509;&#30697;&#38453;&#65292;&#32780;&#26080;&#38656;&#22312;&#20219;&#20309;&#27493;&#39588;&#20013;&#35780;&#20272;&#38750;&#29615;&#24615;&#12290;&#23613;&#31649;&#27809;&#26377;&#26174;&#24335;&#32422;&#26463;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;COSMO&#22987;&#32456;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#29615;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#28176;&#36817;&#24555;&#36895;&#65292;&#32780;&#19988;&#27604;&#20854;&#20182;&#26377;&#32422;&#26463;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#26159;&#23558;&#30001;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#27491;&#30830;&#22320;&#37325;&#26500;&#20854;&#24359;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#24494;&#21270;&#26041;&#27861;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#30340;&#38750;&#29615;&#24615;&#36136;&#23545;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#32422;&#26463;&#25110;&#35268;&#33539;&#21270;&#12290;&#35780;&#20272;&#22270;&#30340;&#38750;&#29615;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#33410;&#28857;&#25968;&#37327;&#21576;&#19977;&#27425;&#26041;&#20851;&#31995;&#65292;&#20005;&#37325;&#24433;&#21709;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;COSMO&#65292;&#19968;&#31181;&#26080;&#32422;&#26463;&#36830;&#32493;&#20248;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#38750;&#29615;&#32467;&#26500;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21487;&#24494;&#36817;&#20284;&#30340;&#26041;&#21521;&#30697;&#38453;&#65292;&#20854;&#30001;&#19968;&#20010;&#20248;&#20808;&#21521;&#37327;&#21442;&#25968;&#21270;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21442;&#25968;&#21270;&#24471;&#21040;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#26041;&#21521;&#30697;&#38453;&#21644;&#30456;&#24212;&#30340;&#38750;&#29615;&#37051;&#25509;&#30697;&#38453;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#20219;&#20309;&#27493;&#39588;&#20013;&#35780;&#20272;&#38750;&#29615;&#24615;&#12290;&#23613;&#31649;&#27809;&#26377;&#26174;&#24335;&#32422;&#26463;&#65292;&#25105;&#20204;&#35777;&#26126;COSMO&#22987;&#32456;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#29615;&#35299;&#12290;&#38500;&#20102;&#28176;&#36817;&#24555;&#36895;&#22806;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#20998;&#26512;&#36824;&#34920;&#26126;COSMO&#19982;&#20854;&#20182;&#26377;&#32422;&#26463;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The structure learning problem consists of fitting data generated by a Directed Acyclic Graph (DAG) to correctly reconstruct its arcs. In this context, differentiable approaches constrain or regularize the optimization problem using a continuous relaxation of the acyclicity property. The computational cost of evaluating graph acyclicity is cubic on the number of nodes and significantly affects scalability. In this paper we introduce COSMO, a constraint-free continuous optimization scheme for acyclic structure learning. At the core of our method, we define a differentiable approximation of an orientation matrix parameterized by a single priority vector. Differently from previous work, our parameterization fits a smooth orientation matrix and the resulting acyclic adjacency matrix without evaluating acyclicity at any step. Despite the absence of explicit constraints, we prove that COSMO always converges to an acyclic solution. In addition to being asymptotically faster, our empirical ana
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36951;&#20256;&#31639;&#27861;&#19982;&#35789;&#20856;&#24335;&#35780;&#20272;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#30340;&#32452;&#21512;&#65292;&#20197;&#20811;&#26381;&#20197;&#24448;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35774;&#35745;&#31354;&#38388;&#19981;&#36275;&#21644;&#36866;&#24212;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27604;&#20197;&#24448;&#33539;&#22260;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08399</link><description>&lt;p&gt;
&#20248;&#21270;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#32452;&#21512;&#65306;&#19968;&#31181;&#35789;&#20856;&#36951;&#20256;&#31639;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Modular Robot Composition: A Lexicographic Genetic Algorithm Approach. (arXiv:2309.08399v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36951;&#20256;&#31639;&#27861;&#19982;&#35789;&#20856;&#24335;&#35780;&#20272;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#30340;&#32452;&#21512;&#65292;&#20197;&#20811;&#26381;&#20197;&#24448;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35774;&#35745;&#31354;&#38388;&#19981;&#36275;&#21644;&#36866;&#24212;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27604;&#20197;&#24448;&#33539;&#22260;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#26426;&#22120;&#20154;&#34987;&#35774;&#35745;&#20026;&#36890;&#29992;&#30828;&#20214;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#36866;&#24212;&#20219;&#21153;&#38656;&#27714;&#25110;&#29615;&#22659;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#32780;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#21017;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#65292;&#21363;&#26426;&#22120;&#20154;&#30340;&#24418;&#24335;&#21644;&#32467;&#26500;&#65292;&#23545;&#20027;&#35201;&#24615;&#33021;&#25351;&#26631;--&#37319;&#36141;&#25104;&#26412;&#12289;&#21608;&#26399;&#26102;&#38388;&#21644;&#33021;&#28304;&#25928;&#29575;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#27169;&#22359;&#32452;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22312;&#24320;&#21457;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#20013;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#26080;&#27861;&#20805;&#20998;&#25506;&#32034;&#35774;&#35745;&#31354;&#38388;&#65292;&#35201;&#20040;&#26080;&#27861;&#36866;&#24212;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36951;&#20256;&#31639;&#27861;&#19982;&#35789;&#20856;&#24335;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#30340;&#32452;&#21512;&#65292;&#20197;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#21487;&#33021;&#32452;&#21512;&#30340;&#25968;&#37327;&#19978;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#33539;&#22260;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial robots are designed as general-purpose hardware, which limits their ability to adapt to changing task requirements or environments. Modular robots, on the other hand, offer flexibility and can be easily customized to suit diverse needs. The morphology, i.e., the form and structure of a robot, significantly impacts the primary performance metrics acquisition cost, cycle time, and energy efficiency. However, identifying an optimal module composition for a specific task remains an open problem, presenting a substantial hurdle in developing task-tailored modular robots. Previous approaches either lack adequate exploration of the design space or the possibility to adapt to complex tasks. We propose combining a genetic algorithm with a lexicographic evaluation of solution candidates to overcome this problem and navigate search spaces exceeding those in prior work by magnitudes in the number of possible compositions. We demonstrate that our approach outperforms a state-of-the-art b
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20803;&#20449;&#24687;&#26469;&#25913;&#21892;&#22522;&#20110;&#38899;&#39057;&#30340;&#38646;&#26679;&#26412;&#40479;&#31867;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#21644;&#38899;&#39057;&#29305;&#24449;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08398</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#20803;&#20449;&#24687;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#38646;&#26679;&#26412;&#40479;&#31867;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring Meta Information for Audio-based Zero-shot Bird Classification. (arXiv:2309.08398v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20803;&#20449;&#24687;&#26469;&#25913;&#21892;&#22522;&#20110;&#38899;&#39057;&#30340;&#38646;&#26679;&#26412;&#40479;&#31867;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#21644;&#38899;&#39057;&#29305;&#24449;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#21160;&#22768;&#23398;&#30417;&#27979;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#24050;&#32463;&#20026;&#35745;&#31639;&#29983;&#29289;&#22768;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31232;&#26377;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#29289;&#31181;&#26469;&#35828;&#65292;&#25968;&#25454;&#31232;&#32570;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20016;&#23500;&#21644;&#22810;&#26679;&#30340;&#20803;&#25968;&#25454;&#65292;&#20197;&#40479;&#31867;&#29289;&#31181;&#20026;&#20363;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20803;&#20449;&#24687;&#26469;&#25913;&#21892;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#26469;&#28304;&#65306;&#36890;&#36807;(S)BERT&#32534;&#30721;&#30340;&#25991;&#26412;&#40479;&#40483;&#25551;&#36848;&#65292;&#21151;&#33021;&#29305;&#24615;(AVONET)&#21644;&#40479;&#31867;&#29983;&#27963;&#21490;(BLH)&#29305;&#24449;&#12290;&#20316;&#20026;&#38899;&#39057;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#21462;&#38899;&#39057;&#39057;&#35889;&#22270;&#21464;&#25442;&#22120;(AST)&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#21333;&#20010;&#32447;&#24615;&#23618;&#23558;&#20854;&#25237;&#24433;&#21040;&#36741;&#21161;&#20449;&#24687;&#30340;&#32500;&#24230;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#28857;&#31215;&#20316;&#20026;&#20860;&#23481;&#24615;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#25490;&#21517;&#38128;&#38142;&#25439;&#22833;&#30830;&#23450;&#27491;&#30830;&#30340;&#31867;&#21035;&#12290;&#36890;&#36807;&#36830;&#25509;AVONET&#21644;BLH&#29305;&#24449;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in passive acoustic monitoring and machine learning have led to the procurement of vast datasets for computational bioacoustic research. Nevertheless, data scarcity is still an issue for rare and underrepresented species. This study investigates how meta-information can improve zero-shot audio classification, utilising bird species as an example case study due to the availability of rich and diverse metadata. We investigate three different sources of metadata: textual bird sound descriptions encoded via (S)BERT, functional traits (AVONET), and bird life-history (BLH) characteristics. As audio features, we extract audio spectrogram transformer (AST) embeddings and project them to the dimension of the auxiliary information by adopting a single linear layer. Then, we employ the dot product as compatibility function and a standard zero-shot learning ranking hinge loss to determine the correct class. The best results are achieved by concatenating the AVONET and BLH features attaini
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#65288;LSX&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#32473;&#20104;&#35299;&#37322;&#21644;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#26469;&#25913;&#36827;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.08395</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Learning by Self-Explaining. (arXiv:2309.08395v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08395
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#65288;LSX&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#32473;&#20104;&#35299;&#37322;&#21644;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#26469;&#25913;&#36827;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#20174;&#29983;&#29289;&#23398;&#20013;&#23547;&#25214;&#28789;&#24863;&#65292;&#29305;&#21035;&#26159;&#20154;&#31867;&#26234;&#33021;&#12290;&#19982;&#30446;&#21069;&#20027;&#35201;&#23558;&#35299;&#37322;&#35270;&#20026;&#27169;&#22411;&#26816;&#26597;&#25163;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30456;&#27604;&#65292;&#20174;&#24515;&#29702;&#23398;&#20013;&#21457;&#29616;&#33258;&#25105;&#35299;&#37322;&#22312;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22909;&#22788;&#26377;&#20123;&#34987;&#24573;&#35270;&#20102;&#12290;&#21463;&#21040;&#36825;&#20010;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322; (LSX)&#12290;&#20854;&#20013;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#19968;&#20010;&#23398;&#20064;&#27169;&#22359; (&#23398;&#20064;&#32773;) &#25191;&#34892;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#27604;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#24182;&#23545;&#20854;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20869;&#37096;&#25209;&#35780;&#32773;&#27169;&#22359;&#22522;&#20110;&#21407;&#22987;&#20219;&#21153;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#24471;&#21040;&#25913;&#36827;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#37325;&#22797;&#36825;&#20010;&#24490;&#29615;&#12290;&#32972;&#21518;&#30340;&#30452;&#35273;&#26159;&#65292;&#22914;&#26524;&#25209;&#35780;&#32773;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#30340;&#35299;&#37322;&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#21017;&#35813;&#35299;&#37322;&#34987;&#35748;&#20026;&#26159;&#8220;&#22909;&#8221;&#30340;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#23454;&#29616;&#21487;&#33021;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#23454;&#26045;&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#30340;&#19968;&#33324;&#25351;&#23548;&#21407;&#21017;&#12290;&#26377;&#24453;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#26469;&#25506;&#32034;&#36825;&#19968;&#23398;&#20064;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) research has a long track record of drawing inspirations from findings from biology, in particular human intelligence. In contrast to current AI research that mainly treats explanations as a means for model inspection, a somewhat neglected finding from human psychology is the benefit of self-explaining in an agents' learning process. Motivated by this, we introduce a novel learning paradigm, termed Learning by Self-Explaining (LSX). The underlying idea is that a learning module (learner) performs a base task, e.g. image classification, and provides explanations to its decisions. An internal critic module next evaluates the quality of these explanations given the original task. Finally, the learner is refined with the critic's feedback and the loop is repeated as required. The intuition behind this is that an explanation is considered "good" if the critic can perform the same task given the respective explanation. Despite many implementation possibilities th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21407;&#35821;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#22810;&#23610;&#24230;&#26597;&#25214;&#34920;&#20316;&#20026;&#22270;&#24418;&#31649;&#32447;&#20013;&#20256;&#32479;&#35745;&#31639;&#21644;&#25968;&#25454;&#25805;&#20316;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#23427;&#22312;&#22810;&#20010;&#22270;&#24418;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08387</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#38388;&#25509;&#24341;&#29992;&#30340;&#39640;&#25928;&#22270;&#24418;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Efficient Graphics Representation with Differentiable Indirection. (arXiv:2309.08387v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21407;&#35821;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#22810;&#23610;&#24230;&#26597;&#25214;&#34920;&#20316;&#20026;&#22270;&#24418;&#31649;&#32447;&#20013;&#20256;&#32479;&#35745;&#31639;&#21644;&#25968;&#25454;&#25805;&#20316;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#23427;&#22312;&#22810;&#20010;&#22270;&#24418;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#24494;&#38388;&#25509;&#24341;&#29992;&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#21407;&#35821;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#22810;&#23610;&#24230;&#26597;&#25214;&#34920;&#20316;&#20026;&#22270;&#24418;&#31649;&#32447;&#20013;&#20256;&#32479;&#35745;&#31639;&#21644;&#25968;&#25454;&#25805;&#20316;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22270;&#24418;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#20960;&#20309;&#21644;&#22270;&#20687;&#34920;&#31034;&#12289;&#32441;&#29702;&#26144;&#23556;&#12289;&#30528;&#33394;&#21644;&#36752;&#23556;&#22330;&#34920;&#31034;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#21487;&#24494;&#38388;&#25509;&#24341;&#29992;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#26550;&#26500;&#20013;&#65292;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce differentiable indirection -- a novel learned primitive that employs differentiable multi-scale lookup tables as an effective substitute for traditional compute and data operations across the graphics pipeline. We demonstrate its flexibility on a number of graphics tasks, i.e., geometric and image representation, texture mapping, shading, and radiance field representation. In all cases, differentiable indirection seamlessly integrates into existing architectures, trains rapidly, and yields both versatile and efficient results.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#35270;&#35282;&#65292;&#23558;&#24352;&#37327;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#21644;&#36229;&#22270;&#20449;&#21495;&#21435;&#22122;&#65288;HyperGSD&#65289;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20102;&#24352;&#37327;&#36229;&#22270;&#36845;&#20195;&#32593;&#32476;&#65288;T-HGIN&#65289;&#26469;&#24212;&#29992;&#20110;&#20449;&#21495;&#21435;&#22122;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.08385</link><description>&lt;p&gt;
&#12298;&#24352;&#37327;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20449;&#21495;&#21435;&#22122;&#20043;&#38388;&#30340;&#32479;&#19968;&#35270;&#35282;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Unified View Between Tensor Hypergraph Neural Networks And Signal Denoising. (arXiv:2309.08385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08385
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#35270;&#35282;&#65292;&#23558;&#24352;&#37327;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#21644;&#36229;&#22270;&#20449;&#21495;&#21435;&#22122;&#65288;HyperGSD&#65289;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20102;&#24352;&#37327;&#36229;&#22270;&#36845;&#20195;&#32593;&#32476;&#65288;T-HGIN&#65289;&#26469;&#24212;&#29992;&#20110;&#20449;&#21495;&#21435;&#22122;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#21644;&#36229;&#22270;&#20449;&#21495;&#21435;&#22122;&#65288;HyperGSD&#65289;&#26159;&#39640;&#38454;&#32593;&#32476;&#24314;&#27169;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#20027;&#39064;&#12290;&#20102;&#35299;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#23545;&#20110;&#20174;HyperGSD&#30340;&#35282;&#24230;&#35774;&#35745;&#26032;&#30340;HyperGNNs&#20197;&#21450;&#21453;&#20043;&#20134;&#28982;&#38750;&#24120;&#26377;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#24352;&#37327;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;T-HGCN&#65289;&#24050;&#32463;&#25104;&#20026;&#22312;&#36229;&#22270;&#19978;&#20445;&#30041;&#39640;&#38454;&#20132;&#20114;&#30340;&#24378;&#22823;&#32467;&#26500;&#65292;&#24182;&#19988;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;HyperGSD&#38382;&#39064;&#19982;T-HGCN&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#12290;&#21463;&#21040;&#36825;&#19968;&#26377;&#36259;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;HyperGSD&#38382;&#39064;&#30340;&#24352;&#37327;&#36229;&#22270;&#36845;&#20195;&#32593;&#32476;&#65288;T-HGIN&#65289;&#65292;&#23427;&#22312;&#27599;&#20010;&#21333;&#23618;&#20013;&#21033;&#29992;&#20102;&#22810;&#27493;&#26356;&#26032;&#26041;&#26696;&#30340;&#20248;&#21183;&#12290;&#25968;&#20540;&#23454;&#39564;&#34987;&#36827;&#34892;&#20197;&#23637;&#31034;&#25152;&#25552;&#20986;&#30340;T-HGIN&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Neural networks (HyperGNNs) and hypergraph signal denoising (HyperGSD) are two fundamental topics in higher-order network modeling. Understanding the connection between these two domains is particularly useful for designing novel HyperGNNs from a HyperGSD perspective, and vice versa. In particular, the tensor-hypergraph convolutional network (T-HGCN) has emerged as a powerful architecture for preserving higher-order interactions on hypergraphs, and this work shows an equivalence relation between a HyperGSD problem and the T-HGCN. Inspired by this intriguing result, we further design a tensor-hypergraph iterative network (T-HGIN) based on the HyperGSD problem, which takes advantage of a multi-step updating scheme in every single layer. Numerical experiments are conducted to show the promising applications of the proposed T-HGIN approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08375</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20248;&#20808;&#32423;&#37325;&#26032;&#21152;&#26435;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Adaptive Priority Reweighing for Generalizing Fairness Improvement. (arXiv:2309.08375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#20851;&#38190;&#20915;&#31574;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#21628;&#22768;&#36234;&#26469;&#36234;&#22823;&#12290;&#23613;&#31649;&#24050;&#32463;&#36890;&#36807;&#23398;&#20064;&#20844;&#24179;&#32422;&#26463;&#26469;&#25913;&#21892;&#31639;&#27861;&#30340;&#20844;&#24179;&#24615;&#30340;&#21508;&#31181;&#26041;&#24335;&#65292;&#20294;&#23427;&#20204;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#38656;&#35201;&#19968;&#31181;&#24615;&#33021;&#26377;&#21069;&#26223;&#19988;&#20855;&#26377;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#20844;&#24179;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#20559;&#31227;&#23545;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#25552;&#35758;&#20026;&#27599;&#20010;&#65288;&#23376;&#65289;&#32452;&#20998;&#37197;&#19968;&#20010;&#32479;&#19968;&#30340;&#26435;&#37325;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32454;&#31890;&#24230;&#22320;&#24314;&#27169;&#20102;&#26679;&#26412;&#39044;&#27979;&#19982;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#65292;&#24182;&#20998;&#37197;&#36739;&#39640;&#30340;&#26435;&#37325;&#26469;&#25552;&#39640;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing penetration of machine learning applications in critical decision-making areas, calls for algorithmic fairness are more prominent. Although there have been various modalities to improve algorithmic fairness through learning with fairness constraints, their performance does not generalize well in the test set. A performance-promising fair algorithm with better generalizability is needed. This paper proposes a novel adaptive reweighing method to eliminate the impact of the distribution shifts between training and test data on model generalizability. Most previous reweighing methods propose to assign a unified weight for each (sub)group. Rather, our method granularly models the distance from the sample predictions to the decision boundary. Our adaptive reweighing method prioritizes samples closer to the decision boundary and assigns a higher weight to improve the generalizability of fair classifiers. Extensive experiments are performed to validate the generalizability 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08374</link><description>&lt;p&gt;
&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the limitations of self-supervised learning for tabular anomaly detection. (arXiv:2309.08374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25913;&#36827;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#26159;&#21542;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;26&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#28041;&#21450;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#65292;&#19982;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#34920;&#31034;&#30456;&#27604;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#34920;&#26684;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23376;&#31354;&#38388;&#21487;&#20197;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-supervised learning has improved anomaly detection in computer vision and natural language processing, it is unclear whether tabular data can benefit from it. This paper explores the limitations of self-supervision for tabular anomaly detection. We conduct several experiments spanning various pretext tasks on 26 benchmark datasets to understand why this is the case. Our results confirm representations derived from self-supervision do not improve tabular anomaly detection performance compared to using the raw representations of the data. We show this is due to neural networks introducing irrelevant features, which reduces the effectiveness of anomaly detectors. However, we demonstrate that using a subspace of the neural network's representation can recover performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#27969;&#27491;&#21017;&#21270;&#21028;&#21035;&#20998;&#26512;&#19979;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#20351;&#29992;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#25209;&#37327;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#27969;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08353</link><description>&lt;p&gt;
&#28145;&#24230;&#27969;&#27491;&#21017;&#21270;&#21028;&#21035;&#20998;&#26512;&#19979;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning with Deep Streaming Regularized Discriminant Analysis. (arXiv:2309.08353v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#27969;&#27491;&#21017;&#21270;&#21028;&#21035;&#20998;&#26512;&#19979;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#20351;&#29992;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#25209;&#37327;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#27969;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#36861;&#25447;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#20197;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#29992;&#38750;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#35206;&#30422;&#20102;&#29616;&#26377;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25209;&#37327;&#23398;&#20064;&#19978;&#65292;&#21363;&#25353;&#39034;&#24207;&#20174;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#25105;&#20204;&#24076;&#26395;&#30452;&#25509;&#38598;&#25104;&#26032;&#25968;&#25454;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#23601;&#38656;&#35201;&#23545;&#27969;&#24335;&#23398;&#20064;&#36827;&#34892;&#33539;&#24335;&#36716;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#24335;&#29256;&#26412;&#30340;&#27491;&#21017;&#21270;&#21028;&#21035;&#20998;&#26512;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#24182;&#35777;&#26126;&#22312;ImageNet&#19978;&#20248;&#20110;&#25209;&#37327;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#27969;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is increasingly sought after in real world machine learning applications, as it enables learning in a more human-like manner. Conventional machine learning approaches fail to achieve this, as incrementally updating the model with non-identically distributed data leads to catastrophic forgetting, where existing representations are overwritten. Although traditional continual learning methods have mostly focused on batch learning, which involves learning from large collections of labeled data sequentially, this approach is not well-suited for real-world applications where we would like new data to be integrated directly. This necessitates a paradigm shift towards streaming learning. In this paper, we propose a streaming version of regularized discriminant analysis as a solution to this challenge. We combine our algorithm with a convolutional neural network and demonstrate that it outperforms both batch learning and existing streaming learning algorithms on the ImageNet 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65292;&#32473;&#20986;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.08339</link><description>&lt;p&gt;
ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof. (arXiv:2309.08339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ADAM&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#25910;&#25947;&#24615;&#65292;&#32473;&#20986;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;RMSProp&#21644;ADAM&#20173;&#28982;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20851;&#38190;&#20043;&#19968;&#22312;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#27493;&#38271;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#22240;&#36873;&#25321;&#30340;&#27493;&#38271;&#32780;&#21464;&#21270;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#23545;ADAM&#30340;&#24658;&#23450;&#27493;&#38271;&#29256;&#26412;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27493;&#38271;&#36798;&#21040;&#20960;&#20046;&#32943;&#23450;&#28176;&#36817;&#25910;&#25947;&#21040;&#38646;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#32780;&#21482;&#38656;&#26368;&#23567;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#30830;&#23450;&#24615;ADAM&#22312;&#22788;&#29702;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#26102;&#36798;&#21040;&#36817;&#20284;&#20020;&#30028;&#24615;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In neural network training, RMSProp and ADAM remain widely favoured optimization algorithms. One of the keys to their performance lies in selecting the correct step size, which can significantly influence their effectiveness. It is worth noting that these algorithms performance can vary considerably, depending on the chosen step sizes. Additionally, questions about their theoretical convergence properties continue to be a subject of interest. In this paper, we theoretically analyze a constant stepsize version of ADAM in the non-convex setting. We show sufficient conditions for the stepsize to achieve almost sure asymptotic convergence of the gradients to zero with minimal assumptions. We also provide runtime bounds for deterministic ADAM to reach approximate criticality when working with smooth, non-convex functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#35841;&#20250;&#25442;&#24037;&#20316;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#20351;&#29992;&#22810;&#31181;ML&#31639;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#12290;&#35780;&#20272;&#27169;&#22411;&#26102;&#20351;&#29992;&#20102;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1-Score&#21644;&#20934;&#30830;&#29575;&#31561;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.08333</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#39044;&#27979;&#35841;&#20250;&#25442;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
Let's Predict Who Will Move to a New Job. (arXiv:2309.08333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#35841;&#20250;&#25442;&#24037;&#20316;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#20351;&#29992;&#22810;&#31181;ML&#31639;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#12290;&#35780;&#20272;&#27169;&#22411;&#26102;&#20351;&#29992;&#20102;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1-Score&#21644;&#20934;&#30830;&#29575;&#31561;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#19968;&#23478;&#20844;&#21496;&#30340;&#20154;&#21147;&#36164;&#28304;&#37096;&#38376;&#37117;&#38754;&#20020;&#30528;&#39044;&#27979;&#30003;&#35831;&#20154;&#26159;&#21542;&#20250;&#23547;&#25214;&#26032;&#24037;&#20316;&#25110;&#32773;&#30041;&#22312;&#20844;&#21496;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#39044;&#27979;&#35841;&#20250;&#25442;&#24037;&#20316;&#12290;&#39318;&#20808;&#65292;&#23558;&#25968;&#25454;&#39044;&#22788;&#29702;&#25104;&#36866;&#21512;ML&#27169;&#22411;&#30340;&#26684;&#24335;&#12290;&#20026;&#20102;&#22788;&#29702;&#20998;&#31867;&#29305;&#24449;&#65292;&#24212;&#29992;&#25968;&#25454;&#32534;&#30721;&#24182;&#25191;&#34892;&#20960;&#31181;ML&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#12289;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#12290;&#20026;&#20102;&#25552;&#39640;ML&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#36827;&#34892;&#20445;&#30041;&#12290;&#20351;&#29992;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1-Score&#21644;&#20934;&#30830;&#29575;&#31561;&#20915;&#31574;&#25903;&#25345;&#24230;&#37327;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any company's human resources department faces the challenge of predicting whether an applicant will search for a new job or stay with the company. In this paper, we discuss how machine learning (ML) is used to predict who will move to a new job. First, the data is pre-processed into a suitable format for ML models. To deal with categorical features, data encoding is applied and several MLA (ML Algorithms) are performed including Random Forest (RF), Logistic Regression (LR), Decision Tree (DT), and eXtreme Gradient Boosting (XGBoost). To improve the performance of ML models, the synthetic minority oversampling technique (SMOTE) is used to retain them. Models are assessed using decision support metrics such as precision, recall, F1-Score, and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#35299;&#20915;&#36830;&#32493;&#24773;&#20917;&#19979;&#21453;&#20107;&#23454;&#24178;&#39044;&#20272;&#35745;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25512;&#23548;&#36125;&#21494;&#26031;&#21464;&#24418;&#39640;&#26031;&#36807;&#31243;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#39640;&#26031;&#20998;&#24067;&#21644;&#38750;&#21487;&#21152;&#24773;&#20917;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2309.08332</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#21453;&#20107;&#23454;&#24178;&#39044;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of Counterfactual Interventions under Uncertainties. (arXiv:2309.08332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#35299;&#20915;&#36830;&#32493;&#24773;&#20917;&#19979;&#21453;&#20107;&#23454;&#24178;&#39044;&#20272;&#35745;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25512;&#23548;&#36125;&#21494;&#26031;&#21464;&#24418;&#39640;&#26031;&#36807;&#31243;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#39640;&#26031;&#20998;&#24067;&#21644;&#38750;&#21487;&#21152;&#24773;&#20917;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#20998;&#26512;&#26159;&#20154;&#31867;&#27599;&#22825;&#30452;&#35266;&#36827;&#34892;&#30340;&#27963;&#21160;&#65292;&#20363;&#22914;&#8220;&#25105;&#24212;&#35813;&#24590;&#20040;&#20570;&#25165;&#33021;&#20351;&#36151;&#27454;&#33719;&#24471;&#25209;&#20934;&#65311;&#8221;&#36825;&#26679;&#30340;&#21453;&#20107;&#23454;&#38382;&#39064;&#20063;&#24341;&#23548;&#20102;&#31185;&#23398;&#20551;&#35774;&#30340;&#21046;&#23450;&#12290;&#26356;&#27491;&#24335;&#22320;&#35828;&#65292;&#36890;&#36807;&#25512;&#26029;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#36807;&#21435;&#35266;&#23519;&#30340;&#20551;&#35774;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#31995;&#32479;&#28508;&#22312;&#25913;&#36827;&#30340;&#35265;&#35299;&#65292;&#36825;&#22312;&#21508;&#31181;&#24037;&#19994;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30001;&#20110;&#36825;&#31181;&#20998;&#26512;&#30340;&#20551;&#35774;&#24615;&#36136;&#65292;&#21453;&#20107;&#23454;&#20998;&#24067;&#26412;&#36136;&#19978;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#12290;&#36825;&#31181;&#27169;&#26865;&#20004;&#21487;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23545;&#20110;&#30456;&#21516;&#35266;&#23519;&#23384;&#22312;&#35768;&#22810;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#23618;&#27425;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#36125;&#21494;&#26031;&#21464;&#24418;&#39640;&#26031;&#36807;&#31243;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#20174;&#32780;&#20801;&#35768;&#38750;&#39640;&#26031;&#20998;&#24067;&#21644;&#38750;&#21487;&#21152;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual analysis is intuitively performed by humans on a daily basis eg. "What should I have done differently to get the loan approved?". Such counterfactual questions also steer the formulation of scientific hypotheses. More formally it provides insights about potential improvements of a system by inferring the effects of hypothetical interventions into a past observation of the system's behaviour which plays a prominent role in a variety of industrial applications. Due to the hypothetical nature of such analysis, counterfactual distributions are inherently ambiguous. This ambiguity is particularly challenging in continuous settings in which a continuum of explanations exist for the same observation. In this paper, we address this problem by following a hierarchical Bayesian approach which explicitly models such uncertainty. In particular, we derive counterfactual distributions for a Bayesian Warped Gaussian Process thereby allowing for non-Gaussian distributions and non-additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#22914;&#20309;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20197;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#24322;&#26041;&#24046;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.08313</link><description>&lt;p&gt;
&#24322;&#26041;&#24046;&#25311;&#21512;&#32622;&#20449;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Heteroskedastic conformal regression. (arXiv:2309.08313v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#22914;&#20309;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20197;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#24322;&#26041;&#24046;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#20197;&#21450;&#29305;&#23450;&#30340;&#25286;&#20998;&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#19987;&#27880;&#20110;&#36793;&#38469;&#35206;&#30422;&#26102;&#65292;&#21363;&#22312;&#26657;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#39044;&#27979;&#21306;&#38388;&#24179;&#22343;&#21253;&#21547;&#39044;&#23450;&#20041;&#35206;&#30422;&#27700;&#24179;&#30340;&#30495;&#23454;&#20540;&#65292;&#25286;&#20998;&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#21487;&#20197;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21306;&#38388;&#36890;&#24120;&#19981;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#22238;&#24402;&#38382;&#39064;&#21487;&#33021;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#26412;&#25991;&#35797;&#22270;&#38416;&#26126;&#22914;&#20309;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#25552;&#20986;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction, and split conformal prediction as a specific implementation, offer a distribution-free approach to estimating prediction intervals with statistical guarantees. Recent work has shown that split conformal prediction can produce state-of-the-art prediction intervals when focusing on marginal coverage, i.e., on a calibration dataset the method produces on average prediction intervals that contain the ground truth with a predefined coverage level. However, such intervals are often not adaptive, which can be problematic for regression problems with heteroskedastic noise. This paper tries to shed new light on how adaptive prediction intervals can be constructed using methods such as normalized and Mondrian conformal prediction. We present theoretical and experimental results in which these methods are investigated in a systematic way.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#26102;&#27963;&#21160;&#35828;&#35805;&#20154;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#38899;&#39057;-&#35270;&#35273;&#20449;&#21495;&#19982;&#31354;&#38388;&#26597;&#35810;&#26426;&#21046;&#25972;&#21512;&#65292;&#21033;&#29992;&#20302;&#21151;&#32791;&#36793;&#32536;&#35745;&#31639;&#23454;&#29616;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#20248;&#38597;&#30340;&#36864;&#21270;&#24615;&#33021;&#65292;&#33021;&#22815;&#22312;&#35745;&#31639;&#39044;&#31639;&#32791;&#23613;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#36816;&#34892;&#65292;&#24182;&#22312;&#30495;&#23454;&#20250;&#35758;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08295</link><description>&lt;p&gt;
&#19968;&#20010;&#23454;&#26102;&#27963;&#21160;&#35828;&#35805;&#20154;&#26816;&#27979;&#31995;&#32479;&#65292;&#23558;&#38899;&#39057;-&#35270;&#35273;&#20449;&#21495;&#19982;&#31354;&#38388;&#26597;&#35810;&#26426;&#21046;&#25972;&#21512;&#22312;&#19968;&#36215;
&lt;/p&gt;
&lt;p&gt;
A Real-Time Active Speaker Detection System Integrating an Audio-Visual Signal with a Spatial Querying Mechanism. (arXiv:2309.08295v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#26102;&#27963;&#21160;&#35828;&#35805;&#20154;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#38899;&#39057;-&#35270;&#35273;&#20449;&#21495;&#19982;&#31354;&#38388;&#26597;&#35810;&#26426;&#21046;&#25972;&#21512;&#65292;&#21033;&#29992;&#20302;&#21151;&#32791;&#36793;&#32536;&#35745;&#31639;&#23454;&#29616;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#20248;&#38597;&#30340;&#36864;&#21270;&#24615;&#33021;&#65292;&#33021;&#22815;&#22312;&#35745;&#31639;&#39044;&#31639;&#32791;&#23613;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#36816;&#34892;&#65292;&#24182;&#22312;&#30495;&#23454;&#20250;&#35758;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#23454;&#26102;&#12289;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27963;&#21160;&#35828;&#35805;&#20154;&#26816;&#27979;&#31995;&#32479;&#65292;&#32463;&#36807;&#20302;&#21151;&#32791;&#36793;&#32536;&#35745;&#31639;&#20248;&#21270;&#12290;&#35813;&#31995;&#32479;&#39537;&#21160;&#19968;&#20010;&#34394;&#25311;&#24433;&#35270;&#27169;&#22359;&#65292;&#24182;&#19988;&#37096;&#32626;&#22312;&#21830;&#19994;&#35774;&#22791;&#19978;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#26469;&#33258;&#40614;&#20811;&#39118;&#38453;&#21015;&#21644;360&#24230;&#30456;&#26426;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#27599;&#20010;&#21442;&#19982;&#32773;&#21482;&#38656;&#35201;127MFLOPs&#65292;&#23545;&#20110;&#19968;&#20010;&#26377;14&#20010;&#21442;&#19982;&#32773;&#30340;&#20250;&#35758;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#24403;&#35745;&#31639;&#39044;&#31639;&#32791;&#23613;&#26102;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#25105;&#20204;&#30340;&#32593;&#32476;&#30340;&#38169;&#35823;&#29575;&#65292;&#24182;&#21457;&#29616;&#23427;&#34920;&#29616;&#20986;&#20102;&#20248;&#38597;&#30340;&#36864;&#21270;&#65292;&#21363;&#20351;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31995;&#32479;&#20173;&#28982;&#33021;&#22815;&#36816;&#34892;&#24471;&#30456;&#24403;&#22909;&#12290;&#19982;&#20256;&#32479;&#30340;&#26041;&#21521;&#20272;&#35745;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#23398;&#20064;&#26597;&#35810;&#21487;&#29992;&#30340;&#22768;&#23398;&#25968;&#25454;&#65292;&#24182;&#32771;&#34385;&#21040;&#26816;&#27979;&#21040;&#30340;&#22836;&#37096;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;&#26368;&#22810;14&#20010;&#21442;&#19982;&#32773;&#12289;&#37325;&#21472;&#30340;&#35821;&#38899;&#21644;&#20854;&#20182;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#30495;&#23454;&#20250;&#35758;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a distinctive real-time, causal, neural network-based active speaker detection system optimized for low-power edge computing. This system drives a virtual cinematography module and is deployed on a commercial device. The system uses data originating from a microphone array and a 360-degree camera. Our network requires only 127 MFLOPs per participant, for a meeting with 14 participants. Unlike previous work, we examine the error rate of our network when the computational budget is exhausted, and find that it exhibits graceful degradation, allowing the system to operate reasonably well even in this case. Departing from conventional DOA estimation approaches, our network learns to query the available acoustic data, considering the detected head locations. We train and evaluate our algorithm on a realistic meetings dataset featuring up to 14 participants in the same meeting, overlapped speech, and other challenging scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08289</link><description>&lt;p&gt;
&#21033;&#29992;&#28857;&#25193;&#25955;&#27169;&#22411;&#23545;&#22823;&#32928;&#30340;3D&#24418;&#29366;&#36827;&#34892;&#31934;&#21270;&#20197;&#29983;&#25104;&#25968;&#23383;&#24187;&#24433;
&lt;/p&gt;
&lt;p&gt;
Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation. (arXiv:2309.08289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24314;&#27169;&#20154;&#20307;&#22120;&#23448;&#22312;&#26500;&#24314;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#30340;&#35745;&#31639;&#20223;&#30495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#29983;&#25104;&#35299;&#21078;&#23398;&#19978;&#21487;&#20449;&#30340;&#22120;&#23448;&#34920;&#38754;&#37325;&#24314;&#20173;&#28982;&#23545;&#20154;&#20307;&#32467;&#26500;&#20013;&#30340;&#35768;&#22810;&#22120;&#23448;&#26469;&#35828;&#26159;&#20010;&#25361;&#25112;&#12290;&#22312;&#22788;&#29702;&#22823;&#32928;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#20248;&#21270;&#22823;&#32928;&#20998;&#21106;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22120;&#23448;&#34920;&#31034;&#20026;&#20174;3D&#20998;&#21106;&#25513;&#27169;&#34920;&#38754;&#37319;&#26679;&#24471;&#21040;&#30340;&#28857;&#20113;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#22120;&#23448;&#24418;&#29366;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#20004;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#24418;&#29366;&#31934;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#24418;&#29366;&#30340;&#26356;&#22909;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D modeling of human organs plays a crucial role in building computational phantoms for virtual imaging trials. However, generating anatomically plausible reconstructions of organ surfaces from computed tomography scans remains challenging for many structures in the human body. This challenge is particularly evident when dealing with the large intestine. In this study, we leverage recent advancements in geometric deep learning and denoising diffusion probabilistic models to refine the segmentation results of the large intestine. We begin by representing the organ as point clouds sampled from the surface of the 3D segmentation mask. Subsequently, we employ a hierarchical variational autoencoder to obtain global and local latent representations of the organ's shape. We train two conditional denoising diffusion models in the hierarchical latent space to perform shape refinement. To further enhance our method, we incorporate a state-of-the-art surface reconstruction model, allowin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37319;&#26679;&#30340;&#27010;&#29575;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31532;&#19968;&#20010;&#30830;&#23450;&#24615;&#25512;&#26029;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2309.08256</link><description>&lt;p&gt;
&#26080;&#38656;&#37319;&#26679;&#30340;&#27010;&#29575;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sampling-Free Probabilistic Deep State-Space Models. (arXiv:2309.08256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37319;&#26679;&#30340;&#27010;&#29575;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31532;&#19968;&#20010;&#30830;&#23450;&#24615;&#25512;&#26029;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21160;&#24577;&#31995;&#32479;&#21487;&#20197;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#26469;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#34920;&#36848;&#20013;&#65292;&#27599;&#20010;&#35266;&#23519;&#20540;&#37117;&#30001;&#19968;&#20010;&#28508;&#22312;&#29366;&#24577;&#21457;&#23556;&#65292;&#35813;&#29366;&#24577;&#36981;&#24490;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#12290;&#27010;&#29575;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;ProDSSM&#65289;&#23558;&#36825;&#19968;&#26694;&#26550;&#25512;&#24191;&#21040;&#26410;&#30693;&#21442;&#25968;&#24418;&#24335;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#20854;&#20013;&#36807;&#28193;&#27169;&#22411;&#21644;&#21457;&#23556;&#27169;&#22411;&#30001;&#20855;&#26377;&#19981;&#30830;&#23450;&#26435;&#37325;&#30340;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#36825;&#31867;&#27169;&#22411;&#30340;&#31532;&#19968;&#20010;&#30830;&#23450;&#24615;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#36817;&#20284;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#39044;&#31639;&#20043;&#38388;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world dynamical systems can be described as State-Space Models (SSMs). In this formulation, each observation is emitted by a latent state, which follows first-order Markovian dynamics. A Probabilistic Deep SSM (ProDSSM) generalizes this framework to dynamical systems of unknown parametric form, where the transition and emission models are described by neural networks with uncertain weights. In this work, we propose the first deterministic inference algorithm for models of this type. Our framework allows efficient approximations for training and testing. We demonstrate in our experiments that our new method can be employed for a variety of tasks and enjoys a superior balance between predictive performance and computational budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#35821;&#38899;&#21512;&#25104;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#38899;&#36716;&#25442;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#20248;&#20110;&#22522;&#20110;&#22810;&#35821;&#31181;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.08255</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27969;&#30340;&#35821;&#38899;&#36716;&#25442;&#23454;&#29616;&#36328;&#35821;&#35328;&#30693;&#35782;&#33976;&#39311;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Knowledge Distillation via Flow-based Voice Conversion for Robust Polyglot Text-To-Speech. (arXiv:2309.08255v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#35821;&#38899;&#21512;&#25104;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#38899;&#36716;&#25442;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#20248;&#20110;&#22522;&#20110;&#22810;&#35821;&#31181;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#35821;&#38899;&#21512;&#25104;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#19978;&#28216;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#27169;&#22411;&#21644;&#19968;&#20010;&#19979;&#28216;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;4&#20010;&#38454;&#27573;&#12290;&#22312;&#21069;&#20004;&#20010;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;VC&#27169;&#22411;&#23558;&#30446;&#26631;&#21306;&#22495;&#30340;&#35805;&#35821;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#12290;&#22312;&#31532;&#19977;&#20010;&#38454;&#27573;&#65292;&#23558;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#19982;&#30446;&#26631;&#35821;&#35328;&#24405;&#38899;&#20013;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#25345;&#32493;&#26102;&#38388;&#32467;&#21512;&#36215;&#26469;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#21333;&#35828;&#35805;&#20154;&#22768;&#23398;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#26368;&#21518;&#19968;&#20010;&#38454;&#27573;&#23558;&#35757;&#32451;&#19968;&#20010;&#19982;&#35821;&#35328;&#26080;&#20851;&#30340;&#22768;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#25552;&#20986;&#30340;&#33539;&#20363;&#20248;&#20110;&#22522;&#20110;&#35757;&#32451;&#22823;&#22411;&#22810;&#35821;&#31181;TTS&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#35821;&#35328;&#12289;&#35828;&#35805;&#32773;&#21644;&#25968;&#25454;&#37327;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a framework for cross-lingual speech synthesis, which involves an upstream Voice Conversion (VC) model and a downstream Text-To-Speech (TTS) model. The proposed framework consists of 4 stages. In the first two stages, we use a VC model to convert utterances in the target locale to the voice of the target speaker. In the third stage, the converted data is combined with the linguistic features and durations from recordings in the target language, which are then used to train a single-speaker acoustic model. Finally, the last stage entails the training of a locale-independent vocoder. Our evaluations show that the proposed paradigm outperforms state-of-the-art approaches which are based on training a large multilingual TTS model. In addition, our experiments demonstrate the robustness of our approach with different model architectures, languages, speakers and amounts of data. Moreover, our solution is especially beneficial in low-resource settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#21644;&#27745;&#26579;&#26469;&#32531;&#35299;&#20132;&#36890;&#38459;&#22622;&#38382;&#39064;&#65292;&#32463;&#23454;&#35777;&#20998;&#26512;&#21644;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08254</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quantitative and Qualitative Evaluation of Reinforcement Learning Policies for Autonomous Vehicles. (arXiv:2309.08254v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#21644;&#27745;&#26579;&#26469;&#32531;&#35299;&#20132;&#36890;&#38459;&#22622;&#38382;&#39064;&#65292;&#32463;&#23454;&#35777;&#20998;&#26512;&#21644;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#20132;&#36890;&#29615;&#22659;&#20013;&#20248;&#21270;&#20132;&#36890;&#21160;&#21147;&#23398;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#19982;&#20154;&#39550;&#39542;&#36710;&#36742;&#24182;&#23384;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;AVs&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#20132;&#36890;&#38459;&#22622;&#65288;&#21363;&#26368;&#23567;&#21270;&#27178;&#36807;&#31859;&#20848;&#30340;&#29615;&#24418;&#36947;&#30340;&#26102;&#38388;&#65289;&#24182;&#20943;&#23569;&#27745;&#26579;&#12290;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#27745;&#26579;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;&#39550;&#39542;&#33329;&#23450;&#24615;&#35780;&#20272;&#20102;&#23398;&#21040;&#30340;&#31574;&#30053;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#31574;&#30053;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25509;&#21463;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22120;&#36827;&#34892;&#20102;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20132;&#36890;&#24179;&#31283;&#24615;&#21644;&#23433;&#20840;&#24863;&#31561;&#19968;&#31995;&#21015;&#25351;&#26631;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#21644;&#34892;&#36710;&#24179;&#28369;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing traffic dynamics in an evolving transportation landscape is crucial, particularly in scenarios where autonomous vehicles (AVs) with varying levels of autonomy coexist with human-driven cars. This paper presents a novel approach to optimizing choices of AVs using Proximal Policy Optimization (PPO), a reinforcement learning algorithm. We learned a policy to minimize traffic jams (i.e., minimize the time to cross the scenario) and to minimize pollution in a roundabout in Milan, Italy. Through empirical analysis, we demonstrate that our approach can reduce time and pollution levels. Furthermore, we qualitatively evaluate the learned policy using a cutting-edge cockpit to assess its performance in near-real-world conditions. To gauge the practicality and acceptability of the policy, we conducted evaluations with human participants using the simulator, focusing on a range of metrics like traffic smoothness and safety perception. In general, our findings show that human-driven vehi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#20027;&#39064;&#35782;&#21035;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#26448;&#26009;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.08249</link><description>&lt;p&gt;
&#24102;&#26377;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Nonnegative Matrix Factorization with Beta Divergences. (arXiv:2309.08249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#20027;&#39064;&#35782;&#21035;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#26448;&#26009;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;deep NMF&#65289;&#26368;&#36817;&#25104;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25552;&#21462;&#22810;&#23618;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#28145;&#24230;NMF&#27169;&#22411;&#21644;&#31639;&#27861;&#20027;&#35201;&#37117;&#20197;&#26368;&#23567;&#20108;&#20056;&#35823;&#24046;&#20026;&#35780;&#20272;&#26631;&#20934;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#35780;&#20272;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#36817;&#20284;&#36136;&#37327;&#30340;&#26368;&#21512;&#36866;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#24403;&#22788;&#29702;&#38899;&#39057;&#20449;&#21495;&#21644;&#25991;&#26723;&#31561;&#25968;&#25454;&#31867;&#22411;&#26102;&#65292;&#24191;&#27867;&#35748;&#21487;&#30340;&#26159;$\beta$-divergences&#25552;&#20379;&#20102;&#26356;&#36866;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#22522;&#20110;$\beta$-divergences&#24320;&#21457;&#20102;&#26032;&#30340;&#28145;&#24230;NMF&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#38598;&#21512;&#20013;&#30340;&#20027;&#39064;&#35782;&#21035;&#20197;&#21450;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#26448;&#26009;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Nonnegative Matrix Factorization (deep NMF) has recently emerged as a valuable technique for extracting multiple layers of features across different scales. However, all existing deep NMF models and algorithms have primarily centered their evaluation on the least squares error, which may not be the most appropriate metric for assessing the quality of approximations on diverse datasets. For instance, when dealing with data types such as audio signals and documents, it is widely acknowledged that $\beta$-divergences offer a more suitable alternative. In this paper, we develop new models and algorithms for deep NMF using $\beta$-divergences. Subsequently, we apply these techniques to the extraction of facial features, the identification of topics within document collections, and the identification of materials within hyperspectral images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#35299;&#21644;&#30072;&#21464;&#34920;&#31034;&#38382;&#39064;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08247</link><description>&lt;p&gt;
&#23545;&#33258;&#32534;&#30721;&#22120;&#30340;&#20960;&#20309;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Geometric Perspective on Autoencoders. (arXiv:2309.08247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#35299;&#21644;&#30072;&#21464;&#34920;&#31034;&#38382;&#39064;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#30340;&#20960;&#20309;&#26041;&#38754;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#34987;&#30456;&#23545;&#36739;&#23569;&#22320;&#35748;&#35782;&#21040;&#12290;&#32473;&#23450;&#19968;&#32452;&#20960;&#20046;&#20301;&#20110;&#26576;&#20010;&#36739;&#20302;&#32500;&#24230;&#27969;&#24418;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#28857;&#65292;&#33258;&#32534;&#30721;&#22120;&#21516;&#26102;&#23398;&#20064;&#27969;&#24418;&#21644;&#20854;&#22352;&#26631;&#22270;&#12290;&#36825;&#31181;&#20960;&#20309;&#35282;&#24230;&#33258;&#28982;&#24341;&#21457;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#8220;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23545;&#24212;&#20110;&#21333;&#19968;&#30340;&#27969;&#24418;&#21527;&#65311;&#8221;&#25110;&#32773;&#8220;&#21482;&#26377;&#19968;&#20010;&#22352;&#26631;&#22270;&#21487;&#20197;&#34920;&#31034;&#27969;&#24418;&#21527;&#65311;&#8221;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#22238;&#31572;&#26159;&#21542;&#23450;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#26377;&#22810;&#20010;&#35299;&#30340;&#33258;&#32534;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#20855;&#26377;&#20005;&#37325;&#30072;&#21464;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#30340;&#38169;&#35823;&#27969;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#36817;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the geometric aspect of the autoencoder framework, which, despite its importance, has been relatively less recognized. Given a set of high-dimensional data points that approximately lie on some lower-dimensional manifold, an autoencoder learns the \textit{manifold} and its \textit{coordinate chart}, simultaneously. This geometric perspective naturally raises inquiries like "Does a finite set of data points correspond to a single manifold?" or "Is there only one coordinate chart that can represent the manifold?". The responses to these questions are negative, implying that there are multiple solution autoencoders given a dataset. Consequently, they sometimes produce incorrect manifolds with severely distorted latent space representations. In this paper, we introduce recent geometric approaches that address these issues.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#25439;&#22833;&#39033;&#21644;&#36866;&#24212;&#25345;&#32493;&#21516;&#35843;&#24230;&#37327;&#30340;&#29109;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;Node2vec&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#36824;&#21407;&#36755;&#20837;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.08241</link><description>&lt;p&gt;
&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#30340;&#25299;&#25169;Node2vec&#65306;&#22686;&#24378;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Node2vec: Enhanced Graph Embedding via Persistent Homology. (arXiv:2309.08241v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08241
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#25439;&#22833;&#39033;&#21644;&#36866;&#24212;&#25345;&#32493;&#21516;&#35843;&#24230;&#37327;&#30340;&#29109;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;Node2vec&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#36824;&#21407;&#36755;&#20837;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Node2vec&#26159;&#19968;&#31181;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20102;&#21152;&#26435;&#22270;&#27599;&#20010;&#33410;&#28857;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#21516;&#26102;&#23613;&#21147;&#20445;&#25345;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#23545;&#36317;&#31163;&#21644;&#20840;&#23616;&#32467;&#26500;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;Node2vec&#38590;&#20197;&#20877;&#29616;&#36755;&#20837;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;Node2vec&#30340;&#35757;&#32451;&#25439;&#22833;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#25299;&#25169;&#25439;&#22833;&#39033;&#65292;&#35813;&#25439;&#22833;&#39033;&#35797;&#22270;&#23558;&#29983;&#25104;&#30340;&#23884;&#20837;&#30340;&#25345;&#32493;&#21516;&#35843;&#22270;&#19982;&#36755;&#20837;&#22270;&#30340;&#25345;&#32493;&#21516;&#35843;&#22270;&#23613;&#21487;&#33021;&#22320;&#23545;&#40784;&#12290;&#25105;&#20204;&#26681;&#25454;&#35745;&#31639;&#20248;&#21270;&#20256;&#36755;&#20013;&#30340;&#32467;&#26524;&#65292;&#31934;&#24515;&#35843;&#25972;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#21516;&#35843;&#24230;&#37327;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#34913;&#37327;&#25345;&#32493;&#21516;&#35843;&#22270;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26368;&#23567;&#21270;&#25105;&#20204;&#20462;&#25913;&#21518;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#37325;&#24314;&#36755;&#20837;&#22270;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20123;&#31034;&#20363;&#21512;&#25104;&#22270;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node2vec is a graph embedding method that learns a vector representation for each node of a weighted graph while seeking to preserve relative proximity and global structure. Numerical experiments suggest Node2vec struggles to recreate the topology of the input graph. To resolve this we introduce a topological loss term to be added to the training loss of Node2vec which tries to align the persistence diagram (PD) of the resulting embedding as closely as possible to that of the input graph. Following results in computational optimal transport, we carefully adapt entropic regularization to PD metrics, allowing us to measure the discrepancy between PDs in a differentiable way. Our modified loss function can then be minimized through gradient descent to reconstruct both the geometry and the topology of the input graph. We showcase the benefits of this approach using demonstrative synthetic examples.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#39640;&#26031;-&#21202;&#35753;&#24503;&#33410;&#28857;&#19978;&#36827;&#34892;&#28508;&#22312;&#31354;&#38388;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#30830;&#20445;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#20445;&#25345;&#25299;&#25169;&#25968;&#25454;&#32467;&#26500;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08228</link><description>&lt;p&gt;
&#22312;&#39640;&#26031;&#65293;&#21202;&#35753;&#24503;&#33410;&#28857;&#19978;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#27491;&#21017;&#21270;&#32780;&#21387;&#32553;&#20013;&#20445;&#25345;&#25299;&#25169;&#25968;&#25454;&#32467;&#26500;&#23436;&#25972;&#24615;&#30340;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Ensuring Toplogical Data-Structure Preservation under Autoencoder Compression due to Latent Space Regularization in Gauss--Legendre nodes. (arXiv:2309.08228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08228
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#39640;&#26031;-&#21202;&#35753;&#24503;&#33410;&#28857;&#19978;&#36827;&#34892;&#28508;&#22312;&#31354;&#38388;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#30830;&#20445;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#20445;&#25345;&#25299;&#25169;&#25968;&#25454;&#32467;&#26500;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#19968;&#33324;&#30340;&#26080;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#21046;&#23450;&#20102;&#19968;&#20010;&#25968;&#25454;&#26080;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#27491;&#21017;&#21270;&#32422;&#26463;&#12290;&#35813;&#27491;&#21017;&#21270;&#22522;&#20110;&#22312;&#21202;&#35753;&#24503;&#33410;&#28857;&#19978;&#23545;&#33258;&#32534;&#30721;&#22120;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20123;&#33410;&#28857;&#26159;&#39640;&#26031;-&#21202;&#35753;&#24503;&#31215;&#20998;&#30340;&#20013;&#24515;&#12290;&#37325;&#26032;&#23457;&#35270;&#36825;&#20010;&#32463;&#20856;&#38382;&#39064;&#33021;&#22815;&#35777;&#26126;&#65292;&#32463;&#36807;&#27491;&#21017;&#21270;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#23558;&#21021;&#22987;&#25968;&#25454;&#27969;&#24418;&#19968;&#23545;&#19968;&#22320;&#37325;&#26032;&#23884;&#20837;&#21040;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20043;&#21069;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;&#22914;&#25910;&#32553;&#33258;&#32534;&#30721;&#65289;&#22312;&#31616;&#21333;&#31034;&#20363;&#20013;&#24050;&#32463;&#23548;&#33268;&#20102;&#25299;&#25169;&#32570;&#38519;&#65292;&#32780;&#22522;&#20110;&#21367;&#31215;&#30340;&#65288;&#21464;&#20998;&#65289;&#33258;&#32534;&#30721;&#22120;&#20063;&#26159;&#22914;&#27492;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#36129;&#29486;&#65292;&#26631;&#20934;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#22312;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#24050;&#32463;&#30830;&#20445;&#20102;&#25299;&#25169;&#23436;&#25972;&#24615;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#36866;&#29992;&#20110;&#32463;&#20856;&#30340;FashionMNIST&#25968;&#25454;&#38598;&#20197;&#21450;MRI&#33041;&#37096;&#25195;&#25551;&#30340;&#30495;&#23454;&#19990;&#30028;&#32534;&#30721;&#38382;&#39064;&#65292;&#36825;&#34920;&#26126;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#65292;&#21487;&#38752;&#30340;&#20302;&#32500;&#34920;&#31034;&#24050;&#24471;&#20197;&#30830;&#20445;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate a data independent latent space regularisation constraint for general unsupervised autoencoders. The regularisation rests on sampling the autoencoder Jacobian in Legendre nodes, being the centre of the Gauss-Legendre quadrature. Revisiting this classic enables to prove that regularised autoencoders ensure a one-to-one re-embedding of the initial data manifold to its latent representation. Demonstrations show that prior proposed regularisation strategies, such as contractive autoencoding, cause topological defects already for simple examples, and so do convolutional based (variational) autoencoders. In contrast, topological preservation is ensured already by standard multilayer perceptron neural networks when being regularised due to our contribution. This observation extends through the classic FashionMNIST dataset up to real world encoding problems for MRI brain scans, suggesting that, across disciplines, reliable low dimensional representations of complex high-dimensiona
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#34394;&#25311;&#26799;&#24230;&#36827;&#34892;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#65292;&#20511;&#21161;&#35821;&#20041;&#35760;&#24518;&#26469;&#25233;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.08227</link><description>&lt;p&gt;
VERSE&#65306;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#34394;&#25311;&#26799;&#24230;&#24863;&#30693;&#27969;&#36716;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference. (arXiv:2309.08227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08227
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#34394;&#25311;&#26799;&#24230;&#36827;&#34892;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#65292;&#20511;&#21161;&#35821;&#20041;&#35760;&#24518;&#26469;&#25233;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26159;&#25351;&#22312;&#35757;&#32451;AI&#20195;&#29702;&#30340;&#21516;&#26102;&#65292;&#38450;&#27490;&#20854;&#36951;&#24536;&#20197;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#32456;&#36523;&#23398;&#20064;&#65292;&#24182;&#19988;&#32570;&#20047;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#20943;&#36731;&#36951;&#24536;&#30340;&#33021;&#21147;&#12290;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26159;&#32456;&#36523;&#23398;&#20064;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#21160;&#24577;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#32780;&#19981;&#36951;&#24536;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#27969;&#24335;&#30340;&#65292;&#20165;&#38656;&#35201;&#23545;&#25968;&#25454;&#36827;&#34892;&#19968;&#27425;&#36941;&#21382;&#65292;&#21487;&#20197;&#20197;&#31867;&#22686;&#37327;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#21363;&#26102;&#35780;&#20272;&#65288;&#23454;&#26102;&#25512;&#29702;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#34394;&#25311;&#26799;&#24230;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#20511;&#21161;&#22522;&#20110;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#30340;&#35821;&#20041;&#35760;&#24518;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning, also referred to as continual learning, is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Most of the existing methods primarily focus on lifelong learning within a static environment and lack the ability to mitigate forgetting in a quickly-changing dynamic environment. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming, requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose virtual gradients for continual representation learning to prevent catastrophic forgetting and leverage an exponential-moving-average-based semantic memory to further enhance performance. Extensive experiments on diverse data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#32479;&#19968;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26694;&#26550;&#30340;&#34920;&#36798;&#37096;&#20998;&#25552;&#20379;&#20102;&#23545;&#24369;&#30417;&#30563;&#24418;&#25104;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#21253;&#21547;15&#31181;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#35774;&#32622;&#65307;&#24341;&#23548;&#29983;&#25104;&#30340;&#20943;&#23569;&#22270;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36830;&#25509;&#12290;&#35813;&#26694;&#26550;&#30340;&#20998;&#26512;&#37096;&#20998;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#36827;&#34892;&#39118;&#38505;&#37325;&#20889;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#27745;&#20998;&#24067;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.08216</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#19968;&#39118;&#38505;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unified Risk Analysis for Weakly Supervised Learning. (arXiv:2309.08216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#32479;&#19968;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26694;&#26550;&#30340;&#34920;&#36798;&#37096;&#20998;&#25552;&#20379;&#20102;&#23545;&#24369;&#30417;&#30563;&#24418;&#25104;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#21253;&#21547;15&#31181;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#35774;&#32622;&#65307;&#24341;&#23548;&#29983;&#25104;&#30340;&#20943;&#23569;&#22270;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36830;&#25509;&#12290;&#35813;&#26694;&#26550;&#30340;&#20998;&#26512;&#37096;&#20998;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#36827;&#34892;&#39118;&#38505;&#37325;&#20889;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#27745;&#20998;&#24067;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#32321;&#33635;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#24369;&#30417;&#30563;&#24773;&#26223;&#32972;&#21518;&#26426;&#21046;&#30340;&#32479;&#19968;&#35299;&#37322;&#32570;&#22833;&#65292;&#26356;&#19981;&#29992;&#35828;&#23545;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#39118;&#38505;&#37325;&#20889;&#38382;&#39064;&#30340;&#31995;&#32479;&#22788;&#29702;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#32479;&#19968;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26694;&#26550;&#30340;&#34920;&#36798;&#37096;&#20998;&#21033;&#29992;&#20102;&#19968;&#20010;&#27745;&#26579;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#24369;&#30417;&#30563;&#24418;&#25104;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#21253;&#21547;&#20102;15&#31181;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#35774;&#32622;&#12290;&#24341;&#23548;&#29983;&#25104;&#30340;&#20943;&#23569;&#22270;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36830;&#25509;&#12290;&#35813;&#26694;&#26550;&#30340;&#20998;&#26512;&#37096;&#20998;&#34987;&#35270;&#20026;&#19968;&#20010;&#21435;&#27745;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#36827;&#34892;&#39118;&#38505;&#37325;&#20889;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#36870;&#30697;&#38453;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36793;&#38469;&#38142;&#30340;&#26032;&#31574;&#30053;&#65292;&#26088;&#22312;&#21435;&#27745;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the flourishing research of weakly supervised learning (WSL), we recognize the lack of a unified interpretation of the mechanism behind the weakly supervised scenarios, let alone a systematic treatment of the risk rewrite problem, a crucial step in the empirical risk minimization approach. In this paper, we introduce a framework providing a comprehensive understanding and a unified methodology for WSL. The formulation component of the framework, leveraging a contamination perspective, provides a unified interpretation of how weak supervision is formed and subsumes fifteen existing WSL settings. The induced reduction graphs offer comprehensive connections over WSLs. The analysis component of the framework, viewed as a decontamination process, provides a systematic method of conducting risk rewrite. In addition to the conventional inverse matrix approach, we devise a novel strategy called marginal chain aiming to decontaminate distributions. We justify the feasibility of the propos
&lt;/p&gt;</description></item><item><title>HM-Conformer&#26159;&#19968;&#31181;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#20998;&#23618;&#27719;&#32858;&#21644;&#22810;&#32423;&#20998;&#31867;&#20196;&#29260;&#32858;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#24182;&#26816;&#27979;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#30340;&#27450;&#39575;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.08208</link><description>&lt;p&gt;
HM-Conformer: &#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31995;&#32479;&#65292;&#20855;&#26377;&#20998;&#23618;&#27719;&#32858;&#21644;&#22810;&#32423;&#20998;&#31867;&#20196;&#29260;&#32858;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
HM-Conformer: A Conformer-based audio deepfake detection system with hierarchical pooling and multi-level classification token aggregation methods. (arXiv:2309.08208v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08208
&lt;/p&gt;
&lt;p&gt;
HM-Conformer&#26159;&#19968;&#31181;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#20998;&#23618;&#27719;&#32858;&#21644;&#22810;&#32423;&#20998;&#31867;&#20196;&#29260;&#32858;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#24182;&#26816;&#27979;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#30340;&#27450;&#39575;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65288;ADD&#65289;&#26159;&#26816;&#27979;&#30001;&#25991;&#26412;&#21040;&#35821;&#38899;&#25110;&#35821;&#38899;&#36716;&#25442;&#31995;&#32479;&#29983;&#25104;&#30340;&#27450;&#39575;&#25915;&#20987;&#30340;&#20219;&#21153;&#12290;&#29992;&#20110;&#21306;&#20998;&#20266;&#36896;&#21644;&#30495;&#23454;&#35805;&#35821;&#30340;&#27450;&#39575;&#35777;&#25454;&#21487;&#33021;&#23384;&#22312;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#23616;&#37096;&#25110;&#20840;&#23616;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#35777;&#25454;&#65292;Conformer&#32467;&#21512;&#20102;Transformer&#21644;CNN&#65292;&#20855;&#26377;&#36866;&#21512;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;Conformer&#26159;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#65292;&#30452;&#25509;&#24212;&#29992;&#20110;ADD&#20219;&#21153;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HM-Conformer&#65292;&#37319;&#29992;&#20102;&#20004;&#20010;&#32452;&#20214;&#65306;&#65288;1&#65289;&#20998;&#23618;&#27719;&#32858;&#26041;&#27861;&#65292;&#36880;&#27493;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#20197;&#28040;&#38500;&#37325;&#22797;&#20449;&#24687;&#65307;&#65288;2&#65289;&#22810;&#32423;&#20998;&#31867;&#20196;&#29260;&#32858;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#31867;&#20196;&#29260;&#20174;&#19981;&#21516;&#30340;&#22359;&#20013;&#25910;&#38598;&#20449;&#24687;&#12290;&#30001;&#20110;&#36825;&#20123;&#32452;&#20214;&#30340;&#23384;&#22312;&#65292;HM-Conformer&#21487;&#20197;&#36890;&#36807;&#22788;&#29702;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#24182;&#32858;&#21512;&#23427;&#20204;&#26469;&#39640;&#25928;&#22320;&#26816;&#27979;&#21040;&#27450;&#39575;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio deepfake detection (ADD) is the task of detecting spoofing attacks generated by text-to-speech or voice conversion systems. Spoofing evidence, which helps to distinguish between spoofed and bona-fide utterances, might exist either locally or globally in the input features. To capture these, the Conformer, which consists of Transformers and CNN, possesses a suitable structure. However, since the Conformer was designed for sequence-to-sequence tasks, its direct application to ADD tasks may be sub-optimal. To tackle this limitation, we propose HM-Conformer by adopting two components: (1) Hierarchical pooling method progressively reducing the sequence length to eliminate duplicated information (2) Multi-level classification token aggregation method utilizing classification tokens to gather information from different blocks. Owing to these components, HM-Conformer can efficiently detect spoofing evidence by processing various sequence lengths and aggregating them. In experimental resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#22312;&#22810;&#32500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26684;&#28857;&#35889;&#28151;&#21512;&#26680;&#20844;&#24335;&#65292;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20351;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08201</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#65306;&#39057;&#35889;&#35774;&#35745;&#21644;&#22810;&#32500;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gaussian Processes with Linear Multiple Kernel: Spectrum Design and Distributed Learning for Multi-Dimensional Data. (arXiv:2309.08201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#22312;&#22810;&#32500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26684;&#28857;&#35889;&#28151;&#21512;&#26680;&#20844;&#24335;&#65292;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20351;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;GP&#24314;&#27169;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#26680;&#20989;&#25968;&#30340;&#36873;&#25321;&#65292;&#32447;&#24615;&#22810;&#26680;&#65288;LMKs&#65289;&#22240;&#20854;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#25104;&#20026;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#26680;&#20989;&#25968;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#26684;&#28857;&#35889;&#28151;&#21512;&#65288;GSM&#65289;&#26680;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#20197;&#36817;&#20284;&#20219;&#24847;&#24179;&#31283;&#26680;&#30340;LMK&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GSM&#26680;&#20844;&#24335;&#65292;&#29992;&#20110;&#22810;&#32500;&#25968;&#25454;&#65292;&#30456;&#27604;&#29616;&#26377;&#20844;&#24335;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#26377;&#21033;&#30340;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;GSM&#26680;&#20013;&#30340;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;SCA&#65288;DSCA&#65289;&#31639;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#26694;&#26550;&#25552;&#20986;&#20102;&#21452;&#37325;&#20998;&#24067;&#24335;SCA&#65288;D$^2$SCA&#65289;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21512;&#20316;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs) have emerged as a prominent technique for machine learning and signal processing. A key component in GP modeling is the choice of kernel, and linear multiple kernels (LMKs) have become an attractive kernel class due to their powerful modeling capacity and interpretability. This paper focuses on the grid spectral mixture (GSM) kernel, an LMK that can approximate arbitrary stationary kernels. Specifically, we propose a novel GSM kernel formulation for multi-dimensional data that reduces the number of hyper-parameters compared to existing formulations, while also retaining a favorable optimization structure and approximation capability. In addition, to make the large-scale hyper-parameter optimization in the GSM kernel tractable, we first introduce the distributed SCA (DSCA) algorithm. Building on this, we propose the doubly distributed SCA (D$^2$SCA) algorithm based on the alternating direction method of multipliers (ADMM) framework, which allows us to cooperativ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20351;&#29992;&#28779;&#26143;&#22823;&#27668;&#21644;&#25381;&#21457;&#29289;&#28436;&#21270; (MAVEN) &#30340;&#35266;&#27979;&#36164;&#26009;&#65292;&#26469;&#35299;&#37322;&#28779;&#26143;&#36136;&#23376;&#26497;&#20809;&#12290;&#36890;&#36807;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#37325;&#29616;&#27599;&#20010;Ly alpha&#36752;&#23556;&#30340;&#24378;&#24230;&#65292;&#24182;&#23545;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#24544;&#23454;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.08195</link><description>&lt;p&gt;
&#28779;&#26143;&#36136;&#23376;&#26497;&#20809;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Explainable Deep-learning Model of Proton Auroras on Mars. (arXiv:2309.08195v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20351;&#29992;&#28779;&#26143;&#22823;&#27668;&#21644;&#25381;&#21457;&#29289;&#28436;&#21270; (MAVEN) &#30340;&#35266;&#27979;&#36164;&#26009;&#65292;&#26469;&#35299;&#37322;&#28779;&#26143;&#36136;&#23376;&#26497;&#20809;&#12290;&#36890;&#36807;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#37325;&#29616;&#27599;&#20010;Ly alpha&#36752;&#23556;&#30340;&#24378;&#24230;&#65292;&#24182;&#23545;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#24544;&#23454;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28779;&#26143;&#30333;&#22825;&#20391;&#24191;&#27867;&#35266;&#23519;&#21040;&#36136;&#23376;&#26497;&#20809;&#65292;&#34987;&#35748;&#20026;&#26159;&#27682; Ly alpha (121.6 nm) &#36752;&#23556;&#22312;120&#33267;150&#20844;&#37324;&#39640;&#24230;&#20043;&#38388;&#30340;&#26174;&#33879;&#22686;&#24378;&#12290;&#22826;&#38451;&#39118;&#36136;&#23376;&#20316;&#20026;&#39640;&#33021;&#20013;&#24615;&#21407;&#23376;&#31359;&#36807;&#28779;&#26143;&#28909;&#23618;&#36827;&#20837;&#22823;&#27668;&#23618;&#65292;&#34987;&#35748;&#20026;&#26159;&#36136;&#23376;&#26497;&#20809;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36136;&#23376;&#26497;&#20809;&#23545;&#20110;&#25551;&#32472;&#22826;&#38451;&#39118;&#19982;&#28779;&#26143;&#22823;&#27668;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#35266;&#27979;&#21040;&#23616;&#37096;"&#26001;&#22359;&#29366;"&#36136;&#23376;&#26497;&#20809;&#65292;&#26263;&#31034;&#22312;&#19981;&#31283;&#23450;&#30340;&#22826;&#38451;&#39118;&#26465;&#20214;&#19979;&#65292;&#36136;&#23376;&#21487;&#33021;&#30452;&#25509;&#27785;&#31215;&#21040;&#28779;&#26143;&#22823;&#27668;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#28779;&#26143;&#22823;&#27668;&#21644;&#25381;&#21457;&#29289;&#28436;&#21270; (MAVEN) &#38750;&#29616;&#22330;&#35266;&#27979;&#21644;&#36793;&#32536;&#25195;&#25551;&#30340; Ly alpha &#36752;&#23556;&#36164;&#26009;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26469;&#27169;&#25311;&#36136;&#23376;&#26497;&#20809;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20197;0.95&#30340;Pearson&#30456;&#20851;&#24615;&#37325;&#29616;&#27599;&#20010;Ly alpha&#36752;&#23556;&#30340;&#24378;&#24230;&#65292;&#24182;&#23545;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#24544;&#23454;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proton auroras are widely observed on the day side of Mars, identified as a significant intensity enhancement in the hydrogen Ly alpha (121.6 nm) emission between 120 and 150~km altitudes. Solar wind protons penetrating as energetic neutral atoms into the Martian thermosphere are thought to be responsible for these auroras. Understanding proton auroras is therefore important for characterizing the solar wind interaction with the atmosphere of Mars. Recent observations of spatially localized "patchy" proton auroras suggest a possible direct deposition of protons into the atmosphere of Mars during unstable solar wind conditions. Here, we develop a purely data-driven model of proton auroras using Mars Atmosphere and Volatile EvolutioN (MAVEN) in situ observations and limb scans of Ly alpha emissions between 2014 and 2022. We train an artificial neural network that reproduces individual Ly alpha intensities with a Pearson correlation of 0.95 along with a faithful reconstruction of the obse
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#24230;&#21487;&#25193;&#23637;&#24615;&#30340;RISC-V DNN&#22788;&#29702;&#22120;&#65292;&#35813;&#22788;&#29702;&#22120;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#31934;&#24230;&#32423;&#21035;&#30340;&#23450;&#28857;DNN&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;FP16&#25805;&#20316;&#22686;&#24378;&#20102;&#22312;&#35774;&#22791;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08186</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#31934;&#24230;&#21487;&#25193;&#23637;&#24615;&#30340;&#22312;&#26497;&#38480;&#36793;&#32536;&#20855;&#26377;&#22312;&#35774;&#22791;&#23398;&#20064;&#33021;&#21147;&#30340;RISC-V DNN&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Precision-Scalable RISC-V DNN Processor with On-Device Learning Capability at the Extreme Edge. (arXiv:2309.08186v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08186
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#24230;&#21487;&#25193;&#23637;&#24615;&#30340;RISC-V DNN&#22788;&#29702;&#22120;&#65292;&#35813;&#22788;&#29702;&#22120;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#31934;&#24230;&#32423;&#21035;&#30340;&#23450;&#28857;DNN&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;FP16&#25805;&#20316;&#22686;&#24378;&#20102;&#22312;&#35774;&#22791;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#38480;&#36793;&#32536;&#24179;&#21488;&#65292;&#20363;&#22914;&#36710;&#36733;&#26234;&#33021;&#35774;&#22791;&#65292;&#38656;&#35201;&#39640;&#25928;&#37096;&#32626;&#37327;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#20197;&#20415;&#22312;&#33021;&#28304;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26234;&#33021;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37327;&#21270;&#27700;&#24179;&#30340;&#21464;&#21270;&#65292;&#35768;&#22810;&#36793;&#32536;&#35774;&#22791;&#38590;&#20197;&#25552;&#39640;&#21508;&#31181;&#37327;&#21270;DNN&#30340;&#25512;&#26029;&#21534;&#21520;&#37327;&#65292;&#24182;&#19988;&#36825;&#20123;&#35774;&#22791;&#32570;&#20047;&#28014;&#28857;&#65288;FP&#65289;&#25903;&#25345;&#30340;&#22312;&#35774;&#22791;&#23398;&#20064;&#33021;&#21147;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#24230;&#21487;&#25193;&#23637;&#24615;&#30340;RISC-V DNN&#22788;&#29702;&#22120;&#65292;&#20855;&#26377;&#22312;&#35774;&#22791;&#23398;&#20064;&#33021;&#21147;&#12290;&#23427;&#21487;&#20197;&#26041;&#20415;&#22320;&#36827;&#34892;2&#20301;&#21040;16&#20301;&#30340;&#22810;&#31181;&#31934;&#24230;&#32423;&#21035;&#30340;&#23450;&#28857;DNN&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;FP16&#25805;&#20316;&#26469;&#22686;&#24378;&#22312;&#35774;&#22791;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#65292;&#22914;FP16&#20056;&#27861;&#22120;&#37325;&#29992;&#21644;&#22810;&#31934;&#24230;&#25972;&#25968;&#20056;&#27861;&#22120;&#37325;&#29992;&#65292;&#20197;&#21450;FPGA&#36164;&#28304;&#30340;&#24179;&#34913;&#26144;&#23556;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extreme edge platforms, such as in-vehicle smart devices, require efficient deployment of quantized deep neural networks (DNNs) to enable intelligent applications with limited amounts of energy, memory, and computing resources. However, many edge devices struggle to boost inference throughput of various quantized DNNs due to the varying quantization levels, and these devices lack floating-point (FP) support for on-device learning, which prevents them from improving model accuracy while ensuring data privacy. To tackle the challenges above, we propose a precision-scalable RISC-V DNN processor with on-device learning capability. It facilitates diverse precision levels of fixed-point DNN inference, spanning from 2-bit to 16-bit, and enhances on-device learning through improved support with FP16 operations. Moreover, we employ multiple methods such as FP16 multiplier reuse and multi-precision integer multiplier reuse, along with balanced mapping of FPGA resources, to significantly improve 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26469;&#23398;&#20064;&#25429;&#25417;&#25968;&#25454;&#30456;&#20851;&#30340;&#19981;&#21464;&#24615;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#22312;&#35270;&#35273;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#33394;&#65292;&#19981;&#20165;&#25928;&#29575;&#39640;&#65292;&#32780;&#19988;&#25928;&#26524;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.08171</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#25581;&#31034;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unveiling Invariances via Neural Network Pruning. (arXiv:2309.08171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26469;&#23398;&#20064;&#25429;&#25417;&#25968;&#25454;&#30456;&#20851;&#30340;&#19981;&#21464;&#24615;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#22312;&#35270;&#35273;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#33394;&#65292;&#19981;&#20165;&#25928;&#29575;&#39640;&#65292;&#32780;&#19988;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#24615;&#25551;&#36848;&#20102;&#23545;&#25968;&#25454;&#24213;&#23618;&#35821;&#20041;&#27809;&#26377;&#24433;&#21709;&#30340;&#36716;&#25442;&#12290;&#20445;&#25345;&#33258;&#28982;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#29616;&#20195;&#32593;&#32476;&#34987;&#25163;&#24037;&#35774;&#35745;&#29992;&#26469;&#22788;&#29702;&#20247;&#25152;&#21608;&#30693;&#30340;&#19981;&#21464;&#24615;&#65288;&#20363;&#22914;&#24179;&#31227;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21098;&#26525;&#26469;&#23398;&#20064;&#25429;&#25417;&#25968;&#25454;&#30456;&#20851;&#30340;&#19981;&#21464;&#24615;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#32593;&#32476;&#26550;&#26500;&#22312;&#35270;&#35273;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#19978;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;3&#20010;&#35270;&#35273;&#21644;40&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariance describes transformations that do not alter data's underlying semantics. Neural networks that preserve natural invariance capture good inductive biases and achieve superior performance. Hence, modern networks are handcrafted to handle well-known invariances (ex. translations). We propose a framework to learn novel network architectures that capture data-dependent invariances via pruning. Our learned architectures consistently outperform dense neural networks on both vision and tabular datasets in both efficiency and effectiveness. We demonstrate our framework on multiple deep learning models across 3 vision and 40 tabular datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22270;&#28145;&#24230;&#26680;&#23398;&#20064;&#26694;&#26550;&#26469;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#19978;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#27491;&#24615;&#20551;&#35774;&#36829;&#21453;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08165</link><description>&lt;p&gt;
&#39044;&#27979;&#36824;&#26159;&#25298;&#32477;&#65306;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#19982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
To Predict or to Reject: Causal Effect Estimation with Uncertainty on Networked Data. (arXiv:2309.08165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22270;&#28145;&#24230;&#26680;&#23398;&#20064;&#26694;&#26550;&#26469;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#19978;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#27491;&#24615;&#20551;&#35774;&#36829;&#21453;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32593;&#32476;&#35266;&#23519;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#24615;&#65292;&#23545;&#20110;&#26576;&#20123;&#20010;&#20307;&#30340;&#22240;&#26524;&#25928;&#24212;&#39044;&#27979;&#21487;&#33021;&#20005;&#37325;&#36829;&#21453;&#27491;&#24615;/&#37325;&#21472;&#20551;&#35774;&#65292;&#23548;&#33268;&#20272;&#35745;&#19981;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32593;&#32476;&#25968;&#25454;&#20010;&#20307;&#32423;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#30340;&#36825;&#31181;&#28508;&#22312;&#39118;&#38505;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#21019;&#24314;&#19968;&#20010;&#26356;&#21487;&#20449;&#36182;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#22270;&#28145;&#24230;&#26680;&#23398;&#20064; (GraphDKL) &#26694;&#26550;&#65292;&#24182;&#36890;&#36807;Lipschitz&#32422;&#26463;&#26469;&#24314;&#27169;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20197;&#35782;&#21035;&#19981;&#21487;&#38752;&#30340;&#20272;&#35745;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GraphDKL&#26159;&#31532;&#19968;&#20010;&#22312;&#25191;&#34892;&#22270;&#19978;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26102;&#22788;&#29702;&#27491;&#24615;&#20551;&#35774;&#36829;&#21453;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the imbalanced nature of networked observational data, the causal effect predictions for some individuals can severely violate the positivity/overlap assumption, rendering unreliable estimations. Nevertheless, this potential risk of individual-level treatment effect estimation on networked data has been largely under-explored. To create a more trustworthy causal effect estimator, we propose the uncertainty-aware graph deep kernel learning (GraphDKL) framework with Lipschitz constraint to model the prediction uncertainty with Gaussian process and identify unreliable estimations. To the best of our knowledge, GraphDKL is the first framework to tackle the violation of positivity assumption when performing causal effect estimation with graphs. With extensive experiments, we demonstrate the superiority of our proposed method in uncertainty-aware causal effect estimation on networked data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#26679;&#24335;&#32534;&#36753;&#23545;&#24191;&#21578;&#21560;&#24341;&#21147;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;StyleGAN&#30340;&#38754;&#37096;&#35821;&#20041;&#32534;&#36753;&#21644;&#21453;&#36716;&#65292;&#24182;&#32467;&#21512;&#20256;&#32479;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdSEE&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#22312;&#32447;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#23545;QQ-AD&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;AdSEE&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08159</link><description>&lt;p&gt;
AdSEE: &#30740;&#31350;&#22270;&#20687;&#26679;&#24335;&#32534;&#36753;&#23545;&#24191;&#21578;&#21560;&#24341;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness. (arXiv:2309.08159v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#26679;&#24335;&#32534;&#36753;&#23545;&#24191;&#21578;&#21560;&#24341;&#21147;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;StyleGAN&#30340;&#38754;&#37096;&#35821;&#20041;&#32534;&#36753;&#21644;&#21453;&#36716;&#65292;&#24182;&#32467;&#21512;&#20256;&#32479;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdSEE&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#22312;&#32447;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#23545;QQ-AD&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;AdSEE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#12289;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21644;&#25628;&#32034;&#24341;&#25806;&#20013;&#65292;&#22312;&#32447;&#24191;&#21578;&#26159;&#37325;&#35201;&#30340;&#20803;&#32032;&#12290;&#38543;&#30528;&#31227;&#21160;&#27983;&#35272;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#35768;&#22810;&#22312;&#32447;&#24191;&#21578;&#37117;&#36890;&#36807;&#23553;&#38754;&#22270;&#29255;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#26469;&#21560;&#24341;&#29992;&#25143;&#30340;&#27880;&#24847;&#21147;&#12290;&#26368;&#36817;&#30340;&#21508;&#31181;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#32771;&#34385;&#35270;&#35273;&#29305;&#24449;&#26469;&#39044;&#27979;&#22312;&#32447;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#65292;&#25110;&#32773;&#36890;&#36807;&#32452;&#21512;&#26368;&#20339;&#30340;&#24191;&#21578;&#20803;&#32032;&#26469;&#22686;&#24378;&#21487;&#35265;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#21578;&#26679;&#24335;&#32534;&#36753;&#21644;&#21560;&#24341;&#21147;&#22686;&#24378;&#65288;AdSEE&#65289;&#65292;&#25506;&#35752;&#20102;&#24191;&#21578;&#22270;&#20687;&#30340;&#35821;&#20041;&#32534;&#36753;&#26159;&#21542;&#20250;&#24433;&#21709;&#25110;&#25913;&#21464;&#22312;&#32447;&#24191;&#21578;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;StyleGAN&#30340;&#38754;&#37096;&#35821;&#20041;&#32534;&#36753;&#21644;&#21453;&#36716;&#65292;&#23545;&#24191;&#21578;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#38754;&#37096;&#28508;&#22312;&#34920;&#31034;&#20197;&#21450;&#20256;&#32479;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#39044;&#27979;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;QQ-AD&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;20,527&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#23545;AdSEE&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online advertisements are important elements in e-commerce sites, social media platforms, and search engines. With the increasing popularity of mobile browsing, many online ads are displayed with visual information in the form of a cover image in addition to text descriptions to grab the attention of users. Various recent studies have focused on predicting the click rates of online advertisements aware of visual features or composing optimal advertisement elements to enhance visibility. In this paper, we propose Advertisement Style Editing and Attractiveness Enhancement (AdSEE), which explores whether semantic editing to ads images can affect or alter the popularity of online advertisements. We introduce StyleGAN-based facial semantic editing and inversion to ads images and train a click rate predictor attributing GAN-based face latent representations in addition to traditional visual and textual features to click rates. Through a large collected dataset named QQ-AD, containing 20,527 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#26631;&#35760;&#36924;&#30495;&#30340;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#31243;&#24207;&#27969;&#37327;&#65292;&#20026;&#25913;&#21892;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.08158</link><description>&lt;p&gt;
&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#20998;&#26512;&#31227;&#21160;&#35774;&#22791;&#21450;&#20854;&#24212;&#29992;&#30340;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A Testbed for Automating and Analysing Mobile Devices and their Applications. (arXiv:2309.08158v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08158
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#26631;&#35760;&#36924;&#30495;&#30340;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#31243;&#24207;&#27969;&#37327;&#65292;&#20026;&#25913;&#21892;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#20005;&#37325;&#24615;&#22686;&#21152;&#65292;&#23545;&#25913;&#36827;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#30340;&#38656;&#27714;&#26085;&#30410;&#20984;&#26174;&#12290;&#30001;&#20110;&#31227;&#21160;&#30005;&#35805;&#30340;&#21160;&#24577;&#34892;&#20026;&#21644;&#22312;&#32593;&#32476;&#19978;&#30340;&#32570;&#20047;&#21487;&#35265;&#24615;&#65292;&#23427;&#20204;&#23545;&#32593;&#32476;&#24577;&#21183;&#24863;&#30693;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#21521;&#31649;&#29702;&#21592;&#25552;&#20379;&#26377;&#20851;&#26500;&#25104;&#20854;&#32593;&#32476;&#30340;&#35774;&#22791;&#21644;&#27963;&#21160;&#30340;&#35265;&#35299;&#65292;&#22686;&#24378;&#20102;&#24577;&#21183;&#24863;&#30693;&#12290;&#20026;&#20102;&#24320;&#21457;&#29992;&#20110;&#24577;&#21183;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#21644;&#26631;&#35760;&#32593;&#32476;&#27969;&#37327;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27979;&#35797;&#24179;&#21488;&#26080;&#27861;&#33258;&#21160;&#21270;&#29983;&#25104;&#21644;&#26631;&#35760;&#36924;&#30495;&#30340;&#32593;&#32476;&#27969;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#21487;&#20197;&#33258;&#21160;&#21270;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#31243;&#24207;&#20197;&#29983;&#25104;&#21644;&#26631;&#35760;&#36924;&#30495;&#30340;&#27969;&#37327;&#12290;&#36890;&#36807;&#36825;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20004;&#20010;&#26631;&#35760;&#30340;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#27979;&#35797;&#24179;&#21488;&#30340;&#33258;&#21160;&#21270;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24212;&#29992;&#31243;&#24207;&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for improved network situational awareness has been highlighted by the growing complexity and severity of cyber-attacks. Mobile phones pose a significant risk to network situational awareness due to their dynamic behaviour and lack of visibility on a network. Machine learning techniques enhance situational awareness by providing administrators insight into the devices and activities which form their network. Developing machine learning techniques for situational awareness requires a testbed to generate and label network traffic. Current testbeds, however, are unable to automate the generation and labelling of realistic network traffic. To address this, we describe a testbed which automates applications on mobile devices to generate and label realistic traffic. From this testbed, two labelled datasets of network traffic have been created. We provide an analysis of the testbed automation reliability and benchmark the datasets for the task of application classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#27493;&#27861;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#29992;&#20110;&#24494;&#24369;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#12290;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30446;&#26631;&#39044;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#28982;&#21518;&#20999;&#25442;&#21040;&#23436;&#20840;&#30417;&#30563;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#32454;&#31890;&#24230;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#65292;&#23558;&#23398;&#29983;&#27169;&#22411;&#30340;&#28608;&#27963;&#20869;&#37096;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#25945;&#24072;&#27169;&#22411;&#21305;&#37197;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39640;&#21387;&#32553;&#21644;&#20302;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08144</link><description>&lt;p&gt;
&#20004;&#27493;&#27861;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#24494;&#24369;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Two-Step Knowledge Distillation for Tiny Speech Enhancement. (arXiv:2309.08144v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#27493;&#27861;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#29992;&#20110;&#24494;&#24369;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#12290;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30446;&#26631;&#39044;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#28982;&#21518;&#20999;&#25442;&#21040;&#23436;&#20840;&#30417;&#30563;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#32454;&#31890;&#24230;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#65292;&#23558;&#23398;&#29983;&#27169;&#22411;&#30340;&#28608;&#27963;&#20869;&#37096;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#25945;&#24072;&#27169;&#22411;&#21305;&#37197;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39640;&#21387;&#32553;&#21644;&#20302;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23884;&#20837;&#24335;&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#32780;&#35328;&#65292;&#24494;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#21387;&#32553;&#21487;&#20197;&#36890;&#36807;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#27493;&#27861;&#26469;&#36827;&#34892;&#24494;&#24369;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#33976;&#39311;&#12290;&#19982;&#26631;&#20934;&#26041;&#27861;&#20013;&#20351;&#29992;&#33976;&#39311;&#25439;&#22833;&#21644;&#30417;&#30563;&#25439;&#22833;&#30340;&#21152;&#26435;&#28151;&#21512;&#19981;&#21516;&#65292;&#25105;&#20204;&#39318;&#20808;&#21482;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#28982;&#21518;&#20999;&#25442;&#21040;&#23436;&#20840;&#30417;&#30563;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#31890;&#24230;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;KD&#25439;&#22833;&#65292;&#26088;&#22312;&#23558;&#23398;&#29983;&#27169;&#22411;&#30340;&#28608;&#27963;&#20869;&#37096;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23588;&#20854;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#65292;&#21253;&#25324;&#39640;&#21387;&#32553;&#21644;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#36755;&#20837;SNR&#20026;-5 dB&#21644;63&#20493;&#21387;&#32553;&#19979;&#65292;&#20449;&#21495;&#22833;&#30495;&#27604;&#20998;&#21035;&#25552;&#39640;&#20102;0.9 dB&#21644;1.1 dB&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiny, causal models are crucial for embedded audio machine learning applications. Model compression can be achieved via distilling knowledge from a large teacher into a smaller student model. In this work, we propose a novel two-step approach for tiny speech enhancement model distillation. In contrast to the standard approach of a weighted mixture of distillation and supervised losses, we firstly pre-train the student using only the knowledge distillation (KD) objective, after which we switch to a fully supervised training regime. We also propose a novel fine-grained similarity-preserving KD loss, which aims to match the student's intra-activation Gram matrices to that of the teacher. Our method demonstrates broad improvements, but particularly shines in adverse conditions including high compression and low signal to noise ratios (SNR), yielding signal to distortion ratio gains of 0.9 dB and 1.1 dB, respectively, at -5 dB input SNR and 63x compression compared to baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#38899;&#39057;&#24046;&#24322;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#29305;&#24449;&#34920;&#31034;&#31354;&#38388;&#26469;&#25913;&#36827;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21442;&#32771;&#38899;&#39057;&#21644;&#36755;&#20837;&#38899;&#39057;&#65292;&#29983;&#25104;&#25551;&#36848;&#23427;&#20204;&#24046;&#24322;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#28151;&#21512;&#25216;&#26415;&#26469;&#28040;&#38500;&#24046;&#24322;&#21644;&#21407;&#22987;&#36755;&#20837;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.08141</link><description>&lt;p&gt;
&#38899;&#39057;&#24046;&#24322;&#23398;&#20064;&#29992;&#20110;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Difference Learning for Audio Captioning. (arXiv:2309.08141v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#38899;&#39057;&#24046;&#24322;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#29305;&#24449;&#34920;&#31034;&#31354;&#38388;&#26469;&#25913;&#36827;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21442;&#32771;&#38899;&#39057;&#21644;&#36755;&#20837;&#38899;&#39057;&#65292;&#29983;&#25104;&#25551;&#36848;&#23427;&#20204;&#24046;&#24322;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#28151;&#21512;&#25216;&#26415;&#26469;&#28040;&#38500;&#24046;&#24322;&#21644;&#21407;&#22987;&#36755;&#20837;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#38899;&#39057;&#24046;&#24322;&#23398;&#20064;&#65292;&#29992;&#20110;&#25913;&#36827;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#26412;&#27010;&#24565;&#26159;&#21019;&#24314;&#19968;&#20010;&#20445;&#30041;&#38899;&#39057;&#20043;&#38388;&#20851;&#31995;&#30340;&#29305;&#24449;&#34920;&#31034;&#31354;&#38388;&#65292;&#20174;&#32780;&#33021;&#22815;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#22797;&#26434;&#38899;&#39057;&#20449;&#24687;&#30340;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21442;&#32771;&#38899;&#39057;&#21644;&#36755;&#20837;&#38899;&#39057;&#65292;&#36890;&#36807;&#20849;&#20139;&#32534;&#30721;&#22120;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#20174;&#36825;&#20123;&#24046;&#24322;&#29305;&#24449;&#29983;&#25104;&#23383;&#24149;&#25551;&#36848;&#23427;&#20204;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#25216;&#26415;&#65292;&#28041;&#21450;&#23558;&#36755;&#20837;&#38899;&#39057;&#19982;&#39069;&#22806;&#38899;&#39057;&#28151;&#21512;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#38899;&#39057;&#20316;&#20026;&#21442;&#32771;&#12290;&#36825;&#26679;&#65292;&#28151;&#21512;&#38899;&#39057;&#19982;&#21442;&#32771;&#38899;&#39057;&#20043;&#38388;&#30340;&#24046;&#24322;&#22238;&#21040;&#21407;&#22987;&#36755;&#20837;&#38899;&#39057;&#12290;&#36825;&#20801;&#35768;&#23558;&#21407;&#22987;&#36755;&#20837;&#30340;&#23383;&#24149;&#20316;&#20026;&#20854;&#24046;&#24322;&#30340;&#23383;&#24149;&#20351;&#29992;&#65292;&#28040;&#38500;&#20102;&#20026;&#24046;&#24322;&#28155;&#21152;&#39069;&#22806;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel training paradigm, audio difference learning, for improving audio captioning. The fundamental concept of the proposed learning method is to create a feature representation space that preserves the relationship between audio, enabling the generation of captions that detail intricate audio information. This method employs a reference audio along with the input audio, both of which are transformed into feature representations via a shared encoder. Captions are then generated from these differential features to describe their differences. Furthermore, a unique technique is proposed that involves mixing the input audio with additional audio, and using the additional audio as a reference. This results in the difference between the mixed audio and the reference audio reverting back to the original input audio. This allows the original input's caption to be used as the caption for their difference, eliminating the need for additional annotations for the difference
&lt;/p&gt;</description></item><item><title>PromptTTS++&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2309.08140</link><description>&lt;p&gt;
PromptTTS++&#65306;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#25552;&#31034;&#24335;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08140
&lt;/p&gt;
&lt;p&gt;
PromptTTS++&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PromptTTS++&#65292;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#20026;&#20102;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;TTS&#26694;&#26550;&#20013;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35828;&#35805;&#32773;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#35813;&#25552;&#31034;&#25551;&#36848;&#20102;&#35821;&#38899;&#29305;&#24449;&#65288;&#22914;&#20013;&#24615;&#12289;&#24180;&#36731;&#12289;&#32769;&#24180;&#21644;&#27785;&#38391;&#65289;&#65292;&#26088;&#22312;&#19982;&#35828;&#35805;&#39118;&#26684;&#22823;&#33268;&#29420;&#31435;&#12290;&#30001;&#20110;&#30446;&#21069;&#27809;&#26377;&#21253;&#21547;&#35828;&#35805;&#32773;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;LibriTTS-R&#35821;&#26009;&#24211;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#35828;&#35805;&#32773;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#22768;&#23398;&#27169;&#22411;&#19982;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#26469;&#24314;&#27169;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22810;&#26679;&#21270;&#35828;&#35805;&#32773;&#22240;&#32032;&#12290;&#19982;&#20043;&#21069;&#20165;&#20381;&#36182;&#26679;&#24335;&#25552;&#31034;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26679;&#24335;&#25552;&#31034;&#20165;&#25551;&#36848;&#20102;&#35828;&#35805;&#32773;&#20010;&#24615;&#21270;&#30340;&#26377;&#38480;&#26041;&#38754;&#65292;&#22914;&#38899;&#35843;&#12289;&#35828;&#35805;&#36895;&#24230;&#21644;&#33021;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39069;&#22806;&#30340;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21040;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system that allows control over speaker identity using natural language descriptions. To control speaker identity within the prompt-based TTS framework, we introduce the concept of speaker prompt, which describes voice characteristics (e.g., gender-neutral, young, old, and muffled) designed to be approximately independent of speaking style. Since there is no large-scale dataset containing speaker prompts, we first construct a dataset based on the LibriTTS-R corpus with manually annotated speaker prompts. We then employ a diffusion-based acoustic model with mixture density networks to model diverse speaker factors in the training data. Unlike previous studies that rely on style prompts describing only a limited aspect of speaker individuality, such as pitch, speaking speed, and energy, our method utilizes an additional speaker prompt to effectively learn the mapping from natural language descriptions to the acoustic f
&lt;/p&gt;</description></item><item><title>Oobleck&#37319;&#29992;&#27969;&#27700;&#32447;&#27169;&#26495;&#21644;&#24050;&#22797;&#21046;&#27169;&#22411;&#29366;&#24577;&#26469;&#23454;&#29616;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36164;&#28304;&#21644;&#24555;&#36895;&#24674;&#22797;&#26469;&#25552;&#20379;&#39640;&#21534;&#21520;&#37327;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;Oobleck&#22312;&#21534;&#21520;&#37327;&#19978;&#32988;&#36807;&#20102;Bamboo&#21644;Varuna&#31561;&#26368;&#20808;&#36827;&#30340;&#23481;&#38169;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.08125</link><description>&lt;p&gt;
Oobleck&#65306;&#20351;&#29992;&#27969;&#27700;&#32447;&#27169;&#26495;&#23454;&#29616;&#22823;&#22411;&#27169;&#22411;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates. (arXiv:2309.08125v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08125
&lt;/p&gt;
&lt;p&gt;
Oobleck&#37319;&#29992;&#27969;&#27700;&#32447;&#27169;&#26495;&#21644;&#24050;&#22797;&#21046;&#27169;&#22411;&#29366;&#24577;&#26469;&#23454;&#29616;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36164;&#28304;&#21644;&#24555;&#36895;&#24674;&#22797;&#26469;&#25552;&#20379;&#39640;&#21534;&#21520;&#37327;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;Oobleck&#22312;&#21534;&#21520;&#37327;&#19978;&#32988;&#36807;&#20102;Bamboo&#21644;Varuna&#31561;&#26368;&#20808;&#36827;&#30340;&#23481;&#38169;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Oobleck&#36890;&#36807;&#37319;&#29992;&#35268;&#23450;&#30340;&#23481;&#38169;&#29575;&#65292;&#21487;&#23454;&#29616;&#23545;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#23427;&#37319;&#29992;&#20102;&#35268;&#21010;-&#25191;&#34892;&#30340;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#39318;&#20808;&#29983;&#25104;&#19968;&#32452;&#24322;&#26500;&#30340;&#27969;&#27700;&#32447;&#27169;&#26495;&#65292;&#24182;&#23454;&#20363;&#21270;&#33267;&#23569;$ f + 1 $&#20010;&#36923;&#36753;&#31561;&#25928;&#30340;&#27969;&#27700;&#32447;&#21103;&#26412;&#65292;&#20197;&#23481;&#32435;&#20219;&#20309;$f$&#20010;&#21516;&#26102;&#25925;&#38556;&#12290;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#65292;&#23427;&#20381;&#36182;&#20110;&#36328;&#21103;&#26412;&#30340;&#24050;&#22797;&#21046;&#27169;&#22411;&#29366;&#24577;&#26469;&#25552;&#20379;&#24555;&#36895;&#24674;&#22797;&#12290;Oobleck&#21487;&#20197;&#21487;&#38752;&#22320;&#20445;&#35777;&#22312;$f$&#20010;&#25110;&#26356;&#23569;&#30340;&#21516;&#26102;&#25925;&#38556;&#21518;&#65292;&#21021;&#22987;&#21019;&#24314;&#30340;&#27969;&#27700;&#32447;&#27169;&#26495;&#30340;&#26576;&#31181;&#32452;&#21512;&#21487;&#20197;&#29992;&#20110;&#35206;&#30422;&#25152;&#26377;&#21487;&#29992;&#36164;&#28304;&#65292;&#20174;&#32780;&#22987;&#32456;&#36991;&#20813;&#36164;&#28304;&#38386;&#32622;&#12290;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Oobleck&#25552;&#20379;&#20102;&#19968;&#33268;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#19988;&#22312;&#21534;&#21520;&#37327;&#19978;&#32988;&#36807;&#20102;Bamboo&#21644;Varuna&#31561;&#26368;&#20808;&#36827;&#30340;&#23481;&#38169;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oobleck enables resilient distributed training of large DNN models with guaranteed fault tolerance. It takes a planning-execution co-design approach, where it first generates a set of heterogeneous pipeline templates and instantiates at least $f+1$ logically equivalent pipeline replicas to tolerate any $f$ simultaneous failures. During execution, it relies on already-replicated model states across the replicas to provide fast recovery. Oobleck provably guarantees that some combination of the initially created pipeline templates can be used to cover all available resources after $f$ or fewer simultaneous failures, thereby avoiding resource idling at all times. Evaluation on large DNN models with billions of parameters shows that Oobleck provides consistently high throughput, and it outperforms state-of-the-art fault tolerance solutions like Bamboo and Varuna by up to $13.9x$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LCR-Net&#30340;&#22810;&#22836;&#32593;&#32476;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#23039;&#24577;&#24863;&#30693;&#26426;&#21046;&#26469;&#24555;&#36895;&#20934;&#30830;&#22320;&#22788;&#29702;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LCR-Net&#22312;&#20505;&#36873;&#26816;&#32034;&#12289;&#38381;&#29615;&#28857;&#20113;&#37197;&#20934;&#21644;&#22810;&#25968;&#25454;&#38598;&#36830;&#32493;&#20877;&#23450;&#20301;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08086</link><description>&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#30340;&#28145;&#24230;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;LiDAR SLAM
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Deep Loop Closing and Relocalization for Reliable LiDAR SLAM. (arXiv:2309.08086v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LCR-Net&#30340;&#22810;&#22836;&#32593;&#32476;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#23039;&#24577;&#24863;&#30693;&#26426;&#21046;&#26469;&#24555;&#36895;&#20934;&#30830;&#22320;&#22788;&#29702;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LCR-Net&#22312;&#20505;&#36873;&#26816;&#32034;&#12289;&#38381;&#29615;&#28857;&#20113;&#37197;&#20934;&#21644;&#22810;&#25968;&#25454;&#38598;&#36830;&#32493;&#20877;&#23450;&#20301;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#26159;&#24314;&#31435;&#21487;&#38752;&#21644;&#31283;&#23450;&#30340;&#38271;&#26399;SLAM&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#23039;&#24577;&#20272;&#35745;&#28418;&#31227;&#21644;&#36864;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#34920;&#36848;&#20102;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22836;&#32593;&#32476;LCR-Net&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#23039;&#24577;&#24863;&#30693;&#26426;&#21046;&#26469;&#31934;&#30830;&#20272;&#35745;LiDAR&#25195;&#25551;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;6&#33258;&#30001;&#24230;&#23039;&#24577;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;LCR-Net&#25972;&#21512;&#21040;&#19968;&#20010;SLAM&#31995;&#32479;&#20013;&#65292;&#22312;&#23460;&#22806;&#39550;&#39542;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#31283;&#20581;&#20934;&#30830;&#30340;&#22312;&#32447;LiDAR SLAM&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#20174;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#24471;&#20986;&#30340;&#35774;&#32622;&#65292;&#21253;&#25324;&#20505;&#36873;&#26816;&#32034;&#12289;&#38381;&#29615;&#28857;&#20113;&#37197;&#20934;&#21644;&#22810;&#25968;&#25454;&#38598;&#30340;&#36830;&#32493;&#20877;&#23450;&#20301;&#65292;&#23545;LCR-Net&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LCR-Net&#22312;&#36825;&#19977;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loop closing and relocalization are crucial techniques to establish reliable and robust long-term SLAM by addressing pose estimation drift and degeneration. This article begins by formulating loop closing and relocalization within a unified framework. Then, we propose a novel multi-head network LCR-Net to tackle both tasks effectively. It exploits novel feature extraction and pose-aware attention mechanism to precisely estimate similarities and 6-DoF poses between pairs of LiDAR scans. In the end, we integrate our LCR-Net into a SLAM system and achieve robust and accurate online LiDAR SLAM in outdoor driving environments. We thoroughly evaluate our LCR-Net through three setups derived from loop closing and relocalization, including candidate retrieval, closed-loop point cloud registration, and continuous relocalization using multiple datasets. The results demonstrate that LCR-Net excels in all three tasks, surpassing the state-of-the-art methods and exhibiting a remarkable generalizati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#21040;&#20840;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#20801;&#35768;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#65292;&#24182;&#22312;&#20445;&#30041;&#25968;&#25454;&#38598;&#37051;&#22495;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#23558;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#32858;&#38598;&#20998;&#24320;&#12290;</title><link>http://arxiv.org/abs/2309.08077</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#30417;&#30563;&#22411;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Supervised Stochastic Neighbor Embedding Using Contrastive Learning. (arXiv:2309.08077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#21040;&#20840;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#20801;&#35768;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#65292;&#24182;&#22312;&#20445;&#30041;&#25968;&#25454;&#38598;&#37051;&#22495;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#23558;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#32858;&#38598;&#20998;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;SNE&#65289;&#26041;&#27861;t-SNE&#21644;UMAP&#26159;&#20004;&#31181;&#24120;&#29992;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#38477;&#32500;&#26041;&#27861;&#12290;&#23545;&#27604;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SSCL&#65289;&#65292;&#22312;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23884;&#20837;&#29305;&#24449;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#22312;&#20445;&#30041;&#25968;&#25454;&#38598;&#37051;&#22495;&#20449;&#24687;&#30340;&#33539;&#22260;&#20869;&#65292;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#21040;&#20840;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#12290;&#22312;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#23558;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#23558;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#32858;&#38598;&#20998;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic neighbor embedding (SNE) methods $t$-SNE, UMAP are two most popular dimensionality reduction methods for data visualization. Contrastive learning, especially self-supervised contrastive learning (SSCL), has showed great success in embedding features from unlabeled data. The conceptual connection between SNE and SSCL has been exploited. In this work, within the scope of preserving neighboring information of a dataset, we extend the self-supervised contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of samples belonging to the same class are pulled together in low-dimensional embedding space, while simultaneously pushing apart clusters of samples from different classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#36845;&#20195;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;&#36317;&#31163;&#30340;Fr&#233;chet&#22343;&#20540;&#26500;&#24314;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#19968;&#33268;&#24615;&#20998;&#21106;&#12290;&#19982;STAPLE&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#22270;&#20687;&#32972;&#26223;&#22823;&#23567;&#21644;&#20808;&#39564;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.08066</link><description>&lt;p&gt;
&#22522;&#20110;&#21551;&#21457;&#24335;&#36845;&#20195;&#20248;&#21270;&#30340;&#24418;&#24577;&#23398;&#24863;&#30693;&#19968;&#33268;&#24615;&#35745;&#31639;&#65288;MACCHIatO&#65289;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Morphologically-Aware Consensus Computation via Heuristics-based IterATive Optimization (MACCHIatO). (arXiv:2309.08066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#36845;&#20195;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;&#36317;&#31163;&#30340;Fr&#233;chet&#22343;&#20540;&#26500;&#24314;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#19968;&#33268;&#24615;&#20998;&#21106;&#12290;&#19982;STAPLE&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#22270;&#20687;&#32972;&#26223;&#22823;&#23567;&#21644;&#20808;&#39564;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#20010;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#25513;&#27169;&#20013;&#25552;&#21462;&#19968;&#33268;&#24615;&#20998;&#21106;&#26159;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#22914;&#35299;&#26512;&#26631;&#27880;&#32773;&#38388;&#30340;&#24046;&#24322;&#24615;&#25110;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#34701;&#21512;&#12290;&#26412;&#25991;&#39318;&#20808;&#35777;&#26126;&#20102;STAPLE&#31639;&#27861;&#30340;&#36755;&#20986;&#21463;&#21040;&#22270;&#20687;&#32972;&#26223;&#22823;&#23567;&#21644;&#20808;&#39564;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#36317;&#31163;&#30340;Fr&#233;chet&#22343;&#20540;&#26500;&#24314;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#19968;&#33268;&#24615;&#20998;&#21106;&#65292;&#20351;&#20854;&#23436;&#20840;&#29420;&#31435;&#20110;&#22270;&#20687;&#32972;&#26223;&#22823;&#23567;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20248;&#21270;&#27492;&#20934;&#21017;&#65292;&#20174;&#32780;&#20351;&#19968;&#20010;&#20307;&#32032;&#30340;&#31867;&#21035;&#23436;&#20840;&#30001;&#23427;&#19982;&#19981;&#21516;&#25513;&#27169;&#30340;&#20307;&#32032;&#32423;&#36317;&#31163;&#12289;&#23427;&#25152;&#23646;&#30340;&#36830;&#36890;&#32452;&#20214;&#21644;&#20998;&#21106;&#23427;&#30340;&#26631;&#27880;&#32773;&#32452;&#20915;&#23450;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#19982;STAPLE&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extraction of consensus segmentations from several binary or probabilistic masks is important to solve various tasks such as the analysis of inter-rater variability or the fusion of several neural network outputs. One of the most widely used methods to obtain such a consensus segmentation is the STAPLE algorithm. In this paper, we first demonstrate that the output of that algorithm is heavily impacted by the background size of images and the choice of the prior. We then propose a new method to construct a binary or a probabilistic consensus segmentation based on the Fr\'{e}chet means of carefully chosen distances which makes it totally independent of the image background size. We provide a heuristic approach to optimize this criterion such that a voxel's class is fully determined by its voxel-wise distance to the different masks, the connected component it belongs to and the group of raters who segmented it. We compared extensively our method on several datasets with the STAPLE met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Wave-RNN (wRNN)&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26053;&#34892;&#27874;&#26426;&#21046;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#24182;&#22312;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#20013;&#27604;&#27874;&#21160;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.08045</link><description>&lt;p&gt;
&#26053;&#34892;&#27874;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#24182;&#22686;&#24378;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Traveling Waves Encode the Recent Past and Enhance Sequence Learning. (arXiv:2309.08045v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Wave-RNN (wRNN)&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26053;&#34892;&#27874;&#26426;&#21046;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#24182;&#22312;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#20013;&#27604;&#27874;&#21160;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27963;&#21160;&#30340;&#26053;&#34892;&#27874;&#29616;&#35937;&#22312;&#22823;&#33041;&#30340;&#19981;&#21516;&#21306;&#22495;&#21644;&#23610;&#24230;&#19978;&#37117;&#26377;&#25152;&#35266;&#23519;&#21040;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#35282;&#33394;&#19978;&#30340;&#20855;&#20307;&#20316;&#29992;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#30382;&#36136;&#23618;&#21487;&#20197;&#20687;&#27874;&#21160;&#22330;&#19968;&#26679;&#65292;&#36890;&#36807;&#27839;&#30528;&#30382;&#36136;&#34920;&#38754;&#20256;&#25773;&#30340;&#27874;&#21160;&#26469;&#23384;&#20648;&#39034;&#24207;&#21050;&#28608;&#30340;&#30701;&#26399;&#35760;&#24518;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#31616;&#21333;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33021;&#22815;&#23637;&#29616;&#20986;&#36825;&#31181;&#27874;&#21160;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#24819;&#27861;&#30340;&#35745;&#31639;&#24847;&#20041;&#19968;&#30452;&#26159;&#20551;&#35774;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Wave-RNN (wRNN)&#65292;&#24182;&#23637;&#31034;&#20102;&#36830;&#36890;&#24615;&#32422;&#26463;&#21644;&#21021;&#22987;&#21270;&#22312;&#27874;&#21160;&#21160;&#21147;&#23398;&#20986;&#29616;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#20102;&#36825;&#26679;&#30340;&#26550;&#26500;&#30340;&#30830;&#36890;&#36807;&#19968;&#31995;&#21015;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;wRNN&#27604;&#27874;&#21160;&#27169;&#22411;&#23398;&#20064;&#26356;&#24555;&#12289;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically grounded hypothesis suggests that the cortical sheet may act like a wave-field capable of storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how both connectivity constraints and initialization play a crucial role in the emergence of wave-like dynamics. We then empirically show how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and perform significantly better than wave-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#33539;&#24335;&#19979;&#65292;&#36890;&#36807;&#20998;&#26512;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#26435;&#37325;&#20445;&#25345;&#22312;&#21021;&#22987;&#20301;&#32622;&#38468;&#36817;&#65292;&#21322;&#24452;&#19982;&#22238;&#24402;&#20989;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;NTK&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#31243;&#24230;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.08044</link><description>&lt;p&gt;
&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#20010;&#31070;&#32463;&#20803;&#65311;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#27973;&#23618;&#32593;&#32476;&#30340;&#31934;&#32454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How many Neurons do we need? A refined Analysis for Shallow Networks trained with Gradient Descent. (arXiv:2309.08044v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#33539;&#24335;&#19979;&#65292;&#36890;&#36807;&#20998;&#26512;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#26435;&#37325;&#20445;&#25345;&#22312;&#21021;&#22987;&#20301;&#32622;&#38468;&#36817;&#65292;&#21322;&#24452;&#19982;&#22238;&#24402;&#20989;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;NTK&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#31243;&#24230;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#33539;&#24335;&#19979;&#65292;&#20998;&#26512;&#20102;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;&#23545;&#20110;&#26089;&#20572;&#30340;GD&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#36895;&#24230;&#65292;&#36825;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26694;&#26550;&#20013;&#24050;&#30693;&#26159;&#26368;&#23567;&#20540;&#30340;&#26368;&#20248;&#35299;&#12290;&#22312;&#36825;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#36861;&#36394;&#20102;&#27867;&#21270;&#25152;&#38656;&#30340;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#26435;&#37325;&#20445;&#25345;&#22312;&#21021;&#22987;&#20301;&#32622;&#38468;&#36817;&#30340;&#24773;&#20917;&#65292;&#20854;&#21322;&#24452;&#21462;&#20915;&#20110;&#22238;&#24402;&#20989;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;&#19982;NTK&#30456;&#20851;&#32852;&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the generalization properties of two-layer neural networks in the neural tangent kernel (NTK) regime, trained with gradient descent (GD). For early stopped GD we derive fast rates of convergence that are known to be minimax optimal in the framework of non-parametric regression in reproducing kernel Hilbert spaces. On our way, we precisely keep track of the number of hidden neurons required for generalization and improve over existing results. We further show that the weights during training remain in a vicinity around initialization, the radius being dependent on structural assumptions such as degree of smoothness of the regression function and eigenvalue decay of the integral operator associated to the NTK.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#38750;&#38543;&#26426;&#32570;&#22833;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#30340;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#38477;&#20302;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Heckman-FA&#26694;&#26550;&#26469;&#33719;&#21462;&#24688;&#24403;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.08043</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;Heckman&#36873;&#25321;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#29305;&#24449;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Prediction Feature Assignment in the Heckman Selection Model. (arXiv:2309.08043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#38750;&#38543;&#26426;&#32570;&#22833;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#30340;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#38477;&#20302;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Heckman-FA&#26694;&#26550;&#26469;&#33719;&#21462;&#24688;&#24403;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#38543;&#26426;&#32570;&#22833;&#65288;MNAR&#65289;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#24448;&#24448;&#19979;&#38477;&#12290;&#26412;&#25991;&#20851;&#27880;MNAR&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#30340;&#19968;&#20010;&#32463;&#20856;&#20363;&#23376;&#65292;&#21363;&#19968;&#37096;&#20998;&#26679;&#26412;&#20855;&#26377;&#38750;&#38543;&#26426;&#32570;&#22833;&#32467;&#26524;&#12290;Heckman&#36873;&#25321;&#27169;&#22411;&#21450;&#20854;&#21464;&#31181;&#36890;&#24120;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#31867;&#22411;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#12290;Heckman&#27169;&#22411;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#26041;&#31243;&#26469;&#27169;&#25311;&#26679;&#26412;&#30340;&#39044;&#27979;&#21644;&#36873;&#25321;&#65292;&#20854;&#20013;&#36873;&#25321;&#29305;&#24449;&#21253;&#25324;&#25152;&#26377;&#39044;&#27979;&#29305;&#24449;&#12290;&#22312;&#20351;&#29992;Heckman&#27169;&#22411;&#26102;&#65292;&#24517;&#39035;&#20174;&#36873;&#25321;&#29305;&#24449;&#38598;&#20013;&#27491;&#30830;&#36873;&#25321;&#39044;&#27979;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;Heckman&#27169;&#22411;&#26469;&#35828;&#65292;&#36873;&#25321;&#27491;&#30830;&#30340;&#39044;&#27979;&#29305;&#24449;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#24403;&#36873;&#25321;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#22810;&#26102;&#12290;&#29616;&#26377;&#30340;&#20351;&#29992;Heckman&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#19968;&#20010;&#25163;&#21160;&#36873;&#25321;&#30340;&#39044;&#27979;&#29305;&#24449;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Heckman-FA&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#26469;&#33719;&#24471;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under missing-not-at-random (MNAR) sample selection bias, the performance of a prediction model is often degraded. This paper focuses on one classic instance of MNAR sample selection bias where a subset of samples have non-randomly missing outcomes. The Heckman selection model and its variants have commonly been used to handle this type of sample selection bias. The Heckman model uses two separate equations to model the prediction and selection of samples, where the selection features include all prediction features. When using the Heckman model, the prediction features must be properly chosen from the set of selection features. However, choosing the proper prediction features is a challenging task for the Heckman model. This is especially the case when the number of selection features is large. Existing approaches that use the Heckman model often provide a manually chosen set of prediction features. In this paper, we propose Heckman-FA as a novel data-driven framework for obtaining pr
&lt;/p&gt;</description></item><item><title>USM-SCD&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08023</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65288;USM-SCD&#65289;
&lt;/p&gt;
&lt;p&gt;
USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models. (arXiv:2309.08023v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08023
&lt;/p&gt;
&lt;p&gt;
USM-SCD&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65288;USM-SCD&#65289;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#19968;&#20010;&#32463;&#36807;&#22823;&#37327;&#21463;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25968;&#25454;&#35757;&#32451;&#30340;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#32780;&#26469;&#30340;&#65292;&#23637;&#31034;&#20102;&#20174;&#22823;&#22411;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;USM-SCD&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#26469;&#33258;96&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#26500;&#25104;&#30340;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;75%&#30340;&#24179;&#22343;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;F1&#24471;&#20998;&#12290;&#22312;&#32654;&#24335;&#33521;&#35821;&#19978;&#65292;USM-SCD&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#20844;&#20849;&#21644;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;85.8%&#30340;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;F1&#24471;&#20998;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#21333;&#35821;&#35328;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;21%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21482;&#38656;&#35201;&#24494;&#35843;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#22235;&#20998;&#20043;&#19968;&#23601;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a multilingual speaker change detection model (USM-SCD) that can simultaneously detect speaker turns and perform ASR for 96 languages. This model is adapted from a speech foundation model trained on a large quantity of supervised and unsupervised data, demonstrating the utility of fine-tuning from a large generic foundation model for a downstream task. We analyze the performance of this multilingual speaker change detection model through a series of ablation studies. We show that the USM-SCD model can achieve more than 75% average speaker change detection F1 score across a test set that consists of data from 96 languages. On American English, the USM-SCD model can achieve an 85.8% speaker change detection F1 score across various public and internal test sets, beating the previous monolingual baseline model by 21% relative. We also show that we only need to fine-tune one-quarter of the trainable model parameters to achieve the best model performance. The USM-SCD model exhib
&lt;/p&gt;</description></item><item><title>CRYPTO-MINE&#26159;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#36873;&#25321;&#26126;&#25991;&#25915;&#20987;&#20013;&#26126;&#25991;&#21644;&#23494;&#25991;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20998;&#26512;&#23494;&#30721;&#31995;&#32479;&#30340;&#35745;&#31639;&#23433;&#20840;&#24615;&#21644;&#20449;&#24687;&#27844;&#38706;&#19982;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.08019</link><description>&lt;p&gt;
CRYPTO-MINE: &#36890;&#36807;&#20114;&#20449;&#24687;&#31070;&#32463;&#20272;&#35745;&#36827;&#34892;&#23494;&#30721;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CRYPTO-MINE: Cryptanalysis via Mutual Information Neural Estimation. (arXiv:2309.08019v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08019
&lt;/p&gt;
&lt;p&gt;
CRYPTO-MINE&#26159;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#36873;&#25321;&#26126;&#25991;&#25915;&#20987;&#20013;&#26126;&#25991;&#21644;&#23494;&#25991;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20998;&#26512;&#23494;&#30721;&#31995;&#32479;&#30340;&#35745;&#31639;&#23433;&#20840;&#24615;&#21644;&#20449;&#24687;&#27844;&#38706;&#19982;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20316;&#20026;&#35780;&#20272;&#23494;&#30721;&#31995;&#32479;&#25928;&#29575;&#30340;&#25351;&#26631;&#20855;&#26377;&#24191;&#27867;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20272;&#35745;&#26410;&#30693;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20114;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20114;&#20449;&#24687;&#20272;&#35745;&#22312;&#23494;&#30721;&#23398;&#39046;&#22495;&#30340;&#26032;&#24212;&#29992;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36825;&#31181;&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26126;&#25991;&#25915;&#20987;&#20013;&#26126;&#25991;&#21644;&#23494;&#25991;&#20043;&#38388;&#30340;&#20272;&#35745;&#20114;&#20449;&#24687;&#12290;&#22914;&#26524;&#26377;&#30340;&#35805;&#65292;&#21152;&#23494;&#20013;&#30340;&#27844;&#38706;&#20449;&#24687;&#21487;&#33021;&#20250;&#34987;&#23545;&#25163;&#21033;&#29992;&#26469;&#30772;&#22351;&#23494;&#30721;&#31995;&#32479;&#30340;&#35745;&#31639;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#20010;&#21152;&#23494;&#26041;&#26696;&#21644;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#26469;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#23545;&#25552;&#20379;&#20010;&#20307;&#20445;&#23494;&#24615;&#30340;&#22522;&#20110;&#32593;&#32476;&#32534;&#30721;&#30340;&#23494;&#30721;&#31995;&#32479;&#30340;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#20449;&#24687;&#27844;&#38706;&#21644;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Mutual Information (MI) as a measure to evaluate the efficiency of cryptosystems has an extensive history. However, estimating MI between unknown random variables in a high-dimensional space is challenging. Recent advances in machine learning have enabled progress in estimating MI using neural networks. This work presents a novel application of MI estimation in the field of cryptography. We propose applying this methodology directly to estimate the MI between plaintext and ciphertext in a chosen plaintext attack. The leaked information, if any, from the encryption could potentially be exploited by adversaries to compromise the computational security of the cryptosystem. We evaluate the efficiency of our approach by empirically analyzing multiple encryption schemes and baseline approaches. Furthermore, we extend the analysis to novel network coding-based cryptosystems that provide individual secrecy and study the relationship between information leakage and input distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#23792;&#20540;&#27169;&#24335;&#12290;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07992</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#20869;&#30740;&#31350;&#27969;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#23792;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone. (arXiv:2309.07992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#23792;&#20540;&#27169;&#24335;&#12290;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#27700;&#25991;&#23398;&#23478;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#35782;&#21035;&#23792;&#20540;&#27169;&#24335;&#24322;&#24120;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#25110;&#33258;&#28982;&#29616;&#35937;&#24341;&#36215;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#23384;&#22312;&#25361;&#25112;&#65292;&#20363;&#22914;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#22522;&#20934;&#21644;&#36873;&#25321;&#26368;&#36866;&#21512;&#32473;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#21512;&#25104;&#30340;&#23792;&#20540;&#27169;&#24335;&#27880;&#20837;&#21040;&#21512;&#25104;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#33258;&#21160;&#21270;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#12290;&#35813;&#26426;&#21046;&#20174;&#20116;&#31181;&#36873;&#25321;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#26550;&#26500;&#21644;&#35757;&#32451;&#21442;&#25968;&#30340;&#20248;&#21270;&#27169;&#22411;&#23454;&#20363;&#65292;&#21363;&#26102;&#24207;&#21367;&#31215;&#32593;&#32476;&#65288;
&lt;/p&gt;
&lt;p&gt;
This paper presents an automated machine learning framework designed to assist hydrologists in detecting anomalies in time series data generated by sensors in a research watershed in the northeastern United States critical zone. The framework specifically focuses on identifying peak-pattern anomalies, which may arise from sensor malfunctions or natural phenomena. However, the use of classification methods for anomaly detection poses challenges, such as the requirement for labeled data as ground truth and the selection of the most suitable deep learning model for the given task and dataset. To address these challenges, our framework generates labeled datasets by injecting synthetic peak patterns into synthetically generated time series data and incorporates an automated hyperparameter optimization mechanism. This mechanism generates an optimized model instance with the best architectural and training parameters from a pool of five selected models, namely Temporal Convolutional Network (
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25240;&#21472;&#27880;&#24847;&#21147;&#30340;&#25216;&#26415;&#65292;&#22312;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23569;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;24%&#12289;&#21151;&#32791;&#20943;&#23567;23%&#12290;</title><link>http://arxiv.org/abs/2309.07988</link><description>&lt;p&gt;
&#25240;&#21472;&#27880;&#24847;&#21147;&#65306;&#38754;&#21521;&#35774;&#22791;&#30340;Transformer&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#20869;&#23384;&#21644;&#21151;&#32791;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition. (arXiv:2309.07988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25240;&#21472;&#27880;&#24847;&#21147;&#30340;&#25216;&#26415;&#65292;&#22312;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23569;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;24%&#12289;&#21151;&#32791;&#20943;&#23567;23%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#20248;&#21270;Transformer&#25512;&#26029;&#30340;&#21162;&#21147;&#65292;&#36890;&#24120;&#38024;&#23545;&#38271;&#19978;&#19979;&#25991;&#24212;&#29992;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21270;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#19978;&#12290;&#28982;&#32780;&#65292;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36890;&#24120;&#27599;&#27425;&#21482;&#22788;&#29702;&#26377;&#38480;&#25968;&#37327;&#30340;&#20196;&#29260;&#65292;&#22240;&#27492;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24182;&#19981;&#26159;&#29942;&#39048;&#25152;&#22312;&#12290;&#30456;&#21453;&#65292;&#29942;&#39048;&#22312;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#32447;&#24615;&#25237;&#24433;&#23618;&#65292;&#23427;&#20204;&#26500;&#25104;&#20102;&#27169;&#22411;&#22823;&#23567;&#30340;&#30456;&#24403;&#37096;&#20998;&#65292;&#24182;&#23545;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#21151;&#32791;&#30340;&#20351;&#29992;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25240;&#21472;&#27880;&#24847;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#32447;&#24615;&#23618;&#30340;&#25216;&#26415;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;&#35774;&#22791;&#19978;&#30340;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25240;&#21472;&#27880;&#24847;&#21147;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#65288;&#21644;&#30456;&#24212;&#30340;&#20869;&#23384;&#28040;&#32791;&#65289;&#20943;&#23567;&#22810;&#36798;24%&#65292;&#24182;&#23558;&#21151;&#32791;&#20943;&#23567;&#22810;&#36798;23%&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models excel in speech recognition. Existing efforts to optimize Transformer inference, typically for long-context applications, center on simplifying attention score calculations. However, streaming speech recognition models usually process a limited number of tokens each time, making attention score calculation less of a bottleneck. Instead, the bottleneck lies in the linear projection layers of multi-head attention and feedforward networks, constituting a substantial portion of the model size and contributing significantly to computation, memory, and power usage.  To address this bottleneck, we propose folding attention, a technique targeting these linear layers, significantly reducing model size and improving memory and power efficiency. Experiments on on-device Transformer-based streaming speech recognition models show that folding attention reduces model size (and corresponding memory consumption) by up to 24% and power consumption by up to 23%, all without comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07986</link><description>&lt;p&gt;
&#35266;&#28857;&#25991;&#26412;&#20498;&#32622;&#65306;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#37322;&#25918;&#26032;&#39062;&#30340;&#35270;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models. (arXiv:2309.07986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20165;&#36890;&#36807;2D&#30417;&#30563;&#26469;&#34920;&#31034;&#19990;&#30028;&#30340;&#30495;&#23454;3D&#32467;&#26500;&#65311;&#25105;&#20204;&#35777;&#26126;&#65292;&#26159;&#30340;&#65292;3D&#30693;&#35782;&#34987;&#32534;&#30721;&#22312;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#65292;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#26144;&#23556;&#22120;&#65292;&#29992;&#20110;&#33719;&#21462;&#30456;&#26426;&#35270;&#28857;&#21442;&#25968;&#24182;&#39044;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#21521;&#37327;&#65307;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#28508;&#22312;&#21521;&#37327;&#26469;&#35843;&#25972;&#25193;&#25955;&#29983;&#25104;&#36807;&#31243;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#30456;&#26426;&#35270;&#28857;&#30340;&#22270;&#20687;&#12290;ViewNeTI&#33258;&#28982;&#35299;&#20915;&#20102;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#34987;&#20923;&#32467;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#36755;&#20837;&#35270;&#22270;&#26469;&#35299;&#20915;NVS&#38382;&#39064;&#65307;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#21333;&#35270;&#22270;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21333;&#35270;&#22270;NVS&#39044;&#27979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models understand spatial relationship between objects, but do they represent the true 3D structure of the world from only 2D supervision? We demonstrate that yes, 3D knowledge is encoded in 2D image diffusion models like Stable Diffusion, and we show that this structure can be exploited for 3D vision tasks. Our method, Viewpoint Neural Textual Inversion (ViewNeTI), controls the 3D viewpoint of objects in generated images from frozen diffusion models. We train a small neural mapper to take camera viewpoint parameters and predict text encoder latents; the latents then condition the diffusion generation process to produce images with the desired camera viewpoint.  ViewNeTI naturally addresses Novel View Synthesis (NVS). By leveraging the frozen diffusion model as a prior, we can solve NVS with very few input views; we can even do single-view novel view synthesis. Our single-view NVS predictions have good semantic details and photorealism compared to prior methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SLMIA-SR&#65292;&#36825;&#26159;&#38024;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#30340;&#31532;&#19968;&#20010;&#38024;&#23545;&#35828;&#35805;&#20154;&#32423;&#21035;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#19982;&#20256;&#32479;&#30340;&#31034;&#20363;&#32423;&#25915;&#20987;&#19981;&#21516;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#30830;&#23450;&#19968;&#20010;&#32473;&#23450;&#30340;&#22768;&#38899;&#26159;&#21542;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#22768;&#38899;&#26377;&#20851;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#30456;&#21516;&#12290;&#36825;&#23545;&#23454;&#36341;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#25512;&#26029;&#22768;&#38899;&#36890;&#24120;&#26159;&#19981;&#21516;&#30340;&#65292;&#32780;&#19988;&#32771;&#34385;&#21040;&#35828;&#35805;&#20154;&#35782;&#21035;&#30340;&#24320;&#25918;&#24615;&#36136;&#65292;&#20063;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07983</link><description>&lt;p&gt;
SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems
&lt;/p&gt;
&lt;p&gt;
SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems. (arXiv:2309.07983v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SLMIA-SR&#65292;&#36825;&#26159;&#38024;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#30340;&#31532;&#19968;&#20010;&#38024;&#23545;&#35828;&#35805;&#20154;&#32423;&#21035;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#19982;&#20256;&#32479;&#30340;&#31034;&#20363;&#32423;&#25915;&#20987;&#19981;&#21516;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#30830;&#23450;&#19968;&#20010;&#32473;&#23450;&#30340;&#22768;&#38899;&#26159;&#21542;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#22768;&#38899;&#26377;&#20851;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#30456;&#21516;&#12290;&#36825;&#23545;&#23454;&#36341;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#25512;&#26029;&#22768;&#38899;&#36890;&#24120;&#26159;&#19981;&#21516;&#30340;&#65292;&#32780;&#19988;&#32771;&#34385;&#21040;&#35828;&#35805;&#20154;&#35782;&#21035;&#30340;&#24320;&#25918;&#24615;&#36136;&#65292;&#20063;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#31034;&#20363;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#23454;&#20102;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36827;&#34892;&#27492;&#31867;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#30740;&#31350;&#19987;&#27880;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65288;SR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#22522;&#20110;&#22768;&#38899;&#30340;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#25216;&#26415;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLMIA-SR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;SR&#37327;&#36523;&#23450;&#21046;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#19982;&#20256;&#32479;&#30340;&#31034;&#20363;&#32423;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#29305;&#28857;&#26159;&#35828;&#35805;&#20154;&#32423;&#21035;&#30340;&#25104;&#21592;&#25512;&#26029;&#65292;&#21363;&#30830;&#23450;&#32473;&#23450;&#25512;&#26029;&#22768;&#38899;&#20013;&#26159;&#21542;&#26377;&#20219;&#20309;&#32473;&#23450;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#19982;&#32473;&#23450;&#25512;&#26029;&#22768;&#38899;&#30456;&#21516;&#12290;&#36825;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#25512;&#26029;&#22768;&#38899;&#36890;&#24120;&#26159;&#19981;&#21516;&#30340;&#65292;&#32780;&#19988;&#32771;&#34385;&#21040;SR&#30340;&#24320;&#25918;&#24615;&#36136;&#65292;&#20063;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#21363;&#35782;&#21035;&#35828;&#35805;&#20154;&#24448;&#24448;&#27809;&#26377;&#20986;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#35757;&#32451;&#30446;&#26631;&#65306;&#20869;&#37096;&#25509;&#36817;&#24230;&#21644;&#22806;&#37096;&#36828;&#31163;&#24230;&#65292;&#26469;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks allow adversaries to determine whether a particular example was contained in the model's training dataset. While previous works have confirmed the feasibility of such attacks in various applications, none has focused on speaker recognition (SR), a promising voice-based biometric recognition technique. In this work, we propose SLMIA-SR, the first membership inference attack tailored to SR. In contrast to conventional example-level attack, our attack features speaker-level membership inference, i.e., determining if any voices of a given speaker, either the same as or different from the given inference voices, have been involved in the training of a model. It is particularly useful and practical since the training and inference voices are usually distinct, and it is also meaningful considering the open-set nature of SR, namely, the recognition speakers were often not present in the training data. We utilize intra-closeness and inter-farness, two training objec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;LISTA&#20272;&#35745;&#37327;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20026;&#27169;&#22411;-based&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.07982</link><description>&lt;p&gt;
&#20026;&#23398;&#20064;&#30340;ISTA&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification for learned ISTA. (arXiv:2309.07982v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;LISTA&#20272;&#35745;&#37327;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20026;&#27169;&#22411;-based&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36870;&#38382;&#39064;&#20013;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#25968;&#20540;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#22788;&#20110;&#26368;&#21069;&#27839;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#20351;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#65292;&#22240;&#20026;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#20801;&#35768;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#20123;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25216;&#26415;&#20013;&#65292;&#31639;&#27861;&#23637;&#24320;&#26041;&#26696;&#33073;&#39062;&#32780;&#20986;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24555;&#36895;&#21457;&#23637;&#19982;&#20256;&#32479;&#30340;&#39640;&#32500;&#32479;&#35745;&#26041;&#27861;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#29702;&#35770;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;LISTA&#20272;&#35745;&#37327;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20174;&#32780;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based deep learning solutions to inverse problems have attracted increasing attention in recent years as they bridge state-of-the-art numerical performance with interpretability. In addition, the incorporated prior domain knowledge can make the training more efficient as the smaller number of parameters allows the training step to be executed with smaller datasets. Algorithm unrolling schemes stand out among these model-based learning techniques. Despite their rapid advancement and their close connection to traditional high-dimensional statistical methods, they lack certainty estimates and a theory for uncertainty quantification is still elusive. This work provides a step towards closing this gap proposing a rigorous way to obtain confidence intervals for the LISTA estimator.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#20307;&#39564;&#26234;&#33021;&#20307;&#38598;&#25104;&#30340;&#26032;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#26426;&#22120;&#25512;&#29702;&#12290;&#35813;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#21253;&#25324;&#27169;&#26495;&#21270;&#30340;&#25991;&#26412;&#26597;&#35810;&#21644;&#31572;&#26696;&#65292;&#24182;&#19982;&#32534;&#30721;&#20026;&#25968;&#25454;&#24211;&#30340;&#19990;&#30028;&#29366;&#24577;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#24403;&#21069;&#27169;&#22411;&#21487;&#20197;&#22238;&#31572;&#19968;&#20123;&#20851;&#20110;&#19990;&#30028;&#29366;&#24577;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#20854;&#20182;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.07974</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#29702;&#20307;&#39564;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Data Source for Reasoning Embodied Agents. (arXiv:2309.07974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#20307;&#39564;&#26234;&#33021;&#20307;&#38598;&#25104;&#30340;&#26032;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#26426;&#22120;&#25512;&#29702;&#12290;&#35813;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#21253;&#25324;&#27169;&#26495;&#21270;&#30340;&#25991;&#26412;&#26597;&#35810;&#21644;&#31572;&#26696;&#65292;&#24182;&#19982;&#32534;&#30721;&#20026;&#25968;&#25454;&#24211;&#30340;&#19990;&#30028;&#29366;&#24577;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#24403;&#21069;&#27169;&#22411;&#21487;&#20197;&#22238;&#31572;&#19968;&#20123;&#20851;&#20110;&#19990;&#30028;&#29366;&#24577;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#20854;&#20182;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#24471;&#30410;&#20110;&#26032;&#39062;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#21327;&#35758;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#24494;&#35843;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;&#36825;&#20123;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19982;&#20307;&#39564;&#26234;&#33021;&#20307;&#38598;&#25104;&#30340;&#26032;&#25968;&#25454;&#29983;&#25104;&#22120;&#29992;&#20110;&#26426;&#22120;&#25512;&#29702;&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#21253;&#25324;&#27169;&#26495;&#21270;&#30340;&#25991;&#26412;&#26597;&#35810;&#21644;&#31572;&#26696;&#65292;&#19982;&#32534;&#30721;&#20026;&#25968;&#25454;&#24211;&#30340;&#19990;&#30028;&#29366;&#24577;&#30456;&#21305;&#37197;&#12290;&#36825;&#20123;&#19990;&#30028;&#29366;&#24577;&#26159;&#19990;&#30028;&#21160;&#24577;&#21644;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#22522;&#20934;&#27169;&#22411;&#22312;&#35757;&#32451;&#38598;&#23454;&#20363;&#21270;&#19978;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#22522;&#20934;&#27169;&#22411;&#21253;&#25324;&#22312;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#26684;&#24335;&#21270;&#34920;&#31034;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#30693;&#35782;&#22270;&#34920;&#31034;&#30340;&#22270;&#32467;&#26500;Transformer&#19978;&#25805;&#20316;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22238;&#31572;&#19968;&#20123;&#20851;&#20110;&#19990;&#30028;&#29366;&#24577;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#20854;&#20182;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#32467;&#26524;&#26263;&#31034;&#20102;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in using machine learning models for reasoning tasks has been driven by novel model architectures, large-scale pre-training protocols, and dedicated reasoning datasets for fine-tuning. In this work, to further pursue these advances, we introduce a new data generator for machine reasoning that integrates with an embodied agent. The generated data consists of templated text queries and answers, matched with world-states encoded into a database. The world-states are a result of both world dynamics and the actions of the agent. We show the results of several baseline models on instantiations of train sets. These include pre-trained language models fine-tuned on a text-formatted representation of the database, and graph-structured Transformers operating on a knowledge-graph representation of the database. We find that these models can answer some questions about the world-state, but struggle with others. These results hint at new research directions in designing neural reaso
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#23454;&#29616;&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#21644;&#26550;&#26500;&#30340;&#36731;&#37327;&#32423;&#25509;&#21475;&#12290;&#35813;&#36719;&#20214;&#21253;&#30340;&#30446;&#26631;&#26159;&#20026;&#20449;&#21495;&#22788;&#29702;&#12289;&#24863;&#30693;&#21644;&#36890;&#20449;&#31561;&#39046;&#22495;&#25552;&#20379;&#39640;&#25928;&#30340;&#22797;&#25968;&#20540;&#27169;&#22411;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.07948</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#20449;&#21495;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Complex-Valued Neural Networks for Data-Driven Signal Processing and Signal Understanding. (arXiv:2309.07948v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#23454;&#29616;&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#21644;&#26550;&#26500;&#30340;&#36731;&#37327;&#32423;&#25509;&#21475;&#12290;&#35813;&#36719;&#20214;&#21253;&#30340;&#30446;&#26631;&#26159;&#20026;&#20449;&#21495;&#22788;&#29702;&#12289;&#24863;&#30693;&#21644;&#36890;&#20449;&#31561;&#39046;&#22495;&#25552;&#20379;&#39640;&#25928;&#30340;&#22797;&#25968;&#20540;&#27169;&#22411;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#20986;&#29616;&#65292;&#23545;&#20110;&#20449;&#21495;&#22788;&#29702;&#12289;&#24863;&#30693;&#21644;&#36890;&#20449;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#20855;&#26377;&#21331;&#36234;&#30340;&#24314;&#27169;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21457;&#23637;&#22797;&#25968;&#20540;&#27169;&#22411;&#38656;&#35201;&#24320;&#21457;&#22522;&#26412;&#30340;&#28145;&#24230;&#23398;&#20064;&#25805;&#20316;&#65292;&#20363;&#22914;&#32447;&#24615;&#25110;&#21367;&#31215;&#23618;&#65292;&#22240;&#20026;&#29616;&#20195;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;PyTorch&#21644;TensorFlow&#23545;&#20110;&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#25903;&#25345;&#19981;&#36275;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#19968;&#20010;&#24314;&#31435;&#22312;PyTorch&#19978;&#30340;&#36719;&#20214;&#21253;&#65292;&#26088;&#22312;&#23454;&#29616;&#24120;&#35265;&#22797;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#21644;&#26550;&#26500;&#30340;&#36731;&#37327;&#32423;&#25509;&#21475;&#12290;&#31867;&#20284;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#26368;&#36817;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#23556;&#39057;&#20449;&#21495;&#29702;&#35299;&#65288;RFSU&#65289;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#65292;&#23427;&#20351;&#29992;&#20449;&#21495;&#21147;&#23398;&#22522;&#30784;&#27934;&#23519;&#21147;&#19982;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#33021;&#21147;&#30340;&#28151;&#21512;&#26041;&#27861;&#25193;&#23637;&#20102;&#20256;&#32479;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#32447;&#24615;&#12289;&#21367;&#31215;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Complex-valued neural networks have emerged boasting superior modeling performance for many tasks across the signal processing, sensing, and communications arenas. However, developing complex-valued models currently demands development of basic deep learning operations, such as linear or convolution layers, as modern deep learning frameworks like PyTorch and Tensor flow do not adequately support complex-valued neural networks. This paper overviews a package built on PyTorch with the intention of implementing light-weight interfaces for common complex-valued neural network operations and architectures. Similar to natural language understanding (NLU), which as recently made tremendous leaps towards text-based intelligence, RF Signal Understanding (RFSU) is a promising field extending conventional signal processing algorithms using a hybrid approach of signal mechanics-based insight with data-driven modeling power. Notably, we include efficient implementations for linear, convolution, and
&lt;/p&gt;</description></item><item><title>TiBGL&#26159;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#30340;&#33041;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#12290;&#23427;&#20855;&#26377;&#21028;&#21035;&#21644;&#21487;&#35299;&#37322;&#33021;&#21147;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21151;&#33021;&#36830;&#25509;&#25968;&#25454;&#30340;&#26377;&#29992;&#29305;&#24449;&#26469;&#25913;&#36827;&#31070;&#32463;&#30142;&#30149;&#30340;&#35786;&#26029;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07947</link><description>&lt;p&gt;
TiBGL: &#27169;&#26495;&#24341;&#23548;&#30340;&#33041;&#22270;&#23398;&#20064;&#29992;&#20110;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TiBGL: Template-induced Brain Graph Learning for Functional Neuroimaging Analysis. (arXiv:2309.07947v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07947
&lt;/p&gt;
&lt;p&gt;
TiBGL&#26159;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#30340;&#33041;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#12290;&#23427;&#20855;&#26377;&#21028;&#21035;&#21644;&#21487;&#35299;&#37322;&#33021;&#21147;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21151;&#33021;&#36830;&#25509;&#25968;&#25454;&#30340;&#26377;&#29992;&#29305;&#24449;&#26469;&#25913;&#36827;&#31070;&#32463;&#30142;&#30149;&#30340;&#35786;&#26029;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#24050;&#25104;&#20026;&#30740;&#31350;&#20154;&#31867;&#22823;&#33041;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#30456;&#20851;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#39046;&#22495;&#20173;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#38480;&#21046;&#30528;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#21151;&#33021;&#36830;&#25509;&#25968;&#25454;&#20013;&#23384;&#22312;&#22823;&#37327;&#22122;&#38899;&#21644;&#20887;&#20313;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#33041;&#32593;&#32476;&#27169;&#22411;&#24448;&#24448;&#20559;&#21521;&#20110;&#20998;&#31867;&#24615;&#33021;&#25110;&#23545;&#23398;&#20064;&#27169;&#22411;&#32972;&#21518;&#30340;&#31070;&#32463;&#31185;&#23398;&#21457;&#29616;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#27169;&#26495;&#24341;&#23548;&#30340;&#33041;&#22270;&#23398;&#20064;&#65288;TiBGL&#65289;&#65292;&#20855;&#26377;&#21028;&#21035;&#21644;&#21487;&#35299;&#37322;&#33021;&#21147;&#12290;&#21463;&#21040;&#19982;&#21151;&#33021;&#36830;&#25509;&#30456;&#20851;&#30340;&#21307;&#23398;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;TiBGL&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#27169;&#26495;&#24341;&#23548;&#26041;&#27861;&#26469;&#23398;&#20064;&#21151;&#33021;&#36830;&#25509;&#25968;&#25454;&#30340;&#26377;&#29992;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, functional magnetic resonance imaging has emerged as a powerful tool for investigating the human brain's functional connectivity networks. Related studies demonstrate that functional connectivity networks in the human brain can help to improve the efficiency of diagnosing neurological disorders. However, there still exist two challenges that limit the progress of functional neuroimaging. Firstly, there exists an abundance of noise and redundant information in functional connectivity data, resulting in poor performance. Secondly, existing brain network models have tended to prioritize either classification performance or the interpretation of neuroscience findings behind the learned models. To deal with these challenges, this paper proposes a novel brain graph learning framework called Template-induced Brain Graph Learning (TiBGL), which has both discriminative and interpretable abilities. Motivated by the related medical findings on functional connectivites, TiBGL prop
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#22855;&#24322;&#25668;&#21160;&#31995;&#32479;&#30340;&#24930;&#19981;&#21464;&#27969;&#24418;&#65292;&#25552;&#20379;&#20102;&#26174;&#24335;&#24418;&#24335;&#30340;&#20989;&#25968;&#26469;&#26500;&#24314;&#21644;&#25968;&#20540;&#31215;&#20998;&#32553;&#20943;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#22522;&#20934;&#38382;&#39064;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07946</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35745;&#31639;&#22855;&#24322;&#25668;&#21160;&#31995;&#32479;&#30340;&#24930;&#19981;&#21464;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
Slow Invariant Manifolds of Singularly Perturbed Systems via Physics-Informed Machine Learning. (arXiv:2309.07946v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#22855;&#24322;&#25668;&#21160;&#31995;&#32479;&#30340;&#24930;&#19981;&#21464;&#27969;&#24418;&#65292;&#25552;&#20379;&#20102;&#26174;&#24335;&#24418;&#24335;&#30340;&#20989;&#25968;&#26469;&#26500;&#24314;&#21644;&#25968;&#20540;&#31215;&#20998;&#32553;&#20943;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#22522;&#20934;&#38382;&#39064;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29289;&#29702;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#22855;&#24322;&#25668;&#21160;&#31995;&#32479;&#30340;&#24930;&#19981;&#21464;&#27969;&#24418;&#65292;&#24182;&#25552;&#20379;&#20102;&#26174;&#24335;&#24418;&#24335;&#30340;&#20989;&#25968;&#26469;&#20415;&#20110;&#26500;&#24314;&#21644;&#25968;&#20540;&#31215;&#20998;&#32553;&#20943;&#27169;&#22411;&#12290;&#35813;&#26041;&#26696;&#22312;&#20960;&#20309;&#22855;&#24322;&#25668;&#21160;&#29702;&#35770;&#26694;&#26550;&#19979;&#35299;&#20915;&#19982;&#19981;&#21464;&#26041;&#31243;&#65288;IE&#65289;&#23545;&#24212;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;IE&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21363;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNNs&#65289;&#21644;&#38543;&#26426;&#25237;&#24433;&#31070;&#32463;&#32593;&#32476;&#65288;RPNNs&#65289;&#65292;&#21033;&#29992;&#31526;&#21495;&#24494;&#20998;&#26469;&#35745;&#31639;&#23398;&#20064;&#36807;&#31243;&#25152;&#38656;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#22522;&#20934;&#38382;&#39064;&#65292;&#21363;Michaelis-Menten&#21453;&#24212;&#26426;&#21046;&#12289;&#38774;&#21521;&#20171;&#23548;&#30340;&#33647;&#29289;&#20998;&#24067;&#21453;&#24212;&#26426;&#21046;&#21644;3D Sel'kov&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;PIML&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PIML&#26041;&#26696;&#33021;&#22815;&#25552;&#20379;&#31561;&#20215;&#25110;&#29978;&#33267;&#26356;&#39640;&#31934;&#24230;&#30340;&#36817;&#20284;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a physics-informed machine-learning (PIML) approach for the approximation of slow invariant manifolds (SIMs) of singularly perturbed systems, providing functionals in an explicit form that facilitate the construction and numerical integration of reduced order models (ROMs). The proposed scheme solves a partial differential equation corresponding to the invariance equation (IE) within the Geometric Singular Perturbation Theory (GSPT) framework. For the solution of the IE, we used two neural network structures, namely feedforward neural networks (FNNs), and random projection neural networks (RPNNs), with symbolic differentiation for the computation of the gradients required for the learning process. The efficiency of our PIML method is assessed via three benchmark problems, namely the Michaelis-Menten, the target mediated drug disposition reaction mechanism, and the 3D Sel'kov model. We show that the proposed PIML scheme provides approximations, of equivalent or even higher ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#37319;&#26679;&#26041;&#26696; (ESS)&#65292;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#12290;&#35813;&#26041;&#26696;&#33021;&#22815;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#24182;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#29992;&#20110;&#37319;&#26679;&#26631;&#35760;&#38598;&#65292;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#29992;&#20110;&#25513;&#30422;&#19981;&#30495;&#23454;&#30340;&#26631;&#35760;&#24182;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#37319;&#26679;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07945</link><description>&lt;p&gt;
&#22686;&#24378;&#37319;&#26679;&#26041;&#26696;&#30340;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Masked Generative Modeling with Enhanced Sampling Scheme. (arXiv:2309.07945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#37319;&#26679;&#26041;&#26696; (ESS)&#65292;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#12290;&#35813;&#26041;&#26696;&#33021;&#22815;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#24182;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#29992;&#20110;&#37319;&#26679;&#26631;&#35760;&#38598;&#65292;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#29992;&#20110;&#25513;&#30422;&#19981;&#30495;&#23454;&#30340;&#26631;&#35760;&#24182;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#37319;&#26679;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#22411;&#37319;&#26679;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;TimeVQVAE&#12289;MaskGIT&#21644;Token-Critic&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#24378;&#37319;&#26679;&#26041;&#26696; (ESS) &#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;ESS&#26126;&#30830;&#30830;&#20445;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;ESS&#39318;&#20808;&#20351;&#29992;MaskGIT&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#26469;&#37319;&#26679;&#19968;&#20010;&#26631;&#35760;&#38598;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#21518;&#65292;&#26631;&#35760;&#38598;&#32463;&#36807;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#65292;&#25513;&#30422;&#23548;&#33268;&#19981;&#30495;&#23454;&#26679;&#26412;&#30340;&#26631;&#35760;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#20851;&#38190;&#37325;&#37319;&#26679;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#36798;&#21040;&#26368;&#32456;&#37319;&#26679;&#27493;&#39588;&#20197;&#30830;&#20445;&#39640;&#24230;&#20445;&#30495;&#24230;&#12290;&#20851;&#38190;&#37325;&#37319;&#26679;&#20351;&#29992;&#26469;&#33258;&#33258;&#25105;Token-Critic&#33719;&#24471;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26356;&#22909;&#22320;&#34913;&#37327;&#37319;&#26679;&#26631;&#35760;&#30340;&#30495;&#23454;&#24615;&#65292;&#32780;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#20351;&#29992;&#37327;&#21270;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel sampling scheme for masked non-autoregressive generative modeling. We identify the limitations of TimeVQVAE, MaskGIT, and Token-Critic in their sampling processes, and propose Enhanced Sampling Scheme (ESS) to overcome these limitations. ESS explicitly ensures both sample diversity and fidelity, and consists of three stages: Naive Iterative Decoding, Critical Reverse Sampling, and Critical Resampling. ESS starts by sampling a token set using the naive iterative decoding as proposed in MaskGIT, ensuring sample diversity. Then, the token set undergoes the critical reverse sampling, masking tokens leading to unrealistic samples. After that, critical resampling reconstructs masked tokens until the final sampling step is reached to ensure high fidelity. Critical resampling uses confidence scores obtained from a self-Token-Critic to better measure the realism of sampled tokens, while critical reverse sampling uses the structure of the quantized latent vector space
&lt;/p&gt;</description></item><item><title>Voxtlm&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.07937</link><description>&lt;p&gt;
Voxtlm: &#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#24182;&#35821;&#38899;&#35782;&#21035;/&#21512;&#25104;&#21644;&#35821;&#38899;/&#25991;&#26412;&#34917;&#20805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks. (arXiv:2309.07937v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07937
&lt;/p&gt;
&lt;p&gt;
Voxtlm&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21482;&#35299;&#30721;&#35821;&#35328;&#27169;&#22411;VoxtLM&#65292;&#33021;&#22815;&#25191;&#34892;&#22235;&#20010;&#20219;&#21153;&#65306;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#12290;VoxtLM&#23558;&#25991;&#26412;&#35789;&#27719;&#19982;&#33258;&#30417;&#30563;&#35821;&#38899;&#29305;&#24449;&#20013;&#30340;&#31163;&#25955;&#35821;&#38899;&#20196;&#29260;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#20196;&#29260;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#19982;&#21333;&#20219;&#21153;&#27169;&#22411;&#30456;&#27604;&#65292;VoxtLM&#22312;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#35821;&#38899;&#21487;&#29702;&#35299;&#24615;&#20174;28.9&#25552;&#39640;&#21040;5.6&#65292;&#23458;&#35266;&#36136;&#37327;&#20174;2.68&#25552;&#39640;&#21040;3.90&#12290;VoxtLM&#36824;&#25913;&#21892;&#20102;&#35821;&#38899;&#29983;&#25104;&#21644;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;VoxtLM&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#25552;&#20379;&#35757;&#32451;&#33050;&#26412;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20197;&#23454;&#29616;&#23436;&#20840;&#21487;&#22797;&#29616;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. VoxtLM is trained with publicly available data and training recipes and model checkpoints will be open-sourced to make fully reproducible work.
&lt;/p&gt;</description></item><item><title>Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07936</link><description>&lt;p&gt;
Landscape-Sketch-Step: &#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07936
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25104;&#26412;&#20989;&#25968;&#30340;&#35780;&#20272;&#38750;&#24120;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#29978;&#33267;&#31105;&#27490;&#30340;&#22330;&#26223;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;Landscape-Sketch-Step&#65288;LSS&#65289;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20808;&#21069;&#37319;&#26679;&#28857;&#30340;&#21382;&#21490;&#20449;&#24687;&#65292;&#20197;&#26126;&#26234;&#22320;&#36873;&#25321;&#24212;&#35780;&#20272;&#25104;&#26412;&#20989;&#25968;&#30340;&#21442;&#25968;&#20540;&#12290;&#19982;&#22797;&#21046;&#20132;&#25442;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25152;&#38656;&#30340;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#19982;&#27169;&#25311;&#36864;&#28779;&#26041;&#27861;&#30456;&#24403;&#65292;&#36825;&#22312;&#39640;&#36890;&#37327;&#35745;&#31639;&#25110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#31561;&#29615;&#22659;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#35780;&#20272;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#35201;&#20040;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#19982;&#26631;&#20934;&#30340;&#20195;&#29702;&#20248;&#21270;&#25216;&#26415;&#20063;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#19981;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Racing Control Variable Genetic Programming (Racing-CVGP) &#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#35745;&#21010;&#26469;&#21152;&#36895;&#31526;&#21495;&#22238;&#24402;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#22266;&#23450;&#23454;&#39564;&#35745;&#21010;&#36873;&#25321;&#19981;&#20339;&#23548;&#33268;&#21457;&#29616;&#36807;&#31243;&#24310;&#36831;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.07934</link><description>&lt;p&gt;
&#20351;&#29992;&#31454;&#36895;&#25511;&#21046;&#21464;&#37327;&#36951;&#20256;&#32534;&#31243;&#36827;&#34892;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Racing Control Variable Genetic Programming for Symbolic Regression. (arXiv:2309.07934v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07934
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Racing Control Variable Genetic Programming (Racing-CVGP) &#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#35745;&#21010;&#26469;&#21152;&#36895;&#31526;&#21495;&#22238;&#24402;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#22266;&#23450;&#23454;&#39564;&#35745;&#21010;&#36873;&#25321;&#19981;&#20339;&#23548;&#33268;&#21457;&#29616;&#36807;&#31243;&#24310;&#36831;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#23427;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#12290;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#12289;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#21487;&#20197;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31526;&#21495;&#22238;&#24402;&#12290;&#23588;&#20854;&#26159;&#22312;&#23398;&#20064;&#28041;&#21450;&#22810;&#20010;&#21464;&#37327;&#30340;&#22797;&#26434;&#26041;&#31243;&#26102;&#65292;&#23427;&#20204;&#38656;&#35201;&#28023;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#25511;&#21046;&#21464;&#37327;&#36951;&#20256;&#32534;&#31243;&#65288;CVGP&#65289;&#65292;&#23427;&#36890;&#36807;&#20174;&#35774;&#35745;&#30340;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#20013;&#21457;&#29616;&#26041;&#31243;&#26469;&#21152;&#36895;&#22238;&#24402;&#36807;&#31243;&#12290;&#20294;&#26159;&#65292;&#22312;CVGP&#20013;&#23454;&#39564;&#38598;&#26159;&#20808;&#39564;&#22266;&#23450;&#30340;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23454;&#39564;&#35745;&#21010;&#30340;&#27425;&#20248;&#36873;&#25321;&#20250;&#26174;&#33879;&#24310;&#36831;&#21457;&#29616;&#36807;&#31243;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31454;&#36895;&#25511;&#21046;&#21464;&#37327;&#36951;&#20256;&#32534;&#31243;&#65288;Racing-CVGP&#65289;&#65292;&#23427;&#21516;&#26102;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#35745;&#21010;&#12290;&#31867;&#20284;&#20110;&#36873;&#25321;&#22909;&#30340;&#31526;&#21495;&#26041;&#31243;&#30340;&#36873;&#25321;&#26041;&#26696;&#34987;&#29992;&#20110;&#36873;&#25321;&#23454;&#39564;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression, as one of the most crucial tasks in AI for science, discovers governing equations from experimental data. Popular approaches based on genetic programming, Monte Carlo tree search, or deep reinforcement learning learn symbolic regression from a fixed dataset. They require massive datasets and long training time especially when learning complex equations involving many variables. Recently, Control Variable Genetic Programming (CVGP) has been introduced which accelerates the regression process by discovering equations from designed control variable experiments. However, the set of experiments is fixed a-priori in CVGP and we observe that sub-optimal selection of experiment schedules delay the discovery process significantly. To overcome this limitation, we propose Racing Control Variable Genetic Programming (Racing-CVGP), which carries out multiple experiment schedules simultaneously. A selection scheme similar to that used in selecting good symbolic equations in the 
&lt;/p&gt;</description></item><item><title>"&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;"&#25351;&#30340;&#26159;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#29983;&#25104;&#26032;&#39062;&#26377;&#24847;&#20041;&#20869;&#23481;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#38899;&#39057;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#27169;&#22411;&#12289;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#31034;&#20363;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#24403;&#21069;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#21830;&#19994;&#19982;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#30740;&#31350;&#30340;&#35758;&#31243;&#65292;&#21253;&#25324;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07930</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Generative AI. (arXiv:2309.07930v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07930
&lt;/p&gt;
&lt;p&gt;
"&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;"&#25351;&#30340;&#26159;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#29983;&#25104;&#26032;&#39062;&#26377;&#24847;&#20041;&#20869;&#23481;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#38899;&#39057;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#27169;&#22411;&#12289;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#31034;&#20363;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#24403;&#21069;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#21830;&#19994;&#19982;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#30740;&#31350;&#30340;&#35758;&#31243;&#65292;&#21253;&#25324;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;"&#19968;&#35789;&#25351;&#30340;&#26159;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#29983;&#25104;&#30475;&#20284;&#26032;&#39062;&#26377;&#24847;&#20041;&#30340;&#20869;&#23481;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#38899;&#39057;&#30340;&#35745;&#31639;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;Dall-E 2&#65292;GPT-4&#21644;Copilot&#65292;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#24037;&#20316;&#21644;&#19982;&#20182;&#20154;&#20132;&#27969;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24418;&#23481;&#20026;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#19968;&#31181;&#23454;&#20307;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#12289;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#31034;&#20363;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#21830;&#19994;&#19982;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#65288;BISE&#65289;&#30740;&#31350;&#30340;&#35758;&#31243;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#35752;&#35770;&#20102;&#20449;&#24687;&#31995;&#32479;&#32972;&#26223;&#19979;&#30340;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#35752;&#35770;&#20102;BISE&#31038;&#21306;&#29420;&#29305;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;BISE&#30740;&#31350;&#30340;&#26377;&#24433;&#21709;&#30340;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "generative AI" refers to computational techniques that are capable of generating seemingly new, meaningful content such as text, images, or audio from training data. The widespread diffusion of this technology with examples such as Dall-E 2, GPT-4, and Copilot is currently revolutionizing the way we work and communicate with each other. In this article, we provide a conceptualization of generative AI as an entity in socio-technical systems and provide examples of models, systems, and applications. Based on that, we introduce limitations of current generative AI and provide an agenda for Business &amp; Information Systems Engineering (BISE) research. Different from previous works, we focus on generative AI in the context of information systems, and, to this end, we discuss several opportunities and challenges that are unique to the BISE community and make suggestions for impactful directions for BISE research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#33539;&#24335;&#12289;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#21644;&#30456;&#20851;&#36866;&#37197;&#22120;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2309.07929</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer. (arXiv:2309.07929v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#33539;&#24335;&#12289;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#21644;&#30456;&#20851;&#36866;&#37197;&#22120;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20174;&#26410;&#21516;&#26102;&#30475;&#21040;&#29289;&#20307;&#21644;&#21548;&#21040;&#20854;&#22768;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#36755;&#20837;&#38899;&#39057;&#20013;&#23450;&#20301;&#20854;&#35270;&#35273;&#20301;&#32622;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#30340;&#33539;&#24335;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#32534;&#30721;&#22120;&#34701;&#21512;&#35299;&#30721;&#22120;&#33539;&#24335;&#20174;&#34701;&#21512;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#20013;&#35299;&#30721;&#23450;&#20301;&#20449;&#24687;&#65292;&#25105;&#20204;&#26088;&#22312;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#26356;&#22909;&#22320;&#36866;&#24212;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#65288;SAP&#65289;&#26469;&#24110;&#21161;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20851;&#27880;&#26377;&#22768;&#23545;&#35937;&#65292;&#21516;&#26102;&#20063;&#40723;&#21169;&#35270;&#35273;&#21644;&#38899;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#32553;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30456;&#20851;&#36866;&#37197;&#22120;&#65288;ColA&#65289;&#26469;&#20445;&#25345;&#26368;&#23567;&#30340;&#35757;&#32451;&#24037;&#20316;&#37327;&#24182;&#32500;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Never having seen an object and heard its sound simultaneously, can the model still accurately localize its visual position from the input audio? In this work, we concentrate on the Audio-Visual Localization and Segmentation tasks but under the demanding zero-shot and few-shot scenarios. To achieve this goal, different from existing approaches that mostly employ the encoder-fusion-decoder paradigm to decode localization information from the fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm, aiming to better fit the data scarcity and varying data distribution dilemmas with the help of abundant knowledge from pre-trained models. Specifically, we first propose to construct Semantic-aware Audio Prompt (SAP) to help the visual foundation model focus on sounding objects, meanwhile, the semantic gap between the visual and audio modalities is also encouraged to shrink. Then, we develop a Correlation Adapter (ColA) to keep minimal training efforts as well as maintain 
&lt;/p&gt;</description></item><item><title>Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.07778</link><description>&lt;p&gt;
Virchow: &#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07778
&lt;/p&gt;
&lt;p&gt;
Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#30149;&#29702;&#23398;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#30284;&#30151;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23545;&#20110;&#35768;&#22810;&#29305;&#23450;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#65292;&#25968;&#25454;&#37327;&#19981;&#36275;&#20197;&#36827;&#34892;&#24320;&#21457;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Virchow&#65292;&#19968;&#20010;632&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#30149;&#29702;&#23398;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;Virchow&#22312;1.5&#30334;&#19975;&#20010;&#19981;&#21516;&#32452;&#32455;&#26679;&#26412;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#25968;&#25454;&#37327;&#22823;&#24471;&#22810;&#12290;&#22312;&#21253;&#25324;&#29926;&#29255;&#32423;&#20840;&#30284;&#26816;&#27979;&#21644;&#20122;&#22411;&#20197;&#21450;&#24187;&#28783;&#29255;&#32423;&#29983;&#29289;&#26631;&#24535;&#29289;&#39044;&#27979;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;Virchow&#22312;&#26469;&#33258;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20154;&#32676;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#22343;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07579</link><description>&lt;p&gt;
&#20445;&#25345;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#24207;&#21015;&#30340;SPD&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25968;&#25454;&#31867;&#22411;&#30340;&#20998;&#26512;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#31561;&#65292;&#21253;&#25324;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#65292;&#24182;&#22312;&#25972;&#20010;&#20998;&#26512;&#36807;&#31243;&#20013;&#20445;&#25345;&#23427;&#20204;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#33041;&#30005;&#22270;&#21327;&#26041;&#24046;&#30697;&#38453;&#24207;&#21015;&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based auto-attention mechanisms have been successfully applied to the analysis of a variety of context-reliant data types, from texts to images and beyond, including data from non-Euclidean geometries. In this paper, we present such a mechanism, designed to classify sequences of Symmetric Positive Definite matrices while preserving their Riemannian geometry throughout the analysis. We apply our method to automatic sleep staging on timeseries of EEG-derived covariance matrices from a standard dataset, obtaining high levels of stage-wise performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;&#20013;&#22797;&#21512;&#23376;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#26500;&#36896;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#65292;&#30740;&#31350;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#22797;&#21512;&#23376;&#25910;&#25947;&#26102;&#23545;&#24212;&#30340;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23637;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.07169</link><description>&lt;p&gt;
&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#39057;&#29575;&#25910;&#25947;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;&#20013;&#22797;&#21512;&#23376;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#26500;&#36896;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#65292;&#30740;&#31350;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#22797;&#21512;&#23376;&#25910;&#25947;&#26102;&#23545;&#24212;&#30340;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23637;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;(TSP)&#21033;&#29992;&#21333;&#32431;&#24418;&#22797;&#21512;&#26469;&#24314;&#27169;&#27604;&#39030;&#28857;&#21644;&#36793;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;TSP&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#22797;&#21512;&#23376;&#30340;&#24191;&#20041;&#39640;&#38454;&#22270;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#22797;&#21512;&#23376;&#30340;&#27010;&#24565;&#65292;&#21363;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#30340;&#26497;&#38480;[1]&#12290;&#21463;&#22270;&#31227;&#20301;&#31639;&#23376;&#30340;&#31215;&#20998;&#31639;&#23376;&#24418;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26681;&#25454;&#22797;&#21512;&#23376;&#30340;&#25152;&#26377;&#21487;&#33021;&#23610;&#23544;&#30340;&#32452;&#20214;&#26500;&#36896;&#20102;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;(CSO)&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;CSO&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#19968;&#31867;&#26032;&#30340;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#19968;&#20010;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#25910;&#25947;&#21040;&#19968;&#20010;&#22797;&#21512;&#23376;&#26102;&#65292;&#30456;&#24212;&#30340;CSO&#30340;&#29305;&#24449;&#20540;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#26263;&#31034;&#20102;&#22312;&#22823;&#22411;&#21333;&#32431;&#24418;&#22797;&#21512;&#25110;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#19978;&#30340;&#23398;&#20064;&#21487;&#36716;&#31227;&#24615;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological signal processing (TSP) utilizes simplicial complexes to model structures with higher order than vertices and edges. In this paper, we study the transferability of TSP via a generalized higher-order version of graphon, known as complexon. We recall the notion of a complexon as the limit of a simplicial complex sequence [1]. Inspired by the integral operator form of graphon shift operators, we construct a marginal complexon and complexon shift operator (CSO) according to components of all possible dimensions from the complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate them to a new family of weighted adjacency matrices. We prove that when a simplicial complex sequence converges to a complexon, the eigenvalues of the corresponding CSOs converge to that of the limit complexon. These results hint at learning transferability on large simplicial complexes or simplicial complex sequences, which generalize the graphon signal processing framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#23398;&#20064;&#21327;&#35758;&#30340;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#29615;&#22659;&#20132;&#20114;&#32972;&#26223;&#20013;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#38598;&#20013;&#21270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.06938</link><description>&lt;p&gt;
&#26080;&#38598;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collectionless Artificial Intelligence. (arXiv:2309.06938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#23398;&#20064;&#21327;&#35758;&#30340;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#29615;&#22659;&#20132;&#20114;&#32972;&#26223;&#20013;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#38598;&#20013;&#21270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#20307;&#19978;&#65292;&#22788;&#29702;&#24222;&#22823;&#25968;&#25454;&#38598;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#22766;&#35266;&#32467;&#26524;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#38598;&#30340;&#38598;&#20013;&#21270;&#23384;&#22312;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#39118;&#38505;&#24847;&#35782;&#12290;&#26412;&#25991;&#25903;&#25345;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21327;&#35758;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#30495;&#27491;&#20197;&#29615;&#22659;&#20132;&#20114;&#20026;&#20013;&#24515;&#30340;&#31867;&#20154;&#35748;&#30693;&#32972;&#26223;&#19979;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#23398;&#20064;&#21327;&#35758;&#38656;&#35201;&#36981;&#24490;&#26080;&#38598;&#21512;&#21407;&#21017;&#65292;&#21363;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#65292;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#26356;&#26032;&#24403;&#21069;&#29615;&#22659;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#19988;&#20195;&#29702;&#19981;&#33021;&#23545;&#26102;&#38388;&#27969;&#36827;&#34892;&#35760;&#24405;&#12290;&#22522;&#26412;&#19978;&#65292;&#19981;&#33021;&#23384;&#20648;&#26469;&#33258;&#20256;&#24863;&#22120;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
By and large, the professional handling of huge data collections is regarded as a fundamental ingredient of the progress of machine learning and of its spectacular results in related disciplines, with a growing agreement on risks connected to the centralization of such data collections. This paper sustains the position that the time has come for thinking of new learning protocols where machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, there is neither permission to store the temporal information coming from the sensors, thus promoting the development of s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;LQR&#20013;&#24212;&#29992;MAML&#26102;&#30340;&#23616;&#37096;&#25910;&#25947;&#29305;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20445;&#25345;&#21160;&#24577;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#23616;&#37096;&#25910;&#25947;&#20445;&#35777;&#12290;&#35770;&#25991;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;MAML&#22312;LQR&#20219;&#21153;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.06588</link><description>&lt;p&gt;
Gradient-based MAML&#22312;LQR&#20013;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of Gradient-based MAML in LQR. (arXiv:2309.06588v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;LQR&#20013;&#24212;&#29992;MAML&#26102;&#30340;&#23616;&#37096;&#25910;&#25947;&#29305;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20445;&#25345;&#21160;&#24577;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#23616;&#37096;&#25910;&#25947;&#20445;&#35777;&#12290;&#35770;&#25991;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;MAML&#22312;LQR&#20219;&#21153;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25506;&#32034;&#22312;&#32447;&#24615;&#31995;&#32479;&#20108;&#27425;&#20248;&#21270;&#25511;&#21046;&#65288;LQR&#65289;&#20013;&#24212;&#29992;Model-agnostic Meta-learning&#65288;MAML&#65289;&#26102;&#30340;&#23616;&#37096;&#25910;&#25947;&#29305;&#24615;&#12290;MAML&#21450;&#20854;&#21464;&#20307;&#24050;&#25104;&#20026;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#20808;&#21069;&#23398;&#20064;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20984;&#24615;&#21644;&#20854;&#32467;&#26500;&#65292;MAML&#30340;&#29702;&#35770;&#20445;&#35777;&#20173;&#28982;&#26410;&#30693;&#65292;&#36825;&#20351;&#24471;&#22312;&#21160;&#24577;&#31995;&#32479;&#35774;&#32622;&#20013;&#30830;&#20445;&#31283;&#23450;&#24615;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;MAML&#22312;LQR&#35774;&#32622;&#20013;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#21160;&#24577;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#20197;&#23637;&#31034;MAML&#22312;LQR&#20219;&#21153;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main objective of this research paper is to investigate the local convergence characteristics of Model-agnostic Meta-learning (MAML) when applied to linear system quadratic optimal control (LQR). MAML and its variations have become popular techniques for quickly adapting to new tasks by leveraging previous learning knowledge in areas like regression, classification, and reinforcement learning. However, its theoretical guarantees remain unknown due to non-convexity and its structure, making it even more challenging to ensure stability in the dynamic system setting. This study focuses on exploring MAML in the LQR setting, providing its local convergence guarantees while maintaining the stability of the dynamical system. The paper also presents simple numerical results to demonstrate the convergence properties of MAML in LQR tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#21457;&#29616;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#36824;&#33021;&#25581;&#31034;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21644;&#20854;&#23545;&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#33021;&#25552;&#20379;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.06584</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction. (arXiv:2309.06584v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#21457;&#29616;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#36824;&#33021;&#25581;&#31034;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21644;&#20854;&#23545;&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#33021;&#25552;&#20379;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#65288;ADRD&#65289;&#22312;&#32654;&#22269;&#26159;&#31532;&#20845;&#22823;&#27515;&#20129;&#21407;&#22240;&#65292;&#20934;&#30830;&#30340;ADRD&#39118;&#38505;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;ADRD&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#22270;&#20687;&#20998;&#26512;&#65292;&#32780;&#24182;&#38750;&#25152;&#26377;&#24739;&#32773;&#22312;ADRD&#35786;&#26029;&#21069;&#37117;&#25509;&#21463;&#21307;&#23398;&#24433;&#20687;&#26816;&#26597;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#21487;&#20197;&#25581;&#31034;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#24182;&#21457;&#29616;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#32034;&#36180;&#25968;&#25454;&#36827;&#34892;ADRD&#39118;&#38505;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#32972;&#21518;&#32570;&#20047;&#21487;&#35299;&#37322;&#21407;&#22240;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;ADRD&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#30830;&#20445;&#20840;&#38754;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;VGNN&#65289;&#26469;&#20272;&#35745;ADRD&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#31181;&#24773;&#26223;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20351;&#29992;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#36731;&#26799;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.  We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25308;&#21344;&#24237;&#33410;&#28857;&#21644;&#26381;&#21153;&#22120;&#38544;&#31169;&#20405;&#29359;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05395</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Practical Homomorphic Aggregation for Byzantine ML. (arXiv:2309.05395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25308;&#21344;&#24237;&#33410;&#28857;&#21644;&#26381;&#21153;&#22120;&#38544;&#31169;&#20405;&#29359;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21487;&#29992;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#27491;&#22312;&#20998;&#24067;&#24335;&#25299;&#25169;&#20013;&#37096;&#32626;&#65292;&#19981;&#21516;&#30340;&#33410;&#28857;&#36890;&#36807;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20132;&#25442;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#26799;&#24230;&#65289;&#26469;&#20849;&#21516;&#35757;&#32451;&#20854;&#20010;&#20307;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#20004;&#31181;&#23041;&#32961;&#12290;&#39318;&#20808;&#65292;&#25308;&#21344;&#24237;&#24335;&#33410;&#28857;&#21487;&#20197;&#36890;&#36807;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#38169;&#35823;&#30340;&#26799;&#24230;&#65289;&#21333;&#29420;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#32531;&#35299;&#27492;&#31867;&#34892;&#20026;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#20351;&#29992;&#38750;&#32447;&#24615;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#20405;&#29359;&#33410;&#28857;&#30340;&#38544;&#31169;&#12290;&#26368;&#36817;&#30340;&#25915;&#20987;&#24050;&#32463;&#34920;&#26126;&#65292;&#20132;&#25442;&#65288;&#26410;&#21152;&#23494;&#30340;&#65289;&#26799;&#24230;&#20351;&#24471;&#19968;&#20010;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#33021;&#22815;&#24674;&#22797;&#20986;&#25152;&#26377;&#33410;&#28857;&#30340;&#25968;&#25454;&#12290;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#19968;&#31181;&#37329;&#26631;&#20934;&#23433;&#20840;&#21407;&#35821;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#38750;&#25308;&#21344;&#24237;&#22330;&#26223;&#20013;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the large-scale availability of data, machine learning (ML) algorithms are being deployed in distributed topologies, where different nodes collaborate to train ML models over their individual data by exchanging model-related information (e.g., gradients) with a central server. However, distributed learning schemes are notably vulnerable to two threats. First, Byzantine nodes can single-handedly corrupt the learning by sending incorrect information to the server, e.g., erroneous gradients. The standard approach to mitigate such behavior is to use a non-linear robust aggregation method at the server. Second, the server can violate the privacy of the nodes. Recent attacks have shown that exchanging (unencrypted) gradients enables a curious server to recover the totality of the nodes' data. The use of homomorphic encryption (HE), a gold standard security primitive, has extensively been studied as a privacy-preserving solution to distributed learning in non-Byzantine scenarios. Howev
&lt;/p&gt;</description></item><item><title>CenTime&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03851</link><description>&lt;p&gt;
CenTime: &#20107;&#20214;&#26465;&#20214;&#27169;&#22411;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CenTime: Event-Conditional Modelling of Censoring in Survival Analysis. (arXiv:2309.03851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03851
&lt;/p&gt;
&lt;p&gt;
CenTime&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22522;&#20110;&#22522;&#32447;&#35266;&#27979;&#26469;&#20272;&#35745;&#29305;&#23450;&#20107;&#20214;&#65288;&#22914;&#27515;&#20129;&#25110;&#30284;&#30151;&#22797;&#21457;&#65289;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#36825;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#25968;&#25454;&#39044;&#27979;&#20020;&#24202;&#37325;&#35201;&#20107;&#20214;&#30340;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#23384;&#22312;&#23616;&#38480;&#24615;&#65307;&#26377;&#20123;&#26041;&#27861;&#21482;&#20851;&#27880;&#23558;&#24739;&#32773;&#25353;&#29983;&#23384;&#33021;&#21147;&#36827;&#34892;&#25490;&#21517;&#65292;&#24573;&#35270;&#20102;&#23545;&#23454;&#38469;&#20107;&#20214;&#26102;&#38388;&#30340;&#20272;&#35745;&#65307;&#32780;&#20854;&#20182;&#26041;&#27861;&#23558;&#38382;&#39064;&#35270;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24573;&#35270;&#20102;&#20107;&#20214;&#30340;&#26102;&#38388;&#39034;&#24207;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#26679;&#26412;&#65288;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#20854;&#20013;&#30830;&#20999;&#20107;&#20214;&#26102;&#38388;&#19981;&#21487;&#30693;&#65289;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CenTime&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#65292;&#21363;&#20351;&#27809;&#26377;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is a valuable tool for estimating the time until specific events, such as death or cancer recurrence, based on baseline observations. This is particularly useful in healthcare to prognostically predict clinically important events based on patient data. However, existing approaches often have limitations; some focus only on ranking patients by survivability, neglecting to estimate the actual event time, while others treat the problem as a classification task, ignoring the inherent time-ordered structure of the events. Furthermore, the effective utilization of censored samples - training data points where the exact event time is unknown - is essential for improving the predictive accuracy of the model. In this paper, we introduce CenTime, a novel approach to survival analysis that directly estimates the time to event. Our method features an innovative event-conditional censoring mechanism that performs robustly even when uncensored data is scarce. We demonstrate that ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36882;&#24402;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#20837;&#20405;&#21709;&#24212;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#24182;&#34892;&#23376;&#28216;&#25103;&#21644;&#35745;&#31639;&#38408;&#20540;&#32467;&#26500;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.03292</link><description>&lt;p&gt;
&#36890;&#36807;&#36882;&#24402;&#20998;&#35299;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#20837;&#20405;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Scalable Learning of Intrusion Responses through Recursive Decomposition. (arXiv:2309.03292v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36882;&#24402;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#20837;&#20405;&#21709;&#24212;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#24182;&#34892;&#23376;&#28216;&#25103;&#21644;&#35745;&#31639;&#38408;&#20540;&#32467;&#26500;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#20837;&#20405;&#24212;&#23545;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#24418;&#24335;&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#27979;&#30340;&#38543;&#26426;&#28216;&#25103;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#25105;&#23545;&#24328;&#36827;&#34892;&#21327;&#21516;&#28436;&#21270;&#65292;&#20197;&#36798;&#21040;&#24179;&#34913;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23567;&#22411;&#22522;&#30784;&#35774;&#26045;&#30340;&#21487;&#34892;&#24615;&#65292;&#20294;&#38754;&#23545;&#23454;&#38469;&#24773;&#22659;&#30001;&#20110;&#22522;&#30784;&#35774;&#26045;&#35268;&#27169;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#32780;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28216;&#25103;&#36882;&#24402;&#20998;&#35299;&#25104;&#21487;&#20197;&#24182;&#34892;&#35299;&#20915;&#30340;&#23376;&#28216;&#25103;&#30340;&#26041;&#27861;&#12290;&#24212;&#29992;&#26368;&#20248;&#20572;&#27490;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#23376;&#28216;&#25103;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#20855;&#26377;&#38408;&#20540;&#32467;&#26500;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#39640;&#25928;&#22320;&#35745;&#31639;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#35299;&#30340;&#28216;&#25103;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Decompositional Fictitious Self-Play (DFSP) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study automated intrusion response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed stochastic game. To solve the game we follow an approach where attack and defense strategies co-evolve through reinforcement learning and self-play toward an equilibrium. Solutions proposed in previous work prove the feasibility of this approach for small infrastructures but do not scale to realistic scenarios due to the exponential growth in computational complexity with the infrastructure size. We address this problem by introducing a method that recursively decomposes the game into subgames which can be solved in parallel. Applying optimal stopping theory we show that the best response strategies in these subgames exhibit threshold structures, which allows us to compute them efficiently. To solve the decomposed game we introduce an algorithm called Decompositional Fictitious Self-Play (DFSP), which learns Nash equilibria through stoc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#22522;&#20110;&#33609;&#22270;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#22871;&#20214;PROMISE&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#22312;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2309.02014</link><description>&lt;p&gt;
PROMISE: &#36890;&#36807;&#24341;&#20837;&#21487;&#25193;&#23637;&#26354;&#29575;&#20272;&#35745;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates. (arXiv:2309.02014v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#22522;&#20110;&#33609;&#22270;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#22871;&#20214;PROMISE&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#22312;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PROMISE&#65288;&#36890;&#36807;&#24341;&#20837;&#21487;&#25193;&#23637;&#26354;&#29575;&#20272;&#35745;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#22871;&#22522;&#20110;&#33609;&#22270;&#30340;&#39044;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#22871;&#20214;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;PROMISE&#21253;&#25324;SVRG&#12289;SAGA&#21644;Katyusha&#30340;&#39044;&#26465;&#20214;&#29256;&#26412;&#65307;&#27599;&#20010;&#31639;&#27861;&#37117;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#26377;&#25928;&#30340;&#40664;&#35748;&#36229;&#21442;&#25968;&#20540;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#38656;&#35201;&#20180;&#32454;&#35843;&#33410;&#36229;&#21442;&#25968;&#25165;&#33021;&#25104;&#21151;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#30149;&#24577;&#26465;&#20214;&#19979;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#40664;&#35748;&#36229;&#21442;&#25968;&#20540;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#30001;&#22522;&#20934;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#27719;&#32534;&#30340;51&#20010;&#23725;&#22238;&#24402;&#21644;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#19978;&#20248;&#20110;&#25110;&#19982;&#27969;&#34892;&#30340;&#35843;&#25972;&#21518;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#22120;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces PROMISE ($\textbf{Pr}$econditioned Stochastic $\textbf{O}$ptimization $\textbf{M}$ethods by $\textbf{I}$ncorporating $\textbf{S}$calable Curvature $\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#36793;&#32536;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31232;&#30095;&#32454;&#32990;&#22797;&#21512;&#20307;&#26469;&#34920;&#31034;&#36793;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#65292;&#21033;&#29992;Hodge-Laplacian&#30340;&#29305;&#24449;&#21521;&#37327;&#21644;&#20851;&#32852;&#30697;&#38453;&#36827;&#34892;Hodge&#20998;&#35299;&#65292;&#24471;&#21040;&#26799;&#24230;&#12289;&#26059;&#37327;&#21644;&#35856;&#27874;&#27969;&#30340;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32990;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#28155;&#21152;&#32454;&#32990;&#26469;&#22686;&#24378;&#35266;&#27979;&#21040;&#30340;&#22270;&#65292;&#20351;&#34920;&#31034;&#31232;&#30095;&#21487;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32454;&#32990;&#22797;&#21512;&#20307;&#22312;&#22270;&#19978;&#34920;&#31034;&#36793;&#27969;
&lt;/p&gt;
&lt;p&gt;
Representing Edge Flows on Graphs via Sparse Cell Complexes. (arXiv:2309.01632v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#36793;&#32536;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31232;&#30095;&#32454;&#32990;&#22797;&#21512;&#20307;&#26469;&#34920;&#31034;&#36793;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#65292;&#21033;&#29992;Hodge-Laplacian&#30340;&#29305;&#24449;&#21521;&#37327;&#21644;&#20851;&#32852;&#30697;&#38453;&#36827;&#34892;Hodge&#20998;&#35299;&#65292;&#24471;&#21040;&#26799;&#24230;&#12289;&#26059;&#37327;&#21644;&#35856;&#27874;&#27969;&#30340;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32990;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#28155;&#21152;&#32454;&#32990;&#26469;&#22686;&#24378;&#35266;&#27979;&#21040;&#30340;&#22270;&#65292;&#20351;&#34920;&#31034;&#31232;&#30095;&#21487;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#33719;&#21462;&#31232;&#30095;&#21487;&#35299;&#37322;&#30340;&#21487;&#35266;&#27979;&#25968;&#25454;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23545;&#20110;&#34920;&#31034;&#27839;&#22270;&#36793;&#32536;&#30340;&#27969;&#21160;&#30340;&#25968;&#25454;&#65292;&#19968;&#31181;&#30452;&#35266;&#21487;&#35299;&#37322;&#30340;&#33719;&#21462;&#34920;&#31034;&#30340;&#26041;&#27861;&#26159;&#23558;&#22270;&#32467;&#26500;&#25552;&#21319;&#21040;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#65306;&#30456;&#20851;Hodge-Laplacian&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#21450;&#30456;&#24212;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#20851;&#32852;&#30697;&#38453;&#65292;&#21487;&#24341;&#23548;&#20986;Hodge&#20998;&#35299;&#65292;&#29992;&#20110;&#20197;&#26799;&#24230;&#65292;&#26059;&#37327;&#21644;&#35856;&#27874;&#27969;&#30340;&#24418;&#24335;&#34920;&#31034;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#32454;&#32990;&#22797;&#21512;&#20307;&#65292;&#24182;&#24341;&#20837;&#32454;&#32990;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#28155;&#21152;&#19968;&#32452;&#32454;&#32990;&#26469;&#22686;&#24378;&#35266;&#27979;&#21040;&#30340;&#22270;&#65292;&#20351;&#24471;&#20851;&#32852;Hodge Laplacian&#30340;&#29305;&#24449;&#21521;&#37327;&#33021;&#22815;&#25552;&#20379;&#23545;&#22270;&#19978;&#35266;&#27979;&#21040;&#30340;&#36793;&#32536;&#27969;&#30340;&#31232;&#30095;&#21487;&#35299;&#37322;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining sparse, interpretable representations of observable data is crucial in many machine learning and signal processing tasks. For data representing flows along the edges of a graph, an intuitively interpretable way to obtain such representations is to lift the graph structure to a simplicial complex: The eigenvectors of the associated Hodge-Laplacian, respectively the incidence matrices of the corresponding simplicial complex then induce a Hodge decomposition, which can be used to represent the observed data in terms of gradient, curl, and harmonic flows. In this paper, we generalize this approach to cellular complexes and introduce the cell inference optimization problem, i.e., the problem of augmenting the observed graph by a set of cells, such that the eigenvectors of the associated Hodge Laplacian provide a sparse, interpretable representation of the observed edge flows on the graph. We show that this problem is NP-hard and introduce an efficient approximation algorithm for i
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#21319;&#32423;&#20102;Python RTNI&#30340;&#31532;&#20108;&#29256;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#38543;&#26426;&#24352;&#37327;&#36827;&#34892;&#31526;&#21495;&#24615;&#25972;&#21512;&#65292;&#25903;&#25345;Haar&#20998;&#24067;&#30340;&#37193;&#30697;&#38453;&#12289;&#27491;&#20132;&#30697;&#38453;&#21644;&#27491;&#24577;&#20998;&#24067;&#30340;&#24352;&#37327;&#12290;&#36890;&#36807;&#23548;&#20986;TensorNetwork&#26684;&#24335;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#21487;&#20197;&#36827;&#34892;&#20302;&#32500;&#35745;&#31639;&#65292;&#24182;&#35299;&#37322;&#20102;&#25968;&#23398;&#21407;&#29702;&#21644;&#24352;&#37327;&#32593;&#32476;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.01167</link><description>&lt;p&gt;
&#31526;&#21495;&#24615;&#22320;&#25972;&#21512;&#19981;&#21516;&#38543;&#26426;&#24352;&#37327;&#30340;&#24352;&#37327;&#32593;&#32476;&#35745;&#31639; - Python RTNI&#30340;&#31532;&#20108;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
Symbolically integrating tensor networks over various random tensors -- the second version of Python RTNI. (arXiv:2309.01167v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01167
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21319;&#32423;&#20102;Python RTNI&#30340;&#31532;&#20108;&#29256;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#38543;&#26426;&#24352;&#37327;&#36827;&#34892;&#31526;&#21495;&#24615;&#25972;&#21512;&#65292;&#25903;&#25345;Haar&#20998;&#24067;&#30340;&#37193;&#30697;&#38453;&#12289;&#27491;&#20132;&#30697;&#38453;&#21644;&#27491;&#24577;&#20998;&#24067;&#30340;&#24352;&#37327;&#12290;&#36890;&#36807;&#23548;&#20986;TensorNetwork&#26684;&#24335;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#21487;&#20197;&#36827;&#34892;&#20302;&#32500;&#35745;&#31639;&#65292;&#24182;&#35299;&#37322;&#20102;&#25968;&#23398;&#21407;&#29702;&#21644;&#24352;&#37327;&#32593;&#32476;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#21319;&#32423;RTNI&#30340;Python&#29256;&#26412;&#65292;&#35813;&#29256;&#26412;&#33021;&#22815;&#31526;&#21495;&#24615;&#22320;&#25972;&#21512;Haar&#20998;&#24067;&#30340;&#37193;&#30697;&#38453;&#19978;&#30340;&#24352;&#37327;&#32593;&#32476;&#12290;&#29616;&#22312;&#65292;PyRTNI2&#36824;&#21487;&#20197;&#22788;&#29702;Haar&#20998;&#24067;&#30340;&#27491;&#20132;&#30697;&#38453;&#20197;&#21450;&#23454;&#25968;&#21644;&#22797;&#25968;&#27491;&#24577;&#20998;&#24067;&#30340;&#24352;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#23558;&#24352;&#37327;&#32593;&#32476;&#20197;TensorNetwork&#30340;&#26684;&#24335;&#23548;&#20986;&#65292;&#36825;&#26679;&#21487;&#20197;&#20351;&#29992;&#20855;&#20307;&#30340;&#24352;&#37327;&#36827;&#34892;&#36827;&#19968;&#27493;&#35745;&#31639;&#65292;&#21363;&#20351;&#26159;&#20302;&#32500;&#24773;&#20917;&#19979;&#30340;&#35745;&#31639;&#65292;&#20854;&#20013;Weingarten&#20989;&#25968;&#19982;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#20989;&#25968;&#19981;&#21516;&#12290;&#25945;&#31243;&#31508;&#35760;&#26412;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/MotohisaFukuda/PyRTNI2&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#31243;&#24207;&#32972;&#21518;&#30340;&#25968;&#23398;&#21407;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#23427;&#36827;&#34892;&#30340;&#21508;&#31181;&#24352;&#37327;&#32593;&#32476;&#35745;&#31639;&#12290;&#20851;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#23558;&#19978;&#36848;&#38543;&#26426;&#30697;&#38453;&#21644;&#24352;&#37327;&#30340;&#36880;&#20803;&#32032;&#30697;&#38453;&#24494;&#31215;&#20998;&#35299;&#37322;&#20026;&#24352;&#37327;&#32593;&#32476;&#22270;&#65292;&#35748;&#20026;&#36825;&#31181;&#35266;&#28857;&#26159;&#33258;&#28982;&#30340;&#65292;&#23558;&#24494;&#31215;&#20998;&#20013;&#30340;delta&#20989;&#25968;&#19982;&#24352;&#37327;&#32593;&#32476;&#22270;&#20013;&#30340;&#36793;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are upgrading the Python-version of RTNI, which symbolically integrates tensor networks over the Haar-distributed unitary matrices. Now, PyRTNI2 can treat the Haar-distributed orthogonal matrices and the real and complex normal Gaussian tensors as well. Moreover, it can export tensor networks in the format of TensorNetwork so that one can make further calculations with concrete tensors, even for low dimensions, where the Weingarten functions differ from the ones for high dimensions. The tutorial notebooks are found at GitHub: https://github.com/MotohisaFukuda/PyRTNI2. In this paper, we explain maths behind the program and show what kind of tensor network calculations can be made with it. For the former, we interpret the element-wise moment calculus of the above random matrices and tensors in terms of tensor network diagrams, and argue that the view is natural, relating delta functions in the calculus to edges in tensor network diagrams.
&lt;/p&gt;</description></item><item><title>&#27492;&#25968;&#25454;&#21457;&#24067;&#21253;&#21547;Majorana&#31034;&#33539;&#22120;&#23454;&#39564;&#30340;&#26657;&#20934;&#25968;&#25454;&#23376;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.10856</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;Majorana&#31034;&#33539;&#22120;&#25968;&#25454;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Majorana Demonstrator Data Release for AI/ML Applications. (arXiv:2308.10856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10856
&lt;/p&gt;
&lt;p&gt;
&#27492;&#25968;&#25454;&#21457;&#24067;&#21253;&#21547;Majorana&#31034;&#33539;&#22120;&#23454;&#39564;&#30340;&#26657;&#20934;&#25968;&#25454;&#23376;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#25968;&#25454;&#21457;&#24067;&#21253;&#21547;Majorana&#31034;&#33539;&#22120;&#23454;&#39564;&#30340;&#26657;&#20934;&#25968;&#25454;&#23376;&#38598;&#12290;&#27599;&#20010;Majorana&#20107;&#20214;&#37117;&#26377;&#21407;&#22987;&#30340;&#38167;&#25506;&#27979;&#22120;&#27874;&#24418;&#12289;&#33033;&#20914;&#24418;&#29366;&#35782;&#21035;&#20999;&#21106;&#21644;&#26657;&#20934;&#21518;&#30340;&#26368;&#32456;&#33021;&#37327;&#65292;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#20197;HDF5&#25991;&#20214;&#26684;&#24335;&#19982;&#30456;&#20851;&#20803;&#25968;&#25454;&#19968;&#21516;&#20998;&#20139;&#12290;&#27492;&#21457;&#24067;&#26088;&#22312;&#25903;&#25345;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enclosed data release consists of a subset of the calibration data from the Majorana Demonstrator experiment. Each Majorana event is accompanied by raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, all shared in an HDF5 file format along with relevant metadata. This release is specifically designed to support the training and testing of Artificial Intelligence (AI) and Machine Learning (ML) algorithms upon our data. This document is structured as follows. Section I provides an overview of the dataset's content and format; Section II outlines the location of this dataset and the method for accessing it; Section III presents the NPML Machine Learning Challenge associated with this dataset; Section IV contains a disclaimer from the Majorana collaboration regarding the use of this dataset; Appendix A contains technical details of this data release. Please direct questions about the material provided within this release to liaobo77@ucsd.ed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>FE-PINN&#26159;&#19968;&#31181;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#22312;&#20027;&#35757;&#32451;&#20043;&#21069;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#35299;&#20915;&#38382;&#39064;&#30340;&#27169;&#24335;&#12290;&#19982;&#20256;&#32479;PINN&#30456;&#27604;&#65292;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#21644;&#26356;&#39640;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.08873</link><description>&lt;p&gt;
&#29305;&#24449;&#24378;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;FE-PINN&#65289;&#65306;&#22312;&#30446;&#26631;&#20219;&#21153;&#20043;&#21069;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task. (arXiv:2308.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08873
&lt;/p&gt;
&lt;p&gt;
FE-PINN&#26159;&#19968;&#31181;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#22312;&#20027;&#35757;&#32451;&#20043;&#21069;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#35299;&#20915;&#38382;&#39064;&#30340;&#27169;&#24335;&#12290;&#19982;&#20256;&#32479;PINN&#30456;&#27604;&#65292;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#21644;&#26356;&#39640;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#24378;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;FE-PINN&#65289;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#20027;&#35757;&#32451;&#24490;&#29615;&#20043;&#21069;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#20219;&#20309;&#38382;&#39064;&#30340;&#24213;&#23618;&#27169;&#24335;&#12290;&#30001;&#20110;&#23384;&#22312;&#20559;&#24494;&#20998;&#27531;&#24046;&#21644;&#36793;&#30028;&#26465;&#20214;&#22343;&#26041;&#35823;&#24046;&#20004;&#20010;&#39033;&#65292;&#26222;&#36890;PINN&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#12290;FE-PINN&#36890;&#36807;&#21482;&#38656;&#19968;&#20998;&#38047;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#32791;&#26102;&#25968;&#23567;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#23436;&#25104;&#36825;&#20010;&#36807;&#31243;&#12290;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#23398;&#20064;&#26377;&#20851;&#24213;&#23618;&#29289;&#29702;&#30340;&#26377;&#29992;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#20197;&#23436;&#21892;&#35745;&#31639;&#12290;FE-PINN&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#38382;&#39064;&#65306;&#22278;&#26609;&#20307;&#19978;&#30340;&#27969;&#21160;&#12289;&#20108;&#32500;&#28909;&#20256;&#23548;&#20197;&#21450;&#35745;&#31639;&#20837;&#21475;&#36895;&#24230;&#30340;&#36870;&#38382;&#39064;&#12290;FE-PINN&#21487;&#20197;&#20998;&#21035;&#21152;&#36895;15&#20493;&#12289;2&#20493;&#21644;5&#20493;&#22320;&#35299;&#20915;&#27599;&#20010;&#26696;&#20363;&#12290;&#21478;&#22806;
&lt;/p&gt;
&lt;p&gt;
In this work, a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN) is introduced. This framework is capable of learning the underlying pattern of any problem with low computational cost before the main training loop. The loss function of vanilla PINN due to the existence of two terms of partial differential residuals and boundary condition mean squared error is imbalanced. FE-PINN solves this challenge with just one minute of training instead of time-consuming hyperparameter tuning for loss function that can take hours. The FE-PINN accomplishes this process by performing a sequence of sub-tasks. The first sub-task learns useful features about the underlying physics. Then, the model trains on the target task to refine the calculations. FE-PINN is applied to three benchmarks, flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity. FE-PINN can solve each case with, 15x, 2x, and 5x speed up accordingly. Another
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08469</link><description>&lt;p&gt;
LLM4TS:&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#20004;&#38454;&#27573;&#24494;&#35843;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08469
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20511;&#37492;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#32479;&#19968;&#27169;&#22411;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#65292;&#25105;&#20204;&#35774;&#24819;&#21019;&#24314;&#19968;&#20010;&#31867;&#20284;&#30340;&#27169;&#22411;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#26500;&#24314;&#31283;&#20581;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;LLM4TS&#19987;&#27880;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20462;&#34917;&#19982;&#26102;&#38388;&#32534;&#30721;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20248;&#20808;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#24494;&#35843;&#36807;&#31243;&#65306;&#39318;&#20808;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#20351;LLMs&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#28982;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#19979;&#28216;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#19981;&#36827;&#34892;&#22823;&#37327;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#39044;&#35757;&#32451;LLMs&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06378</link><description>&lt;p&gt;
DCNFIS&#65306;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#36879;&#26126;&#24230;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#33879;&#21517;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#20294;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#24182;&#22312;&#22235;&#20010;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#19982;&#19977;&#20010;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;DCNFIS&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#36879;&#26126;&#24230;&#65292;&#20174;DCNFIS&#20013;&#32534;&#30721;&#30340;&#27169;&#31946;&#35268;&#21017;&#20013;&#25552;&#21462;&#35299;&#37322;&#65292;&#20197;&#28176;&#21464;&#26144;&#23556;&#30340;&#24418;&#24335;&#23637;&#31034;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;Fashion-MNIST&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#35299;&#37322;&#30340;&#29305;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.01921</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#29983;&#29289;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#33647;&#29289;&#20998;&#23376;&#24555;&#36895;&#31579;&#36873;&#26159;&#33647;&#29289;&#21457;&#29616;&#31649;&#32447;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#39640;&#36890;&#37327;&#21644;&#39640;&#20934;&#30830;&#24615;&#20998;&#23376;&#23545;&#25509;&#20195;&#29702;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;30&#19975;&#31181;&#33647;&#29289;&#20505;&#36873;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22270;&#31070;&#32463;&#25351;&#32441;&#23545;&#25509;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23545;&#22823;&#22810;&#25968;&#23545;&#25509;&#38774;&#28857;&#30340;&#22343;&#26041;&#35823;&#24046;&#20302;&#20110;0.21 kcal/mol&#65292;&#30456;&#27604;&#20256;&#32479;&#22278;&#24418;&#25351;&#32441;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20351;&#31070;&#32463;&#25351;&#32441;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#38774;&#28857;&#19978;&#35757;&#32451;&#30340;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#33258;&#21160;&#26816;&#27979;&#23454;&#29616;&#22836;&#37096;CT&#22270;&#20687;&#19977;&#32500;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#37325;&#24314;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#20943;&#23569;&#20102;&#25163;&#21160;&#24178;&#39044;&#12290;&#36890;&#36807;&#35782;&#21035;&#21644;&#35780;&#20272;&#30518;&#19979;&#32447;&#26631;&#24535;&#28857;&#65292;&#23454;&#29616;&#22312;&#37325;&#24314;&#20043;&#21069;&#33258;&#21160;&#37325;&#26032;&#26684;&#24335;&#21270;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2307.16440</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#33258;&#21160;&#26816;&#27979;&#23454;&#29616;&#22836;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25104;&#20687;&#37325;&#24314;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Head Computed Tomography Image Reconstruction Standardization with Deep Learning Assisted Automatic Detection. (arXiv:2307.16440v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16440
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#33258;&#21160;&#26816;&#27979;&#23454;&#29616;&#22836;&#37096;CT&#22270;&#20687;&#19977;&#32500;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#37325;&#24314;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#20943;&#23569;&#20102;&#25163;&#21160;&#24178;&#39044;&#12290;&#36890;&#36807;&#35782;&#21035;&#21644;&#35780;&#20272;&#30518;&#19979;&#32447;&#26631;&#24535;&#28857;&#65292;&#23454;&#29616;&#22312;&#37325;&#24314;&#20043;&#21069;&#33258;&#21160;&#37325;&#26032;&#26684;&#24335;&#21270;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25104;&#20687;(CT)&#22270;&#20687;&#30340;&#19977;&#32500;&#37325;&#24314;&#25581;&#31034;&#20102;&#32452;&#32455;&#32467;&#26500;&#30340;&#22797;&#26434;&#31354;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#24110;&#21161;&#20934;&#30830;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#25216;&#26415;&#20154;&#21592;&#25670;&#20301;&#19981;&#33391;&#12289;&#24739;&#32773;&#36523;&#20307;&#38480;&#21046;&#25110;CT&#25195;&#25551;&#20202;&#20542;&#26012;&#35282;&#24230;&#38480;&#21046;&#65292;&#33719;&#24471;&#29702;&#24819;&#30340;&#22836;&#37096;CT&#25195;&#25551;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25163;&#21160;&#26684;&#24335;&#21270;&#21644;&#37325;&#24314;&#19981;&#20165;&#24341;&#20837;&#20027;&#35266;&#24615;&#65292;&#32780;&#19988;&#32791;&#36153;&#26102;&#38388;&#21644;&#21171;&#21160;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#21160;&#22836;&#37096;CT&#22270;&#20687;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#20943;&#23569;&#20102;&#25163;&#21160;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#65292;&#35782;&#21035;&#21644;&#35780;&#20272;&#30518;&#19979;&#32447;&#26631;&#24535;&#28857;&#65292;&#20197;&#22312;&#37325;&#24314;&#20043;&#21069;&#33258;&#21160;&#37325;&#26032;&#26684;&#24335;&#21270;&#22270;&#20687;&#12290;&#37492;&#20110;&#20851;&#20110;&#22836;&#37096;CT&#22270;&#20687;&#32972;&#26223;&#19979;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#30340;&#35780;&#20272;&#36739;&#23569;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three-dimensional (3D) reconstruction of head Computed Tomography (CT) images elucidates the intricate spatial relationships of tissue structures, thereby assisting in accurate diagnosis. Nonetheless, securing an optimal head CT scan without deviation is challenging in clinical settings, owing to poor positioning by technicians, patient's physical constraints, or CT scanner tilt angle restrictions. Manual formatting and reconstruction not only introduce subjectivity but also strain time and labor resources. To address these issues, we propose an efficient automatic head CT images 3D reconstruction method, improving accuracy and repeatability, as well as diminishing manual intervention. Our approach employs a deep learning-based object detection algorithm, identifying and evaluating orbitomeatal line landmarks to automatically reformat the images prior to reconstruction. Given the dearth of existing evaluations of object detection algorithms in the context of head CT images, we compared
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#19968;&#32452;&#29305;&#24449;&#26144;&#23556;&#65292;&#20877;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#20844;&#24335;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13149</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions. (arXiv:2307.13149v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#22810;&#39033;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#19968;&#32452;&#29305;&#24449;&#26144;&#23556;&#65292;&#20877;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#20844;&#24335;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#24377;&#24615;&#22609;&#24615;&#27169;&#22411;&#36890;&#24120;&#34987;&#35748;&#20026;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20004;&#27493;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19987;&#23478;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#65292;&#20854;&#20013;&#23624;&#26381;&#26354;&#38754;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#19968;&#32452;&#21333;&#21464;&#37327;&#29305;&#24449;&#26144;&#23556;&#26469;&#34920;&#31034;&#30340;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#23558;&#36825;&#32452;&#21333;&#21464;&#37327;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#20989;&#25968;&#37325;&#26032;&#35299;&#37322;&#20026;&#25968;&#23398;&#24418;&#24335;&#12290;&#36825;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#37325;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#25552;&#39640;&#20102;&#29992;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26448;&#26009;&#30340;&#23646;&#24615;&#65288;&#22914;&#20984;&#24615;&#21644;&#23545;&#31216;&#24615;&#65289;&#26377;&#19968;&#20010;&#20855;&#20307;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#36335;&#24452;&#26469;&#38477;&#20302;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#20272;&#35745;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;&#30456;&#20851;&#20989;&#25968;&#24182;&#36827;&#34892;&#26657;&#20934;&#12290;&#36825;&#19982;&#26368;&#22823;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.12703</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#26368;&#20248;&#30456;&#20851;&#25628;&#32034;&#29992;&#20110;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#21644;&#26368;&#22823;&#26368;&#20248;&#20256;&#36755;&#20013;&#30340;&#26041;&#24046;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport. (arXiv:2307.12703v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#36335;&#24452;&#26469;&#38477;&#20302;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#20272;&#35745;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;&#30456;&#20851;&#20989;&#25968;&#24182;&#36827;&#34892;&#26657;&#20934;&#12290;&#36825;&#19982;&#26368;&#22823;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;$f(X_T)$&#30340;&#26041;&#24046;&#38477;&#20302;&#31639;&#27861;&#65292;&#20854;&#20013;$X$&#26159;&#26576;&#20010;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;$f$&#26159;&#19968;&#20010;&#27979;&#35797;&#20989;&#25968;&#12290;&#26032;&#30340;&#20272;&#35745;&#22120;&#26159;$(f(X^1_T) + f(X^2_T))/2$&#65292;&#20854;&#20013;$X^1$&#21644;$X^2$&#20855;&#26377;&#19982;$X$&#30456;&#21516;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#20294;&#36335;&#24452;&#19978;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20197;&#38477;&#20302;&#26041;&#24046;&#12290;&#26368;&#20248;&#30456;&#20851;&#20989;&#25968;$\rho$&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#65292;&#24182;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;$(X^1, X^2)$&#30340;&#36712;&#36857;&#19978;&#36827;&#34892;&#26657;&#20934;&#12290;&#22312;&#32473;&#23450;&#36793;&#38469;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20248;&#32806;&#21512;&#19982;&#26368;&#22823;&#26368;&#20248;&#20256;&#36755;&#26377;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new algorithm for variance reduction when estimating $f(X_T)$ where $X$ is the solution to some stochastic differential equation and $f$ is a test function. The new estimator is $(f(X^1_T) + f(X^2_T))/2$, where $X^1$ and $X^2$ have same marginal law as $X$ but are pathwise correlated so that to reduce the variance. The optimal correlation function $\rho$ is approximated by a deep neural network and is calibrated along the trajectories of $(X^1, X^2)$ by policy gradient and reinforcement learning techniques. Finding an optimal coupling given marginal laws has links with maximum optimal transport.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#32463;&#36807;&#33976;&#39311;&#30340;&#25968;&#25454;&#26080;&#27861;&#24456;&#22909;&#22320;&#36827;&#34892;&#26657;&#20934;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#30340; logits &#20998;&#24067;&#26356;&#21152;&#38598;&#20013;&#65292;&#24182;&#19988;&#35821;&#20041;&#26126;&#30830;&#20294;&#19982;&#20998;&#31867;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#20250;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36974;&#34109;&#28201;&#24230;&#32553;&#25918; (MTS) &#21644;&#36974;&#34109;&#33976;&#39311;&#35757;&#32451; (MDT) &#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.12463</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25968;&#25454;&#33976;&#39311;&#65306;&#19981;&#35201;&#24573;&#35270;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rethinking Data Distillation: Do Not Overlook Calibration. (arXiv:2307.12463v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#32463;&#36807;&#33976;&#39311;&#30340;&#25968;&#25454;&#26080;&#27861;&#24456;&#22909;&#22320;&#36827;&#34892;&#26657;&#20934;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#30340; logits &#20998;&#24067;&#26356;&#21152;&#38598;&#20013;&#65292;&#24182;&#19988;&#35821;&#20041;&#26126;&#30830;&#20294;&#19982;&#20998;&#31867;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#20250;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36974;&#34109;&#28201;&#24230;&#32553;&#25918; (MTS) &#21644;&#36974;&#34109;&#33976;&#39311;&#35757;&#32451; (MDT) &#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#36807;&#33976;&#39311;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#36755;&#20986;&#65292;&#24182;&#38656;&#35201;&#36890;&#36807;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20462;&#27491;&#12290;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#22914;&#28201;&#24230;&#32553;&#25918;&#21644;&#28151;&#21512;&#35757;&#32451;&#65292;&#22312;&#21407;&#22987;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#25928;&#26524;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23545;&#20174;&#22823;&#28304;&#25968;&#25454;&#38598;&#33976;&#39311;&#20986;&#30340;&#25968;&#25454;&#36827;&#34892;&#26657;&#20934;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33976;&#39311;&#25968;&#25454;&#20250;&#23548;&#33268;&#32593;&#32476;&#26080;&#27861;&#26657;&#20934;&#65292;&#21407;&#22240;&#26159;&#65288;i&#65289;&#26368;&#22823;logit&#20998;&#24067;&#26356;&#20026;&#38598;&#20013;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22312;&#20998;&#31867;&#20219;&#21153;&#26080;&#20851;&#20294;&#35821;&#20041;&#24847;&#20041;&#26126;&#30830;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36974;&#34109;&#28201;&#24230;&#32553;&#25918;&#65288;MTS&#65289;&#21644;&#36974;&#34109;&#33976;&#39311;&#35757;&#32451;&#65288;MDT&#65289;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#33976;&#39311;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#20445;&#25345;&#25968;&#25454;&#33976;&#39311;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#26356;&#22909;&#30340;&#26657;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#30340;&#21453;&#28436;&#31934;&#24230;&#65292;&#32780;&#19988;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11099</link><description>&lt;p&gt;
&#29992;&#23398;&#20064;&#30340;&#20195;&#29702;&#21644;&#32422;&#26463;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving multiphysics-based inverse problems with learned surrogates and constraints. (arXiv:2307.11099v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#30340;&#21453;&#28436;&#31934;&#24230;&#65292;&#32780;&#19988;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#36136;&#30899;&#23553;&#23384;&#30417;&#27979;&#20013;&#65292;&#24403;&#22810;&#27169;&#24577;&#26102;&#21464;&#25968;&#25454;&#26114;&#36149;&#19988;&#25968;&#20540;&#27169;&#25311;&#25104;&#26412;&#39640;&#26114;&#26102;&#65292;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;&#23398;&#20064;&#20195;&#29702;&#19982;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#31181;&#32452;&#21512;&#19981;&#20165;&#33021;&#22815;&#22823;&#22823;&#25913;&#21892;&#23545;&#37325;&#35201;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#65288;&#28183;&#36879;&#29575;&#65289;&#30340;&#21453;&#28436;&#65292;&#36824;&#33021;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#21253;&#25324;&#20117;&#27979;&#37327;&#21644;&#20027;&#21160;&#28304;&#26102;&#21464;&#22320;&#38663;&#25968;&#25454;&#65289;&#25552;&#20379;&#19968;&#20010;&#33258;&#28982;&#30340;&#24179;&#21488;&#12290;&#36890;&#36807;&#28155;&#21152;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#34892;&#30340;&#21453;&#28436;&#26041;&#27861;&#65292;&#20854;&#31934;&#24230;&#20173;&#28982;&#20934;&#30830;&#12290;&#36825;&#36890;&#36807;&#21253;&#21547;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;&#24402;&#19968;&#21270;&#27969;&#65289;&#65292;&#20351;&#27169;&#22411;&#36845;&#20195;&#20445;&#25345;&#22312;&#20998;&#24067;&#20869;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#20316;&#20026;&#20195;&#29702;&#30340;&#32463;&#36807;&#35757;&#32451;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#31639;&#23376;&#29992;&#20110;&#20195;&#26367;&#28041;&#21450;&#37096;&#20998;&#35745;&#31639;&#26114;&#36149;&#30340;&#22810;&#30456;&#27969;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving multiphysics-based inverse problems for geological carbon storage monitoring can be challenging when multimodal time-lapse data are expensive to collect and costly to simulate numerically. We overcome these challenges by combining computationally cheap learned surrogates with learned constraints. Not only does this combination lead to vastly improved inversions for the important fluid-flow property, permeability, it also provides a natural platform for inverting multimodal data including well measurements and active-source time-lapse seismic data. By adding a learned constraint, we arrive at a computationally feasible inversion approach that remains accurate. This is accomplished by including a trained deep neural network, known as a normalizing flow, which forces the model iterates to remain in-distribution, thereby safeguarding the accuracy of trained Fourier neural operators that act as surrogates for the computationally expensive multiphase flow simulations involving partia
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#20197;&#21450;&#21019;&#24314;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;</title><link>http://arxiv.org/abs/2307.01717</link><description>&lt;p&gt;
&#20851;&#20110;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Constrained Time-Series Generation Problem. (arXiv:2307.01717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#20197;&#21450;&#21019;&#24314;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#32463;&#24120;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#29992;&#20110;&#22686;&#21152;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#24182;&#21019;&#24314;&#30001;&#26102;&#38388;&#24207;&#21015;&#25551;&#36848;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#20998;&#24067;&#30456;&#20284;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30495;&#23454;&#24615;&#65289;&#20197;&#21450;&#28385;&#36275;&#19968;&#23450;&#25968;&#20540;&#32422;&#26463;&#26159;&#21453;&#20107;&#23454;&#26102;&#38388;&#24207;&#21015;&#22330;&#26223;&#29983;&#25104;&#35831;&#27714;&#20013;&#24120;&#35265;&#30340;&#35201;&#27714;&#12290;&#20363;&#22914;&#65292;&#32654;&#32852;&#20648;&#21457;&#24067;&#20102;&#32473;&#23450;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#30340;&#21512;&#25104;&#24066;&#22330;&#21387;&#21147;&#24773;&#26223;&#65292;&#20379;&#37329;&#34701;&#26426;&#26500;&#35780;&#20272;&#20854;&#22312;&#20551;&#35774;&#24615;&#34928;&#36864;&#20013;&#30340;&#34920;&#29616;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24809;&#32602;&#26469;&#24378;&#21046;&#28385;&#36275;&#32422;&#26463;&#65292;&#24182;&#25298;&#32477;&#19981;&#31526;&#21512;&#32422;&#26463;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#25913;&#21464;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#32780;&#25298;&#32477;&#25277;&#26679;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints.
&lt;/p&gt;</description></item><item><title>Engression&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#21644;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#22806;&#20063;&#33021;&#21487;&#38752;&#22320;&#36827;&#34892;&#22806;&#25512;&#12290;</title><link>http://arxiv.org/abs/2307.00835</link><description>&lt;p&gt;
Engression: &#38750;&#32447;&#24615;&#22238;&#24402;&#30340;&#22806;&#25512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Engression: Extrapolation for Nonlinear Regression?. (arXiv:2307.00835v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00835
&lt;/p&gt;
&lt;p&gt;
Engression&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#21644;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#22806;&#20063;&#33021;&#21487;&#38752;&#22320;&#36827;&#34892;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#25512;&#23545;&#20110;&#35768;&#22810;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24120;&#24120;&#20250;&#36935;&#21040;&#36229;&#20986;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#26469;&#35828;&#65292;&#22806;&#25512;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36890;&#24120;&#36935;&#21040;&#22256;&#38590;&#65306;&#26641;&#38598;&#25104;&#27169;&#22411;&#22312;&#25903;&#25345;&#33539;&#22260;&#22806;&#25552;&#20379;&#36830;&#32493;&#30340;&#39044;&#27979;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24448;&#24448;&#21464;&#24471;&#19981;&#21487;&#25511;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#20854;&#21487;&#38752;&#24615;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#19981;&#20250;&#31435;&#21363;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#21517;&#20026;&#8220;engression&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#65292;&#20854;&#20013;&#22122;&#22768;&#28155;&#21152;&#21040;&#21327;&#21464;&#37327;&#19978;&#24182;&#24212;&#29992;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36890;&#24120;&#36866;&#29992;&#20110;&#35768;&#22810;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;engression&#21487;&#20197;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#25104;&#21151;&#36827;&#34892;&#22806;&#25512;&#65292;&#20363;&#22914;&#20005;&#26684;&#38480;&#21046;&#22122;&#22768;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extrapolation is crucial in many statistical and machine learning applications, as it is common to encounter test data outside the training support. However, extrapolation is a considerable challenge for nonlinear models. Conventional models typically struggle in this regard: while tree ensembles provide a constant prediction beyond the support, neural network predictions tend to become uncontrollable. This work aims at providing a nonlinear regression methodology whose reliability does not break down immediately at the boundary of the training support. Our primary contribution is a new method called `engression' which, at its core, is a distributional regression technique for pre-additive noise models, where the noise is added to the covariates before applying a nonlinear transformation. Our experimental results indicate that this model is typically suitable for many real data sets. We show that engression can successfully perform extrapolation under some assumptions such as a strictl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#24635;&#32467;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.16559</link><description>&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#65306;&#23545;&#23646;&#24615;&#38388;&#21327;&#20316;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Feature Selection: A perspective on inter-attribute cooperation. (arXiv:2306.16559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#24635;&#32467;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#23545;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#20219;&#21153;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#29305;&#24449;&#36873;&#25321;&#26159;&#22788;&#29702;&#32500;&#24230;&#32553;&#20943;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#65292;&#36890;&#24120;&#26159;&#22312;&#24212;&#29992;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#30340;&#37325;&#35201;&#25968;&#25454;&#22788;&#29702;&#27493;&#39588;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#30456;&#20851;&#24615;&#25490;&#24207;&#31639;&#27861;&#21457;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;-&#20887;&#20313;&#26435;&#34913;&#21644;&#22522;&#20110;&#22810;&#20803;&#20381;&#36182;&#24615;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#25429;&#25417;&#22810;&#21464;&#37327;&#20381;&#36182;&#30340;&#36235;&#21183;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#38388;&#30340;&#20114;&#30456;&#21512;&#20316;&#33719;&#21462;&#20851;&#20110;&#31867;&#21035;&#30340;&#29420;&#29305;&#20449;&#24687;&#12290;&#26412;&#25991;&#23545;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#19981;&#21516;&#26041;&#27861;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional datasets depict a challenge for learning tasks in data mining and machine learning. Feature selection is an effective technique in dealing with dimensionality reduction. It is often an essential data processing step prior to applying a learning algorithm. Over the decades, filter feature selection methods have evolved from simple univariate relevance ranking algorithms to more sophisticated relevance-redundancy trade-offs and to multivariate dependencies-based approaches in recent years. This tendency to capture multivariate dependence aims at obtaining unique information about the class from the intercooperation among features. This paper presents a comprehensive survey of the state-of-the-art work on filter feature selection methods assisted by feature intercooperation, and summarizes the contributions of different approaches found in the literature. Furthermore, current issues and challenges are introduced to identify promising future research and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#20102;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.14066</link><description>&lt;p&gt;
SEEDS&#65306;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20223;&#30495;&#22825;&#27668;&#39044;&#27979;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models. (arXiv:2306.14066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#20102;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#26410;&#26469;&#22825;&#27668;&#26102;&#65292;&#27010;&#29575;&#39044;&#27979;&#23545;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#20027;&#35201;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#39044;&#27979;&#38598;&#21512;&#26469;&#34920;&#31034;&#21644;&#37327;&#21270;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#38598;&#21512;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#26368;&#36817;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;5&#25104;&#21592;&#38598;&#21512;GEFS&#37325;&#26032;&#39044;&#25253;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#20135;&#29983;&#32852;&#21512;&#24773;&#20917;&#19979;&#30495;&#23454;&#30340;&#22825;&#27668;&#39044;&#27979;&#65292;&#36825;&#20123;&#24773;&#20917;&#21487;&#20197;&#22522;&#20110;&#25805;&#20316;GEFS&#39044;&#27979;&#31995;&#32479;&#30340;&#23569;&#25968;&#25104;&#21592;&#26465;&#20214;&#21270;&#12290;&#26681;&#25454;ERA5&#20998;&#26512;&#35780;&#20272;&#65292;&#29983;&#25104;&#30340;&#38598;&#21512;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#20855;&#26377;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23558;&#30456;&#21516;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#24320;&#21457;&#25193;&#25955;&#27169;&#22411;&#65292;&#36827;&#34892;&#29983;&#25104;&#21518;&#22788;&#29702;&#12290;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#23569;&#25968;&#39044;&#27979;&#25104;&#21592;&#26465;&#20214;&#21270;&#22320;&#29983;&#25104;&#31867;&#20284;&#20110;&#29289;&#29702;&#22823;&#27169;&#22411;&#38598;&#21512;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic forecasting is crucial to decision-making under uncertainty about future weather. The dominant approach is to use an ensemble of forecasts to represent and quantify uncertainty in operational numerical weather prediction. However, generating ensembles is computationally costly. In this paper, we propose to generate ensemble forecasts at scale by leveraging recent advances in generative artificial intelligence. Our approach learns a data-driven probabilistic diffusion model from the 5-member ensemble GEFS reforecast dataset. The model can then be sampled efficiently to produce realistic weather forecasts, conditioned on a few members of the operational GEFS forecasting system. The generated ensembles have similar predictive skill as the full GEFS 31-member ensemble, evaluated against ERA5 reanalysis, and emulate well the statistics of large physics-based ensembles. We also apply the same methodology to developing a diffusion model for generative post-processing: the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.10359</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#39537;&#21160;Foley&#38899;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foley&#38899;&#25928;&#29983;&#25104;&#26088;&#22312;&#20026;&#22810;&#23186;&#20307;&#20869;&#23481;&#29983;&#25104;&#32972;&#26223;&#38899;&#25928;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#24320;&#21457;&#38598;&#20316;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#25968;&#23383;&#25110;one-hot&#21521;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#37197;&#23545;&#65288;CLAP&#65289;&#25216;&#26415;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26469;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#20043;&#21518;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#38899;&#39057;&#29255;&#27573;&#24182;&#36873;&#25321;&#26368;&#20339;&#29255;&#27573;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#29983;&#25104;&#30340;&#27874;&#24418;&#65292;&#26368;&#20339;&#29255;&#27573;&#26159;&#26681;&#25454;&#23884;&#20837;&#20043;&#38388;&#30456;&#20284;&#24615;&#24471;&#20998;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#21098;&#20108;&#36827;&#21046;&#21270;&#65288;APB&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#21270;&#21644;&#20462;&#21098;&#65292;&#21033;&#29992;&#23569;&#37327;&#20840;&#31934;&#24230;&#26435;&#37325;&#26469;&#22686;&#24378;&#20108;&#36827;&#21046;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#30340;&#31639;&#27861;&#22312;CPU&#19978;&#23454;&#29616;&#20102;&#39640;&#36895;&#30340;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#36816;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.08960</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#36827;&#21046;&#21270;&#21644;&#23569;&#37327;&#20840;&#31934;&#24230;&#26435;&#37325;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Neural Network Compression using Binarization and Few Full-Precision Weights. (arXiv:2306.08960v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#21098;&#20108;&#36827;&#21046;&#21270;&#65288;APB&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#21270;&#21644;&#20462;&#21098;&#65292;&#21033;&#29992;&#23569;&#37327;&#20840;&#31934;&#24230;&#26435;&#37325;&#26469;&#22686;&#24378;&#20108;&#36827;&#21046;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#30340;&#31639;&#27861;&#22312;CPU&#19978;&#23454;&#29616;&#20102;&#39640;&#36895;&#30340;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#36816;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#21644;&#20462;&#21098;&#26159;&#20004;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#20462;&#21098;&#20108;&#36827;&#21046;&#21270;&#65288;APB&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#37327;&#21270;&#21644;&#20462;&#21098;&#30340;&#26032;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;APB&#21033;&#29992;&#23569;&#37327;&#20840;&#31934;&#24230;&#26435;&#37325;&#22686;&#24378;&#20102;&#20108;&#36827;&#21046;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#26368;&#22823;&#21270;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#20854;&#20869;&#23384;&#21344;&#29992;&#65292;&#36890;&#36807;&#20915;&#23450;&#27599;&#20010;&#26435;&#37325;&#26159;&#24212;&#35813;&#36827;&#34892;&#20108;&#36827;&#21046;&#21270;&#36824;&#26159;&#20445;&#25345;&#20840;&#31934;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#20854;&#20998;&#35299;&#20026;&#20108;&#36827;&#21046;&#21644;&#31232;&#30095;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#26469;&#39640;&#25928;&#22320;&#25191;&#34892;&#20351;&#29992;APB&#21387;&#32553;&#30340;&#21069;&#21521;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;CPU&#19978;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#39640;&#25928;&#30340;&#20301;&#25805;&#20316;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#24555;6.9&#20493;&#21644;1.5&#20493;&#12290;&#25105;&#20204;&#23545;APB&#22312;&#20004;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#27169;&#22411;&#21387;&#32553;&#25968;&#25454;&#38598;CIFAR10&#21644;Imag&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization and pruning are two effective Deep Neural Networks model compression methods. In this paper, we propose Automatic Prune Binarization (APB), a novel compression technique combining quantization with pruning. APB enhances the representational capability of binary networks using a few full-precision weights. Our technique jointly maximizes the accuracy of the network while minimizing its memory impact by deciding whether each weight should be binarized or kept in full precision. We show how to efficiently perform a forward pass through layers compressed using APB by decomposing it into a binary and a sparse-dense matrix multiplication. Moreover, we design two novel efficient algorithms for extremely quantized matrix multiplication on CPU, leveraging highly efficient bitwise operations. The proposed algorithms are 6.9x and 1.5x faster than available state-of-the-art solutions. We extensively evaluate APB on two widely adopted model compression datasets, namely CIFAR10 and Imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05554</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#20013;&#30340;&#24212;&#29992;&#21644;&#39044;&#27979;&#65306;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks. (arXiv:2306.05554v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#65288;COUCSI&#65289;&#26159;&#19968;&#31181;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#28070;&#28287;&#30456;&#21462;&#20195;&#20102;&#38750;&#28070;&#28287;&#30456;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#26089;&#26399;&#65288;ET&#65289;&#21644;&#26202;&#26399;&#65288;LT&#65289;COUCSI&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#20197;&#25913;&#36827;PINNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#33258;&#21464;&#37327;&#23558;COUCSI&#38382;&#39064;&#20998;&#21035;&#29992;XT-&#65292;XY-&#21644;Z-&#19977;&#31181;&#31561;&#25928;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65306;&#31532;&#19968;&#20010;&#25551;&#36848;&#20102;&#39281;&#21644;&#24230;&#20316;&#20026;&#35268;&#33539;&#21270;&#20301;&#32622;X&#21644;&#26102;&#38388;T&#30340;&#20989;&#25968;;&#31532;&#20108;&#20010;&#25551;&#36848;&#20102;X&#21644;Y=T^0.5&#20316;&#20026;&#20989;&#25968;&#30340;&#39281;&#21644;&#24230;;&#31532;&#19977;&#20010;&#20316;&#20026;Z=X/T^0.5&#30340;&#21807;&#19968;&#20989;&#25968;&#65288;&#20165;&#22312;ET&#19979;&#26377;&#25928;&#65289;&#12290;&#35813;PINN&#27169;&#22411;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#65292;&#24182;&#22522;&#20110;&#26368;&#23567;&#21270;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#20002;&#22833;&#39033;&#21644;&#19982;&#21021;&#22987;&#36793;&#30028;&#26465;&#20214;&#30456;&#23545;&#24212;&#30340;&#39033;&#12290;&#27809;&#26377;&#21512;&#25104;&#25110;&#23454;&#39564;&#25968;&#25454;&#34987;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were invo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;AUC&#20248;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#19978;&#37117;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.15776</link><description>&lt;p&gt;
&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;AUC&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AUC Optimization from Multiple Unlabeled Datasets. (arXiv:2305.15776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;AUC&#20248;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#19978;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#22312;&#32570;&#20047;&#23436;&#32654;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36171;&#20104;&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#65292;&#36825;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26696;&#20363;&#20043;&#19968;&#26159;&#20165;&#20102;&#35299;&#31867;&#21035;&#20808;&#39564;&#30693;&#35782;&#30340;&#22810;&#20010;&#26410;&#26631;&#35760;(U)&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#25110;&#31216;&#20026;U^m&#23398;&#20064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#25104;&#23545;&#25490;&#21517;&#33021;&#21147;&#30340;AUC (ROC&#26354;&#32447;&#19979;&#38754;&#31215;) &#20248;&#21270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;U^m-AUC&#65292;&#19968;&#31181;&#23558;U^m&#25968;&#25454;&#36716;&#25442;&#20026;&#22810;&#26631;&#35760;AUC&#20248;&#21270;&#38382;&#39064;&#24182;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#30340;AUC&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;U^m-AUC&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning aims to empower machine learning when the perfect supervision is unavailable, which has drawn great attention from researchers. Among various types of weak supervision, one of the most challenging cases is to learn from multiple unlabeled (U) datasets with only a little knowledge of the class priors, or U$^m$ learning for short. In this paper, we study the problem of building an AUC (area under ROC curve) optimization model from multiple unlabeled datasets, which maximizes the pairwise ranking ability of the classifier. We propose U$^m$-AUC, an AUC optimization approach that converts the U$^m$ data into a multi-label AUC optimization problem, and can be trained efficiently. We show that the proposed U$^m$-AUC is effective theoretically and empirically.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;DARTS&#20248;&#21270;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20197;&#24448;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14402</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;iable&#30340;&#25628;&#32034;&#20307;&#31995;&#26550;&#26500;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Emotion Recognition Performance using Differentiable Architecture Search. (arXiv:2305.14402v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;DARTS&#20248;&#21270;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20197;&#24448;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;(SER)&#26159;&#23454;&#29616;&#24773;&#24863;&#24863;&#30693;&#20132;&#20114;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28145;&#24230;&#23398;&#20064;(DL)&#25913;&#21892;&#20102;SER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#35774;&#35745;DL&#20307;&#31995;&#32467;&#26500;&#38656;&#35201;&#20808;&#21069;&#30340;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#12290;&#40723;&#21169;&#22320;&#65292;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;(NAS)&#20801;&#35768;&#33258;&#21160;&#25628;&#32034;&#26368;&#20248;DL&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#21487;&#21306;&#20998;&#30340;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;(DARTS)&#26159;&#19968;&#31181;&#20351;&#29992;NAS&#25628;&#32034;&#26368;&#20248;&#21270;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DARTS&#29992;&#20110;&#32852;&#21512;CNN&#21644;LSTM&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25913;&#21892;SER&#24615;&#33021;&#12290;&#25105;&#20204;&#36873;&#25321;CNN LSTM&#32806;&#21512;&#30340;&#21407;&#22240;&#26159;&#32467;&#26524;&#34920;&#26126;&#31867;&#20284;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;&#34429;&#28982;SER&#30740;&#31350;&#20154;&#21592;&#24050;&#23558;CNN&#21644;RNN&#20998;&#21035;&#32771;&#34385;&#65292;&#20294;DARTs&#21516;&#26102;&#29992;&#20110;CNN&#21644;LSTM&#30340;&#21487;&#34892;&#24615;&#20173;&#38656;&#35201;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;IEMOCAP&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;DA&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) is a critical enabler of emotion-aware communication in human-computer interactions. Deep Learning (DL) has improved the performance of SER models by improving model complexity. However, designing DL architectures requires prior experience and experimental evaluations. Encouragingly, Neural Architecture Search (NAS) allows automatic search for an optimum DL model. In particular, Differentiable Architecture Search (DARTS) is an efficient method of using NAS to search for optimised models. In this paper, we propose DARTS for a joint CNN and LSTM architecture for improving SER performance. Our choice of the CNN LSTM coupling is inspired by results showing that similar models offer improved performance. While SER researchers have considered CNNs and RNNs separately, the viability of using DARTs jointly for CNN and LSTM still needs exploration. Experimenting with the IEMOCAP dataset, we demonstrate that our approach outperforms best-reported results using DA
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21464;&#28857;&#30417;&#27979;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#27604;&#29616;&#26377;&#30340;&#22312;&#32447;&#30417;&#27979;&#26041;&#27861;&#26356;&#22909;&#22320;&#25511;&#21046;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;ARIMA&#27169;&#22411;&#21644;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06630</link><description>&lt;p&gt;
&#24322;&#36136;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predictive change point detection for heterogeneous data. (arXiv:2305.06630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21464;&#28857;&#30417;&#27979;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#27604;&#29616;&#26377;&#30340;&#22312;&#32447;&#30417;&#27979;&#26041;&#27861;&#26356;&#22909;&#22320;&#25511;&#21046;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;ARIMA&#27169;&#22411;&#21644;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36741;&#21161;&#30340;&#21464;&#28857;&#26816;&#27979;&#65288;CPD&#65289;&#26694;&#26550;&#65292;&#24182;&#19982;&#20854;&#20182;&#22312;&#32447;CPD&#20363;&#31243;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;&#35813;&#26041;&#27861;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#39044;&#27979;&#27493;&#39588;&#65289;&#20195;&#26367;&#36890;&#24120;&#20351;&#29992;&#30340;&#36235;&#21183;&#20272;&#35745;&#20989;&#25968;&#65288;&#22914;&#28369;&#21160;&#24179;&#22343;&#65289;&#65292;&#24182;&#23558;&#20854;&#39044;&#27979;&#19982;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65288;&#27604;&#36739;&#27493;&#39588;&#65289;&#65292;&#20174;&#32780;&#25913;&#21892;&#39034;&#24207;&#20998;&#26512;&#20013;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20363;&#22914;CUSUM&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#36825;&#20123;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
A change point detection (CPD) framework assisted by a predictive machine learning model called ''Predict and Compare'' is introduced and characterised in relation to other state-of-the-art online CPD routines which it outperforms in terms of false positive rate and out-of-control average run length. The method's focus is on improving standard methods from sequential analysis such as the CUSUM rule in terms of these quality measures.  This is achieved by replacing typically used trend estimation functionals such as the running mean with more sophisticated predictive models (Predict step), and comparing their prognosis with actual data (Compare step). The two models used in the Predict step are the ARIMA model and the LSTM recursive neural network. However, the framework is formulated in general terms, so as to allow the use of other prediction or comparison methods than those tested here. The power of the method is demonstrated in a tribological case study in which change points separa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#23383;&#20856;&#30340;&#25345;&#20037;&#22270;&#30340;&#32039;&#20945;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#25968;&#25454;&#38477;&#32500;&#21644;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2304.14852</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#23383;&#20856;&#30340;&#25345;&#20037;&#22270;&#30340;&#32039;&#20945;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Dictionaries of Persistence Diagrams. (arXiv:2304.14852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#23383;&#20856;&#30340;&#25345;&#20037;&#22270;&#30340;&#32039;&#20945;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#25968;&#25454;&#38477;&#32500;&#21644;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#20197;&#21407;&#23376;&#22270;&#23383;&#20856;&#30340;&#21152;&#26435;Wasserstein barycenters [99]&#65292;[101] &#30340;&#24418;&#24335;&#23545;&#19968;&#32452;&#25345;&#20037;&#22270;&#36827;&#34892;&#31616;&#27905;&#32534;&#30721;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#35299;&#20915;&#30456;&#24212;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23558;Barycenter&#26435;&#37325;&#30340;&#20248;&#21270;&#19982;Atom&#22270;&#30340;&#20248;&#21270;&#20132;&#38169;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#20010;&#23376;&#38382;&#39064;&#26799;&#24230;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#30830;&#20445;&#24555;&#36895;&#36845;&#20195;&#65292;&#24182;&#19988;&#36824;&#21033;&#29992;&#20102;&#20849;&#20139;&#20869;&#23384;&#24182;&#34892;&#24615;&#12290;&#23545;&#20844;&#20849;&#21512;&#22863;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#26368;&#22823;&#31034;&#20363;&#30340;Wasserstein&#23383;&#20856;&#35745;&#31639;&#26102;&#38388;&#22312;&#25968;&#20998;&#38047;&#20043;&#20869;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#25928;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;Wassserstein&#23383;&#20856;&#24212;&#29992;&#20110;&#25968;&#25454;&#38477;&#32500;&#65292;&#24182;&#36890;&#36807;&#20165;&#29992;&#20854;&#37325;&#37327;&#26469;&#32039;&#20945;&#22320;&#34920;&#31034;Persistence&#22270;&#26469;&#21487;&#38752;&#22320;&#21387;&#32553;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a computational framework for the concise encoding of an ensemble of persistence diagrams, in the form of weighted Wasserstein barycenters [99], [101] of a dictionary of atom diagrams. We introduce a multi-scale gradient descent approach for the efficient resolution of the corresponding minimization problem, which interleaves the optimization of the barycenter weights with the optimization of the atom diagrams. Our approach leverages the analytic expressions for the gradient of both sub-problems to ensure fast iterations and it additionally exploits shared-memory parallelism. Extensive experiments on public ensembles demonstrate the efficiency of our approach, with Wasserstein dictionary computations in the orders of minutes for the largest examples. We show the utility of our contributions in two applications. First, we apply Wassserstein dictionaries to data reduction and reliably compress persistence diagrams by concisely representing them with their weights in t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.14633</link><description>&lt;p&gt;
CVRecon: &#37325;&#26032;&#24605;&#32771;&#31070;&#32463;&#37325;&#24314;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14633
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#31070;&#32463;&#37325;&#24314;&#30340;&#36827;&#23637;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20307;&#31215;&#30340;&#25216;&#26415;&#20165;&#27839;&#25972;&#20010;&#30456;&#26426;&#20809;&#32447;&#22797;&#21046;&#23545;&#35937;&#34920;&#38754;&#30340;2D&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#22797;&#21046;&#20250;&#22312;&#31354;&#27934;&#21644;&#36974;&#25377;&#31354;&#38388;&#20013;&#24341;&#20837;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#20960;&#20309;&#20307;&#25104;&#24418;&#26041;&#38754;&#20135;&#29983;&#25361;&#25112;&#12290;&#21463;&#20256;&#32479;&#22810;&#35270;&#35282;&#31435;&#20307;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#26088;&#22312;&#21033;&#29992;&#20195;&#20215;&#20307;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#34920;&#31034;&#27861;&#8212;&#8212;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#32534;&#30721;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#24674;&#22797;&#20102;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#21644;&#29702;&#35299;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#20132;&#20114;&#20013;&#37325;&#29616;&#36825;&#20123;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.13787</link><description>&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Surrogate Assisted Generation of Human-Robot Interaction Scenarios. (arXiv:2304.13787v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#21644;&#29702;&#35299;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#20132;&#20114;&#20013;&#37325;&#29616;&#36825;&#20123;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#19981;&#21516;&#29615;&#22659;&#21644;&#29992;&#25143;&#19979;&#35780;&#20272;&#21644;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#20248;&#32570;&#28857;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#20849;&#20139;&#25511;&#21046;&#36965;&#25805;&#20316;&#20219;&#21153;&#30340;&#31995;&#32479;&#22833;&#25928;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#31574;&#30053;&#21644;&#20154;&#31867;&#34892;&#20026;&#26469;&#30452;&#25509;&#35780;&#20272;&#29983;&#25104;&#30340;&#22330;&#26223;&#12290;&#36825;&#20123;&#35780;&#20272;&#25152;&#38656;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#26367;&#20195;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#20026;&#26469;&#22686;&#24378;&#22330;&#26223;&#29983;&#25104;&#31995;&#32479;&#30340;&#24314;&#35758;&#12290;&#22312;&#20849;&#20139;&#25511;&#21046;&#36965;&#25805;&#20316;&#22495;&#21644;&#26356;&#22797;&#26434;&#30340;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26367;&#20195;&#27169;&#22411;&#36741;&#21161;&#30340;&#22330;&#26223;&#29983;&#25104;&#21487;&#20197;&#39640;&#25928;&#22320;&#21512;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25925;&#38556;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20132;&#20114;&#20013;&#26159;&#21487;&#37325;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#24490;&#29615;&#24179;&#34913;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#20351;&#24471;&#32593;&#32476;&#20855;&#26377;&#25910;&#32553;&#21644;&#32791;&#25955;&#24615;&#36136;&#12290;&#27492;&#22806;&#25552;&#20986;&#30340;&#38750;&#32422;&#26463;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#24471;&#35813;&#32593;&#32476;&#23398;&#20064;&#30340;&#21442;&#25968;&#37327;&#24471;&#20197;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2304.02976</link><description>&lt;p&gt;
&#26080;&#32422;&#26463;&#21442;&#25968;&#21270;&#30340;&#32791;&#25955;&#24615;&#21644;&#25910;&#32553;&#24615;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations. (arXiv:2304.02976v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#24490;&#29615;&#24179;&#34913;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#20351;&#24471;&#32593;&#32476;&#20855;&#26377;&#25910;&#32553;&#21644;&#32791;&#25955;&#24615;&#36136;&#12290;&#27492;&#22806;&#25552;&#20986;&#30340;&#38750;&#32422;&#26463;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#24471;&#35813;&#32593;&#32476;&#23398;&#20064;&#30340;&#21442;&#25968;&#37327;&#24471;&#20197;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#21644;&#30740;&#31350;&#20102;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#30340;&#26550;&#26500;&#28304;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#26368;&#36817;&#24341;&#20837;&#30340;&#24490;&#29615;&#24179;&#34913;&#32593;&#32476;&#65288;RENs&#65289;&#30340;&#27169;&#22411;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36171;&#20104;&#25105;&#20204;&#25552;&#20986;&#30340;NodeRENs&#25910;&#32553;&#21644;&#32791;&#25955;&#24615;&#8212;&#8212;&#23545;&#20110;&#20581;&#22766;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;RENs&#19968;&#26679;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#25910;&#32553;&#21644;&#32791;&#25955;NodeRENs&#30340;&#21442;&#25968;&#21270;&#65292;&#36825;&#20123;&#21442;&#25968;&#27809;&#26377;&#32422;&#26463;&#65292;&#22240;&#27492;&#33021;&#22815;&#23398;&#20064;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;NodeRENs&#30340;&#23646;&#24615;&#65292;&#21253;&#25324;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce and study a class of Deep Neural Networks (DNNs) in continuous-time. The proposed architecture stems from the combination of Neural Ordinary Differential Equations (Neural ODEs) with the model structure of recently introduced Recurrent Equilibrium Networks (RENs). We show how to endow our proposed NodeRENs with contractivity and dissipativity -- crucial properties for robust learning and control. Most importantly, as for RENs, we derive parametrizations of contractive and dissipative NodeRENs which are unconstrained, hence enabling their learning for a large number of parameters. We validate the properties of NodeRENs, including the possibility of handling irregularly sampled data, in a case study in nonlinear system identification.
&lt;/p&gt;</description></item><item><title>BoundaryCAM&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.07853</link><description>&lt;p&gt;
BoundaryCAM&#65306;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images. (arXiv:2303.07853v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07853
&lt;/p&gt;
&lt;p&gt;
BoundaryCAM&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#21033;&#29992;&#22270;&#20687;&#32423;&#21035;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26159;&#35299;&#20915;&#20998;&#21106;&#32593;&#32476;&#38656;&#27714;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22823;&#37327;&#20687;&#32032;&#32423;&#25513;&#27169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#32423;WSSS&#25216;&#26415;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#20960;&#20309;&#29305;&#24449;&#30340;&#29702;&#35299;&#65292;&#22240;&#20026;&#32593;&#32476;&#26080;&#27861;&#20174;&#20165;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#20013;&#23548;&#20986;&#20219;&#20309;&#23545;&#35937;&#36793;&#30028;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;BoundaryCAM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#31867;&#28608;&#27963;&#22270;&#32467;&#21512;&#21508;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#29992;&#20110;&#26500;&#24314;&#36793;&#30028;&#22270;&#65292;&#20197;&#20351;BoundaryCAM&#33021;&#22815;&#39640;&#31934;&#24230;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision is a promising approach to deal with the need for Segmentation networks, especially for generating a large number of pixel-wise masks in a given dataset. However, most state-of-the-art image-level WSSS techniques lack an understanding of the geometric features embedded in the images since the network cannot derive any object boundary information from just image-level labels. We define a boundary here as the line separating an object and its background, or two different objects. To address this drawback, we propose our novel BoundaryCAM framework, which deploys state-of-the-art class activation maps combined with various post-processing techniques in order to achieve fine-grained higher-accuracy segmentation masks. To achieve this, we investigate a state-of-the-art unsupervised semantic segmentation network that can be used to construct a boundary map, which enables BoundaryCAM to predict object locations w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#30340;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13221</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#31163;&#25955;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#23454;&#29616;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Data-Centric AI: Deep Generative Differentiable Feature Selection via Discrete Subsetting as Continuous Embedding Space Optimization. (arXiv:2302.13221v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#30340;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#65288;FS&#65289;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#65292;&#20363;&#22914;&#36807;&#28388;&#22120;&#12289;&#21253;&#35013;&#22120;&#21644;&#23884;&#20837;&#24335;&#26041;&#27861;&#12290;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;FS&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21464;&#21270;&#65292;&#24182;&#19988;&#24403;&#25968;&#25454;&#26159;&#39640;&#32500;&#21644;&#23567;&#26679;&#26412;&#26102;&#65292;FS&#23481;&#26131;&#20986;&#29616;&#38382;&#39064;&#12290;&#36873;&#25321;&#30340;&#29305;&#24449;&#23376;&#38598;&#26159;&#21542;&#21487;&#20197;&#26356;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#27867;&#21270;&#20026;&#19968;&#20010;&#28145;&#24230;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65306;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#21644;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#28145;&#24230;&#29305;&#24449;&#23376;&#38598;&#32534;&#30721;&#22120;&#12289;&#20934;&#30830;&#24615;&#35780;&#20272;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#26799;&#24230;&#19978;&#21319;&#20248;&#21270;&#22120;&#12290;&#36825;&#20010;&#26694;&#26550;&#23454;&#29616;&#20102;&#22235;&#20010;&#27493;&#39588;&#65306;1) &#29305;&#24449;-&#20934;&#30830;&#24615;&#35757;&#32451;&#25968;&#25454;&#20934;&#22791;&#65307;2) &#28145;&#24230;&#29305;&#24449;&#23376;&#38598;&#23884;&#20837;&#65307;3) &#26799;&#24230;&#20248;&#21270;&#25628;&#32034;&#65307;4) &#29305;&#24449;&#23376;&#38598;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#27934;&#35265;&#65306;&#23558;&#24378;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#27169;&#22411;&#35270;&#20026;&#25628;&#32034;&#21152;&#36895;&#22120;&#12289;&#22810;&#23610;&#24230;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#36880;&#28176;&#22686;&#24378;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Feature Selection (FS), such as filter, wrapper, and embedded methods, aims to find the optimal feature subset for a given downstream task. However, in many real-world practices, 1) the criteria of FS vary across domains; 2) FS is brittle when data is a high-dimensional and small sample size. Can selected feature subsets be more generalized, accurate, and input dimensionality agnostic? We generalize this problem into a deep differentiable feature selection task and propose a new perspective: discrete feature subsetting as continuous embedding space optimization. We develop a generic and principled framework including a deep feature subset encoder, accuracy evaluator, decoder, and gradient ascent optimizer. This framework implements four steps: 1) features-accuracy training data preparation; 2) deep feature subset embedding; 3) gradient-optimized search; 4) feature subset reconstruction. We develop new technical insights: reinforcement as a training data generator, ensembles of diverse 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#27714;&#12289;&#33539;&#24335;&#21644;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#20445;&#25252;&#38544;&#31169;&#12289;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#23454;&#26102;&#24615;&#33021;&#21644;&#36164;&#28304;&#20248;&#21270;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08571</link><description>&lt;p&gt;
&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#30340;&#32508;&#36848;&#19982;&#20998;&#31867;&#65306;&#38656;&#27714;&#65292;&#33539;&#24335;&#21644;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques. (arXiv:2302.08571v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08571
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#27714;&#12289;&#33539;&#24335;&#21644;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#20445;&#25252;&#38544;&#31169;&#12289;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#23454;&#26102;&#24615;&#33021;&#21644;&#36164;&#28304;&#20248;&#21270;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;(EC)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#32467;&#21512;&#25552;&#20986;&#20102;&#36793;&#32536;AI&#30340;&#27010;&#24565;&#65292;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#23454;&#26102;&#24615;&#33021;&#21644;&#36164;&#28304;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#25509;&#36817;&#26368;&#32456;&#29992;&#25143;&#29615;&#22659;&#30340;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#26426;&#22120;&#23398;&#20064;(ML)&#20316;&#20026;&#36817;&#24180;&#26469;AI&#20013;&#26368;&#20808;&#36827;&#30340;&#20998;&#25903;&#65292;&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#39537;&#21160;&#30340;ML&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#22797;&#26434;&#65292;&#22240;&#20026;&#23427;&#21516;&#26102;&#32771;&#34385;&#21040;&#36793;&#32536;&#35745;&#31639;&#21644;AI&#39046;&#22495;&#30340;&#32422;&#26463;&#65292;&#24182;&#19988;&#26399;&#26395;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#36793;&#32536;ML&#30340;&#38656;&#27714;&#26041;&#38754;&#39640;&#25928;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#22914;&#25968;&#25454;&#22788;&#29702;&#65292;&#27169;&#22411;&#21387;&#32553;&#65292;&#20998;&#24067;&#24335;&#25512;&#29702;&#21644;&#39640;&#32423;&#23398;&#20064;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20102;&#36793;&#32536;ML&#30340;&#20851;&#27880;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;&#32570;&#20047;&#23545;&#29616;&#26377;&#36793;&#32536;ML&#25216;&#26415;&#30340;&#23436;&#25972;&#35843;&#26597;&#65292;&#20197;&#25552;&#20379;&#19968;&#20010;&#20849;&#21516;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The union of Edge Computing (EC) and Artificial Intelligence (AI) has brought forward the Edge AI concept to provide intelligent solutions close to the end-user environment, for privacy preservation, low latency to real-time performance, and resource optimization. Machine Learning (ML), as the most advanced branch of AI in the past few years, has shown encouraging results and applications in the edge environment. Nevertheless, edge-powered ML solutions are more complex to realize due to the joint constraints from both edge computing and AI domains, and the corresponding solutions are expected to be efficient and adapted in technologies such as data processing, model compression, distributed inference, and advanced learning paradigms for Edge ML requirements. Despite the fact that a great deal of the attention garnered by Edge ML is gained in both the academic and industrial communities, we noticed the lack of a complete survey on existing Edge ML technologies to provide a common unders
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#24314;&#35758;&#30340;&#30446;&#26631;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;</title><link>http://arxiv.org/abs/2302.06692</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Guiding Pretraining in Reinforcement Learning with Large Language Models. (arXiv:2302.06692v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06692
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#24314;&#35758;&#30340;&#30446;&#26631;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27809;&#26377;&#23494;&#38598;&#19988;&#24418;&#29366;&#33391;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#35775;&#38382;&#26032;&#39062;&#29366;&#24577;&#25110;&#36716;&#25442;&#30340;&#20869;&#22312;&#21160;&#26426;&#25506;&#32034;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26377;&#38480;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#32972;&#26223;&#30693;&#35782;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;ELLM&#65288;&#20351;&#29992;LLMs&#36827;&#34892;&#25506;&#32034;&#65289;&#65292;&#36890;&#36807;&#32473;&#20195;&#29702;&#22870;&#21169;&#20854;&#36798;&#25104;&#30001;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20195;&#29702;&#24403;&#21069;&#29366;&#24577;&#25551;&#36848;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#65292;&#24341;&#23548;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;&#25105;&#20204;&#22312;Crafter&#28216;&#25103;&#29615;&#22659;&#21644;Housekeep&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;ELLM&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;ELLM&#35757;&#32451;&#30340;&#20195;&#29702;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26377;&#26356;&#22909;&#30340;&#24120;&#35782;&#34892;&#20026;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#36890;&#24120;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually matc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#31639;&#27861;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;&#25919;&#31574;&#29615;&#22659;&#20013;&#30340;&#20116;&#31181;&#24212;&#29992;&#26041;&#24335;&#65306;&#23545;&#35299;&#37322;&#30340;&#20855;&#20307;&#35201;&#27714;&#65307;&#22312;&#31639;&#27861;&#20869;&#37096;&#27835;&#29702;&#30340;&#38750;&#32422;&#26463;&#24615;&#25351;&#21335;&#20013;&#65307;&#36866;&#29992;&#20110;&#39640;&#24230;&#31649;&#21046;&#29615;&#22659;&#30340;&#27861;&#35268;&#65307;&#26088;&#22312;&#25552;&#39640;&#31639;&#27861;&#27861;&#24459;&#36131;&#20219;&#30340;&#23454;&#29992;&#24615;&#30340;&#25351;&#21335;&#65307;&#20197;&#21450;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#36879;&#26126;&#24615;&#30340;&#24191;&#27867;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.03080</link><description>&lt;p&gt;
&#20116;&#31181;&#31639;&#27861;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25919;&#31574;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Five policy uses of algorithmic transparency and explainability. (arXiv:2302.03080v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#31639;&#27861;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;&#25919;&#31574;&#29615;&#22659;&#20013;&#30340;&#20116;&#31181;&#24212;&#29992;&#26041;&#24335;&#65306;&#23545;&#35299;&#37322;&#30340;&#20855;&#20307;&#35201;&#27714;&#65307;&#22312;&#31639;&#27861;&#20869;&#37096;&#27835;&#29702;&#30340;&#38750;&#32422;&#26463;&#24615;&#25351;&#21335;&#20013;&#65307;&#36866;&#29992;&#20110;&#39640;&#24230;&#31649;&#21046;&#29615;&#22659;&#30340;&#27861;&#35268;&#65307;&#26088;&#22312;&#25552;&#39640;&#31639;&#27861;&#27861;&#24459;&#36131;&#20219;&#30340;&#23454;&#29992;&#24615;&#30340;&#25351;&#21335;&#65307;&#20197;&#21450;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#36879;&#26126;&#24615;&#30340;&#24191;&#27867;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#31639;&#27861;&#31995;&#32479;&#24212;&#35813;&#20855;&#26377;&#8220;&#36879;&#26126;&#24615;&#8221;&#21644;&#8220;&#21487;&#35299;&#37322;&#24615;&#8221;&#30340;&#35266;&#24565;&#22312;&#25919;&#24220;&#12289;&#20844;&#21496;&#21644;&#20513;&#23548;&#32452;&#32455;&#21046;&#23450;&#30340;&#35768;&#22810;&#20849;&#35782;&#21407;&#21017;&#20013;&#24456;&#24120;&#35265;&#12290;&#20294;&#25919;&#31574;&#21644;&#27861;&#24459;&#34892;&#20026;&#32773;&#31350;&#31455;&#35201;&#27714;&#36825;&#20123;&#25216;&#26415;&#27010;&#24565;&#30340;&#21738;&#20123;&#26041;&#38754;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#35201;&#27714;&#19982;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30456;&#27604;&#22914;&#20309;&#65311;&#20026;&#20102;&#26356;&#22909;&#22320;&#36830;&#25509;&#25919;&#31574;&#21644;&#25216;&#26415;&#31038;&#21306;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#35828;&#26126;&#31639;&#27861;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;&#25919;&#31574;&#29615;&#22659;&#20013;&#30340;&#20116;&#31181;&#24212;&#29992;&#26041;&#24335;&#65306;&#23545;&#35299;&#37322;&#30340;&#20855;&#20307;&#35201;&#27714;&#65307;&#22312;&#31639;&#27861;&#20869;&#37096;&#27835;&#29702;&#30340;&#38750;&#32422;&#26463;&#24615;&#25351;&#21335;&#20013;&#65307;&#36866;&#29992;&#20110;&#39640;&#24230;&#31649;&#21046;&#29615;&#22659;&#30340;&#27861;&#35268;&#65307;&#26088;&#22312;&#25552;&#39640;&#31639;&#27861;&#27861;&#24459;&#36131;&#20219;&#30340;&#23454;&#29992;&#24615;&#30340;&#25351;&#21335;&#65307;&#20197;&#21450;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#36879;&#26126;&#24615;&#30340;&#24191;&#27867;&#35201;&#27714;&#12290;&#26696;&#20363;&#30740;&#31350;&#28085;&#30422;&#20102;&#20174;&#23545;&#29305;&#23450;&#31867;&#22411;&#35299;&#37322;&#30340;&#31934;&#30830;&#35201;&#27714;&#21040;&#38750;&#20855;&#20307;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The notion that algorithmic systems should be "transparent" and "explainable" is common in the many statements of consensus principles developed by governments, companies, and advocacy organizations. But what exactly do policy and legal actors want from these technical concepts, and how do their desiderata compare with the explainability techniques developed in the machine learning literature? In hopes of better connecting the policy and technical communities, we provide case studies illustrating five ways in which algorithmic transparency and explainability have been used in policy settings: specific requirements for explanations; in nonbinding guidelines for internal governance of algorithms; in regulations applicable to highly regulated settings; in guidelines meant to increase the utility of legal liability for algorithms; and broad requirements for model and data transparency. The case studies span a spectrum from precise requirements for specific types of explanations to nonspeci
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02173</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02173
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#39057;&#29575;&#21464;&#25442;&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#24615;&#21644;&#20840;&#23616;&#35270;&#35282;&#65292;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#34987;&#36805;&#36895;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23637;&#31034;&#20102;&#39057;&#29575;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31995;&#32479;&#22238;&#39038;&#21644;&#28145;&#20837;&#20998;&#26512;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#39057;&#29575;&#21464;&#25442;&#21487;&#20197;&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#23427;&#22312;&#35813;&#39046;&#22495;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20027;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;3D-MIM&#65292;&#29992;&#20110;&#39044;&#27979;&#36229;&#26032;&#26143;&#29190;&#28856;&#21518;&#30340;&#22771;&#23618;&#25193;&#24352;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#26816;&#27979;&#24182;&#39044;&#27979;&#36229;&#26032;&#26143;&#24433;&#21709;&#31890;&#23376;&#25152;&#22312;&#30340;&#22771;&#23618;&#24418;&#29366;&#65292;&#35299;&#20915;&#20102;&#39640;&#20998;&#36776;&#29575;&#26143;&#31995;&#27169;&#25311;&#20013;&#36229;&#26032;&#26143;&#31215;&#20998;&#26102;&#38388;&#27493;&#38271;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00026</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#36229;&#26032;&#26143;&#22771;&#23618;&#25193;&#24352;&#30340;3D&#26102;&#31354;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#26143;&#31995;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
3D-Spatiotemporal Forecasting the Expansion of Supernova Shells Using Deep Learning toward High-Resolution Galaxy Simulations. (arXiv:2302.00026v2 [astro-ph.GA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;3D-MIM&#65292;&#29992;&#20110;&#39044;&#27979;&#36229;&#26032;&#26143;&#29190;&#28856;&#21518;&#30340;&#22771;&#23618;&#25193;&#24352;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#26816;&#27979;&#24182;&#39044;&#27979;&#36229;&#26032;&#26143;&#24433;&#21709;&#31890;&#23376;&#25152;&#22312;&#30340;&#22771;&#23618;&#24418;&#29366;&#65292;&#35299;&#20915;&#20102;&#39640;&#20998;&#36776;&#29575;&#26143;&#31995;&#27169;&#25311;&#20013;&#36229;&#26032;&#26143;&#31215;&#20998;&#26102;&#38388;&#27493;&#38271;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#26032;&#26143;&#22312;&#26143;&#31995;&#24418;&#25104;&#21644;&#28436;&#21270;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#26143;&#31995;&#27169;&#25311;&#26102;&#65292;&#36229;&#26032;&#26143;&#30340;&#30701;&#31215;&#20998;&#26102;&#38388;&#27493;&#38271;&#25104;&#20026;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;Hamiltonian&#20998;&#35010;&#26041;&#27861;&#65292;&#21363;&#23558;&#38656;&#35201;&#30701;&#31215;&#20998;&#26102;&#38388;&#27493;&#38271;&#30340;&#21306;&#22495;&#19982;&#25972;&#20010;&#31995;&#32479;&#20998;&#24320;&#31215;&#20998;&#12290;&#20026;&#20102;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#21463;&#36229;&#26032;&#26143;&#24433;&#21709;&#30340;&#31890;&#23376;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#38543;&#21518;&#30340;&#20840;&#23616;&#27493;&#39588;&#20013;&#25552;&#21069;&#26816;&#27979;&#21040;&#36825;&#20123;&#31890;&#23376;&#25152;&#22312;&#30340;&#22771;&#23618;&#30340;&#24418;&#29366;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;3D-MIM&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#36229;&#26032;&#26143;&#29190;&#28856;&#21518;&#30340;&#22771;&#23618;&#25193;&#24352;&#12290;&#36890;&#36807;&#23545;&#24102;&#26377;&#31890;&#23376;&#36136;&#37327;$m_{\rm gas}$ = 1 M$_\odot$&#30340;&#28237;&#27969;&#20113;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#20877;&#29616;&#20986;&#21508;&#21521;&#24322;&#24615;&#30340;&#22771;&#23618;&#24418;&#29366;&#65292;&#20854;&#20013;&#23494;&#24230;&#36880;&#28176;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supernova (SN) plays an important role in galaxy formation and evolution. In high-resolution galaxy simulations using massively parallel computing, short integration timesteps for SNe are serious bottlenecks. This is an urgent issue that needs to be resolved for future higher-resolution galaxy simulations. One possible solution would be to use the Hamiltonian splitting method, in which regions requiring short timesteps are integrated separately from the entire system. To apply this method to the particles affected by SNe in a smoothed-particle hydrodynamics simulation, we need to detect the shape of the shell on and within which such SN-affected particles reside during the subsequent global step in advance. In this paper, we develop a deep learning model, 3D-MIM, to predict a shell expansion after a SN explosion. Trained on turbulent cloud simulations with particle mass $m_{\rm gas}$~=~1 M$_\odot$, the model accurately reproduces the anisotropic shell shape, where densities decrease by
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#23558;&#22240;&#26524;&#24615;&#21407;&#21017;&#34701;&#20837;DGM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;</title><link>http://arxiv.org/abs/2301.12351</link><description>&lt;p&gt;
&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emerging Synergies in Causality and Deep Generative Models: A Survey. (arXiv:2301.12351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#23558;&#22240;&#26524;&#24615;&#21407;&#21017;&#34701;&#20837;DGM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20102;&#35299;&#21644;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65288;DGP&#65289;&#30340;&#36861;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#32780;&#22240;&#26524;&#24615;&#21017;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#39537;&#21160;&#25968;&#25454;&#29983;&#25104;&#30340;&#26426;&#21046;&#65292;&#24182;&#31361;&#26174;&#20102;&#36825;&#20123;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#22240;&#26524;&#25928;&#24212;&#21160;&#21147;&#23398;&#12290;&#34429;&#28982;&#22240;&#26524;&#24615;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21364;&#38754;&#20020;&#30528;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#24847;&#35782;&#21040;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;DGM&#30340;&#20132;&#27719;&#28857;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#22240;&#26524;&#24615;&#21407;&#21017;&#22312;DGM&#20013;&#30340;&#25972;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DGM&#36827;&#34892;&#22240;&#26524;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#22240;&#26524;&#24615;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#30740;&#31350;&#21069;&#27839;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#29983;&#25104;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26041;&#27861;&#35770;&#65292;&#31361;&#20986;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of artificial intelligence (AI), the quest to understand and model data-generating processes (DGPs) is of paramount importance. Deep generative models (DGMs) have proven adept in capturing complex data distributions but often fall short in generalization and interpretability. On the other hand, causality offers a structured lens to comprehend the mechanisms driving data generation and highlights the causal-effect dynamics inherent in these processes. While causality excels in interpretability and the ability to extrapolate, it grapples with intricacies of high-dimensional spaces. Recognizing the synergistic potential, we delve into the confluence of causality and DGMs. We elucidate the integration of causal principles within DGMs, investigate causal identification using DGMs, and navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs). We offer insights into methodologies, highlight open cha
&lt;/p&gt;</description></item><item><title>SPEC5G&#26159;&#39318;&#20010;&#20844;&#20849;5G&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;5G&#34562;&#31389;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#20998;&#26512;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2301.09201</link><description>&lt;p&gt;
SPEC5G&#65306;&#29992;&#20110;5G&#34562;&#31389;&#32593;&#32476;&#21327;&#35758;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis. (arXiv:2301.09201v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09201
&lt;/p&gt;
&lt;p&gt;
SPEC5G&#26159;&#39318;&#20010;&#20844;&#20849;5G&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;5G&#34562;&#31389;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#20998;&#26512;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#26159;&#31532;&#20116;&#20195;&#34562;&#31389;&#32593;&#32476;&#21327;&#35758;&#65292;&#26159;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#26080;&#32447;&#26631;&#20934;&#65292;&#33021;&#22815;&#20197;&#25552;&#39640;&#36895;&#24230;&#21644;&#38477;&#20302;&#24310;&#36831;&#30340;&#26041;&#24335;&#36830;&#25509;&#20960;&#20046;&#25152;&#26377;&#20154;&#21644;&#29289;&#12290;&#22240;&#27492;&#65292;&#20854;&#21457;&#23637;&#12289;&#20998;&#26512;&#21644;&#23433;&#20840;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;5G&#21327;&#35758;&#30340;&#24320;&#21457;&#21644;&#23433;&#20840;&#20998;&#26512;&#26041;&#27861;&#37117;&#26159;&#23436;&#20840;&#25163;&#21160;&#30340;&#65292;&#27604;&#22914;&#23646;&#24615;&#25552;&#21462;&#12289;&#21327;&#35758;&#25688;&#35201;&#21644;&#21327;&#35758;&#35268;&#33539;&#21644;&#23454;&#29616;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#25163;&#21160;&#24037;&#20316;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SPEC5G&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#20844;&#20849;5G&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;13094&#20221;&#34562;&#31389;&#32593;&#32476;&#35268;&#33539;&#21644;13&#20010;&#32593;&#31449;&#30340;3,547,586&#20010;&#21477;&#23376;&#65292;&#24635;&#35745;134M&#20010;&#21333;&#35789;&#12290;&#36890;&#36807;&#21033;&#29992;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#20998;&#31867;&#21644;&#25688;&#35201;&#12290;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#20998;&#31867;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
5G is the 5th generation cellular network protocol. It is the state-of-the-art global wireless standard that enables an advanced kind of network designed to connect virtually everyone and everything with increased speed and reduced latency. Therefore, its development, analysis, and security are critical. However, all approaches to the 5G protocol development and security analysis, e.g., property extraction, protocol summarization, and semantic analysis of the protocol specifications and implementations are completely manual. To reduce such manual effort, in this paper, we curate SPEC5G the first-ever public 5G dataset for NLP research. The dataset contains 3,547,586 sentences with 134M words, from 13094 cellular network specifications and 13 online websites. By leveraging large-scale pre-trained language models that have achieved state-of-the-art results on NLP tasks, we use this dataset for security-related text classification and summarization. Security-related text classification ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#26657;&#27491;CMIP6&#32423;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#23616;&#37096;&#39057;&#29575;&#20998;&#24067;&#21644;&#31354;&#38388;&#27169;&#24335;&#20013;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2301.01253</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#32416;&#27491;CMIP6&#32423;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Deep learning for bias-correcting CMIP6-class Earth system models. (arXiv:2301.01253v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#26657;&#27491;CMIP6&#32423;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#23616;&#37096;&#39057;&#29575;&#20998;&#24067;&#21644;&#31354;&#38388;&#27169;&#24335;&#20013;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#27169;&#25311;&#38477;&#27700;&#23545;&#20110;&#21487;&#38752;&#39044;&#27979;&#20154;&#20026;&#20840;&#29699;&#21464;&#26262;&#23545;&#29983;&#24577;&#21644;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#38477;&#27700;&#30340;&#22797;&#26434;&#36328;&#23610;&#24230;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#24456;&#38590;&#27169;&#25311;&#65292;&#23548;&#33268;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65288;ESMs&#65289;&#30340;&#23383;&#27573;&#21487;&#33021;&#23384;&#22312;&#24378;&#28872;&#30340;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#20165;&#22312;&#27599;&#20010;&#21333;&#29420;&#30340;&#26684;&#28857;&#23616;&#37096;&#22320;&#32416;&#27491;&#27169;&#25311;&#39057;&#29575;&#20998;&#24067;&#30340;&#35823;&#24046;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25913;&#21892;ESM&#36755;&#20986;&#30340;&#19981;&#30495;&#23454;&#31354;&#38388;&#27169;&#24335;&#65292;&#21363;&#38656;&#35201;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#19968;&#30452;&#26080;&#27861;&#35299;&#20915;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGANs&#65289;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#26657;&#27491;&#26368;&#20808;&#36827;&#30340;CMIP6&#32423;ESM&#30340;&#23616;&#37096;&#39057;&#29575;&#20998;&#24067;&#21644;&#31354;&#38388;&#27169;&#24335;&#20013;&#30340;&#20559;&#24046;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25913;&#21892;&#23616;&#37096;&#39057;&#29575;&#20998;&#24067;&#26041;&#38754;&#19982;&#37329;&#26631;&#20934;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#19968;&#26679;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate representation of precipitation in Earth system models (ESMs) is crucial for reliable projections of the ecological and socioeconomic impacts in response to anthropogenic global warming. The complex cross-scale interactions of processes that produce precipitation are challenging to model, however, inducing potentially strong biases in ESM fields, especially regarding extremes. State-of-the-art bias correction methods only address errors in the simulated frequency distributions locally at every individual grid cell. Improving unrealistic spatial patterns of the ESM output, which would require spatial context, has not been possible so far. Here, we show that a post-processing method based on physically constrained generative adversarial networks (cGANs) can correct biases of a state-of-the-art, CMIP6-class ESM both in local frequency distributions and in the spatial patterns at once. While our method improves local frequency distributions equally well as gold-standard bias-a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Temporal Saliency Detection (TSD)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#21387;&#32553;&#22810;&#22836;&#27880;&#24847;&#21147;&#36827;&#34892;&#26174;&#33879;&#24615;&#27169;&#24335;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2212.07771</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#38388;&#26174;&#33879;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Temporal Saliency Detection Towards Explainable Transformer-based Timeseries Forecasting. (arXiv:2212.07771v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07771
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Temporal Saliency Detection (TSD)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#21387;&#32553;&#22810;&#22836;&#27880;&#24847;&#21147;&#36827;&#34892;&#26174;&#33879;&#24615;&#27169;&#24335;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35768;&#22810;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#38271;&#26399;&#22810;&#27493;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#24120;&#29992;&#30340;&#26174;&#33879;&#24615;&#22270;&#35299;&#37322;DNN&#30340;&#24605;&#24819;&#65292;&#26500;&#24314;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#22836;&#24314;&#31435;&#36830;&#25509;&#65292;&#33258;&#21160;&#32534;&#30721;&#19982;&#26174;&#33879;&#24615;&#30456;&#20851;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Temporal Saliency Detection (TSD) &#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#34429;&#28982;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#36981;&#24490;&#24120;&#35268;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#20294;&#22312;&#32534;&#30721;&#22120;&#32452;&#20214;&#20013;&#32463;&#21382;&#20102;&#37325;&#22823;&#30340;&#25913;&#36827;&#65292;&#20854;&#20013;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;U-Net&#39118;&#26684;&#26550;&#26500;&#21551;&#21457;&#30340;&#19968;&#31995;&#21015;&#20449;&#24687;&#25910;&#32553;&#21644;&#25193;&#23637;&#27169;&#22359;&#12290;TSD&#26041;&#27861;&#36890;&#36807;&#21387;&#32553;&#22810;&#22836;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#27169;&#24335;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the notable advancements in numerous Transformer-based models, the task of long multi-horizon time series forecasting remains a persistent challenge, especially towards explainability. Focusing on commonly used saliency maps in explaining DNN in general, our quest is to build attention-based architecture that can automatically encode saliency-related temporal patterns by establishing connections with appropriate attention heads. Hence, this paper introduces Temporal Saliency Detection (TSD), an effective approach that builds upon the attention mechanism and applies it to multi-horizon time series prediction. While our proposed architecture adheres to the general encoder-decoder structure, it undergoes a significant renovation in the encoder component, wherein we incorporate a series of information contracting and expanding blocks inspired by the U-Net style architecture. The TSD approach facilitates the multiresolution analysis of saliency patterns by condensing multi-heads, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; TIDE&#65292;&#36890;&#36807;&#26102;&#38388;&#23548;&#25968;&#22270;&#25193;&#25955;&#20811;&#26381;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#32467;&#26500;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#22320;&#20013;&#38271;&#36317;&#31163;&#36890;&#20449;&#65292;&#24182;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102; state-of-the-art &#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.02483</link><description>&lt;p&gt;
TIDE&#65306;&#29992;&#20110;&#22270;&#19978;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#23548;&#25968;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
TIDE: Time Derivative Diffusion for Deep Learning on Graphs. (arXiv:2212.02483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; TIDE&#65292;&#36890;&#36807;&#26102;&#38388;&#23548;&#25968;&#22270;&#25193;&#25955;&#20811;&#26381;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#32467;&#26500;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#22320;&#20013;&#38271;&#36317;&#31163;&#36890;&#20449;&#65292;&#24182;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102; state-of-the-art &#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#37325;&#35201;&#33539;&#24335;&#26159;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20449;&#24687;&#36890;&#20449;&#20165;&#22312;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#23454;&#29616;&#12290;&#20351;&#29992;&#36825;&#31181;&#33539;&#24335;&#30340;&#26041;&#27861;&#30340;&#25361;&#25112;&#26159;&#30830;&#20445;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#38271;&#36317;&#31163;&#36890;&#20449;&#65292;&#22240;&#20026;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#23481;&#26131;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23548;&#25968;&#22270;&#25193;&#25955;&#65288;TIDE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#36825;&#20123;&#32467;&#26500;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20248;&#21270;&#25193;&#25955;&#30340;&#31354;&#38388;&#33539;&#22260;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#32593;&#32476;&#36890;&#36947;&#65292;&#20174;&#32780;&#23454;&#29616;&#20013;&#38271;&#36317;&#31163;&#36890;&#20449;&#30340;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#35774;&#35745;&#20063;&#20351;&#26412;&#22320;&#28040;&#24687;&#20256;&#36882;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#32487;&#25215;&#20102;&#26412;&#22320;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#22522;&#20934;&#21644;&#21512;&#25104;&#32593;&#26684;&#21644;&#22270;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20248;&#20110;	state-of-the-art &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prominent paradigm for graph neural networks is based on the message-passing framework. In this framework, information communication is realized only between neighboring nodes. The challenge of approaches that use this paradigm is to ensure efficient and accurate long-distance communication between nodes, as deep convolutional networks are prone to oversmoothing. In this paper, we present a novel method based on time derivative graph diffusion (TIDE) to overcome these structural limitations of the message-passing framework. Our approach allows for optimizing the spatial extent of diffusion across various tasks and network channels, thus enabling medium and long-distance communication efficiently. Furthermore, we show that our architecture design also enables local message-passing and thus inherits from the capabilities of local message-passing approaches. We show that on both widely used graph benchmarks and synthetic mesh and graph datasets, the proposed framework outperforms state-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35266;&#27979;&#30740;&#31350;&#20013;&#24212;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#28102;&#22240;&#32032;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#26465;&#20214;&#20998;&#24067;&#65292;&#23398;&#20064;&#20102;&#21508;&#21333;&#20301;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22343;&#26041;&#35823;&#24046;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2211.08209</link><description>&lt;p&gt;
&#20851;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
On counterfactual inference with unobserved confounding. (arXiv:2211.08209v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35266;&#27979;&#30740;&#31350;&#20013;&#24212;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#28102;&#22240;&#32032;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#26465;&#20214;&#20998;&#24067;&#65292;&#23398;&#20064;&#20102;&#21508;&#21333;&#20301;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22343;&#26041;&#35823;&#24046;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35266;&#27979;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#27599;&#20010;&#21333;&#20301;&#21482;&#26377;&#19968;&#20010;&#21253;&#21547;&#21327;&#21464;&#37327;&#12289;&#24178;&#39044;&#21644;&#32467;&#26524;&#30340;$p$&#32500;&#26679;&#26412;&#26469;&#23398;&#20064;&#27599;&#20010;&#21333;&#20301;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#36825;&#20123;&#21333;&#20301;&#26159;&#29420;&#31435;&#20294;&#24322;&#36136;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#23427;&#24341;&#20837;&#20102;&#24178;&#39044;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#32479;&#35745;&#20559;&#24046;&#65292;&#24182;&#21152;&#21095;&#20102;&#21333;&#20301;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;&#23558;&#32467;&#26524;&#30340;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#20026;&#25351;&#25968;&#26063;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#21333;&#20301;&#32423;&#21453;&#20107;&#23454;&#20998;&#24067;&#31616;&#21270;&#20026;&#23398;&#20064;&#20855;&#26377;&#24322;&#36136;&#21442;&#25968;&#21644;&#20165;&#26377;&#19968;&#20010;&#26679;&#26412;&#30340;$n$&#20010;&#25351;&#25968;&#26063;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20984;&#20248;&#21270;&#30446;&#26631;&#65292;&#23558;&#25152;&#26377;$n$&#20010;&#26679;&#26412;&#27719;&#38598;&#36215;&#26469;&#20849;&#21516;&#23398;&#20064;&#25152;&#26377;$n$&#20010;&#21442;&#25968;&#21521;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21333;&#20301;&#32423;&#22343;&#26041;&#35823;&#24046;&#30340;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#19982;&#21442;&#25968;&#31354;&#38388;&#30340;&#24230;&#37327;&#29109;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#20363;&#22914;&#65292;&#24403;&#21442;&#25968;&#26159;$k$&#20010;&#24050;&#30693;&#21521;&#37327;&#30340;$s$&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26102;&#65292;&#35823;&#24046;&#20026;$O(k\sqrt{\frac{s\log p}{n}})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an observational study with $n$ independent but heterogeneous units, our goal is to learn the counterfactual distribution for each unit using only one $p$-dimensional sample per unit containing covariates, interventions, and outcomes. Specifically, we allow for unobserved confounding that introduces statistical biases between interventions and outcomes as well as exacerbates the heterogeneity across units. Modeling the conditional distribution of the outcomes as an exponential family, we reduce learning the unit-level counterfactual distributions to learning $n$ exponential family distributions with heterogeneous parameters and only one sample per distribution. We introduce a convex objective that pools all $n$ samples to jointly learn all $n$ parameter vectors, and provide a unit-wise mean squared error bound that scales linearly with the metric entropy of the parameter space. For example, when the parameters are $s$-sparse linear combination of $k$ known vectors, the error is $
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22810;&#24863;&#23448;&#38598;&#25104;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#25509;&#21463;&#36866;&#24403;&#30456;&#20851;&#20449;&#21495;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24178;&#25200;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#20250;&#27704;&#20037;&#25439;&#23475;&#25216;&#33021;&#30340;&#21457;&#23637;&#12290;&#26089;&#26399;&#30636;&#24577;&#21160;&#21147;&#23398;&#23545;&#26368;&#32456;&#30340;&#31995;&#32479;&#24615;&#33021;&#21644;&#23398;&#20064;&#34920;&#31034;&#20855;&#26377;&#20915;&#23450;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.04643</link><description>&lt;p&gt;
&#22810;&#24863;&#23448;&#38598;&#25104;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#23398;&#20064;&#26399;
&lt;/p&gt;
&lt;p&gt;
Critical Learning Periods for Multisensory Integration in Deep Networks. (arXiv:2210.04643v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04643
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#24863;&#23448;&#38598;&#25104;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#25509;&#21463;&#36866;&#24403;&#30456;&#20851;&#20449;&#21495;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24178;&#25200;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#20250;&#27704;&#20037;&#25439;&#23475;&#25216;&#33021;&#30340;&#21457;&#23637;&#12290;&#26089;&#26399;&#30636;&#24577;&#21160;&#21147;&#23398;&#23545;&#26368;&#32456;&#30340;&#31995;&#32479;&#24615;&#33021;&#21644;&#23398;&#20064;&#34920;&#31034;&#20855;&#26377;&#20915;&#23450;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#20449;&#24687;&#30340;&#33021;&#21147;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#25509;&#21463;&#36866;&#24403;&#30456;&#20851;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#24178;&#25200;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#20250;&#27704;&#20037;&#25439;&#23475;&#25216;&#33021;&#30340;&#21457;&#23637;&#65292;&#26080;&#35770;&#26159;&#22312;&#20154;&#36896;&#31995;&#32479;&#36824;&#26159;&#29983;&#29289;&#31995;&#32479;&#20013;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20851;&#38190;&#23398;&#20064;&#26399;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#23398;&#20064;&#26399;&#28304;&#20110;&#22797;&#26434;&#32780;&#19981;&#31283;&#23450;&#30340;&#26089;&#26399;&#30636;&#24577;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#35757;&#32451;&#31995;&#32479;&#30340;&#26368;&#32456;&#24615;&#33021;&#21644;&#23398;&#20064;&#34920;&#31034;&#20855;&#26377;&#20915;&#23450;&#24615;&#24433;&#21709;&#12290;&#36825;&#19968;&#35777;&#25454;&#25361;&#25112;&#20102;&#36890;&#36807;&#20998;&#26512;&#23485;&#32780;&#27973;&#30340;&#32593;&#32476;&#24471;&#20986;&#30340;&#35748;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26089;&#26399;&#23398;&#20064;&#21160;&#24577;&#26159;&#31616;&#21333;&#30340;&#12289;&#31867;&#20284;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#35266;&#28857;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#22312;&#22810;&#28304;&#38598;&#25104;&#26041;&#38754;&#20063;&#20250;&#20986;&#29616;&#20851;&#38190;&#23398;&#20064;&#26399;&#65292;&#32780;&#27973;&#23618;&#32593;&#32476;&#21017;&#19981;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the ability of a neural network to integrate information from diverse sources hinges critically on being exposed to properly correlated signals during the early phases of training. Interfering with the learning process during this initial stage can permanently impair the development of a skill, both in artificial and biological systems where the phenomenon is known as a critical learning period. We show that critical periods arise from the complex and unstable early transient dynamics, which are decisive of final performance of the trained system and their learned representations. This evidence challenges the view, engendered by analysis of wide and shallow networks, that early learning dynamics of neural networks are simple, akin to those of a linear model. Indeed, we show that even deep linear networks exhibit critical learning periods for multi-source integration, while shallow networks do not. To better understand how the internal representations change according to di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#26410;&#30693;&#26680;&#22823;&#23567;&#21644;&#22823;&#37327;&#22122;&#22768;&#19979;&#30340;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#21333;&#23454;&#20363;&#26041;&#27861;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#19981;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2208.09483</link><description>&lt;p&gt;
&#26410;&#30693;&#26680;&#22823;&#23567;&#21644;&#22823;&#37327;&#22122;&#22768;&#19979;&#30340;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;
&lt;/p&gt;
&lt;p&gt;
Blind Image Deblurring with Unknown Kernel Size and Substantial Noise. (arXiv:2208.09483v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#26410;&#30693;&#26680;&#22823;&#23567;&#21644;&#22823;&#37327;&#22122;&#22768;&#19979;&#30340;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#21333;&#23454;&#20363;&#26041;&#27861;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#65288;BID&#65289;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#29616;&#20195;BID&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#21333;&#23454;&#20363;&#26041;&#27861;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#21333;&#23454;&#20363;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#26410;&#35299;&#20915;&#22914;&#20309;&#22788;&#29702;&#26410;&#30693;&#26680;&#22823;&#23567;&#21644;&#22823;&#37327;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#21333;&#23454;&#20363;&#26041;&#27861;&#22312;&#26680;&#22823;&#23567;&#36807;&#24230;&#35268;&#23450;&#21644;/&#25110;&#22122;&#22768;&#36807;&#22810;&#26102;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blind image deblurring (BID) has been extensively studied in computer vision and adjacent fields. Modern methods for BID can be grouped into two categories: single-instance methods that deal with individual instances using statistical inference and numerical optimization, and data-driven methods that train deep-learning models to deblur future instances directly. Data-driven methods can be free from the difficulty in deriving accurate blur models, but are fundamentally limited by the diversity and quality of the training data -collecting sufficiently expressive and realistic training data is a standing challenge. In this paper, we focus on single-instance methods that remain competitive and indispensable. However, most such methods do not prescribe how to deal with unknown kernel size and substantial noise, precluding practical deployment. Indeed, we show that several state-of-the-art (SOTA) single-instance methods are unstable when the kernel size is overspecified, and/or the noise 
&lt;/p&gt;</description></item><item><title>DPA-1&#26159;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#21183;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#34920;&#31034;&#21407;&#23376;&#31995;&#32479;&#30340;&#26500;&#35937;&#21644;&#21270;&#23398;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#20998;&#23376;&#27169;&#25311;&#20013;&#33021;&#22815;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21462;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.08236</link><description>&lt;p&gt;
DPA-1: &#36816;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#21183;&#33021;&#27169;&#22411;&#22312;&#20998;&#23376;&#27169;&#25311;&#20013;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DPA-1: Pretraining of Attention-based Deep Potential Model for Molecular Simulation. (arXiv:2208.08236v4 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08236
&lt;/p&gt;
&lt;p&gt;
DPA-1&#26159;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#21183;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#34920;&#31034;&#21407;&#23376;&#31995;&#32479;&#30340;&#26500;&#35937;&#21644;&#21270;&#23398;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#20998;&#23376;&#27169;&#25311;&#20013;&#33021;&#22815;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21462;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#24314;&#27169;&#30340;&#21407;&#23376;&#38388;&#21183;&#33021;&#33021;&#37327;&#38754;&#65288;PES&#65289;&#27491;&#24443;&#24213;&#25913;&#21464;&#20998;&#23376;&#27169;&#25311;&#39046;&#22495;&#12290;&#38543;&#30528;&#39640;&#36136;&#37327;&#30005;&#23376;&#32467;&#26500;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#19968;&#20010;&#33021;&#22815;&#39044;&#20808;&#35757;&#32451;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36890;&#36807;&#23569;&#37327;&#39069;&#22806;&#24037;&#20316;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#23558;&#20351;&#35813;&#39046;&#22495;&#36827;&#20837;&#19968;&#20010;&#26032;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DPA-1&#65292;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#21183;&#33021;&#27169;&#22411;&#65292;&#23545;&#21407;&#23376;&#31995;&#32479;&#30340;&#26500;&#35937;&#21644;&#21270;&#23398;&#31354;&#38388;&#20855;&#26377;&#39640;&#25928;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21040;PES&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#31995;&#32479;&#19978;&#27979;&#35797;&#20102;DPA-1&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#24403;&#22312;&#21253;&#21547;56&#20010;&#20803;&#32032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;DPA-1&#21487;&#20197;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#26497;&#22823;&#30340;&#26679;&#26412;&#25928;&#29575;&#25913;&#36827;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#20803;&#32032;&#65292;&#23398;&#21040;&#30340;&#31867;&#22411;&#23884;&#20837;&#21442;&#25968;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24418;&#25104;&#20102;&#19968;&#20010;"&#34746;&#26059;"&#24418;&#29366;&#65292;&#24182;&#19988;&#19982;&#23427;&#20204;&#30340;p
&lt;/p&gt;
&lt;p&gt;
Machine learning assisted modeling of the inter-atomic potential energy surface (PES) is revolutionizing the field of molecular simulation. With the accumulation of high-quality electronic structure data, a model that can be pretrained on all available data and finetuned on downstream tasks with a small additional effort would bring the field to a new stage. Here we propose DPA-1, a Deep Potential model with a novel attention mechanism, which is highly effective for representing the conformation and chemical spaces of atomic systems and learning the PES. We tested DPA-1 on a number of systems and observed superior performance compared with existing benchmarks. When pretrained on large-scale datasets containing 56 elements, DPA-1 can be successfully applied to various downstream tasks with a great improvement of sample efficiency. Surprisingly, for different elements, the learned type embedding parameters form a $spiral$ in the latent space and have a natural correspondence with their p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;&#26494;&#24347;&#38543;&#26426;&#25511;&#21046;&#35270;&#35282;&#35774;&#35745;&#20102;&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#24615;&#25511;&#21046;&#26041;&#27861;&#21644;&#36817;&#31471;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#23454;&#29616;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#20197;&#35299;&#20915;&#26377;&#38480;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.04466</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#29109;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimal scheduling of entropy regulariser for continuous-time linear-quadratic reinforcement learning. (arXiv:2208.04466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;&#26494;&#24347;&#38543;&#26426;&#25511;&#21046;&#35270;&#35282;&#35774;&#35745;&#20102;&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#24615;&#25511;&#21046;&#26041;&#27861;&#21644;&#36817;&#31471;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#23454;&#29616;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#20197;&#35299;&#20915;&#26377;&#38480;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;&#26494;&#24347;&#38543;&#26426;&#25511;&#21046;&#35270;&#35282;&#20316;&#20026;&#35774;&#35745;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#26694;&#26550;&#12290;&#22312;&#36825;&#37324;&#65292;Agent&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#26368;&#20248;&#26494;&#24347;&#31574;&#30053;&#30340;&#22122;&#22768;&#25511;&#21046;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#22122;&#22768;&#31574;&#30053;&#19968;&#26041;&#38754;&#21487;&#20197;&#25506;&#32034;&#31354;&#38388;&#24182;&#20419;&#36827;&#23398;&#20064;&#65292;&#20294;&#21478;&#19968;&#26041;&#38754;&#20063;&#20250;&#24341;&#20837;&#20559;&#24046;&#65292;&#23558;&#27491;&#27010;&#29575;&#20998;&#37197;&#32473;&#38750;&#26368;&#20248;&#21160;&#20316;&#12290;&#36825;&#31181;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#26435;&#34913;&#30001;&#29109;&#27491;&#21017;&#21270;&#30340;&#24378;&#24230;&#26469;&#30830;&#23450;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#29109;&#27491;&#21017;&#21270;&#24418;&#24335;&#24471;&#21040;&#30340;&#31639;&#27861;&#65306;&#25506;&#32034;&#24615;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312;&#25104;&#26412;&#30446;&#26631;&#20013;&#28155;&#21152;&#29109;&#65307;&#36817;&#31471;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#36830;&#32493;&#30340;Episode&#20043;&#38388;&#23545;&#31574;&#30053;&#24046;&#24322;&#36827;&#34892;&#29109;&#24809;&#32602;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20855;&#26377;&#26410;&#30693;&#28418;&#31227;&#31995;&#25968;&#30340;&#32447;&#24615;&#21160;&#21147;&#23398;&#21463;&#21040;&#22235;&#27425;&#26041;&#32422;&#26463;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work uses the entropy-regularised relaxed stochastic control perspective as a principled framework for designing reinforcement learning (RL) algorithms. Herein agent interacts with the environment by generating noisy controls distributed according to the optimal relaxed policy. The noisy policies on the one hand, explore the space and hence facilitate learning but, on the other hand, introduce bias by assigning a positive probability to non-optimal actions. This exploration-exploitation trade-off is determined by the strength of entropy regularisation. We study algorithms resulting from two entropy regularisation formulations: the exploratory control approach, where entropy is added to the cost objective, and the proximal policy update approach, where entropy penalises policy divergence between consecutive episodes. We focus on the finite horizon continuous-time linear-quadratic (LQ) RL problem, where a linear dynamics with unknown drift coefficients is controlled subject to quadr
&lt;/p&gt;</description></item><item><title>Tac2Pose&#26159;&#19968;&#31181;&#20174;&#31532;&#19968;&#27425;&#35302;&#35273;&#20013;&#20272;&#35745;&#29289;&#20307;&#23039;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#29289;&#20307;&#30340;&#24863;&#30693;&#27169;&#22411;&#65292;&#26681;&#25454;&#35302;&#35273;&#35266;&#27979;&#20272;&#35745;&#21487;&#33021;&#30340;&#29289;&#20307;&#23039;&#24577;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#19968;&#27425;&#30495;&#23454;&#35302;&#35273;&#35266;&#27979;&#21363;&#21487;&#23450;&#20301;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2204.11701</link><description>&lt;p&gt;
Tac2Pose&#65306;&#20174;&#31532;&#19968;&#27425;&#25509;&#35302;&#20013;&#30340;&#35302;&#35273;&#23545;&#35937;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Tac2Pose: Tactile Object Pose Estimation from the First Touch. (arXiv:2204.11701v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11701
&lt;/p&gt;
&lt;p&gt;
Tac2Pose&#26159;&#19968;&#31181;&#20174;&#31532;&#19968;&#27425;&#35302;&#35273;&#20013;&#20272;&#35745;&#29289;&#20307;&#23039;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#29289;&#20307;&#30340;&#24863;&#30693;&#27169;&#22411;&#65292;&#26681;&#25454;&#35302;&#35273;&#35266;&#27979;&#20272;&#35745;&#21487;&#33021;&#30340;&#29289;&#20307;&#23039;&#24577;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#19968;&#27425;&#30495;&#23454;&#35302;&#35273;&#35266;&#27979;&#21363;&#21487;&#23450;&#20301;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Tac2Pose&#65292;&#19968;&#31181;&#38024;&#23545;&#24050;&#30693;&#23545;&#35937;&#30340;&#20174;&#31532;&#19968;&#27425;&#25509;&#35302;&#20013;&#20272;&#35745;&#35302;&#35273;&#23039;&#24577;&#30340;&#23545;&#35937;&#29305;&#23450;&#26041;&#27861;&#12290;&#32473;&#23450;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#65292;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#20102;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#24863;&#30693;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#35302;&#35273;&#35266;&#27979;&#26469;&#20272;&#35745;&#21487;&#33021;&#30340;&#29289;&#20307;&#23039;&#24577;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#32452;&#23494;&#38598;&#30340;&#29289;&#20307;&#23039;&#24577;&#22312;&#20256;&#24863;&#22120;&#19978;&#20135;&#29983;&#30340;&#25509;&#35302;&#24418;&#29366;&#12290;&#28982;&#21518;&#65292;&#32473;&#23450;&#20174;&#20256;&#24863;&#22120;&#20013;&#33719;&#24471;&#30340;&#26032;&#25509;&#35302;&#24418;&#29366;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#30340;&#23545;&#35937;&#29305;&#23450;&#23884;&#20837;&#23558;&#20854;&#19982;&#39044;&#20808;&#35745;&#31639;&#30340;&#38598;&#21512;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#38024;&#23545;&#23545;&#35937;&#26080;&#20851;&#30340;&#26657;&#20934;&#27493;&#39588;&#23558;RGB&#35302;&#35273;&#35266;&#27979;&#26144;&#23556;&#21040;&#20108;&#20540;&#25509;&#35302;&#24418;&#29366;&#65292;&#20174;&#20256;&#24863;&#22120;&#20013;&#33719;&#24471;&#25509;&#35302;&#24418;&#29366;&#12290;&#36825;&#20010;&#26144;&#23556;&#21487;&#20197;&#22312;&#23545;&#35937;&#21644;&#20256;&#24863;&#22120;&#23454;&#20363;&#20043;&#38388;&#37325;&#22797;&#20351;&#29992;&#65292;&#26159;&#21807;&#19968;&#20351;&#29992;&#30495;&#23454;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27493;&#39588;&#12290;&#36825;&#26679;&#23601;&#21487;&#20197;&#36890;&#36807;&#31532;&#19968;&#27425;&#30495;&#23454;&#35302;&#35273;&#35266;&#27979;&#26469;&#23450;&#20301;&#29289;&#20307;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#23039;&#24577;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#23558;&#20854;&#20182;&#20256;&#24863;&#22120;&#25968;&#25454;&#25972;&#21512;&#21040;&#23039;&#24577;&#20272;&#35745;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Tac2Pose, an object-specific approach to tactile pose estimation from the first touch for known objects. Given the object geometry, we learn a tailored perception model in simulation that estimates a probability distribution over possible object poses given a tactile observation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor, we match it against the pre-computed set using an object-specific embedding learned using contrastive learning. We obtain contact shapes from the sensor with an object-agnostic calibration step that maps RGB tactile observations to binary contact shapes. This mapping, which can be reused across object and sensor instances, is the only step trained with real sensor data. This results in a perception model that localizes objects from the first real tactile observation. Importantly, it produces pose distributions and can incorpor
&lt;/p&gt;</description></item><item><title>GP-BART&#26159;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#26032;&#22411;&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641;&#26041;&#27861;&#65292;&#30456;&#27604;&#26631;&#20934;BART&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#24179;&#28369;&#24615;&#21644;&#26126;&#30830;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#20551;&#35774;&#65292;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#26174;&#31034;&#20986;&#36229;&#36234;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.02112</link><description>&lt;p&gt;
GP-BART: &#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#26032;&#22411;&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes. (arXiv:2204.02112v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02112
&lt;/p&gt;
&lt;p&gt;
GP-BART&#26159;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#26032;&#22411;&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641;&#26041;&#27861;&#65292;&#30456;&#27604;&#26631;&#20934;BART&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#24179;&#28369;&#24615;&#21644;&#26126;&#30830;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#20551;&#35774;&#65292;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#26174;&#31034;&#20986;&#36229;&#36234;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641; (BART) &#27169;&#22411;&#26159;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#22987;&#32456;&#24378;&#22823;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#24191;&#27867;&#19988;&#25104;&#21151;&#22320;&#20351;&#29992;&#12290;BART&#36890;&#36807;&#19968;&#32452;&#32553;&#20943;&#20808;&#39564;&#23558;&#8220;&#24369;&#8221;&#26641;&#27169;&#22411;&#32452;&#21512;&#36215;&#26469;&#65292;&#20854;&#20013;&#27599;&#20010;&#26641;&#35299;&#37322;&#20102;&#25968;&#25454;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#21464;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;BART&#27169;&#22411;&#20013;&#32570;&#20047;&#24179;&#28369;&#24615;&#24182;&#19988;&#23545;&#35266;&#27979;&#20540;&#20043;&#38388;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#27809;&#26377;&#26126;&#30830;&#20551;&#35774;&#65292;&#36825;&#22312;&#38656;&#35201;&#36825;&#20123;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#39640;&#26031;&#36807;&#31243;&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641; (GP-BART) &#27169;&#22411;&#26159;&#23545;BART&#30340;&#25193;&#23637;&#65292;&#23427;&#36890;&#36807;&#20551;&#35774;&#27599;&#20010;&#32456;&#31471;&#33410;&#28857;&#30340;&#39044;&#27979;&#26381;&#20174;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#24212;&#29992;&#26469;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#36229;&#36234;&#20102;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bayesian additive regression trees (BART) model is an ensemble method extensively and successfully used in regression tasks due to its consistently strong predictive performance and its ability to quantify uncertainty. BART combines "weak" tree models through a set of shrinkage priors, whereby each tree explains a small portion of the variability in the data. However, the lack of smoothness and the absence of an explicit covariance structure over the observations in standard BART can yield poor performance in cases where such assumptions would be necessary. The Gaussian processes Bayesian additive regression trees (GP-BART) model is an extension of BART which addresses this limitation by assuming Gaussian process (GP) priors for the predictions of each terminal node among all trees. The model's effectiveness is demonstrated through applications to simulated and real-world data, surpassing the performance of traditional modeling approaches in various scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.11155</link><description>&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#22312;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#34920;&#31034;&#25972;&#20010;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#23558;&#23494;&#24230;&#30697;&#38453;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#20219;&#21153;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#12290;&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#26032;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#36755;&#20837;&#20026;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;QA&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#20197;&#22686;&#24378;&#32463;&#20856;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#26694;&#26550;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20845;&#20010;&#24230;&#37327;&#26469;&#35780;&#20272;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#20449;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.07861</link><description>&lt;p&gt;
&#19981;&#35201;&#35823;&#20250;&#25105;&#65306;&#22914;&#20309;&#23558;&#28145;&#24230;&#35270;&#35273;&#35299;&#37322;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Don't Get Me Wrong: How to Apply Deep Visual Interpretations to Time Series. (arXiv:2203.07861v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20845;&#20010;&#24230;&#37327;&#26469;&#35780;&#20272;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#20449;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27491;&#30830;&#35299;&#37322;&#21644;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#38024;&#23545;&#22270;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#37322;&#24615;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#21644;&#29702;&#35299;&#20960;&#20046;&#20219;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#24191;&#21040;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#26356;&#21152;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#12290;&#19968;&#20010;&#21487;&#35270;&#21270;&#35299;&#37322;&#26159;&#21542;&#35299;&#37322;&#20102;&#26377;&#25928;&#30340;&#25512;&#29702;&#25110;&#25429;&#25417;&#20102;&#23454;&#38469;&#29305;&#24449;&#26159;&#38590;&#20197;&#21028;&#26029;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#23458;&#35266;&#35780;&#20272;&#26469;&#33719;&#24471;&#21487;&#20449;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#30450;&#30446;&#20449;&#20219;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20845;&#20010;&#27491;&#20132;&#24230;&#37327;&#65292;&#29992;&#20110;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#30740;&#31350;&#21253;&#25324;&#20102;&#24120;&#35265;&#30340;&#26102;&#38388;&#24207;&#21015;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20061;&#31181;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;UCR r&#31561;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#36825;&#20123;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The correct interpretation and understanding of deep learning models are essential in many applications. Explanatory visual interpretation approaches for image, and natural language processing allow domain experts to validate and understand almost any deep learning model. However, they fall short when generalizing to arbitrary time series, which is inherently less intuitive and more diverse. Whether a visualization explains valid reasoning or captures the actual features is difficult to judge. Hence, instead of blind trust, we need an objective evaluation to obtain trustworthy quality metrics. We propose a framework of six orthogonal metrics for gradient-, propagation- or perturbation-based post-hoc visual interpretation methods for time series classification and segmentation tasks. An experimental study includes popular neural network architectures for time series and nine visual interpretation methods. We evaluate the visual interpretation methods with diverse datasets from the UCR r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HAKE&#30340;&#30693;&#35782;&#24341;&#25806;&#65292;&#29992;&#20110;&#20154;&#31867;&#27963;&#21160;&#29702;&#35299;&#12290;&#35813;&#24341;&#25806;&#36890;&#36807;&#23558;&#20687;&#32032;&#26144;&#23556;&#21040;&#20013;&#38388;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#25512;&#26029;&#35821;&#20041;&#65292;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.06851</link><description>&lt;p&gt;
HAKE:&#20154;&#31867;&#27963;&#21160;&#29702;&#35299;&#30340;&#30693;&#35782;&#24341;&#25806;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
HAKE: A Knowledge Engine Foundation for Human Activity Understanding. (arXiv:2202.06851v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HAKE&#30340;&#30693;&#35782;&#24341;&#25806;&#65292;&#29992;&#20110;&#20154;&#31867;&#27963;&#21160;&#29702;&#35299;&#12290;&#35813;&#24341;&#25806;&#36890;&#36807;&#23558;&#20687;&#32032;&#26144;&#23556;&#21040;&#20013;&#38388;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#25512;&#26029;&#35821;&#20041;&#65292;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#29702;&#35299;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#28041;&#21450;&#21040;&#20581;&#24247;&#25252;&#29702;&#21644;&#34892;&#20026;&#20998;&#26512;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36890;&#24120;&#65292;&#20687;&#29289;&#20307;&#35782;&#21035;&#19968;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#35797;&#22270;&#30452;&#25509;&#23558;&#20687;&#32032;&#26144;&#23556;&#21040;&#35821;&#20041;&#65292;&#20294;&#27963;&#21160;&#27169;&#24335;&#19982;&#29289;&#20307;&#27169;&#24335;&#38750;&#24120;&#19981;&#21516;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#23558;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#23558;&#20687;&#32032;&#26144;&#23556;&#21040;&#30001;&#21407;&#23376;&#27963;&#21160;&#22522;&#20803;&#26500;&#25104;&#30340;&#20013;&#38388;&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#35268;&#21017;&#23545;&#26816;&#27979;&#21040;&#30340;&#22522;&#20803;&#36827;&#34892;&#32534;&#31243;&#20197;&#25512;&#26029;&#35821;&#20041;&#12290;&#20026;&#20102;&#24471;&#21040;&#19968;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20803;&#31354;&#38388;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;26+M&#20010;&#22522;&#20803;&#26631;&#31614;&#21644;&#36923;&#36753;&#35268;&#21017;&#30340;&#30693;&#35782;&#24211;&#65292;&#36825;&#20123;&#35268;&#21017;&#26159;&#36890;&#36807;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#25110;&#33258;&#21160;&#21457;&#29616;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#20154;&#31867;&#27963;&#21160;&#30693;&#35782;&#24341;&#25806;&#65288;HAKE&#65289;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity understanding is of widespread interest in artificial intelligence and spans diverse applications like health care and behavior analysis. Although there have been advances in deep learning, it remains challenging. The object recognition-like solutions usually try to map pixels to semantics directly, but activity patterns are much different from object patterns, thus hindering success. In this work, we propose a novel paradigm to reformulate this task in two stages: first mapping pixels to an intermediate space spanned by atomic activity primitives, then programming detected primitives with interpretable logic rules to infer semantics. To afford a representative primitive space, we build a knowledge base including 26+ M primitive labels and logic rules from human priors or automatic discovering. Our framework, the Human Activity Knowledge Engine (HAKE), exhibits superior generalization ability and performance upon canonical methods on challenging benchmarks. Code and data
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#28145;&#24230;&#20195;&#29702;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22120;&#26469;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#31934;&#30830;&#24615;&#21644;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2111.05841</link><description>&lt;p&gt;
&#29289;&#29702;&#22686;&#24378;&#30340;&#28145;&#24230;&#20195;&#29702;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-enhanced deep surrogates for PDEs. (arXiv:2111.05841v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.05841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#28145;&#24230;&#20195;&#29702;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22120;&#26469;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#31934;&#30830;&#24615;&#21644;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#29702;&#21644;&#24037;&#31243;&#24212;&#29992;&#38656;&#35201;&#20256;&#32479;&#19978;&#29992;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#39640;&#20445;&#30495;&#25968;&#20540;&#27714;&#35299;&#22120;&#35745;&#31639;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#23646;&#24615;&#35780;&#20272;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#35757;&#32451;&#25104;&#26412;&#24456;&#39640;&#12290;&#26032;&#20852;&#24212;&#29992;&#23558;&#33719;&#30410;&#20110;&#20855;&#26377;&#25913;&#36827;&#30340;&#20934;&#30830;&#24615;-&#25104;&#26412;&#24179;&#34913;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#19978;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"&#29289;&#29702;&#22686;&#24378;&#30340;&#28145;&#24230;&#20195;&#29702;"&#65288;"PEDS"&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#20195;&#29702;&#27169;&#22411;&#65292;&#35813;&#31995;&#32479;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#20302;&#20445;&#30495;&#24230;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22120;&#30340;&#32452;&#21512;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#20840;&#23616;&#21305;&#37197;&#26114;&#36149;&#39640;&#20445;&#30495;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#36755;&#20986;&#12290;&#22312;&#25193;&#25955;&#12289;&#21453;&#24212;&#25193;&#25955;&#21644;&#30005;&#30913;&#25955;&#23556;&#27169;&#22411;&#30340;&#19977;&#20010;&#31034;&#20363;&#27979;&#35797;&#29992;&#20363;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PEDS&#20195;&#29702;&#27604;&#19968;&#20010;&#20363;&#23376;&#21152;&#19978;&#30340;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many physics and engineering applications demand Partial Differential Equations (PDE) property evaluations that are traditionally computed with resource-intensive high-fidelity numerical solvers. Data-driven surrogate models provide an efficient alternative but come with a significant cost of training. Emerging applications would benefit from surrogates with an improved accuracy-cost tradeoff, while studied at scale. Here we present a "physics-enhanced deep-surrogate" ("PEDS") approach towards developing fast surrogate models for complex physical systems, which is described by PDEs. Specifically, a combination of a low-fidelity, explainable physics simulator and a neural network generator is proposed, which is trained end-to-end to globally match the output of an expensive high-fidelity numerical solver. Experiments on three exemplar testcases, diffusion, reaction-diffusion, and electromagnetic scattering models, show that a PEDS surrogate can be up to 3$\times$ more accurate than an e
&lt;/p&gt;</description></item><item><title>MixStyle&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#39046;&#22495;&#36716;&#31227;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#20004;&#20010;&#38543;&#26426;&#23454;&#20363;&#30340;&#29305;&#24449;&#32479;&#35745;&#26469;&#21512;&#25104;&#26032;&#39046;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#12290;MixStyle&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31867;&#23398;&#20064;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2107.02053</link><description>&lt;p&gt;
MixStyle&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#21644;&#36866;&#24212;&#24615;&#30340;&#32763;&#35793;&#21644;&#25688;&#35201;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
MixStyle Neural Networks for Domain Generalization and Adaptation. (arXiv:2107.02053v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.02053
&lt;/p&gt;
&lt;p&gt;
MixStyle&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#39046;&#22495;&#36716;&#31227;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#20004;&#20010;&#38543;&#26426;&#23454;&#20363;&#30340;&#29305;&#24449;&#32479;&#35745;&#26469;&#21512;&#25104;&#26032;&#39046;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#12290;MixStyle&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31867;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#39046;&#22495;&#36716;&#31227;&#30340;&#26410;&#35265;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MixStyle&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#21363;&#25554;&#21363;&#29992;&#65292;&#26080;&#38656;&#21442;&#25968;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#26356;&#22810;&#30340;&#25968;&#25454;&#25110;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#12290;MixStyle&#30340;&#35774;&#35745;&#24456;&#31616;&#21333;&#65306;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22312;&#19968;&#20010;&#21069;&#21521;&#20256;&#25773;&#20013;&#23558;&#20004;&#20010;&#38543;&#26426;&#23454;&#20363;&#30340;&#29305;&#24449;&#32479;&#35745;&#28151;&#21512;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#22522;&#20110;&#26368;&#26032;&#30340;&#39118;&#26684;&#36716;&#25442;&#30740;&#31350;&#21457;&#29616;&#30340;&#65292;&#29305;&#24449;&#32479;&#35745;&#25429;&#25417;&#21040;&#22270;&#20687;&#39118;&#26684;&#20449;&#24687;&#65292;&#32780;&#22270;&#20687;&#39118;&#26684;&#26412;&#36136;&#19978;&#23450;&#20041;&#20102;&#35270;&#35273;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#28151;&#21512;&#29305;&#24449;&#32479;&#35745;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21512;&#25104;&#26032;&#39046;&#22495;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;MixStyle&#24456;&#23481;&#26131;&#29992;&#20960;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#21253;&#25324;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#65292;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks do not generalize well to unseen data with domain shifts -- a longstanding problem in machine learning and AI. To overcome the problem, we propose MixStyle, a simple plug-and-play, parameter-free module that can improve domain generalization performance without the need to collect more data or increase model capacity. The design of MixStyle is simple: it mixes the feature statistics of two random instances in a single forward pass during training. The idea is grounded by the finding from recent style transfer research that feature statistics capture image style information, which essentially defines visual domains. Therefore, mixing feature statistics can be seen as an efficient way to synthesize new domains in the feature space, thus achieving data augmentation. MixStyle is easy to implement with a few lines of code, does not require modification to training objectives, and can fit a variety of learning paradigms including supervised domain generalization, semi-supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#28041;&#21450;&#21453;&#39304;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#65292;&#24212;&#29992;Fast Causal Inference (FCI)&#31639;&#27861;&#21487;&#20197;&#24471;&#21040;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#34987;&#29992;&#20110;&#19968;&#33268;&#22320;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#12289;&#30452;&#25509;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#12289;&#28151;&#28102;&#22240;&#32032;&#30340;&#32570;&#22833;&#20197;&#21450;&#22240;&#26524;&#22270;&#20013;&#29305;&#23450;&#24490;&#29615;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2005.00610</link><description>&lt;p&gt;
&#22312;&#24490;&#29615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#25512;&#26029;&#21033;&#29992;&#37096;&#20998;&#31062;&#20808;&#22270;
&lt;/p&gt;
&lt;p&gt;
Constraint-Based Causal Discovery using Partial Ancestral Graphs in the presence of Cycles. (arXiv:2005.00610v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#28041;&#21450;&#21453;&#39304;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#65292;&#24212;&#29992;Fast Causal Inference (FCI)&#31639;&#27861;&#21487;&#20197;&#24471;&#21040;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#34987;&#29992;&#20110;&#19968;&#33268;&#22320;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#12289;&#30452;&#25509;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#12289;&#28151;&#28102;&#22240;&#32032;&#30340;&#32570;&#22833;&#20197;&#21450;&#22240;&#26524;&#22270;&#20013;&#29305;&#23450;&#24490;&#29615;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21453;&#39304;&#22238;&#36335;&#22312;&#35768;&#22810;&#22797;&#26434;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#22312;&#22823;&#37096;&#20998;&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#20013;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#23384;&#22312;&#65292;&#22240;&#20026;&#36890;&#24120;&#20551;&#35774;&#31995;&#32479;&#20174;&#19968;&#24320;&#22987;&#23601;&#26159;&#38750;&#24490;&#29615;&#30340;&#12290;&#24403;&#23558;&#20026;&#38750;&#24490;&#29615;&#29615;&#22659;&#35774;&#35745;&#30340;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#24212;&#29992;&#20110;&#28041;&#21450;&#21453;&#39304;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#19981;&#20250;&#26399;&#26395;&#24471;&#21040;&#27491;&#30830;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20986;&#20154;&#24847;&#26009;&#30340;&#26159;&#65292;&#24555;&#36895;&#22240;&#26524;&#25512;&#26029;&#65288;FCI&#65289;&#31639;&#27861;&#22312;&#24212;&#29992;&#20110;&#28041;&#21450;&#21453;&#39304;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#35266;&#23519;&#25968;&#25454;&#26102;&#30340;&#36755;&#20986;&#26159;&#27491;&#30830;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#30001;&#31616;&#21333;&#19988;$\sigma$-&#21487;&#20449;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#29983;&#25104;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;FCI&#26159;&#21487;&#38752;&#32780;&#23436;&#25972;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#19968;&#33268;&#22320;&#20272;&#35745;&#65306;&#65288;i&#65289;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#65292;&#65288;ii&#65289;&#30452;&#25509;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#65292;&#65288;iii&#65289;&#28151;&#28102;&#22240;&#32032;&#30340;&#32570;&#22833;&#65292;&#20197;&#21450;&#65288;iv&#65289;&#22240;&#26524;&#22270;&#20013;&#29305;&#23450;&#24490;&#29615;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
While feedback loops are known to play important roles in many complex systems, their existence is ignored in a large part of the causal discovery literature, as systems are typically assumed to be acyclic from the outset. When applying causal discovery algorithms designed for the acyclic setting on data generated by a system that involves feedback, one would not expect to obtain correct results. In this work, we show that -- surprisingly -- the output of the Fast Causal Inference (FCI) algorithm is correct if it is applied to observational data generated by a system that involves feedback. More specifically, we prove that for observational data generated by a simple and $\sigma$-faithful Structural Causal Model (SCM), FCI is sound and complete, and can be used to consistently estimate (i) the presence and absence of causal relations, (ii) the presence and absence of direct causal relations, (iii) the absence of confounders, and (iv) the absence of specific cycles in the causal graph o
&lt;/p&gt;</description></item></channel></rss>