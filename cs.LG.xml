<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>DeepSeek LLM&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#39033;&#30446;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#24212;&#29992;&#25193;&#23637;&#35268;&#24459;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.02954</link><description>&lt;p&gt;
DeepSeek LLM&#65306;&#20511;&#21161;&#38271;&#26399;&#20027;&#20041;&#25193;&#23637;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02954
&lt;/p&gt;
&lt;p&gt;
DeepSeek LLM&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#39033;&#30446;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#24212;&#29992;&#25193;&#23637;&#35268;&#24459;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20196;&#20154;&#30633;&#30446;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#25193;&#23637;&#35268;&#24459;&#24471;&#20986;&#20102;&#19981;&#21516;&#30340;&#32467;&#35770;&#65292;&#36825;&#23545;&#25193;&#23637;LLM&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#25193;&#23637;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#20102;&#25105;&#20204;&#29420;&#29305;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;&#20419;&#36827;&#20004;&#31181;&#24120;&#29992;&#30340;&#24320;&#28304;&#37197;&#32622;&#65288;7B&#21644;67B&#65289;&#20013;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#25193;&#23637;&#12290;&#22312;&#25193;&#23637;&#35268;&#24459;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;DeepSeek LLM&#39033;&#30446;&#65292;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#25903;&#25345;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#30446;&#21069;&#21253;&#21547;2&#19975;&#20159;&#20010;&#26631;&#35760;&#65292;&#24182;&#19981;&#26029;&#25193;&#22823;&#12290;&#25105;&#20204;&#36824;&#22312;DeepSeek LLM&#22522;&#26412;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;DeepSeek LLM 67B&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;LLaMA-2 70B&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code
&lt;/p&gt;</description></item><item><title>The Tactician's Web&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#25968;&#23398;&#30693;&#35782;&#32593;&#32476;&#65292;&#36890;&#36807;Coq&#35777;&#26126;&#21161;&#25163;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#34920;&#31034;&#19982;&#35777;&#26126;&#24037;&#31243;&#24072;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#35777;&#26126;&#24037;&#31243;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.02950</link><description>&lt;p&gt;
The Tactician's Web&#30340;&#22823;&#35268;&#27169;&#24418;&#24335;&#30693;&#35782;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
The Tactician's Web of Large-Scale Formal Knowledge. (arXiv:2401.02950v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02950
&lt;/p&gt;
&lt;p&gt;
The Tactician's Web&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#25968;&#23398;&#30693;&#35782;&#32593;&#32476;&#65292;&#36890;&#36807;Coq&#35777;&#26126;&#21161;&#25163;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#34920;&#31034;&#19982;&#35777;&#26126;&#24037;&#31243;&#24072;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#35777;&#26126;&#24037;&#31243;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
The Tactician's Web&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24378;&#20851;&#32852;&#30340;&#12289;&#26426;&#22120;&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;&#25968;&#23398;&#30693;&#35782;&#32593;&#32476;&#65292;&#26041;&#20415;&#26426;&#22120;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#35777;&#26126;&#24037;&#31243;&#12290;&#22522;&#20110;Coq&#35777;&#26126;&#21161;&#25163;&#26500;&#24314;&#65292;&#35813;&#24179;&#21488;&#23548;&#20986;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#24418;&#24335;&#29702;&#35770;&#30340;&#25968;&#25454;&#38598;&#65292;&#21576;&#29616;&#20026;&#23450;&#20041;&#12289;&#23450;&#29702;&#12289;&#35777;&#26126;&#39033;&#12289;&#31574;&#30053;&#21644;&#35777;&#26126;&#29366;&#24577;&#30340;&#32593;&#32476;&#12290;&#29702;&#35770;&#26082;&#21487;&#20197;&#32534;&#30721;&#20026;&#35821;&#20041;&#22270;&#65288;&#22914;&#19979;&#25152;&#31034;&#65289;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#21487;&#35835;&#30340;&#25991;&#26412;&#65292;&#21508;&#26377;&#20248;&#32570;&#28857;&#12290;&#35777;&#26126;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#30456;&#21516;&#30340;&#20016;&#23500;&#25968;&#25454;&#34920;&#31034;&#19982;Coq&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#21487;&#20197;&#22312;&#19968;&#32452;&#23450;&#29702;&#19978;&#33258;&#21160;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19982;Coq&#30340;&#32039;&#23494;&#38598;&#25104;&#25552;&#20379;&#20102;&#23558;&#20195;&#29702;&#20316;&#20026;&#23454;&#29992;&#24037;&#20855;&#25552;&#20379;&#32473;&#35777;&#26126;&#24037;&#31243;&#24072;&#30340;&#29420;&#29305;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Tactician's Web is a platform offering a large web of strongly interconnected, machine-checked, formal mathematical knowledge conveniently packaged for machine learning, analytics, and proof engineering. Built on top of the Coq proof assistant, the platform exports a dataset containing a wide variety of formal theories, presented as a web of definitions, theorems, proof terms, tactics, and proof states. Theories are encoded both as a semantic graph (rendered below) and as human-readable text, each with a unique set of advantages and disadvantages. Proving agents may interact with Coq through the same rich data representation and can be automatically benchmarked on a set of theorems. Tight integration with Coq provides the unique possibility to make agents available to proof engineers as practical tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02949</link><description>&lt;p&gt;
Graph2Tac: &#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21450;&#20854;&#24212;&#29992;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#23398;&#31185;&#39046;&#22495;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#27599;&#31687;&#25968;&#23398;&#35770;&#25991;&#25110;&#24212;&#29992;&#20013;&#37117;&#20250;&#24341;&#20837;&#26032;&#30340;&#27010;&#24565;&#12290;&#24418;&#24335;&#21270;&#29702;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#23450;&#20041;&#12289;&#23450;&#29702;&#21644;&#30456;&#20114;&#24341;&#29992;&#30340;&#35777;&#26126;&#12290;&#24403;&#19968;&#20010;AI&#20195;&#29702;&#20154;&#35777;&#26126;&#19968;&#20010;&#26032;&#30340;&#23450;&#29702;&#26102;&#65292;&#22823;&#22810;&#25968;&#19982;&#35813;&#23450;&#29702;&#30456;&#20851;&#30340;&#25968;&#23398;&#27010;&#24565;&#21644;&#24341;&#29702;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20174;&#26410;&#34987;&#35265;&#36807;&#12290;&#36825;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#35813;&#21161;&#25163;&#25317;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;Coq&#39033;&#30446;&#65292;&#27599;&#20010;&#39033;&#30446;&#37117;&#26377;&#33258;&#24049;&#30340;&#23450;&#20041;&#12289;&#24341;&#29702;&#65292;&#29978;&#33267;&#29992;&#20110;&#35777;&#26126;&#36825;&#20123;&#24341;&#29702;&#30340;&#33258;&#23450;&#20041;&#31574;&#30053;&#36807;&#31243;&#12290;&#23558;&#36825;&#26679;&#30340;&#26032;&#20449;&#24687;&#21363;&#26102;&#22320;&#34701;&#20837;&#21040;&#20195;&#29702;&#20154;&#30340;&#30693;&#35782;&#24211;&#20013;&#23545;&#20110;&#20195;&#29702;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26032;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;Coq&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;Coq&#26415;&#35821;&#30340;&#24544;&#23454;&#22270;&#34920;&#31034;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;Graph2Tac&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23450;&#20041;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Rydberg&#21407;&#23376;&#38453;&#21015;&#19978;&#36827;&#34892;&#25968;&#23383;-&#27169;&#25311;&#37327;&#23376;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36817;&#26399;&#23454;&#29616;&#36825;&#19968;&#31639;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#25968;&#23383;&#23398;&#20064;&#26041;&#26696;&#65292;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#38656;&#35201;&#26356;&#30701;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#29616;&#23454;&#35823;&#24046;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#36825;&#20351;&#24471;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#25104;&#20026;&#20013;&#26399;&#25913;&#36827;&#21464;&#20998;&#37327;&#23376;&#23398;&#20064;&#23454;&#39564;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02940</link><description>&lt;p&gt;
Rydberg&#21407;&#23376;&#38453;&#21015;&#30340;&#25968;&#23383;-&#27169;&#25311;&#37327;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Digital-analog quantum learning on Rydberg atom arrays. (arXiv:2401.02940v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Rydberg&#21407;&#23376;&#38453;&#21015;&#19978;&#36827;&#34892;&#25968;&#23383;-&#27169;&#25311;&#37327;&#23376;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36817;&#26399;&#23454;&#29616;&#36825;&#19968;&#31639;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#25968;&#23383;&#23398;&#20064;&#26041;&#26696;&#65292;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#38656;&#35201;&#26356;&#30701;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#29616;&#23454;&#35823;&#24046;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#36825;&#20351;&#24471;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#25104;&#20026;&#20013;&#26399;&#25913;&#36827;&#21464;&#20998;&#37327;&#23376;&#23398;&#20064;&#23454;&#39564;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Rydberg&#21407;&#23376;&#38453;&#21015;&#19978;&#30340;&#28151;&#21512;&#25968;&#23383;-&#27169;&#25311;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#37327;&#23376;&#23398;&#20064;&#30340;&#28508;&#22312;&#23454;&#29992;&#24615;&#21644;&#36817;&#26399;&#21487;&#23454;&#29616;&#24615;&#20197;&#21450;&#20013;&#24615;&#21407;&#23376;&#30340;&#24555;&#36895;&#25193;&#23637;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26500;&#24314;&#21482;&#38656;&#22312;&#25968;&#23383;&#35774;&#32622;&#20013;&#36827;&#34892;&#21333;&#37327;&#23376;&#27604;&#29305;&#25805;&#20316;&#65292;&#24182;&#26681;&#25454;Rydberg&#21704;&#23494;&#39039;&#37327;&#36827;&#34892;&#27169;&#25311;&#35774;&#32622;&#20013;&#30340;&#20840;&#23616;&#39537;&#21160;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#21644;&#37327;&#23376;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#20998;&#21035;&#36890;&#36807;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#26080;&#30417;&#30563;&#37327;&#23376;&#30456;&#36793;&#30028;&#23398;&#20064;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#36825;&#20004;&#20010;&#20856;&#22411;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#19981;&#20165;&#22312;&#36817;&#26399;&#26159;&#21487;&#34892;&#30340;&#65292;&#32780;&#19988;&#19982;&#25968;&#23383;&#23398;&#20064;&#26041;&#26696;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#30701;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#29616;&#23454;&#35823;&#24046;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20013;&#26399;&#25552;&#39640;&#21464;&#20998;&#37327;&#23376;&#23398;&#20064;&#23454;&#39564;&#30340;&#25928;&#26524;&#26041;&#38754;&#65292;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#25171;&#24320;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose hybrid digital-analog learning algorithms on Rydberg atom arrays, combining the potentially practical utility and near-term realizability of quantum learning with the rapidly scaling architectures of neutral atoms. Our construction requires only single-qubit operations in the digital setting and global driving according to the Rydberg Hamiltonian in the analog setting. We perform a comprehensive numerical study of our algorithm on both classical and quantum data, given respectively by handwritten digit classification and unsupervised quantum phase boundary learning. We show in the two representative problems that digital-analog learning is not only feasible in the near term, but also requires shorter circuit depths and is more robust to realistic error models as compared to digital learning schemes. Our results suggest that digital-analog learning opens a promising path towards improved variational quantum learning experiments in the near term.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#22312;&#24191;&#27867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02938</link><description>&lt;p&gt;
&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#22312;&#24191;&#27867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35268;&#27169;&#24222;&#22823;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#24494;&#35843;&#65292;&#36825;&#26159;&#20026;&#20102;&#24674;&#22797;&#22240;&#21024;&#38500;&#26435;&#37325;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#35201;&#20040;&#23436;&#20840;&#24573;&#30053;&#20102;&#24494;&#35843;&#65292;&#19987;&#27880;&#20110;&#39640;&#25928;&#30340;&#20462;&#21098;&#26631;&#20934;&#65292;&#35201;&#20040;&#23581;&#35797;&#36880;&#23618;&#26435;&#37325;&#26356;&#26032;&#65292;&#20445;&#25345;&#27599;&#20010;&#23618;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#36880;&#23618;&#26435;&#37325;&#26356;&#26032;&#23545;LLMs&#26469;&#35828;&#20063;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#24471;&#19981;&#37319;&#29992;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#12290;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24191;&#27867;&#30340;LLMs&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/fmfi-compbio/admm-pruning&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations.  In our paper, we propose a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). Coupled with a simple iterative pruning mask selection, our algorithm achieves state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.
&lt;/p&gt;</description></item><item><title>Dagma-DCE&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#12289;&#38750;&#21442;&#25968;&#30340;&#21487;&#24494;&#22240;&#26524;&#21457;&#29616;&#26041;&#26696;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#23450;&#20041;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.02930</link><description>&lt;p&gt;
Dagma-DCE&#65306;&#21487;&#35299;&#37322;&#30340;&#12289;&#38750;&#21442;&#25968;&#30340;&#21487;&#24494;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery. (arXiv:2401.02930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02930
&lt;/p&gt;
&lt;p&gt;
Dagma-DCE&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#12289;&#38750;&#21442;&#25968;&#30340;&#21487;&#24494;&#22240;&#26524;&#21457;&#29616;&#26041;&#26696;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#23450;&#20041;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Dagma-DCE&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#24494;&#22240;&#26524;&#21457;&#29616;&#26041;&#26696;&#12290;&#24403;&#21069;&#30340;&#38750;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#26041;&#27861;&#22312;&#21487;&#24494;&#22240;&#26524;&#21457;&#29616;&#20013;&#20351;&#29992;&#19981;&#36879;&#26126;&#30340;``&#29420;&#31435;&#24615;''&#20195;&#29702;&#26469;&#35777;&#26126;&#26159;&#21542;&#21253;&#21547;&#25110;&#25490;&#38500;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#36825;&#20123;&#20195;&#29702;&#21487;&#33021;&#19982;&#23454;&#38469;&#30340;&#22240;&#26524;&#24378;&#24230;&#20219;&#24847;&#19981;&#21516;&#12290;&#19982;&#29616;&#26377;&#30340;&#21487;&#24494;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30456;&#27604;&#65292;Dagma-DCE&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#26469;&#23450;&#20041;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#12290;&#22312;&#19968;&#20123;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Dagma-DCE&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#26377;&#21407;&#21017;&#30340;&#38408;&#20540;&#35774;&#23450;&#21644;&#31232;&#30095;&#24809;&#32602;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20195;&#30721;&#22312;https://github.com/DanWaxman/DAGMA-DCE&#19978;&#24320;&#28304;&#65292;&#24182;&#19988;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;&#20219;&#24847;&#30340;&#21487;&#24494;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Dagma-DCE, an interpretable and model-agnostic scheme for differentiable causal discovery. Current non- or over-parametric methods in differentiable causal discovery use opaque proxies of ``independence'' to justify the inclusion or exclusion of a causal relationship. We show theoretically and empirically that these proxies may be arbitrarily different than the actual causal strength. Juxtaposed to existing differentiable causal discovery algorithms, \textsc{Dagma-DCE} uses an interpretable measure of causal strength to define weighted adjacency matrices. In a number of simulated datasets, we show our method achieves state-of-the-art level performance. We additionally show that \textsc{Dagma-DCE} allows for principled thresholding and sparsity penalties by domain-experts. The code for our method is available open-source at https://github.com/DanWaxman/DAGMA-DCE, and can easily be adapted to arbitrary differentiable models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25506;&#32034;&#31639;&#27861;&#65292;&#23558;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25506;&#32034;&#65292;&#36890;&#36807;&#37327;&#21270;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#32508;&#21512;&#25928;&#24212;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.02914</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25506;&#32034;&#65306;&#32467;&#21512;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
A unified uncertainty-aware exploration: Combining epistemic and aleatory uncertainty. (arXiv:2401.02914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25506;&#32034;&#31639;&#27861;&#65292;&#23558;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25506;&#32034;&#65292;&#36890;&#36807;&#37327;&#21270;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#32508;&#21512;&#25928;&#24212;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#32780;&#23558;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25506;&#32034;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25429;&#25417;&#20598;&#28982;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#32852;&#21512;&#25928;&#24212;&#29992;&#20110;&#20915;&#31574;&#26159;&#22256;&#38590;&#30340;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20998;&#21035;&#20272;&#35745;&#20102;&#20598;&#28982;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#32452;&#21512;&#19981;&#30830;&#23450;&#24615;&#35270;&#20026;&#20004;&#32773;&#30340;&#21152;&#27861;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21152;&#27861;&#24418;&#24335;&#20250;&#23548;&#33268;&#36807;&#24230;&#20882;&#38505;&#34892;&#20026;&#65292;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#28548;&#28165;&#20102;&#20598;&#28982;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#32479;&#19968;&#20102;&#20598;&#28982;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#37327;&#21270;&#20102;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#25506;&#32034;&#30340;&#32508;&#21512;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#65292;&#20854;&#20272;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#22238;&#25253;&#20998;&#24067;&#65292;&#20854;&#21442;&#25968;&#21270;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration is a significant challenge in practical reinforcement learning (RL), and uncertainty-aware exploration that incorporates the quantification of epistemic and aleatory uncertainty has been recognized as an effective exploration strategy. However, capturing the combined effect of aleatory and epistemic uncertainty for decision-making is difficult. Existing works estimate aleatory and epistemic uncertainty separately and consider the composite uncertainty as an additive combination of the two. Nevertheless, the additive formulation leads to excessive risk-taking behavior, causing instability. In this paper, we propose an algorithm that clarifies the theoretical connection between aleatory and epistemic uncertainty, unifies aleatory and epistemic uncertainty estimation, and quantifies the combined effect of both uncertainties for a risk-sensitive exploration. Our method builds on a novel extension of distributional RL that estimates a parameterized return distribution whose para
&lt;/p&gt;</description></item><item><title>H2G2-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.02905</link><description>&lt;p&gt;
H2G2-Net:&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#21457;&#29616;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02905
&lt;/p&gt;
&lt;p&gt;
H2G2-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#30740;&#31350;&#24212;&#29992;&#20013;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#26469;&#21457;&#29616;&#20154;&#31867;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20154;&#20307;&#30340;&#29983;&#29702;&#21453;&#24212;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#24433;&#21709;&#65292;&#24120;&#29992;&#20110;&#20998;&#26512;&#35748;&#30693;&#29366;&#24577;&#12290;&#20174;&#32593;&#32476;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#24322;&#26500;&#29983;&#29702;&#27169;&#24335;&#22312;&#22270;&#32467;&#26500;&#20013;&#30340;&#20114;&#21160;&#21487;&#33021;&#25552;&#20379;&#26377;&#30410;&#30340;&#20449;&#24687;&#26469;&#25903;&#25345;&#35748;&#30693;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21150;&#27861;&#24471;&#21040;&#24322;&#26500;&#27169;&#24577;&#20043;&#38388;&#30340;&#31934;&#30830;&#36830;&#25509;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#31181;&#20998;&#23618;&#32467;&#26500;&#30340;&#23376;&#27169;&#24577;&#12290;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#29992;&#20110;&#22312;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#19978;&#23398;&#20064;&#38750;&#23618;&#27425;&#21270;&#30340;&#21516;&#36136;&#22270;&#65292;&#26080;&#27861;&#20174;&#23618;&#27425;&#21270;&#30340;&#22810;&#27169;&#24577;&#29983;&#29702;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#65288;H2G2-Net&#65289;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering human cognitive and emotional states using multi-modal physiological signals draws attention across various research applications. Physiological responses of the human body are influenced by human cognition and commonly used to analyze cognitive states. From a network science perspective, the interactions of these heterogeneous physiological modalities in a graph structure may provide insightful information to support prediction of cognitive states. However, there is no clue to derive exact connectivity between heterogeneous modalities and there exists a hierarchical structure of sub-modalities. Existing graph neural networks are designed to learn on non-hierarchical homogeneous graphs with pre-defined graph structures; they failed to learn from hierarchical, multi-modal physiological data without a pre-defined graph structure. To this end, we propose a hierarchical heterogeneous graph generative network (H2G2-Net) that automatically learns a graph structure without domain 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#26512;&#30740;&#31350;&#20102;&#31867;&#21035;&#26222;&#36866;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;KL&#25955;&#24230;&#30340;&#20449;&#24687;&#35770;&#30028;&#38480;&#21644;&#20351;&#29992;&#26465;&#20214;&#20114;&#20449;&#24687;(CMI)&#30340;&#26356;&#32039;&#30028;&#38480;&#65292;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#30340;&#31867;&#21035;&#26222;&#36866;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.02904</link><description>&lt;p&gt;
&#31867;&#21035;&#26222;&#36866;&#35823;&#24046;&#65306;&#20449;&#24687;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Class-wise Generalization Error: an Information-Theoretic Analysis. (arXiv:2401.02904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#26512;&#30740;&#31350;&#20102;&#31867;&#21035;&#26222;&#36866;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;KL&#25955;&#24230;&#30340;&#20449;&#24687;&#35770;&#30028;&#38480;&#21644;&#20351;&#29992;&#26465;&#20214;&#20114;&#20449;&#24687;(CMI)&#30340;&#26356;&#32039;&#30028;&#38480;&#65292;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#30340;&#31867;&#21035;&#26222;&#36866;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26222;&#36866;&#24615;&#29702;&#35770;&#36890;&#24120;&#37319;&#29992;&#25972;&#20307;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#19978;&#39044;&#26399;&#26222;&#36866;&#21270;&#30340;&#30028;&#38480;&#65292;&#36825;&#38544;&#21547;&#22320;&#20551;&#23450;&#27169;&#22411;&#23545;&#25152;&#26377;&#31867;&#21035;&#37117;&#20855;&#26377;&#31867;&#20284;&#30340;&#26222;&#36866;&#24615;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#19981;&#21516;&#31867;&#21035;&#30340;&#26222;&#36866;&#24615;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#36825;&#26080;&#27861;&#34987;&#29616;&#26377;&#30340;&#26222;&#36866;&#24615;&#30028;&#38480;&#25152;&#25429;&#25417;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#19978;&#30740;&#31350;&#31867;&#21035;&#26222;&#36866;&#35823;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#35823;&#24046;&#37327;&#21270;&#20102;&#27599;&#20010;&#20010;&#20307;&#31867;&#21035;&#30340;&#26222;&#36866;&#24615;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;KL&#25955;&#24230;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#30028;&#38480;&#26469;&#34913;&#37327;&#31867;&#21035;&#26222;&#36866;&#35823;&#24046;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#26465;&#20214;&#20114;&#20449;&#24687;(CMI)&#24471;&#21040;&#20102;&#20960;&#20010;&#26356;&#32039;&#30340;&#30028;&#38480;&#65292;&#22312;&#23454;&#36341;&#20013;&#26356;&#23481;&#26131;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#23454;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#30028;&#38480;&#20934;&#30830;&#22320;&#25429;&#25417;&#20102;&#22797;&#26434;&#30340;&#31867;&#21035;&#26222;&#36866;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing generalization theories of supervised learning typically take a holistic approach and provide bounds for the expected generalization over the whole data distribution, which implicitly assumes that the model generalizes similarly for all the classes. In practice, however, there are significant variations in generalization performance among different classes, which cannot be captured by the existing generalization bounds. In this work, we tackle this problem by theoretically studying the class-generalization error, which quantifies the generalization performance of each individual class. We derive a novel information-theoretic bound for class-generalization error using the KL divergence, and we further obtain several tighter bounds using the conditional mutual information (CMI), which are significantly easier to estimate in practice. We empirically validate our proposed bounds in different neural networks and show that they accurately capture the complex class-generalization err
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36870;&#21521;&#22686;&#24378;&#23398;&#20064;&#26469;&#23454;&#29616;&#33258;&#20027;Formula SAE&#36710;&#36742;&#30340;&#23616;&#37096;&#36335;&#24452;&#36319;&#38543;&#12290;&#20351;&#29992;&#36719;actor&#35780;&#21028;&#21644;&#23545;&#25239;&#24615;&#36870;&#21521;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#39062;&#22870;&#21169;&#20989;&#25968;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23616;&#37096;&#36335;&#24452;&#36319;&#38543;&#12290;</title><link>http://arxiv.org/abs/2401.02903</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33258;&#20027;Formula SAE&#36710;&#36742;&#30340;&#23616;&#37096;&#36335;&#24452;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle. (arXiv:2401.02903v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36870;&#21521;&#22686;&#24378;&#23398;&#20064;&#26469;&#23454;&#29616;&#33258;&#20027;Formula SAE&#36710;&#36742;&#30340;&#23616;&#37096;&#36335;&#24452;&#36319;&#38543;&#12290;&#20351;&#29992;&#36719;actor&#35780;&#21028;&#21644;&#23545;&#25239;&#24615;&#36870;&#21521;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#39062;&#22870;&#21169;&#20989;&#25968;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23616;&#37096;&#36335;&#24452;&#36319;&#38543;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#20154;&#39550;&#39542;&#27604;&#36187;&#22312;&#20840;&#29699;Formula:Society of Automotive Engineers (F:SAE)&#31454;&#36187;&#20013;&#30340;&#25345;&#32493;&#25512;&#20986;&#65292;&#36710;&#38431;&#27491;&#22312;&#30740;&#31350;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#31995;&#32479;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;Deep Reinforcement Learning&#65292;DRL&#65289;&#21644;&#36870;&#21521;&#22686;&#24378;&#23398;&#20064;&#65288;Inverse Reinforcement Learning&#65292;IRL&#65289;&#26469;&#23558;&#23616;&#37096;&#35266;&#23519;&#21040;&#30340;&#38181;&#24418;&#20301;&#32622;&#26144;&#23556;&#21040;&#26399;&#26395;&#30340;&#36716;&#21521;&#35282;&#24230;&#20197;&#23454;&#29616;&#36187;&#36947;&#36319;&#38543;&#12290;&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#20004;&#31181;&#22312;&#27492;&#32972;&#26223;&#19979;&#23578;&#26410;&#27979;&#35797;&#36807;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65306;&#36719;actor&#35780;&#21028;&#65288;soft actor critic&#65292;SAC&#65289;&#21644;&#23545;&#25239;&#24615;&#36870;&#21521;&#22686;&#24378;&#23398;&#20064;&#65288;adversarial inverse reinforcement learning&#65292;AIRL&#65289;&#65292;&#24182;&#22312;&#20195;&#34920;&#24615;&#20223;&#30495;&#20013;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#36824;&#35752;&#35770;&#20102;&#19977;&#31181;&#22312;&#33258;&#20027;&#36187;&#36710;&#32972;&#26223;&#19979;&#30001;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#26032;&#39062;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#20986;&#29992;&#20110;&#23616;&#37096;&#36335;&#24452;&#36319;&#38543;&#30340;&#27169;&#22411;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#65292;&#20197;&#20351;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#23436;&#25972;&#30340;F:SAE&#36710;&#36742;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continued introduction of driverless events to Formula:Society of Automotive Engineers (F:SAE) competitions around the world, teams are investigating all aspects of the autonomous vehicle stack. This paper presents the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL) to map locally-observed cone positions to a desired steering angle for race track following. Two state-of-the-art algorithms not previously tested in this context: soft actor critic (SAC) and adversarial inverse reinforcement learning (AIRL), are used to train models in a representative simulation. Three novel reward functions for use by RL algorithms in an autonomous racing context are also discussed. Tests performed in simulation and the real world suggest that both algorithms can successfully train models for local path following. Suggestions for future work are presented to allow these models to scale to a full F:SAE vehicle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20272;&#35745;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#23548;&#25968;&#32423;&#21035;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#65292;&#35299;&#20915;&#20102;&#38544;&#34255;&#29366;&#24577;&#12289;&#38544;&#34255;&#29366;&#24577;&#23548;&#25968;&#20197;&#21450;&#26102;&#38388;&#38388;&#38548;&#30340;&#26631;&#20934;&#21270;&#25361;&#25112;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#19982;&#24453;&#35782;&#21035;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30456;&#20851;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#33719;&#24471;&#26377;&#25928;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02902</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#23548;&#25968;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Derivative Normalization for Continuous-Time Deep Neural Networks. (arXiv:2401.02902v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20272;&#35745;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#23548;&#25968;&#32423;&#21035;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#65292;&#35299;&#20915;&#20102;&#38544;&#34255;&#29366;&#24577;&#12289;&#38544;&#34255;&#29366;&#24577;&#23548;&#25968;&#20197;&#21450;&#26102;&#38388;&#38388;&#38548;&#30340;&#26631;&#20934;&#21270;&#25361;&#25112;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#19982;&#24453;&#35782;&#21035;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30456;&#20851;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#33719;&#24471;&#26377;&#25928;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#24403;&#25968;&#25454;&#26631;&#20934;&#21270;&#30340;&#37325;&#35201;&#24615;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20272;&#35745;&#20013;&#65292;&#35266;&#23519;&#21040;&#27169;&#22411;&#20272;&#35745;&#30340;&#38544;&#34255;&#29366;&#24577;&#25110;&#38544;&#34255;&#29366;&#24577;&#23548;&#25968;&#65292;&#29978;&#33267;&#26102;&#38388;&#38388;&#38548;&#30340;&#19981;&#36866;&#24403;&#26631;&#20934;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26102;&#30340;&#25968;&#20540;&#21644;&#20248;&#21270;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19977;&#20010;&#26631;&#20934;&#21270;&#20219;&#21153;&#30340;&#20869;&#22312;&#32806;&#21512;&#12290;&#30001;&#20110;&#23384;&#22312;&#36825;&#31181;&#32806;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29366;&#24577;&#23548;&#25968;&#27700;&#24179;&#24341;&#20837;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24403;&#36873;&#25321;&#26631;&#20934;&#21270;&#24120;&#25968;&#19982;&#24453;&#35782;&#21035;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30456;&#20851;&#65292;&#24182;&#25512;&#23548;&#20102;&#22810;&#31181;&#33719;&#24471;&#26377;&#25928;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#23454;&#39564;&#25968;&#25454;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#27604;&#36739;&#21644;&#35752;&#35770;&#20102;&#25152;&#26377;&#26631;&#20934;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of proper data normalization for deep neural networks is well known. However, in continuous-time state-space model estimation, it has been observed that improper normalization of either the hidden state or hidden state derivative of the model estimate, or even of the time interval can lead to numerical and optimization challenges with deep learning based methods. This results in a reduced model quality. In this contribution, we show that these three normalization tasks are inherently coupled. Due to the existence of this coupling, we propose a solution to all three normalization challenges by introducing a normalization constant at the state derivative level. We show that the appropriate choice of the normalization constant is related to the dynamics of the to-be-identified system and we derive multiple methods of obtaining an effective normalization constant. We compare and discuss all the normalization strategies on a benchmark problem based on experimental data from a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#28369;&#26680;&#31215;&#20998;&#21464;&#25442;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#32500;&#24230;&#32553;&#20943;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02890</link><description>&lt;p&gt;
&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonlinear functional regression by functional deep neural network with kernel embedding. (arXiv:2401.02890v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#28369;&#26680;&#31215;&#20998;&#21464;&#25442;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#32500;&#24230;&#32553;&#20943;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#23427;&#20063;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26080;&#38480;&#32500;&#30340;&#36755;&#20837;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#24378;&#22823;&#30340;&#32500;&#24230;&#32553;&#20943;&#26041;&#27861;&#26469;&#22788;&#29702;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#24179;&#28369;&#26680;&#31215;&#20998;&#21464;&#25442;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#19988;&#23436;&#20840;&#25968;&#25454;&#20381;&#36182;&#30340;&#32500;&#24230;&#32553;&#20943;&#26041;&#27861;&#30340;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20989;&#25968;&#32593;&#32476;&#30001;&#20197;&#19979;&#27493;&#39588;&#32452;&#25104;&#65306;&#26680;&#23884;&#20837;&#27493;&#39588;&#65306;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#30340;&#24179;&#28369;&#26680;&#36827;&#34892;&#31215;&#20998;&#21464;&#25442;&#65307;&#25237;&#24433;&#27493;&#39588;&#65306;&#36890;&#36807;&#22522;&#20110;&#23884;&#20837;&#26680;&#30340;&#29305;&#24449;&#20989;&#25968;&#22522;&#24213;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65307;&#26368;&#21518;&#26159;&#19968;&#20010;&#34920;&#36798;&#20016;&#23500;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of deep learning in various fields of science and technology, such as speech recognition, image classification, and natural language processing, recently it is also widely applied in the functional data analysis (FDA) with some empirical success. However, due to the infinite dimensional input, we need a powerful dimension reduction method for functional learning tasks, especially for the nonlinear functional regression. In this paper, based on the idea of smooth kernel integral transformation, we propose a functional deep neural network with an efficient and fully data-dependent dimension reduction method. The architecture of our functional net consists of a kernel embedding step: an integral transformation with a data-dependent smooth kernel; a projection step: a dimension reduction by projection with eigenfunction basis based on the embedding kernel; and finally an expressive deep ReLU neural network for the prediction. The utilization of smooth kernel embe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33410;&#32422;&#33021;&#37327;&#30340;&#20943;&#23569;&#25805;&#20316;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35774;&#35745;&#21644;&#25511;&#21046;&#20013;&#30340;&#22810;&#27425;&#35745;&#31639;&#20219;&#21153;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20445;&#25345;&#33021;&#37327;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.02889</link><description>&lt;p&gt;
&#39640;&#25928;&#35774;&#35745;&#21644;&#25511;&#21046;&#30340;&#33410;&#32422;&#33021;&#37327;&#32422;&#26463;&#19979;&#30340;&#20943;&#23569;&#25805;&#20316;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Energy-Preserving Reduced Operator Inference for Efficient Design and Control. (arXiv:2401.02889v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33410;&#32422;&#33021;&#37327;&#30340;&#20943;&#23569;&#25805;&#20316;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35774;&#35745;&#21644;&#25511;&#21046;&#20013;&#30340;&#22810;&#27425;&#35745;&#31639;&#20219;&#21153;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20445;&#25345;&#33021;&#37327;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35774;&#35745;&#21644;&#25511;&#21046;&#20013;&#38656;&#35201;&#22810;&#27425;&#35745;&#31639;&#30340;&#24037;&#31243;&#31995;&#32479;&#65292;&#35745;&#31639;&#27169;&#22411;&#30340;&#39640;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25511;&#21046;&#30340;&#31995;&#32479;&#65292;&#24120;&#35265;&#30340;&#39640;&#20445;&#30495;&#25968;&#20540;&#27169;&#22411;&#26159;&#39640;&#32500;&#30340;&#65292;&#23545;&#20110;&#22810;&#27425;&#35745;&#31639;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#23454;&#29616;&#35774;&#35745;&#21644;&#25511;&#21046;&#20013;&#30340;&#20302;&#25104;&#26412;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#29289;&#29702;&#32422;&#26463;&#30340;&#20943;&#23569;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#33021;&#37327;&#20445;&#25345;&#30340;PDE&#65292;&#20363;&#22914;&#35768;&#22810;&#27969;&#20307;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25805;&#20316;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#23558;&#20943;&#23569;&#27169;&#22411;&#31639;&#23376;&#19982;&#29366;&#24577;&#24555;&#29031;&#21644;&#26102;&#38388;&#23548;&#25968;&#25968;&#25454;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#25805;&#20316;&#25512;&#26029;&#36890;&#24120;&#19981;&#33021;&#23398;&#20064;&#21040;&#20855;&#26377;&#33021;&#37327;&#20445;&#25345;&#23646;&#24615;&#30340;&#20943;&#23569;&#20108;&#27425;&#31639;&#23376;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33021;&#37327;&#20445;&#25345;&#25805;&#20316;&#25512;&#26029;&#65288;EP-O&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many-query computations, in which a computational model for an engineering system must be evaluated many times, are crucial in design and control. For systems governed by partial differential equations (PDEs), typical high-fidelity numerical models are high-dimensional and too computationally expensive for the many-query setting. Thus, efficient surrogate models are required to enable low-cost computations in design and control. This work presents a physics-preserving reduced model learning approach that targets PDEs whose quadratic operators preserve energy, such as those arising in governing equations in many fluids problems. The approach is based on the Operator Inference method, which fits reduced model operators to state snapshot and time derivative data in a least-squares sense. However, Operator Inference does not generally learn a reduced quadratic operator with the energy-preserving property of the original PDE. Thus, we propose a new energy-preserving Operator Inference (EP-O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#26680;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23376;&#37319;&#26679;&#35757;&#32451;&#30340;&#26041;&#24335;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20197;&#35299;&#20915;&#37327;&#23376;&#26680;&#23545;&#40784;&#30340;&#35757;&#32451;&#20195;&#20215;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02879</link><description>&lt;p&gt;
&#29992;&#20110;&#37327;&#23376;&#26680;&#23545;&#40784;&#30340;&#39640;&#25928;&#21442;&#25968;&#20248;&#21270;&#65306;&#19968;&#31181;&#22312;&#21487;&#21464;&#35757;&#32451;&#20013;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Parameter Optimisation for Quantum Kernel Alignment: A Sub-sampling Approach in Variational Training. (arXiv:2401.02879v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#26680;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23376;&#37319;&#26679;&#35757;&#32451;&#30340;&#26041;&#24335;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20197;&#35299;&#20915;&#37327;&#23376;&#26680;&#23545;&#40784;&#30340;&#35757;&#32451;&#20195;&#20215;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#30340;&#37327;&#23376;&#26680;&#23545;&#40784;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#26680;&#20989;&#25968;&#30340;&#37327;&#23376;&#26680;&#23545;&#40784;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#26680;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#19982;&#29305;&#23450;&#25968;&#25454;&#38598;&#23545;&#40784;&#12290;&#23613;&#31649;&#37327;&#23376;&#26680;&#23545;&#40784;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#37117;&#24517;&#39035;&#26500;&#24314;&#23436;&#25972;&#30340;&#26680;&#30697;&#38453;&#65292;&#22240;&#27492;&#19968;&#30452;&#21463;&#21040;&#26174;&#33879;&#30340;&#35757;&#32451;&#25104;&#26412;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26088;&#22312;&#24179;&#34913;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23376;&#37319;&#26679;&#35757;&#32451;&#26041;&#27861;&#65292;&#27599;&#27425;&#35757;&#32451;&#27493;&#39588;&#20351;&#29992;&#26680;&#30697;&#38453;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35757;&#32451;&#30340;&#24635;&#20307;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#23376;&#37319;&#26679;&#26041;&#27861;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#30340;&#20083;&#33146;&#30284;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#22312;&#32500;&#25345;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#25152;&#38656;&#35757;&#32451;&#37327;&#23376;&#26680;&#30340;&#30005;&#36335;&#25968;&#37327;&#22823;&#22823;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning with quantum kernels for classification problems is a growing area of research. Recently, quantum kernel alignment techniques that parameterise the kernel have been developed, allowing the kernel to be trained and therefore aligned with a specific dataset. While quantum kernel alignment is a promising technique, it has been hampered by considerable training costs because the full kernel matrix must be constructed at every training iteration. Addressing this challenge, we introduce a novel method that seeks to balance efficiency and performance. We present a sub-sampling training approach that uses a subset of the kernel matrix at each training step, thereby reducing the overall computational cost of the training. In this work, we apply the sub-sampling method to synthetic datasets and a real-world breast cancer dataset and demonstrate considerable reductions in the number of circuits required to train the quantum kernel while maintaining classification accuracy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02860</link><description>&lt;p&gt;
&#21464;&#21270;&#28382;&#21518;&#27169;&#24335;&#36319;&#38543;&#20851;&#31995;&#25512;&#29702;&#30340;&#26102;&#38388;&#24207;&#21015;&#30697;&#38453;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis. (arXiv:2401.02860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#36947;&#35841;&#36319;&#38543;&#35841;&#20197;&#21450;&#20182;&#20204;&#36319;&#38543;&#30340;&#27169;&#24335;&#26159;&#29702;&#35299;&#38598;&#20307;&#34892;&#20026;&#65288;&#22914;&#20154;&#32676;&#65292;&#40060;&#32676;&#25110;&#32929;&#24066;&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26102;&#38388;&#24207;&#21015;&#26159;&#29992;&#20110;&#33719;&#21462;&#36319;&#38543;&#20851;&#31995;&#27934;&#23519;&#30340;&#36164;&#28304;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36319;&#38543;&#27169;&#24335;&#25110;&#27169;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21457;&#29616;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36319;&#38543;&#27169;&#24335;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#26029;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#36319;&#38543;&#27169;&#24335;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#32034;&#27169;&#24335;&#65292;&#31216;&#20026;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26694;&#26550;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#22312;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#19968;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#32034;&#20986;&#20004;&#20301;&#27468;&#25163;&#30456;&#20114;&#36319;&#38543;&#21809;&#27468;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowing who follows whom and what patterns they are following are crucial steps to understand collective behaviors (e.g. a group of human, a school of fish, or a stock market). Time series is one of resources that can be used to get insight regarding following relations. However, the concept of following patterns or motifs and the solution to find them in time series are not obvious. In this work, we formalize a concept of following motifs between two time series and present a framework to infer following patterns between two time series. The framework utilizes one of efficient and scalable methods to retrieve motifs from time series called the Matrix Profile Method. We compare our proposed framework with several baselines. The framework performs better than baselines in the simulation datasets. In the dataset of sound recording, the framework is able to retrieve the following motifs within a pair of time series that two singers sing following each other. In the cryptocurrency dataset,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#26657;&#27491;&#26469;&#29983;&#25104;&#38750;&#24179;&#31283;&#32441;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#20462;&#25913;&#30340;&#21442;&#32771;&#32441;&#29702;&#32454;&#21270;&#20026;&#19968;&#31181;&#36830;&#36143;&#12289;&#26080;&#32541;&#30340;&#32441;&#29702;&#65292;&#24182;&#20445;&#30041;&#21442;&#32771;&#26679;&#26412;&#30340;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#23454;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#32441;&#29702;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#22312;&#32441;&#29702;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.02847</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#26657;&#27491;&#29983;&#25104;&#38750;&#24179;&#31283;&#32441;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Non-Stationary Textures using Self-Rectification. (arXiv:2401.02847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#26657;&#27491;&#26469;&#29983;&#25104;&#38750;&#24179;&#31283;&#32441;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#20462;&#25913;&#30340;&#21442;&#32771;&#32441;&#29702;&#32454;&#21270;&#20026;&#19968;&#31181;&#36830;&#36143;&#12289;&#26080;&#32541;&#30340;&#32441;&#29702;&#65292;&#24182;&#20445;&#30041;&#21442;&#32771;&#26679;&#26412;&#30340;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#23454;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#32441;&#29702;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#22312;&#32441;&#29702;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22522;&#20110;&#31034;&#20363;&#30340;&#38750;&#24179;&#31283;&#32441;&#29702;&#21512;&#25104;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#20462;&#25913;&#21442;&#32771;&#32441;&#29702;&#65292;&#24471;&#21040;&#21512;&#25104;&#30340;&#21021;&#22987;&#30446;&#26631;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#8220;&#33258;&#26657;&#27491;&#8221;&#33258;&#21160;&#23558;&#36825;&#20010;&#30446;&#26631;&#32454;&#21270;&#20026;&#19968;&#31181;&#36830;&#36143;&#12289;&#26080;&#32541;&#30340;&#32441;&#29702;&#65292;&#21516;&#26102;&#24544;&#23454;&#22320;&#20445;&#30041;&#20102;&#21442;&#32771;&#26679;&#26412;&#30340;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36880;&#28176;&#23558;&#21512;&#25104;&#32441;&#29702;&#19982;&#21442;&#32771;&#23545;&#40784;&#65292;&#30830;&#20445;&#20445;&#30041;&#25152;&#25552;&#20379;&#30446;&#26631;&#20013;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#32441;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#22312;&#32441;&#29702;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/xiaorongjun000/Self-Rectific&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of example-based non-stationary texture synthesis. We introduce a novel twostep approach wherein users first modify a reference texture using standard image editing tools, yielding an initial rough target for the synthesis. Subsequently, our proposed method, termed "self-rectification", automatically refines this target into a coherent, seamless texture, while faithfully preserving the distinct visual characteristics of the reference exemplar. Our method leverages a pre-trained diffusion network, and uses self-attention mechanisms, to gradually align the synthesized texture with the reference, ensuring the retention of the structures in the provided target. Through experimental validation, our approach exhibits exceptional proficiency in handling non-stationary textures, demonstrating significant advancements in texture synthesis when compared to existing state-of-the-art techniques. Code is available at https://github.com/xiaorongjun000/Self-Rectific
&lt;/p&gt;</description></item><item><title>&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#21040;2047&#24180;&#20026;50%&#12290;</title><link>http://arxiv.org/abs/2401.02843</link><description>&lt;p&gt;
&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Thousands of AI Authors on the Future of AI. (arXiv:2401.02843v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02843
&lt;/p&gt;
&lt;p&gt;
&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#21040;2047&#24180;&#20026;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#35843;&#26597;&#20013;&#65292;2778&#21517;&#22312;&#39030;&#32423;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20250;&#35758;&#19978;&#21457;&#34920;&#36807;&#35770;&#25991;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;AI&#36827;&#23637;&#30340;&#36895;&#24230;&#12289;&#39640;&#32423;AI&#31995;&#32479;&#30340;&#24615;&#36136;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#24635;&#20307;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#33267;&#23569;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#21487;&#20197;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22914;&#26524;&#31185;&#23398;&#25345;&#32493;&#19981;&#21463;&#24178;&#25200;&#65292;2027&#24180;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#65292;&#21040;2047&#24180;&#20026;50%&#12290;&#21518;&#32773;&#30340;&#20272;&#35745;&#27604;&#25105;&#20204;&#19968;&#24180;&#21069;&#36827;&#34892;&#30340;&#31867;&#20284;&#35843;&#26597;[Grace et al., 2022]&#25552;&#21069;&#20102;13&#24180;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20154;&#31867;&#32844;&#19994;&#23436;&#20840;&#21487;&#33258;&#21160;&#21270;&#30340;&#20960;&#29575;&#39044;&#35745;&#35201;&#21040;2037&#24180;&#36798;&#21040;10%&#65292;&#21040;2116&#24180;&#25165;&#36798;&#21040;50%&#65288;&#19982;2022&#24180;&#35843;&#26597;&#20013;&#30340;2164&#24180;&#30456;&#27604;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).  Most
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Deezer&#38899;&#20048;&#26381;&#21153;&#19978;&#20419;&#36827;&#26032;&#21457;&#24067;&#20869;&#23481;&#21457;&#29616;&#33021;&#21147;&#30340;&#26368;&#26032;&#20030;&#25514;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#25512;&#33616;&#12289;&#20919;&#21551;&#21160;&#23884;&#20837;&#21644;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31561;&#12290;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20030;&#25514;&#22312;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#21644;&#26032;&#21457;&#24067;&#20869;&#23481;&#26333;&#20809;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.02827</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#24320;&#22987;&#21543;&#65306;&#20419;&#36827;Deezer&#38899;&#20048;&#26381;&#21153;&#19978;&#26032;&#21457;&#24067;&#20869;&#23481;&#30340;&#21457;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Let's Get It Started: Fostering the Discoverability of New Releases on Deezer. (arXiv:2401.02827v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Deezer&#38899;&#20048;&#26381;&#21153;&#19978;&#20419;&#36827;&#26032;&#21457;&#24067;&#20869;&#23481;&#21457;&#29616;&#33021;&#21147;&#30340;&#26368;&#26032;&#20030;&#25514;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#25512;&#33616;&#12289;&#20919;&#21551;&#21160;&#23884;&#20837;&#21644;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31561;&#12290;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20030;&#25514;&#22312;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#21644;&#26032;&#21457;&#24067;&#20869;&#23481;&#26333;&#20809;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#26368;&#36817;&#22312;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;Deezer&#19978;&#20419;&#36827;&#26032;&#21457;&#24067;&#20869;&#23481;&#21457;&#29616;&#33021;&#21147;&#26041;&#38754;&#30340;&#20030;&#25514;&#12290;&#22312;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;&#26032;&#21457;&#24067;&#20869;&#23481;&#30340;&#25628;&#32034;&#21644;&#25512;&#33616;&#21151;&#33021;&#20043;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#20174;&#32534;&#36753;&#25512;&#33616;&#21521;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#36716;&#21464;&#65292;&#21253;&#25324;&#20351;&#29992;&#20919;&#21551;&#21160;&#23884;&#20837;&#21644;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#36716;&#21464;&#22312;&#25512;&#33616;&#36136;&#37327;&#21644;&#26032;&#21457;&#24067;&#20869;&#23481;&#30340;&#26333;&#20809;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our recent initiatives to foster the discoverability of new releases on the music streaming service Deezer. After introducing our search and recommendation features dedicated to new releases, we outline our shift from editorial to personalized release suggestions using cold start embeddings and contextual bandits. Backed by online experiments, we discuss the advantages of this shift in terms of recommendation quality and exposure of new releases on the service.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#32463;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#65292;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.02810</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#39640;&#39057;&#29575;&#21644;&#22810;&#23610;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#32463;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#65292;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs/PDEs&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#27714;&#35299;&#22120;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#24120;&#24120;&#23548;&#33268;&#35757;&#32451;&#22833;&#36133;&#12290;&#24403;&#35299;&#20915;&#39640;&#39057;&#29575;&#21644;&#22810;&#23610;&#24230;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#35757;&#32451;PINN&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#24320;&#22987;&#35757;&#32451;&#65292;&#24182;&#36880;&#28176;&#25509;&#36817;&#39640;&#39057;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural network (PINN) is a data-driven solver for partial and ordinary differential equations(ODEs/PDEs). It provides a unified framework to address both forward and inverse problems. However, the complexity of the objective function often leads to training failures. This issue is particularly prominent when solving high-frequency and multi-scale problems. We proposed using transfer learning to boost the robustness and convergence of training PINN, starting training from low-frequency problems and gradually approaching high-frequency problems. Through two case studies, we discovered that transfer learning can effectively train PINN to approximate solutions from low-frequency problems to high-frequency problems without increasing network parameters. Furthermore, it requires fewer data points and less training time. We elaborately described our training strategy, including optimizer selection, and suggested guidelines for using transfer learning to train neural networks 
&lt;/p&gt;</description></item><item><title>Credence&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22686;&#24378;&#30340;&#20002;&#24323;&#24335;&#32531;&#20914;&#21306;&#20849;&#20139;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#20013;&#24515;&#20132;&#25442;&#26426;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02801</link><description>&lt;p&gt;
Credence:&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#20013;&#24515;&#20132;&#25442;&#26426;&#32531;&#20914;&#21306;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Credence: Augmenting Datacenter Switch Buffer Sharing with ML Predictions. (arXiv:2401.02801v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02801
&lt;/p&gt;
&lt;p&gt;
Credence&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22686;&#24378;&#30340;&#20002;&#24323;&#24335;&#32531;&#20914;&#21306;&#20849;&#20139;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#20013;&#24515;&#20132;&#25442;&#26426;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20132;&#25442;&#26426;&#20013;&#30340;&#25968;&#25454;&#21253;&#32531;&#20914;&#21306;&#34987;&#20849;&#20139;&#22312;&#25152;&#26377;&#20132;&#25442;&#26426;&#31471;&#21475;&#19978;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#21534;&#21520;&#37327;&#12290;&#25968;&#25454;&#20013;&#24515;&#20132;&#25442;&#26426;&#32531;&#20914;&#21306;&#22823;&#23567;&#19981;&#26029;&#32553;&#23567;&#30340;&#36235;&#21183;&#20351;&#24471;&#32531;&#20914;&#21306;&#20849;&#20139;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#25104;&#20026;&#20851;&#38190;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#25991;&#29486;&#34920;&#26126;&#65292;&#25512;&#20986;&#24335;&#32531;&#20914;&#21306;&#20849;&#20139;&#31639;&#27861;&#30456;&#23545;&#20110;&#20002;&#24323;&#24335;&#31639;&#27861;&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#30828;&#20214;&#19981;&#25903;&#25345;&#25512;&#20986;&#25805;&#20316;&#65292;&#20132;&#25442;&#26426;&#26080;&#27861;&#20174;&#36825;&#20123;&#31639;&#27861;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22914;&#26524;&#26410;&#26469;&#25968;&#25454;&#21253;&#21040;&#36798;&#30340;&#26102;&#38388;&#20107;&#20808;&#30693;&#36947;&#65292;&#20002;&#24323;&#24335;&#32531;&#20914;&#21306;&#21487;&#20197;&#27169;&#25311;&#25512;&#20986;&#24335;&#32531;&#20914;&#21306;&#12290;&#36825;&#34920;&#26126;&#65292;&#23558;&#20851;&#20110;&#26410;&#26469;&#21040;&#36798;&#30340;&#39044;&#27979;&#19982;&#20002;&#24323;&#24335;&#31639;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#26159;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#39318;&#27425;&#30740;&#31350;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Credence&#65292;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22686;&#24378;&#30340;&#20002;&#24323;&#24335;&#32531;&#20914;&#21306;&#20849;&#20139;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Packet buffers in datacenter switches are shared across all the switch ports in order to improve the overall throughput. The trend of shrinking buffer sizes in datacenter switches makes buffer sharing extremely challenging and a critical performance issue. Literature suggests that push-out buffer sharing algorithms have significantly better performance guarantees compared to drop-tail algorithms. Unfortunately, switches are unable to benefit from these algorithms due to lack of support for push-out operations in hardware. Our key observation is that drop-tail buffers can emulate push-out buffers if the future packet arrivals are known ahead of time. This suggests that augmenting drop-tail algorithms with predictions about the future arrivals has the potential to significantly improve performance.  This paper is the first research attempt in this direction. We propose Credence, a drop-tail buffer sharing algorithm augmented with machine-learned predictions. Credence can unlock the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#21019;&#25163;&#26415;&#35270;&#39057;&#20013;&#24369;&#21322;&#30417;&#30563;&#19979;&#26816;&#27979;&#25163;&#26415;&#24037;&#20855;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#29616;&#25439;&#22833;&#26469;&#21033;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#20013;&#24037;&#20855;&#23545;&#20043;&#38388;&#30340;&#20849;&#29616;&#20851;&#31995;&#65292;&#24179;&#34913;&#20102;&#27880;&#37322;&#36127;&#25285;&#21644;&#26816;&#27979;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#20998;&#31867;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.02791</link><description>&lt;p&gt;
&#24369;&#21322;&#30417;&#30563;&#19979;&#30340;&#24494;&#21019;&#25163;&#26415;&#35270;&#39057;&#24037;&#20855;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos. (arXiv:2401.02791v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#21019;&#25163;&#26415;&#35270;&#39057;&#20013;&#24369;&#21322;&#30417;&#30563;&#19979;&#26816;&#27979;&#25163;&#26415;&#24037;&#20855;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#29616;&#25439;&#22833;&#26469;&#21033;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#20013;&#24037;&#20855;&#23545;&#20043;&#38388;&#30340;&#20849;&#29616;&#20851;&#31995;&#65292;&#24179;&#34913;&#20102;&#27880;&#37322;&#36127;&#25285;&#21644;&#26816;&#27979;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#20998;&#31867;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#24037;&#20855;&#26816;&#27979;&#23545;&#20110;&#20998;&#26512;&#21644;&#35780;&#20272;&#24494;&#21019;&#25163;&#26415;&#35270;&#39057;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#38656;&#35201;&#22823;&#37327;&#23436;&#25972;&#30340;&#23454;&#20363;&#32423;&#26631;&#31614;&#65288;&#21363;&#36793;&#30028;&#26694;&#65289;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#36127;&#25285;&#65292;&#20855;&#26377;&#23454;&#20363;&#32423;&#26631;&#31614;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#25552;&#20379;&#22270;&#20687;&#32423;&#26631;&#31614;&#32780;&#19981;&#26159;&#23454;&#20363;&#32423;&#26631;&#31614;&#26102;&#65292;&#25163;&#26415;&#24037;&#20855;&#26816;&#27979;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#22270;&#20687;&#32423;&#27880;&#37322;&#27604;&#23454;&#20363;&#32423;&#27880;&#37322;&#26356;&#20855;&#26102;&#38388;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#26497;&#39640;&#30340;&#27880;&#37322;&#36127;&#25285;&#21644;&#26816;&#27979;&#24615;&#33021;&#20043;&#38388;&#23547;&#27714;&#24179;&#34913;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#29616;&#25439;&#22833;&#65292;&#35813;&#25439;&#22833;&#32771;&#34385;&#20102;&#26576;&#20123;&#24037;&#20855;&#23545;&#22312;&#22270;&#20687;&#20013;&#32463;&#24120;&#20849;&#21516;&#20986;&#29616;&#30340;&#29305;&#24615;&#65292;&#20197;&#21033;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#12290;&#29992;&#20849;&#29616;&#25439;&#22833;&#23545;&#20849;&#29616;&#20851;&#31995;&#30340;&#30693;&#35782;&#36827;&#34892;&#23553;&#35013;&#26377;&#21161;&#20110;&#20811;&#26381;&#20998;&#31867;&#22256;&#38590;&#65292;&#22240;&#20026;&#19968;&#20123;&#25163;&#26415;&#24037;&#20855;&#30340;&#20998;&#31867;&#22256;&#38590;&#28304;&#20110;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#23427;&#20204;&#32463;&#24120;&#20197;&#25104;&#23545;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical tool detection is essential for analyzing and evaluating minimally invasive surgery videos. Current approaches are mostly based on supervised methods that require large, fully instance-level labels (i.e., bounding boxes). However, large image datasets with instance-level labels are often limited because of the burden of annotation. Thus, surgical tool detection is important when providing image-level labels instead of instance-level labels since image-level annotations are considerably more time-efficient than instance-level annotations. In this work, we propose to strike a balance between the extremely costly annotation burden and detection performance. We further propose a co-occurrence loss, which considers a characteristic that some tool pairs often co-occur together in an image to leverage image-level labels. Encapsulating the knowledge of co-occurrence using the co-occurrence loss helps to overcome the difficulty in classification that originates from the fact that some 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23039;&#21183;&#35782;&#21035;&#20013;&#30005;&#26497;&#28418;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;HD-EMG&#30005;&#26497;&#23376;&#38598;&#65292;&#24182;&#22686;&#21152;&#26469;&#33258;&#19981;&#21516;&#30005;&#26497;&#20301;&#32622;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02773</link><description>&lt;p&gt;
&#36890;&#36807;HD-EMG&#30005;&#26497;&#23376;&#38598;&#35299;&#20915;&#23039;&#21183;&#35782;&#21035;&#20013;&#30340;&#30005;&#26497;&#28418;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets. (arXiv:2401.02773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23039;&#21183;&#35782;&#21035;&#20013;&#30005;&#26497;&#28418;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;HD-EMG&#30005;&#26497;&#23376;&#38598;&#65292;&#24182;&#22686;&#21152;&#26469;&#33258;&#19981;&#21516;&#30005;&#26497;&#20301;&#32622;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
sEMG&#27169;&#24335;&#35782;&#21035;&#31639;&#27861;&#22312;&#35299;&#30721;&#36816;&#21160;&#24847;&#22270;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24050;&#30693;&#23545;&#20110;&#19981;&#26029;&#21464;&#21270;&#30340;&#35760;&#24405;&#26465;&#20214;&#38750;&#24120;&#33030;&#24369;&#65292;&#24615;&#33021;&#22312;&#19981;&#21516;&#21463;&#35797;&#32773;&#21644;&#19981;&#21516;&#20250;&#35805;&#20043;&#38388;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#22810;&#36890;&#36947;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;&#20063;&#31216;&#20026;&#39640;&#23494;&#24230;sEMG&#65289;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#30005;&#26497;&#25910;&#38598;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#30005;&#26497;&#25918;&#32622;&#31561;&#21464;&#37327;&#26469;&#28304;&#30340;&#22256;&#38590;&#65292;&#32570;&#20047;&#31283;&#20581;&#24615;&#19968;&#30452;&#23384;&#22312;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#32452;&#36755;&#20837;&#36890;&#36947;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26469;&#33258;&#19981;&#21516;&#30005;&#26497;&#20301;&#32622;&#30340;&#25968;&#25454;&#22686;&#24378;&#25105;&#20204;&#30340;&#35757;&#32451;&#20998;&#24067;&#65292;&#20174;&#32780;&#21516;&#26102;&#35299;&#20915;&#30005;&#26497;&#28418;&#31227;&#38382;&#39064;&#21644;&#38477;&#20302;&#36755;&#20837;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#23545;&#30005;&#26497;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#21463;&#35797;&#32773;&#21644;&#20998;&#31867;&#31639;&#27861;&#38388;&#26174;&#33879;&#25552;&#39640;&#20102;&#20250;&#35805;&#38388;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
sEMG pattern recognition algorithms have been explored extensively in decoding movement intent, yet are known to be vulnerable to changing recording conditions, exhibiting significant drops in performance across subjects, and even across sessions. Multi-channel surface EMG, also referred to as high-density sEMG (HD-sEMG) systems, have been used to improve performance with the information collected through the use of additional electrodes. However, a lack of robustness is ever present due to limited datasets and the difficulties in addressing sources of variability, such as electrode placement. In this study, we propose training on a collection of input channel subsets and augmenting our training distribution with data from different electrode locations, simultaneously targeting electrode shift and reducing input dimensionality. Our method increases robustness against electrode shift and results in significantly higher intersession performance across subjects and classification algorith
&lt;/p&gt;</description></item><item><title>Powerformer&#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#12290;&#23427;&#36890;&#36807;&#24320;&#21457;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#20379;&#26356;&#21152;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02771</link><description>&lt;p&gt;
Powerformer&#65306;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#29992;&#20110;&#30005;&#21147;&#27969;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Powerformer: A Section-adaptive Transformer for Power Flow Adjustment. (arXiv:2401.02771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02771
&lt;/p&gt;
&lt;p&gt;
Powerformer&#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#12290;&#23427;&#36890;&#36807;&#24320;&#21457;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#20379;&#26356;&#21152;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#20248;&#21270;&#36328;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#30005;&#21147;&#35843;&#24230;&#20197;&#36827;&#34892;&#30005;&#21147;&#27969;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;Powerformer&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#19982;&#20256;&#32479;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#20998;&#31163;&#24320;&#26469;&#12290;&#35813;&#26426;&#21046;&#26377;&#25928;&#22320;&#23558;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#19982;&#20256;&#36755;&#21306;&#27573;&#20449;&#24687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#30005;&#21147;&#31995;&#32479;&#30340;&#22270;&#25299;&#25169;&#21644;&#27597;&#32447;&#33410;&#28857;&#30340;&#30005;&#27668;&#23646;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#23450;&#21046;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#65288;&#21253;&#25324;IEEE 118&#33410;&#28857;&#31995;&#32479;&#12289;&#20013;&#22269;&#23454;&#38469;300&#33410;&#28857;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#22411;&#31995;&#32479;&#65289;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel transformer architecture tailored for learning robust power system state representations, which strives to optimize power dispatch for the power flow adjustment across different transmission sections. Specifically, our proposed approach, named Powerformer, develops a dedicated section-adaptive attention mechanism, separating itself from the self-attention used in conventional transformers. This mechanism effectively integrates power system states with transmission section information, which facilitates the development of robust state representations. Furthermore, by considering the graph topology of power system and the electrical attributes of bus nodes, we introduce two customized strategies to further enhance the expressiveness: graph neural network propagation and multi-factor attention mechanism. Extensive evaluations are conducted on three power system scenarios, including the IEEE 118-bus system, a realistic 300-bus system in China, and a large-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.02740</link><description>&lt;p&gt;
&#20026;&#22810;&#20219;&#21153;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20844;&#24179;&#24615;&#24863;&#30693;&#30340;&#20316;&#19994;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25935;&#24863;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22404;&#26029;&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#21333;&#20010;FL&#26381;&#21153;&#22120;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;FL&#23458;&#25143;&#31471;&#26469;&#26356;&#26032;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#23454;&#38469;&#19978;&#65292;&#21487;&#33021;&#20250;&#26377;&#22810;&#20010;FL&#26381;&#21153;&#22120;&#21516;&#26102;&#23581;&#35797;&#20174;&#21516;&#19968;&#20010;&#27744;&#20013;&#36873;&#25321;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#22522;&#20110;Lyapunov&#20248;&#21270;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#24403;&#21069;&#38656;&#27714;&#21644;&#20316;&#19994;&#20184;&#27454;&#20986;&#20215;&#65292;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#20197;&#38450;&#27490;&#31561;&#24453;&#26102;&#38388;&#36807;&#38271;&#12290;&#22522;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;FairFedJS&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;&#23427;&#22312;&#24179;&#22343;&#19978;&#20987;&#36133;&#20102;&#26368;&#20339;&#22522;&#20934;&#32447;31.9%&#21644;1.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21453;&#36716;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#20860;&#23481;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#22312;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.02739</link><description>&lt;p&gt;
&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65306;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;
&lt;/p&gt;
&lt;p&gt;
Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21453;&#36716;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#20860;&#23481;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#22312;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#65292;&#19968;&#31181;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36817;&#20284;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36741;&#21161;&#28508;&#21464;&#37327;&#22686;&#21152;&#20102;&#21464;&#20998;&#21518;&#39564;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#34920;&#36798;&#24615;&#30340;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#21453;&#36716;&#29992;&#25143;&#25351;&#23450;&#30340;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#25105;&#20204;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#21463;&#21040;&#35273;&#37266;-&#30561;&#30496;&#31639;&#27861;&#21551;&#21457;&#30340;&#36793;&#38469;&#20284;&#28982;&#26032;&#19979;&#30028;&#26469;&#25311;&#21512;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65288;&#23427;&#36866;&#37197;&#20102;&#27491;&#21017;&#21270;&#30340;ELBO&#25193;&#23637;&#65289;&#65292;&#19982;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#20860;&#23481;&#65292;&#24182;&#19988;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#25110;&#23545;&#25239;&#32593;&#32476;&#30340;&#26367;&#20195;&#36817;&#20284;&#21518;&#39564;&#31867;&#21035;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;DD-VAE&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#23398;&#20013;&#30340;&#19968;&#20010;&#28608;&#21169;&#20219;&#21153; -- &#20174;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#25512;&#26029;&#28508;&#22312;&#34880;&#32479; -- &#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose denoising diffusion variational inference (DDVI), an approximate inference algorithm for latent variable models which relies on diffusion models as expressive variational posteriors. Our method augments variational posteriors with auxiliary latents, which yields an expressive class of models that perform diffusion in latent space by reversing a user-specified noising process. We fit these models by optimizing a novel lower bound on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. When applied to deep latent variable models, our method yields the denoising diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in biology -- inferring latent ancestry from human genomes -- outperforming strong baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;AD&#20960;&#20046;&#22312;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65292;&#38656;&#35201;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#36873;&#25321;&#30340;&#38750;&#24179;&#28369;MaxPool&#38597;&#21487;&#27604;&#30697;&#38453;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20998;&#27495;&#21306;&#21644;&#34917;&#20607;&#21306;&#20004;&#20010;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.02736</link><description>&lt;p&gt;
&#20851;&#20110;&#38750;&#24179;&#28369;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65306;MaxPool&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the numerical reliability of nonsmooth autodiff: a MaxPool case study. (arXiv:2401.02736v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;AD&#20960;&#20046;&#22312;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65292;&#38656;&#35201;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#36873;&#25321;&#30340;&#38750;&#24179;&#28369;MaxPool&#38597;&#21487;&#27604;&#30697;&#38453;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20998;&#27495;&#21306;&#21644;&#34917;&#20607;&#21306;&#20004;&#20010;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#31934;&#24230;&#32423;&#21035;&#65288;16&#20301;&#12289;32&#20301;&#12289;64&#20301;&#65289;&#21644;&#21367;&#31215;&#26550;&#26500;&#65288;LeNet&#12289;VGG&#21644;ResNet&#65289;&#20197;&#21450;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR10&#12289;SVHN&#21644;ImageNet&#65289;&#19978;&#30340;AD&#34892;&#20026;&#12290;&#23613;&#31649;AD&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#20960;&#20046;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#65288;&#22914;MaxPool&#21644;ReLU&#65289;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65288;&#32780;&#19981;&#26159;&#23454;&#25968;&#65289;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;AD&#21487;&#33021;&#22312;&#25968;&#20540;&#19978;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;&#36825;&#20123;&#23376;&#38598;&#21253;&#25324;&#20998;&#27495;&#21306;&#65288;AD&#22312;&#23454;&#25968;&#19978;&#19981;&#27491;&#30830;&#65289;&#21644;&#34917;&#20607;&#21306;&#65288;AD&#22312;&#28014;&#28857;&#25968;&#19978;&#19981;&#27491;&#30830;&#20294;&#22312;&#23454;&#25968;&#19978;&#27491;&#30830;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;SGD&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#30740;&#31350;&#20102;MaxPool&#38750;&#24179;&#28369;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#19981;&#21516;&#36873;&#25321;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the reliability of automatic differentiation (AD) for neural networks involving the nonsmooth MaxPool operation. We investigate the behavior of AD across different precision levels (16, 32, 64 bits) and convolutional architectures (LeNet, VGG, and ResNet) on various datasets (MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent research has shown that it coincides with the derivative almost everywhere, even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the other hand, in practice, AD operates with floating-point numbers (not real numbers), and there is, therefore, a need to explore subsets on which AD can be numerically incorrect. These subsets include a bifurcation zone (where AD is incorrect over reals) and a compensation zone (where AD is incorrect over floating-point numbers but correct over reals). Using SGD for the training process, we study the impact of different choices of the nonsmooth Jacobian for the MaxPool
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#20027;&#21160;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#20803;&#21521;&#37327;&#20540;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#25805;&#32437;&#26799;&#24230;&#25110;&#35745;&#31639;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPD&#32423;&#21035;&#30340;&#26041;&#27861;&#27604;&#26799;&#24230;&#32423;&#21035;&#30340;&#26041;&#27861;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#27491;&#24577;&#20998;&#24067;&#24773;&#20917;&#19979;&#34920;&#29616;&#25509;&#36817;&#21521;&#37327;&#20540;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02735</link><description>&lt;p&gt;
&#20849;&#20139;&#20027;&#21160;&#23376;&#31354;&#38388;&#29992;&#20110;&#22810;&#20803;&#21521;&#37327;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Shared active subspace for multivariate vector-valued functions. (arXiv:2401.02735v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#20027;&#21160;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#20803;&#21521;&#37327;&#20540;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#25805;&#32437;&#26799;&#24230;&#25110;&#35745;&#31639;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPD&#32423;&#21035;&#30340;&#26041;&#27861;&#27604;&#26799;&#24230;&#32423;&#21035;&#30340;&#26041;&#27861;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#27491;&#24577;&#20998;&#24067;&#24773;&#20917;&#19979;&#34920;&#29616;&#25509;&#36817;&#21521;&#37327;&#20540;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#20316;&#20026;&#22522;&#32447;&#26469;&#35745;&#31639;&#22810;&#20803;&#21521;&#37327;&#20540;&#20989;&#25968;&#30340;&#20849;&#20139;&#20027;&#21160;&#23376;&#31354;&#38388;&#12290;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21407;&#22987;&#31354;&#38388;&#19978;&#30340;&#20989;&#25968;&#35780;&#20272;&#19982;&#37325;&#26500;&#31354;&#38388;&#19978;&#30340;&#20989;&#25968;&#35780;&#20272;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#25805;&#32437;&#26799;&#24230;&#25110;&#20174;&#27599;&#20010;&#20998;&#37327;&#20989;&#25968;&#30340;&#26799;&#24230;&#35745;&#31639;&#30340;&#23545;&#31216;&#27491;&#23450;&#65288;&#21322;&#27491;&#23450;&#65289;&#30697;&#38453;&#26469;&#33719;&#24471;&#25152;&#26377;&#20998;&#37327;&#20989;&#25968;&#30340;&#20849;&#21516;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;&#21482;&#36866;&#29992;&#20110;&#27491;&#24577;&#20998;&#24067;&#30340;&#21521;&#37327;&#20540;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#26080;&#35770;&#20854;&#28508;&#22312;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#20248;&#21270;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292; SPD&#32423;&#21035;&#30340;&#26041;&#27861;&#20248;&#20110;&#26799;&#24230;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27491;&#24577;&#20998;&#24067;&#24773;&#20917;&#19979;&#25509;&#36817;&#21521;&#37327;&#20540;&#26041;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#21462;SPD&#30697;&#38453;&#20043;&#21644;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes several approaches as baselines to compute a shared active subspace for multivariate vector-valued functions. The goal is to minimize the deviation between the function evaluations on the original space and those on the reconstructed one. This is done either by manipulating the gradients or the symmetric positive (semi-)definite (SPD) matrices computed from the gradients of each component function so as to get a single structure common to all component functions. These approaches can be applied to any data irrespective of the underlying distribution unlike the existing vector-valued approach that is constrained to a normal distribution. We test the effectiveness of these methods on five optimization problems. The experiments show that, in general, the SPD-level methods are superior to the gradient-level ones, and are close to the vector-valued approach in the case of a normal distribution. Interestingly, in most cases it suffices to take the sum of the SPD matrices 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNS&#30340;&#24555;&#36895;&#32472;&#22270;&#29275;&#39039;&#22411;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#29275;&#39039;&#22411;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#36890;&#20449;&#22797;&#26434;&#24230;&#19978;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20256;&#36755;&#33609;&#22270;&#21270;&#30340;&#24179;&#26041;&#26681;&#28023;&#26862;&#30697;&#38453;&#26469;&#36817;&#20284;&#38598;&#20013;&#24335;&#29275;&#39039;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02734</link><description>&lt;p&gt;
FedNS:&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#24555;&#36895;&#32472;&#22270;&#29275;&#39039;&#22411;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedNS: A Fast Sketching Newton-Type Algorithm for Federated Learning. (arXiv:2401.02734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNS&#30340;&#24555;&#36895;&#32472;&#22270;&#29275;&#39039;&#22411;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#29275;&#39039;&#22411;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#36890;&#20449;&#22797;&#26434;&#24230;&#19978;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20256;&#36755;&#33609;&#22270;&#21270;&#30340;&#24179;&#26041;&#26681;&#28023;&#26862;&#30697;&#38453;&#26469;&#36817;&#20284;&#38598;&#20013;&#24335;&#29275;&#39039;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29275;&#39039;&#22411;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#19982;&#36890;&#20449;&#36718;&#25968;&#25104;&#32447;&#24615;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20108;&#27425;&#36890;&#20449;&#22797;&#26434;&#24615;&#65292;&#36890;&#20449;&#28023;&#26862;&#30697;&#38453;&#36890;&#24120;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#28982;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;&#32852;&#37030;&#29275;&#39039;&#32472;&#22270;&#26041;&#27861;(FedNS)&#65292;&#36890;&#36807;&#20256;&#36755;&#33609;&#22270;&#21270;&#30340;&#24179;&#26041;&#26681;&#28023;&#26862;&#30697;&#38453;&#32780;&#19981;&#26159;&#31934;&#30830;&#30340;&#28023;&#26862;&#30697;&#38453;&#65292;&#26469;&#36817;&#20284;&#38598;&#20013;&#24335;&#29275;&#39039;&#26041;&#27861;&#12290;&#20026;&#20102;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#65292;&#25105;&#20204;&#23558;&#33609;&#22270;&#30340;&#22823;&#23567;&#32553;&#23567;&#21040;&#19982;&#28023;&#26862;&#30697;&#38453;&#30340;&#26377;&#25928;&#32500;&#24230;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#22522;&#20110;&#32479;&#35745;&#23398;&#20064;&#25552;&#20379;&#20102;&#32852;&#37030;&#29275;&#39039;&#32472;&#22270;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#27425;&#22312;&#36890;&#20449;&#36718;&#25968;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#36229;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#19982;&#25105;&#20204;&#30340;&#29702;&#35770;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Newton-type federated learning algorithms have demonstrated linear convergence with respect to the communication rounds. However, communicating Hessian matrices is often unfeasible due to their quadratic communication complexity. In this paper, we introduce a novel approach to tackle this issue while still achieving fast convergence rates. Our proposed method, named as Federated Newton Sketch methods (FedNS), approximates the centralized Newton's method by communicating the sketched square-root Hessian instead of the exact Hessian. To enhance communication efficiency, we reduce the sketch size to match the effective dimension of the Hessian matrix. We provide convergence analysis based on statistical learning for the federated Newton sketch approaches. Specifically, our approaches reach super-linear convergence rates w.r.t. the communication rounds for the first time. We validate the effectiveness of our algorithms through various experiments, which coincide with our theoretical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLAGCN&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24322;&#27493;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#23454;&#26102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02723</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#24322;&#27493;&#35745;&#31639;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20132;&#36890;&#27969;&#37327;
&lt;/p&gt;
&lt;p&gt;
Predicting Traffic Flow with Federated Learning and Graph Neural with Asynchronous Computations Network. (arXiv:2401.02723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLAGCN&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24322;&#27493;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#23454;&#26102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#21644;&#24322;&#27493;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;FLAGCN&#65289;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#24322;&#27493;&#22270;&#21367;&#31215;&#32593;&#32476;&#21407;&#29702;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#23454;&#26102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;FLAGCN&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#31354;&#26102;&#22270;&#21367;&#31215;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24322;&#27493;&#22788;&#29702;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#39640;&#25928;&#22788;&#29702;&#19982;&#35813;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;GraphFL&#30340;&#22270;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#20419;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Real-time traffic flow prediction holds significant importance within the domain of Intelligent Transportation Systems (ITS). The task of achieving a balance between prediction precision and computational efficiency presents a significant challenge. In this article, we present a novel deep-learning method called Federated Learning and Asynchronous Graph Convolutional Network (FLAGCN). Our framework incorporates the principles of asynchronous graph convolutional networks with federated learning to enhance the accuracy and efficiency of real-time traffic flow prediction. The FLAGCN model employs a spatial-temporal graph convolution technique to asynchronously address spatio-temporal dependencies within traffic data effectively. To efficiently handle the computational requirements associated with this deep learning model, this study used a graph federated learning technique known as GraphFL. This approach is designed to facilitate the training process. The experimental results obtained fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.02721</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;ODE&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE. (arXiv:2401.02721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#20852;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#24050;&#32463;&#34987;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;CNN&#21644;RNN&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#27880;&#24847;&#26426;&#21046;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#32452;&#20214;&#65292;&#20294;&#26159;&#35768;&#22810;Transformer&#27169;&#22411;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#30456;&#27604;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#24182;&#23558;&#37096;&#20998;&#21367;&#31215;&#23618;&#26367;&#25442;&#20026;MHSA&#65288;&#22810;&#22836;&#33258;&#27880;&#24847;&#65289;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#65288;&#24120;&#24494;&#20998;&#26041;&#31243;&#65289;&#32780;&#19981;&#26159;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36825;&#31181;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#65292;&#32780;&#19988;&#27809;&#26377;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37096;&#32626;&#22312;&#19968;&#21488;&#36866;&#24230;&#35268;&#27169;&#30340;FPGA&#35774;&#22791;&#19978;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is an emerging neural network model with attention mechanism. It has been adopted to various tasks and achieved a favorable accuracy compared to CNNs and RNNs. While the attention mechanism is recognized as a general-purpose component, many of the Transformer models require a significant number of parameters compared to the CNN-based ones. To mitigate the computational complexity, recently, a hybrid approach has been proposed, which uses ResNet as a backbone architecture and replaces a part of its convolution layers with an MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly reduce the parameter size of such models by using Neural ODE (Ordinary Differential Equation) as a backbone architecture instead of ResNet. The proposed hybrid model reduces the parameter size by 94.6% compared to the CNN-based ones without degrading the accuracy. We then deploy the proposed model on a modest-sized FPGA device for edge computing. To further reduce FPGA resource u
&lt;/p&gt;</description></item><item><title>&#26657;&#20934;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#32452;&#32455;&#25915;&#20987;&#26469;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#32622;&#20449;&#20998;&#25968;&#30340;&#20915;&#31574;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65292;&#24182;&#23545;&#24120;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.02718</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#26657;&#20934;&#25915;&#20987;&#65306;&#38024;&#23545;&#26657;&#20934;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration. (arXiv:2401.02718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02718
&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#32452;&#32455;&#25915;&#20987;&#26469;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#32622;&#20449;&#20998;&#25968;&#30340;&#20915;&#31574;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65292;&#24182;&#23545;&#24120;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#26657;&#20934;&#25915;&#20987;&#30340;&#26032;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#20854;&#20013;&#25915;&#20987;&#34987;&#29983;&#25104;&#21644;&#32452;&#32455;&#20197;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20005;&#37325;&#21361;&#21450;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#20854;&#32622;&#20449;&#20998;&#25968;&#30340;&#20219;&#20309;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#31181;&#26032;&#22411;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65306;&#20302;&#32622;&#20449;&#25915;&#20987;&#12289;&#39640;&#32622;&#20449;&#25915;&#20987;&#12289;&#26368;&#22823;&#22833;&#30495;&#25915;&#20987;&#21644;&#38543;&#26426;&#32622;&#20449;&#25915;&#20987;&#65292;&#36866;&#29992;&#20110;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#23545;&#20856;&#22411;&#30340;&#21463;&#23475;&#27169;&#22411;&#36827;&#34892;&#20102;&#36825;&#20123;&#26032;&#22411;&#25915;&#20987;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#21363;&#20351;&#21482;&#36827;&#34892;&#30456;&#23545;&#36739;&#23569;&#30340;&#26597;&#35810;&#65292;&#25915;&#20987;&#20063;&#33021;&#36896;&#25104;&#37325;&#22823;&#30340;&#26657;&#20934;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#20197;&#20102;&#35299;&#26657;&#20934;&#25915;&#20987;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#23545;&#36825;&#20123;&#25915;&#20987;&#31867;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new framework of adversarial attacks, named calibration attacks, in which the attacks are generated and organized to trap victim models to be miscalibrated without altering their original accuracy, hence seriously endangering the trustworthiness of the models and any decision-making based on their confidence scores. Specifically, we identify four novel forms of calibration attacks: underconfidence attacks, overconfidence attacks, maximum miscalibration attacks, and random confidence attacks, in both the black-box and white-box setups. We then test these new attacks on typical victim models with comprehensive datasets, demonstrating that even with a relatively low number of queries, the attacks can create significant calibration mistakes. We further provide detailed analyses to understand different aspects of calibration attacks. Building on that, we investigate the effectiveness of widely used adversarial defences and calibration methods against these types of attacks, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#30693;&#35782;&#25552;&#28860;&#65288;SKR&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35299;&#20915;&#20102;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.02713</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#30693;&#35782;&#25552;&#28860;&#36827;&#34892;&#22270;&#32423;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-level Protein Representation Learning by Structure Knowledge Refinement. (arXiv:2401.02713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#30693;&#35782;&#25552;&#28860;&#65288;SKR&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35299;&#20915;&#20102;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20197;&#38750;&#30417;&#30563;&#26041;&#24335;&#22312;&#25972;&#20010;&#22270;&#32423;&#21035;&#19978;&#23398;&#20064;&#34920;&#31034;&#12290;&#23398;&#20064;&#22270;&#32423;&#34920;&#31034;&#22312;&#35832;&#22914;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#29305;&#24449;&#25552;&#21462;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#31561;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20419;&#36827;&#22270;&#29305;&#24449;&#25552;&#21462;&#65292;&#31216;&#20026;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#12290;&#23613;&#31649;GCL&#26377;&#25928;&#65292;&#20294;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20123;&#22797;&#26434;&#38382;&#39064;&#65292;&#27604;&#22914;&#34394;&#20551;&#36127;&#26679;&#26412;&#23545;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;GCL&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#23545;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#36866;&#24212;&#24615;&#36739;&#24369;&#12290;&#21463;&#21040;&#36825;&#20123;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#32467;&#26500;&#30693;&#35782;&#25552;&#28860;&#65288;SKR&#65289;&#65292;&#23427;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#30830;&#23450;&#19968;&#23545;&#26679;&#26412;&#26159;&#27491;&#26679;&#26412;&#36824;&#26159;&#36127;&#26679;&#26412;&#30340;&#27010;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#31574;&#30053;&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#24182;&#19988;&#19982;&#25105;&#20204;&#30340;SKR&#26694;&#26550;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on learning representation on the whole graph level in an unsupervised manner. Learning graph-level representation plays an important role in a variety of real-world issues such as molecule property prediction, protein structure feature extraction, and social network analysis. The mainstream method is utilizing contrastive learning to facilitate graph feature extraction, known as Graph Contrastive Learning (GCL). GCL, although effective, suffers from some complications in contrastive learning, such as the effect of false negative pairs. Moreover, augmentation strategies in GCL are weakly adaptive to diverse graph datasets. Motivated by these problems, we propose a novel framework called Structure Knowledge Refinement (SKR) which uses data structure to determine the probability of whether a pair is positive or negative. Meanwhile, we propose an augmentation strategy that naturally preserves the semantic meaning of the original data and is compatible with our SKR frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#26102;&#38388;&#30340;&#19977;&#20803;&#32452;&#22352;&#26631;&#25439;&#22833;&#20989;&#25968;TripleSurv&#65292;&#36890;&#36807;&#24341;&#20837;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#29983;&#23384;&#26102;&#38388;&#24046;&#24322;&#26469;&#40723;&#21169;&#27169;&#22411;&#37327;&#21270;&#25490;&#21517;&#30456;&#23545;&#39118;&#38505;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#23384;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02708</link><description>&lt;p&gt;
TripleSurv&#65306;&#36866;&#24212;&#26102;&#38388;&#19977;&#20803;&#32452;&#22352;&#26631;&#25439;&#22833;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis. (arXiv:2401.02708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#26102;&#38388;&#30340;&#19977;&#20803;&#32452;&#22352;&#26631;&#25439;&#22833;&#20989;&#25968;TripleSurv&#65292;&#36890;&#36807;&#24341;&#20837;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#29983;&#23384;&#26102;&#38388;&#24046;&#24322;&#26469;&#40723;&#21169;&#27169;&#22411;&#37327;&#21270;&#25490;&#21517;&#30456;&#23545;&#39118;&#38505;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#23384;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#23545;&#34987;&#25130;&#23614;&#30340;&#20107;&#20214;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#20854;&#20013;&#24863;&#20852;&#36259;&#30340;&#20107;&#20214;&#21487;&#33021;&#26159;&#27515;&#20129;&#12289;&#22833;&#36133;&#25110;&#29305;&#23450;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25490;&#24207;&#25439;&#22833;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#25439;&#22833;&#20989;&#25968;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#25490;&#24207;&#25439;&#22833;&#20165;&#20851;&#27880;&#29983;&#23384;&#26102;&#38388;&#25490;&#21517;&#65292;&#19981;&#32771;&#34385;&#26679;&#26412;&#23545;&#20110;&#30830;&#20999;&#29983;&#23384;&#26102;&#38388;&#20540;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;MLE&#26159;&#26080;&#30028;&#30340;&#19988;&#23481;&#26131;&#21463;&#21040;&#24322;&#24120;&#20540;&#65288;&#20363;&#22914;&#65292;&#34987;&#25130;&#23614;&#25968;&#25454;&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24314;&#27169;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#22788;&#29702;&#23398;&#20064;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#24182;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#29983;&#23384;&#26102;&#38388;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#26102;&#38388;&#19977;&#20803;&#32452;&#22352;&#26631;&#25439;&#22833;&#20989;&#25968;TripleSurv&#65292;&#36890;&#36807;&#23558;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#29983;&#23384;&#26102;&#38388;&#24046;&#24322;&#24341;&#20837;&#25490;&#24207;&#20013;&#65292;&#20197;&#40723;&#21169;&#27169;&#22411;&#37327;&#21270;&#25490;&#21517;&#30456;&#23545;&#39118;&#38505;&#65292;&#26368;&#32456;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core challenge in survival analysis is to model the distribution of censored time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. Previous studies have showed that ranking and maximum likelihood estimation (MLE)loss functions are widely-used for survival analysis. However, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples for exact survival time values. Furthermore, the MLE is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. To handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, TripleSurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;SAR&#22270;&#20687;&#19978;&#21033;&#29992;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#24863;&#30693;&#21160;&#20316;&#30340;&#20154;&#31867;&#20915;&#31574;&#65292;&#26088;&#22312;&#25552;&#20379;&#35814;&#32454;&#20449;&#24687;&#24110;&#21161;&#25351;&#25381;&#23448;&#20570;&#20986;&#20934;&#30830;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.02687</link><description>&lt;p&gt;
&#22522;&#20110;&#24863;&#30693;&#21160;&#20316;&#30340;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;SAR&#22270;&#20687;&#19978;&#36827;&#34892;&#20154;&#31867;&#20915;&#31574;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PAHD: Perception-Action based Human Decision Making using Explainable Graph Neural Networks on SAR Images. (arXiv:2401.02687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;SAR&#22270;&#20687;&#19978;&#21033;&#29992;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#24863;&#30693;&#21160;&#20316;&#30340;&#20154;&#31867;&#20915;&#31574;&#65292;&#26088;&#22312;&#25552;&#20379;&#35814;&#32454;&#20449;&#24687;&#24110;&#21161;&#25351;&#25381;&#23448;&#20570;&#20986;&#20934;&#30830;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#36890;&#24120;&#22312;&#20891;&#20107;&#24212;&#29992;&#20013;&#29992;&#20110;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;ATR&#65289;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24120;&#29992;&#20110;&#35782;&#21035;&#22320;&#38754;&#30446;&#26631;&#65292;&#21253;&#25324;&#25112;&#36710;&#12289;&#20154;&#21592;&#36816;&#36755;&#36710;&#21644;&#23548;&#24377;&#21457;&#23556;&#22120;&#12290;&#30830;&#23450;&#36710;&#36742;&#30340;&#31867;&#21035;&#65288;&#22914;BRDM2&#22374;&#20811;&#12289;BMP2&#22374;&#20811;&#12289;BTR60&#22374;&#20811;&#21644;BTR70&#22374;&#20811;&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#24110;&#21161;&#30830;&#23450;&#30446;&#26631;&#29289;&#20307;&#26159;&#30431;&#21451;&#36824;&#26159;&#25932;&#20154;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#23545;&#35782;&#21035;&#30446;&#26631;&#30340;&#21453;&#39304;&#65292;&#26368;&#32456;&#20915;&#31574;&#21364;&#25484;&#25569;&#22312;&#25351;&#25381;&#23448;&#25163;&#20013;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#35814;&#32454;&#20449;&#24687;&#19982;&#35782;&#21035;&#21040;&#30340;&#30446;&#26631;&#19968;&#36215;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#20182;&#20204;&#30340;&#34892;&#21160;&#12290;&#36825;&#20123;&#35814;&#32454;&#20449;&#24687;&#21253;&#25324;&#23545;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;SAR&#22270;&#20687;&#29305;&#24449;&#12289;&#20998;&#31867;&#32622;&#20449;&#24230;&#20197;&#21450;&#34987;&#35782;&#21035;&#20026;&#19981;&#21516;&#30446;&#26631;&#31867;&#22411;&#25110;&#31867;&#21035;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Aperture Radar (SAR) images are commonly utilized in military applications for automatic target recognition (ATR). Machine learning (ML) methods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks (GNN), are frequently used to identify ground-based objects, including battle tanks, personnel carriers, and missile launchers. Determining the vehicle class, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is crucial, as it can help determine whether the target object is an ally or an enemy. While the ML algorithm provides feedback on the recognized target, the final decision is left to the commanding officers. Therefore, providing detailed information alongside the identified target can significantly impact their actions. This detailed information includes the SAR image features that contributed to the classification, the classification confidence, and the probability of the identified object being classified as a different object type or class. W
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#22270;&#21644;&#24207;&#21015;&#34920;&#31034;&#30340;&#21313;&#31181;&#28431;&#27934;&#26816;&#27979;&#22120;&#35299;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21333;&#32431;&#30340;&#24544;&#35802;&#24230;&#35780;&#20272;&#19981;&#36275;&#22815;&#12290;</title><link>http://arxiv.org/abs/2401.02686</link><description>&lt;p&gt;
&#36229;&#36234;&#24544;&#35802;&#24230;&#65306;&#35299;&#37322;&#22522;&#20110;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#22120;&#30340;&#28431;&#27934;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors. (arXiv:2401.02686v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#22270;&#21644;&#24207;&#21015;&#34920;&#31034;&#30340;&#21313;&#31181;&#28431;&#27934;&#26816;&#27979;&#22120;&#35299;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21333;&#32431;&#30340;&#24544;&#35802;&#24230;&#35780;&#20272;&#19981;&#36275;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28431;&#27934;&#26816;&#27979;&#22120;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#20915;&#31574;&#36807;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#20351;&#23433;&#20840;&#20998;&#26512;&#24072;&#38590;&#20197;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#65292;&#36890;&#36807;&#31361;&#20986;&#37325;&#35201;&#29305;&#24449;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20854;&#20182;&#39046;&#22495;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#28431;&#27934;&#26816;&#27979;&#22120;&#23398;&#20064;&#21644;&#29702;&#35299;&#30340;&#32454;&#31890;&#24230;&#30340;&#28431;&#27934;&#30456;&#20851;&#20195;&#30721;&#34892;&#31561;&#20851;&#38190;&#29305;&#24449;&#30340;&#28145;&#20837;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#23450;&#37327;&#25351;&#26631;&#8212;&#8212;&#24544;&#35802;&#24230;&#21644;&#28431;&#27934;&#34892;&#35206;&#30422;&#29575;&#8212;&#8212;&#35780;&#20272;&#20102;&#22522;&#20110;&#22270;&#21644;&#24207;&#21015;&#34920;&#31034;&#30340;&#21313;&#31181;&#28431;&#27934;&#26816;&#27979;&#22120;&#35299;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20165;&#20165;&#20381;&#38752;&#24544;&#35802;&#24230;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability detectors based on deep learning (DL) models have proven their effectiveness in recent years. However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend. To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in other domains such as computer vision and natural language processing. Unfortunately, an in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches remains lacking. In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate. Our results show that fidelity alone is not sufficient f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20415;&#21033;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#22810;&#20307;&#21407;&#23376;&#20851;&#31995;&#24314;&#27169;&#21644;&#38190;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02683</link><description>&lt;p&gt;
&#29992;&#20110;3D&#20998;&#23376;&#29983;&#25104;&#30340;&#20960;&#20309;&#20415;&#21033;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02683
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20415;&#21033;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#22810;&#20307;&#21407;&#23376;&#20851;&#31995;&#24314;&#27169;&#21644;&#38190;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#38754;&#21521;&#20840;&#26032;3D&#20998;&#23376;&#29983;&#25104;&#30340;&#25193;&#25955;&#22522;&#20110;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#20998;&#23376;&#20013;&#30340;&#22823;&#22810;&#25968;&#37325;&#21407;&#23376;&#36890;&#36807;&#21333;&#38190;&#19982;&#22810;&#20010;&#21407;&#23376;&#30456;&#36830;&#65292;&#20165;&#20351;&#29992;&#25104;&#23545;&#36317;&#31163;&#26469;&#27169;&#25311;&#20998;&#23376;&#20960;&#20309;&#26159;&#19981;&#36275;&#30340;&#12290;&#22240;&#27492;&#65292;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#22810;&#20307;&#21407;&#23376;&#38388;&#20851;&#31995;&#21644;&#23398;&#20064;&#39640;&#36136;&#37327;&#29305;&#24449;&#30340;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21435;&#22122;&#20869;&#26680;&#12290;&#30001;&#20110;&#22270;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#38754;&#23545;&#20998;&#23376;&#30340;&#20027;&#27969;&#25193;&#25955;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#24182;&#20197;&#38388;&#25509;&#26041;&#24335;&#29983;&#25104;&#36793;&#32536;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#28041;&#21450;&#23558;&#20998;&#23376;&#30340;&#29983;&#25104;&#19982;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#24182;&#20934;&#30830;&#39044;&#27979;&#38190;&#30340;&#23384;&#22312;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#26356;&#26032;&#20998;&#23376;&#26500;&#22411;&#30340;&#36845;&#20195;&#26041;&#24335;&#19982;&#20998;&#23376;&#21160;&#21147;&#23398;&#19968;&#33268;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have shown great potential in multiple research areas. Existing diffusion-based generative methods on de novo 3D molecule generation face two major challenges. Since majority heavy atoms in molecules allow connections to multiple atoms through single bonds, solely using pair-wise distance to model molecule geometries is insufficient. Therefore, the first one involves proposing an effective neural network as the denoising kernel that is capable to capture complex multi-body interatomic relationships and learn high-quality features. Due to the discrete nature of graphs, mainstream diffusion-based methods for molecules heavily rely on predefined rules and generate edges in an indirect manner. The second challenge involves accommodating molecule generation to diffusion and accurately predicting the existence of bonds. In our research, we view the iterative way of updating molecule conformations in diffusion process is consistent with molecular dynamics and introd
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#28151;&#21512;&#22270;&#36807;&#28388;&#22120;&#29992;&#20110;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#21516;&#36136;&#24615;&#22270;&#65292;&#32780;&#35813;&#30740;&#31350;&#38024;&#23545;&#24191;&#27867;&#23384;&#22312;&#30340;&#24322;&#36136;&#24615;&#22270;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#19982;&#22270;&#30340;&#21516;&#36136;&#24615;&#31243;&#24230;&#23494;&#20999;&#30456;&#20851;&#30340;&#22270;&#36807;&#28388;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.02682</link><description>&lt;p&gt;
&#21516;&#36136;&#24615;&#30456;&#20851;&#65306;&#33258;&#36866;&#24212;&#28151;&#21512;&#22270;&#36807;&#28388;&#22120;&#29992;&#20110;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Homophily-Related: Adaptive Hybrid Graph Filter for Multi-View Graph Clustering. (arXiv:2401.02682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#28151;&#21512;&#22270;&#36807;&#28388;&#22120;&#29992;&#20110;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#21516;&#36136;&#24615;&#22270;&#65292;&#32780;&#35813;&#30740;&#31350;&#38024;&#23545;&#24191;&#27867;&#23384;&#22312;&#30340;&#24322;&#36136;&#24615;&#22270;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#19982;&#22270;&#30340;&#21516;&#36136;&#24615;&#31243;&#24230;&#23494;&#20999;&#30456;&#20851;&#30340;&#22270;&#36807;&#28388;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22270;&#25968;&#25454;&#30340;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#65292;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#25104;&#20026;&#30740;&#31350;&#30340;&#28909;&#28857;&#39046;&#22495;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#21516;&#36136;&#24615;&#22270;&#65292;&#28982;&#32780;&#24191;&#27867;&#23384;&#22312;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#25968;&#25454;&#24456;&#38590;&#28385;&#36275;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#21363;&#36830;&#25509;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#23646;&#20110;&#21516;&#19968;&#20010;&#31867;&#12290;&#19968;&#20123;&#30740;&#31350;&#25351;&#20986;&#65292;&#22312;&#24322;&#36136;&#24615;&#22270;&#19978;&#34920;&#29616;&#19981;&#20339;&#23454;&#38469;&#19978;&#26159;&#22240;&#20026;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#26412;&#36136;&#19978;&#26159;&#20302;&#36890;&#28388;&#27874;&#22120;&#65292;&#20002;&#24323;&#20102;&#38500;&#22270;&#19978;&#20302;&#39057;&#20449;&#24687;&#20043;&#22806;&#30340;&#20854;&#20182;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#22270;&#19978;&#65292;&#29305;&#21035;&#26159;&#24322;&#36136;&#24615;&#22270;&#19978;&#65292;&#24573;&#30053;&#39640;&#39057;&#20449;&#24687;&#65292;&#21482;&#20851;&#27880;&#20302;&#39057;&#20449;&#24687;&#38459;&#30861;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#31361;&#30772;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#36827;&#34892;&#19982;&#32473;&#23450;&#22270;&#30340;&#21516;&#36136;&#24615;&#31243;&#24230;&#23494;&#20999;&#30456;&#20851;&#30340;&#22270;&#36807;&#28388;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there is a growing focus on graph data, and multi-view graph clustering has become a popular area of research interest. Most of the existing methods are only applicable to homophilous graphs, yet the extensive real-world graph data can hardly fulfill the homophily assumption, where the connected nodes tend to belong to the same class. Several studies have pointed out that the poor performance on heterophilous graphs is actually due to the fact that conventional graph neural networks (GNNs), which are essentially low-pass filters, discard information other than the low-frequency information on the graph. Nevertheless, on certain graphs, particularly heterophilous ones, neglecting high-frequency information and focusing solely on low-frequency information impedes the learning of node representations. To break this limitation, our motivation is to perform graph filtering that is closely related to the homophily degree of the given graph, with the aim of fully leveraging both low-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Large Model as a Service (LMaaS)&#20316;&#20026;&#26234;&#33021;&#36890;&#20449;&#20013;&#30340;&#23450;&#20215;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;Stackelberg&#21338;&#24328;&#35299;&#20915;&#20102;&#23450;&#20215;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02675</link><description>&lt;p&gt;
LMaaS&#65306;&#25506;&#32034;&#38754;&#21521;&#36890;&#20449;&#30340;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#26381;&#21153;&#30340;&#23450;&#20215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
LMaaS: Exploring Pricing Strategy of Large Model as a Service for Communication. (arXiv:2401.02675v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Large Model as a Service (LMaaS)&#20316;&#20026;&#26234;&#33021;&#36890;&#20449;&#20013;&#30340;&#23450;&#20215;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;Stackelberg&#21338;&#24328;&#35299;&#20915;&#20102;&#23450;&#20215;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#19979;&#19968;&#20195;&#36890;&#20449;&#23558;&#26159;&#26234;&#33021;&#36890;&#20449;&#65292;&#21487;&#21462;&#20195;&#20256;&#32479;&#30340;&#31526;&#21495;&#36890;&#20449;&#65292;&#20854;&#20013;&#39640;&#24230;&#21387;&#32553;&#30340;&#35821;&#20041;&#20449;&#24687;&#23558;&#32771;&#34385;&#28304;&#21644;&#36890;&#36947;&#65292;&#24182;&#20197;&#39640;&#25928;&#29575;&#25552;&#21462;&#21644;&#20256;&#36755;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#27426;&#36814;&#30340;&#22823;&#22411;&#27169;&#22411;&#22914;GPT4&#21644;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#20026;&#26234;&#33021;&#36890;&#20449;&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#65292;&#24182;&#20419;&#20351;&#20854;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#24471;&#20197;&#23454;&#38469;&#37096;&#32626;&#12290;&#37492;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#35757;&#32451;&#19968;&#27425;&#65292;&#24191;&#27867;&#20351;&#29992;&#8221;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#35748;&#20026;&#25353;&#38656;&#20184;&#36153;&#30340;&#26381;&#21153;&#27169;&#24335;&#23558;&#36866;&#29992;&#20110;&#36825;&#31181;&#32972;&#26223;&#65292;&#31216;&#20026;Large Model as a Service (LMaaS)&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#26500;&#21644;&#21160;&#24577;&#30340;&#23458;&#25143;&#29615;&#22659;&#65292;&#20132;&#26131;&#21644;&#23450;&#20215;&#38382;&#39064;&#38750;&#24120;&#22797;&#26434;&#65292;&#20351;&#24471;&#23547;&#25214;&#21363;&#26102;&#35299;&#20915;&#26041;&#26696;&#30340;&#23450;&#20215;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#23558;LMaaS&#24066;&#22330;&#20132;&#26131;&#24418;&#24335;&#21270;&#20026;Stackelberg&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
The next generation of communication is envisioned to be intelligent communication, that can replace traditional symbolic communication, where highly condensed semantic information considering both source and channel will be extracted and transmitted with high efficiency. The recent popular large models such as GPT4 and the boosting learning techniques lay a solid foundation for the intelligent communication, and prompt the practical deployment of it in the near future. Given the characteristics of "training once and widely use" of those multimodal large language models, we argue that a pay-as-you-go service mode will be suitable in this context, referred to as Large Model as a Service (LMaaS). However, the trading and pricing problem is quite complex with heterogeneous and dynamic customer environments, making the pricing optimization problem challenging in seeking on-hand solutions. In this paper, we aim to fill this gap and formulate the LMaaS market trading as a Stackelberg game wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GAI-oriented synthetical network (GaisNet)&#65292;&#36825;&#26159;&#19968;&#20010;&#21327;&#20316;&#30340;&#20113;&#31471;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#25968;&#25454;&#30693;&#35782;&#20013;&#32487;&#26469;&#32531;&#35299;&#30683;&#30462;&#65292;&#23454;&#29616;&#20102;GAI&#30340;&#24490;&#29615;&#27169;&#22411;&#24494;&#35843;&#21644;&#20219;&#21153;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;GAI&#21644;EI&#30340;&#20114;&#21033;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.02668</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#36793;&#32536;&#26234;&#33021;&#30456;&#36935;&#26102;&#23454;&#29616;&#32508;&#21512;&#24494;&#35843;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Integrated Fine-tuning and Inference when Generative AI meets Edge Intelligence. (arXiv:2401.02668v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GAI-oriented synthetical network (GaisNet)&#65292;&#36825;&#26159;&#19968;&#20010;&#21327;&#20316;&#30340;&#20113;&#31471;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#25968;&#25454;&#30693;&#35782;&#20013;&#32487;&#26469;&#32531;&#35299;&#30683;&#30462;&#65292;&#23454;&#29616;&#20102;GAI&#30340;&#24490;&#29615;&#27169;&#22411;&#24494;&#35843;&#21644;&#20219;&#21153;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;GAI&#21644;EI&#30340;&#20114;&#21033;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20195;&#34920;&#20102;&#35745;&#31639;&#26234;&#33021;&#30340;&#26368;&#26032;&#28436;&#21464;&#65292;&#32780;&#26410;&#26469;&#30340;6G&#32593;&#32476;&#30340;&#21457;&#23637;&#28508;&#21147;&#20063;&#20351;&#24471;&#36793;&#32536;&#26234;&#33021;&#65288;EI&#65289;&#20805;&#28385;&#20102;&#21457;&#23637;&#26426;&#20250;&#12290;GAI&#21644;EI&#30340;&#24517;&#28982;&#30456;&#36935;&#21487;&#20197;&#37322;&#25918;&#26032;&#30340;&#26426;&#20250;&#65292;GAI&#22522;&#20110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20026;EI&#25552;&#20379;&#24378;&#22823;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#32780;EI&#21487;&#20197;&#21033;&#29992;&#30862;&#29255;&#21270;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#27719;&#24635;&#20010;&#24615;&#21270;&#30340;&#30693;&#35782;&#20026;GAI&#25152;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33258;&#28982;&#30340;&#30683;&#30462;&#29305;&#28857;&#32473;&#30452;&#25509;&#30340;&#30693;&#35782;&#20849;&#20139;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;GAI&#30340;&#32508;&#21512;&#32593;&#32476;&#65288;GaisNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21327;&#20316;&#30340;&#20113;&#31471;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#25968;&#25454;&#30693;&#35782;&#20013;&#32487;&#26469;&#32531;&#35299;&#30683;&#30462;&#65292;&#20854;&#20013;&#21452;&#21521;&#30340;&#30693;&#35782;&#27969;&#20351;&#24471;GAI&#30340;&#24490;&#29615;&#27169;&#22411;&#24494;&#35843;&#21644;&#20219;&#21153;&#25512;&#29702;&#25104;&#20026;&#21487;&#33021;&#65292;&#23454;&#29616;&#20102;GAI&#21644;EI&#30340;&#20114;&#21033;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high-performance generative artificial intelligence (GAI) represents the latest evolution of computational intelligence, while the blessing of future 6G networks also makes edge intelligence (EI) full of development potential. The inevitable encounter between GAI and EI can unleash new opportunities, where GAI's pre-training based on massive computing resources and large-scale unlabeled corpora can provide strong foundational knowledge for EI, while EI can harness fragmented computing resources to aggregate personalized knowledge for GAI. However, the natural contradictory features pose significant challenges to direct knowledge sharing. To address this, in this paper, we propose the GAI-oriented synthetical network (GaisNet), a collaborative cloud-edge-end intelligence framework that buffers contradiction leveraging data-free knowledge relay, where the bidirectional knowledge flow enables GAI's virtuous-cycle model fine-tuning and task inference, achieving mutualism between GAI an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20854;&#20182;&#22320;&#29702;&#20301;&#32622;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#21644;&#26410;&#30417;&#27979;&#21040;&#30340;&#22320;&#28857;&#30340;&#21508;&#31181;&#27668;&#20505;&#27979;&#37327;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.02665</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#24494;&#27668;&#20505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Microclimate Prediction with Deep Learning. (arXiv:2401.02665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20854;&#20182;&#22320;&#29702;&#20301;&#32622;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#21644;&#26410;&#30417;&#27979;&#21040;&#30340;&#22320;&#28857;&#30340;&#21508;&#31181;&#27668;&#20505;&#27979;&#37327;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#31449;&#25968;&#25454;&#26159;&#27668;&#20505;&#39044;&#27979;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#20294;&#22312;&#20559;&#36828;&#22320;&#21306;&#20854;&#21487;&#38752;&#24615;&#21487;&#33021;&#21463;&#38480;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#26412;&#22320;&#39044;&#27979;&#36890;&#24120;&#20381;&#36182;&#20110;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#26032;&#30340;&#12289;&#20197;&#21069;&#26410;&#30417;&#27979;&#21040;&#30340;&#22320;&#28857;&#26469;&#35828;&#23588;&#20854;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#26032;&#30340;&#12289;&#26410;&#30417;&#27979;&#21040;&#30340;&#22320;&#28857;&#30340;&#21508;&#31181;&#27668;&#20505;&#27979;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20174;&#20854;&#20182;&#22320;&#29702;&#20301;&#32622;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22825;&#27668;&#39044;&#25253;&#25216;&#26415;&#65292;&#39044;&#27979;&#24494;&#27668;&#20505;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather station data is a valuable resource for climate prediction, however, its reliability can be limited in remote locations. To compound the issue, making local predictions often relies on sensor data that may not be accessible for a new, previously unmonitored location. In response to these challenges, we propose a novel zero-shot learning approach designed to forecast various climate measurements at new and unmonitored locations. Our method surpasses conventional weather forecasting techniques in predicting microclimate variables by leveraging knowledge extracted from other geographic locations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21457;&#29616;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02663</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack against link prediction tasks with graph neural networks. (arXiv:2401.02663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21457;&#29616;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31867;&#33021;&#22815;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#24403;&#20855;&#20307;&#30340;&#27169;&#24335;&#65288;&#31216;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#20363;&#22914;&#23376;&#22270;&#12289;&#33410;&#28857;&#31561;&#65289;&#20986;&#29616;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#26102;&#65292;&#23884;&#20837;&#22312;GNN&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#20250;&#34987;&#28608;&#27963;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#35823;&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#31867;&#26631;&#31614;&#65292;&#32780;&#24403;&#36755;&#20837;&#20013;&#27809;&#26377;&#21518;&#38376;&#35302;&#21457;&#22120;&#26102;&#65292;&#23884;&#20837;&#22312;GNN&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#19981;&#20250;&#34987;&#28608;&#27963;&#65292;&#27169;&#22411;&#27491;&#24120;&#24037;&#20316;&#12290;&#21518;&#38376;&#25915;&#20987;&#20855;&#26377;&#26497;&#39640;&#30340;&#38544;&#34109;&#24615;&#65292;&#32473;GNN&#27169;&#22411;&#24102;&#26469;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30446;&#21069;&#65292;&#23545;GNN&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20998;&#31867;&#21644;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#65292;&#23545;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a class of deep learning models capable of processing graph-structured data, and they have demonstrated significant performance in a variety of real-world applications. Recent studies have found that GNN models are vulnerable to backdoor attacks. When specific patterns (called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input data, the backdoor embedded in the GNN models is activated, which misclassifies the input data into the target class label specified by the attacker, whereas when there are no backdoor triggers in the input, the backdoor embedded in the GNN models is not activated, and the models work normally. Backdoor attacks are highly stealthy and expose GNN models to serious security risks. Currently, research on backdoor attacks against GNNs mainly focus on tasks such as graph classification and node classification, and backdoor attacks against link prediction tasks are rarely studied. In this paper, we propose a backdoor a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#25252;&#22763;&#21442;&#19982;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#20307;&#26469;&#23454;&#29616;&#38024;&#23545;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#31934;&#20934;&#31649;&#29702;&#12290;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#25968;&#25454;&#28304;&#30340;&#27169;&#24335;&#21644;&#25252;&#22763;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#21453;&#39304;&#65292;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02661</link><description>&lt;p&gt;
&#25252;&#22763;&#21442;&#19982;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#38024;&#23545;2&#22411;&#31958;&#23615;&#30149;&#30340;&#31934;&#20934;&#31649;&#29702;&#65306;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
Nurse-in-the-Loop Artificial Intelligence for Precision Management of Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive Digital Twin. (arXiv:2401.02661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#25252;&#22763;&#21442;&#19982;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#20307;&#26469;&#23454;&#29616;&#38024;&#23545;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#31934;&#20934;&#31649;&#29702;&#12290;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#25968;&#25454;&#28304;&#30340;&#27169;&#24335;&#21644;&#25252;&#22763;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#21453;&#39304;&#65292;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24930;&#24615;&#30142;&#30149;&#65292;&#20854;&#20250;&#22686;&#21152;&#20005;&#37325;&#20581;&#24247;&#24182;&#23545;&#29983;&#27963;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#32771;&#34385;&#21040;&#20010;&#20307;&#29305;&#24449;&#21644;&#29983;&#27963;&#26041;&#24335;&#23545;&#27835;&#30103;&#35745;&#21010;&#21644;&#24739;&#32773;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24320;&#21457;&#31934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#31649;&#29702;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32467;&#21512;&#26469;&#33258;&#21508;&#31181;&#25968;&#25454;&#28304;&#30340;&#27169;&#24335;&#21644;&#25252;&#22763;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#25252;&#29702;&#25928;&#26524;&#12290;&#26041;&#27861;&#65306;&#36825;&#26159;&#19968;&#39033;&#38024;&#23545;T2D&#24739;&#32773;&#65288;n = 20&#65292;&#24180;&#40836;= 57+-10&#65289;&#30340;&#20026;&#26399;6&#20010;&#26376;&#30340;&#38468;&#23646;&#30740;&#31350;&#12290;&#21442;&#19982;&#32773;&#38543;&#26426;&#20998;&#37197;&#20026;&#24178;&#39044;&#32452;&#65288;AI&#65292;n = 10&#65289;&#21644;&#23545;&#29031;&#32452;&#65292;&#22312;&#26368;&#21518;&#19977;&#20010;&#26376;&#20013;&#24178;&#39044;&#32452;&#27599;&#22825;&#25509;&#25910;AI&#29983;&#25104;&#30340;&#20010;&#20307;&#21270;&#21453;&#39304;&#65292;&#32780;&#23545;&#29031;&#32452;&#19981;&#25509;&#25910;&#27599;&#26085;&#21453;&#39304;&#65288;&#38750;AI&#65292;n = 10&#65289;&#12290;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#25252;&#22763;&#21442;&#19982;&#22411;&#39044;&#27979;&#25511;&#21046;&#27169;&#22411;&#65288;ONLC&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#20307;&#65288;PDT&#65289;&#12290;PDT&#26159;&#36890;&#36807;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a significant risk of serious health complications and negative impacts on the quality of life. Given the impact of individual characteristics and lifestyle on the treatment plan and patient outcomes, it is crucial to develop precise and personalized management strategies. Artificial intelligence (AI) provides great promise in combining patterns from various data sources with nurses' expertise to achieve optimal care. Methods: This is a 6-month ancillary study among T2D patients (n = 20, age = 57 +- 10). Participants were randomly assigned to an intervention (AI, n=10) group to receive daily AI-generated individualized feedback or a control group without receiving the daily feedback (non-AI, n=10) in the last three months. The study developed an online nurse-in-the-loop predictive control (ONLC) model that utilizes a predictive digital twin (PDT). The PDT was developed using a transfer-learning-based Artificial Neura
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTA&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#20013;&#24341;&#23548;&#31354;&#38388;&#27880;&#24847;&#21147;&#30340;&#36716;&#31227;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;ViT&#20013;&#36716;&#31227;&#34920;&#31034;&#23481;&#26131;&#36807;&#25311;&#21512;&#21644;&#22833;&#21435;&#26377;&#20215;&#20540;&#29305;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02656</link><description>&lt;p&gt;
GTA: &#20174;&#29289;&#20307;&#20013;&#24515;&#30340;&#34920;&#31034;&#20013;&#24341;&#23548;&#31354;&#38388;&#27880;&#24847;&#21147;&#30340;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
GTA: Guided Transfer of Spatial Attention from Object-Centric Representations. (arXiv:2401.02656v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTA&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#20013;&#24341;&#23548;&#31354;&#38388;&#27880;&#24847;&#21147;&#30340;&#36716;&#31227;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;ViT&#20013;&#36716;&#31227;&#34920;&#31034;&#23481;&#26131;&#36807;&#25311;&#21512;&#21644;&#22833;&#21435;&#26377;&#20215;&#20540;&#29305;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35757;&#32451;&#33391;&#22909;&#30340;&#34920;&#31034;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#36890;&#24120;&#20250;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36825;&#26679;&#30340;&#22909;&#30340;&#34920;&#31034;&#34987;&#36716;&#31227;&#65292;&#27169;&#22411;&#20173;&#28982;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#22833;&#21435;&#20102;&#36716;&#31227;&#34920;&#31034;&#30340;&#26377;&#20215;&#20540;&#30340;&#29305;&#24615;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312; ViT &#20013;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#23427;&#30340;&#24402;&#32435;&#20559;&#32622;&#36739;&#20302;&#12290;&#36890;&#36807;&#20351;&#29992; ViT &#20013;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#36827;&#34892;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#20016;&#23500;&#30340;&#34920;&#31034;&#20250;&#36864;&#21270;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Guided Transfer of spatial Attention (GTA) &#30340;&#26032;&#39062;&#32780;&#31616;&#21333;&#30340; ViT &#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#26174;&#24335;&#30340;&#27491;&#21017;&#21270;&#22312;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#36890;&#36807;&#36825;&#31181;&#26174;&#24335;&#30340;&#27491;&#21017;&#21270;&#65292;&#30446;&#26631;&#27169;&#22411;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#19982;&#29289;&#20307;&#23450;&#20301;&#23646;&#24615;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340; GTA &#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing well-trained representations in transfer learning often results in superior performance and faster convergence compared to training from scratch. However, even if such good representations are transferred, a model can easily overfit the limited training dataset and lose the valuable properties of the transferred representations. This phenomenon is more severe in ViT due to its low inductive bias. Through experimental analysis using attention maps in ViT, we observe that the rich representations deteriorate when trained on a small dataset. Motivated by this finding, we propose a novel and simple regularization method for ViT called Guided Transfer of spatial Attention (GTA). Our proposed method regularizes the self-attention maps between the source and target models. A target model can fully exploit the knowledge related to object localization properties through this explicit regularization. Our experimental results show that the proposed GTA consistently improves the accuracy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#35843;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21160;&#36710;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#35843;&#24230;&#30005;&#21160;&#36710;&#30340;&#20805;&#25918;&#30005;&#27963;&#21160;&#65292;&#20197;&#19982;&#37197;&#30005;&#31995;&#32479;&#25805;&#20316;&#21592;&#25552;&#20379;&#30340;&#30446;&#26631;&#33021;&#37327;&#37197;&#32622;&#25991;&#20214;&#19968;&#33268;&#65292;&#21487;&#20197;&#23454;&#29616;&#23616;&#37096;&#32593;&#32476;&#30340;&#24179;&#34913;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.02653</link><description>&lt;p&gt;
EV&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#26234;&#33021;&#35843;&#24230;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids. (arXiv:2401.02653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#35843;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21160;&#36710;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#35843;&#24230;&#30005;&#21160;&#36710;&#30340;&#20805;&#25918;&#30005;&#27963;&#21160;&#65292;&#20197;&#19982;&#37197;&#30005;&#31995;&#32479;&#25805;&#20316;&#21592;&#25552;&#20379;&#30340;&#30446;&#26631;&#33021;&#37327;&#37197;&#32622;&#25991;&#20214;&#19968;&#33268;&#65292;&#21487;&#20197;&#23454;&#29616;&#23616;&#37096;&#32593;&#32476;&#30340;&#24179;&#34913;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#21644;&#25919;&#31574;&#22240;&#32032;&#25512;&#21160;&#20102;&#30005;&#21160;&#36710;(EV)&#30340;&#19981;&#26029;&#22686;&#21152;&#21644;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;EV&#26159;&#19968;&#31181;&#27604;&#29123;&#27833;&#36710;&#26356;&#28165;&#27905;&#30340;&#26367;&#20195;&#21697;&#65292;&#20294;&#30001;&#20110;&#30005;&#21147;&#38656;&#27714;&#22686;&#21152;&#21644;&#20351;&#29992;&#26102;&#38388;&#23548;&#33268;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;EV&#23545;&#24494;&#30005;&#32593;&#35774;&#22791;&#23551;&#21629;&#21644;&#33021;&#28304;&#24179;&#34913;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#30005;&#32593;&#31649;&#29702;&#24212;&#35813;&#21033;&#29992;EV&#30340;&#35843;&#24230;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#31215;&#26497;&#21442;&#19982;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#26469;&#25903;&#25345;&#23616;&#37096;&#32593;&#32476;&#24179;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;EV&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#27963;&#21160;&#35843;&#24230;&#21040;&#24494;&#30005;&#32593;&#20013;&#65292;&#20197;&#19982;&#37197;&#30005;&#31995;&#32479;&#25805;&#20316;&#21592;&#25552;&#20379;&#30340;&#30446;&#26631;&#33021;&#37327;&#37197;&#32622;&#25991;&#20214;&#19968;&#33268;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;Bellman&#26041;&#31243;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#22870;&#21169;&#35780;&#20272;&#29366;&#24577;&#30340;&#20215;&#20540;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#21487;&#29992;&#21160;&#20316;&#30340;Q&#20540;&#65292;&#24182;&#20351;&#29992;epsilon-greedy&#31639;&#27861;&#24179;&#34913;&#24320;&#21457;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic and policy factors are driving the continuous increase in the adoption and usage of electrical vehicles (EVs). However, despite being a cleaner alternative to combustion engine vehicles, EVs have negative impacts on the lifespan of microgrid equipment and energy balance due to increased power demand and the timing of their usage. In our view grid management should leverage on EVs scheduling flexibility to support local network balancing through active participation in demand response programs. In this paper, we propose a model-free solution, leveraging Deep Q-Learning to schedule the charging and discharging activities of EVs within a microgrid to align with a target energy profile provided by the distribution system operator. We adapted the Bellman Equation to assess the value of a state based on specific rewards for EV scheduling actions and used a neural network to estimate Q-values for available actions and the epsilon-greedy algorithm to balance exploitation and explorati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#23384;&#22312;&#29615;&#22659;&#21160;&#24577;&#21644;&#30456;&#23545;&#20110;&#21463;&#23475;&#32773;&#30446;&#26631;&#30340;&#38750;&#26368;&#20248;&#24615;&#26102;&#65292;&#21363;&#20351;&#30446;&#26631;&#34892;&#20026;&#26080;&#27861;&#34987;&#37319;&#32435;&#65292;&#20173;&#28982;&#21487;&#33021;&#36827;&#34892;C-TTA&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;gammaDDPG&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#31181;&#26356;&#24378;&#29256;&#26412;&#30340;C-TTA&#65292;&#24182;&#26681;&#25454;&#21463;&#23475;&#32773;&#24403;&#21069;&#30340;&#34892;&#20026;&#21160;&#24577;&#25913;&#21464;&#25915;&#20987;&#31574;&#30053;&#35268;&#21010;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.02652</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25240;&#25187;&#35757;&#32451;&#26102;&#38388;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adaptive Discounting of Training Time Attacks. (arXiv:2401.02652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#23384;&#22312;&#29615;&#22659;&#21160;&#24577;&#21644;&#30456;&#23545;&#20110;&#21463;&#23475;&#32773;&#30446;&#26631;&#30340;&#38750;&#26368;&#20248;&#24615;&#26102;&#65292;&#21363;&#20351;&#30446;&#26631;&#34892;&#20026;&#26080;&#27861;&#34987;&#37319;&#32435;&#65292;&#20173;&#28982;&#21487;&#33021;&#36827;&#34892;C-TTA&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;gammaDDPG&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#31181;&#26356;&#24378;&#29256;&#26412;&#30340;C-TTA&#65292;&#24182;&#26681;&#25454;&#21463;&#23475;&#32773;&#24403;&#21069;&#30340;&#34892;&#20026;&#21160;&#24577;&#25913;&#21464;&#25915;&#20987;&#31574;&#30053;&#35268;&#21010;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#35757;&#32451;&#26102;&#38388;&#25915;&#20987;&#65288;TTAs&#65289;&#26159;&#26368;&#38452;&#38505;&#30340;&#25915;&#20987;&#20043;&#19968;&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#20013;&#21046;&#36896;&#28431;&#27934;&#21644;&#21518;&#38376;&#12290;&#29616;&#22312;&#24050;&#32463;&#26377;&#20102;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#30772;&#22351;&#30340;&#24314;&#35774;&#24615;TTAs&#65288;C-TTAs&#65289;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#24378;&#21046;&#35757;&#32451;RL agent&#65288;&#21463;&#23475;&#32773;&#65289;&#34920;&#29616;&#20986;&#29305;&#23450;&#30340;&#30446;&#26631;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;C-TTAs&#20063;&#21482;&#38024;&#23545;&#37027;&#20123;&#22914;&#26524;&#19981;&#22240;&#29615;&#22659;&#21160;&#24577;&#30340;&#19968;&#20010;&#29305;&#23450;&#29305;&#24449;&#34987;&#21033;&#29992;&#65292;&#21463;&#23475;&#32773;&#26412;&#21487;&#20197;&#33258;&#28982;&#22320;&#37319;&#32435;&#30340;&#30446;&#26631;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#30446;&#26631;&#34892;&#20026;&#30001;&#20110;&#29615;&#22659;&#21160;&#24577;&#21644;&#30456;&#23545;&#20110;&#21463;&#23475;&#32773;&#30446;&#26631;&#30340;&#38750;&#26368;&#20248;&#24615;&#32780;&#26080;&#27861;&#37319;&#32435;&#65292;C-TTA&#20063;&#26159;&#21487;&#33021;&#30340;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;DDPG&#31639;&#27861;&#65292;&#31216;&#20026;gammaDDPG&#65292;&#29992;&#20110;&#23398;&#20064;&#36825;&#31181;&#26356;&#24378;&#29256;&#26412;&#30340;C-TTA&#12290;gammaDDPG&#26681;&#25454;&#21463;&#23475;&#32773;&#24403;&#21069;&#30340;&#34892;&#20026;&#21160;&#24577;&#25913;&#21464;&#25915;&#20987;&#31574;&#30053;&#35268;&#21010;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the most insidious attacks on Reinforcement Learning (RL) solutions are training-time attacks (TTAs) that create loopholes and backdoors in the learned behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are now available, where the attacker forces a specific, target behaviour upon a training RL agent (victim). However, even state-of-the-art C-TTAs focus on target behaviours that could be naturally adopted by the victim if not for a particular feature of the environment dynamics, which C-TTAs exploit. In this work, we show that a C-TTA is possible even when the target behaviour is un-adoptable due to both environment dynamics as well as non-optimality with respect to the victim objective(s). To find efficient attacks in this context, we develop a specialised flavour of the DDPG algorithm, which we term gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically alters the attack policy planning horizon based on the victim's current behaviour
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39640;&#32500;&#39034;&#24207;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02650</link><description>&lt;p&gt;
&#36890;&#36807;MCMC&#25913;&#36827;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving sample efficiency of high dimensional Bayesian optimization with MCMC. (arXiv:2401.02650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39640;&#32500;&#39034;&#24207;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#39034;&#24207;&#20248;&#21270;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#32500;&#24230;&#35781;&#21650;&#12290;&#30446;&#21069;&#30340;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#22312;&#36319;&#36394;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#20173;&#23384;&#22312;&#36127;&#25285;&#65292;&#24182;&#19988;&#38656;&#35201;&#23558;&#20248;&#21270;&#38382;&#39064;&#21010;&#20998;&#20026;&#23567;&#21306;&#22495;&#20197;&#30830;&#20445;&#25506;&#32034;&#25110;&#20551;&#35774;&#19968;&#20010;&#28508;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20505;&#36873;&#28857;&#36716;&#31227;&#21040;&#26356;&#26377;&#24076;&#26395;&#30340;&#20301;&#32622;&#26469;&#26377;&#25928;&#22320;&#20174;&#36817;&#20284;&#21518;&#39564;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#39640;&#26031;&#36807;&#31243;&#27748;&#26222;&#26862;&#37319;&#26679;&#35774;&#32622;&#19979;&#25552;&#20379;&#20102;&#20854;&#25910;&#25947;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;Metropolis-Hastings&#29256;&#26412;&#21644;Langevin Dynamics&#29256;&#26412;&#22312;&#39640;&#32500;&#39034;&#24207;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential optimization methods are often confronted with the curse of dimensionality in high-dimensional spaces. Current approaches under the Gaussian process framework are still burdened by the computational complexity of tracking Gaussian process posteriors and need to partition the optimization problem into small regions to ensure exploration or assume an underlying low-dimensional structure. With the idea of transiting the candidate points towards more promising positions, we propose a new method based on Markov Chain Monte Carlo to efficiently sample from an approximated posterior. We provide theoretical guarantees of its convergence in the Gaussian process Thompson sampling setting. We also show experimentally that both the Metropolis-Hastings and the Langevin Dynamics version of our algorithm outperform state-of-the-art methods in high-dimensional sequential optimization and reinforcement learning benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#23618;&#25193;&#25955;&#35268;&#21010;(Hierarchical Diffuser)&#65292;&#32467;&#21512;&#20102;&#20998;&#23618;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#19988;&#26377;&#25928;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#36739;&#39640;&#23618;&#38754;&#19978;&#37319;&#29992;&#8220;&#36339;&#36291;&#8221;&#35268;&#21010;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20855;&#26377;&#36739;&#22823;&#30340;&#24863;&#21463;&#37326;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#21516;&#26102;&#36339;&#36291;&#30340;&#23376;&#30446;&#26631;&#36824;&#33021;&#25351;&#23548;&#20302;&#23618;&#35268;&#21010;&#22120;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02644</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#20998;&#23618;&#25193;&#25955;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Simple Hierarchical Planning with Diffusion. (arXiv:2401.02644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#23618;&#25193;&#25955;&#35268;&#21010;(Hierarchical Diffuser)&#65292;&#32467;&#21512;&#20102;&#20998;&#23618;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#19988;&#26377;&#25928;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#36739;&#39640;&#23618;&#38754;&#19978;&#37319;&#29992;&#8220;&#36339;&#36291;&#8221;&#35268;&#21010;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20855;&#26377;&#36739;&#22823;&#30340;&#24863;&#21463;&#37326;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#21516;&#26102;&#36339;&#36291;&#30340;&#23376;&#30446;&#26631;&#36824;&#33021;&#25351;&#23548;&#20302;&#23618;&#35268;&#21010;&#22120;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;&#24314;&#27169;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21487;&#33021;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#38271;&#26102;&#20219;&#21153;&#30340;&#26102;&#38388;&#25277;&#35937;&#26041;&#38754;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#25193;&#25955;&#35268;&#21010;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#20294;&#20196;&#20154;&#24778;&#35766;&#22320;&#26377;&#25928;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20998;&#23618;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#35268;&#21010;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36739;&#39640;&#23618;&#38754;&#19978;&#37319;&#29992;&#20102;&#8220;&#36339;&#36291;&#8221;&#35268;&#21010;&#31574;&#30053;&#65292;&#20351;&#20854;&#20855;&#26377;&#36739;&#22823;&#30340;&#24863;&#21463;&#37326;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302; - &#36825;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#35268;&#21010;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#25105;&#20204;&#24050;&#32463;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#36339;&#36291;&#30340;&#23376;&#30446;&#26631;&#25351;&#23548;&#25105;&#20204;&#30340;&#20302;&#23618;&#35268;&#21010;&#22120;&#65292;&#20419;&#36827;&#20102;&#19968;&#20010;&#24494;&#35843;&#38454;&#27573;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23545;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a "jumpy" planning strategy at the higher level, which allows it to have a larger receptive field but at a lower computational cost -- a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25805;&#20316;&#21644;&#34701;&#21512;&#22810;&#31181;&#35299;&#37322;&#25216;&#26415;&#23454;&#29616;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#35299;&#37322;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02630</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26694;&#26550;&#65306;NBA&#20307;&#32946;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model-Agnostic Interpretation Framework in Machine Learning: A Comparative Study in NBA Sports. (arXiv:2401.02630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25805;&#20316;&#21644;&#34701;&#21512;&#22810;&#31181;&#35299;&#37322;&#25216;&#26415;&#23454;&#29616;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#35299;&#37322;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#20316;&#20026;&#19981;&#36879;&#26126;&#30340;&#8220;&#40657;&#31665;&#8221;&#25805;&#20316;&#65292;&#20351;&#20854;&#20915;&#31574;&#32972;&#21518;&#30340;&#29702;&#30001;&#21464;&#24471;&#38590;&#20197;&#29702;&#35299;&#12290;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#21487;&#20197;&#38480;&#21046;&#23545;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#24182;&#38459;&#30861;&#20854;&#22312;&#25935;&#24863;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#25110;&#37329;&#34701;&#65289;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21327;&#35843;&#27169;&#22411;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22260;&#32469;&#30528;&#23545;&#39640;&#32500;&#25968;&#25454;&#30340;&#27169;&#22359;&#21270;&#25805;&#20316;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;&#22788;&#29702;&#21516;&#26102;&#20445;&#25345;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#34701;&#21512;&#22810;&#31181;&#35299;&#37322;&#25216;&#26415;&#21644;&#27169;&#22359;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25581;&#31034;&#22797;&#26434;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#32780;&#19981;&#25439;&#23475;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of machine learning has seen tremendous progress in recent years, with deep learning models delivering exceptional performance across a range of tasks. However, these models often come at the cost of interpretability, as they operate as opaque "black boxes" that obscure the rationale behind their decisions. This lack of transparency can limit understanding of the models' underlying principles and impede their deployment in sensitive domains, such as healthcare or finance. To address this challenge, our research team has proposed an innovative framework designed to reconcile the trade-off between model performance and interpretability. Our approach is centered around modular operations on high-dimensional data, which enable end-to-end processing while preserving interpretability. By fusing diverse interpretability techniques and modularized data processing, our framework sheds light on the decision-making processes of complex models without compromising their performance. We h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21464;&#37327;&#21644;&#20854;&#22495;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02602</link><description>&lt;p&gt;
&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Neural Causal Abstractions. (arXiv:2401.02602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21464;&#37327;&#21644;&#20854;&#22495;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29702;&#35299;&#19990;&#30028;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#23558;&#20449;&#24687;&#21387;&#32553;&#25104;&#25277;&#35937;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#20004;&#20010;&#26631;&#24535;&#24615;&#29305;&#24449;&#12290;&#36825;&#20004;&#20010;&#20027;&#39064;&#22312;&#25991;&#29486;&#20013;&#34987;&#32479;&#31216;&#20026;&#22240;&#26524;&#25277;&#35937;&#29702;&#35770;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#22312;&#30495;&#23454;&#30340;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#20805;&#20998;&#21033;&#29992;&#25277;&#35937;&#29702;&#35770;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#30495;&#23454;&#26426;&#21046;&#26159;&#26410;&#30693;&#30340;&#65292;&#21482;&#26377;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21464;&#37327;&#21450;&#20854;&#22495;&#36827;&#34892;&#32858;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25277;&#35937;&#23478;&#26063;&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#21644;&#27010;&#25324;&#20102;&#20043;&#21069;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#24341;&#21457;&#30340;&#20010;&#20307;&#22240;&#26524;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#65288;Xia&#31561;&#65292;2021&#65289;&#21487;&#20197;&#23398;&#24471;&#36825;&#26679;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#20174;&#32780;&#33021;&#22815;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abilities of humans to understand the world in terms of cause and effect relationships, as well as to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem in the literature under the rubric of causal abstractions theory. In practice, it remains an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true mechanisms are unknown and only limited data is available. In this paper, we develop a new family of causal abstractions by clustering variables and their domains. This approach refines and generalizes previous notions of abstractions to better accommodate individual causal distributions that are spawned by Pearl's causal hierarchy. We show that such abstractions are learnable in practical settings through Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning toolkit to solve various challenging causal inference tasks -- iden
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#26469;&#23454;&#29616;&#27491;&#20132;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20248;&#21270;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.02592</link><description>&lt;p&gt;
&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery. (arXiv:2401.02592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#26469;&#23454;&#29616;&#27491;&#20132;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20248;&#21270;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#35299;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#23610;&#24230;&#27495;&#20041;&#24182;&#20415;&#20110;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#20248;&#21270;&#25152;&#35859;&#30340;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#65292;&#24378;&#21046;&#20351;&#22823;&#37096;&#20998;&#22240;&#23376;&#24444;&#27492;&#27491;&#20132;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#20132;&#32467;&#26500;&#65292;&#25105;&#20204;&#21033;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#65288;RGD&#65289;&#26469;&#20248;&#21270;Stiefel&#27969;&#24418;&#19978;&#30340;&#36825;&#20123;&#22240;&#23376;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;TT&#20998;&#35299;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;RGD&#30340;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#24352;&#37327;&#38454;&#25968;&#30340;&#22686;&#21152;&#65292;&#25910;&#25947;&#36895;&#29575;&#20165;&#32463;&#21382;&#32447;&#24615;&#19979;&#38477;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24863;&#30693;&#38382;&#39064;&#65292;&#21363;&#20174;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;TT&#26684;&#24335;&#24352;&#37327;&#12290;&#20551;&#35774;&#24863;&#30693;&#31639;&#23376;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#65288;RIP&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#36890;&#36807;&#35889;&#21021;&#22987;&#21270;&#33719;&#24471;&#65292;RGD&#20063;&#20250;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide the first convergence guarantee for the factorization approach. Specifically, to avoid the scaling ambiguity and to facilitate theoretical analysis, we optimize over the so-called left-orthogonal TT format which enforces orthonormality among most of the factors. To ensure the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for optimizing those factors over the Stiefel manifold. We first delve into the TT factorization problem and establish the local linear convergence of RGD. Notably, the rate of convergence only experiences a linear decline as the tensor order increases. We then study the sensing problem that aims to recover a TT format tensor from linear measurements. Assuming the sensing operator satisfies the restricted isometry property (RIP), we show that with a proper initialization, which could be obtained through spectral initialization, RGD also converges to the ground-truth tensor at a linear rate. Furthermore, we expand our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#24179;&#34913;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#20248;&#20808;&#24179;&#34913;&#20449;&#24687;&#20016;&#23500;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31867;&#21035;&#21518;&#39564;&#27604;&#29575;&#26469;&#26368;&#22823;&#21270;&#22312;&#27491;&#30830;&#30340;&#31867;&#21035;&#21306;&#22495;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#25216;&#26415;&#22312;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02591</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23545;&#19981;&#22343;&#34913;&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#22823;&#21518;&#39564;&#27604;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data. (arXiv:2401.02591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#24179;&#34913;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#20248;&#20808;&#24179;&#34913;&#20449;&#24687;&#20016;&#23500;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31867;&#21035;&#21518;&#39564;&#27604;&#29575;&#26469;&#26368;&#22823;&#21270;&#22312;&#27491;&#30830;&#30340;&#31867;&#21035;&#21306;&#22495;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#25216;&#26415;&#22312;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#23569;&#25968;&#31867;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#24179;&#34913;&#25968;&#25454;&#30340;&#25216;&#26415;&#12290;&#19982;&#22522;&#20110;&#38543;&#26426;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20808;&#24179;&#34913;&#20449;&#24687;&#20016;&#23500;&#21306;&#22495;&#65292;&#36890;&#36807;&#35782;&#21035;&#39640;&#29109;&#26679;&#26412;&#12290;&#29983;&#25104;&#36866;&#24403;&#20301;&#32622;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#32780;&#29983;&#25104;&#20301;&#32622;&#19981;&#24403;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#35823;&#20998;&#31867;&#29575;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31867;&#21035;&#21518;&#39564;&#27604;&#29575;&#26469;&#26368;&#22823;&#21270;&#22312;&#27491;&#30830;&#30340;&#31867;&#21035;&#21306;&#22495;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#30340;&#27010;&#29575;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#25968;&#25454;&#25299;&#25169;&#65292;&#21512;&#25104;&#25968;&#25454;&#22312;&#27599;&#20010;&#23569;&#25968;&#31867;&#21035;&#26679;&#26412;&#30340;&#37051;&#22495;&#20869;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;41&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the impact of class-imbalanced data on deep learning models and proposes a technique for data balancing by generating synthetic data for the minority class. Unlike random-based oversampling, our method prioritizes balancing the informative regions by identifying high entropy samples. Generating well-placed synthetic data can enhance machine learning algorithms accuracy and efficiency, whereas poorly-placed ones may lead to higher misclassification rates. We introduce an algorithm that maximizes the probability of generating a synthetic sample in the correct region of its class by optimizing the class posterior ratio. Additionally, to maintain data topology, synthetic data are generated within each minority sample's neighborhood. Our experimental results on forty-one datasets demonstrate the superior performance of our technique in enhancing deep-learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#26012;&#25968;&#25454;&#24773;&#20917;&#19979;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#26469;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;&#20027;&#35201;&#24605;&#36335;&#26159;&#36890;&#36807;&#35843;&#25972;&#23458;&#25143;&#31471;&#20998;&#24067;&#20351;&#20854;&#26356;&#25509;&#36817;&#20840;&#23616;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#24555;&#22320;&#25910;&#25947;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02586</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#36827;&#34892;&#20998;&#24067;&#20559;&#26012;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for distribution skewed data using sample weights. (arXiv:2401.02586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#26012;&#25968;&#25454;&#24773;&#20917;&#19979;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#26469;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;&#20027;&#35201;&#24605;&#36335;&#26159;&#36890;&#36807;&#35843;&#25972;&#23458;&#25143;&#31471;&#20998;&#24067;&#20351;&#20854;&#26356;&#25509;&#36817;&#20840;&#23616;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#24555;&#22320;&#25910;&#25947;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25968;&#25454;&#36890;&#24120;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65288;nonIID&#65289;&#12290;&#23458;&#25143;&#31471;&#34987;&#26399;&#26395;&#36129;&#29486;&#30456;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#24182;&#20174;&#19968;&#20010;&#20840;&#23616;&#20998;&#24067;&#20013;&#25277;&#21462;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24448;&#24448;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#20174;&#19981;&#21516;&#36164;&#28304;&#25910;&#38598;&#12290;&#22240;&#27492;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#19982;&#24213;&#23618;&#20840;&#23616;&#20998;&#24067;&#19981;&#21516;&#12290;&#36825;&#23601;&#20135;&#29983;&#20102;&#26435;&#37325;&#21457;&#25955;&#38382;&#39064;&#65292;&#24182;&#38477;&#20302;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#35813;&#24037;&#20316;&#20391;&#37325;&#20110;&#25913;&#21892;&#23458;&#25143;&#31471;&#20043;&#38388;&#20998;&#24067;&#20559;&#26012;&#30340;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#23558;&#23458;&#25143;&#31471;&#20998;&#24067;&#35843;&#25972;&#21040;&#20840;&#23616;&#20998;&#24067;&#26356;&#25509;&#36817;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25910;&#25947;&#26356;&#24555;&#19988;&#31934;&#24230;&#26356;&#39640;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#22522;&#26412;&#27010;&#24565;&#24320;&#22987;&#65292;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#35843;&#25972;&#20998;&#24067;&#20559;&#26012;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#30830;&#23450;&#26679;&#26412;&#26435;&#37325;&#65292;&#25105;&#20204;&#38544;&#21547;&#22320;&#20132;&#25442;...
&lt;/p&gt;
&lt;p&gt;
One of the most challenging issues in federated learning is that the data is often not independent and identically distributed (nonIID). Clients are expected to contribute the same type of data and drawn from one global distribution. However, data are often collected in different ways from different resources. Thus, the data distributions among clients might be different from the underlying global distribution. This creates a weight divergence issue and reduces federated learning performance. This work focuses on improving federated learning performance for skewed data distribution across clients. The main idea is to adjust the client distribution closer to the global distribution using sample weights. Thus, the machine learning model converges faster with higher accuracy. We start from the fundamental concept of empirical risk minimization and theoretically derive a solution for adjusting the distribution skewness using sample weights. To determine sample weights, we implicitly exchan
&lt;/p&gt;</description></item><item><title>t-DGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02576</link><description>&lt;p&gt;
t-DGR: &#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02576
&lt;/p&gt;
&lt;p&gt;
t-DGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#24050;&#32463;&#25104;&#20026;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20174;&#20197;&#21069;&#36935;&#21040;&#30340;&#20219;&#21153;&#29983;&#25104;&#36712;&#36857;&#26469;&#22686;&#21152;&#24403;&#21069;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#30340;&#36712;&#36857;&#20013;&#20250;&#20986;&#29616;&#32047;&#31215;&#35823;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#19988;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26681;&#25454;&#36712;&#36857;&#26102;&#38388;&#27493;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#22343;&#25104;&#21151;&#29575;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/WilliamYue37/t-DGR&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative replay has emerged as a promising approach for continual learning in decision-making tasks. This approach addresses the problem of catastrophic forgetting by leveraging the generation of trajectories from previously encountered tasks to augment the current dataset. However, existing deep generative replay methods for continual learning rely on autoregressive models, which suffer from compounding errors in the generated trajectories. In this paper, we propose a simple, scalable, and non-autoregressive method for continual learning in decision-making tasks using a generative model that generates task samples conditioned on the trajectory timestep. We evaluate our method on Continual World benchmarks and find that our approach achieves state-of-the-art performance on the average success rate metric among continual learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;LLM&#24212;&#29992;&#20998;&#20026;&#30693;&#35782;&#20219;&#21153;&#12289;&#23089;&#20048;&#20219;&#21153;&#21644;&#22522;&#30784;&#20219;&#21153;&#65292;&#24182;&#20998;&#20139;&#20102;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>http://arxiv.org/abs/2401.02575</link><description>&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Social Networks: Applications, Challenges, and Solutions. (arXiv:2401.02575v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;LLM&#24212;&#29992;&#20998;&#20026;&#30693;&#35782;&#20219;&#21153;&#12289;&#23089;&#20048;&#20219;&#21153;&#21644;&#22522;&#30784;&#20219;&#21153;&#65292;&#24182;&#20998;&#20139;&#20102;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#25913;&#21464;&#20154;&#20204;&#29983;&#25104;&#12289;&#25506;&#32034;&#21644;&#21442;&#19982;&#20869;&#23481;&#30340;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#24320;&#21457;LLM&#24212;&#29992;&#12290;&#23613;&#31649;LLMs&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#20135;&#21697;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#30740;&#31350;&#30028;&#20013;&#30456;&#23545;&#36739;&#23569;&#25253;&#36947;&#12290;&#25105;&#20204;&#23558;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;LLM&#24212;&#29992;&#20998;&#20026;&#19977;&#31867;&#12290;&#31532;&#19968;&#31867;&#26159;&#30693;&#35782;&#20219;&#21153;&#65292;&#29992;&#25143;&#24819;&#35201;&#26597;&#25214;&#26032;&#30693;&#35782;&#21644;&#20449;&#24687;&#65292;&#20363;&#22914;&#25628;&#32034;&#21644;&#38382;&#31572;&#12290;&#31532;&#20108;&#31867;&#26159;&#23089;&#20048;&#20219;&#21153;&#65292;&#29992;&#25143;&#24819;&#35201;&#28040;&#36153;&#26377;&#36259;&#30340;&#20869;&#23481;&#65292;&#20363;&#22914;&#33719;&#21462;&#23089;&#20048;&#24615;&#36890;&#30693;&#20869;&#23481;&#12290;&#31532;&#19977;&#31867;&#26159;&#22522;&#30784;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#31038;&#20132;&#32593;&#32476;&#30340;&#20869;&#23481;&#27880;&#37322;&#21644;LLM&#30417;&#25511;&#12290;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#25361;&#25112;&#12289;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#21560;&#21462;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#30740;&#31350;&#20851;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#24212;&#29992;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are transforming the way people generate, explore, and engage with content. We study how we can develop LLM applications for online social networks. Despite LLMs' successes in other domains, it is challenging to develop LLM-based products for social networks for numerous reasons, and it has been relatively under-reported in the research community. We categorize LLM applications for social networks into three categories. First is knowledge tasks where users want to find new knowledge and information, such as search and question-answering. Second is entertainment tasks where users want to consume interesting content, such as getting entertaining notification content. Third is foundational tasks that need to be done to moderate and operate the social networks, such as content annotation and LLM monitoring. For each task, we share the challenges we found, solutions we developed, and lessons we learned. To the best of our knowledge, this is the first comprehensi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38050;&#29748;&#28436;&#22863;&#35780;&#20272;&#20013;&#38899;&#20048;&#24418;&#29366;&#35780;&#20215;&#30340;&#36731;&#37327;&#32423;&#36830;&#20307;&#27531;&#20313;&#31070;&#32463;&#32593;&#32476;&#65288;S-ResNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;S-ResNN&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22810;&#20010;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02566</link><description>&lt;p&gt;
&#38050;&#29748;&#28436;&#22863;&#35780;&#20272;&#20013;&#29992;&#20110;&#38899;&#20048;&#24418;&#29366;&#35780;&#20215;&#30340;&#36830;&#20307;&#27531;&#20313;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Siamese Residual Neural Network for Musical Shape Evaluation in Piano Performance Assessment. (arXiv:2401.02566v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38050;&#29748;&#28436;&#22863;&#35780;&#20272;&#20013;&#38899;&#20048;&#24418;&#29366;&#35780;&#20215;&#30340;&#36731;&#37327;&#32423;&#36830;&#20307;&#27531;&#20313;&#31070;&#32463;&#32593;&#32476;&#65288;S-ResNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;S-ResNN&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22810;&#20010;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#35782;&#21035;&#38899;&#20048;&#24418;&#29366;&#22312;&#38899;&#20048;&#25945;&#32946;&#21644;&#28436;&#22863;&#35780;&#20272;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#20026;&#20102;&#31616;&#21270;&#32791;&#26102;&#19988;&#25104;&#26412;&#39640;&#26114;&#30340;&#38899;&#20048;&#24418;&#29366;&#35780;&#20272;&#36807;&#31243;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#27169;&#22411;&#12290;&#23558;&#38899;&#20048;&#24418;&#29366;&#35780;&#20272;&#35270;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36830;&#20307;&#27531;&#20313;&#31070;&#32463;&#32593;&#32476;&#65288;S-ResNN&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#38899;&#20048;&#24418;&#29366;&#12290;&#20026;&#20102;&#22312;&#38050;&#29748;&#38899;&#20048;&#24418;&#29366;&#35780;&#20272;&#30340;&#32972;&#26223;&#19979;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;147&#20010;&#38050;&#29748;&#39044;&#22791;&#32451;&#20064;&#20013;&#30340;4116&#20010;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#20998;&#20026;28&#20010;&#38899;&#20048;&#24418;&#29366;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;S-ResNN&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22810;&#20010;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and identifying musical shape plays an important role in music education and performance assessment. To simplify the otherwise time- and cost-intensive musical shape evaluation, in this paper we explore how artificial intelligence (AI) driven models can be applied. Considering musical shape evaluation as a classification problem, a light-weight Siamese residual neural network (S-ResNN) is proposed to automatically identify musical shapes. To assess the proposed approach in the context of piano musical shape evaluation, we have generated a new dataset, containing 4116 music pieces derived by 147 piano preparatory exercises and performed in 28 categories of musical shapes. The experimental results show that the S-ResNN significantly outperforms a number of benchmark methods in terms of the precision, recall and F1 score.
&lt;/p&gt;</description></item><item><title>MeTA&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#22810;&#28304;&#27979;&#35797;&#26102;&#36866;&#24212;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#32452;&#21512;&#26435;&#37325;&#36866;&#24212;&#22810;&#20010;&#28304;&#27169;&#22411;&#21040;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2401.02561</link><description>&lt;p&gt;
MeTA: &#22810;&#28304;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
MeTA: Multi-source Test Time Adaptation. (arXiv:2401.02561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02561
&lt;/p&gt;
&lt;p&gt;
MeTA&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#22810;&#28304;&#27979;&#35797;&#26102;&#36866;&#24212;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#32452;&#21512;&#26435;&#37325;&#36866;&#24212;&#22810;&#20010;&#28304;&#27169;&#22411;&#21040;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#36807;&#31243;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#36866;&#24212;&#21040;&#27599;&#20010;&#36827;&#20837;&#30340;&#27979;&#35797;&#25968;&#25454;&#25209;&#27425;&#20013;&#65288;&#21363;&#65292;&#26080;&#38656;&#22823;&#37327;&#30340;&#27979;&#35797;&#25968;&#25454;&#21487;&#29992;&#65292;&#23601;&#20687;&#20256;&#32479;&#39046;&#22495;&#36866;&#24212;&#20013;&#37027;&#26679;&#65289;&#65292;&#24182;&#19988;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#26435;&#38480;&#12290;&#30001;&#20110;&#23427;&#19982;&#27599;&#20010;&#27979;&#35797;&#25968;&#25454;&#25209;&#27425;&#19968;&#36215;&#24037;&#20316;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#38656;&#35201;&#22312;&#25968;&#25454;&#27969;&#20837;&#26102;&#36827;&#34892;&#20915;&#31574;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;&#24403;&#21069;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#28304;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#22810;&#28304;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;MeTA&#65289;&#26694;&#26550;&#65292;&#23427;&#22788;&#29702;&#22810;&#20010;&#28304;&#27169;&#22411;&#24182;&#23558;&#23427;&#20204;&#26368;&#20339;&#32452;&#21512;&#20197;&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#12290;MeTA&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#26377;&#25928;&#22320;&#33719;&#24471;&#26368;&#20339;&#32452;&#21512;&#26435;&#37325;&#65292;&#20197;&#23558;&#28304;&#27169;&#22411;&#32452;&#21512;&#20197;&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;&#23427;&#30830;&#23450;&#35201;&#26356;&#26032;&#21738;&#20010;&#28304;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#20415;&#21482;&#26356;&#26032;&#19982;&#27979;&#35797;&#25968;&#25454;&#26368;&#30456;&#20851;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test time adaptation is the process of adapting, in an unsupervised manner, a pre-trained source model to each incoming batch of the test data (i.e., without requiring a substantial portion of the test data to be available, as in traditional domain adaptation) and without access to the source data. Since it works with each batch of test data, it is well-suited for dynamic environments where decisions need to be made as the data is streaming in. Current test time adaptation methods are primarily focused on a single source model. We propose the first completely unsupervised Multi-source Test Time Adaptation (MeTA) framework that handles multiple source models and optimally combines them to adapt to the test data. MeTA has two distinguishing features. First, it efficiently obtains the optimal combination weights to combine the source models to adapt to the test data distribution. Second, it identifies which of the source model parameters to update so that only the model which is most corr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21463;&#38480;&#22312;&#32447;&#20248;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20915;&#31574;&#31995;&#32479;&#22312;&#23454;&#26102;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#38271;&#26399;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02552</link><description>&lt;p&gt;
&#23454;&#26102;&#20915;&#31574;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;: &#19968;&#31181;&#21463;&#38480;&#22312;&#32447;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Long-term Fairness For Real-time Decision Making: A Constrained Online Optimization Approach. (arXiv:2401.02552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02552
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#22312;&#32447;&#20248;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20915;&#31574;&#31995;&#32479;&#22312;&#23454;&#26102;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#38271;&#26399;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20174;&#39044;&#27979;&#24314;&#27169;&#21040;&#26234;&#33021;&#33258;&#21160;&#21270;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#20063;&#20351;&#24471;&#26377;&#24517;&#35201;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20915;&#31574;&#31995;&#32479;&#19981;&#36829;&#21453;&#25152;&#22312;&#31038;&#20250;&#30340;&#36947;&#24503;&#21407;&#21017;&#21644;&#20215;&#20540;&#35266;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20915;&#31574;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#65292;&#20844;&#24179;&#24615;&#21644;&#20844;&#27491;&#24615;&#30340;&#38656;&#35201;&#25104;&#20026;&#19968;&#39033;&#22522;&#26412;&#20851;&#27880;&#28857;&#12290;&#22312;&#38656;&#35201;&#23454;&#26102;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#20844;&#24179;&#30446;&#26631;&#21464;&#24471;&#26356;&#21152;&#24494;&#22937;&#21644;&#22797;&#26434;&#65306;&#21363;&#26102;&#20844;&#24179;&#20197;&#30830;&#20445;&#27599;&#20010;&#26102;&#38388;&#27573;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#21450;&#38271;&#26399;&#20844;&#24179;&#20197;&#30830;&#20445;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#20844;&#24179;&#24615;&#12290;&#36234;&#26469;&#36234;&#22810;&#20154;&#24847;&#35782;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#38656;&#35201;&#22312;&#38271;&#26102;&#38388;&#20869;&#20445;&#25345;&#20844;&#24179;&#65292;&#38656;&#35201;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#27573;&#19978;&#20445;&#25345;&#20844;&#24179;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#26102;&#38388;&#21464;&#21270;&#25104;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has demonstrated remarkable capabilities across many real-world systems, from predictive modeling to intelligent automation. However, the widespread integration of machine learning also makes it necessary to ensure machine learning-driven decision-making systems do not violate ethical principles and values of society in which they operate. As ML-driven decisions proliferate, particularly in cases involving sensitive attributes such as gender, race, and age, to name a few, the need for equity and impartiality has emerged as a fundamental concern. In situations demanding real-time decision-making, fairness objectives become more nuanced and complex: instantaneous fairness to ensure equity in every time slot, and long-term fairness to ensure fairness over a period of time. There is a growing awareness that real-world systems that operate over long periods and require fairness over different timelines. However, existing approaches mainly address dynamic costs with tim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;SBL&#27169;&#22411;&#36229;&#21442;&#25968;&#20272;&#35745;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#12289;MacKay&#21644;&#20984;&#36793;&#30028;&#31561;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#26368;&#23567;&#21270;&#21644;&#20108;&#27425;&#36924;&#36817;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02544</link><description>&lt;p&gt;
SBL&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Estimation for Sparse Bayesian Learning Models. (arXiv:2401.02544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;SBL&#27169;&#22411;&#36229;&#21442;&#25968;&#20272;&#35745;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#12289;MacKay&#21644;&#20984;&#36793;&#30028;&#31561;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#26368;&#23567;&#21270;&#21644;&#20108;&#27425;&#36924;&#36817;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#65288;SBL&#65289;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23618;&#27425;&#20808;&#39564;&#26469;&#20419;&#36827;&#31232;&#30095;&#24615;&#12290;SBL&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#30456;&#20851;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#21644;&#39640;&#32500;&#24230;&#65292;&#24448;&#24448;&#38590;&#20197;&#20272;&#35745;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;SBL&#27169;&#22411;&#36229;&#21442;&#25968;&#20272;&#35745;&#26694;&#26550;&#65292;&#21253;&#25324;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#12289;MacKay&#21644;&#20984;&#36793;&#30028;&#65288;CB&#65289;&#31639;&#27861;&#31561;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#20132;&#26367;&#26368;&#23567;&#21270;&#21644;&#32447;&#24615;&#21270;&#65288;AML&#65289;&#33539;&#24335;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#35299;&#37322;&#65292;&#20854;&#29305;&#28857;&#26159;&#29420;&#29305;&#30340;&#32447;&#24615;&#21270;&#20195;&#29702;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;AML&#26694;&#26550;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22686;&#24378;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#12290;&#36825;&#19968;&#25928;&#26524;&#36890;&#36807;&#26032;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#21644;&#20108;&#27425;&#36924;&#36817;&#65288;AMQ&#65289;&#33539;&#24335;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;
&lt;/p&gt;
&lt;p&gt;
Sparse Bayesian Learning (SBL) models are extensively used in signal processing and machine learning for promoting sparsity through hierarchical priors. The hyperparameters in SBL models are crucial for the model's performance, but they are often difficult to estimate due to the non-convexity and the high-dimensionality of the associated objective function. This paper presents a comprehensive framework for hyperparameter estimation in SBL models, encompassing well-known algorithms such as the expectation-maximization (EM), MacKay, and convex bounding (CB) algorithms. These algorithms are cohesively interpreted within an alternating minimization and linearization (AML) paradigm, distinguished by their unique linearized surrogate functions. Additionally, a novel algorithm within the AML framework is introduced, showing enhanced efficiency, especially under low signal noise ratios. This is further improved by a new alternating minimization and quadratic approximation (AMQ) paradigm, which
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#38459;&#30861;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#20809;&#21051;&#25104;&#20026;&#29983;&#20135;&#23601;&#32490;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#29983;&#20135;&#23601;&#32490;&#30340;&#26426;&#22120;&#23398;&#20064;-RET&#20462;&#27491;&#12290;</title><link>http://arxiv.org/abs/2401.02536</link><description>&lt;p&gt;
&#21033;&#29992;&#20840;&#26032;&#30340;&#31471;&#21040;&#31471;&#29983;&#20135;&#23601;&#32490;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#36827;&#34892;&#32435;&#31859;&#20809;&#21051;&#24314;&#27169;&#21644;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Novel End-to-End Production-Ready Machine Learning Flow for Nanolithography Modeling and Correction. (arXiv:2401.02536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#38459;&#30861;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#20809;&#21051;&#25104;&#20026;&#29983;&#20135;&#23601;&#32490;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#29983;&#20135;&#23601;&#32490;&#30340;&#26426;&#22120;&#23398;&#20064;-RET&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#21051;&#25216;&#26415;&#26159;&#21322;&#23548;&#20307;&#21046;&#36896;&#30340;&#20027;&#35201;&#25512;&#21160;&#22240;&#32032;&#65292;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;&#22788;&#29702;&#26469;&#25191;&#34892;&#20998;&#36776;&#29575;&#22686;&#24378;&#25216;&#26415;&#65288;RET&#65289;&#20197;&#23558;&#35774;&#35745;&#25968;&#25454;&#36716;&#31227;&#21040;&#24037;&#20316;&#38598;&#25104;&#30005;&#36335;&#65288;ICs&#65289;&#12290;&#30001;&#20110;&#29305;&#24449;&#23610;&#23544;&#30340;&#25345;&#32493;&#20943;&#23567;&#21644;&#33455;&#29255;&#38754;&#31215;&#30340;&#25193;&#22823;&#65292;RET&#20219;&#21153;&#30340;&#22788;&#29702;&#33021;&#21147;&#21644;&#35745;&#31639;&#36816;&#34892;&#26102;&#38388;&#19981;&#26029;&#22686;&#21152;&#12290;&#29616;&#26377;&#30740;&#31350;&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26469;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#21644;&#35745;&#31639;&#21151;&#29575;&#65292;&#20294;&#23427;&#20204;&#30446;&#21069;&#36824;&#26410;&#22312;&#23454;&#38469;&#29983;&#20135;&#20013;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#38459;&#30861;ML&#35745;&#31639;&#20809;&#21051;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#29983;&#20135;&#23601;&#32490;&#30340;ML-RET&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical lithography is the main enabler to semiconductor manufacturing. It requires extensive processing to perform the Resolution Enhancement Techniques (RETs) required to transfer the design data to a working Integrated Circuits (ICs). The processing power and computational runtime for RETs tasks is ever increasing due to the continuous reduction of the feature size and the expansion of the chip area. State-of-the-art research sought Machine Learning (ML) technologies to reduce runtime and computational power, however they are still not used in production yet. In this study, we analyze the reasons holding back ML computational lithography from being production ready and present a novel highly scalable end-to-end flow that enables production ready ML-RET correction.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#25903;&#65292;&#23558;&#20998;&#31867;&#20449;&#24687;&#34701;&#20837;&#21040;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02526</link><description>&lt;p&gt;
&#20998;&#25903;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Branched Variational Autoencoder Classifiers. (arXiv:2401.02526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02526
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#25903;&#65292;&#23558;&#20998;&#31867;&#20449;&#24687;&#34701;&#20837;&#21040;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAEs)&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#25903;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340;&#20998;&#25903;VAE&#65288;BVAE&#65289;&#36890;&#36807;&#24635;&#25439;&#22833;&#21521;&#20998;&#31867;&#32452;&#20214;&#36129;&#29486;&#20102;&#22522;&#20110;&#31867;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#28508;&#22312;&#34920;&#31034;&#36171;&#20104;&#20102;&#20998;&#31867;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#36755;&#20837;&#31867;&#21035;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#24067;&#34987;&#20998;&#31163;&#21644;&#25490;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#37319;&#29992;&#22522;&#20934;MNIST&#25968;&#25454;&#38598;&#23545;&#26410;&#26059;&#36716;&#21644;&#26059;&#36716;&#25968;&#23383;&#36827;&#34892;&#25968;&#20540;&#35745;&#31639;&#65292;&#37327;&#21270;&#20102;&#25913;&#36827;&#30340;&#31243;&#24230;&#12290;&#25552;&#20986;&#30340;&#25216;&#26415;&#28982;&#21518;&#19982;&#22266;&#23450;&#36755;&#20986;&#20998;&#24067;&#30340;VAE&#36827;&#34892;&#27604;&#36739;&#21644;&#25972;&#21512;&#12290;&#36825;&#20010;&#36807;&#31243;&#34987;&#21457;&#29616;&#23545;&#24191;&#27867;&#30340;&#36755;&#20986;&#20998;&#24067;&#33021;&#22815;&#25552;&#20379;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a modified variational autoencoder (VAEs) that contains an additional neural network branch. The resulting branched VAE (BVAE) contributes a classification component based on the class labels to the total loss and therefore imparts categorical information to the latent representation. As a result, the latent space distributions of the input classes are separated and ordered, thereby enhancing the classification accuracy. The degree of improvement is quantified by numerical calculations employing the benchmark MNIST dataset for both unrotated and rotated digits. The proposed technique is then compared to and then incorporated into a VAE with fixed output distributions. This procedure is found to yield improved performance for a wide range of output distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20027;&#35201;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;</title><link>http://arxiv.org/abs/2401.02524</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#21512;&#25506;&#32034;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20027;&#35201;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#26114;&#21644;&#38544;&#31169;&#27861;&#35268;&#30340;&#38480;&#21046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#36827;&#23637;&#12290;&#21512;&#25104;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21457;&#24067;&#30340;&#27169;&#22411;&#36807;&#22810;&#21644;&#26377;&#38480;&#30340;&#32508;&#36848;&#25991;&#29486;&#32473;&#20915;&#31574;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#30830;&#23450;&#20102;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#36235;&#21183;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20026;&#20027;&#35201;&#36235;&#21183;&#65292;&#38500;&#20102;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#29983;&#25104;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#26159;GAN&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#12289;&#36716;&#25442;&#22120;&#21644;RNN&#20063;&#22312;&#31454;&#20105;&#20013;&#12290;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20849;&#21516;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a surge in the popularity of Machine Learning (ML), applied across diverse domains. However, progress is impeded by the scarcity of training data due to expensive acquisition and privacy legislation. Synthetic data emerges as a solution, but the abundance of released models and limited overview literature pose challenges for decision-making. This work surveys 417 Synthetic Data Generation (SDG) models over the last decade, providing a comprehensive overview of model types, functionality, and improvements. Common attributes are identified, leading to a classification and trend analysis. The findings reveal increased model performance and complexity, with neural network-based approaches prevailing, except for privacy-preserving data generation. Computer vision dominates, with GANs as primary generative models, while diffusion models, transformers, and RNNs compete. Implications from our performance evaluation highlight the scarcity of common metrics and datase
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#26234;&#33021;&#25968;&#23383;&#23402;&#29983;&#20307; (SDTs) &#30340;&#21457;&#23637;&#26041;&#27861;&#21644;&#25361;&#25112;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#36890;&#36807;&#25345;&#32493;&#21516;&#21270;&#22270;&#20687;&#25968;&#25454;&#26469;&#35266;&#23519;&#21644;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#21644;&#23454;&#29616;SDTs&#30340;&#28145;&#24230;&#23398;&#20064; (DL) &#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.02523</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#26234;&#33021;&#25968;&#23383;&#23402;&#29983;&#20307;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Image-based Deep Learning for Smart Digital Twins: a Review. (arXiv:2401.02523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#26234;&#33021;&#25968;&#23383;&#23402;&#29983;&#20307; (SDTs) &#30340;&#21457;&#23637;&#26041;&#27861;&#21644;&#25361;&#25112;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#36890;&#36807;&#25345;&#32493;&#21516;&#21270;&#22270;&#20687;&#25968;&#25454;&#26469;&#35266;&#23519;&#21644;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#21644;&#23454;&#29616;SDTs&#30340;&#28145;&#24230;&#23398;&#20064; (DL) &#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25968;&#23383;&#23402;&#29983;&#20307;(SDTs)&#36890;&#36807;&#25345;&#32493;&#25968;&#25454;&#21516;&#21270;&#26469;&#34394;&#25311;&#22797;&#21046;&#21644;&#39044;&#27979;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#36890;&#36807;&#25511;&#21046;&#31995;&#32479;&#30340;&#34892;&#20026;&#26469;&#20248;&#21270;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;(DL)&#27169;&#22411;&#26174;&#33879;&#22686;&#24378;&#20102;SDTs&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#39044;&#27979;&#24615;&#32500;&#20462;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#20248;&#21270;&#31561;&#20219;&#21153;&#19978;&#12290;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#23398;&#12289;&#24037;&#31243;&#21644;&#25945;&#32946;&#65292;&#22312;&#35266;&#23519;&#21644;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#21644;&#25511;&#21046;&#20854;&#34892;&#20026;&#26041;&#38754;&#65292;SDTs&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;(&#22522;&#20110;&#22270;&#20687;&#30340;SDTs)&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#24320;&#21457;&#22522;&#20110;&#22270;&#20687;&#30340;SDTs&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#30456;&#20851;&#25361;&#25112;&#65292;&#21253;&#25324;&#25345;&#32493;&#21516;&#21270;&#26469;&#33258;&#29289;&#29702;&#31995;&#32479;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#35774;&#35745;&#21644;&#23454;&#29616;SDTs&#30340;DL&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#33719;&#21462;&#12289;&#22788;&#29702;&#21644;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart Digital twins (SDTs) are being increasingly used to virtually replicate and predict the behaviors of complex physical systems through continual data assimilation enabling the optimization of the performance of these systems by controlling the actions of systems. Recently, deep learning (DL) models have significantly enhanced the capabilities of SDTs, particularly for tasks such as predictive maintenance, anomaly detection, and optimization. In many domains, including medicine, engineering, and education, SDTs use image data (image-based SDTs) to observe and learn system behaviors and control their behaviors. This paper focuses on various approaches and associated challenges in developing image-based SDTs by continually assimilating image data from physical systems. The paper also discusses the challenges involved in designing and implementing DL models for SDTs, including data acquisition, processing, and interpretation. In addition, insights into the future directions and opport
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#36827;&#34892;&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#30340;&#32039;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#35770;&#36848;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02520</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#23398;&#20064;&#19982;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel. (arXiv:2401.02520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#36827;&#34892;&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#30340;&#32039;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#35770;&#36848;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#36890;&#24120;&#22312;&#24378;&#22122;&#22768;&#20381;&#36182;&#20551;&#35774;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#32771;&#34385;&#22122;&#22768;&#20302;&#31209;&#21152;&#31232;&#30095;&#30697;&#38453;&#24674;&#22797;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#20854;&#20013;&#22122;&#22768;&#30697;&#38453;&#21487;&#20197;&#26469;&#33258;&#20219;&#24847;&#20855;&#26377;&#20803;&#32032;&#38388;&#20219;&#24847;&#20381;&#36182;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20851;&#30456;&#20301;&#32422;&#26463;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23427;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#37117;&#26159;&#32039;&#33268;&#30340;&#65292;&#26082;&#28385;&#36275;&#30830;&#23450;&#24615;&#19979;&#30028;&#21448;&#21305;&#37197;&#26368;&#23567;&#21270;&#39118;&#38505;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#26029;&#35328;&#20004;&#20010;&#20219;&#24847;&#30340;&#20302;&#31209;&#26080;&#20851;&#30697;&#38453;&#20043;&#38388;&#30340;&#24046;&#24322;&#24517;&#39035;&#22312;&#20854;&#20803;&#32032;&#19978;&#25193;&#25955;&#33021;&#37327;&#65292;&#25442;&#21477;&#35805;&#35828;&#19981;&#33021;&#22826;&#31232;&#30095;&#65292;&#36825;&#25581;&#31034;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#21487;&#33021;&#24341;&#36215;&#29420;&#31435;&#20852;&#36259;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#22312;&#20960;&#20010;&#37325;&#35201;&#30340;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#20272;&#35745;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#30340;&#38382;&#39064;&#20013;&#65292;&#37319;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of structured matrix estimation has been studied mostly under strong noise dependence assumptions. This paper considers a general framework of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come from any joint distribution with arbitrary dependence across entries. We propose an incoherent-constrained least-square estimator and prove its tightness both in the sense of deterministic lower bound and matching minimax risks under various noise distributions. To attain this, we establish a novel result asserting that the difference between two arbitrary low-rank incoherent matrices must spread energy out across its entries, in other words cannot be too sparse, which sheds light on the structure of incoherent low-rank matrices and may be of independent interest. We then showcase the applications of our framework to several important statistical machine learning problems. In the problem of estimating a structured Markov transition kernel, the proposed method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#22686;&#30410;&#35843;&#24230;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#38750;&#32447;&#24615;&#24490;&#29615;&#30340;&#36755;&#36816;PDE&#65292;&#24182;&#22312;&#23616;&#37096;&#23454;&#29616;&#20102;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2401.02511</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#30340;&#22686;&#30410;&#35843;&#24230;&#26041;&#27861;&#22788;&#29702;&#20855;&#26377;&#38750;&#32447;&#24615;&#24490;&#29615;&#30340;&#36755;&#36816;PDE
&lt;/p&gt;
&lt;p&gt;
Gain Scheduling with a Neural Operator for a Transport PDE with Nonlinear Recirculation. (arXiv:2401.02511v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#22686;&#30410;&#35843;&#24230;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#38750;&#32447;&#24615;&#24490;&#29615;&#30340;&#36755;&#36816;PDE&#65292;&#24182;&#22312;&#23616;&#37096;&#23454;&#29616;&#20102;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#31283;&#23450;PDE&#27169;&#22411;&#65292;&#25511;&#21046;&#24459;&#38656;&#35201;&#36890;&#36807;&#38750;&#32447;&#24615;&#31639;&#23376;&#23558;&#31354;&#38388;&#30456;&#20851;&#30340;&#22686;&#30410;&#26144;&#23556;&#21040;PDE&#20989;&#25968;&#31995;&#25968;&#19978;&#12290;&#24403;PDE&#26159;&#38750;&#32447;&#24615;&#30340;&#19988;"&#20266;&#31995;&#25968;"&#20989;&#25968;&#20381;&#36182;&#20110;&#29366;&#24577;&#26102;&#65292;&#22686;&#30410;&#35843;&#24230;&#38750;&#32447;&#24615;&#35774;&#35745;&#26159;&#22788;&#29702;&#38750;&#32447;&#24615;&#21453;&#39304;&#35774;&#35745;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#12290;PDE&#22238;&#28335;&#30340;&#22686;&#30410;&#35843;&#24230;&#29256;&#26412;&#21033;&#29992;&#22312;&#27599;&#20010;&#29366;&#24577;&#20540;&#22788;&#27714;&#35299;PDE&#24471;&#21040;&#30340;&#22686;&#30410;&#12290;&#20294;&#26159;&#22312;&#23454;&#26102;&#24773;&#20917;&#19979;&#36827;&#34892;&#36825;&#31181;PDE&#35745;&#31639;&#21487;&#33021;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;NO&#65289;&#21487;&#20197;&#22312;&#27599;&#20010;&#29366;&#24577;&#20540;&#19978;&#24555;&#36895;&#19988;&#23454;&#26102;&#22320;&#35757;&#32451;&#65292;&#20135;&#29983;&#22686;&#30410;&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#27714;&#35299;PDE&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;NO&#29992;&#20110;GS-PDE&#22238;&#28335;&#30340;&#26041;&#27861;&#12290;GS&#25511;&#21046;&#22120;&#20551;&#35774;&#29366;&#24577;&#21464;&#21270;&#32531;&#24930;&#65292;&#24182;&#19988;&#20165;&#20445;&#35777;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;ODE&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36890;&#36807;"&#20840;&#26680;"&#26041;&#27861;&#21644;"&#20165;&#22686;&#30410;"&#26041;&#27861;&#65292;&#30830;&#31435;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#24490;&#29615;&#30340;&#21452;&#26354;&#22411;PDE&#30340;&#23616;&#37096;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To stabilize PDE models, control laws require space-dependent functional gains mapped by nonlinear operators from the PDE functional coefficients. When a PDE is nonlinear and its "pseudo-coefficient" functions are state-dependent, a gain-scheduling (GS) nonlinear design is the simplest approach to the design of nonlinear feedback. The GS version of PDE backstepping employs gains obtained by solving a PDE at each value of the state. Performing such PDE computations in real time may be prohibitive. The recently introduced neural operators (NO) can be trained to produce the gain functions, rapidly in real time, for each state value, without requiring a PDE solution. In this paper we introduce NOs for GS-PDE backstepping. GS controllers act on the premise that the state change is slow and, as a result, guarantee only local stability, even for ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear recirculation using both a "full-kernel" approach and the "gain-only" approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#33539;&#24182;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#65292;&#29992;&#20110;&#26356;&#26032;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.02508</link><description>&lt;p&gt;
&#22312;&#20915;&#31574;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#19968;&#31181;&#36866;&#24212;&#24615;&#21644;&#21487;&#25512;&#24191;&#30340;&#20248;&#21270;&#24341;&#25806;&#65306;&#19968;&#31181;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards an Adaptable and Generalizable Optimization Engine in Decision and Control: A Meta Reinforcement Learning Approach. (arXiv:2401.02508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#33539;&#24182;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#65292;&#29992;&#20110;&#26356;&#26032;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22312;&#20855;&#26377;&#38750;&#24179;&#28369;&#31995;&#32479;&#21160;&#21147;&#23398;&#21644;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#25552;&#20986;&#36890;&#36807;&#23398;&#20064;&#25110;&#24494;&#35843;&#21160;&#21147;&#23398;/&#25104;&#26412;&#20989;&#25968;&#25110;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#26356;&#26032;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#26469;&#25913;&#36827;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#22120;&#26469;&#26356;&#26032;&#25511;&#21046;&#22120;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#33539;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38754;&#23545;&#26410;&#30693;&#29615;&#22659;&#26102;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#65288;&#20363;&#22914;&#65292;&#23569;&#37327;&#31034;&#33539;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling-based model predictive control (MPC) has found significant success in optimal control problems with non-smooth system dynamics and cost function. Many machine learning-based works proposed to improve MPC by a) learning or fine-tuning the dynamics/ cost function, or b) learning to optimize for the update of the MPC controllers. For the latter, imitation learning-based optimizers are trained to update the MPC controller by mimicking the expert demonstrations, which, however, are expensive or even unavailable. More significantly, many sequential decision-making problems are in non-stationary environments, requiring that an optimizer should be adaptable and generalizable to update the MPC controller for solving different tasks. To address those issues, we propose to learn an optimizer based on meta-reinforcement learning (RL) to update the controllers. This optimizer does not need expert demonstration and can enable fast adaptation (e.g., few-shots) when it is deployed in unseen c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02501</link><description>&lt;p&gt;
&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#32467;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;$(x,y,z,channel,time)$&#35270;&#39057;&#26174;&#31034;&#20102;&#32454;&#32990;&#36816;&#21160;&#21644;&#20449;&#21495;&#21160;&#21147;&#23398;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#19968;&#31181;&#22312;&#20116;&#32500;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#19981;&#38656;&#35201;&#39044;&#20808;&#20102;&#35299;&#39044;&#26399;&#30340;&#27169;&#24335;&#21160;&#21147;&#23398;&#20197;&#21450;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#26159;&#19968;&#31181;Kolmogorov&#32467;&#26500;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#26680;&#24515;&#21306;&#22495;&#30456;&#23545;&#20110;&#21608;&#22260;&#32454;&#32990;&#36136;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#26469;&#26368;&#20248;&#22320;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24230;&#37327;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;NCD&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#22312;&#20302;&#32500;&#23884;&#20837;&#20013;&#20316;&#20026;&#28857;&#30340;Hilbert&#31354;&#38388;&#30340;&#20877;&#29983;&#26680;&#65292;&#21487;&#20197;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display patterns of cellular motion and signaling dynamics. We present here an approach to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell microscopy movies unique in requiring no \emph{a priori} knowledge of expected pattern dynamics, and no training data. The proposed cell signaling structure function (SSF) is a Kolmogorov structure function that optimally measures cell signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement compared to the current state-of-the-art cytonuclear ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value, or a functional output such as velocity. Patterns of similarity are identified via the metric normalized compression distance (NCD). The NCD is a reproducing kernel for a Hilbert space that represents the input SSF kymographs as points in a low dimensional embedding that optimally captures the pattern
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#27668;&#20505;&#21464;&#21270;&#23545;&#27745;&#27700;&#31649;&#29702;&#24102;&#26469;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24223;&#27700;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#20851;&#38190;&#27700;&#20301;&#28857;&#24182;&#25913;&#21892;&#24223;&#27700;&#31649;&#29702;&#21644;&#29615;&#22659;&#27745;&#26579;&#39044;&#38450;&#12290;</title><link>http://arxiv.org/abs/2401.02465</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#29992;&#20110;&#32508;&#21512;&#19979;&#27700;&#36947;&#28322;&#27969;&#30340;&#24223;&#27700;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Interpretable Time Series Models for Wastewater Modeling in Combined Sewer Overflows. (arXiv:2401.02465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#27668;&#20505;&#21464;&#21270;&#23545;&#27745;&#27700;&#31649;&#29702;&#24102;&#26469;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24223;&#27700;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#20851;&#38190;&#27700;&#20301;&#28857;&#24182;&#25913;&#21892;&#24223;&#27700;&#31649;&#29702;&#21644;&#29615;&#22659;&#27745;&#26579;&#39044;&#38450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#23545;&#25105;&#20204;&#30340;&#31038;&#20250;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#27946;&#27700;&#12289;&#37326;&#28779;&#25110;&#24178;&#26097;&#31561;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#21464;&#24471;&#36234;&#26469;&#36234;&#39057;&#32321;&#12289;&#31361;&#21457;&#19988;&#38590;&#20197;&#39044;&#35265;&#25110;&#24212;&#23545;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#35299;&#20915;&#20102;&#37325;&#38632;&#20107;&#20214;&#23548;&#33268;&#19979;&#38632;&#32592;&#28322;&#20986;&#20197;&#21518;&#65292;&#27745;&#27700;&#27745;&#26579;&#22320;&#34920;&#27700;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#22312;&#22914;&#20309;&#39044;&#27979;&#36825;&#20123;&#20020;&#30028;&#27700;&#20301;&#28857;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#20197;&#20415;&#33021;&#22815;&#21450;&#26102;&#23558;&#22810;&#20313;&#30340;&#27700;&#37325;&#26032;&#20998;&#37197;&#21040;&#19979;&#27700;&#36947;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#20026;&#25913;&#21892;&#24223;&#27700;&#31649;&#29702;&#21644;&#39044;&#38450;&#19979;&#27700;&#36947;&#31995;&#32479;&#23545;&#29615;&#22659;&#30340;&#27745;&#26579;&#20570;&#20986;&#36129;&#29486;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#23454;&#39564;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#23384;&#20648;&#24211;&#20013;&#25214;&#21040;&#65306;https://github.com/TeodorChiaburu/RIWWER_TimeSeries&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change poses increasingly complex challenges to our society. Extreme weather events such as floods, wild fires or droughts are becoming more frequent, spontaneous and difficult to foresee or counteract. In this work we specifically address the problem of sewage water polluting surface water bodies after spilling over from rain tanks as a consequence of heavy rain events. We investigate to what extent state-of-the-art interpretable time series models can help predict such critical water level points, so that the excess can promptly be redistributed across the sewage network. Our results indicate that modern time series models can contribute to better waste water management and prevention of environmental pollution from sewer systems. All the code and experiments can be found in our repository: https://github.com/TeodorChiaburu/RIWWER_TimeSeries.
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#26377;&#26395;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02458</link><description>&lt;p&gt;
&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02458
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#26377;&#26395;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#22871;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#20986;&#29616;&#20026;&#35745;&#31639;&#21307;&#30103;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#26426;&#36935;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#24615;&#30001;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#20154;&#31867;&#25351;&#20196;&#24341;&#23548;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#24378;&#35843;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#24449;&#12289;&#36136;&#37327;&#21644;&#35268;&#27169;&#12290;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#33719;&#21462;&#21644;&#22788;&#29702;&#39640;&#36136;&#37327;&#30340;&#20020;&#24202;&#25968;&#25454;&#35760;&#24405;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#25968;&#25454;&#25968;&#37327;&#12289;&#27880;&#37322;&#12289;&#24739;&#32773;&#38544;&#31169;&#21644;&#20262;&#29702;&#31561;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FM&#26102;&#20195;&#30340;&#24191;&#27867;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#65288;&#20174;&#27169;&#22411;&#39044;&#35757;&#32451;&#21040;&#25512;&#29702;&#65289;&#65292;&#20197;&#25913;&#36827;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#20197;&#21450;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#22312;&#21307;&#30103;&#21644;&#21307;&#33647;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#26684;&#23616;&#20013;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#30340;&#21069;&#26223;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#21307;&#30103;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare
&lt;/p&gt;</description></item><item><title>eCIL-MU&#26159;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#25216;&#26415;&#30340;&#36880;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#30340;&#38750;&#30772;&#22351;&#24615;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24555;&#36895;&#33719;&#21462;&#20851;&#20110;&#26032;&#31867;&#21035;&#30340;&#30693;&#35782;&#65292;&#24182;&#28040;&#38500;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.02457</link><description>&lt;p&gt;
eCIL-MU: &#22522;&#20110;&#23884;&#20837;&#30340;&#36880;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning. (arXiv:2401.02457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02457
&lt;/p&gt;
&lt;p&gt;
eCIL-MU&#26159;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#25216;&#26415;&#30340;&#36880;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#30340;&#38750;&#30772;&#22351;&#24615;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24555;&#36895;&#33719;&#21462;&#20851;&#20110;&#26032;&#31867;&#21035;&#30340;&#30693;&#35782;&#65292;&#24182;&#28040;&#38500;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#21487;&#33021;&#20250;&#19981;&#26029;&#24341;&#20837;&#26032;&#30340;&#31867;&#21035;&#65292;&#25110;&#32773;&#38656;&#35201;&#23545;&#29616;&#26377;&#31867;&#21035;&#36827;&#34892;&#37325;&#26032;&#20998;&#31867;&#12290;&#36880;&#31867;&#22686;&#37327;&#23398;&#20064; (CIL) &#29992;&#20110;&#22312;&#33719;&#21462;&#20851;&#20110;&#26032;&#31867;&#21035;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#20445;&#30041;&#23545;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#36866;&#24212;&#37325;&#26032;&#20998;&#31867;&#65292;&#36824;&#21487;&#33021;&#38656;&#35201;&#28040;&#38500;&#30456;&#20851;&#31867;&#21035;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;CIL&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#31867;&#21035;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064; (MU)&#12290;&#36890;&#24120;&#65292;MU&#26041;&#27861;&#20542;&#21521;&#20110;&#32791;&#26102;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36830;&#32493;&#30340;&#21462;&#28040;&#23398;&#20064;&#35831;&#27714;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30772;&#22351;&#24615;&#30340;&#22522;&#20110;&#23884;&#20837;&#25216;&#26415;&#30340;eCIL-MU&#26694;&#26550;&#65292;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#21521;&#37327;&#24182;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;CIL&#21644;MU&#20219;&#21153;&#20043;&#38388;&#30340;&#37325;&#21472;&#26469;&#21152;&#36895;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23454;&#29616;&#21462;&#28040;&#23398;&#20064;&#25928;&#26524;&#21644;&#25968;&#37327;&#32423;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
New categories may be introduced over time, or existing categories may need to be reclassified. Class incremental learning (CIL) is employed for the gradual acquisition of knowledge about new categories while preserving information about previously learned ones in such dynamic environments. It might also be necessary to also eliminate the influence of related categories on the model to adapt to reclassification. We thus introduce class-level machine unlearning (MU) within CIL. Typically, MU methods tend to be time-consuming and can potentially harm the model's performance. A continuous stream of unlearning requests could lead to catastrophic forgetting. To address these issues, we propose a non-destructive eCIL-MU framework based on embedding techniques to map data into vectors and then be stored in vector databases. Our approach exploits the overlap between CIL and MU tasks for acceleration. Experiments demonstrate the capability of achieving unlearning effectiveness and orders of mag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#25506;&#35752;&#20102;AI&#25903;&#25345;&#30340;&#26080;&#20154;&#26426;&#31995;&#32479;&#22312;&#28779;&#28798;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#28779;&#28798;&#21069;&#21040;&#20027;&#21160;&#28779;&#28798;&#38454;&#27573;&#20877;&#21040;&#28779;&#28798;&#21518;&#65292;&#36890;&#36807;&#25972;&#21512;&#26080;&#20154;&#26426;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#37326;&#28779;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.02456</link><description>&lt;p&gt;
&#38754;&#21521;&#28779;&#28798;&#31649;&#29702;&#30340;AI&#25903;&#25345;&#26080;&#20154;&#26426;&#31995;&#32479;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management. (arXiv:2401.02456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#25506;&#35752;&#20102;AI&#25903;&#25345;&#30340;&#26080;&#20154;&#26426;&#31995;&#32479;&#22312;&#28779;&#28798;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#28779;&#28798;&#21069;&#21040;&#20027;&#21160;&#28779;&#28798;&#38454;&#27573;&#20877;&#21040;&#28779;&#28798;&#21518;&#65292;&#36890;&#36807;&#25972;&#21512;&#26080;&#20154;&#26426;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#37326;&#28779;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37326;&#28779;&#24050;&#25104;&#20026;&#20840;&#29699;&#26368;&#20855;&#30772;&#22351;&#24615;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#32473;&#20154;&#31867;&#29983;&#21629;&#21644;&#26862;&#26519;&#37326;&#29983;&#21160;&#29289;&#36896;&#25104;&#20102;&#28798;&#38590;&#24615;&#25439;&#22833;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#37326;&#28779;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#21019;&#36896;&#20102;&#23454;&#26045;&#21644;&#21457;&#23637;&#26356;&#26377;&#25928;&#30340;&#37326;&#28779;&#31649;&#29702;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#21160;&#21147;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#30340;&#35843;&#26597;&#35770;&#25991;&#24050;&#25506;&#35752;&#20102;&#21508;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#26126;&#26174;&#32570;&#20047;&#19968;&#31687;&#20840;&#38754;&#32508;&#36848;&#65292;&#24378;&#35843;AI&#25903;&#25345;&#30340;UAV&#31995;&#32479;&#22312;&#22810;&#38454;&#27573;&#37326;&#28779;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#21518;&#32493;&#24433;&#21709;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#36890;&#36807;&#31995;&#32479;&#22238;&#39038;&#26368;&#26032;&#30340;&#25216;&#26415;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20174;&#28779;&#28798;&#21069;&#21040;&#20027;&#21160;&#28779;&#28798;&#38454;&#27573;&#20877;&#21040;&#28779;&#28798;&#21518;&#31649;&#29702;&#30340;UAV&#31995;&#32479;&#21644;AI&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#36965;&#24863;&#31995;&#32479;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfires have emerged as one of the most destructive natural disasters worldwide, causing catastrophic losses in both human lives and forest wildlife. Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models, has created an unprecedented momentum to implement and develop more effective wildfire management. Although some of the existing survey papers have explored various learning-based approaches, a comprehensive review emphasizing the application of AI-enabled UAV systems and their subsequent impact on multi-stage wildfire management is notably lacking. This survey aims to bridge these gaps by offering a systematic review of the recent state-of-the-art technologies, highlighting the advancements of UAV systems and AI models from pre-fire, through the active-fire stage, to post-fire management. To this aim, we provide an extensive analysis of the existing remote sensing systems with a parti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#29305;&#24449;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#20915;&#23450;&#27880;&#20837;&#30340;&#22122;&#22768;&#20540;&#65292;&#20197;&#24179;&#34913;&#38544;&#31169;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02453</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;: &#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Differential Privacy in Federated Learning: A Priority-Based Approach. (arXiv:2401.02453v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#29305;&#24449;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#20915;&#23450;&#27880;&#20837;&#30340;&#22122;&#22768;&#20540;&#65292;&#20197;&#24179;&#34913;&#38544;&#31169;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#39046;&#22495;&#20043;&#19968;&#65292;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#31169;&#26377;&#36807;&#31243;&#24320;&#21457;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#35775;&#38382;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#30340;&#27169;&#22411;&#26356;&#26032;&#65288;&#20363;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26799;&#24230;&#26356;&#26032;&#65289;&#21487;&#33021;&#21521;&#23545;&#25163;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#36890;&#36807;&#21521;&#21442;&#25968;&#20013;&#28155;&#21152;&#19968;&#23450;&#37327;&#30340;&#22122;&#22768;&#26469;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#38544;&#31169;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#22122;&#22768;&#30340;&#20171;&#20837;&#65292;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#22312;&#22122;&#22768;&#27880;&#20837;&#21644;&#29306;&#29298;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#25214;&#21040;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#37319;&#29992;&#33258;&#36866;&#24212;&#22122;&#22768;&#27880;&#20837;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#29305;&#24449;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#20915;&#23450;&#27880;&#20837;&#22122;&#22768;&#30340;&#20540;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#29305;&#24449;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#28982;&#21518;&#22522;&#20110;&#27492;&#23545;&#27169;&#22411;&#30340;&#26435;&#37325;&#36827;&#34892;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) as one of the novel branches of distributed machine learning (ML), develops global models through a private procedure without direct access to local datasets. However, access to model updates (e.g. gradient updates in deep neural networks) transferred between clients and servers can reveal sensitive information to adversaries. Differential privacy (DP) offers a framework that gives a privacy guarantee by adding certain amounts of noise to parameters. This approach, although being effective in terms of privacy, adversely affects model performance due to noise involvement. Hence, it is always needed to find a balance between noise injection and the sacrificed accuracy. To address this challenge, we propose adaptive noise addition in FL which decides the value of injected noise based on features' relative importance. Here, we first propose two effective methods for prioritizing features in deep neural network models and then perturb models' weights based on this in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#35268;&#21017;&#26469;&#28304;&#30340;&#26234;&#33021;&#23478;&#23621;&#33258;&#21160;&#21270;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#31649;&#29702;&#35268;&#21017;&#30340;&#36807;&#31243;&#21644;&#26426;&#26500;&#12289;&#39640;&#32423;&#20915;&#31574;&#19982;&#23454;&#29616;&#32454;&#33410;&#30340;&#20998;&#31163;&#20197;&#21450;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#23433;&#20840;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.02451</link><description>&lt;p&gt;
&#22810;&#35268;&#21017;&#26469;&#28304;&#30340;&#26234;&#33021;&#23478;&#23621;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automation of Smart Homes with Multiple Rule Sources. (arXiv:2401.02451v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#35268;&#21017;&#26469;&#28304;&#30340;&#26234;&#33021;&#23478;&#23621;&#33258;&#21160;&#21270;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#31649;&#29702;&#35268;&#21017;&#30340;&#36807;&#31243;&#21644;&#26426;&#26500;&#12289;&#39640;&#32423;&#20915;&#31574;&#19982;&#23454;&#29616;&#32454;&#33410;&#30340;&#20998;&#31163;&#20197;&#21450;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#23433;&#20840;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35268;&#21017;&#36827;&#34892;&#23478;&#23621;&#33258;&#21160;&#21270;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#38500;&#20102;&#23621;&#27665;&#22806;&#36824;&#26377;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#22914;&#25151;&#20027;&#12289;&#24403;&#22320;&#25919;&#24220;&#12289;&#33021;&#28304;&#20379;&#24212;&#21830;&#21644;&#31995;&#32479;&#25552;&#20379;&#21830;&#65292;&#20182;&#20204;&#24076;&#26395;&#36129;&#29486;&#35268;&#21017;&#20197;&#20445;&#25252;&#33258;&#24049;&#30340;&#21033;&#30410;&#12290;&#31649;&#29702;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#35268;&#21017;&#38656;&#35201;&#19968;&#20010;&#26377;&#32467;&#26500;&#30340;&#36807;&#31243;&#12289;&#30456;&#20851;&#25919;&#31574;&#21644;&#25351;&#23450;&#30340;&#26426;&#26500;&#26469;&#30830;&#20445;&#25480;&#26435;&#21644;&#27491;&#30830;&#30340;&#36129;&#29486;&#65292;&#24182;&#35299;&#20915;&#28508;&#22312;&#30340;&#20914;&#31361;&#12290;&#27492;&#22806;&#65292;&#26234;&#33021;&#23478;&#23621;&#35268;&#21017;&#35821;&#35328;&#38656;&#35201;&#20197;&#39640;&#24230;&#25277;&#35937;&#30340;&#26041;&#24335;&#34920;&#36798;&#26465;&#20214;&#21644;&#20915;&#31574;&#65292;&#32780;&#19981;&#25351;&#23450;&#23454;&#29616;&#32454;&#33410;&#65292;&#22914;&#25509;&#21475;&#12289;&#35775;&#38382;&#21327;&#35758;&#21644;&#25151;&#38388;&#24067;&#23616;&#12290;&#23558;&#39640;&#32423;&#20915;&#31574;&#19982;&#36825;&#20123;&#32454;&#33410;&#20998;&#31163;&#25903;&#25345;&#35268;&#21017;&#23545;&#31867;&#20284;&#23478;&#23621;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#36825;&#31181;&#20998;&#31163;&#36824;&#23545;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#21644;&#23433;&#20840;&#26550;&#26500;&#30340;&#32467;&#26500;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#31995;&#32479;&#23454;&#29616;&#24341;&#20837;&#20102;&#19968;&#20010;&#35268;&#21017;&#31649;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using rules for home automation presents several challenges, especially when considering multiple stakeholders in addition to residents, such as homeowners, local authorities, energy suppliers, and system providers, who will wish to contribute rules to safeguard their interests. Managing rules from various sources requires a structured procedure, a relevant policy, and a designated authority to ensure authorized and correct contributions and address potential conflicts. In addition, the smart home rule language needs to express conditions and decisions at a high level of abstraction without specifying implementation details such as interfaces, access protocols, and room layout. Decoupling high-level decisions from these details supports the transferability and adaptability of rules to similar homes. This separation also has important implications for structuring the smart home system and the security architecture. Our proposed approach and system implementation introduce a rule managem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#30340;&#21327;&#20316;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#27450;&#35784;&#39044;&#38450;&#31995;&#32479;&#20013;&#26500;&#24314;&#23433;&#20840;&#30340;&#25968;&#25454;&#21457;&#24067;&#26426;&#21046;&#65292;&#20197;&#25903;&#25345;&#22806;&#37096;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#23545;&#22810;&#31181;&#25915;&#20987;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02450</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#27450;&#35784;&#39044;&#38450;&#31995;&#32479;&#20013;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems. (arXiv:2401.02450v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#30340;&#21327;&#20316;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#27450;&#35784;&#39044;&#38450;&#31995;&#32479;&#20013;&#26500;&#24314;&#23433;&#20840;&#30340;&#25968;&#25454;&#21457;&#24067;&#26426;&#21046;&#65292;&#20197;&#25903;&#25345;&#22806;&#37096;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#23545;&#22810;&#31181;&#25915;&#20987;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#37329;&#34701;&#29359;&#32618;&#27963;&#21160;&#20419;&#20351;&#27450;&#35784;&#39044;&#38450;&#20013;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25285;&#24515;&#24847;&#22806;&#27844;&#28431;&#21644;&#25932;&#23545;&#25915;&#20987;&#65292;&#39044;&#38450;&#31995;&#32479;&#36890;&#24120;&#21482;&#20026;&#37329;&#34701;&#26426;&#26500;&#25552;&#20379;&#26381;&#21153;&#32780;&#32570;&#20047;&#25968;&#25454;&#20849;&#20139;&#30340;&#26426;&#21046;&#12290;&#37329;&#34701;&#39046;&#22495;&#30340;&#21327;&#20316;&#23398;&#20064;&#36827;&#23637;&#36739;&#23569;&#65292;&#20063;&#24456;&#38590;&#20174;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#20013;&#33719;&#24471;&#23454;&#38469;&#27934;&#23519;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;&#38544;&#31169;&#35282;&#24230;&#35774;&#35745;&#30340;&#21327;&#20316;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27450;&#35784;&#39044;&#38450;&#65292;&#24182;&#22312;&#26368;&#36817;&#30340;PETs Prize Challenges&#20013;&#33719;&#22870;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#38271;&#20132;&#26131;&#24207;&#21015;&#30340;&#28508;&#22312;&#23884;&#20837;&#34920;&#31034;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#21457;&#24067;&#26426;&#21046;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#25552;&#20379;&#32473;&#22806;&#37096;&#25176;&#31649;&#30340;&#27450;&#35784;&#21644;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26469;&#33258;&#22823;&#22411;&#25903;&#20184;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#24120;&#35265;&#25915;&#20987;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global financial crime activity is driving demand for machine learning solutions in fraud prevention. However, prevention systems are commonly serviced to financial institutions in isolation, and few provisions exist for data sharing due to fears of unintentional leaks and adversarial attacks. Collaborative learning advances in finance are rare, and it is hard to find real-world insights derived from privacy-preserving data processing systems. In this paper, we present a collaborative deep learning framework for fraud prevention, designed from a privacy standpoint, and awarded at the recent PETs Prize Challenges. We leverage latent embedded representations of varied-length transaction sequences, along with local differential privacy, in order to construct a data release mechanism which can securely inform externally hosted fraud and anomaly detection models. We assess our contribution on two distributed data sets donated by large payment networks, and demonstrate robustness to popular 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#21628;&#20986;&#27668;&#20307;&#29289;&#29702;&#30340;&#29992;&#25143;&#35748;&#35777;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#21628;&#20986;&#27668;&#20307;&#30340;&#28237;&#27969;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#21487;&#20197;&#23454;&#29616;&#29992;&#25143;&#30830;&#35748;&#21644;&#29992;&#25143;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#25143;&#30830;&#35748;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.02447</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#21628;&#20986;&#27668;&#20307;&#29289;&#29702;&#30340;&#29992;&#25143;&#35748;&#35777;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
User authentication system based on human exhaled breath physics. (arXiv:2401.02447v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#21628;&#20986;&#27668;&#20307;&#29289;&#29702;&#30340;&#29992;&#25143;&#35748;&#35777;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#21628;&#20986;&#27668;&#20307;&#30340;&#28237;&#27969;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#21487;&#20197;&#23454;&#29616;&#29992;&#25143;&#30830;&#35748;&#21644;&#29992;&#25143;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#25143;&#30830;&#35748;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#35797;&#22270;&#24314;&#31435;&#19968;&#20010;&#32431;&#31929;&#22522;&#20110;&#21628;&#20986;&#27668;&#20307;&#27969;&#20307;&#21147;&#23398;&#30340;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#21363;&#21628;&#20986;&#20154;&#31867;&#27668;&#24687;&#20013;&#30340;&#28237;&#27969;&#32467;&#26500;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#29983;&#29289;&#29305;&#24449;&#31639;&#27861;&#12290;&#35813;&#30740;&#31350;&#20381;&#36182;&#20110;&#39069;&#22806;&#33016;&#22806;&#27668;&#36947;&#23545;&#27599;&#20010;&#20010;&#20307;&#26469;&#35828;&#26159;&#29420;&#29305;&#30340;&#36825;&#19968;&#24819;&#27861;&#65292;&#20351;&#24471;&#21628;&#20986;&#27668;&#20307;&#25104;&#20026;&#20102;&#19968;&#20010;&#29983;&#29289;&#26631;&#35760;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#22810;&#32500;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26500;&#24314;&#29992;&#25143;&#35748;&#35777;&#31639;&#27861;&#65292;&#21363;&#29992;&#25143;&#30830;&#35748;&#21644;&#29992;&#25143;&#35782;&#21035;&#12290;&#29992;&#25143;&#30830;&#35748;&#31639;&#27861;&#26088;&#22312;&#39564;&#35777;&#29992;&#25143;&#26159;&#21542;&#20026;&#20854;&#22768;&#31216;&#30340;&#20154;&#12290;&#29992;&#25143;&#35782;&#21035;&#31639;&#27861;&#21017;&#26088;&#22312;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#21069;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#29992;&#25143;&#30340;&#36523;&#20221;&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;94&#21517;&#20154;&#31867;&#21463;&#35797;&#32773;&#30340;&#21628;&#20986;&#27668;&#20307;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#29992;&#25143;&#30830;&#35748;&#31639;&#27861;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work, in a pioneering approach, attempts to build a biometric system that works purely based on the fluid mechanics governing exhaled breath. We test the hypothesis that the structure of turbulence in exhaled human breath can be exploited to build biometric algorithms. This work relies on the idea that the extrathoracic airway is unique for every individual, making the exhaled breath a biomarker. Methods including classical multi-dimensional hypothesis testing approach and machine learning models are employed in building user authentication algorithms, namely user confirmation and user identification. A user confirmation algorithm tries to verify whether a user is the person they claim to be. A user identification algorithm tries to identify a user's identity with no prior information available. A dataset of exhaled breath time series samples from 94 human subjects was used to evaluate the performance of these algorithms. The user confirmation algorithms performed exceedingly well
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20013;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20256;&#24863;&#22120;&#20165;&#25918;&#32622;&#22312;&#20851;&#38190;&#38142;&#36335;&#19978;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25512;&#26029;&#25972;&#20010;&#32593;&#32476;&#20013;&#32570;&#22833;&#30340;&#27979;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2401.02438</link><description>&lt;p&gt;
&#27969;&#32593;&#32476;&#20013;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Sensor Placement for Learning in Flow Networks. (arXiv:2401.02438v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20013;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20256;&#24863;&#22120;&#20165;&#25918;&#32622;&#22312;&#20851;&#38190;&#38142;&#36335;&#19978;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25512;&#26029;&#25972;&#20010;&#32593;&#32476;&#20013;&#32570;&#22833;&#30340;&#27979;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#35774;&#26045;&#32593;&#32476;&#65288;&#22914;&#20132;&#36890;&#21644;&#30005;&#21147;&#20998;&#24067;&#65289;&#38656;&#35201;&#25345;&#32493;&#30417;&#27979;&#25925;&#38556;&#12289;&#25317;&#22622;&#21644;&#20854;&#20182;&#19981;&#21033;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24067;&#32622;&#21644;&#32500;&#25252;&#25104;&#26412;&#30340;&#21407;&#22240;&#65292;&#24448;&#24448;&#26080;&#27861;&#22312;&#32593;&#32476;&#30340;&#27599;&#20010;&#38142;&#36335;&#19978;&#35774;&#32622;&#20256;&#24863;&#22120;&#12290;&#30456;&#21453;&#65292;&#21487;&#20197;&#20165;&#22312;&#20960;&#20010;&#20851;&#38190;&#38142;&#36335;&#19978;&#25918;&#32622;&#20256;&#24863;&#22120;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#25512;&#26029;&#32570;&#22833;&#30340;&#27979;&#37327;&#65288;&#22914;&#20132;&#36890;&#37327;&#12289;&#21151;&#29575;&#27969;&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20256;&#24863;&#22120;&#24067;&#32622;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#27969;&#37327;&#23432;&#24658;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#35813;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#30830;&#23450;&#30340;&#20256;&#24863;&#22120;&#38598;&#21512;&#19978;&#36827;&#34892;&#26368;&#20248;&#24067;&#32622;&#26159;NP-hard&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#30340;&#36138;&#24515;&#31639;&#27861;&#29992;&#20110;&#20256;&#24863;&#22120;&#24067;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36866;&#24212;&#22823;&#22411;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large infrastructure networks (e.g. for transportation and power distribution) require constant monitoring for failures, congestion, and other adversarial events. However, assigning a sensor to every link in the network is often infeasible due to placement and maintenance costs. Instead, sensors can be placed only on a few key links, and machine learning algorithms can be leveraged for the inference of missing measurements (e.g. traffic counts, power flows) across the network. This paper investigates the sensor placement problem for networks. We first formalize the problem under a flow conservation assumption and show that it is NP-hard to place a fixed set of sensors optimally. Next, we propose an efficient and adaptive greedy heuristic for sensor placement that scales to large networks. Our experiments, using datasets from real-world application domains, show that the proposed approach enables more accurate inference than existing alternatives from the literature. We demonstrate that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#37319;&#29992;&#38543;&#26426;&#21152;&#26435;&#31070;&#32463;&#35843;&#33410;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#23398;&#20064;&#36328;&#20219;&#21153;&#20844;&#20849;&#27969;&#24418;&#12290;&#36890;&#36807;&#23450;&#20041;&#8220;&#20219;&#21153;&#29305;&#23450;&#30340;&#20960;&#20309;&#25935;&#24863;&#21704;&#24076;&#8221;&#65292;&#21033;&#29992;&#31867;&#20284;&#20110;&#22823;&#33041;&#27169;&#22411;&#30340;&#31070;&#32463;&#35843;&#33410;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02437</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#21152;&#26435;&#31070;&#32463;&#35843;&#33410;&#26377;&#21161;&#20110;&#23398;&#20064;&#36328;&#20219;&#21153;&#20844;&#20849;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
Randomly Weighted Neuromodulation in Neural Networks Facilitates Learning of Manifolds Common Across Tasks. (arXiv:2401.02437v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#37319;&#29992;&#38543;&#26426;&#21152;&#26435;&#31070;&#32463;&#35843;&#33410;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#23398;&#20064;&#36328;&#20219;&#21153;&#20844;&#20849;&#27969;&#24418;&#12290;&#36890;&#36807;&#23450;&#20041;&#8220;&#20219;&#21153;&#29305;&#23450;&#30340;&#20960;&#20309;&#25935;&#24863;&#21704;&#24076;&#8221;&#65292;&#21033;&#29992;&#31867;&#20284;&#20110;&#22823;&#33041;&#27169;&#22411;&#30340;&#31070;&#32463;&#35843;&#33410;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#25935;&#24863;&#21704;&#24076;&#20989;&#25968;&#26159;&#19968;&#31867;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#23398;&#20064;&#29305;&#23450;&#31867;&#21035;&#30340;&#27969;&#24418;&#20960;&#20309;&#12290;&#28982;&#32780;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26102;&#65292;&#20851;&#20110;&#27599;&#20010;&#20219;&#21153;&#33021;&#22815;&#34920;&#31034;&#30340;&#27969;&#24418;&#20960;&#20309;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#29983;&#25104;&#36807;&#31243;&#30340;&#24418;&#24335;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#19982;&#19968;&#20010;&#39640;&#32500;&#27969;&#24418;&#30456;&#20851;&#32852;&#65292;&#21487;&#20197;&#21033;&#29992;&#31867;&#20284;&#20110;&#22823;&#33041;&#27169;&#22411;&#30340;&#31070;&#32463;&#35843;&#33410;&#31995;&#32479;&#26469;&#23436;&#25104;&#12290;&#26681;&#25454;&#36825;&#20010;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#8220;&#20219;&#21153;&#29305;&#23450;&#30340;&#20960;&#20309;&#25935;&#24863;&#21704;&#24076;&#65288;T-GSH&#65289;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#24102;&#26377;&#31070;&#32463;&#35843;&#33410;&#31995;&#32479;&#30340;&#38543;&#26426;&#21152;&#26435;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#36825;&#20010;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric Sensitive Hashing functions, a family of Local Sensitive Hashing functions, are neural network models that learn class-specific manifold geometry in supervised learning. However, given a set of supervised learning tasks, understanding the manifold geometries that can represent each task and the kinds of relationships between the tasks based on them has received little attention. We explore a formalization of this question by considering a generative process where each task is associated with a high-dimensional manifold, which can be done in brain-like models with neuromodulatory systems. Following this formulation, we define \emph{Task-specific Geometric Sensitive Hashing~(T-GSH)} and show that a randomly weighted neural network with a neuromodulation system can realize this function.
&lt;/p&gt;</description></item><item><title>FedDiff&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21327;&#20316;&#25193;&#25955;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#23433;&#20840;&#34701;&#21512;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#20998;&#25903;&#25193;&#25955;&#27169;&#22411;&#29305;&#24449;&#25552;&#21462;&#35774;&#32622;&#26469;&#39537;&#21160;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.02433</link><description>&lt;p&gt;
FedDiff: &#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#23458;&#25143;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02433
&lt;/p&gt;
&lt;p&gt;
FedDiff&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21327;&#20316;&#25193;&#25955;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#23433;&#20840;&#34701;&#21512;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#20998;&#25903;&#25193;&#25955;&#27169;&#22411;&#29305;&#24449;&#25552;&#21462;&#35774;&#32622;&#26469;&#39537;&#21160;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36965;&#24863;&#39046;&#22495;&#25104;&#20687;&#20256;&#24863;&#22120;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22810;&#27169;&#24577;&#36965;&#24863;&#25968;&#25454;&#34701;&#21512;&#24050;&#25104;&#20026;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#27169;&#24577;&#21644;&#21333;&#23458;&#25143;&#25511;&#21046;&#19978;&#65292;&#21363;&#25193;&#25955;&#36807;&#31243;&#30001;&#21333;&#20010;&#27169;&#24577;&#22312;&#21333;&#20010;&#35745;&#31639;&#33410;&#28857;&#39537;&#21160;&#12290;&#20026;&#20102;&#23454;&#29616;&#26469;&#33258;&#23458;&#25143;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#23433;&#20840;&#34701;&#21512;&#65292;&#38656;&#35201;&#23454;&#29616;&#20998;&#24067;&#24335;&#30340;&#22810;&#27169;&#24577;&#25511;&#21046;&#65292;&#20363;&#22914;&#22312;&#27599;&#20010;&#22522;&#31449;&#23458;&#25143;&#31471;&#19978;&#65292;&#23558;&#32452;&#32455;A&#30340;&#39640;&#20809;&#35889;&#25968;&#25454;&#21644;&#32452;&#32455;B&#30340;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#31169;&#23494;&#21512;&#24182;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDiff&#30340;&#22810;&#27169;&#24577;&#21327;&#20316;&#25193;&#25955;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#25193;&#25955;&#27169;&#22411;&#29305;&#24449;&#25552;&#21462;&#35774;&#32622;&#65292;&#20854;&#20013;&#20004;&#31181;&#27169;&#24577;&#25968;&#25454;&#34987;&#36755;&#20837;&#21040;&#32534;&#30721;&#22120;&#30340;&#20998;&#25903;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of imaging sensor technology in the field of remote sensing, multi-modal remote sensing data fusion has emerged as a crucial research direction for land cover classification tasks. While diffusion models have made great progress in generative models and image classification tasks, existing models primarily focus on single-modality and single-client control, that is, the diffusion process is driven by a single modal in a single computing node. To facilitate the secure fusion of heterogeneous data from clients, it is necessary to enable distributed multi-modal control, such as merging the hyperspectral data of organization A and the LiDAR data of organization B privately on each base station client. In this study, we propose a multi-modal collaborative diffusion federated learning framework called FedDiff. Our framework establishes a dual-branch diffusion model feature extraction setup, where the two modal data are inputted into separate branches of the encoder
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#38169;&#35823;&#20998;&#31867;&#26694;&#26550;&#26469;&#35299;&#20915;ImageNet&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#38169;&#35823;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#27169;&#22411;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#38169;&#35823;&#20998;&#24067;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.02430</link><description>&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;ImageNet&#19978;&#30340;&#27169;&#22411;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Automated Classification of Model Errors on ImageNet. (arXiv:2401.02430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#38169;&#35823;&#20998;&#31867;&#26694;&#26550;&#26469;&#35299;&#20915;ImageNet&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#38169;&#35823;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#27169;&#22411;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#38169;&#35823;&#20998;&#24067;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;ImageNet&#25968;&#25454;&#38598;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#65292;&#20294;&#26174;&#33879;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#27495;&#20041;&#20351;&#24471;Top-1&#20934;&#30830;&#29575;&#25104;&#20026;&#36827;&#19968;&#27493;&#36827;&#23637;&#30340;&#19981;&#22815;&#20805;&#20998;&#30340;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26631;&#31614;&#38598;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#26174;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#36229;&#36807;95%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#23558;&#37325;&#28857;&#36716;&#21521;&#20026;&#20160;&#20040;&#21097;&#20313;&#30340;&#38169;&#35823;&#20173;&#28982;&#23384;&#22312;&#30340;&#35843;&#26597;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#37319;&#29992;&#19987;&#23478;&#23567;&#32452;&#23545;&#20004;&#20010;&#36873;&#25321;&#30340;&#27169;&#22411;&#30340;&#25152;&#26377;&#21097;&#20313;&#20998;&#31867;&#38169;&#35823;&#36827;&#34892;&#25163;&#21160;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#23481;&#26131;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#21463;&#36807;&#35757;&#32451;&#30340;&#19987;&#23478;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#24120;&#35268;&#27169;&#22411;&#35780;&#20272;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#25928;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#38169;&#35823;&#20998;&#31867;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#30740;&#31350;&#24314;&#27169;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#38169;&#35823;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#20840;&#38754;&#35780;&#20272;&#20102;&#36229;&#36807;90&#20010;&#27169;&#22411;&#30340;&#38169;&#35823;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the ImageNet dataset has been driving computer vision research over the past decade, significant label noise and ambiguity have made top-1 accuracy an insufficient measure of further progress. To address this, new label-sets and evaluation protocols have been proposed for ImageNet showing that state-of-the-art models already achieve over 95% accuracy and shifting the focus on investigating why the remaining errors persist.  Recent work in this direction employed a panel of experts to manually categorize all remaining classification errors for two selected models. However, this process is time-consuming, prone to inconsistencies, and requires trained experts, making it unsuitable for regular model evaluation thus limiting its utility. To overcome these limitations, we propose the first automated error classification framework, a valuable tool to study how modeling choices affect error distributions. We use our framework to comprehensively evaluate the error distribution of over 90
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20110;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.02429</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#35843;&#26597;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities. (arXiv:2401.02429v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02429
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20110;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#65288;IFD&#65289;&#20316;&#20026;&#19968;&#38376;&#20851;&#27880;&#26816;&#27979;&#21644;&#25910;&#38598;&#24037;&#19994;&#35774;&#22791;&#20581;&#24247;&#29366;&#20917;&#37325;&#35201;&#20449;&#24687;&#30340;&#23398;&#31185;&#32780;&#20986;&#29616;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#25925;&#38556;&#31867;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#35782;&#21035;&#12290;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23548;&#33268;&#23545;&#33258;&#21160;&#21270;&#35774;&#22791;&#30417;&#27979;&#30340;&#20851;&#27880;&#65292;&#20197;&#36991;&#20813;&#23433;&#20840;&#20107;&#25925;&#24182;&#20943;&#23569;&#23545;&#20154;&#21147;&#30340;&#20381;&#36182;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#20986;&#29616;&#22312;&#22686;&#24378;&#26234;&#33021;IFD&#31639;&#27861;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#25968;&#25454;&#32972;&#26223;&#19979;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20316;&#20026;&#19968;&#31181;&#31616;&#21270;&#30340;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;ANNs&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#36164;&#28304;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#20197;&#21450;&#21463;&#38480;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#22522;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#21407;&#29702;&#30340;&#31532;&#19977;&#20195;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial discipline concerned with detecting and gathering vital information about industrial equipment's health condition, thereby facilitating the identification of failure types and severities. The pursuit of precise and effective fault recognition has garnered substantial attention, culminating in a focus on automating equipment monitoring to preclude safety accidents and reduce reliance on human labor. The advent of artificial neural networks (ANNs) has been instrumental in augmenting intelligent IFD algorithms, particularly in the context of big data. Despite these advancements, ANNs, being a simplified biomimetic neural network model, exhibit inherent limitations such as resource and data dependencies and restricted cognitive capabilities. To address these limitations, the third-generation Spiking Neural Network (SNN), founded on principles of Brain-inspired computing, has surfaced as a promising alternative. Th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;EuroSAT&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#23545;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#36827;&#34892;&#26144;&#23556;&#65292;&#36890;&#36807;&#20351;&#29992;&#39068;&#33394;&#27874;&#27573;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;99.19%&#30340;&#31934;&#30830;&#24230;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#20445;&#25252;&#21644;&#22478;&#24066;&#35268;&#21010;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.02424</link><description>&lt;p&gt;
&#20351;&#29992;EuroSAT&#21644;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#65288;LULC&#65289;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning. (arXiv:2401.02424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;EuroSAT&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#23545;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#36827;&#34892;&#26144;&#23556;&#65292;&#36890;&#36807;&#20351;&#29992;&#39068;&#33394;&#27874;&#27573;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;99.19%&#30340;&#31934;&#30830;&#24230;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#20445;&#25252;&#21644;&#22478;&#24066;&#35268;&#21010;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#20154;&#21475;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#23545;&#33258;&#28982;&#36164;&#28304;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#31867;&#27963;&#21160;&#21344;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;23%&#12290;&#22909;&#28040;&#24687;&#26159;&#65292;&#36965;&#24863;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#31649;&#29702;&#25105;&#20204;&#29615;&#22659;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#30417;&#27979;&#22303;&#22320;&#21033;&#29992;&#65292;&#35268;&#21010;&#22478;&#24066;&#21306;&#22495;&#65292;&#24182;&#25512;&#21160;&#20892;&#19994;&#12289;&#27668;&#20505;&#21464;&#21270;&#32531;&#35299;&#12289;&#28798;&#23475;&#24674;&#22797;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#36827;&#23637;&#20351;&#22303;&#22320;&#21033;&#29992;&#26144;&#23556;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#27700;&#24179;&#12290;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#29992;RGB&#27874;&#27573;&#24494;&#35843;&#65292;&#25105;&#20204;&#22312;&#22303;&#22320;&#21033;&#29992;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;99.19%&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#26679;&#30340;&#21457;&#29616;&#21487;&#20197;&#29992;&#20110;&#21046;&#23450;&#20445;&#25252;&#21644;&#22478;&#24066;&#35268;&#21010;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the global population continues to expand, the demand for natural resources increases. Unfortunately, human activities account for 23% of greenhouse gas emissions. On a positive note, remote sensing technologies have emerged as a valuable tool in managing our environment. These technologies allow us to monitor land use, plan urban areas, and drive advancements in areas such as agriculture, climate change mitigation, disaster recovery, and environmental monitoring. Recent advances in AI, computer vision, and earth observation data have enabled unprecedented accuracy in land use mapping. By using transfer learning and fine-tuning with RGB bands, we achieved an impressive 99.19% accuracy in land use analysis. Such findings can be used to inform conservation and urban planning policies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02329</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#23569;&#25968;&#32676;&#20307;&#37117;&#26159;&#24179;&#31561;&#30340;: &#31354;&#31867;&#21035;&#24863;&#30693;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#34920;&#29616;&#20026;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#31867;&#21035;&#24179;&#34913;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#26412;&#22320;&#31867;&#21035;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#25968;&#31867;&#21035;&#20013;&#30001;&#20110;&#36807;&#25311;&#21512;&#26412;&#22320;&#19981;&#24179;&#34913;&#25968;&#25454;&#32780;&#23548;&#33268;&#20934;&#30830;&#24615;&#36739;&#24046;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedED&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31354;&#31867;&#21035;&#33976;&#39311;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#20013;&#20445;&#30041;&#20102;&#19982;&#31354;&#31867;&#21035;&#30456;&#20851;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36923;&#36753;&#25233;&#21046;&#30452;&#25509;&#38459;&#26029;&#20102;&#39044;&#27979;&#32467;&#26524;&#20013;&#23545;&#31354;&#31867;&#21035;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#24449;&#20102;&#38217;&#38156;&#30898;&#21270;&#29289;&#65288;CZT&#65289;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#22312;&#35782;&#21035;&#21508;&#31181;&#32452;&#32455;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#25216;&#26415;&#26469;&#20811;&#26381;&#20256;&#32479;CT&#25506;&#27979;&#22120;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.02106</link><description>&lt;p&gt;
&#38024;&#23545;&#36719;&#32452;&#32455;&#25104;&#20687;&#30340;&#38217;&#38156;&#30898;&#21270;&#29289;(CZT)&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Cadmium Zinc Telluride (CZT) photon counting detector Characterisation for soft tissue imaging. (arXiv:2401.02106v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#24449;&#20102;&#38217;&#38156;&#30898;&#21270;&#29289;&#65288;CZT&#65289;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#22312;&#35782;&#21035;&#21508;&#31181;&#32452;&#32455;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#25216;&#26415;&#26469;&#20811;&#26381;&#20256;&#32479;CT&#25506;&#27979;&#22120;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20809;&#23376;&#35745;&#25968;&#26816;&#27979;&#25216;&#26415;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;X&#23556;&#32447;&#25104;&#20687;&#30740;&#31350;&#20852;&#36259;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#25195;&#25551;&#20202;&#21487;&#20197;&#20174;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#20013;&#21463;&#30410;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#26377;&#28508;&#21147;&#20811;&#26381;&#20256;&#32479;CT&#25506;&#27979;&#22120;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#30740;&#31350;&#22312;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#20013;&#24212;&#29992;&#21322;&#23548;&#20307;&#25506;&#27979;&#26448;&#26009;&#29992;&#20110;&#26816;&#27979;&#36719;&#32452;&#32455;&#23545;&#27604;&#24230;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#25935;&#24230;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#34920;&#24449;&#38217;&#38156;&#30898;&#21270;&#29289;&#20809;&#23376;&#35745;&#25968;&#25506;&#27979;&#22120;&#22312;&#35782;&#21035;&#21508;&#31181;&#32452;&#32455;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;X&#23556;&#32447;&#31649;&#30005;&#21387;&#21644;&#30005;&#27969;&#20998;&#21035;&#35774;&#32622;&#20026;25 keV&#12289;35 keV&#21644;0.5 mA&#12289;1.0 mA&#65292;&#35780;&#20272;&#20102;CZT&#25506;&#27979;&#22120;&#30340;&#26368;&#20339;&#24103;&#36895;&#29575;&#65288;FPS&#65289;&#65292;&#36890;&#36807;&#20445;&#25345;&#26368;&#20339;FPS&#22266;&#23450;&#65292;&#23558;&#25506;&#27979;&#22120;&#33021;&#37327;&#38408;&#20540;&#20174;15 keV&#21040;35 keV&#35774;&#32622;&#20026;&#23567;&#27493;&#65292;&#23558;X&#23556;&#32447;&#31649;&#30340;&#30005;&#27969;&#35774;&#32622;&#20026;0.1 mA&#21040;1.0 mA&#30340;&#33539;&#22260;&#65292;&#20197;&#25214;&#21040;&#30005;&#21387;&#21644;&#30005;&#27969;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of photon counting detection technology has resulted in significant X-ray imaging research interest in recent years. Computed Tomography (CT) scanners can benefit from photon-counting detectors, which are new technology with the potential to overcome key limitations of conventional CT detectors. Researchers are still studying the effectiveness and sensitivity of semiconductor detector materials in photon counting detectors for detecting soft tissue contrasts. This study aimed to characterize the performance of the Cadmium Zinc Telluride photon counting detector in identifying various tissues. An optimal frame rate per second (FPS) of CZT detector was evaluated by setting the X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA respectively by keeping the optimum FPS fixed, the detector energy thresholds were set in small steps from 15 keV to 35 keV and the Currents were set for X-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between voltage and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25193;&#23637;&#20102;AstroLLaMA&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#30340;LLaMA-2&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#24067;&#20102;&#24102;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;</title><link>http://arxiv.org/abs/2401.01916</link><description>&lt;p&gt;
AstroLLaMA-Chat: &#20351;&#29992;&#23545;&#35805;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#25193;&#23637;AstroLLaMA
&lt;/p&gt;
&lt;p&gt;
AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01916
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25193;&#23637;&#20102;AstroLLaMA&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#30340;LLaMA-2&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#24067;&#20102;&#24102;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#22686;&#24378;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;7B&#21442;&#25968;&#30340;LLaMA-2&#27169;&#22411;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#19968;&#32452;&#32463;&#36807;&#31579;&#36873;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#20171;&#32461;&#21644;&#32467;&#35770;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#20687;GPT-4&#36825;&#26679;&#30340;&#36890;&#29992;LLMs&#22312;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#22238;&#31572;&#22330;&#26223;&#20013;&#30001;&#20110;&#26356;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26377;&#38480;&#36164;&#28304;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19987;&#38376;&#20027;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AstroLLaMA&#30340;&#25193;&#23637;&#65306;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23545;7B LLaMA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#26368;&#32456;&#21457;&#24067;&#20102;&#36866;&#29992;&#20110;&#31038;&#21306;&#20351;&#29992;&#30340;&#20855;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;&#20840;&#38754;&#30340;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#24182;&#23558;&#22312;&#21363;&#23558;&#21457;&#24067;&#30340;&#23436;&#25972;&#35770;&#25991;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;&#27169;&#22411;AstroLLaMA-Chat&#29616;&#24050;&#22312;...
&lt;/p&gt;
&lt;p&gt;
We explore the potential of enhancing LLM performance in astronomy-focused question-answering through targeted, continual pre-training. By employing a compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of astronomy corpus -- comprising abstracts, introductions, and conclusions -- we achieve notable improvements in specialized topic comprehension. While general LLMs like GPT-4 outperform in broader question-answering scenarios due to superior reasoning capabilities, our findings suggest that continual pre-training with limited resources can still enhance model performance on specialized topics. Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset, culminating in the release of the chat-enabled AstroLLaMA for community use. Comprehensive quantitative benchmarking is currently in progress and will be detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now available at
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#65292;&#20294;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#38656;&#35201;&#34987;&#20934;&#30830;&#35782;&#21035;&#21644;&#25490;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.01386</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#30340;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#65292;&#20294;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#38656;&#35201;&#34987;&#20934;&#30830;&#35782;&#21035;&#21644;&#25490;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#30149;&#29702;&#23398;&#20998;&#26512;&#21644;&#35786;&#26029;&#26159;&#30001;&#19987;&#23478;&#22312;&#26174;&#24494;&#38236;&#19979;&#36890;&#36807;&#35266;&#23519;&#29627;&#29827;&#20999;&#29255;&#26631;&#26412;&#36827;&#34892;&#25163;&#21160;&#30524;&#29699;&#21028;&#26029;&#26469;&#23436;&#25104;&#30340;&#12290;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#26159;&#20174;&#29627;&#29827;&#20999;&#29255;&#21046;&#20316;&#30340;&#25968;&#23383;&#26631;&#26412;&#12290;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#20351;&#24471;&#26631;&#26412;&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#23631;&#24149;&#19978;&#35266;&#23519;&#65292;&#24182;&#24341;&#21457;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#65292;&#20854;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#33258;&#21160;&#20998;&#26512;&#21644;&#35786;&#26029;&#12290;&#20511;&#21161;&#24403;&#21069;&#30340;&#35745;&#31639;&#36827;&#23637;&#65292;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#65288;&#22914;&#32452;&#32455;&#35126;&#30385;&#25110;&#27668;&#27873;&#65289;&#30340;&#24433;&#21709;&#65292;&#21017;&#20998;&#26512;&#21487;&#33021;&#20250;&#22833;&#36133;&#25110;&#23548;&#33268;&#38169;&#35823;&#30340;&#35786;&#26029;&#65292;&#36825;&#21462;&#20915;&#20110;&#20266;&#24433;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#29616;&#26377;&#30340;&#20266;&#24433;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#19987;&#23478;&#23545;&#20005;&#37325;&#31243;&#24230;&#30340;&#35780;&#20272;&#65292;&#20197;&#28040;&#38500;&#21463;&#21040;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#36827;&#34892;&#20998;&#26512;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#12289;&#32321;&#29712;&#65292;&#24182;&#19988;&#26377;&#25439;&#20110;&#33258;&#21160;&#21270;&#20998;&#26512;&#25110;&#20266;&#24433;&#21435;&#38500;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, pathological analysis and diagnosis are performed by manually eyeballing glass slide specimens under a microscope by an expert. The whole slide image is the digital specimen produced from the glass slide. Whole slide image enabled specimens to be observed on a computer screen and led to computational pathology where computer vision and artificial intelligence are utilized for automated analysis and diagnosis. With the current computational advancement, the entire whole slide image can be analyzed autonomously without human supervision. However, the analysis could fail or lead to wrong diagnosis if the whole slide image is affected by tissue artifacts such as tissue fold or air bubbles depending on the severity. Existing artifact detection methods rely on experts for severity assessment to eliminate artifact affected regions from the analysis. This process is time consuming, exhausting and undermines the goal of automated analysis or removal of artifacts without evaluatin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01148</link><description>&lt;p&gt;
&#26080;&#30028;&#25439;&#22833;&#30340;PAC-Bayes-Chernoff&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#12290;&#36825;&#20010;&#32467;&#26524;&#21487;&#20197;&#29702;&#35299;&#20026;Chernoff&#30028;&#38480;&#30340;PAC-Bayes&#29256;&#26412;&#12290;&#35777;&#26126;&#25216;&#24039;&#20381;&#36182;&#20110;&#36890;&#36807;Cram&#233;r&#21464;&#25442;&#23545;&#25439;&#22833;&#36827;&#34892;&#32479;&#19968;&#36793;&#30028;&#30340;&#23614;&#37096;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#35299;&#20915;&#20102;&#35768;&#22810;PAC-Bayes&#30028;&#38480;&#19978;&#30340;&#33258;&#30001;&#21442;&#25968;&#20248;&#21270;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#36827;&#34892;&#28789;&#27963;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#24191;&#20041;&#20102;&#20043;&#21069;&#30340;&#30028;&#38480;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#31867;&#20284;Gibbs&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#21644;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01100</link><description>&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#23454;&#29616;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding. (arXiv:2401.01100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01100
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#21644;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#26088;&#22312;&#25581;&#31034;&#39640;&#32500;&#31354;&#38388;&#20013;&#22797;&#26434;&#38750;&#32447;&#24615;&#27969;&#24418;&#20869;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#27969;&#24418;&#20551;&#35774;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#26469;&#20419;&#36827;&#21487;&#35270;&#21270;&#12289;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#33719;&#24471;&#20851;&#38190;&#27934;&#23519;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20840;&#23616;&#32467;&#26500;&#20013;&#30340;&#22823;&#37327;&#22833;&#30495;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#24213;&#23618;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;(scML)&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#23547;&#25214;&#19968;&#32452;&#22320;&#26631;&#26469;&#26500;&#24314;&#25972;&#20010;&#25968;&#25454;&#30340;&#20302;&#32500;&#39592;&#26550;&#65292;&#28982;&#21518;&#23558;&#38750;&#22320;&#26631;&#24341;&#20837;&#22320;&#26631;&#31354;&#38388;&#20013;
&lt;/p&gt;
&lt;p&gt;
As a pivotal approach in machine learning and data science, manifold learning aims to uncover the intrinsic low-dimensional structure within complex nonlinear manifolds in high-dimensional space. By exploiting the manifold hypothesis, various techniques for nonlinear dimension reduction have been developed to facilitate visualization, classification, clustering, and gaining key insights. Although existing manifold learning methods have achieved remarkable successes, they still suffer from extensive distortions incurred in the global structure, which hinders the understanding of underlying patterns. Scalability issues also limit their applicability for handling large-scale data. Here, we propose a scalable manifold learning (scML) method that can manipulate large-scale and high-dimensional data in an efficient manner. It starts by seeking a set of landmarks to construct the low-dimensional skeleton of the entire data and then incorporates the non-landmarks into the landmark space based 
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.00867</link><description>&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00867
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#22914;&#20309;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65288;MPS&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#23545;&#25163;&#29983;&#25104;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;MPS&#22312;&#24615;&#33021;&#26041;&#38754;&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#20419;&#36827;&#20102;&#29305;&#24449;&#27010;&#29575;&#12289;&#20911;&#183;&#35834;&#20234;&#26364;&#29109;&#21644;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#20026;&#24322;&#24120;&#20998;&#31867;&#25552;&#20379;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#20419;&#36827;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#22522;&#26412;&#21407;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
&lt;/p&gt;</description></item><item><title>Unicron&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#33258;&#24840;&#35774;&#35745;&#30340;&#24037;&#20316;&#36127;&#36733;&#31649;&#29702;&#22120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25925;&#38556;&#30456;&#20851;&#25104;&#26412;&#26469;&#20248;&#21270;&#35757;&#32451;&#36807;&#31243;&#65292;&#25317;&#26377;&#24102;&#20869;&#38169;&#35823;&#26816;&#27979;&#12289;&#21160;&#24577;&#25104;&#26412;&#24863;&#30693;&#30340;&#35745;&#21010;&#29983;&#25104;&#26426;&#21046;&#21644;&#39640;&#25928;&#30340;&#36716;&#25442;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.00134</link><description>&lt;p&gt;
Unicron: &#22312;&#35268;&#27169;&#21270;&#33258;&#24840;LLM&#35757;&#32451;&#20013;&#30340;&#32463;&#27982;&#33410;&#32422;
&lt;/p&gt;
&lt;p&gt;
Unicron: Economizing Self-Healing LLM Training at Scale. (arXiv:2401.00134v1 [cs.DC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00134
&lt;/p&gt;
&lt;p&gt;
Unicron&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#33258;&#24840;&#35774;&#35745;&#30340;&#24037;&#20316;&#36127;&#36733;&#31649;&#29702;&#22120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25925;&#38556;&#30456;&#20851;&#25104;&#26412;&#26469;&#20248;&#21270;&#35757;&#32451;&#36807;&#31243;&#65292;&#25317;&#26377;&#24102;&#20869;&#38169;&#35823;&#26816;&#27979;&#12289;&#21160;&#24577;&#25104;&#26412;&#24863;&#30693;&#30340;&#35745;&#21010;&#29983;&#25104;&#26426;&#21046;&#21644;&#39640;&#25928;&#30340;&#36716;&#25442;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#20294;&#30001;&#20110;&#39057;&#32321;&#30340;&#22833;&#36133;&#65292;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#26102;&#38388;&#21644;&#32463;&#27982;&#25104;&#26412;&#12290;&#20113;&#31471;&#29615;&#22659;&#20013;&#24403;&#21069;&#30340;&#25925;&#38556;&#24674;&#22797;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#35299;&#20915;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#30340;&#38382;&#39064;&#65292;&#20165;&#20165;&#23616;&#38480;&#20110;&#20943;&#23569;&#20010;&#21035;&#20219;&#21153;&#30340;&#20572;&#26426;&#26102;&#38388;&#32780;&#24573;&#35270;&#20102;&#23545;&#25972;&#20010;&#38598;&#32676;&#25104;&#26412;&#24433;&#21709;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Unicron&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#33258;&#24840;&#32780;&#35774;&#35745;&#30340;&#24037;&#20316;&#36127;&#36733;&#31649;&#29702;&#22120;&#12290;Unicron&#36890;&#36807;&#22312;&#38598;&#32676;&#20869;&#30340;&#22810;&#20010;&#24182;&#21457;&#20219;&#21153;&#20013;&#26368;&#23567;&#21270;&#19982;&#25925;&#38556;&#30456;&#20851;&#30340;&#25104;&#26412;&#26469;&#20248;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#20854;&#20027;&#35201;&#29305;&#28857;&#21253;&#25324;&#24102;&#20869;&#38169;&#35823;&#26816;&#27979;&#65292;&#23454;&#26102;&#35782;&#21035;&#38169;&#35823;&#32780;&#26080;&#38656;&#39069;&#22806;&#24320;&#38144;&#65307;&#21160;&#24577;&#25104;&#26412;&#24863;&#30693;&#30340;&#35745;&#21010;&#29983;&#25104;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#37325;&#26032;&#37197;&#32622;&#65307;&#20197;&#21450;&#39640;&#25928;&#30340;&#36716;&#25442;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#29366;&#24577;&#21464;&#21270;&#26399;&#38388;&#30340;&#20572;&#26426;&#26102;&#38388;&#12290;&#22312;&#19968;&#20010;128-GPU&#30340;&#20998;&#24067;&#24335;&#38598;&#32676;&#19978;&#37096;&#32626;&#65292;Unicron&#23637;&#29616;&#20102;&#22810;&#36798;1.9&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large-scale language models is increasingly critical in various domains, but it is hindered by frequent failures, leading to significant time and economic costs. Current failure recovery methods in cloud-based settings inadequately address the diverse and complex scenarios that arise, focusing narrowly on erasing downtime for individual tasks without considering the overall cost impact on a cluster. We introduce Unicron, a workload manager designed for efficient self-healing in large-scale language model training. Unicron optimizes the training process by minimizing failure-related costs across multiple concurrent tasks within a cluster. Its key features include in-band error detection for real-time error identification without extra overhead, a dynamic cost-aware plan generation mechanism for optimal reconfiguration, and an efficient transition strategy to reduce downtime during state changes. Deployed on a 128-GPU distributed cluster, Unicron demonstrates up to a 1.9x improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;&#23558;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#30340;&#30693;&#35782;&#19982;&#20915;&#31574;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#35299;&#20915;&#20915;&#31574;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#20808;&#39044;&#35757;&#32451;&#20877;&#33258;&#36866;&#24212;&#30340;&#27969;&#31243;&#65292;&#24182;&#35843;&#30740;&#20102;&#20915;&#31574;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#30830;&#23450;&#20102;&#21457;&#23637;&#20915;&#31574;&#22522;&#30784;&#27169;&#22411;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.00031</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#20915;&#31574;&#22522;&#30784;&#27169;&#22411;&#65306;&#24418;&#24335;&#21270;&#12289;&#27969;&#31243;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges. (arXiv:2401.00031v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#23558;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#30340;&#30693;&#35782;&#19982;&#20915;&#31574;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#35299;&#20915;&#20915;&#31574;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#20808;&#39044;&#35757;&#32451;&#20877;&#33258;&#36866;&#24212;&#30340;&#27969;&#31243;&#65292;&#24182;&#35843;&#30740;&#20102;&#20915;&#31574;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#30830;&#23450;&#20102;&#21457;&#23637;&#20915;&#31574;&#22522;&#30784;&#27169;&#22411;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#21160;&#24577;&#36807;&#31243;&#65292;&#38656;&#35201;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#26469;&#36827;&#34892;&#36873;&#25321;&#24182;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#20915;&#31574;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#32780;&#22823;&#35268;&#27169;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#32463;&#20351;&#24471;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#30340;&#24555;&#36895;&#36866;&#24212;&#25104;&#20026;&#21487;&#33021;&#65292;&#36890;&#36807;&#24494;&#35843;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20174;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#19982;&#19979;&#28216;&#20915;&#31574;&#38382;&#39064;&#34701;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#39044;&#35757;&#32451;&#20877;&#33258;&#36866;&#24212;&#30340;&#27969;&#31243;&#65292;&#24182;&#35843;&#30740;&#20102;&#20915;&#31574;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36890;&#29992;&#28789;&#27963;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#19979;&#65292;&#21457;&#23637;&#20915;&#31574;&#22522;&#30784;&#27169;&#22411;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making is a dynamic process requiring perception, memory, and reasoning to make choices and find optimal policies. Traditional approaches to decision-making suffer from sample efficiency and generalization, while large-scale self-supervised pretraining has enabled fast adaptation with fine-tuning or few-shot learning in language and vision. We thus argue to integrate knowledge acquired from generic large-scale self-supervised pretraining into downstream decision-making problems. We propose Pretrain-Then-Adapt pipeline and survey recent work on data collection, pretraining objectives and adaptation strategies for decision-making pretraining and downstream inference. Finally, we identify critical challenges and future directions for developing decision foundation model with the help of generic and flexible self-supervised pretraining.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CycleGAN&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;MRI&#31070;&#32463;&#24433;&#20687;&#20174;&#19968;&#20010;&#22330;&#24378;&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;&#22330;&#24378;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#29983;&#25104;&#21512;&#25104;&#21644;&#37325;&#24314;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.00023</link><description>&lt;p&gt;
CycleGAN&#27169;&#22411;&#29992;&#20110;MRI&#22270;&#20687;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
CycleGAN Models for MRI Image Translation. (arXiv:2401.00023v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CycleGAN&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;MRI&#31070;&#32463;&#24433;&#20687;&#20174;&#19968;&#20010;&#22330;&#24378;&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;&#22330;&#24378;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#29983;&#25104;&#21512;&#25104;&#21644;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#21487;&#20197;&#23558;&#22270;&#20687;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#12290;&#36890;&#36807;&#39046;&#22495;&#36716;&#25442;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#22312;&#33021;&#22815;&#22686;&#21152;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#26356;&#24191;&#20041;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;CycleGAN&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#24433;&#20687;&#20174;&#19968;&#20010;&#22330;&#24378;&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;&#22330;&#24378;&#65288;&#20363;&#22914;&#20174;3&#29305;&#26031;&#25289;&#21040;1.5&#29305;&#26031;&#25289;&#65289;&#12290;&#19982;&#22522;&#20110;DCGAN&#26550;&#26500;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;CycleGAN&#33021;&#22815;&#20197;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#29983;&#25104;&#21512;&#25104;&#21644;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-image translation has gained popularity in the medical field to transform images from one domain to another. Medical image synthesis via domain transformation is advantageous in its ability to augment an image dataset where images for a given class is limited. From the learning perspective, this process contributes to data-oriented robustness of the model by inherently broadening the model's exposure to more diverse visual data and enabling it to learn more generalized features. In the case of generating additional neuroimages, it is advantageous to obtain unidentifiable medical data and augment smaller annotated datasets. This study proposes the development of a CycleGAN model for translating neuroimages from one field strength to another (e.g., 3 Tesla to 1.5). This model was compared to a model based on DCGAN architecture. CycleGAN was able to generate the synthetic and reconstructed images with reasonable accuracy. The mapping function from the source (3 Tesla) to target d
&lt;/p&gt;</description></item><item><title>MoTCoder&#26159;&#19968;&#20010;&#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25351;&#20196;&#35843;&#25972;&#20419;&#36827;&#20219;&#21153;&#30340;&#20998;&#35299;&#21644;&#27169;&#22359;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22359;&#21270;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.15960</link><description>&lt;p&gt;
MoTCoder: &#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks. (arXiv:2312.15960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15960
&lt;/p&gt;
&lt;p&gt;
MoTCoder&#26159;&#19968;&#20010;&#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25351;&#20196;&#35843;&#25972;&#20419;&#36827;&#20219;&#21153;&#30340;&#20998;&#35299;&#21644;&#27169;&#22359;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22359;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#31616;&#21333;&#30340;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32534;&#31243;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20256;&#32479;&#27169;&#22411;&#24448;&#24448;&#29983;&#25104;&#20316;&#20026;&#21333;&#19968;&#20195;&#30721;&#22359;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Modular-of-Thought Coder (MoTCoder)&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;MoT&#25351;&#20196;&#35843;&#25972;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#22521;&#20859;&#21644;&#21033;&#29992;&#23376;&#27169;&#22359;&#65292;MoTCoder&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27169;&#22359;&#21270;&#21644;&#27491;&#30830;&#24615;&#65292;&#23548;&#33268;&#22312;APPS&#19978;&#30456;&#23545;pass@1&#25913;&#36827;&#20102;12.9%&#65292;&#22312;CodeContests&#19978;&#30456;&#23545;pass@1&#25913;&#36827;&#20102;9.43%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/dvlab-research/MoTCoder&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a pioneering framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial relative pass@1 improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are available at https://github.com/dvlab-research/MoTCoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Geo2SigMap&#65292;&#19968;&#31181;&#22522;&#20110;&#22320;&#29702;&#25968;&#25454;&#24211;&#30340;&#39640;&#25928;&#21644;&#39640;&#20445;&#30495;&#24230;&#23556;&#39057;&#20449;&#21495;&#26144;&#23556;&#26694;&#26550;&#12290;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;OpenStreetMap&#65288;&#22320;&#29702;&#25968;&#25454;&#24211;&#65289;&#12289;Blender&#65288;&#35745;&#31639;&#26426;&#22270;&#24418;&#65289;&#31561;&#24320;&#28304;&#24037;&#20855;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#23556;&#39057;&#20449;&#21495;&#20256;&#25773;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#22312;&#8220;&#26410;&#30693;&#8221;&#21306;&#22495;&#36827;&#34892;&#23556;&#39057;&#20449;&#21495;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2312.14303</link><description>&lt;p&gt;
Geo2SigMap: &#20351;&#29992;&#22320;&#29702;&#25968;&#25454;&#24211;&#36827;&#34892;&#39640;&#20445;&#30495;&#24230;&#23556;&#39057;&#20449;&#21495;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Geo2SigMap: High-Fidelity RF Signal Mapping Using Geographic Databases. (arXiv:2312.14303v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Geo2SigMap&#65292;&#19968;&#31181;&#22522;&#20110;&#22320;&#29702;&#25968;&#25454;&#24211;&#30340;&#39640;&#25928;&#21644;&#39640;&#20445;&#30495;&#24230;&#23556;&#39057;&#20449;&#21495;&#26144;&#23556;&#26694;&#26550;&#12290;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;OpenStreetMap&#65288;&#22320;&#29702;&#25968;&#25454;&#24211;&#65289;&#12289;Blender&#65288;&#35745;&#31639;&#26426;&#22270;&#24418;&#65289;&#31561;&#24320;&#28304;&#24037;&#20855;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#23556;&#39057;&#20449;&#21495;&#20256;&#25773;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#22312;&#8220;&#26410;&#30693;&#8221;&#21306;&#22495;&#36827;&#34892;&#23556;&#39057;&#20449;&#21495;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#65288;RF&#65289;&#20449;&#21495;&#26144;&#23556;&#26159;&#20998;&#26512;&#21644;&#39044;&#27979;&#29305;&#23450;&#21306;&#22495;&#20869;&#23556;&#39057;&#20449;&#21495;&#24378;&#24230;&#21644;&#20998;&#24067;&#30340;&#36807;&#31243;&#65292;&#23545;&#34562;&#31389;&#32593;&#32476;&#35268;&#21010;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#23556;&#39057;&#20449;&#21495;&#26144;&#23556;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#27979;&#37327;&#25968;&#25454;&#26500;&#24314;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#22797;&#26434;&#24230;&#20302;&#20294;&#36890;&#24120;&#32570;&#20047;&#20934;&#30830;&#24615;&#65292;&#25110;&#32773;&#20381;&#36182;&#23556;&#32447;&#36319;&#36394;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#25552;&#20379;&#20102;&#23545;&#30446;&#26631;&#21306;&#22495;&#30340;&#22686;&#24378;&#31934;&#24230;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#23556;&#39057;&#20449;&#21495;&#20256;&#25773;&#65292;&#23427;&#21033;&#29992;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#8220;&#26410;&#30693;&#8221;&#21306;&#22495;&#36827;&#34892;&#23556;&#39057;&#20449;&#21495;&#26144;&#23556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Geo2SigMap&#65292;&#19968;&#31181;&#22522;&#20110;ML&#30340;&#20351;&#29992;&#22320;&#29702;&#25968;&#25454;&#24211;&#36827;&#34892;&#39640;&#25928;&#19988;&#39640;&#20445;&#30495;&#24230;RF&#20449;&#21495;&#26144;&#23556;&#30340;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#26080;&#32541;&#38598;&#25104;&#20102;&#19977;&#20010;&#24320;&#28304;&#24037;&#20855;&#65306;OpenStreetMap&#65288;&#22320;&#29702;&#25968;&#25454;&#24211;&#65289;&#12289;Blender&#65288;&#35745;&#31639;&#26426;&#22270;&#24418;&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
Radio frequency (RF) signal mapping, which is the process of analyzing and predicting the RF signal strength and distribution across specific areas, is crucial for cellular network planning and deployment. Traditional approaches to RF signal mapping rely on statistical models constructed based on measurement data, which offer low complexity but often lack accuracy, or ray tracing tools, which provide enhanced precision for the target area but suffer from increased computational complexity. Recently, machine learning (ML) has emerged as a data-driven method for modeling RF signal propagation, which leverages models trained on synthetic datasets to perform RF signal mapping in "unseen" areas.  In this paper, we present Geo2SigMap, an ML-based framework for efficient and high-fidelity RF signal mapping using geographic databases. First, we develop an automated framework that seamlessly integrates three open-source tools: OpenStreetMap (geographic databases), Blender (computer graphics), a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#29305;&#24449;&#30340;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#39057;&#35889;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#19981;&#26029;&#23398;&#20064;&#20197;&#20998;&#31867;&#27700;&#22768;&#20449;&#21495;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#20851;&#31995;&#21644;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2312.13143</link><description>&lt;p&gt;
&#22522;&#20110;&#26174;&#33879;&#29305;&#24449;&#30340;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Underwater Acoustic Signal Recognition Based on Salient Feature. (arXiv:2312.13143v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#29305;&#24449;&#30340;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#39057;&#35889;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#19981;&#26029;&#23398;&#20064;&#20197;&#20998;&#31867;&#27700;&#22768;&#20449;&#21495;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#20851;&#31995;&#21644;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22797;&#26434;&#29615;&#22659;&#20013;&#27700;&#22768;&#20449;&#21495;&#30340;&#35782;&#21035;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#20027;&#27969;&#30340;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;&#20027;&#35201;&#20381;&#36182;&#20110;&#26102;&#39057;&#20998;&#26512;&#26469;&#25552;&#21462;&#39057;&#35889;&#29305;&#24449;&#65292;&#22312;&#35813;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35782;&#21035;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#19987;&#23478;&#31995;&#32479;&#65292;&#38754;&#20020;&#30528;&#30693;&#35782;&#24211;&#21463;&#38480;&#21644;&#22788;&#29702;&#22797;&#26434;&#20851;&#31995;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38480;&#21046;&#28304;&#20110;&#35268;&#21017;&#25110;&#25512;&#29702;&#24341;&#25806;&#30340;&#22797;&#26434;&#24615;&#21644;&#32500;&#25252;&#22256;&#38590;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#22768;&#20449;&#21495;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#39057;&#35889;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#19981;&#26029;&#23398;&#20064;&#20197;&#20998;&#31867;&#27700;&#22768;&#20449;&#21495;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#25277;&#35937;&#29305;&#24449;&#65292;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of technology, the recognition of underwater acoustic signals in complex environments has become increasingly crucial. Currently, mainstream underwater acoustic signal recognition relies primarily on time-frequency analysis to extract spectral features, finding widespread applications in the field. However, existing recognition methods heavily depend on expert systems, facing limitations such as restricted knowledge bases and challenges in handling complex relationships. These limitations stem from the complexity and maintenance difficulties associated with rules or inference engines. Recognizing the potential advantages of deep learning in handling intricate relationships, this paper proposes a method utilizing neural networks for underwater acoustic signal recognition. The proposed approach involves continual learning of features extracted from spectra for the classification of underwater acoustic signals. Deep learning models can automatically learn abstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2312.11514</link><description>&lt;p&gt;
&#38378;&#23384;LLM&#65306;&#22312;&#26377;&#38480;&#20869;&#23384;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;DRAM&#23481;&#37327;&#30340;&#35774;&#22791;&#32780;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#65292;&#24182;&#25353;&#38656;&#23558;&#20854;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#36229;&#36807;&#21487;&#29992;DRAM&#23481;&#37327;&#30340;LLM&#39640;&#25928;&#36816;&#34892;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#32771;&#34385;&#38378;&#23384;&#29305;&#24615;&#30340;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#65292;&#24341;&#23548;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#36827;&#34892;&#20248;&#21270;&#65306;&#20943;&#23569;&#20174;&#38378;&#23384;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#65292;&#24182;&#20197;&#36739;&#22823;&#12289;&#26356;&#36830;&#32493;&#30340;&#22359;&#35835;&#21462;&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#21463;&#30828;&#20214;&#21551;&#21457;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#8220;&#31383;&#21475;&#21270;&#8221;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#26469;&#31574;&#30053;&#24615;&#22320;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#65292;&#20854;&#27425;&#65292;&#8220;&#34892;&#21015;&#32465;&#23450;&#8221;&#36866;&#24212;&#20102;&#38378;&#23384;&#30340;&#39034;&#24207;&#25968;&#25454;&#35775;&#38382;&#29305;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#24635;&#32467;&#20102;&#24320;&#38598;&#35782;&#21035;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#24120;&#35265;&#20570;&#27861;&#12289;&#38480;&#21046;&#20197;&#21450;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.08785</link><description>&lt;p&gt;
&#22788;&#29702;&#26410;&#30693;&#24773;&#20917;&#65306;&#24320;&#38598;&#35782;&#21035;&#21450;&#30456;&#20851;&#39046;&#22495;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Managing the unknown: a survey on Open Set Recognition and tangential areas. (arXiv:2312.08785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08785
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#24635;&#32467;&#20102;&#24320;&#38598;&#35782;&#21035;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#24120;&#35265;&#20570;&#27861;&#12289;&#38480;&#21046;&#20197;&#21450;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20998;&#31867;&#27169;&#22411;&#24120;&#24120;&#38656;&#35201;&#22312;&#39044;&#27979;&#23646;&#20110;&#20854;&#35757;&#32451;&#38454;&#27573;&#26410;&#20986;&#29616;&#30340;&#31867;&#21035;&#26679;&#26412;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#24320;&#38598;&#35782;&#21035;&#36890;&#36807;&#35774;&#35745;&#33021;&#22815;&#22312;&#27979;&#35797;&#38454;&#27573;&#20174;&#26679;&#26412;&#20013;&#26816;&#27979;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#23545;&#23646;&#20110;&#24050;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19982;&#24320;&#38598;&#35782;&#21035;&#30456;&#20851;&#30340;&#26368;&#36817;&#25991;&#29486;&#65292;&#35782;&#21035;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#24120;&#35265;&#20570;&#27861;&#12289;&#38480;&#21046;&#20197;&#21450;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#65288;&#22914;&#36830;&#32493;&#23398;&#20064;&#12289;&#20998;&#24067;&#22806;&#26816;&#27979;&#12289;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65289;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#25581;&#31034;&#20102;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#36825;&#20123;&#26041;&#21521;&#21487;&#33021;&#20419;&#36827;&#21644;&#25512;&#36827;&#26410;&#26469;&#26356;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios classification models are often required to perform robustly when predicting samples belonging to classes that have not appeared during its training stage. Open Set Recognition addresses this issue by devising models capable of detecting unknown classes from samples arriving during the testing phase, while maintaining a good level of performance in the classification of samples belonging to known classes. This review comprehensively overviews the recent literature related to Open Set Recognition, identifying common practices, limitations, and connections of this field with other machine learning research areas, such as continual learning, out-of-distribution detection, novelty detection, and uncertainty estimation. Our work also uncovers open problems and suggests several research directions that may motivate and articulate future efforts towards more safe Artificial Intelligence methods.
&lt;/p&gt;</description></item><item><title>PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2312.07910</link><description>&lt;p&gt;
PromptBench&#65306;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07910
&lt;/p&gt;
&lt;p&gt;
PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#23545;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#32479;&#19968;&#24211;&#12290;&#23427;&#30001;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#21644;&#25193;&#23637;&#65306;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;PromptBench&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#24320;&#25918;&#12289;&#36890;&#29992;&#21644;&#28789;&#27963;&#30340;&#20195;&#30721;&#24211;&#65292;&#20197;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#65292;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#21644;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/promptbench&#19978;&#25214;&#21040;&#65292;&#24182;&#23558;&#25345;&#32493;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#27969;&#31243;&#65292;&#33021;&#22815;&#23545;&#25239;&#27169;&#22411;&#26377;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#65292;&#20026;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.06942</link><description>&lt;p&gt;
AI &#25511;&#21046;&#65306;&#23613;&#31649;&#23384;&#22312;&#24847;&#22270;&#24615;&#30772;&#22351;&#65292;&#20294;&#25552;&#39640;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI Control: Improving Safety Despite Intentional Subversion. (arXiv:2312.06942v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#27969;&#31243;&#65292;&#33021;&#22815;&#23545;&#25239;&#27169;&#22411;&#26377;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#65292;&#20026;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#33258;&#20027;&#37096;&#32626;&#65292;&#38450;&#27490;&#23427;&#20204;&#23548;&#33268;&#26377;&#23475;&#32467;&#26524;&#23558;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#23433;&#20840;&#25216;&#26415;&#65292;&#20363;&#22914;&#20351;&#29992;&#27169;&#22411;&#26469;&#23457;&#26680;&#20854;&#20182;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#25110;&#20351;&#29992;&#32418;&#38431;&#25216;&#26415;&#25581;&#31034;&#24494;&#22937;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#23578;&#26410;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#22312;&#27169;&#22411;&#26377;&#24847;&#23581;&#35797;&#30772;&#22351;&#23427;&#20204;&#26102;&#26159;&#21542;&#20173;&#28982;&#30830;&#20445;&#23433;&#20840;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#23545;&#26377;&#24847;&#30772;&#22351;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23433;&#20840;&#25216;&#26415;&#27969;&#31243;&#65288;&#8220;&#21327;&#35758;&#8221;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#20294;&#19981;&#21487;&#20449;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#26159;GPT-4&#65289;&#12289;&#20351;&#29992;&#36739;&#24369;&#30340;&#21487;&#20449;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#26159;GPT-3.5&#65289;&#20197;&#21450;&#26377;&#38480;&#30340;&#39640;&#36136;&#37327;&#21487;&#20449;&#21171;&#21160;&#21147;&#35775;&#38382;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#19968;&#31995;&#21015;&#32534;&#31243;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26088;&#22312;&#27704;&#36828;&#19981;&#25552;&#20132;&#21253;&#21547;&#21518;&#38376;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21327;&#35758;&#65292;&#20854;&#20013;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques ("protocols") that are robust to intentional subversion.  We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2312.04889</link><description>&lt;p&gt;
KwaiAgents&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30001;&#20110;&#22909;&#22855;&#24515;&#30340;&#39537;&#20351;&#65292;&#19981;&#26029;&#25506;&#32034;&#21644;&#29702;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#65292;&#20174;&#32780;&#21457;&#26126;&#20102;&#21508;&#31181;&#24037;&#20855;&#26469;&#28385;&#36275;&#36825;&#31181;&#22909;&#22855;&#24515;&#12290;&#23613;&#31649;&#20154;&#31867;&#26080;&#27861;&#22312;&#22823;&#33041;&#20013;&#22788;&#29702;&#21644;&#35760;&#24518;&#22823;&#37327;&#20449;&#24687;&#65292;&#20294;&#22312;&#25209;&#21028;&#24605;&#32500;&#12289;&#35268;&#21010;&#12289;&#21453;&#24605;&#20197;&#21450;&#21033;&#29992;&#29616;&#26377;&#24037;&#20855;&#19982;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#35299;&#37322;&#26041;&#38754;&#21331;&#36234;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#23547;&#25214;&#31572;&#26696;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#27493;&#34920;&#26126;&#65292;&#26426;&#22120;&#21487;&#33021;&#20063;&#20855;&#22791;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#21463;&#38480;&#65292;&#20063;&#33021;&#23637;&#31034;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22312; KwaiAgents &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLM&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#30340;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#12289;&#34892;&#20026;&#20934;&#21017;&#21644;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#12290;&#26234;&#33021;&#20307;&#36824;&#21487;&#20197;&#26356;&#26032;&#26597;&#35810;&#32467;&#26524;&#65292;&#19982;&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HGPROMPT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.01878</link><description>&lt;p&gt;
HGPROMPT: &#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#20013;&#29992;&#20110;&#36830;&#25509;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning. (arXiv:2312.01878v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01878
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HGPROMPT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#26159;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#31471;&#21040;&#31471;&#30340;&#30417;&#30563;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#65292;&#23545;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#24335;&#65292;&#20294;&#26159;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#24120;&#24120;&#23384;&#22312;&#24046;&#36317;&#65292;&#23548;&#33268;&#30446;&#26631;&#30340;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25552;&#31034;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#27491;&#22312;&#23835;&#36215;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#12290;&#34429;&#28982;&#26089;&#26399;&#24050;&#32463;&#23545;&#22270;&#19978;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36827;&#34892;&#20102;&#19968;&#20123;&#25506;&#32034;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#28041;&#21450;&#21516;&#36136;&#22270;&#65292;&#24573;&#30053;&#20102;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24322;&#36136;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGPROMPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#20197;&#32479;&#19968;&#39044;&#20808;&#35757;&#32451;&#30340;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs) are prominent techniques for homogeneous and heterogeneous graph representation learning, yet their performance in an end-to-end supervised framework greatly depends on the availability of task-specific supervision. To reduce the labeling cost, pre-training on self-supervised pretext tasks has become a popular paradigm,but there is often a gap between the pre-trained model and downstream tasks, stemming from the divergence in their objectives. To bridge the gap, prompt learning has risen as a promising direction especially in few-shot settings, without the need to fully fine-tune the pre-trained model. While there has been some early exploration of prompt-based learning on graphs, they primarily deal with homogeneous graphs, ignoring the heterogeneous graphs that are prevalent in downstream applications. In this paper, we propose HGPROMPT, a novel pre-training and prompting framework to unify not only pre-trai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23558;&#22522;&#30784;&#27169;&#22411;&#25509;&#22320;&#65292;&#20197;&#35299;&#20915;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#37322;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.17431</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25509;&#22320;&#65306;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23558;&#22522;&#30784;&#27169;&#22411;&#25509;&#22320;&#65292;&#20197;&#35299;&#20915;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#37322;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24191;&#27867;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#32534;&#30721;&#30340;Foundation Models&#65288;FMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#36890;&#36807;&#23558;FMs&#36866;&#24212;&#20110;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#25110;&#22686;&#21152;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23545;&#20854;&#36827;&#34892;&#25509;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#20805;&#20998;&#21457;&#25381;FMs&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25509;&#22320;FMs&#38754;&#20020;&#30528;&#22810;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65288;FTL&#65289;&#65292;&#21363;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36817;&#24180;&#26469;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#36890;&#36807;FTL-FM&#21033;&#29992;FMs&#36827;&#34892;&#25509;&#22320;&#30340;&#38656;&#27714;&#24378;&#28872;&#22686;&#38271;&#12290;&#21463;&#21040;FTL-FM&#30740;&#31350;&#30340;&#24378;&#21170;&#22686;&#38271;&#21644;FTL-FM&#23545;&#24037;&#19994;&#24212;&#29992;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;FTL-FM&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#24314;&#31435;FMs&#30340;&#25509;&#22320;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated lea
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#36873;&#25321;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2311.14212</link><description>&lt;p&gt;
&#27880;&#37322;&#25935;&#24863;&#24615;&#65306;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#36873;&#25321;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#25968;&#25454;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#25910;&#38598;&#26102;&#65292;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#12289;&#32473;&#20104;&#27880;&#37322;&#32773;&#30340;&#25351;&#31034;&#12289;&#27880;&#37322;&#32773;&#30340;&#29305;&#24449;&#20197;&#21450;&#20182;&#20204;&#20043;&#38388;&#30340;&#20114;&#21160;&#37117;&#21487;&#33021;&#23545;&#35757;&#32451;&#25968;&#25454;&#20135;&#29983;&#24433;&#21709;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#21019;&#24314;&#27880;&#37322;&#24037;&#20855;&#26102;&#30340;&#35774;&#35745;&#36873;&#25321;&#20063;&#20250;&#24433;&#21709;&#22522;&#20110;&#24471;&#21040;&#30340;&#27880;&#37322;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#27880;&#37322;&#25935;&#24863;&#24615;"&#36825;&#20010;&#26415;&#35821;&#65292;&#29992;&#26469;&#25351;&#20195;&#27880;&#37322;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#20197;&#21450;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#21644;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20116;&#31181;&#23454;&#39564;&#26465;&#20214;&#19979;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#65292;&#38543;&#26426;&#23558;&#27880;&#37322;&#32773;&#20998;&#37197;&#21040;&#19981;&#21516;&#26465;&#20214;&#19979;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#24471;&#21040;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#27599;&#20010;&#26465;&#20214;&#30340;&#20445;&#30041;&#37096;&#20998;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20197;&#19979;&#26041;&#38754;&#26465;&#20214;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65306;1&#65289;&#20167;&#24680;&#35328;&#35770;/&#20882;&#29359;&#24615;&#35821;&#35328;&#27880;&#37322;&#30340;&#27604;&#20363;&#65292;2&#65289;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.09441</link><description>&lt;p&gt;
&#25506;&#32034;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;-&#33021;&#32791;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#21106;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#23427;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#21019;&#26032;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SFL&#20013;&#27169;&#22411;&#22312;&#29305;&#23450;&#23618;&#65288;&#31216;&#20026;&#20999;&#21106;&#23618;&#65289;&#19978;&#34987;&#20998;&#21106;&#20026;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#27169;&#22411;&#65292;&#36873;&#25321;&#20999;&#21106;&#23618;&#21487;&#33021;&#23545;&#23458;&#25143;&#31471;&#30340;&#33021;&#32791;&#21644;&#38544;&#31169;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#24433;&#21709;&#20102;&#35757;&#32451;&#36127;&#25285;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20999;&#21106;&#23618;&#30340;&#35774;&#35745;&#25361;&#25112;&#38750;&#24120;&#22797;&#26434;&#65292;&#20027;&#35201;&#30001;&#20110;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#32593;&#32476;&#33021;&#21147;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;SFL&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#33021;&#32791;&#21644;&#38544;&#31169;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and
&lt;/p&gt;</description></item><item><title>ConvNet&#21644;Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#19979;&#65292;&#36229;&#36234;&#20102;ImageNet&#20934;&#30830;&#29575;&#30340;&#23545;&#27604;&#20998;&#26512;&#20013;&#21457;&#29616;&#23427;&#20204;&#22312;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.09215</link><description>&lt;p&gt;
ConvNet vs Transformer, Supervised vs CLIP: &#36229;&#36234;ImageNet&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy. (arXiv:2311.09215v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09215
&lt;/p&gt;
&lt;p&gt;
ConvNet&#21644;Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#19979;&#65292;&#36229;&#36234;&#20102;ImageNet&#20934;&#30830;&#29575;&#30340;&#23545;&#27604;&#20998;&#26512;&#20013;&#21457;&#29616;&#23427;&#20204;&#22312;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#20026;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#22810;&#31181;&#27169;&#22411;&#36873;&#25321;&#65292;&#23545;&#20110;&#29305;&#23450;&#24212;&#29992;&#20174;&#22810;&#20010;&#36873;&#39033;&#20013;&#36873;&#25321;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#23427;&#20204;&#22312;ImageNet&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#26469;&#27604;&#36739;&#31454;&#20105;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#21327;&#35758;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#21333;&#19968;&#25351;&#26631;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#23545;&#20110;&#19987;&#19994;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24615;&#33021;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ConvNet&#21644;Vision Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#33539;&#24335;&#19979;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;ImageNet&#30340;&#20934;&#30830;&#29575;&#12290;&#23613;&#31649;&#25105;&#20204;&#36873;&#25321;&#30340;&#27169;&#22411;&#22312;ImageNet&#20934;&#30830;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#19978;&#30456;&#20284;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#35768;&#22810;&#20854;&#20182;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65306;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#12290;&#20256;&#32479;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#21040;&#30340;&#36825;&#31181;&#27169;&#22411;&#29305;&#24615;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#22312;&#36873;&#25321;&#19981;&#21516;&#27169;&#22411;&#26102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our
&lt;/p&gt;</description></item><item><title>FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2311.01282</link><description>&lt;p&gt;
FlashDecoding++: &#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26356;&#24555;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01282
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#26410;&#35299;&#20915;&#65306;(1) &#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12290;softmax&#25805;&#20316;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#27599;&#20010;&#37096;&#20998;softmax&#32467;&#26524;&#65292;&#23548;&#33268;LLM&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#24320;&#38144;&#22686;&#21152;&#32422;20%&#12290;(2) &#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#12290;&#22312;LLM&#25512;&#29702;&#20013;&#25191;&#34892;GEMM&#30340;&#30697;&#38453;&#24418;&#29366;&#26159;&#25153;&#24179;&#30340;&#65292;&#23548;&#33268;&#22312;&#20808;&#21069;&#30340;&#35774;&#35745;&#20013;&#22635;&#20805;&#38646;&#21518;&#35745;&#31639;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24615;&#33021;&#25439;&#22833;&#36229;&#36807;50%&#12290;(3) &#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;LLM&#20013;&#30340;&#20869;&#26680;&#24615;&#33021;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#12289;&#30828;&#20214;&#37197;&#32622;&#31561;&#12290;&#21333;&#19968;&#21644;&#38745;&#24577;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23548;&#33268;LLM&#25512;&#29702;&#20013;&#19981;&#21516;&#24418;&#29366;&#30340;GEMM&#30340;&#24615;&#33021;&#25439;&#22833;&#36798;&#21040;50.25%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlashDecoding++&#65292;&#19968;&#31181;&#24555;&#36895;&#25903;&#25345;&#20027;&#27969;LLM&#21644;&#30828;&#20214;&#21518;&#31471;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FlashDecoding++&#23454;&#29616;&#20102;&#20197;&#19979;&#30446;&#26631;&#65306;
&lt;/p&gt;
&lt;p&gt;
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt;50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.15325</link><description>&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#31185;&#23398;&#21457;&#29616;&#21644;&#24037;&#31243;&#35774;&#35745;&#21463;&#38480;&#20110;&#29289;&#29702;&#23454;&#39564;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#23454;&#39564;&#36890;&#24120;&#26159;&#36890;&#36807;&#35797;&#39564;&#21644;&#30452;&#35273;&#36873;&#25321;&#30340;&#65292;&#24182;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25968;&#20540;&#27169;&#25311;&#26159;&#29289;&#29702;&#23454;&#39564;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#39046;&#22495;&#26469;&#35828;&#65292;&#30001;&#20110;&#29616;&#26377;&#25968;&#20540;&#26041;&#27861;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#29305;&#21035;&#26159;&#65292;&#19968;&#20010;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;AI&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#26144;&#23556;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#20363;&#22914;&#26102;&#31354;&#36807;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#20301;&#32622;&#36827;&#34892;&#22806;&#25512;&#21644;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36827;&#34892;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#12290;&#31070;&#32463;&#36816;&#31639;&#31526;&#21487;&#20197;&#22686;&#24378;&#29978;&#33267;&#26367;&#20195;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#29616;&#26377;&#27169;&#25311;&#22120;&#65292;&#20363;&#22914;&#35745;&#31639;&#21147;&#23398;&#27969;&#20307;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#36830;&#25509;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FC-STGNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#21464;&#37327;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#26102;&#38388;&#25139;&#19978;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.05305</link><description>&lt;p&gt;
&#20840;&#36830;&#25509;&#30340;&#26102;&#31354;&#22270;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#24207;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data. (arXiv:2309.05305v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#36830;&#25509;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FC-STGNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#21464;&#37327;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#26102;&#38388;&#25139;&#19978;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#24207;&#65288;MTS&#65289;&#25968;&#25454;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#37117;&#24456;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#24207;&#21015;&#24615;&#21644;&#22810;&#28304;&#24615;&#65288;&#22810;&#20010;&#20256;&#24863;&#22120;&#65289;&#65292;MTS&#25968;&#25454;&#22266;&#26377;&#22320;&#23637;&#29616;&#20102;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#21253;&#25324;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#27599;&#20010;&#26102;&#38388;&#25139;&#20013;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20998;&#21035;&#25429;&#33719;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#26102;&#38388;&#25139;&#19978;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#24573;&#35270;&#36825;&#26679;&#30340;&#30456;&#20851;&#24615;&#38480;&#21046;&#20102;&#22312;MTS&#25968;&#25454;&#20013;&#20840;&#38754;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29616;&#26377;GNNs&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20840;&#36830;&#25509;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FC-STGNN&#65289;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;FC&#22270;&#26500;&#24314;&#21644;FC&#22270;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolutio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;(MTS)&#20998;&#31867;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24573;&#35270;&#20102;MTS&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22270;&#25193;&#22686;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#20445;&#25345;&#20256;&#24863;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25968;&#25454;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05202</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#22270;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-Aware Contrasting for Multivariate Time-Series Classification. (arXiv:2309.05202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;(MTS)&#20998;&#31867;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24573;&#35270;&#20102;MTS&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22270;&#25193;&#22686;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#20445;&#25345;&#20256;&#24863;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25968;&#25454;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#20998;&#31867;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#23427;&#30830;&#20445;&#20102;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#23398;&#20064;&#36825;&#20123;&#26679;&#26412;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#26102;&#38388;&#25193;&#22686;&#21644;&#23545;&#27604;&#25216;&#26415;&#23454;&#29616;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#26088;&#22312;&#20445;&#25252;MTS&#25968;&#25454;&#30340;&#26102;&#38388;&#27169;&#24335;&#19981;&#21463;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#38656;&#35201;&#30830;&#20445;&#20256;&#24863;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#23427;&#20204;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#30001;&#20110;MTS&#25968;&#25454;&#36890;&#24120;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#65292;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#23545;&#20110;&#23545;&#27604;&#23398;&#20064;&#22312;MTS&#25968;&#25454;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;MTS&#25968;&#25454;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21253;&#25324;&#33410;&#28857;&#21644;&#36793;&#25193;&#22686;&#22312;&#20869;&#30340;&#22270;&#25193;&#22686;&#65292;&#20197;&#20445;&#25345;&#20256;&#24863;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#24212;&#29992;&#22270;&#23545;&#27604;&#30446;&#26631;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning, as a self-supervised learning paradigm, becomes popular for Multivariate Time-Series (MTS) classification. It ensures the consistency across different views of unlabeled samples and then learns effective representations for these samples. Existing contrastive learning methods mainly focus on achieving temporal consistency with temporal augmentation and contrasting techniques, aiming to preserve temporal patterns against perturbations for MTS data. However, they overlook spatial consistency that requires the stability of individual sensors and their correlations. As MTS data typically originate from multiple sensors, ensuring spatial consistency becomes essential for the overall performance of contrastive learning on MTS data. Thus, we propose Graph-Aware Contrasting for spatial consistency across MTS data. Specifically, we propose graph augmentations including node and edge augmentations to preserve the stability of sensors and their correlations, followed by grap
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#21463;&#27169;&#22411;&#26500;&#24314;&#32773;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#38656;&#35201;&#23545;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#36827;&#34892;&#26356;&#21152;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26631;&#20934;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.00201</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20027;&#35266;&#24615;
&lt;/p&gt;
&lt;p&gt;
Subjectivity in Unsupervised Machine Learning Model Selection. (arXiv:2309.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00201
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#21463;&#27169;&#22411;&#26500;&#24314;&#32773;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#38656;&#35201;&#23545;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#36827;&#34892;&#26356;&#21152;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26631;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#24517;&#35201;&#30340;&#27493;&#39588;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#26631;&#20934;&#21644;&#25351;&#26631;&#65292;&#20294;&#27169;&#22411;&#36873;&#25321;&#20173;&#28982;&#23384;&#22312;&#20027;&#35266;&#24615;&#12290;&#39640;&#24230;&#20027;&#35266;&#24615;&#21487;&#33021;&#20250;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#37325;&#22797;&#24615;&#21644;&#21487;&#20877;&#29616;&#24615;&#20135;&#29983;&#30097;&#38382;&#65292;&#24182;&#23545;&#23454;&#38469;&#37096;&#32626;&#30340;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#20013;&#27169;&#22411;&#26500;&#24314;&#32773;&#30340;&#20559;&#22909;&#24433;&#21709;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20197;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20026;&#20363;&#65292;&#35843;&#26597;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20027;&#35266;&#24615;&#12290;&#25105;&#20204;&#36992;&#35831;&#20102;33&#20301;&#21442;&#19982;&#32773;&#21644;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19977;&#20010;&#22330;&#26223;&#20013;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#21442;&#19982;&#32773;&#36824;&#26159;LLMs&#30340;&#36873;&#25321;&#37117;&#23384;&#22312;&#21464;&#24322;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#19981;&#21516;&#30340;&#26631;&#20934;&#21644;&#25351;&#26631;&#23384;&#22312;&#20998;&#27495;&#26102;&#12290;&#20027;&#35266;&#24615;&#26469;&#28304;&#21253;&#25324;&#23545;&#19981;&#21516;&#26631;&#20934;&#21644;&#25351;&#26631;&#37325;&#35201;&#24615;&#30340;&#19981;&#21516;&#24847;&#35265;&#65292;&#23545;&#27169;&#22411;&#24212;&#35813;&#26377;&#22810;&#31616;&#27905;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#35268;&#27169;&#30340;&#22823;&#23567;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#33647;&#29289;&#28342;&#35299;&#24230;&#65292;&#20854;&#20013;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#20102;&#35299;&#27599;&#20010;&#21151;&#33021;&#22242;&#23545;&#28342;&#35299;&#24230;&#30340;&#24433;&#21709;&#12290;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#19982;&#32447;&#24615;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#26159;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.12325</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#33647;&#29289;&#28342;&#35299;&#24230;--&#25552;&#21462;&#21270;&#23398;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#19982;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network. (arXiv:2308.12325v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#33647;&#29289;&#28342;&#35299;&#24230;&#65292;&#20854;&#20013;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#20102;&#35299;&#27599;&#20010;&#21151;&#33021;&#22242;&#23545;&#28342;&#35299;&#24230;&#30340;&#24433;&#21709;&#12290;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#19982;&#32447;&#24615;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#26159;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32473;&#23450;&#20998;&#23376;&#30340;&#28342;&#35299;&#24230;&#26159;&#21046;&#33647;&#34892;&#19994;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#20195;&#35745;&#31639;&#36164;&#28304;&#30340;&#20248;&#21183;&#37325;&#26032;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#20570;&#20986;&#21512;&#29702;&#30340;&#39044;&#27979;&#65292;&#32780;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#32780;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#26356;&#22810;&#26377;&#20851;&#24213;&#23618;&#21270;&#23398;&#24433;&#21709;&#30340;&#27934;&#23519;&#12290;&#21033;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#20010;&#21151;&#33021;&#22242;&#23545;&#25972;&#20307;&#28342;&#35299;&#24230;&#30340;&#24433;&#21709;&#12290;&#26368;&#32456;&#65292;&#22312;&#35774;&#35745;&#26032;&#33647;&#26102;&#65292;&#20102;&#35299;&#21270;&#23398;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#21270;&#23398;&#24615;&#36136;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#33268;&#21147;&#20110;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#19982;&#32447;&#24615;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#65292;&#37322;&#25918;&#20986;&#26032;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advan
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#25299;&#23637;&#20102;&#24050;&#26377;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#32467;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2308.12075</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#31283;&#23450;RNN&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stabilizing RNN Gradients through Pre-training. (arXiv:2308.12075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#25299;&#23637;&#20102;&#24050;&#26377;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#32467;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#20247;&#22810;&#29702;&#35770;&#37117;&#24314;&#35758;&#36890;&#36807;&#38450;&#27490;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#25351;&#25968;&#24418;&#24335;&#38543;&#28145;&#24230;&#25110;&#26102;&#38388;&#22686;&#38271;&#26469;&#31283;&#23450;&#21644;&#25913;&#21892;&#35757;&#32451;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20998;&#26512;&#26159;&#22312;&#21069;&#39304;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25110;&#21333;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#23398;&#30340;&#21487;&#35299;&#24615;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20307;&#31995;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#20197;&#33267;&#20110;&#26080;&#27861;&#36827;&#34892;&#35299;&#26512;&#21021;&#22987;&#21270;&#26102;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24050;&#30693;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#28085;&#30422;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#28145;&#23618;&#24490;&#29615;&#32593;&#32476;&#23478;&#26063;&#65292;&#23545;&#25968;&#25454;&#21644;&#21442;&#25968;&#20998;&#24067;&#30340;&#35201;&#27714;&#36739;&#23569;&#65292;&#36825;&#20010;&#29702;&#35770;&#34987;&#31216;&#20026;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65288;LSC&#65289;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#32463;&#20856;&#30340;Glorot&#12289;He&#21644;&#27491;&#20132;&#21021;&#22987;&#21270;&#26041;&#26696;&#22312;&#24212;&#29992;&#20110;&#21069;&#39304;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#28385;&#36275;LSC&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#28145;&#23618;&#24490;&#29615;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#25968;&#22686;&#38271;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07728</link><description>&lt;p&gt;
&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65306;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#24050;&#20855;&#22791;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#21457;&#29983;&#30072;&#21464;&#12290;&#22312;&#36866;&#24212;&#26032;&#30446;&#26631;&#39046;&#22495;&#26102;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#23545;&#22836;&#23618;&#36827;&#34892;&#23545;&#40784;&#22788;&#29702;&#21487;&#20197;&#22788;&#29702;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25209;&#24402;&#19968;&#21270;&#23618;&#30340;&#22788;&#29702;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#12289;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#25913;&#26469;&#26377;&#25928;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.07688</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#33258;&#28982;&#22270;&#20687;&#22686;&#24378;&#21307;&#30103;AI&#27169;&#22411;&#30340;&#32593;&#32476;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#24378;&#22823;&#29305;&#24449;&#30340;&#26426;&#20250;&#65292;&#20174;&#32780;&#21487;&#20197;&#32469;&#36807;&#32321;&#37325;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;SSL&#39044;&#35757;&#32451;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#65292;&#24182;&#19982;&#38750;&#21307;&#23398;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#24182;&#26681;&#25454;&#20197;&#19979;&#26041;&#24335;&#21021;&#22987;&#21270;&#20854;&#26435;&#37325;&#65306;&#65288;i&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;SSL&#39044;&#35757;&#32451;&#65288;DINOv2&#65289;&#12289;&#65288;ii&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;MIMIC-CXR&#25968;&#25454;&#24211;&#20013;&#30340;&#33016;&#37096;X&#23556;&#32447;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#20845;&#20010;&#20840;&#29699;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;800,000&#22810;&#24352;&#33016;&#37096;X&#23556;&#32447;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35786;&#26029;&#20102;20&#22810;&#31181;&#19981;&#21516;&#30340;&#24433;&#20687;&#25152;&#35265;&#12290;&#25105;&#20204;&#30340;SSL&#39044;&#35757;&#32451;&#22312;&#32463;&#36807;&#31579;&#36873;&#30340;&#22270;&#20687;&#19978;&#19981;&#20165;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#65288;&#23545;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;P&lt;0.001&#65289;&#65292;&#32780;&#19988;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#36824;&#36229;&#36807;&#20102;&#22522;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P&lt;0.001 for all datasets) but, in cert
&lt;/p&gt;</description></item><item><title>FITS&#26159;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#25805;&#20316;&#65292;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2307.03756</link><description>&lt;p&gt;
FITS&#65306;&#27169;&#25311;&#20855;&#26377;10k&#20010;&#21442;&#25968;&#30340;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
FITS: Modeling Time Series with $10k$ Parameters. (arXiv:2307.03756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03756
&lt;/p&gt;
&lt;p&gt;
FITS&#26159;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#25805;&#20316;&#65292;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FITS&#65292;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#12290;&#19982;&#30452;&#25509;&#22788;&#29702;&#21407;&#22987;&#26102;&#38388;&#22495;&#25968;&#25454;&#30340;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;FITS&#22522;&#20110;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#30340;&#21407;&#29702;&#25805;&#20316;&#26102;&#38388;&#24207;&#21015;&#12290;&#36890;&#36807;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;FITS&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#36817;&#20284;10k&#20010;&#21442;&#25968;&#30340;&#26174;&#33879;&#32039;&#20945;&#22823;&#23567;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#20026;&#21508;&#31181;&#24212;&#29992;&#21019;&#36896;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications. The anonymous code repo is available in: \url{https://anonymous.4open.science/r/FITS}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;&#26144;&#23556;&#65292;&#20197;&#24314;&#31435;&#32771;&#34385;&#25509;&#21475;&#30340;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12006</link><description>&lt;p&gt;
&#23398;&#20064;&#23567;&#27874;&#23545;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Homogenization for Elliptic Operators. (arXiv:2306.12006v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;&#26144;&#23556;&#65292;&#20197;&#24314;&#31435;&#32771;&#34385;&#25509;&#21475;&#30340;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#20986;&#29616;&#12290;&#40784;&#27425;&#21270;&#29702;&#35770;&#26159;&#28040;&#38500;&#23567;&#23610;&#24230;&#20381;&#36182;&#30340;&#24378;&#26377;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#31616;&#21270;&#30340;&#26041;&#31243;&#20197;&#21450;&#35745;&#31639;&#19978;&#30340;&#20415;&#21033;&#12290;&#22312;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#39046;&#22495;&#65292;&#40784;&#27425;&#21270;&#23545;&#20110;&#23548;&#20986;&#21253;&#21547;&#24494;&#35266;&#29289;&#29702;&#23398;&#30340;&#26412;&#26500;&#23450;&#24459;&#20197;&#21046;&#23450;&#24863;&#20852;&#36259;&#30340;&#23439;&#35266;&#37327;&#30340;&#24179;&#34913;&#26041;&#31243;&#24456;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#27809;&#26377;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#23637;&#29616;&#22312;&#24494;&#35266;&#23610;&#24230;&#19978;&#19981;&#23384;&#22312;&#30340;&#29616;&#35937;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26412;&#26500;&#23450;&#24459;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26925;&#22278;&#31639;&#23376;&#40784;&#27425;&#21270;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35828;&#26126;&#19982;&#36825;&#31181;&#25509;&#21475;&#26377;&#20851;&#30340;&#40784;&#27425;&#21270;&#24314;&#31435;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#36896;&#36866;&#24403;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#32771;&#34385;&#24213;&#23618;&#20960;&#20309;&#23398;&#21644;&#24494;&#35266;&#32467;&#26500;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#23558;&#36825;&#20123;&#25509;&#21475;&#21512;&#24182;&#21040;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#26377;&#25928;&#30340;&#23439;&#35266;&#34892;&#20026;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale partial differential equations (PDEs) arise in various applications, and several schemes have been developed to solve them efficiently. Homogenization theory is a powerful methodology that eliminates the small-scale dependence, resulting in simplified equations that are computationally tractable. In the field of continuum mechanics, homogenization is crucial for deriving constitutive laws that incorporate microscale physics in order to formulate balance laws for the macroscopic quantities of interest. However, obtaining homogenized constitutive laws is often challenging as they do not in general have an analytic form and can exhibit phenomena not present on the microscale. In response, data-driven learning of the constitutive law has been proposed as appropriate for this task. However, a major challenge in data-driven learning approaches for this problem has remained unexplored: the impact of discontinuities and corner interfaces in the underlying material. These discontinui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#38024;&#23545;&#19968;&#31867;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21482;&#26377;&#37096;&#20998;&#21442;&#25968;&#28385;&#36275;&#24378;&#20984;&#24615;&#65292;Nesterov&#21160;&#37327;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.08109</link><description>&lt;p&gt;
&#38024;&#23545;&#37096;&#20998;&#24378;&#20984;&#24615;&#65292;&#22312;Nesterov&#21160;&#37327;&#27861;&#19979;&#21152;&#36895;&#25910;&#25947;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity. (arXiv:2306.08109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#38024;&#23545;&#19968;&#31867;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21482;&#26377;&#37096;&#20998;&#21442;&#25968;&#28385;&#36275;&#24378;&#20984;&#24615;&#65292;Nesterov&#21160;&#37327;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#20998;&#26512;&#32858;&#28966;&#20110;&#34920;&#24449;&#25439;&#22833;&#20989;&#25968;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;Polyak-Lojaciewicz&#65288;PL&#65289;&#26465;&#20214;&#21644;&#21463;&#38480;&#24378;&#20984;&#24615;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#26799;&#24230;&#19979;&#38477;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#20294;&#26159;Nesterov&#21160;&#37327;&#27861;&#26159;&#21542;&#22312;&#31867;&#20284;&#30340;&#26465;&#20214;&#21644;&#20551;&#35774;&#19979;&#20855;&#26377;&#21152;&#36895;&#25910;&#25947;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21482;&#26377;&#37096;&#20998;&#21442;&#25968;&#28385;&#36275;&#24378;&#20984;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;Nesterov&#21160;&#37327;&#27861;&#22312;&#36825;&#31181;&#30446;&#26631;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#23454;&#29616;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#28145;&#24230;ReLU&#32593;&#32476;&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#35777;&#26126;&#38750;&#24179;&#20961;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20855;&#26377;&#21152;&#36895;&#25910;&#25947;&#29575;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art analyses on the convergence of gradient descent for training neural networks focus on characterizing properties of the loss landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted strong convexity. While gradient descent converges linearly under such conditions, it remains an open question whether Nesterov's momentum enjoys accelerated convergence under similar settings and assumptions. In this work, we consider a new class of objective functions, where only a subset of the parameters satisfies strong convexity, and show Nesterov's momentum achieves acceleration in theory for this objective class. We provide two realizations of the problem class, one of which is deep ReLU networks, which --to the best of our knowledge--constitutes this work the first that proves accelerated convergence rate for non-trivial neural network architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#22235;&#31181;GANs&#21644;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#23545;&#20351;&#29992;&#21512;&#25104;MR&#22270;&#20687;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#21106;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;80% - 90%&#30340;&#23454;&#38469;&#22270;&#20687;&#35757;&#32451;&#24615;&#33021;&#12290;&#20294;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#26469;&#35828;&#65292;&#35757;&#32451;&#22270;&#20687;&#30340;&#35760;&#24518;&#21270;&#21487;&#33021;&#20250;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02986</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;MR&#22270;&#20687;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#21106;--GANs&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Brain tumor segmentation using synthetic MR images -- A comparison of GANs and diffusion models. (arXiv:2306.02986v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#22235;&#31181;GANs&#21644;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#23545;&#20351;&#29992;&#21512;&#25104;MR&#22270;&#20687;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#21106;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;80% - 90%&#30340;&#23454;&#38469;&#22270;&#20687;&#35757;&#32451;&#24615;&#33021;&#12290;&#20294;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#26469;&#35828;&#65292;&#35757;&#32451;&#22270;&#20687;&#30340;&#35760;&#24518;&#21270;&#21487;&#33021;&#20250;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#30001;&#20110;&#20262;&#29702;&#12289;&#21311;&#21517;&#21270;&#21644;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#30340;&#38480;&#21046;&#65292;&#25968;&#25454;&#20849;&#20139;&#24120;&#24120;&#24456;&#22797;&#26434;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#36924;&#30495;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#26377;&#21487;&#33021;&#20419;&#36827;&#25968;&#25454;&#20849;&#20139;&#12290;&#28982;&#32780;&#65292;&#22312;&#20849;&#20139;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#20043;&#21069;&#65292;&#24517;&#39035;&#39318;&#20808;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#30340;&#32593;&#32476;&#65292;&#19988;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#23545;&#22235;&#31181;GANs&#65288;&#28176;&#36827;&#24335;GAN&#65292;StyleGAN 1-3&#65289;&#21644;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#65288;&#20351;&#29992;&#20004;&#31181;&#20998;&#21106;&#32593;&#32476;&#65292;U-Net&#21644;Swin transformer&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#21106;&#32593;&#32476;&#30340;Dice&#20998;&#25968;&#36798;&#21040;&#20102;&#20351;&#29992;&#23454;&#38469;&#22270;&#20687;&#35757;&#32451;&#26102;Dice&#20998;&#25968;&#30340;80% - 90%&#65292;&#20294;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#26469;&#35828;&#65292;&#35757;&#32451;&#22270;&#20687;&#30340;&#35760;&#24518;&#21270;&#21487;&#33021;&#20250;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large annotated datasets are required for training deep learning models, but in medical imaging data sharing is often complicated due to ethics, anonymization and data protection legislation. Generative AI models, such as generative adversarial networks (GANs) and diffusion models, can today produce very realistic synthetic images, and can potentially facilitate data sharing. However, in order to share synthetic medical images it must first be demonstrated that they can be used for training different networks with acceptable performance. Here, we therefore comprehensively evaluate four GANs (progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain tumor segmentation (using two segmentation networks, U-Net and a Swin transformer). Our results show that segmentation networks trained on synthetic images reach Dice scores that are 80% - 90% of Dice scores when training with real images, but that memorization of the training images can be a problem for diffusion models if 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13301</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#28789;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#30340;&#36817;&#20284;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#24182;&#19981;&#20851;&#27880;&#20284;&#28982;&#65292;&#32780;&#26159;&#20851;&#27880;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#36136;&#37327;&#25110;&#33647;&#29289;&#25928;&#21147;&#31561;&#19979;&#28216;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#27492;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#21435;&#22122;&#35270;&#20026;&#22810;&#27493;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#19968;&#31867;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#26367;&#20195;&#30340;&#22870;&#21169;&#21152;&#26435;&#20284;&#28982;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;DDPO&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;DDPO&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#21453;&#39304;&#30340;&#25552;&#31034;-&#22270;&#20687;&#23545;&#40784;&#26041;&#24335;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03890</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#32593;&#32476;&#36924;&#36817;&#29992;&#20110;&#36328;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Approximation by non-symmetric networks for cross-domain learning. (arXiv:2305.03890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;30&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#20247;&#22810;&#36807;&#31243;&#65288;&#22914;&#65306;&#27973;&#23618;&#25110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#12289;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#21644;&#21508;&#31181;&#20869;&#26680;&#26041;&#27861;&#65289;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#34920;&#36798;&#33021;&#21147;&#65289;&#30740;&#31350;&#20013;&#20419;&#36827;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#38024;&#23545;&#19981;&#21464;&#23398;&#20064;&#12289;&#20256;&#36882;&#23398;&#20064;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#31561;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#26469;&#30740;&#31350;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#19968;&#32452;&#20869;&#26680;&#30340;&#26356;&#19968;&#33324;&#26041;&#27861;&#65292;&#22914;&#24191;&#20041;&#24179;&#31227;&#32593;&#32476;&#65288;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#21644;&#24179;&#31227;&#19981;&#21464;&#26680;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65289;&#21644;&#26059;&#36716;&#21306;&#20989;&#25968;&#26680;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#20869;&#26680;&#30340;&#36924;&#36817;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#33021;&#35201;&#27714;&#20869;&#26680;&#26159;&#27491;&#23450;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#21487;&#33021;&#22312;&#20998;&#24067;&#19978;&#19981;&#21516;&#30340;&#36328;&#22495;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. Our results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05292</link><description>&lt;p&gt;
MC-ViViT: &#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-ViViT&#29992;&#20110;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#12289;&#38382;&#21367;&#21644;&#35270;&#39057;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;(MCI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-&#35270;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;(MC-ViViT)&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#21306;&#20998;MCI&#21644;&#27491;&#24120;&#35748;&#30693;&#12290;&#25968;&#25454;&#26469;&#33258;I-CONECT&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#39057;&#32321;&#35270;&#39057;&#32842;&#22825;&#26469;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#34892;&#20026;&#24178;&#39044;&#35797;&#39564;&#12290;MC-ViViT&#22312;&#19968;&#20010;&#20998;&#25903;&#20013;&#25552;&#21462;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;MC&#27169;&#22359;&#22686;&#24378;&#34920;&#31034;&#12290;&#30001;&#20110;I-CONECT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21253;&#21547;&#38590;&#26131;&#21644;&#27491;&#36127;&#26679;&#26412;&#65289;&#65292;&#36825;&#20351;MC-ViViT&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Hard-Easy&#21644;Positive-Negative&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;HP Loss&#65289;&#26469;&#32467;&#21512;&#23545;&#27604;&#24230;&#35843;&#33410;&#25439;&#22833;Focal loss&#21644;AD-CORRE loss&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;I-CONECT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;</title><link>http://arxiv.org/abs/2303.02725</link><description>&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#29615;&#22659;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Local Environment Poisoning Attacks on Federated Reinforcement Learning. (arXiv:2303.02725v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#28909;&#38376;&#24037;&#20855;&#12290;&#22810;&#20195;&#29702;&#32467;&#26500;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38656;&#27714;&#22823;&#30340;&#20027;&#35201;&#38382;&#39064;&#65292;&#32780;&#32852;&#37030;&#26426;&#21046;&#20445;&#25252;&#20102;&#21508;&#20010;&#20195;&#29702;&#20010;&#20307;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#26426;&#21046;&#20063;&#20250;&#26292;&#38706;&#31995;&#32479;&#38754;&#20020;&#24694;&#24847;&#20195;&#29702;&#30340;&#27602;&#21270;&#25915;&#20987;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;&#25105;&#20204;&#20063;&#35752;&#35770;&#20102;&#20174;FL&#32487;&#25215;&#30340;&#19968;&#31181;&#20256;&#32479;&#38450;&#24481;&#31574;&#30053;&#20197;&#20943;&#36731;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#27602;&#21270;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has become a popular tool for solving traditional Reinforcement Learning (RL) tasks. The multi-agent structure addresses the major concern of data-hungry in traditional RL, while the federated mechanism protects the data privacy of individual agents. However, the federated mechanism also exposes the system to poisoning by malicious agents that can mislead the trained policy. Despite the advantage brought by FL, the vulnerability of Federated Reinforcement Learning (FRL) has not been well-studied before. In this work, we propose the first general framework to characterize FRL poisoning as an optimization problem constrained by a limited budget and design a poisoning protocol that can be applied to policy-based FRL and extended to FRL with actor-critic as a local RL algorithm by training a pair of private and public critics. We also discuss a conventional defense strategy inherited from FL to mitigate this risk. We verify our poisoning effectiveness by conducting 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.06683</link><description>&lt;p&gt;
&#25163;&#26415;&#32858;&#21512;&#65306;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#21644;&#22810;&#26679;&#20219;&#21153;&#21327;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#19987;&#27880;&#20110;&#26816;&#27979;&#24739;&#32773;&#21487;&#33021;&#21516;&#26102;&#20986;&#29616;&#30340;&#19968;&#37096;&#20998;&#21457;&#29616;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#21327;&#35843;&#23545;&#20110;&#32858;&#21512;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#26415;&#32858;&#21512;&#65292;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#30340;iid&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25163;&#26415;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25163;&#26415;&#32858;&#21512;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#30340;&#31574;&#30053;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#24182;&#34892;&#35889;&#32858;&#31867;&#30340;&#20998;&#24067;&#24335;&#22359;Chebyshev-Davidson&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#35889;&#20272;&#35745;&#21644;&#20998;&#24067;&#24335;&#24182;&#34892;&#35745;&#31639;&#23454;&#29616;&#39640;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04443</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24182;&#34892;&#35889;&#32858;&#31867;&#30340;&#20998;&#24067;&#24335;&#22359;Chebyshev-Davidson&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Distributed Block Chebyshev-Davidson Algorithm for Parallel Spectral Clustering. (arXiv:2212.04443v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#24182;&#34892;&#35889;&#32858;&#31867;&#30340;&#20998;&#24067;&#24335;&#22359;Chebyshev-Davidson&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#35889;&#20272;&#35745;&#21644;&#20998;&#24067;&#24335;&#24182;&#34892;&#35745;&#31639;&#23454;&#29616;&#39640;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22359;Chebyshev-Davidson&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35889;&#32858;&#31867;&#20013;&#22823;&#35268;&#27169;&#39046;&#20808;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;Chebyshev-Davidson&#31639;&#27861;&#30340;&#25928;&#29575;&#20381;&#36182;&#20110;&#20808;&#21069;&#23545;&#29305;&#24449;&#20540;&#35889;&#30340;&#20102;&#35299;&#65292;&#32780;&#23545;&#20110;&#20272;&#35745;&#26469;&#35828;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#36890;&#36807;&#23545;&#35889;&#32858;&#31867;&#20013;&#30340;&#25289;&#26222;&#25289;&#26031;&#25110;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#36827;&#34892;&#35299;&#26512;&#35889;&#20272;&#35745;&#65292;&#21487;&#20197;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35889;&#32858;&#31867;&#20013;&#38750;&#24120;&#39640;&#25928;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#20351;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20998;&#26512;&#22823;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21644;&#24182;&#34892;&#29256;&#26412;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#24182;&#34892;&#35745;&#31639;&#30340;&#21152;&#36895;&#27604;&#22823;&#32422;&#31561;&#20110;$\sqrt{p}$&#65292;&#20854;&#20013;$p$&#34920;&#31034;&#36827;&#31243;&#30340;&#25968;&#37327;&#12290;{&#25105;&#20204;&#23558;&#25552;&#20379;&#25968;&#20540;&#32467;&#26524;&#65292;&#20197;&#35777;&#26126;&#23427;&#22312;&#35889;&#32858;&#31867;&#20013;&#30340;&#25928;&#29575;&#20197;&#21450;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#19982;&#29616;&#26377;&#29305;&#24449;&#20540;&#27714;&#35299;&#22120;&#30456;&#27604;&#30340;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;}
&lt;/p&gt;
&lt;p&gt;
We develop a distributed Block Chebyshev-Davidson algorithm to solve large-scale leading eigenvalue problems for spectral analysis in spectral clustering. First, the efficiency of the Chebyshev-Davidson algorithm relies on the prior knowledge of the eigenvalue spectrum, which could be expensive to estimate. This issue can be lessened by the analytic spectrum estimation of the Laplacian or normalized Laplacian matrices in spectral clustering, making the proposed algorithm very efficient for spectral clustering. Second, to make the proposed algorithm capable of analyzing big data, a distributed and parallel version has been developed with attractive scalability. The speedup by parallel computing is approximately equivalent to $\sqrt{p}$, where $p$ denotes the number of processes. {Numerical results will be provided to demonstrate its efficiency in spectral clustering and scalability advantage over existing eigensolvers used for spectral clustering in parallel computing environments.}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#31561;&#26041;&#27861;&#26469;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2211.07866</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21512;&#24182;&#19979;&#30340;&#32437;&#21521;&#32593;&#32476;&#26377;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#31561;&#26041;&#27861;&#26469;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#32593;&#32476;&#30001;&#22810;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#24207;&#21015;&#32452;&#25104;&#65292;&#20854;&#20013;&#26102;&#38388;&#36793;&#22312;&#23454;&#26102;&#20013;&#34987;&#35266;&#23519;&#21040;&#12290;&#38543;&#30528;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#20852;&#36215;&#65292;&#23427;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24448;&#24448;&#34987;&#24573;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#30340;&#20248;&#21183;&#12290;&#23427;&#21512;&#24182;&#30456;&#37051;&#30340;&#31232;&#30095;&#32593;&#32476;&#65292;&#20197;&#25193;&#22823;&#35266;&#27979;&#36793;&#30340;&#25968;&#37327;&#24182;&#20943;&#23569;&#20272;&#35745;&#26041;&#24046;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#26102;&#38388;&#32467;&#26500;&#36827;&#34892;&#33258;&#36866;&#24212;&#32593;&#32476;&#37051;&#22495;&#25511;&#21046;&#24341;&#20837;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20419;&#36827;&#20272;&#35745;&#65292;&#20854;&#20013;&#27599;&#27425;&#36845;&#20195;&#30340;&#20272;&#35745;&#38169;&#35823;&#19978;&#30028;&#34987;&#24314;&#31435;&#12290;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#26469;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#26032;&#39062;&#30340;Bregman&#25955;&#24230;&#35270;&#35282;&#65292;&#20998;&#26512;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#25214;&#21040;&#20102;&#20248;&#21270;&#36807;&#31243;&#26356;&#23481;&#26131;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#20004;&#31181;&#26041;&#27861;&#33021;&#22815;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12511</link><description>&lt;p&gt;
&#38477;&#20302;&#38590;&#24230;&#21644;&#25552;&#39640;&#40065;&#26834;&#24615;&#65306;&#22522;&#20110; Bregman &#25955;&#24230;&#35270;&#35282;&#30340;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (arXiv:2208.12511v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#26469;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#26032;&#39062;&#30340;Bregman&#25955;&#24230;&#35270;&#35282;&#65292;&#20998;&#26512;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#25214;&#21040;&#20102;&#20248;&#21270;&#36807;&#31243;&#26356;&#23481;&#26131;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#20004;&#31181;&#26041;&#27861;&#33021;&#22815;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#26469;&#25913;&#21892;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#20013;&#33719;&#24471;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110; Bregman &#25955;&#24230;&#30340;&#23545;&#25239;&#35757;&#32451;&#35270;&#35282;&#65292;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;AT&#21487;&#20197;&#30475;&#20316;&#26159;&#35757;&#32451;&#25968;&#25454;&#28857;&#22312;&#36127;&#29109;&#26354;&#32447;&#19978;&#30340;&#28369;&#21160;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20010;&#35270;&#35282;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;AT&#26041;&#27861;&#65292;&#21363;PGD-AT&#21644;TRADES&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#21457;&#29616;TRADES&#30340;&#20248;&#21270;&#36807;&#31243;&#27604;PGD-AT&#26356;&#23481;&#26131;&#65292;&#22240;&#20026;TRADES&#21487;&#20197;&#20998;&#31163;PGD-AT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;TRADES&#20013;&#29109;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#20855;&#26377;&#39640;&#29109;&#30340;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;&#21463;&#21040;&#20197;&#19978;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;FAIT&#21644;MER&#65292;&#23427;&#20204;&#19981;&#20165;&#21487;&#20197;&#22312;10&#27493;PGD&#23545;&#25163;&#19979;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#65292;&#36824;&#33021;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;10&#27493;PGD&#23545;&#25163;&#19979;&#38477;&#20302;&#20248;&#21270;&#38590;&#24230;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#33021;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate on improving the adversarial robustness obtained in adversarial training (AT) via reducing the difficulty of optimization. To better study this problem, we build a novel Bregman divergence perspective for AT, in which AT can be viewed as the sliding process of the training data points on the negative entropy curve. Based on this perspective, we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and TRADES, and we find that the optimization process of TRADES is easier than PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function of entropy in TRADES, and we find that models with high entropy can be better robustness learners. Inspired by the above findings, we propose two methods, i.e., FAIT and MER, which can both not only reduce the difficulty of optimization under the 10-step PGD adversaries, but also provide better robustness. Our work suggests that reducing the difficulty of optimization under the 10-step PGD a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#32852;&#21512;&#23398;&#20064;&#25216;&#26415;&#23545;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#21307;&#30103;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38598;&#20013;&#23398;&#20064;&#26041;&#24335;&#30456;&#24403;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.10993</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#25216;&#26415;&#23545;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of federated learning techniques for arrhythmia classification using 12-lead ECG signals. (arXiv:2208.10993v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#32852;&#21512;&#23398;&#20064;&#25216;&#26415;&#23545;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#21307;&#30103;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38598;&#20013;&#23398;&#20064;&#26041;&#24335;&#30456;&#24403;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#23545;&#20110;&#21033;&#29992;&#20302;&#21151;&#32791;&#24515;&#30005;&#30417;&#27979;&#35774;&#22791;&#20449;&#24687;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#12289;&#26356;&#24555;&#35786;&#26029;&#21644;&#26356;&#26377;&#25928;&#27835;&#30103;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#24403;&#20351;&#29992;&#12289;&#19981;&#23433;&#20840;&#30340;&#23384;&#20648;&#25110;&#25968;&#25454;&#27844;&#28431;&#21487;&#33021;&#20250;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#65292;&#33719;&#21462;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25935;&#24863;&#21307;&#30103;&#25968;&#25454;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#26469;&#33258;&#20845;&#20010;&#24322;&#26500;&#26469;&#28304;&#30340;12&#23548;&#32852;&#20256;&#24863;&#22120;&#38453;&#21015;&#37319;&#38598;&#30340;&#39640;&#28165;&#24515;&#30005;&#22270;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#24471;&#27169;&#22411;&#19982;&#20197;&#38598;&#20013;&#24335;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#24335;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#31561;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#29420;&#31435;&#21644;&#30456;&#21516;&#20998;&#24067;&#30340;&#32852;&#21512;&#25968;&#25454;&#20197;&#21450;&#38750;&#30456;&#21516;&#20998;&#24067;&#30340;&#32852;&#21512;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence-based (AI) analysis of large, curated medical datasets is promising for providing early detection, faster diagnosis, and more effective treatment using low-power Electrocardiography (ECG) monitoring devices information. However, accessing sensitive medical data from diverse sources is highly restricted since improper use, unsafe storage, or data leakage could violate a person's privacy. This work uses a Federated Learning (FL) privacy-preserving methodology to train AI models over heterogeneous sets of high-definition ECG from 12-lead sensor arrays collected from six heterogeneous sources. We evaluated the capacity of the resulting models to achieve equivalent performance compared to state-of-the-art models trained in a Centralized Learning (CL) fashion. Moreover, we assessed the performance of our solution over Independent and Identical distributed (IID) and non-IID federated data. Our methodology involves machine learning techniques based on Deep Neural Networ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#22312;&#21046;&#36896;&#19994;&#20013;&#26816;&#27979;&#32570;&#38519;&#30340;&#30740;&#31350;&#12290;&#27604;&#36739;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#37327;&#23376;&#36864;&#28779;&#26426;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#37327;&#23376;&#31639;&#27861;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#31639;&#27861;&#12290;&#35813;&#30740;&#31350;&#26159;&#39318;&#27425;&#23558;&#37327;&#23376;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24212;&#29992;&#20110;&#21046;&#36896;&#19994;&#29983;&#20135;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.04988</link><description>&lt;p&gt;
&#37327;&#23376;&#20154;&#24037;&#35270;&#35273;&#22312;&#21046;&#36896;&#19994;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum artificial vision for defect detection in manufacturing. (arXiv:2208.04988v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#22312;&#21046;&#36896;&#19994;&#20013;&#26816;&#27979;&#32570;&#38519;&#30340;&#30740;&#31350;&#12290;&#27604;&#36739;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#37327;&#23376;&#36864;&#28779;&#26426;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#37327;&#23376;&#31639;&#27861;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#31639;&#27861;&#12290;&#35813;&#30740;&#31350;&#26159;&#39318;&#27425;&#23558;&#37327;&#23376;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24212;&#29992;&#20110;&#21046;&#36896;&#19994;&#29983;&#20135;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#22024;&#26434;&#20013;&#22411;&#37327;&#23376; (NISQ) &#35774;&#22791;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#22522;&#20110;&#36890;&#29992;&#38376;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426; (QSVM) &#21644;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#26426;&#30340; QBoost&#12290;&#36825;&#20123;&#37327;&#23376;&#35270;&#35273;&#31995;&#32479;&#22312;&#29992;&#20110;&#26816;&#27979;&#21046;&#36896;&#27773;&#36710;&#38646;&#20214;&#20013;&#30340;&#32570;&#38519;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#37327;&#23376;&#31639;&#27861;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#65292;&#20854;&#20013; QBoost &#20801;&#35768;&#20351;&#29992;&#29616;&#26377;&#30340;&#37327;&#23376;&#36864;&#28779;&#26426;&#26469;&#20998;&#26512;&#26356;&#22823;&#30340;&#38382;&#39064;&#12290;&#25991;&#20013;&#36824;&#35752;&#35770;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#21253;&#25324;&#38477;&#32500;&#21644;&#23545;&#27604;&#24230;&#22686;&#24378;&#65292;&#20197;&#21450; QBoost &#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#37327;&#23376;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24212;&#29992;&#20110;&#21046;&#36896;&#19994;&#29983;&#20135;&#20013;&#20855;&#26377;&#24037;&#19994;&#24847;&#20041;&#30340;&#38382;&#39064;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider several algorithms for quantum computer vision using Noisy Intermediate-Scale Quantum (NISQ) devices, and benchmark them for a real problem against their classical counterparts. Specifically, we consider two approaches: a quantum Support Vector Machine (QSVM) on a universal gate-based quantum computer, and QBoost on a quantum annealer. The quantum vision systems are benchmarked for an unbalanced dataset of images where the aim is to detect defects in manufactured car pieces. We see that the quantum algorithms outperform their classical counterparts in several ways, with QBoost allowing for larger problems to be analyzed with present-day quantum annealers. Data preprocessing, including dimensionality reduction and contrast enhancement, is also discussed, as well as hyperparameter tuning in QBoost. To the best of our knowledge, this is the first implementation of quantum computer vision systems for a problem of industrial relevance in a manufacturing production 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#26381;&#21153;&#29616;&#24050;&#24320;&#22987;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#20197;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#24182;&#38477;&#20302;&#20113;&#24320;&#38144;&#65292;&#32780;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35774;&#22791;&#19978;&#35757;&#32451;&#26694;&#26550;NNTrainer&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#24182;&#23454;&#29616;&#26234;&#33021;&#26381;&#21153;&#30340;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.04688</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#39046;&#22495;&#65306;&#35774;&#22791;&#19978;&#30340;AI&#35757;&#32451;&#21644;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
A New Frontier of AI: On-Device AI Training and Personalization. (arXiv:2206.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04688
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#26381;&#21153;&#29616;&#24050;&#24320;&#22987;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#20197;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#24182;&#38477;&#20302;&#20113;&#24320;&#38144;&#65292;&#32780;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35774;&#22791;&#19978;&#35757;&#32451;&#26694;&#26550;NNTrainer&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#24182;&#23454;&#29616;&#26234;&#33021;&#26381;&#21153;&#30340;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28040;&#36153;&#30005;&#23376;&#35774;&#22791;&#24320;&#22987;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#26381;&#21153;&#65292;&#32780;&#19981;&#26159;&#22312;&#20113;&#26381;&#21153;&#22120;&#19978;&#65292;&#20197;&#20445;&#25345;&#20010;&#20154;&#25968;&#25454;&#22312;&#35774;&#22791;&#19978;&#24182;&#38477;&#20302;&#32593;&#32476;&#21644;&#20113;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#36235;&#21183;&#35270;&#20026;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#25968;&#25454;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#32780;&#26080;&#38656;&#23558;&#25968;&#25454;&#26292;&#38706;&#22312;&#35774;&#22791;&#20043;&#22806;&#26469;&#20010;&#24615;&#21270;&#26234;&#33021;&#26381;&#21153;&#30340;&#26426;&#20250;&#65306;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#36164;&#28304;&#26377;&#38480;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35774;&#22791;&#19978;&#35757;&#32451;&#26694;&#26550;NNTrainer&#65292;&#23427;&#25552;&#20379;&#39640;&#24230;&#20869;&#23384;&#25928;&#29575;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#32454;&#31890;&#24230;&#25191;&#34892;&#39034;&#24207;&#20998;&#26512;&#36827;&#34892;&#20027;&#21160;&#20132;&#25442;&#12290;&#27492;&#22806;&#65292;&#20854;&#20248;&#21270;&#19981;&#20250;&#29306;&#29298;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#31639;&#27861;&#26159;&#36879;&#26126;&#30340;&#65307;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#30740;&#31350;&#21487;&#20197;&#22312;NNTrainer&#20043;&#19978;&#23454;&#29616;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;NNTrainer&#21487;&#20197;&#23558;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#21040;1/20&#65288;&#33410;&#30465;95%&#65281;&#65289;&#65292;&#24182;&#26377;&#25928;&#22320;&#20010;&#24615;&#21270;&#26234;&#33021;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern consumer electronic devices have started executing deep learning-based intelligence services on devices, not cloud servers, to keep personal data on devices and to reduce network and cloud costs. We find such a trend as the opportunity to personalize intelligence services by updating neural networks with user data without exposing the data out of devices: on-device training. However, the limited resources of devices incurs significant difficulties. We propose a light-weight on-device training framework, NNTrainer, which provides highly memory-efficient neural network training techniques and proactive swapping based on fine-grained execution order analysis for neural networks. Moreover, its optimizations do not sacrifice accuracy and are transparent to training algorithms; thus, prior algorithmic studies may be implemented on top of NNTrainer. The evaluations show that NNTrainer can reduce memory consumption down to 1/20 (saving 95%!) and effectively personalizes intelligence ser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#30417;&#30563;&#65288;SUD&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#30417;&#30563;&#37325;&#24314;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#21046;&#20316;&#29305;&#23450;&#20110;&#25104;&#20687;&#39046;&#22495;&#30340;&#21487;&#24494;&#27491;&#21017;&#21270;&#22120;&#12290;</title><link>http://arxiv.org/abs/2202.02952</link><description>&lt;p&gt;
&#21435;&#22122;&#30417;&#30563;&#12290; (arXiv:2202.02952v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Supervision by Denoising. (arXiv:2202.02952v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#30417;&#30563;&#65288;SUD&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#30417;&#30563;&#37325;&#24314;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#21046;&#20316;&#29305;&#23450;&#20110;&#25104;&#20687;&#39046;&#22495;&#30340;&#21487;&#24494;&#27491;&#21017;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#65292;&#20363;&#22914;&#22522;&#20110; U-Net &#30340;&#27169;&#22411;&#65292;&#22914;&#26524;&#35201;&#20445;&#35777;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21017;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#25104;&#20687;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#20855;&#26377;&#20687;&#32032;&#25110;&#20307;&#32032;&#32423;&#21035;&#31934;&#24230;&#30340;&#26631;&#35760;&#25968;&#25454;&#24456;&#23569;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#65292;&#30001;&#20110;&#19981;&#23384;&#22312;&#21333;&#19968;&#30340;&#26631;&#20934;&#26631;&#31614;&#65292;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#22823;&#37327;&#30340;&#37325;&#22797;&#21464;&#24322;&#24615;&#65292;&#22240;&#27492;&#35757;&#32451;&#37325;&#24314;&#32593;&#32476;&#20174;&#24102;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#65288;&#31216;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#65289;&#65292;&#26159;&#19968;&#20010;&#23454;&#36341;&#21644;&#29702;&#35770;&#19978;&#30340;&#38382;&#39064;&#12290; &#28982;&#32780;&#65292;&#38024;&#23545;&#22270;&#20687;&#37325;&#24314;&#30340;&#20256;&#32479;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#38024;&#23545;&#26576;&#20123;&#32473;&#23450;&#25104;&#20687;&#38382;&#39064;&#30340;&#21487;&#24494;&#27491;&#21017;&#21270;&#22120;&#65292;&#36825;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#21435;&#22122;&#30417;&#30563;&#8221;&#65288;SUD&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#30417;&#30563;&#37325;&#24314;&#27169;&#22411;&#65292;&#21516;&#26102;&#36991;&#20813;&#25163;&#21160;&#21046;&#20316;&#29305;&#23450;&#20110;&#25104;&#20687;&#39046;&#22495;&#30340;&#21487;&#24494;&#27491;&#21017;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based image reconstruction models, such as those based on the U-Net, require a large set of labeled images if good generalization is to be guaranteed. In some imaging domains, however, labeled data with pixel- or voxel-level label accuracy are scarce due to the cost of acquiring them. This problem is exacerbated further in domains like medical imaging, where there is no single ground truth label, resulting in large amounts of repeat variability in the labels. Therefore, training reconstruction networks to generalize better by learning from both labeled and unlabeled examples (called semi-supervised learning) is problem of practical and theoretical interest. However, traditional semi-supervised learning methods for image reconstruction often necessitate handcrafting a differentiable regularizer specific to some given imaging problem, which can be extremely time-consuming. In this work, we propose "supervision by denoising" (SUD), a framework that enables us to supervise reconst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#25509;&#21644;&#33258;&#20027;&#36710;&#36742;&#22312;&#28151;&#21512;&#20132;&#36890;&#20013;&#30340;&#21512;&#20316;&#36710;&#36947;&#21464;&#26356;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#27599;&#20010;&#36710;&#36742;&#26681;&#25454;&#21160;&#26426;&#21046;&#23450;&#36710;&#36947;&#21464;&#26356;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2111.06318</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28151;&#21512;&#20132;&#36890;&#20013;&#36830;&#25509;&#21644;&#33258;&#20027;&#36710;&#36947;&#21464;&#26356;&#30340;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning for Cooperative Lane Changing of Connected and Autonomous Vehicles in Mixed Traffic. (arXiv:2111.06318v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#25509;&#21644;&#33258;&#20027;&#36710;&#36742;&#22312;&#28151;&#21512;&#20132;&#36890;&#20013;&#30340;&#21512;&#20316;&#36710;&#36947;&#21464;&#26356;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#27599;&#20010;&#36710;&#36742;&#26681;&#25454;&#21160;&#26426;&#21046;&#23450;&#36710;&#36947;&#21464;&#26356;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;20&#24180;&#26469;&#65292;&#33258;&#20027;&#39550;&#39542;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#35768;&#22810;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#35299;&#25918;&#39550;&#39542;&#21592;&#30340;&#21171;&#32047;&#39550;&#39542;&#21644;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#31561;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36710;&#36947;&#21464;&#26356;&#20173;&#28982;&#26159;&#33258;&#20027;&#36710;&#36742;&#65288;AV&#65289;&#38754;&#20020;&#30340;&#19968;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#28151;&#21512;&#21644;&#21160;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312;AV&#30340;&#36710;&#36947;&#21464;&#26356;&#20915;&#31574;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#30340;&#22823;&#22810;&#25968;&#37117;&#38598;&#20013;&#22312;&#21333;&#19968;&#36710;&#36742;&#35774;&#32622;&#19979;&#65292;&#22312;&#22810;&#20010;AV&#19982;&#20154;&#39550;&#39542;&#36710;&#36742;&#65288;HDV&#65289;&#20849;&#23384;&#30340;&#24773;&#20917;&#19979;&#65292;&#36710;&#36947;&#21464;&#26356;&#30340;&#30740;&#31350;&#21364;&#40092;&#26377;&#28041;&#21450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22810;&#20010;AV&#22312;&#28151;&#21512;&#20132;&#36890;&#39640;&#36895;&#20844;&#36335;&#29615;&#22659;&#20013;&#30340;&#36710;&#36947;&#21464;&#26356;&#20915;&#31574;&#21046;&#23450;&#24314;&#27169;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#65292;&#27599;&#20010;AV&#26681;&#25454;&#21160;&#26426;&#21046;&#23450;&#36710;&#36947;&#21464;&#26356;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL), a powerful data-driven control method, has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the moti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#22312;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#26041;&#27861;&#21644;&#21435;&#22122;&#25216;&#26415;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.06166</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#21338;&#24328;&#35770;
&lt;/p&gt;
&lt;p&gt;
Game Theory for Adversarial Attacks and Defenses. (arXiv:2110.06166v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#22312;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#26041;&#27861;&#21644;&#21435;&#22122;&#25216;&#26415;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#26045;&#21152;&#23567;&#20294;&#26377;&#24847;&#30340;&#26368;&#22351;&#24773;&#20917;&#25200;&#21160;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#36825;&#23548;&#33268;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20063;&#20250;&#20197;&#39640;&#32622;&#20449;&#24230;&#36755;&#20986;&#38169;&#35823;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#34987;&#24320;&#21457;&#20986;&#26469;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#36991;&#20813;&#23427;&#20204;&#34987;&#25915;&#20987;&#12290;&#36880;&#28176;&#24418;&#25104;&#20102;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#31867;&#20284;&#20110;&#28216;&#25103;&#30340;&#31454;&#20105;&#65292;&#21452;&#26041;&#37117;&#35797;&#22270;&#22312;&#24444;&#27492;&#20043;&#38388;&#21457;&#25381;&#26368;&#20339;&#31574;&#30053;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#33258;&#24049;&#30340;&#22238;&#25253;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28216;&#25103;&#65292;&#27599;&#20010;&#29609;&#23478;&#26681;&#25454;&#23545;&#25163;&#30340;&#31574;&#30053;&#36873;&#25321;&#30340;&#39044;&#27979;&#26469;&#36873;&#25321;&#23545;&#25163;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31449;&#22312;&#38450;&#23432;&#30340;&#35282;&#24230;&#65292;&#36816;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#26469;&#38450;&#24481;&#25915;&#20987;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#38543;&#26426;&#28608;&#27963;&#20462;&#21098;&#65292;&#26469;&#21019;&#24314;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21435;&#22122;&#26041;&#27861;&#26469;&#20943;&#23569;&#23545;&#25239;&#24615;&#25200;&#21160;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks can generate adversarial inputs by applying small but intentionally worst-case perturbations to samples from the dataset, which leads to even state-of-the-art deep neural networks outputting incorrect answers with high confidence. Hence, some adversarial defense techniques are developed to improve the security and robustness of the models and avoid them being attacked. Gradually, a game-like competition between attackers and defenders formed, in which both players would attempt to play their best strategies against each other while maximizing their own payoffs. To solve the game, each player would choose an optimal strategy against the opponent based on the prediction of the opponent's strategy choice. In this work, we are on the defensive side to apply game-theoretic approaches on defending against attacks. We use two randomization methods, random initialization and stochastic activation pruning, to create diversity of networks. Furthermore, we use one denoising te
&lt;/p&gt;</description></item><item><title>AutoGL&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24211;&#65292;&#20351;&#29992;&#26041;&#20415;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22270;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2104.04987</link><description>&lt;p&gt;
AutoGL&#65306;&#33258;&#21160;&#22270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
AutoGL: A Library for Automated Graph Learning. (arXiv:2104.04987v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.04987
&lt;/p&gt;
&lt;p&gt;
AutoGL&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24211;&#65292;&#20351;&#29992;&#26041;&#20415;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22270;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#20026;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#25163;&#21160;&#35774;&#35745;&#26368;&#20248;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19981;&#28789;&#27963;&#12289;&#36153;&#26102;&#65292;&#24182;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#22270;&#19978;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#22270;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#33258;&#21160;&#35774;&#35745;&#26368;&#20248;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24211;&#20013;&#27809;&#26377;&#19968;&#20010;&#33021;&#23436;&#20840;&#25903;&#25345;&#22270;&#19978;&#30340;AutoML&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#22270;&#23398;&#20064;&#65288;AutoGL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24211;&#12290;AutoGL&#26159;&#24320;&#28304;&#30340;&#65292;&#26131;&#20110;&#20351;&#29992;&#65292;&#32780;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#35774;&#22791;&#20132;&#20114;&#30340;&#21518;&#31471;&#12289;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#21644;&#25903;&#25345;&#30340;&#22270;&#24212;&#29992;&#30340;&#19977;&#23618;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an upsurge in research interests and applications of machine learning on graphs. However, manually designing the optimal machine learning algorithms for different graph datasets and tasks is inflexible, labor-intensive, and requires expert knowledge, limiting its adaptivity and applicability. Automated machine learning (AutoML) on graphs, aiming to automatically design the optimal machine learning algorithm for a given graph dataset and task, has received considerable attention. However, none of the existing libraries can fully support AutoML on graphs. To fill this gap, we present Automated Graph Learning (AutoGL), the first dedicated library for automated machine learning on graphs. AutoGL is open-source, easy to use, and flexible to be extended. Specifically, we propose a three-layer architecture, consisting of backends to interface with devices, a complete automated graph learning pipeline, and supported graph applications. The automated machine learning
&lt;/p&gt;</description></item></channel></rss>