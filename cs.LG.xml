<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>LagrangeBench&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#19971;&#20010;&#26032;&#30340;&#27969;&#20307;&#21147;&#23398;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#21644;&#29289;&#29702;&#29305;&#24615;&#65289;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;API&#21644;&#24050;&#24314;&#31435;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;JAX&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16342</link><description>&lt;p&gt;
LagrangeBench: &#19968;&#31181;&#25289;&#26684;&#26391;&#26085;&#27969;&#20307;&#21147;&#23398;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite. (arXiv:2309.16342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16342
&lt;/p&gt;
&lt;p&gt;
LagrangeBench&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#19971;&#20010;&#26032;&#30340;&#27969;&#20307;&#21147;&#23398;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#21644;&#29289;&#29702;&#29305;&#24615;&#65289;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;API&#21644;&#24050;&#24314;&#31435;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;JAX&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#20013;&#25104;&#21151;&#24212;&#29992;&#20110;&#22522;&#20110;&#32593;&#26684;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#31163;&#25955;&#21270;&#30340;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#28041;&#21450;&#33258;&#30001;&#34920;&#38754;&#25110;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#26102;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LagrangeBench&#65292;&#36825;&#26159;&#38024;&#23545;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#37325;&#28857;&#26159;&#26102;&#38388;&#31895;&#31890;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#65306;(a)&#20351;&#29992;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;SPH&#65289;&#26041;&#27861;&#29983;&#25104;&#30340;&#19971;&#20010;&#26032;&#30340;&#27969;&#20307;&#21147;&#23398;&#25968;&#25454;&#38598;&#65288;&#20854;&#20013;&#22235;&#20010;&#26159;2D&#30340;&#65292;&#19977;&#20010;&#26159;3D&#30340;&#65289;&#65292;&#21253;&#25324;&#20102;Taylor-Green&#28065;&#26059;&#12289;&#39537;&#21160;&#19978;&#30422;&#12289;&#21453;Poiseuille&#27969;&#21160;&#21644;&#26029;&#22365;&#31561;&#19981;&#21516;&#29289;&#29702;&#29305;&#24615;&#65292;&#22914;&#22266;&#20307;&#22721;&#30456;&#20114;&#20316;&#29992;&#25110;&#33258;&#30001;&#34920;&#38754;&#65292;(b)&#39640;&#25928;&#30340;&#22522;&#20110;JAX&#30340;API&#65292;&#37197;&#22791;&#19981;&#21516;&#30340;&#36817;&#26399;&#35757;&#32451;&#31574;&#30053;&#21644;&#37051;&#23621;&#25628;&#32034;&#20363;&#31243;&#65292;&#20197;&#21450;(c)&#24050;&#24314;&#31435;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22914;GNS&#21644;SEGNN&#30340;JAX&#23454;&#29616;&#19982;&#22522;&#20934;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#24230;&#37327;...
&lt;/p&gt;
&lt;p&gt;
Machine learning has been successfully applied to grid-based PDE modeling in various scientific applications. However, learned PDE solvers based on Lagrangian particle discretizations, which are the preferred approach to problems with free surfaces or complex physics, remain largely unexplored. We present LagrangeBench, the first benchmarking suite for Lagrangian particle problems, focusing on temporal coarse-graining. In particular, our contribution is: (a) seven new fluid mechanics datasets (four in 2D and three in 3D) generated with the Smoothed Particle Hydrodynamics (SPH) method including the Taylor-Green vortex, lid-driven cavity, reverse Poiseuille flow, and dam break, each of which includes different physics like solid wall interactions or free surface, (b) efficient JAX-based API with various recent training strategies and neighbors search routine, and (c) JAX implementation of established Graph Neural Networks (GNNs) like GNS and SEGNN with baseline results. Finally, to measu
&lt;/p&gt;</description></item><item><title>ADGym&#26159;&#19968;&#27454;&#38024;&#23545;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#35774;&#35745;&#36873;&#25321;&#30340;&#32508;&#21512;&#35780;&#20272;&#21644;&#33258;&#21160;&#36873;&#25321;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2309.15376</link><description>&lt;p&gt;
ADGym&#65306;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#35774;&#35745;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
ADGym: Design Choices for Deep Anomaly Detection. (arXiv:2309.15376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15376
&lt;/p&gt;
&lt;p&gt;
ADGym&#26159;&#19968;&#27454;&#38024;&#23545;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#35774;&#35745;&#36873;&#25321;&#30340;&#32508;&#21512;&#35780;&#20272;&#21644;&#33258;&#21160;&#36873;&#25321;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#65292;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#26381;&#21153;&#21644;&#20113;&#35745;&#31639;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#23558;&#28145;&#24230;AD&#31639;&#27861;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#36827;&#34892;&#35780;&#20272;&#65292;&#26410;&#33021;&#29702;&#35299;&#20010;&#21035;&#35774;&#35745;&#36873;&#25321;&#65288;&#22914;&#25439;&#22833;&#20989;&#25968;&#21644;&#32593;&#32476;&#26550;&#26500;&#65289;&#30340;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#21407;&#22987;&#27493;&#39588;&#65288;&#22914;&#39044;&#22788;&#29702;&#65289;&#30340;&#37325;&#35201;&#24615;&#21487;&#33021;&#34987;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#26550;&#26500;&#25152;&#25513;&#30422;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20004;&#20010;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#30095;&#28431;&#65306;&#65288;i&#65289;&#28145;&#24230;AD&#26041;&#27861;&#30340;&#21738;&#20123;&#32452;&#25104;&#37096;&#20998;&#65288;&#21363;&#35774;&#35745;&#36873;&#25321;&#65289;&#22312;&#26816;&#27979;&#24322;&#24120;&#26041;&#38754;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65311;&#65288;ii&#65289;&#25105;&#20204;&#22914;&#20309;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#35774;&#35745;&#36873;&#25321;&#26469;&#26500;&#24314;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#23450;&#21046;AD&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#36890;&#29992;&#30340;&#12289;&#39044;&#20808;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ADGym&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#21644;&#33258;&#21160;&#36873;&#25321;AD&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) techniques have recently been applied to anomaly detection (AD), yielding successful outcomes in areas such as finance, medical services, and cloud computing. However, much of the current research evaluates a deep AD algorithm holistically, failing to understand the contributions of individual design choices like loss functions and network architectures. Consequently, the importance of prerequisite steps, such as preprocessing, might be overshadowed by the spotlight on novel loss functions and architectures. In this paper, we address these oversights by posing two questions: (i) Which components (i.e., design choices) of deep AD methods are pivotal in detecting anomalies? (ii) How can we construct tailored AD algorithms for specific datasets by selecting the best design choices automatically, rather than relying on generic, pre-existing solutions? To this end, we introduce ADGym, the first platform designed for comprehensive evaluation and automatic selection of AD d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#20013;"&#26102;&#38388;&#21516;&#27493;"&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#22681;&#38047;&#26102;&#38388;&#32780;&#19981;&#26159;&#24773;&#33410;&#36827;&#23637;&#26469;&#23454;&#29616;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.14989</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33410;&#22863;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Tempo Adaption in Non-stationary Reinforcement Learning. (arXiv:2309.14989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#20013;"&#26102;&#38388;&#21516;&#27493;"&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#22681;&#38047;&#26102;&#38388;&#32780;&#19981;&#26159;&#24773;&#33410;&#36827;&#23637;&#26469;&#23454;&#29616;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39318;&#20808;&#25105;&#20204;&#25552;&#20986;&#24182;&#35299;&#20915;&#20102;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#8220;&#26102;&#38388;&#21516;&#27493;&#8221;&#38382;&#39064;&#65292;&#36825;&#26159;&#38459;&#30861;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#29616;&#23454;&#20013;&#65292;&#29615;&#22659;&#30340;&#21464;&#21270;&#26159;&#25353;&#29031;&#22681;&#38047;&#26102;&#38388;&#65288;$\mathfrak{t}$&#65289;&#32780;&#19981;&#26159;&#25353;&#29031;&#24773;&#33410;&#36827;&#23637;&#65288;$k$&#65289;&#21457;&#29983;&#30340;&#65292;&#20854;&#20013;&#22681;&#38047;&#26102;&#38388;&#34920;&#31034;&#22266;&#23450;&#25345;&#32493;&#26102;&#38388;$\mathfrak{t} \in [0, T]$&#20869;&#23454;&#38469;&#27969;&#36893;&#30340;&#26102;&#38388;&#12290;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#65292;&#22312;&#24773;&#33410;$k$&#26102;&#65292;&#26234;&#33021;&#20307;&#29983;&#25104;&#19968;&#20010;&#36712;&#36857;&#24182;&#35757;&#32451;&#19968;&#20010;&#31574;&#30053;&#65292;&#28982;&#21518;&#36716;&#20837;&#24773;&#33410;$k+1$&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#19981;&#21516;&#27493;&#30340;&#29615;&#22659;&#19979;&#65292;&#26234;&#33021;&#20307;&#22312;&#26102;&#38388;$\mathfrak{t}_k$&#20998;&#37197;$\Delta \mathfrak{t}$&#29992;&#20110;&#36712;&#36857;&#29983;&#25104;&#21644;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;$\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$&#26102;&#21051;&#36716;&#20837;&#19979;&#19968;&#20010;&#24773;&#33410;&#12290;&#23613;&#31649;&#24773;&#33410;&#24635;&#25968;&#22266;&#23450;&#65288;$K$&#65289;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#30456;&#20114;&#20316;&#29992;&#26102;&#38388;&#30340;&#36873;&#25321;&#65288;$\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfrak{t}_K$&#65289;&#31215;&#32047;&#19981;&#21516;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\mathfrak{t} \in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\mathfrak{t}_k$ allocates $\Delta \mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \textit{interaction times} ($\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#24490;&#29615;&#27169;&#22411;&#32467;&#21512;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;PO&#36830;&#32493;&#25511;&#21046;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#19981;&#35268;&#21017;&#35266;&#23519;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14078</link><description>&lt;p&gt;
&#22522;&#20110;ODE&#30340;&#26080;&#27169;&#22411;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;POMDPs
&lt;/p&gt;
&lt;p&gt;
ODE-based Recurrent Model-free Reinforcement Learning for POMDPs. (arXiv:2309.14078v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#24490;&#29615;&#27169;&#22411;&#32467;&#21512;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;PO&#36830;&#32493;&#25511;&#21046;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#19981;&#35268;&#21017;&#35266;&#23519;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#24314;&#27169;&#29289;&#29702;&#26426;&#21046;&#30340;&#26631;&#20934;&#65292;&#23427;&#20204;&#26377;&#21161;&#20110;&#22312;&#26410;&#30693;&#30340;&#29289;&#29702;&#25110;&#29983;&#29289;&#29615;&#22659;&#20013;&#36827;&#34892;&#36817;&#20284;&#25512;&#26029;&#12290;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#65288;PO&#65289;&#29615;&#22659;&#20013;&#65292;&#22914;&#20309;&#20174;&#21407;&#22987;&#35266;&#27979;&#20013;&#25512;&#26029;&#30475;&#19981;&#35265;&#30340;&#20449;&#24687;&#22256;&#25200;&#30528;&#20195;&#29702;&#20154;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#32039;&#20945;&#19978;&#19979;&#25991;&#30340;&#24490;&#29615;&#31574;&#30053;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#26469;&#20174;&#21382;&#21490;&#36716;&#25442;&#20013;&#25552;&#21462;&#19981;&#21487;&#35266;&#23519;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#24110;&#21161;&#20195;&#29702;&#20154;&#25552;&#21462;&#26356;&#22810;&#19982;&#21160;&#24577;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ODE&#30340;&#24490;&#29615;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#20102;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;PO&#36830;&#32493;&#25511;&#21046;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30001;&#20110;ODEs&#20855;&#26377;&#24314;&#27169;&#19981;&#35268;&#21017;&#24615;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#35268;&#21017;&#35266;&#23519;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (ODEs) are widely recognized as the standard for modeling physical mechanisms, which help to perform approximate inference in unknown physical or biological environments. In partially observable (PO) environments, how to infer unseen information from raw observations puzzled the agents. By using a recurrent policy with a compact context, context-based reinforcement learning provides a flexible way to extract unobservable information from historical transitions. To help the agent extract more dynamics-related information, we present a novel ODE-based recurrent model combines with model-free reinforcement learning (RL) framework to solve partially observable Markov decision processes (POMDPs). We experimentally demonstrate the efficacy of our methods across various PO continuous control and meta-RL tasks. Furthermore, our experiments illustrate that our method is robust against irregular observations, owing to the ability of ODEs to model irregularl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20813;&#21435;&#26679;&#26412;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#65288;CIL&#65289;&#20013;&#30340;&#24322;&#36136;&#24615;&#31867;&#21035;&#20998;&#24067;&#38382;&#39064;&#65292;&#20351;&#29992;&#21407;&#22411;&#32593;&#32476;&#21644;&#25913;&#36827;&#30340;&#21508;&#21521;&#24322;&#24615;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#36827;&#34892;&#29305;&#24449;&#20998;&#31867;&#21644;&#24314;&#27169;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38750;&#24658;&#23450;&#25968;&#25454;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20998;&#24067;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.14062</link><description>&lt;p&gt;
FeCAM&#65306;&#22312;&#20813;&#21435;&#26679;&#26412;&#30340;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#31867;&#21035;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning. (arXiv:2309.14062v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20813;&#21435;&#26679;&#26412;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#65288;CIL&#65289;&#20013;&#30340;&#24322;&#36136;&#24615;&#31867;&#21035;&#20998;&#24067;&#38382;&#39064;&#65292;&#20351;&#29992;&#21407;&#22411;&#32593;&#32476;&#21644;&#25913;&#36827;&#30340;&#21508;&#21521;&#24322;&#24615;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#36827;&#34892;&#29305;&#24449;&#20998;&#31867;&#21644;&#24314;&#27169;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38750;&#24658;&#23450;&#25968;&#25454;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20998;&#24067;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20813;&#21435;&#26679;&#26412;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#65288;CIL&#65289;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#31105;&#27490;&#20102;&#26469;&#33258;&#20808;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22238;&#39038;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26368;&#36817;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20043;&#21518;&#20923;&#32467;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#20110;CIL&#30340;&#21407;&#22411;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#20923;&#32467;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#29983;&#25104;&#26032;&#30340;&#31867;&#21035;&#21407;&#22411;&#65292;&#24182;&#26681;&#25454;&#21040;&#21407;&#22411;&#30340;&#27431;&#27663;&#36317;&#31163;&#23545;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;&#31867;&#21035;&#29305;&#24449;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#27431;&#27663;&#24230;&#37327;&#30340;&#20998;&#31867;&#23545;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#29305;&#24449;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#38750;&#24658;&#23450;&#25968;&#25454;&#20013;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27431;&#27663;&#24230;&#37327;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#19988;&#29305;&#24449;&#20998;&#24067;&#26159;&#24322;&#36136;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;CIL&#30340;&#21508;&#21521;&#24322;&#24615;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#24314;&#27169;&#29305;&#24449;&#21327;&#26041;&#24046;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32416;&#27491;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#26469;&#20943;&#23569;&#27169;&#25311;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#36817;&#20284;&#20505;&#36873;&#29366;&#24577;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#25552;&#39640;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.13985</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20462;&#27491;&#36870;&#38382;&#39064;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Physics-Driven ML-Based Modelling for Correcting Inverse Estimation. (arXiv:2309.13985v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32416;&#27491;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#26469;&#20943;&#23569;&#27169;&#25311;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#36817;&#20284;&#20505;&#36873;&#29366;&#24577;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#25552;&#39640;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;&#26102;&#65292;&#36991;&#20813;&#22833;&#36133;&#30340;&#20272;&#35745;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#20135;&#29983;&#28798;&#38590;&#24615;&#21518;&#26524;&#65292;&#20363;&#22914;&#22312;&#33322;&#31354;&#21457;&#21160;&#26426;&#35774;&#35745;&#20013;&#12290;&#26412;&#25991;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#27169;&#25311;&#21644;&#21463;&#29289;&#29702;&#23450;&#24459;&#25351;&#23548;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#22312;&#37319;&#29992;SAE&#36870;&#38382;&#39064;&#26102;&#26816;&#27979;&#21644;&#32416;&#27491;&#22833;&#36133;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#20854;&#29289;&#29702;&#27169;&#22411;&#35823;&#24046;&#36229;&#36807;&#21487;&#34892;&#38408;&#20540;&#26102;&#26631;&#35760;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GEESE&#26469;&#36890;&#36807;&#20248;&#21270;&#26469;&#32416;&#27491;&#65292;&#26088;&#22312;&#25552;&#20379;&#20302;&#35823;&#24046;&#21644;&#39640;&#25928;&#24615;&#12290;GEESE&#30340;&#20851;&#38190;&#35774;&#35745;&#21253;&#25324;&#65288;1&#65289;&#28151;&#21512;&#20195;&#29702;&#35823;&#24046;&#27169;&#22411;&#65292;&#20197;&#25552;&#20379;&#24555;&#36895;&#30340;&#35823;&#24046;&#20272;&#35745;&#20197;&#20943;&#23569;&#27169;&#25311;&#25104;&#26412;&#65292;&#24182;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#35823;&#24046;&#21453;&#39304;&#20256;&#25773;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#20505;&#36873;&#29366;&#24577;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#29992;&#20110;&#27169;&#25311;&#24320;&#21457;&#21644;&#25506;&#32034;&#34892;&#20026;&#12290;&#36825;&#19977;&#20010;&#27169;&#22411;&#26159;&#21327;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying machine learning estimators in science and engineering (SAE) domains, it is critical to avoid failed estimations that can have disastrous consequences, e.g., in aero engine design. This work focuses on detecting and correcting failed state estimations before adopting them in SAE inverse problems, by utilizing simulations and performance metrics guided by physical laws. We suggest to flag a machine learning estimation when its physical model error exceeds a feasible threshold, and propose a novel approach, GEESE, to correct it through optimization, aiming at delivering both low error and high efficiency. The key designs of GEESE include (1) a hybrid surrogate error model to provide fast error estimations to reduce simulation cost and to enable gradient based backpropagation of error feedback, and (2) two generative models to approximate the probability distributions of the candidate states for simulating the exploitation and exploration behaviours. All three models are co
&lt;/p&gt;</description></item><item><title>&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33410;&#28857;&#32039;&#20945;&#24615;&#8221;&#24230;&#37327;&#26469;&#25351;&#23548;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.13944</link><description>&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#35777;&#26126;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Training for Graph Contrastive Learning. (arXiv:2309.13944v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13944
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33410;&#28857;&#32039;&#20945;&#24615;&#8221;&#24230;&#37327;&#26469;&#25351;&#23548;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20174;&#22686;&#24378;&#22270;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#32780;&#26080;&#38656;&#26631;&#31614;&#30340;&#27969;&#34892;&#35757;&#32451;&#26041;&#27861;&#12290;&#23613;&#31649;&#26368;&#22823;&#21270;&#27491;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#24182;&#26368;&#23567;&#21270;&#36127;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#20851;&#38190;&#21407;&#21017;&#24050;&#32463;&#24471;&#21040;&#30830;&#35748;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#65292;&#26159;&#21542;&#26377;&#19968;&#20123;&#33410;&#28857;&#22987;&#32456;&#25353;&#29031;&#36825;&#19968;&#21407;&#21017;&#36827;&#34892;&#33391;&#22909;&#35757;&#32451;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#19979;&#20063;&#26159;&#22914;&#27492;&#65311;&#36824;&#26159;&#26377;&#19968;&#20123;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#22312;&#22270;&#22686;&#24378;&#20013;&#26410;&#32463;&#35757;&#32451;&#65292;&#24182;&#36829;&#21453;&#36825;&#19968;&#21407;&#21017;&#65311;&#22914;&#20309;&#21306;&#20998;&#36825;&#20123;&#33410;&#28857;&#24182;&#36827;&#19968;&#27493;&#25351;&#23548;GCL&#30340;&#35757;&#32451;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#23454;&#39564;&#35777;&#25454;&#65292;&#34920;&#26126;GCL&#30340;&#35757;&#32451;&#22312;&#25152;&#26377;&#33410;&#28857;&#19978;&#30830;&#23454;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#8220;&#33410;&#28857;&#32039;&#20945;&#24615;&#8221;&#65292;&#23427;&#26159;&#33410;&#28857;&#36981;&#24490;GCL&#21407;&#21017;&#19982;&#22686;&#24378;&#33539;&#22260;&#30456;&#20851;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23548;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node embeddings from augmented graphs without labels. Despite the key principle that maximizing the similarity between positive node pairs while minimizing it between negative node pairs is well established, some fundamental problems are still unclear. Considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? Or are there some nodes more likely to be untrained across graph augmentations and violate the principle? How to distinguish these nodes and further guide the training of GCL? To answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. To address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. We further derive the
&lt;/p&gt;</description></item><item><title>&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#19979;&#65292;&#40065;&#26834;&#24615;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#22312;&#26356;&#23454;&#38469;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#27169;&#22411;&#19979;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#22312;&#23398;&#20064;&#35823;&#24046;&#19979;&#30028;&#21644;&#26029;&#28857;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13591</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#20998;&#24067;&#24335;&#23398;&#20064;&#65306;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#19979;&#30340;&#20005;&#26684;&#35823;&#24046;&#30028;&#21644;&#26029;&#28857;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity. (arXiv:2309.13591v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13591
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#19979;&#65292;&#40065;&#26834;&#24615;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#22312;&#26356;&#23454;&#38469;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#27169;&#22411;&#19979;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#22312;&#23398;&#20064;&#35823;&#24046;&#19979;&#30028;&#21644;&#26029;&#28857;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#22343;&#21248;&#24615;&#30340;&#24773;&#20917;&#19979;&#19982;&#23454;&#38469;&#35266;&#27979;&#21563;&#21512;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#19979;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#35823;&#24046;&#19979;&#30028;&#22522;&#26412;&#19978;&#26159;&#31354;&#27934;&#30340;&#65292;&#24182;&#19988;&#19982;&#23454;&#38469;&#35266;&#27979;&#26126;&#26174;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26356;&#21152;&#23454;&#38469;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#27169;&#22411;&#65292;&#31216;&#20026;&#65288;G&#65292;B&#65289;-&#26799;&#24230;&#24046;&#24322;&#24230;&#65292;&#24182;&#35777;&#26126;&#23427;&#28085;&#30422;&#20102;&#27604;&#29616;&#26377;&#29702;&#35770;&#26356;&#22810;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#19979;&#30340;&#26029;&#28857;&#20998;&#26512;&#20302;&#20110;&#32463;&#20856;&#30340;1/2&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20219;&#20309;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#30340;&#23398;&#20064;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#30340;&#40065;&#26834;&#21464;&#31181;&#30340;&#21305;&#37197;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is homogeneous. Under data heterogeneity however, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely (G,B)-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction 1/2. We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;BLASTNet 2.0&#65292;&#21253;&#21547;&#19977;&#32500;&#39640;&#20445;&#30495;&#21387;&#32553;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#20116;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#31070;&#32463;&#32553;&#25918;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#35268;&#27169;&#12289;&#25104;&#26412;&#21644;&#26550;&#26500;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.13457</link><description>&lt;p&gt;
&#28966;&#28857;&#20013;&#30340;&#28237;&#27969;&#65306;&#29992;BLASTNet 2.0&#25968;&#25454;&#22522;&#20934;&#27979;&#37327;&#19977;&#32500;&#20307;&#31215;&#36229;&#20998;&#36776;&#29575;&#30340;&#32553;&#25918;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data. (arXiv:2309.13457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;BLASTNet 2.0&#65292;&#21253;&#21547;&#19977;&#32500;&#39640;&#20445;&#30495;&#21387;&#32553;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#20116;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#31070;&#32463;&#32553;&#25918;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#35268;&#27169;&#12289;&#25104;&#26412;&#21644;&#26550;&#26500;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#28237;&#27969;&#27969;&#21160;&#30340;&#20998;&#26512;&#23545;&#25512;&#36827;&#12289;&#33021;&#28304;&#29983;&#25104;&#21644;&#29615;&#22659;&#30456;&#20851;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BLASTNet 2.0&#65292;&#23427;&#26159;&#19968;&#20010;&#21253;&#21547;744&#20010;&#23436;&#25972;&#22495;&#26679;&#26412;&#26469;&#33258;34&#20010;&#39640;&#20445;&#30495;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#30340;2.2TB&#25968;&#25454;&#38598;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#19977;&#32500;&#39640;&#20445;&#30495;&#21453;&#24212;&#21644;&#38750;&#21453;&#24212;&#21387;&#32553;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#25968;&#25454;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#20102;49&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20116;&#20010;&#21464;&#20307;&#65292;&#29992;&#20110;&#25913;&#36827;&#31185;&#23398;&#25104;&#20687;&#12289;&#27169;&#25311;&#12289;&#28237;&#27969;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#31070;&#32463;&#32553;&#25918;&#20998;&#26512;&#65292;&#20197;&#26816;&#26597;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20004;&#31181;&#31185;&#23398;ML&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65288;i&#65289;&#39044;&#27979;&#24615;&#33021;&#21487;&#20197;&#38543;&#27169;&#22411;&#35268;&#27169;&#21644;&#25104;&#26412;&#32780;&#25193;&#23637;&#65292;&#65288;ii&#65289;&#26550;&#26500;&#23588;&#20854;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20197;&#21450;&#65288;iii&#65289;b...
&lt;/p&gt;
&lt;p&gt;
Analysis of compressible turbulent flows is essential for applications related to propulsion, energy generation, and the environment. Here, we present BLASTNet 2.0, a 2.2 TB network-of-datasets containing 744 full-domain samples from 34 high-fidelity direct numerical simulations, which addresses the current limited availability of 3D high-fidelity reacting and non-reacting compressible turbulent flow simulation data. With this data, we benchmark a total of 49 variations of five deep learning approaches for 3D super-resolution - which can be applied for improving scientific imaging, simulations, turbulence models, as well as in computer vision applications. We perform neural scaling analysis on these models to examine the performance of different machine learning (ML) approaches, including two scientific ML techniques. We demonstrate that (i) predictive performance can scale with model size and cost, (ii) architecture matters significantly, especially for smaller models, and (iii) the b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#21644;&#22806;&#22312;&#36807;&#31243;&#20043;&#38388;&#30340;&#25910;&#32553;&#36895;&#29575;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#36895;&#29575;&#22312;&#36866;&#24403;&#21305;&#37197;&#24179;&#28369;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.10918</link><description>&lt;p&gt;
Riemannian&#27969;&#24418;&#19978;Matern&#39640;&#26031;&#36807;&#31243;&#30340;&#21518;&#39564;&#25910;&#32553;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds. (arXiv:2309.10918v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#21644;&#22806;&#22312;&#36807;&#31243;&#20043;&#38388;&#30340;&#25910;&#32553;&#36895;&#29575;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#36895;&#29575;&#22312;&#36866;&#24403;&#21305;&#37197;&#24179;&#28369;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#35768;&#22810;&#20381;&#36182;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#22312;&#20960;&#20309;&#35774;&#32622;&#19979;&#22788;&#29702;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#24037;&#20855;&#65292;&#20363;&#22914;&#65292;&#24403;&#36755;&#20837;&#20301;&#20110;Riemannian&#27969;&#24418;&#19978;&#26102;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#20869;&#22312;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#26159;&#21542;&#21487;&#20197;&#35777;&#26126;&#30456;&#27604;&#20110;&#23558;&#25152;&#26377;&#30456;&#20851;&#37327;&#23884;&#20837;&#21040;$\mathbb{R}^d$&#24182;&#20351;&#29992;&#26222;&#36890;&#27431;&#20960;&#37324;&#24503;&#39640;&#26031;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#20248;&#25910;&#32553;&#36895;&#29575;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#27969;&#24418;&#21644;&#29615;&#22659;Sobolev&#31354;&#38388;&#20043;&#38388;&#30340;&#36857;&#21644;&#25193;&#23637;&#23450;&#29702;&#35777;&#26126;&#20102;&#22806;&#22312;&#36807;&#31243;&#30340;&#31867;&#20284;&#36895;&#29575;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#19982;&#20869;&#22312;&#36807;&#31243;&#30340;&#36895;&#29575;&#30456;&#31526;&#65292;&#21069;&#25552;&#26159;&#23427;&#20204;&#30340;&#24179;&#28369;&#21442;&#25968;&#36866;&#24403;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#23454;&#35777;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#36895;&#29575;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Mat\'ern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#21152;&#36895;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#21644;&#32858;&#31867;&#36136;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08710</link><description>&lt;p&gt;
&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Clustered Multi-Agent Linear Bandits. (arXiv:2309.08710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#21152;&#36895;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#21644;&#32858;&#31867;&#36136;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#21363;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#26469;&#21152;&#36895;&#25972;&#20307;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#36129;&#29486;&#20013;&#65292;&#32593;&#32476;&#25511;&#21046;&#22120;&#36127;&#36131;&#20272;&#35745;&#32593;&#32476;&#30340;&#22522;&#26412;&#38598;&#32676;&#32467;&#26500;&#24182;&#20248;&#21270;&#21516;&#19968;&#32452;&#20013;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#32463;&#39564;&#20998;&#20139;&#12290;&#25105;&#20204;&#23545;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#21644;&#32858;&#31867;&#36136;&#37327;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#19982;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#25105;&#20204;&#30340;&#31639;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#25104;&#21151;&#24674;&#22797;&#20102;&#30495;&#23454;&#30340;&#22522;&#26412;&#38598;&#32676;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address in this paper a particular instance of the multi-agent linear stochastic bandit problem, called clustered multi-agent linear bandits. In this setting, we propose a novel algorithm leveraging an efficient collaboration between the agents in order to accelerate the overall optimization problem. In this contribution, a network controller is responsible for estimating the underlying cluster structure of the network and optimizing the experiences sharing among agents within the same groups. We provide a theoretical analysis for both the regret minimization problem and the clustering quality. Through empirical evaluation against state-of-the-art algorithms on both synthetic and real data, we demonstrate the effectiveness of our approach: our algorithm significantly improves regret minimization while managing to recover the true underlying cluster partitioning.
&lt;/p&gt;</description></item><item><title>Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.07778</link><description>&lt;p&gt;
Virchow: &#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07778
&lt;/p&gt;
&lt;p&gt;
Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#30149;&#29702;&#23398;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#30284;&#30151;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23545;&#20110;&#35768;&#22810;&#29305;&#23450;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#65292;&#25968;&#25454;&#37327;&#19981;&#36275;&#20197;&#36827;&#34892;&#24320;&#21457;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Virchow&#65292;&#19968;&#20010;632&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#30149;&#29702;&#23398;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;Virchow&#22312;1.5&#30334;&#19975;&#20010;&#19981;&#21516;&#32452;&#32455;&#26679;&#26412;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#25968;&#25454;&#37327;&#22823;&#24471;&#22810;&#12290;&#22312;&#21253;&#25324;&#29926;&#29255;&#32423;&#20840;&#30284;&#26816;&#27979;&#21644;&#20122;&#22411;&#20197;&#21450;&#24187;&#28783;&#29255;&#32423;&#29983;&#29289;&#26631;&#24535;&#29289;&#39044;&#27979;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;Virchow&#22312;&#26469;&#33258;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20154;&#32676;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#22343;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;BDyMA&#65289;&#26469;&#35299;&#20915;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#26694;&#26550;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#26356;&#31232;&#30095;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#12290;</title><link>http://arxiv.org/abs/2309.07080</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#65306;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain. (arXiv:2309.07080v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;BDyMA&#65289;&#26469;&#35299;&#20915;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#26694;&#26550;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#26356;&#31232;&#30095;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#65288;DEC&#65289;&#21487;&#20197;&#25581;&#31034;&#22823;&#33041;&#30340;&#22797;&#26434;&#26426;&#21046;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#35780;&#20998;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#21457;&#29616;&#26041;&#27861;&#22312;&#25552;&#21462;&#22240;&#26524;&#32467;&#26500;&#21644;&#25512;&#26029;&#26377;&#25928;&#36830;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#23398;&#20064;DEC&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#19968;&#20010;&#26159;&#39640;&#32500;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#30340;&#26681;&#26412;&#26080;&#33021;&#21147;&#65292;&#21478;&#19968;&#20010;&#26159;fMRI&#25968;&#25454;&#36136;&#37327;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;M-&#30697;&#38453;&#26080;&#29615;&#29305;&#24615;&#30340;&#36125;&#21494;&#26031;&#21160;&#24577;DAG&#23398;&#20064;&#65288;BDyMA&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#21457;&#29616;DEC&#20013;&#30340;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#22240;&#26524;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#21452;&#21521;&#36793;&#32536;&#12290;&#21033;&#29992;BDyMA&#26041;&#27861;&#20013;&#30340;&#26080;&#32422;&#26463;&#26694;&#26550;&#22312;&#26816;&#27979;&#39640;&#32500;&#32593;&#32476;&#26041;&#38754;&#21487;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#26356;&#31232;&#30095;&#30340;&#32467;&#26524;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;DEC&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the complex mechanisms of the brain can be unraveled by extracting the Dynamic Effective Connectome (DEC). Recently, score-based Directed Acyclic Graph (DAG) discovery methods have shown significant improvements in extracting the causal structure and inferring effective connectivity. However, learning DEC through these methods still faces two main challenges: one with the fundamental impotence of high-dimensional dynamic DAG discovery methods and the other with the low quality of fMRI data. In this paper, we introduce Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization \textbf{(BDyMA)} method to address the challenges in discovering DEC. The presented dynamic causal model enables us to discover bidirected edges as well. Leveraging an unconstrained framework in the BDyMA method leads to more accurate results in detecting high-dimensional networks, achieving sparser outcomes, making it particularly suitable for extracting DEC. Additionally, the score f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#26469;&#22788;&#29702;&#39046;&#22495;&#36716;&#31227;&#21644;&#35821;&#20041;&#36716;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20840;&#23616;&#20302;&#32423;&#29305;&#24449;&#21644;&#23494;&#38598;&#39640;&#32423;&#29305;&#24449;&#22270;&#26469;&#36866;&#24212;&#27169;&#22411;&#21040;&#26410;&#35265;&#39046;&#22495;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05994</link><description>&lt;p&gt;
ATTA: &#19968;&#31181;&#38024;&#23545;&#20998;&#21106;&#20013;&#30340;&#21306;&#20998;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24322;&#24120;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation. (arXiv:2309.05994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#26469;&#22788;&#29702;&#39046;&#22495;&#36716;&#31227;&#21644;&#35821;&#20041;&#36716;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20840;&#23616;&#20302;&#32423;&#29305;&#24449;&#21644;&#23494;&#38598;&#39640;&#32423;&#29305;&#24449;&#22270;&#26469;&#36866;&#24212;&#27169;&#22411;&#21040;&#26410;&#35265;&#39046;&#22495;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#31264;&#23494;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#20284;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#20551;&#35774;&#23427;&#20204;&#20043;&#38388;&#19981;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#24120;&#24120;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#65292;&#24182;&#19988;&#26174;&#33879;&#24433;&#21709;&#29616;&#26377;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#65292;&#21516;&#26102;&#22788;&#29702;&#39046;&#22495;&#36716;&#31227;&#21644;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#23618;&#21033;&#29992;&#20840;&#23616;&#20302;&#32423;&#29305;&#24449;&#21306;&#20998;&#22270;&#20687;&#20013;&#26159;&#21542;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#65292;&#32780;&#31532;&#20108;&#23618;&#21033;&#29992;&#23494;&#38598;&#39640;&#32423;&#29305;&#24449;&#22270;&#35782;&#21035;&#20855;&#26377;&#35821;&#20041;&#36716;&#31227;&#30340;&#20687;&#32032;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#36866;&#24212;&#27169;&#22411;&#21040;&#26410;&#35265;&#39046;&#22495;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22330;&#22806;&#20998;&#21106;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324; those with significant domain shifts and those without&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, obse
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;&#22312;&#26080;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#26469;&#36798;&#21040;Nash&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.04272</link><description>&lt;p&gt;
&#23398;&#20064;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity. (arXiv:2309.04272v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;&#22312;&#26080;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#26469;&#36798;&#21040;Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#65288;LQ&#65289;&#21338;&#24328;&#22312;&#26368;&#20248;&#25511;&#21046;&#20013;&#26159;&#22522;&#30784;&#24615;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#65288;i&#65289;&#39118;&#38505;&#25935;&#24863;&#25110;&#40065;&#26834;&#25511;&#21046;&#30340;&#21160;&#24577;&#21338;&#24328;&#24418;&#24335;&#65292;&#25110;&#32773;&#65288;ii&#65289;&#20316;&#20026;&#36830;&#32493;&#29366;&#24577;-&#25511;&#21046;&#31354;&#38388;&#20013;&#20004;&#20010;&#31454;&#20105;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#35774;&#32622;&#12290;&#19982;&#24191;&#27867;&#30740;&#31350;&#30340;&#21333;&#26234;&#33021;&#20307;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#38382;&#39064;&#19981;&#21516;&#65292;&#38646;&#21644;LQ&#21338;&#24328;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#32570;&#20047;&#24378;&#21046;&#24615;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;&#38750;&#20985;&#26368;&#23567;-&#26368;&#22823;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#25511;&#21046;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#27809;&#26377;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#27169;&#22411;&#26080;&#20851;&#35774;&#32622;&#20013;&#65292;&#24352;&#31561;&#20154;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#65292;&#20197;&#36798;&#21040;Nash&#22343;&#34913;&#30340;&#949;-&#37051;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#29702;&#24819;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#24494;&#22937;&#36873;&#25321;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.03800</link><description>&lt;p&gt;
&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#25968;&#25454;&#12289;&#35745;&#31639;&#12289;&#23485;&#24230;&#21644;&#36816;&#27668;
&lt;/p&gt;
&lt;p&gt;
Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#24494;&#22937;&#36873;&#25321;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#24494;&#22937;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#31163;&#32447;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#20851;&#22810;&#23618;&#24863;&#30693;&#22120;&#26799;&#24230;&#35757;&#32451;&#30340;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#32479;&#35745;&#26597;&#35810;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#21487;&#20197;&#35299;&#37322;&#20026;&#22810;&#36164;&#28304;&#30340;&#26435;&#34913;&#21069;&#27839;&#65306;&#25104;&#21151;&#23398;&#20064;&#21482;&#26377;&#22312;&#19968;&#20010;&#36275;&#22815;&#20016;&#23500;&#65288;&#22823;&#22411;&#27169;&#22411;&#65289;&#12289;&#30693;&#35782;&#28170;&#21338;&#65288;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65289;&#12289;&#32784;&#24515;&#65288;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#22810;&#65289;&#25110;&#24184;&#36816;&#65288;&#38543;&#26426;&#29468;&#27979;&#27425;&#25968;&#22810;&#65289;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#36825;&#37324;&#65292;&#23485;&#24230;&#36215;&#21040;&#20102;&#24182;&#34892;&#25628;&#32034;&#30340;&#20316;&#29992;&#65306;&#23427;&#22686;&#21152;&#20102;&#25214;&#21040;&#8220;&#24184;&#36816;&#31070;&#32463;&#20803;&#8221;&#30340;&#27010;&#29575;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#31232;&#30095;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#32479;&#35745;&#26174;&#33879;&#21487;&#20998;&#31163;&#30340;&#26368;&#20339;&#39640;&#26031;&#31751;&#30340;&#20998;&#32452;&#65292;&#23454;&#29616;&#36229;&#32858;&#31867;&#12290;&#31639;&#27861;&#20855;&#26377;&#19977;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#34920;&#31034;&#25968;&#25454;&#38598;&#20026;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;-&#31751;&#12289;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#20272;&#35745;&#31751;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#22823;&#23567;&#20197;&#21450;&#23558;&#31751;&#32452;&#21512;&#20026;&#36229;&#31751;&#12290;&#31639;&#27861;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#30697;&#38453;&#36136;&#37327;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21512;&#36866;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#27700;&#24179;&#26469;&#30830;&#23450;&#26368;&#20339;&#36229;&#31751;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.02623</link><description>&lt;p&gt;
&#36890;&#36807;&#25214;&#21040;&#26174;&#33879;&#21487;&#20998;&#31163;&#30340;&#26368;&#20339;&#39640;&#26031;&#31751;&#30340;&#20998;&#32452;&#65292;&#23454;&#29616;&#36229;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Superclustering by finding statistically significant separable groups of optimal gaussian clusters. (arXiv:2309.02623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#32479;&#35745;&#26174;&#33879;&#21487;&#20998;&#31163;&#30340;&#26368;&#20339;&#39640;&#26031;&#31751;&#30340;&#20998;&#32452;&#65292;&#23454;&#29616;&#36229;&#32858;&#31867;&#12290;&#31639;&#27861;&#20855;&#26377;&#19977;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#34920;&#31034;&#25968;&#25454;&#38598;&#20026;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;-&#31751;&#12289;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#20272;&#35745;&#31751;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#22823;&#23567;&#20197;&#21450;&#23558;&#31751;&#32452;&#21512;&#20026;&#36229;&#31751;&#12290;&#31639;&#27861;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#30697;&#38453;&#36136;&#37327;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21512;&#36866;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#27700;&#24179;&#26469;&#30830;&#23450;&#26368;&#20339;&#36229;&#31751;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#39640;&#26031;&#31751;&#20998;&#32452;&#25104;&#32479;&#35745;&#21487;&#20998;&#31163;&#30340;&#36229;&#31751;&#30340;&#31639;&#27861;&#12290;&#31639;&#27861;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#23558;&#25968;&#25454;&#38598;&#34920;&#31034;&#20026;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;-&#31751;&#65292;&#20854;&#25968;&#37327;&#22522;&#20110;BIC&#20934;&#21017;&#30340;&#26368;&#23567;&#20540;&#30830;&#23450;&#65307;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#20272;&#35745;&#31751;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#31751;&#30340;&#22823;&#23567;&#65307;&#20351;&#29992;DBSCAN&#26041;&#27861;&#23558;&#24471;&#21040;&#30340;&#31751;&#32452;&#21512;&#25104;&#36229;&#31751;&#65292;&#36890;&#36807;&#25214;&#21040;&#20854;&#36229;&#21442;&#25968;&#65288;&#26368;&#22823;&#36317;&#31163;&#65289;&#22312;&#26368;&#22823;&#25968;&#37327;&#30340;&#36229;&#31751;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#24341;&#20837;&#30340;&#30697;&#38453;&#36136;&#37327;&#20934;&#21017;&#30340;&#26368;&#22823;&#20540;&#12290;&#30697;&#38453;&#36136;&#37327;&#20934;&#21017;&#23545;&#24212;&#20110;&#25152;&#26377;&#21457;&#29616;&#30340;&#36229;&#31751;&#20013;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#20998;&#31163;&#30340;&#36229;&#31751;&#25152;&#21344;&#27604;&#20363;&#12290;&#35813;&#31639;&#27861;&#21482;&#26377;&#19968;&#20010;&#36229;&#21442;&#25968;-&#32479;&#35745;&#26174;&#33879;&#24615;&#27700;&#24179;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#21160;&#20026;&#20854;&#36873;&#25321;&#21512;&#36866;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the algorithm for clustering a dataset by grouping the optimal, from the point of view of the BIC criterion, number of Gaussian clusters into the optimal, from the point of view of their statistical separability, superclusters.  The algorithm consists of three stages: representation of the dataset as a mixture of Gaussian distributions - clusters, which number is determined based on the minimum of the BIC criterion; using the Mahalanobis distance, to estimate the distances between the clusters and cluster sizes; combining the resulting clusters into superclusters using the DBSCAN method by finding its hyperparameter (maximum distance) providing maximum value of introduced matrix quality criterion at maximum number of superclusters. The matrix quality criterion corresponds to the proportion of statistically significant separated superclusters among all found superclusters.  The algorithm has only one hyperparameter - statistical significance level, and automatically d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02460</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#37329;&#34701;&#24066;&#22330;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#19978;&#30340;&#38750;&#27861;&#27963;&#21160;&#28608;&#22686;&#23548;&#33268;&#20102;&#26222;&#36890;&#29992;&#25143;&#25968;&#21313;&#20159;&#30340;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#33719;&#24471;&#25163;&#24037;&#29305;&#24449;&#65292;&#35201;&#20040;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20122;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#38382;&#39064;&#23450;&#20041;&#20026;&#24102;&#26377;&#36793;&#23646;&#24615;&#30340;&#26377;&#21521;&#22810;&#22270;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;DIAM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#19978;&#26377;&#25928;&#22320;&#26816;&#27979;&#38750;&#27861;&#36134;&#25143;&#12290;&#39318;&#20808;&#65292;DIAM&#21253;&#21547;&#19968;&#20010;Edge2Seq&#27169;&#22359;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#36793;&#23646;&#24615;&#21644;&#26377;&#21521;&#36793;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#33258;&#21160;&#23398;&#20064;&#26377;&#25928;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#12290;&#28982;&#21518;&#21033;&#29992;t
&lt;/p&gt;
&lt;p&gt;
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#30340;&#35282;&#33394;&#20998;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#34892;&#21160;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#20010;&#25163;&#33218;&#29992;&#20110;&#22266;&#23450;&#29289;&#20307;&#65292;&#21478;&#19968;&#20010;&#25163;&#33218;&#29992;&#20110;&#25191;&#34892;&#20219;&#21153;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#31283;&#23450;&#20998;&#31867;&#22120;&#21644;&#34892;&#21160;&#31574;&#30053;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#19968;&#26694;&#26550;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.01087</link><description>&lt;p&gt;
&#31283;&#23450;&#34892;&#21160;&#65306;&#23398;&#20064;&#21327;&#35843;&#21452;&#25163;&#25805;&#20316;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stabilize to Act: Learning to Coordinate for Bimanual Manipulation. (arXiv:2309.01087v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01087
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#30340;&#35282;&#33394;&#20998;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#34892;&#21160;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#20010;&#25163;&#33218;&#29992;&#20110;&#22266;&#23450;&#29289;&#20307;&#65292;&#21478;&#19968;&#20010;&#25163;&#33218;&#29992;&#20110;&#25191;&#34892;&#20219;&#21153;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#31283;&#23450;&#20998;&#31867;&#22120;&#21644;&#34892;&#21160;&#31574;&#30053;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#19968;&#26694;&#26550;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20016;&#23500;&#21644;&#28789;&#24039;&#30340;&#25805;&#20316;&#30340;&#20851;&#38190;&#26159;&#33021;&#22815;&#21327;&#35843;&#25511;&#21046;&#20004;&#21482;&#25163;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#21452;&#25163;&#26426;&#22120;&#20154;&#31995;&#32479;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#65292;&#20294;&#26500;&#24314;&#21452;&#33218;&#33258;&#20027;&#31995;&#32479;&#30340;&#25511;&#21046;&#31574;&#30053;&#21364;&#38754;&#20020;&#38590;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22256;&#38590;&#26159;&#21452;&#25163;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#65292;&#36825;&#32473;&#22522;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20174;&#20154;&#31867;&#36523;&#19978;&#24471;&#21040;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#20998;&#37197;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65306;&#19968;&#20010;&#31283;&#23450;&#30340;&#33218;&#23558;&#29289;&#20307;&#22266;&#23450;&#22312;&#19968;&#20010;&#20301;&#32622;&#19978;&#65292;&#31616;&#21270;&#29615;&#22659;&#65292;&#32780;&#19968;&#20010;&#27963;&#21160;&#30340;&#33218;&#21017;&#25191;&#34892;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#31283;&#23450;&#20998;&#31867;&#22120;&#26469;&#23454;&#26045; BimanUal Dexterity from Stabilization (BUDS) &#26694;&#26550;&#65292;&#35813;&#20998;&#31867;&#22120;&#20132;&#26367;&#26356;&#26032;&#23398;&#20064;&#21040;&#30340;&#31283;&#23450;&#20301;&#32622;&#20197;&#20445;&#25345;&#29615;&#22659;&#31283;&#23450;&#65292;&#24182;&#21033;&#29992;&#31034;&#33539;&#23398;&#20064;&#21040;&#30340;&#34892;&#21160;&#31574;&#30053;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;BUDS&#36827;&#34892;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#21452;&#25163;&#20219;&#21153;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key to rich, dexterous manipulation in the real world is the ability to coordinate control across two hands. However, while the promise afforded by bimanual robotic systems is immense, constructing control policies for dual arm autonomous systems brings inherent difficulties. One such difficulty is the high-dimensionality of the bimanual action space, which adds complexity to both model-based and data-driven methods. We counteract this challenge by drawing inspiration from humans to propose a novel role assignment framework: a stabilizing arm holds an object in place to simplify the environment while an acting arm executes the task. We instantiate this framework with BimanUal Dexterity from Stabilization (BUDS), which uses a learned restabilizing classifier to alternate between updating a learned stabilization position to keep the environment unchanged, and accomplishing the task with an acting policy learned from demonstrations. We evaluate BUDS on four bimanual tasks of varying compl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMILIE&#30340;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36845;&#20195;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#32534;&#36753;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.00613</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#36845;&#20195;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Iterative Multi-granular Image Editing using Diffusion Models. (arXiv:2309.00613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMILIE&#30340;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36845;&#20195;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#32534;&#36753;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#21019;&#24847;&#19987;&#19994;&#20154;&#21592;&#29983;&#25104;&#33402;&#26415;&#21644;&#23457;&#32654;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#35270;&#35273;&#36164;&#20135;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#20805;&#20998;&#25903;&#25345;&#36825;&#26679;&#30340;&#21019;&#24847;&#21162;&#21147;&#65292;&#35813;&#36807;&#31243;&#24212;&#20855;&#22791;&#20197;&#19979;&#33021;&#21147;&#65306;1&#65289;&#36845;&#20195;&#22320;&#32534;&#36753;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;2&#65289;&#25511;&#21046;&#25152;&#38656;&#21464;&#21270;&#30340;&#31354;&#38388;&#33539;&#22260;&#65288;&#20840;&#23616;&#12289;&#23616;&#37096;&#25110;&#20171;&#20110;&#20004;&#32773;&#20043;&#38388;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#23454;&#29992;&#30340;&#38382;&#39064;&#35774;&#23450;&#27491;&#24335;&#21270;&#20026;&#36845;&#20195;&#22810;&#31890;&#24230;&#32534;&#36753;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#21512;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#19968;&#27425;&#24615;&#25805;&#20316;&#65288;&#21363;&#27809;&#26377;&#36845;&#20195;&#32534;&#36753;&#33021;&#21147;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#33258;&#28982;&#20135;&#29983;&#22810;&#31890;&#24230;&#25511;&#21046;&#65288;&#21363;&#28085;&#30422;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#32534;&#36753;&#30340;&#20840;&#35889;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMILIE&#65306;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#12290;EMILIE&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36845;&#20195;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20419;&#36827;&#36845;&#20195;&#32534;&#36753;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#25152;&#38656;&#21464;&#21270;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control opera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#22810;&#27493;&#22238;&#24402;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#38543;&#26426;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00073</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#35299;&#20915;&#22810;&#27493;&#22238;&#24402;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#38543;&#26426;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Variational Autoencoder for Tackling Stochasticity in Multi-Step Regression Stock Price Prediction. (arXiv:2309.00073v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#22810;&#27493;&#22238;&#24402;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#38543;&#26426;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20869;&#30340;&#22810;&#27493;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#23545;&#20110;&#39044;&#27979;&#27874;&#21160;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#24471;&#37329;&#34701;&#26426;&#26500;&#33021;&#22815;&#23450;&#20215;&#21644;&#23545;&#20914;&#34893;&#29983;&#21697;&#65292;&#24182;&#35753;&#38134;&#34892;&#37327;&#21270;&#20854;&#20132;&#26131;&#31807;&#20013;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#37329;&#34701;&#30417;&#31649;&#26426;&#26500;&#36824;&#35201;&#27714;&#26426;&#26500;&#25237;&#36164;&#32773;&#26377;&#20960;&#22825;&#30340;&#27969;&#21160;&#24615;&#26399;&#38480;&#20174;&#20854;&#39118;&#38505;&#36164;&#20135;&#20013;&#36864;&#20986;&#65292;&#20197;&#36991;&#20813;&#23545;&#24066;&#22330;&#20215;&#26684;&#20135;&#29983;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32929;&#31080;&#25968;&#25454;&#20855;&#26377;&#39640;&#24230;&#38543;&#26426;&#24615;&#65292;&#22810;&#27493;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#20219;&#21153;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#21333;&#27493;&#22522;&#20110;&#20998;&#31867;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#34920;&#24449;&#34920;&#36798;&#21147;&#26041;&#38754;&#26377;&#38480;&#12290;&#38543;&#30528;&#30446;&#26631;&#20215;&#26684;&#24207;&#21015;&#30340;&#24341;&#20837;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#27979;&#35797;&#26102;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#22240;&#20026;&#30446;&#26631;&#20215;&#26684;&#24207;&#21015;&#20013;&#20063;&#21253;&#21547;&#38543;&#26426;&#22122;&#22768;&#65292;&#38477;&#20302;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#28145;&#23618;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#21644;&#25193;&#25955;&#27010;&#29575;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step stock price prediction over a long-term horizon is crucial for forecasting its volatility, allowing financial institutions to price and hedge derivatives, and banks to quantify the risk in their trading books. Additionally, most financial regulators also require a liquidity horizon of several days for institutional investors to exit their risky assets, in order to not materially affect market prices. However, the task of multi-step stock price prediction is challenging, given the highly stochastic nature of stock data. Current solutions to tackle this problem are mostly designed for single-step, classification-based predictions, and are limited to low representation expressiveness. The problem also gets progressively harder with the introduction of the target price sequence, which also contains stochastic noise and reduces generalizability at test-time. To tackle these issues, we combine a deep hierarchical variational-autoencoder (VAE) and diffusion probabilistic techniques
&lt;/p&gt;</description></item><item><title>NAS-X&#26159;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#25512;&#26029;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14864</link><description>&lt;p&gt;
NAS-X: &#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NAS-X: Neural Adaptive Smoothing via Twisting. (arXiv:2308.14864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14864
&lt;/p&gt;
&lt;p&gt;
NAS-X&#26159;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#25512;&#26029;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NAS-X&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#36827;&#34892;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#26029;&#12290;NAS-X&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#28508;&#21464;&#37327;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;SMC&#26041;&#27861;&#26469;&#25311;&#21512;&#27604;&#20256;&#32479;&#30340;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#26041;&#27861;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;NAS-X&#65292;&#24182;&#21457;&#29616;&#22312;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#26041;&#38754;&#65292;&#23427;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;&#21464;&#20998;&#21644;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for learning and inference in sequential latent variable models based on reweighted wake-sleep (RWS). NAS-X works with both discrete and continuous latent variables, and leverages smoothing SMC to fit a broader range of models than traditional RWS methods. We test NAS-X on discrete and continuous tasks and find that it substantially outperforms previous variational and RWS-based methods in inference and parameter recovery.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#36125;&#21494;&#26031;&#36873;&#25321;&#21644;&#38646;&#26679;&#26412;&#39044;&#27979;&#22120;&#30340;&#21033;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21512;&#29702;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21160;&#24577;&#36873;&#21462;&#22320;&#22312;&#32447;&#25209;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#20943;&#23569;&#22122;&#22768;&#21644;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.10544</link><description>&lt;p&gt;
&#36739;&#24555;&#30340;&#27169;&#22411;&#35757;&#32451;&#20043;&#36335;: &#22522;&#20110;&#36125;&#21494;&#26031;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Accelerated Model Training via Bayesian Data Selection. (arXiv:2308.10544v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10544
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#36125;&#21494;&#26031;&#36873;&#25321;&#21644;&#38646;&#26679;&#26412;&#39044;&#27979;&#22120;&#30340;&#21033;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21512;&#29702;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21160;&#24577;&#36873;&#21462;&#22320;&#22312;&#32447;&#25209;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#20943;&#23569;&#22122;&#22768;&#21644;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#12289;&#37325;&#22797;&#25110;&#26377;&#20559;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#29978;&#33267;&#38459;&#30861;&#27169;&#22411;&#25910;&#25947;&#12290;&#20256;&#32479;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#31616;&#21333;&#25110;&#22256;&#38590;&#26679;&#26412;&#65292;&#32570;&#20047;&#21516;&#26102;&#22788;&#29702;&#22810;&#26679;&#24773;&#20917;&#30340;&#28789;&#27963;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21512;&#29702;&#30340;&#25968;&#25454;&#36873;&#25321;&#21407;&#21017;&#65292;&#36890;&#36807;&#26816;&#26597;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20854;&#23454;&#38469;&#24212;&#29992;&#20381;&#36182;&#20110;&#19981;&#22826;&#21487;&#38752;&#30340;&#36817;&#20284;&#26041;&#27861;&#21644;&#39069;&#22806;&#30340;holdout&#25968;&#25454;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#22120;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#22823;&#37327;&#25968;&#25454;&#22122;&#22768;&#21644;&#19981;&#24179;&#34913;&#24615;&#30340;&#22312;&#32447;&#25209;&#37327;&#36873;&#25321;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;WebVision&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#36739;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy to implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06828</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#31867;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#34701;&#21512;Electra Transformer&#12289;GloVe&#21644;LSTM
&lt;/p&gt;
&lt;p&gt;
An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24050;&#32463;&#25104;&#20026;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#20998;&#31867;&#26041;&#38754;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#65292;&#38382;&#39064;&#20998;&#31867;&#19987;&#27880;&#20110;&#30830;&#23450;&#25152;&#38656;&#20449;&#24687;&#30340;&#31867;&#22411;&#65292;&#36825;&#26159;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#31561;&#19979;&#28216;&#24212;&#29992;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38382;&#39064;&#20998;&#31867;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;Electra&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#33879;&#21517;&#30340;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#25972;&#21512;&#36825;&#20123;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;Electra&#25552;&#20379;&#20102;&#22522;&#20110;transformer&#30340;&#22797;&#26434;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;GloVe&#25552;&#20379;&#20102;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#20197;&#25429;&#25417;&#35789;&#32423;&#35821;&#20041;&#65292;LSTM&#21017;&#36129;&#29486;&#20102;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#20197;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#32423;&#25439;&#22833;&#39033;&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#24130;&#36816;&#31639;&#65292;&#20351;&#21508;&#20010;&#25439;&#22833;&#39033;&#22312;&#25968;&#37327;&#32423;&#19978;&#22823;&#33268;&#30456;&#31561;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#32452;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#24212;&#23545;&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#26174;&#30528;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06672</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23610;&#24230;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A deep learning framework for multi-scale models based on physics-informed neural networks. (arXiv:2308.06672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#32423;&#25439;&#22833;&#39033;&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#24130;&#36816;&#31639;&#65292;&#20351;&#21508;&#20010;&#25439;&#22833;&#39033;&#22312;&#25968;&#37327;&#32423;&#19978;&#22823;&#33268;&#30456;&#31561;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#32452;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#24212;&#23545;&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#26174;&#30528;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29992;&#20110;&#25968;&#20540;&#27714;&#35299;PDE&#12290;&#38754;&#23545;&#19968;&#31867;&#21253;&#21547;&#19981;&#21516;&#25968;&#37327;&#32423;&#25439;&#22833;&#39033;&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#65292;&#23545;&#20110;&#26631;&#20934;PINN&#26041;&#27861;&#26469;&#35828;&#65292;&#33719;&#24471;&#21487;&#29992;&#30340;&#39044;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#22810;&#23610;&#24230;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#26631;&#20934;PINN&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#19981;&#21516;&#37327;&#32423;&#30340;&#25439;&#22833;&#39033;&#24212;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#24130;&#36816;&#31639;&#26469;&#20462;&#25913;&#26631;&#20934;PINN&#26041;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#20351;&#26500;&#25104;&#25439;&#22833;&#20989;&#25968;&#30340;&#21508;&#20010;&#25439;&#22833;&#39033;&#22312;&#25968;&#37327;&#32423;&#19978;&#22823;&#33268;&#30456;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32452;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#26174;&#30528;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdo
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.06091</link><description>&lt;p&gt;
&#23545;&#21327;&#21516;&#36807;&#28388;&#20002;&#22833;&#20989;&#25968;&#30340;&#26356;&#22909;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Toward a Better Understanding of Loss Functions for Collaborative Filtering. (arXiv:2308.06091v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06091
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;CF&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#20132;&#20114;&#32534;&#30721;&#22120;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#36127;&#37319;&#26679;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;CF&#27169;&#22411;&#26469;&#35774;&#35745;&#22797;&#26434;&#30340;&#20132;&#20114;&#32534;&#30721;&#22120;&#65292;&#20294;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#21046;&#23450;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#20998;&#26512;&#25581;&#31034;&#20102;&#20808;&#21069;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35299;&#37322;&#20026;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#20989;&#25968;&#65306;&#65288;i&#65289;&#23545;&#40784;&#21305;&#37197;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#22343;&#21248;&#24615;&#20998;&#25955;&#29992;&#25143;&#21644;&#29289;&#21697;&#20998;&#24067;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#31216;&#20026;Margin-aware Alignment and Weighted Uniformity&#65288;MAWU&#65289;&#12290;MAWU&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05309</link><description>&lt;p&gt;
&#22270;&#32858;&#31867;&#30340;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#31232;&#30095;&#24615;&#21644;&#22810;&#26679;&#24615;&#25152;&#22266;&#26377;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22270;&#32467;&#26500;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#36830;&#25509;&#21644;&#21024;&#38500;&#38169;&#35823;&#30340;&#36830;&#25509;&#26469;&#20248;&#21270;&#36755;&#20837;&#22270;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#30417;&#30563;&#30340;&#35774;&#32622;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#29305;&#23450;&#32858;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#22270;&#32858;&#31867;&#65288;HoLe&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;&#35266;&#23519;&#21040;&#65292;&#24494;&#22937;&#22320;&#22686;&#24378;&#22270;&#32467;&#26500;&#20013;&#30340;&#21516;&#31867;&#24615;&#31243;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;GNNs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#23558;&#20854;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#23545;&#27599;&#20010;&#31867;&#21035;&#30340;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#24635;&#32467;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.04603</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#65306;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Image Watermarking: A Brief Survey. (arXiv:2308.04603v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#23558;&#20854;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#23545;&#27599;&#20010;&#31867;&#21035;&#30340;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#24635;&#32467;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#27700;&#21360;&#26159;&#25351;&#22312;&#19968;&#24352;&#23553;&#38754;&#22270;&#20687;&#20013;&#31192;&#23494;&#23884;&#20837;&#21644;&#25552;&#21462;&#27700;&#21360;&#20197;&#20445;&#25252;&#22270;&#20687;&#30340;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#23618;&#20986;&#19981;&#31351;&#12290;&#20026;&#20102;&#30740;&#31350;&#26368;&#26032;&#30340;&#25216;&#26415;&#65292;&#26412;&#35843;&#26597;&#23558;&#21069;&#27839;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#20998;&#20026;&#23884;&#20837;&#22120;-&#25552;&#21462;&#22120;&#32852;&#21512;&#35757;&#32451;&#12289;&#28145;&#24230;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#21464;&#25442;&#21644;&#28151;&#21512;&#26041;&#26696;&#12290;&#36824;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The act of secretly embedding and extracting a watermark on a cover image to protect it is known as image watermarking. In recent years, deep learning-based image watermarking techniques have been emerging one after another. To study the state-of-the-art, this survey categorizes cutting-edge deep learning-based image watermarking techniques into Embedder-Extractor Joint Training, Deep Networks as a Feature Transformation, and Hybrid schemes. Research directions in each category are also analyzed and summarized. Additionally, potential future research directions are discussed to envision future studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#35299;&#37322;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23558;LSTM&#23398;&#20064;&#21040;&#30340;&#39034;&#24207;&#27169;&#24335;&#36716;&#31227;&#21040;MOB&#26641;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;MOB&#26641;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;MOB-HSMM&#23558;MOB&#26641;&#19982;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#28508;&#22312;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.15367</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#23454;&#29616;&#36879;&#26126;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toward Transparent Sequence Models with Model-Based Tree Markov Model. (arXiv:2307.15367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#35299;&#37322;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23558;LSTM&#23398;&#20064;&#21040;&#30340;&#39034;&#24207;&#27169;&#24335;&#36716;&#31227;&#21040;MOB&#26641;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;MOB&#26641;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;MOB-HSMM&#23558;MOB&#26641;&#19982;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#28508;&#22312;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#12289;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22266;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#26816;&#27979;&#39640;&#27515;&#20129;&#39118;&#38505;&#20107;&#20214;&#65292;&#24182;&#21457;&#29616;&#19982;&#27515;&#20129;&#39118;&#38505;&#30456;&#20851;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM&#23398;&#20064;&#39034;&#24207;&#27169;&#24335;&#65292;&#36827;&#32780;&#23558;&#20854;&#36716;&#31227;&#32473;MOB&#26641;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#65288;MOB&#26641;&#65289;&#30340;&#24615;&#33021;&#12290;&#23558;MOB&#26641;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#38598;&#25104;&#22312;MOB-HSMM&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#21487;&#29992;&#20449;&#24687;&#25581;&#31034;&#28508;&#22312;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we address the interpretability issue in complex, black-box Machine Learning models applied to sequence data. We introduce the Model-Based tree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model aimed at detecting high mortality risk events and discovering hidden patterns associated with the mortality risk in Intensive Care Units (ICU). This model leverages knowledge distilled from Deep Neural Networks (DNN) to enhance predictive performance while offering clear explanations. Our experimental results indicate the improved performance of Model-Based trees (MOB trees) via employing LSTM for learning sequential patterns, which are then transferred to MOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in the MOB-HSMM enables uncovering potential and explainable sequences using available information.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20559;&#22909;&#30340;&#25919;&#31574;&#23398;&#20064;&#26041;&#27861;&#22312;&#31163;&#32447;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#25919;&#31574;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#27425;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12975</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#25919;&#31574;&#22312;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#21487;&#35777;&#26126;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems. (arXiv:2307.12975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20559;&#22909;&#30340;&#25919;&#31574;&#23398;&#20064;&#26041;&#27861;&#22312;&#31163;&#32447;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#25919;&#31574;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#27425;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#22870;&#21169;&#24037;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24448;&#24448;&#19981;&#23384;&#22312;&#26126;&#26174;&#30340;&#22870;&#21169;&#20989;&#25968;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#21453;&#39304;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#25152;&#26377;&#25919;&#31574;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#22312;&#26368;&#36817;&#30340;&#23454;&#35777;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22914;InstructGPT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#65292;&#21487;&#20197;&#35777;&#26126;&#22312;&#31163;&#32447;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#22312;&#20154;&#31867;&#35780;&#20998;&#26679;&#26412;&#19978;&#36816;&#34892;&#25919;&#31574;&#23398;&#20064;&#26041;&#27861;&#30340;&#24314;&#27169;&#21644;&#27425;&#20248;&#24615;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#27425;&#20248;&#24615;&#20445;&#35777;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#20139;&#26377;&#26356;&#20302;&#30340;&#27425;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UGAT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#36716;&#31227;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12388</link><description>&lt;p&gt;
&#38754;&#21521;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22522;&#20110;&#23454;&#20363;&#30340;&#34892;&#21160;&#36716;&#25442;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control. (arXiv:2307.12388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UGAT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#36716;&#31227;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#26085;&#24120;&#29983;&#27963;&#30340;&#22797;&#26434;&#32780;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;RL&#30340;TSC&#26041;&#27861;&#20027;&#35201;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#23384;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20043;&#38388;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21040;&#23454;&#38469;&#29615;&#22659;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;UGAT&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#20197;&#20943;&#36731;&#36716;&#31227;&#21160;&#24577;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#23558;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#23398;&#20064;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#20132;&#36890;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#36716;&#31227;&#21518;&#30340;RL&#31574;&#30053;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.11949</link><description>&lt;p&gt;
HIQL: &#20197;&#28508;&#22312;&#29366;&#24577;&#20316;&#20026;&#21160;&#20316;&#30340;&#31163;&#32447;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26368;&#36817;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#65288;&#26080;&#22870;&#21169;&#65289;&#25968;&#25454;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#30452;&#25509;&#20174;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20934;&#30830;&#20272;&#35745;&#36828;&#26399;&#30446;&#26631;&#30340;&#20215;&#20540;&#20989;&#25968;&#24456;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#21363;&#36798;&#21040;&#36828;&#26399;&#30446;&#26631;&#38656;&#35201;&#39318;&#20808;&#36890;&#36807;&#36739;&#36817;&#23376;&#30446;&#26631;&#12290;&#36825;&#31181;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35780;&#20272;&#37051;&#36817;&#30446;&#26631;&#30340;&#21160;&#20316;&#36136;&#37327;&#36890;&#24120;&#27604;&#26356;&#36828;&#30446;&#26631;&#23481;&#26131;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;&#21033;&#29992;&#19968;&#20010;&#27809;&#26377;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65306;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#19981;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#30340;&#21333;&#36712;&#36857;&#26102;&#38388;&#21464;&#21270;&#30340;MDP&#20013;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35777;&#26126;&#20102;&#21033;&#29992;Q&#20540;&#24314;&#35758;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25913;&#36827;&#20102;&#20165;&#20351;&#29992;&#40657;&#30418;&#24314;&#35758;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.10524</link><description>&lt;p&gt;
&#36229;&#36234;&#40657;&#30418;&#24314;&#35758;: &#22522;&#20110;&#23398;&#20064;&#30340;&#22686;&#24378;&#31639;&#27861;&#29992;&#20110;&#20855;&#26377;Q&#20540;&#39044;&#27979;&#30340;MDPs
&lt;/p&gt;
&lt;p&gt;
Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions. (arXiv:2307.10524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#19981;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#30340;&#21333;&#36712;&#36857;&#26102;&#38388;&#21464;&#21270;&#30340;MDP&#20013;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35777;&#26126;&#20102;&#21033;&#29992;Q&#20540;&#24314;&#35758;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25913;&#36827;&#20102;&#20165;&#20351;&#29992;&#40657;&#30418;&#24314;&#35758;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21333;&#36712;&#36857;&#26102;&#38388;&#21464;&#21270;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20013;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#35813;&#36807;&#31243;&#20855;&#26377;&#19981;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#21516;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#19981;&#20877;&#23558;&#24314;&#35758;&#35270;&#20026;&#26469;&#33258;&#40657;&#30418;&#26469;&#28304;&#65292;&#32780;&#26159;&#32771;&#34385;&#21040;&#26377;&#20851;&#22914;&#20309;&#29983;&#25104;&#24314;&#35758;&#30340;&#20854;&#20182;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21253;&#25324;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;/&#21160;&#20316;&#31354;&#38388;&#30340;&#19968;&#33324;MDP&#27169;&#22411;&#19979;&#32473;&#20986;&#30340;Q&#20540;&#24314;&#35758;&#30340;&#19968;&#31181;&#26032;&#22411;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;Q&#20540;&#24314;&#35758;&#21487;&#20197;&#21160;&#24577;&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#21644;&#31283;&#20581;&#22522;&#32447;&#20013;&#36739;&#20248;&#30340;&#37027;&#20010;&#65292;&#20174;&#32780;&#20135;&#29983;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#20165;&#20351;&#29992;&#40657;&#30418;&#24314;&#35758;&#25152;&#33021;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#20809;&#35889;&#20559;&#24046;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#26410;&#35265;&#20998;&#24067;&#19979;&#24615;&#33021;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20809;&#35889;&#21551;&#21457;&#24335;&#30340;&#26816;&#27979;&#24037;&#20855;&#26469;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2307.08657</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#65306;&#27867;&#21270;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#20809;&#35889;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Neural Image Compression: Generalization, Robustness, and Spectral Biases. (arXiv:2307.08657v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#20809;&#35889;&#20559;&#24046;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#26410;&#35265;&#20998;&#24067;&#19979;&#24615;&#33021;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20809;&#35889;&#21551;&#21457;&#24335;&#30340;&#26816;&#27979;&#24037;&#20855;&#26469;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#65288;NIC&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#19968;&#20123;&#24320;&#22987;&#36229;&#36234;&#32463;&#20856;&#32534;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#20851;&#20110;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;NIC&#30340;&#20852;&#22859;&#65292;&#20294;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#37117;&#35201;&#27714;&#23427;&#22312;&#37096;&#32626;&#26102;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#65288;&#21644;&#40065;&#26834;&#24615;&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#21644;&#20449;&#24687;&#24037;&#20855;&#26469;&#35780;&#20272;&#21644;&#29702;&#35299;NIC&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#20851;&#38190;&#30340;&#24046;&#36317;&#65292;&#39318;&#20808;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#22871;&#20214;&#26469;&#35780;&#20272;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#20998;&#24067;&#19978;&#26410;&#35265;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#27969;&#34892;&#30340;CLIC&#21644;Kodak&#22522;&#20934;&#27979;&#35797;&#20013;&#24341;&#20837;15&#20010;&#25439;&#22351;&#31867;&#22411;&#65292;&#25552;&#20379;&#20102;CLIC-C&#21644;Kodak-C&#20004;&#20010;&#26032;&#22686;&#25968;&#25454;&#38598;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#35889;&#21551;&#21457;&#24335;&#30340;&#26816;&#27979;&#24037;&#20855;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#24341;&#20837;&#30340;&#38169;&#35823;&#20197;&#21450;&#23427;&#20204;&#22312;&#26410;&#35265;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#24615;&#33021;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in neural image compression (NIC) have produced models that are starting to outperform classic codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to the popular CLIC and Kodak benchmarks. Next, we propose spectrally-inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance c
&lt;/p&gt;</description></item><item><title>SBMLtoODEjax&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;JAX&#20013;&#39640;&#25928;&#27169;&#25311;&#21644;&#20248;&#21270;&#29983;&#29289;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#29983;&#29289;&#31995;&#32479;&#30340;&#22810;&#26679;&#34892;&#20026;&#21644;&#20248;&#21270;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2307.08452</link><description>&lt;p&gt;
SBMLtoODEjax: &#22312;JAX&#20013;&#39640;&#25928;&#27169;&#25311;&#21644;&#20248;&#21270;&#29983;&#29289;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SBMLtoODEjax: Efficient Simulation and Optimization of Biological Network Models in JAX. (arXiv:2307.08452v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08452
&lt;/p&gt;
&lt;p&gt;
SBMLtoODEjax&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;JAX&#20013;&#39640;&#25928;&#27169;&#25311;&#21644;&#20248;&#21270;&#29983;&#29289;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#29983;&#29289;&#31995;&#32479;&#30340;&#22810;&#26679;&#34892;&#20026;&#21644;&#20248;&#21270;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#24037;&#31243;&#21644;&#29983;&#29289;&#21307;&#23398;&#30340;&#36827;&#23637;&#35201;&#27714;&#28145;&#20837;&#20102;&#35299;&#29983;&#29289;&#31995;&#32479;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#20174;&#34507;&#30333;&#36136;&#36884;&#24452;&#21040;&#22797;&#26434;&#30340;&#32454;&#32990;&#36807;&#31243;&#12290;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21644;&#34507;&#30333;&#36136;&#36884;&#24452;&#31561;&#29983;&#29289;&#32593;&#32476;&#26159;&#32986;&#32974;&#21457;&#32946;&#21644;&#29983;&#29702;&#36807;&#31243;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#29702;&#35299;&#23427;&#20204;&#30340;&#22810;&#26679;&#34892;&#20026;&#23545;&#20110;&#35299;&#20915;&#21253;&#25324;&#30284;&#30151;&#22312;&#20869;&#30340;&#30142;&#30149;&#20197;&#21450;&#24037;&#31243;&#26032;&#30340;&#29983;&#29289;&#26500;&#24314;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23613;&#31649;&#22312;&#31995;&#32479;&#29983;&#29289;&#23398;&#26631;&#35760;&#35821;&#35328;&#65288;SBML&#65289;&#20013;&#26377;&#22823;&#37327;&#30340;&#25968;&#23398;&#27169;&#22411;&#21487;&#29992;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#22312;&#25506;&#32034;&#23436;&#25972;&#30340;&#34892;&#20026;&#35889;&#31995;&#21644;&#20248;&#21270;&#24178;&#39044;&#20197;&#26377;&#25928;&#22609;&#36896;&#36825;&#20123;&#34892;&#20026;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35774;&#35745;&#29992;&#20110;&#29983;&#29289;&#32593;&#32476;&#27169;&#22411;&#27169;&#25311;&#30340;&#24037;&#20855;&#24182;&#27809;&#26377;&#38024;&#23545;&#20419;&#36827;&#32593;&#32476;&#21160;&#21147;&#23398;&#19978;&#30340;&#24178;&#39044;&#25110;&#20419;&#36827;&#33258;&#21160;&#21457;&#29616;&#32780;&#23450;&#21046;&#12290;&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;SBMLtoODEjax&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#36827;&#34892;&#29983;&#29289;&#32593;&#32476;&#27169;&#22411;&#30340;&#27169;&#25311;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in bioengineering and biomedicine demand a deep understanding of the dynamic behavior of biological systems, ranging from protein pathways to complex cellular processes. Biological networks like gene regulatory networks and protein pathways are key drivers of embryogenesis and physiological processes. Comprehending their diverse behaviors is essential for tackling diseases, including cancer, as well as for engineering novel biological constructs. Despite the availability of extensive mathematical models represented in Systems Biology Markup Language (SBML), researchers face significant challenges in exploring the full spectrum of behaviors and optimizing interventions to efficiently shape those behaviors. Existing tools designed for simulation of biological network models are not tailored to facilitate interventions on network dynamics nor to facilitate automated discovery. Leveraging recent developments in machine learning (ML), this paper introduces SBMLtoODEjax, a lightweig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#29983;&#25104;&#24314;&#27169;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#22312;&#19982;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#8220;&#36317;&#31163;&#8221;&#65292;&#24182;&#36890;&#36807;&#35813;&#36317;&#31163;&#30340;&#26368;&#23567;&#21270;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08283</link><description>&lt;p&gt;
&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#37325;&#26032;&#24605;&#32771;&#29983;&#25104;&#24314;&#27169;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Complexity Matters: Rethinking the Latent Space for Generative Modeling. (arXiv:2307.08283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#29983;&#25104;&#24314;&#27169;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#22312;&#19982;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#8220;&#36317;&#31163;&#8221;&#65292;&#24182;&#36890;&#36807;&#35813;&#36317;&#31163;&#30340;&#26368;&#23567;&#21270;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#65292;&#35768;&#22810;&#25104;&#21151;&#30340;&#26041;&#27861;&#21033;&#29992;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#65292;&#20363;&#22914;&#65292;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#32534;&#30721;&#22120;&#24341;&#23548;&#30340;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#37197;&#23545;&#30340;&#35299;&#30721;&#22120;&#36827;&#34892;&#29983;&#25104;&#12290;&#23613;&#31649;&#28508;&#22312;&#31354;&#38388;&#30340;&#36873;&#25321;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30830;&#23450;&#26368;&#20248;&#36873;&#25321;&#21644;&#35782;&#21035;&#36807;&#31243;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#28508;&#22312;&#31354;&#38388;&#65292;&#26469;&#25581;&#31034;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#20174;&#32463;&#20856;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24320;&#22987;&#12290;&#21463;&#21040;GAN&#35757;&#32451;&#30446;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#22312;&#19982;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#8220;&#36317;&#31163;&#8221;&#65292;&#20854;&#26368;&#23567;&#21270;&#19982;&#29983;&#25104;&#22120;&#30340;&#22797;&#26434;&#24615;&#26368;&#23567;&#21270;&#30456;&#19968;&#33268;&#12290;&#36825;&#20010;&#36317;&#31163;&#30340;&#26368;&#23567;&#21270;&#32773;&#34987;&#25551;&#36848;&#20026;&#33021;&#22815;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#22120;&#23481;&#37327;&#30340;&#26368;&#20339;&#25968;&#25454;&#30456;&#20851;&#30340;&#28508;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#32534;&#30721;&#22120;&#32593;&#32476;&#23545;&#36825;&#26679;&#30340;&#28508;&#22312;&#20998;&#24067;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;CatBoost&#12289;XGBoost&#21644;LightGBM&#19977;&#31181;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211;&#22312;&#22788;&#29702;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;</title><link>http://arxiv.org/abs/2307.07771</link><description>&lt;p&gt;
CatBoost&#23545;&#27604;XGBoost&#21644;LightGBM&#65306;&#24320;&#21457;&#22686;&#24378;&#30340;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data. (arXiv:2307.07771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;CatBoost&#12289;XGBoost&#21644;LightGBM&#19977;&#31181;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211;&#22312;&#22788;&#29702;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36130;&#20135;&#21644;&#24847;&#22806;&#20107;&#25925;&#20445;&#38505;&#34892;&#19994;&#20013;&#65292;&#30001;&#20110;&#27491;&#21521;&#29702;&#36180;&#25968;&#25454;&#20855;&#26377;&#39640;&#24230;&#21491;&#20559;&#20998;&#24067;&#21644;&#36807;&#37327;&#30340;&#38646;&#20540;&#65292;&#26500;&#24314;&#29702;&#36180;&#39044;&#27979;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#20256;&#32479;&#27169;&#22411;&#65292;&#22914;&#27850;&#26494;&#25110;&#36127;&#20108;&#39033;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;(GLM)&#65292;&#32463;&#24120;&#22312;&#22788;&#29702;&#36807;&#37327;&#38646;&#20540;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#31934;&#31639;&#31185;&#23398;&#30340;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#37319;&#29992;&#20102;&#8220;&#38646;&#33192;&#32960;&#8221;&#27169;&#22411;&#65292;&#23558;&#20256;&#32479;&#35745;&#25968;&#27169;&#22411;&#21644;&#20108;&#20803;&#27169;&#22411;&#21512;&#24182;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#25552;&#21319;&#31639;&#27861;&#26469;&#22788;&#29702;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#65292;&#21253;&#25324;&#38646;&#33192;&#32960;&#30340;&#36965;&#27979;&#25968;&#25454;&#65292;&#20197;&#26500;&#24314;&#29702;&#36180;&#39057;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#19977;&#20010;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211; - XGBoost&#12289;LightGBM&#21644;CatBoost&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the property and casualty insurance industry, some challenges are presented in constructing claim predictive models due to a highly right-skewed distribution of positive claims with excess zeros. Traditional models, such as Poisson or negative binomial Generalized Linear Models(GLMs), frequently struggle with inflated zeros. In response to this, researchers in actuarial science have employed ``zero-inflated" models that merge a traditional count model and a binary model to address these datasets more effectively. This paper uses boosting algorithms to process insurance claim data, including zero-inflated telematics data, in order to construct claim frequency models. We evaluated and compared three popular gradient boosting libraries - XGBoost, LightGBM, and CatBoost - with the aim of identifying the most suitable library for training insurance claim data and fitting actuarial frequency models. Through a rigorous analysis of two distinct datasets, we demonstrated that CatBoost is sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#23545;&#21464;&#20998;&#27969;&#20013;&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;ELBO&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#21464;&#20998;&#27969;&#20135;&#29983;&#30340;&#32467;&#26524;&#22312;&#24212;&#29992;&#20013;&#24120;&#24120;&#36275;&#22815;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2307.06957</link><description>&lt;p&gt;
&#25317;&#25265;&#28151;&#20081;&#65306;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#22312;&#21464;&#20998;&#27969;&#20013;&#30340;&#20998;&#26512;&#21644;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Embracing the chaos: analysis and diagnosis of numerical instability in variational flows. (arXiv:2307.06957v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#23545;&#21464;&#20998;&#27969;&#20013;&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;ELBO&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#21464;&#20998;&#27969;&#20135;&#29983;&#30340;&#32467;&#26524;&#22312;&#24212;&#29992;&#20013;&#24120;&#24120;&#36275;&#22815;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#23545;&#21464;&#20998;&#27969;&#20013;&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#24120;&#35265;&#27969;&#21487;&#33021;&#20986;&#29616;&#20005;&#37325;&#30340;&#38169;&#35823;&#32047;&#31215;&#65306;&#25968;&#20540;&#27969;&#26144;&#23556;&#19982;&#31934;&#30830;&#26144;&#23556;&#30340;&#20559;&#24046;&#26174;&#33879;&#65292;&#24433;&#21709;&#37319;&#26679;&#65307;&#25968;&#20540;&#36870;&#27969;&#26144;&#23556;&#26080;&#27861;&#20934;&#30830;&#24674;&#22797;&#21021;&#22987;&#36755;&#20837;&#65292;&#24433;&#21709;&#23494;&#24230;&#21644;ELBO&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;&#23613;&#31649;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#27969;&#20135;&#29983;&#30340;&#32467;&#26524;&#24120;&#24120;&#36275;&#22815;&#20934;&#30830;&#24212;&#23545;&#24212;&#29992;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#27969;&#35270;&#20026;&#21160;&#21147;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#38452;&#24433;&#29702;&#35770;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#23545;&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;ELBO&#20272;&#35745;&#30340;&#38169;&#35823;&#26469;&#38416;&#26126;&#36825;&#31181;&#34892;&#20026;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#32463;&#39564;&#24615;&#22320;&#27979;&#35797;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#39564;&#35777;&#25968;&#20540;&#32467;&#26524;&#30340;&#35786;&#26029;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the impact of numerical instability on the reliability of sampling, density evaluation, and evidence lower bound (ELBO) estimation in variational flows. We first empirically demonstrate that common flows can exhibit a catastrophic accumulation of error: the numerical flow map deviates significantly from the exact map -- which affects sampling -- and the numerical inverse flow map does not accurately recover the initial input -which affects density and ELBO computations. Surprisingly though, we find that results produced by flows are often accurate enough for applications despite the presence of serious numerical instability. In this work, we treat variational flows as dynamical systems, and leverage shadowing theory to elucidate this behavior via theoretical guarantees on the error of sampling, density evaluation, and ELBO estimation. Finally, we develop and empirically test a diagnostic procedure that can be used to validate results produced by numerica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.06290</link><description>&lt;p&gt;
&#25351;&#20196;&#25366;&#25496;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32463;&#21382;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#24212;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#26102;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#12290;&#20026;&#20102;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#36136;&#37327;&#30340;&#32447;&#24615;&#35268;&#21017;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#26469;&#36827;&#34892;InstructMining&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#19982;&#36825;&#20123;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05194</link><description>&lt;p&gt;
&#36890;&#36807;$\beta$-&#20998;&#35299;&#19968;&#21518;&#39564;&#37319;&#26679;&#23454;&#29616;&#24046;&#20998;&#35745;&#31639;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#31169;&#23494;&#24615;&#30830;&#20445;&#20102;&#21253;&#21547;&#25935;&#24863;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#32467;&#26524;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#20219;&#20309;&#20010;&#20307;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21457;&#24067;&#12290;&#23454;&#29616;&#36825;&#31181;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#22312;&#21442;&#25968;&#20272;&#35745;&#25110;&#20272;&#35745;&#36807;&#31243;&#20013;&#30452;&#25509;&#27880;&#20837;&#22122;&#38899;&#12290;&#32780;&#37319;&#26679;&#26469;&#33258;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#24050;&#34987;&#35777;&#26126;&#26159;&#25351;&#25968;&#26426;&#21046;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#19988;&#39640;&#25928;&#30340;&#31169;&#23494;&#20272;&#35745;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#30340;&#24212;&#29992;&#21463;&#21040;&#36739;&#24378;&#30340;&#36793;&#30028;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#22522;&#26412;&#27169;&#22411;&#65288;&#22914;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#22120;&#65289;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#20174;&#24191;&#20041;&#21518;&#39564;&#20013;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27169;&#22411;&#19982;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#12290;&#36825;&#25552;&#20379;&#20102;&#31169;&#23494;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04333</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20998;&#25968;&#30340;&#20248;&#21270;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26377;&#21487;&#33021;&#36890;&#36807;&#24341;&#20837;&#24494;&#23567;&#25200;&#21160;&#26469;&#35823;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#12290;&#24320;&#21457;&#33021;&#22815;&#20943;&#36731;&#36825;&#20123;&#25915;&#20987;&#24433;&#21709;&#30340;&#31639;&#27861;&#23545;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#23545;&#25239;&#38450;&#24481;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#38450;&#24481;&#20381;&#36182;&#20110;&#39034;&#24207;&#27169;&#25311;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#24182;&#19988;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#22312;&#30001;&#22522;&#20110;&#20998;&#25968;&#20808;&#39564;&#25351;&#23548;&#30340;&#26041;&#21521;&#19978;&#23545;&#21407;&#22987;&#24178;&#20928;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#26469;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;CIFAR10&#12289;CIFAR100&#21644;ImageNet&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustnes
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#35270;&#35273;&#29305;&#24449;&#25551;&#36848;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#35270;&#35273;&#29305;&#24449;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.04317</link><description>&lt;p&gt;
&#25991;&#26412;&#25551;&#36848;&#26159;&#35270;&#35273;&#23398;&#20064;&#20013;&#21387;&#32553;&#21644;&#19981;&#21464;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text Descriptions are Compressive and Invariant Representations for Visual Learning. (arXiv:2307.04317v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04317
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#35270;&#35273;&#29305;&#24449;&#25551;&#36848;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#35270;&#35273;&#29305;&#24449;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#20687;&#20998;&#31867;&#26159;&#22522;&#20110;&#36890;&#36807;&#22823;&#22411;&#21028;&#21035;&#32593;&#32476;&#30452;&#25509;&#39044;&#27979;&#31867;&#21035;&#65292;&#20294;&#36825;&#20123;&#32593;&#32476;&#24182;&#19981;&#30452;&#25509;&#21253;&#21547;&#26500;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#30452;&#35266;&#35270;&#35273;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22914;CLIP&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#25351;&#23450;&#22270;&#20687;&#31867;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#26041;&#24335;&#65292;&#20294;&#36890;&#24120;&#38598;&#20013;&#22312;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#21333;&#19968;&#25551;&#36848;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#29702;&#35299;&#27599;&#20010;&#31867;&#21035;&#30340;&#22810;&#20010;&#35270;&#35273;&#29305;&#24449;&#30456;&#19968;&#33268;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22312;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#19979;&#20063;&#33021;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#8220;SLR-AVD&#65288;&#20351;&#29992;&#22686;&#24378;&#35270;&#35273;&#25551;&#36848;&#30340;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#65289;&#8221;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30340;&#22810;&#20010;&#35270;&#35273;&#25551;&#36848;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;VLM&#23558;&#36825;&#20123;&#25551;&#36848;&#32763;&#35793;&#25104;&#27599;&#20010;&#22270;&#20687;&#30340;&#19968;&#32452;&#35270;&#35273;&#29305;&#24449;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern image classification is based upon directly predicting classes via large discriminative networks, which do not directly contain information about the intuitive visual features that may constitute a classification decision. Recently, work in vision-language models (VLM) such as CLIP has provided ways to specify natural language descriptions of image classes, but typically focuses on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, in line with humans' understanding of multiple visual features per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we introduce a novel method, \textit{SLR-AVD (Sparse Logistic Regression using Augmented Visual Descriptors)}. This method first automatically generates multiple visual descriptions of each class via a large language model (LLM), then uses a VLM to translate these descriptions to a set of visual feature embeddings of each image, an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.03848</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#22238;&#24402;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65306;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#22312;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#36827;&#34892;&#21051;&#30011;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;fat shattering&#32500;&#24230;&#23545;&#20110;PAC&#23398;&#20064;&#30340;&#20805;&#20998;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;scaled Natarajan&#32500;&#24230;&#23545;&#20110;&#24517;&#35201;&#24615;&#30340;&#23384;&#22312;&#65292;&#20294;&#33258;&#20174;Simon 1997&#65288;SICOMP '97&#65289;&#30340;&#24037;&#20316;&#20197;&#26469;&#65292;&#23545;&#20110;&#26356;&#23436;&#25972;&#30340;&#21051;&#30011;&#30340;&#36827;&#23637;&#29978;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#23454;&#20363;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#26469;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#21738;&#20123;&#31867;&#30340;&#23454;&#25968;&#39044;&#27979;&#22120;&#21487;&#20197;&#34987;&#23398;&#20064;&#30340;&#26032;&#39062;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19982;&#22270;&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;ERM&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19982;DS&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#24314;&#31435;&#20102;&#23398;&#20064;&#21487;&#34892;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#29468;&#27979;&#23427;&#20063;&#21487;&#33021;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25506;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#22312;&#36712;&#36857;&#21387;&#32553;&#21644;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03406</link><description>&lt;p&gt;
&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#38544;&#24335;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25506;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#22312;&#36712;&#36857;&#21387;&#32553;&#21644;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36712;&#36857;&#25968;&#25454;&#19978;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#22909;&#22788;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#26159;&#21542;&#20855;&#22791;&#23558;&#36712;&#36857;&#21387;&#32553;&#20026;&#26377;&#29992;&#34920;&#31034;&#24182;&#23545;&#31574;&#30053;&#23398;&#20064;&#26377;&#25152;&#36129;&#29486;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#39318;&#20808;&#20351;&#29992;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#24635;&#32467;&#36712;&#36857;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#20197;&#21450;&#19968;&#20010;&#26399;&#26395;&#30340;&#30446;&#26631;&#12290;&#36825;&#20010;&#35774;&#35745;&#20351;&#24471;&#35768;&#22810;&#29616;&#26377;&#30340;&#30417;&#30563;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26469;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23548;&#33268;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;AntMaze&#65292;FrankaKitchen&#21644;Locomotion&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and obser
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#26032;&#30340;&#36817;&#20284;&#25110;&#19978;&#30028;&#26469;&#34913;&#37327;&#22522;&#20110;&#22238;&#24402;&#30340;&#27979;&#35797;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;CI&#26816;&#39564;&#26041;&#27861;RBPT&#65292;&#23545;&#27169;&#22411;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02520</link><description>&lt;p&gt;
&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Conditional independence testing under model misspecification. (arXiv:2307.02520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#26032;&#30340;&#36817;&#20284;&#25110;&#19978;&#30028;&#26469;&#34913;&#37327;&#22522;&#20110;&#22238;&#24402;&#30340;&#27979;&#35797;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;CI&#26816;&#39564;&#26041;&#27861;RBPT&#65292;&#23545;&#27169;&#22411;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#26816;&#39564;&#26159;&#29616;&#20195;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#29616;&#20195;&#30340;CI&#26816;&#39564;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#22238;&#24402;&#20989;&#25968;&#25110;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20934;&#30830;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#25110;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#26102;&#20445;&#35777;&#20102;&#25511;&#21046;&#31532;&#19968;&#31867;&#38169;&#35823;&#65292;&#20294;&#23427;&#20204;&#22312;&#27169;&#22411;&#38169;&#35823;&#23548;&#33268;&#22833;&#36133;&#26102;&#30340;&#34892;&#20026;&#23578;&#19981;&#28165;&#26970;&#12290;&#20174;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#35762;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#36890;&#29992;&#36924;&#36817;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#27169;&#22411;&#38169;&#35823;&#20063;&#21487;&#33021;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;CI&#26816;&#39564;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#36817;&#20284;&#25110;&#19978;&#30028;&#26469;&#34913;&#37327;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#19977;&#20010;&#22522;&#20110;&#22238;&#24402;&#30340;&#27979;&#35797;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rao-Blackwellized Predictor Test&#65288;RBPT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;CI&#26816;&#39564;&#65292;&#23545;&#27169;&#22411;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional independence (CI) testing is fundamental and challenging in modern statistics and machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to learn regression functions or Bayes predictors as an intermediate step. Although the methods are guaranteed to control Type-I error when the supervised learning methods accurately estimate the regression functions or Bayes predictors, their behavior is less understood when they fail due to model misspecification. In a broader sense, model misspecification can arise even when universal approximators (e.g., deep neural nets) are employed. Then, we study the performance of regression-based CI tests under model misspecification. Namely, we propose new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. Moreover, we introduce the Rao-Blackwellized Predictor Test (RBPT), a novel regression-based CI test robust against model mis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20999;&#29255;Wasserstein&#24191;&#20041;&#27979;&#22320;&#32447;&#36827;&#34892;&#36817;&#20284;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#32500;&#26368;&#20248;&#25237;&#24433;&#30340;&#20195;&#29702;&#36317;&#31163;min-SWGG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#20256;&#36755;&#35745;&#21010;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#36866;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01770</link><description>&lt;p&gt;
&#24555;&#36895;&#36890;&#36807;&#20999;&#29255;Wasserstein&#24191;&#20041;&#27979;&#22320;&#32447;&#23454;&#29616;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics. (arXiv:2307.01770v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20999;&#29255;Wasserstein&#24191;&#20041;&#27979;&#22320;&#32447;&#36827;&#34892;&#36817;&#20284;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#32500;&#26368;&#20248;&#25237;&#24433;&#30340;&#20195;&#29702;&#36317;&#31163;min-SWGG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#20256;&#36755;&#35745;&#21010;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#36866;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wassserstein&#36317;&#31163;&#21644;&#30456;&#20851;&#30340;&#26368;&#20248;&#36755;&#36816;&#35745;&#21010;&#22312;&#35768;&#22810;&#28041;&#21450;&#27010;&#29575;&#24230;&#37327;&#30340;&#24212;&#29992;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#26041;Wasserstein&#36317;&#31163;&#30340;&#20195;&#29702;&#65292;&#31216;&#20026;min-SWGG&#65292;&#23427;&#22522;&#20110;&#20004;&#20010;&#36755;&#20837;&#20998;&#24067;&#30340;&#19968;&#32500;&#26368;&#20248;&#25237;&#24433;&#24341;&#23548;&#30340;&#36816;&#36755;&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;min-SWGG&#21644;Wasserstein&#24191;&#20041;&#27979;&#22320;&#32447;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#65292;&#20854;&#20013;&#26530;&#32445;&#27979;&#24230;&#22312;&#19968;&#26465;&#30452;&#32447;&#19978;&#24471;&#21040;&#25903;&#25345;&#12290;&#25105;&#20204;&#29305;&#21035;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#38381;&#21512;&#24418;&#24335;&#30340;&#31934;&#30830;Wasserstein&#36317;&#31163;&#65292;&#22312;&#20854;&#20013;&#19968;&#20010;&#20998;&#24067;&#25903;&#25345;&#22312;&#19968;&#26465;&#30452;&#32447;&#19978;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#24555;&#36895;&#35745;&#31639;&#26041;&#26696;&#12290;&#25105;&#20204;&#34920;&#26126;min-SWGG&#26159;WD&#30340;&#19978;&#30028;&#65292;&#24182;&#19988;&#23427;&#20855;&#26377;&#19982;Sliced-Wasserstein&#31867;&#20284;&#30340;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#36755;&#36816;&#35745;&#21010;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20123;&#29702;&#35770;&#24615;&#36136;&#65292;&#22914;&#36317;&#31163;&#24615;&#12289;&#24369;&#25910;&#25947;&#12289;&#35745;&#31639;&#21644;&#25299;&#25169;&#24615;&#36136;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined min-SWGG, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between min-SWGG and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that min-SWGG is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and top
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#24403;&#24178;&#39044;&#26041;&#26696;&#26159;&#33258;&#36866;&#24212;&#20998;&#37197;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.01357</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Principal Component Regression with Applications to Panel Data. (arXiv:2307.01357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#24403;&#24178;&#39044;&#26041;&#26696;&#26159;&#33258;&#36866;&#24212;&#20998;&#37197;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#22238;&#24402;(PCR)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22266;&#23450;&#35774;&#35745;&#35823;&#24046;&#21464;&#37327;&#22238;&#24402;&#25216;&#26415;&#65292;&#23427;&#26159;&#32447;&#24615;&#22238;&#24402;&#30340;&#25512;&#24191;&#65292;&#35266;&#27979;&#30340;&#21327;&#21464;&#37327;&#21463;&#21040;&#38543;&#26426;&#22122;&#22768;&#30340;&#27745;&#26579;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#25910;&#38598;&#26102;&#25552;&#20379;&#20102;&#22312;&#32447;&#65288;&#27491;&#21017;&#21270;&#65289;PCR&#30340;&#31532;&#19968;&#27425;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#30001;&#20110;&#20998;&#26512;&#22266;&#23450;&#35774;&#35745;&#20013;PCR&#30340;&#35777;&#26126;&#25216;&#26415;&#26080;&#27861;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#22312;&#32447;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;&#23558;&#29616;&#20195;&#38789;&#27987;&#24230;&#30340;&#24037;&#20855;&#36866;&#24212;&#21040;&#35823;&#24046;&#21464;&#37327;&#35774;&#32622;&#20013;&#12290;&#20316;&#20026;&#25105;&#20204;&#30028;&#38480;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#38754;&#26495;&#25968;&#25454;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#65292;&#24403;&#24178;&#39044;&#34987;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26102;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21512;&#25104;&#25511;&#21046;&#21644;&#21512;&#25104;&#24178;&#39044;&#26694;&#26550;&#30340;&#27867;&#21270;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#36890;&#36807;&#33258;&#36866;&#24212;&#24178;&#39044;&#20998;&#37197;&#31574;&#30053;&#25910;&#38598;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCP&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#26465;&#20214;&#31574;&#30053;&#20998;&#31163;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;</title><link>http://arxiv.org/abs/2307.01217</link><description>&lt;p&gt;
FedCP:&#36890;&#36807;&#26465;&#20214;&#31574;&#30053;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#36827;&#34892;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCP&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#26465;&#20214;&#31574;&#30053;&#20998;&#31163;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#22312;&#38544;&#31169;&#20445;&#25252;&#12289;&#21327;&#20316;&#23398;&#20064;&#20197;&#21450;&#35299;&#20915;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20363;&#22914;&#21307;&#38498;&#12289;&#31227;&#21160;&#26234;&#33021;&#25163;&#26426;&#31561;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#20391;&#37325;&#20110;&#21033;&#29992;&#23458;&#25143;&#31471;&#32423;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#20294;&#24573;&#30053;&#20102;&#25968;&#25454;&#26159;&#36825;&#20004;&#31181;&#20449;&#24687;&#30340;&#28304;&#22836;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#26465;&#20214;&#31574;&#30053;&#65288;FedCP&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#26679;&#26412;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20998;&#31163;&#20854;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#28982;&#21518;&#20998;&#21035;&#36890;&#36807;&#20840;&#23616;&#22836;&#21644;&#20010;&#24615;&#21270;&#22836;&#36827;&#34892;&#22788;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#30456;&#27604;&#65292;FedCP&#26356;&#21152;&#32454;&#31890;&#24230;&#22320;&#32771;&#34385;&#20010;&#24615;&#21270;&#30340;&#26679;&#26412;&#29305;&#23450;&#26041;&#24335;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;FedCP&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21160;&#24577;&#65292;&#24182;&#19988;&#25429;&#25417;&#20102;&#23398;&#20064;&#31639;&#27861;&#19982;&#20854;&#24212;&#29992;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#36890;&#36807;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25112;&#30053;&#24615;&#21453;&#24212;&#12289;&#38750;&#23616;&#37096;&#20869;&#29983;&#20154;&#21475;&#20114;&#21160;&#21644;&#20854;&#20182;&#22806;&#29983;&#20998;&#24067;&#20559;&#31227;&#26469;&#28304;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#38388;&#19978;&#32454;&#24494;&#21464;&#21270;&#30340;&#25429;&#25417;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#21512;&#20316;&#35774;&#32622;&#21644;&#31454;&#20105;&#35774;&#32622;&#20013;&#65292;&#24403;&#31639;&#27861;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#28176;&#36817;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2307.01166</link><description>&lt;p&gt;
&#38024;&#23545;&#25112;&#30053;&#38750;&#23616;&#37096;&#20998;&#24067;&#20559;&#31227;&#30340;&#32806;&#21512;&#26799;&#24230;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Coupled Gradient Flows for Strategic Non-Local Distribution Shift. (arXiv:2307.01166v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21160;&#24577;&#65292;&#24182;&#19988;&#25429;&#25417;&#20102;&#23398;&#20064;&#31639;&#27861;&#19982;&#20854;&#24212;&#29992;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#36890;&#36807;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25112;&#30053;&#24615;&#21453;&#24212;&#12289;&#38750;&#23616;&#37096;&#20869;&#29983;&#20154;&#21475;&#20114;&#21160;&#21644;&#20854;&#20182;&#22806;&#29983;&#20998;&#24067;&#20559;&#31227;&#26469;&#28304;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#38388;&#19978;&#32454;&#24494;&#21464;&#21270;&#30340;&#25429;&#25417;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#21512;&#20316;&#35774;&#32622;&#21644;&#31454;&#20105;&#35774;&#32622;&#20013;&#65292;&#24403;&#31639;&#27861;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#28176;&#36817;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#20998;&#24067;&#20559;&#31227;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#35813;&#26694;&#26550;&#25429;&#25417;&#20102;&#23398;&#20064;&#31639;&#27861;&#19982;&#20854;&#24212;&#29992;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20197;&#23545;&#25239;&#25110;&#36807;&#24230;&#31616;&#21270;&#30340;&#20998;&#24067;&#20559;&#31227;&#32467;&#26500;&#26469;&#24314;&#27169;&#21453;&#39304;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32806;&#21512;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#30001;&#20110;&#23545;&#31639;&#27861;&#20915;&#31574;&#30340;&#25112;&#30053;&#24615;&#21453;&#24212;&#12289;&#38750;&#23616;&#37096;&#20869;&#29983;&#20154;&#21475;&#20114;&#21160;&#21644;&#20854;&#20182;&#22806;&#29983;&#20998;&#24067;&#20559;&#31227;&#26469;&#28304;&#32780;&#20135;&#29983;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#25429;&#25417;&#20998;&#24067;&#38543;&#26102;&#38388;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20004;&#31181;&#24120;&#35265;&#35774;&#32622;&#65306;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#21512;&#20316;&#35774;&#32622;&#20197;&#21450;&#23398;&#20064;&#32773;&#38754;&#23545;&#25112;&#30053;&#29992;&#25143;&#30340;&#31454;&#20105;&#35774;&#32622;&#12290;&#23545;&#20110;&#36825;&#20004;&#31181;&#35774;&#32622;&#65292;&#24403;&#31639;&#27861;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#29366;&#24577;&#30340;&#28176;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00142</link><description>&lt;p&gt;
BuildingsBench&#65306;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. (arXiv:2307.00142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;(STLF)&#20013;&#32570;&#20047;&#24320;&#25918;&#12289;&#22823;&#35268;&#27169;&#12289;&#39640;&#24314;&#31569;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#21253;&#25324;1)&#21253;&#21547;900K&#20010;&#27169;&#25311;&#24314;&#31569;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;Buildings-900K&#65292;&#20197;&#27169;&#25311;&#32654;&#22269;&#30340;&#24314;&#31569;&#24211;&#23384;&#65292;&#20197;&#21450;2)&#25317;&#26377;&#26469;&#33258;7&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#36229;&#36807;1900&#20010;&#30495;&#23454;&#20303;&#23429;&#21644;&#21830;&#19994;&#24314;&#31569;&#29289;&#30340;&#35780;&#20272;&#24179;&#21488;&#12290;BuildingsBench&#20026;&#20004;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#20934;&#65306;&#38646;-shot STLF&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#24314;&#31569;&#19978;&#36827;&#34892;&#35780;&#20272;&#32780;&#26080;&#38656;&#24494;&#35843;&#65307;&#20197;&#21450;&#36801;&#31227;&#23398;&#20064;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30446;&#26631;&#24314;&#31569;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#27425;&#22522;&#20934;&#20998;&#26512;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24847;&#22806;&#22320;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-finetune paradigm for STLF. To help address this, we present BuildingsBench, which consists of 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock, and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#23398;&#20064;&#36523;&#20221;&#25928;&#24212;&#30340;&#32972;&#26223;&#19979;&#65292;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#65292;&#20197;&#21450;&#22312;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#26696;&#20363;&#20013;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.00134</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36523;&#20221;&#25928;&#24212;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Generalization Limits of Graph Neural Networks in Identity Effects Learning. (arXiv:2307.00134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#23398;&#20064;&#36523;&#20221;&#25928;&#24212;&#30340;&#32972;&#26223;&#19979;&#65292;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#65292;&#20197;&#21450;&#22312;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#26696;&#20363;&#20013;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#22270;&#39046;&#22495;&#30340;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;&#23427;&#20204;&#36890;&#24120;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#19982;Weisfeiler-Lehman(WL)&#22270;&#21516;&#26500;&#27979;&#35797;&#32039;&#23494;&#30456;&#36830;&#30340;&#30452;&#35266;&#34920;&#36848;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#20174;&#34920;&#36798;&#33021;&#21147;&#19978;&#35762;&#65292;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#19982;WL&#27979;&#35797;&#31561;&#20215;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#25152;&#35859;&#30340;&#36523;&#20221;&#25928;&#24212;&#65288;&#21363;&#30830;&#23450;&#19968;&#20010;&#23545;&#35937;&#26159;&#21542;&#30001;&#20004;&#20010;&#30456;&#21516;&#30340;&#32452;&#20214;&#32452;&#25104;&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#24314;&#31435;&#20102;GNN&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#20986;&#20110;&#29702;&#35299;GNN&#22312;&#25191;&#34892;&#31616;&#21333;&#35748;&#30693;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#30340;&#38656;&#27714;&#65292;&#21487;&#33021;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#65288;i&#65289;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;GNN&#22312;&#21033;&#29992;&#27491;&#20132;&#26102;&#26080;&#27861;&#23545;&#26410;&#35265;&#23383;&#27597;&#36827;&#34892;&#27867;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful tool for data-driven learning on various graph domains. They are usually based on a message-passing mechanism and have gained increasing popularity for their intuitive formulation, which is closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism to which they have been proven equivalent in terms of expressive power. In this work, we establish new generalization properties and fundamental limits of GNNs in the context of learning so-called identity effects, i.e., the task of determining whether an object is composed of two identical components or not. Our study is motivated by the need to understand the capabilities of GNNs when performing simple cognitive tasks, with potential applications in computational linguistics and chemistry. We analyze two case studies: (i) two-letters words, for which we show that GNNs trained via stochastic gradient descent are unable to generalize to unseen letters when utilizing orthogo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;AutoPoison&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#27602;&#21270;&#65292;&#23545;&#25163;&#33021;&#22815;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17194</link><description>&lt;p&gt;
&#20851;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#21487;&#21033;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;AutoPoison&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#27602;&#21270;&#65292;&#23545;&#25163;&#33021;&#22815;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#25163;&#22914;&#20309;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#27880;&#20837;&#29305;&#23450;&#30340;&#25351;&#20196;&#36319;&#38543;&#31034;&#20363;&#26469;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#65292;&#20174;&#32780;&#26377;&#24847;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#27880;&#20837;&#25552;&#21450;&#30446;&#26631;&#20869;&#23481;&#30340;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#24341;&#35825;&#19979;&#28216;&#27169;&#22411;&#23637;&#31034;&#27492;&#31867;&#34892;&#20026;&#26469;&#23454;&#29616;&#20869;&#23481;&#27880;&#20837;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AutoPoison&#12290;&#23427;&#20351;&#29992;&#20102;&#19968;&#20010;&#39044;&#35328;&#27169;&#22411;&#26469;&#23558;&#22810;&#26679;&#25915;&#20987;&#30446;&#26631;&#33258;&#28982;&#32780;&#36830;&#36143;&#22320;&#27880;&#20837;&#21040;&#27602;&#21270;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#20363;&#25915;&#20987;&#65306;&#20869;&#23481;&#27880;&#20837;&#21644;&#36807;&#24230;&#25298;&#32477;&#25915;&#20987;&#65292;&#27599;&#20010;&#25915;&#20987;&#37117;&#26088;&#22312;&#35825;&#23548;&#29305;&#23450;&#30340;&#21487;&#21033;&#29992;&#34892;&#20026;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#27880;&#20837;&#26041;&#26696;&#30340;&#24378;&#24230;&#21644;&#38544;&#34109;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#27602;&#21270;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;AutoPoison&#20801;&#35768;&#23545;&#25163;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16424</link><description>&lt;p&gt;
&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#29992;&#20110;&#21453;&#27927;&#38065;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37329;&#34701;&#30340;&#24191;&#27867;&#25968;&#23383;&#21270;&#21644;&#21152;&#23494;&#36135;&#24065;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#35774;&#35745;&#30340;&#27450;&#35784;&#26041;&#26696;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#27927;&#38065;&#8212;&#8212;&#23558;&#38750;&#27861;&#36164;&#37329;&#31227;&#21160;&#20197;&#25513;&#30422;&#20854;&#26469;&#28304;&#8212;&#8212;&#21487;&#20197;&#36328;&#36234;&#38134;&#34892;&#21644;&#22269;&#30028;&#65292;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#26131;&#27169;&#24335;&#12290;&#32852;&#21512;&#22269;&#20272;&#35745;&#27599;&#24180;&#20840;&#29699;&#27927;&#38065;&#37329;&#39069;&#21344;&#20840;&#29699;GDP&#30340;2-5%&#65292;&#32422;&#20026;0.8-2.0&#19975;&#20159;&#32654;&#20803;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#27927;&#38065;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#19988;&#20043;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#23384;&#22312;&#26174;&#33879;&#32570;&#38519;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#24182;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#38656;&#35201;&#19968;&#20010;&#36924;&#30495;&#12289;&#26631;&#20934;&#21270;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26681;&#25454;&#23454;&#38469;&#20132;&#26131;&#23613;&#21487;&#33021;&#22320;&#26657;&#20934;&#20102;&#36825;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15156</link><description>&lt;p&gt;
&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27169;&#20223;&#23398;&#20064;&#20551;&#35774;&#33021;&#22815;&#33719;&#24471;&#23637;&#31034;&#32773;&#30340;&#21160;&#20316;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36825;&#20123;&#21160;&#20316;&#36890;&#24120;&#26080;&#27861;&#35266;&#27979;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#34892;&#20026;&#21487;&#33021;&#20559;&#31163;&#26631;&#20934;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;nMDP&#65289;&#20013;&#20165;&#29366;&#24577;&#24207;&#21015;&#30340;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#20854;&#20013;&#31574;&#30053;&#26159;&#28508;&#22312;&#29366;&#24577;&#36716;&#31227;&#29983;&#25104;&#22120;&#30340;&#33021;&#37327;&#20808;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20808;&#39564;&#36827;&#34892;&#30701;&#26399;MCMC&#37319;&#26679;&#21644;&#23545;&#21518;&#39564;&#36827;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#21363;&#26080;&#27169;&#22411;&#31574;&#30053;&#25191;&#34892;&#31561;&#20215;&#20110;&#20808;&#39564;&#37319;&#26679;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21017;&#26159;&#20174;&#31574;&#30053;&#21021;&#22987;&#21270;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#38750;&#39532;&#23572;&#31185;&#22827;&#29305;&#24449;&#30340;&#21407;&#22411;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
&lt;/p&gt;</description></item><item><title>InterCode&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#32534;&#30721;&#30340;&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23433;&#20840;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14898</link><description>&lt;p&gt;
InterCode:&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#25191;&#34892;&#21453;&#39304;&#30340;&#20132;&#20114;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14898
&lt;/p&gt;
&lt;p&gt;
InterCode&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#32534;&#30721;&#30340;&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23433;&#20840;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20197;&#22522;&#26412;&#20132;&#20114;&#26041;&#24335;&#32534;&#20889;&#20195;&#30721;&#65292;&#24182;&#20381;&#36182;&#20110;&#25345;&#32493;&#30340;&#25191;&#34892;&#21453;&#39304;&#26469;&#32416;&#27491;&#38169;&#35823;&#65292;&#35299;&#20915;&#27495;&#20041;&#21644;&#20998;&#35299;&#20219;&#21153;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;LLM&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#32534;&#30721;&#22522;&#20934;&#20027;&#35201;&#32771;&#34385;&#38745;&#24577;&#30340;&#25351;&#20196;&#21040;&#20195;&#30721;&#24207;&#21015;&#36716;&#25442;&#36807;&#31243;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20256;&#25773;&#21644;&#29983;&#25104;&#30340;&#20195;&#30721;&#19982;&#20854;&#26368;&#32456;&#25191;&#34892;&#29615;&#22659;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InterCode&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#28789;&#27963;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#20132;&#20114;&#24335;&#32534;&#30721;&#26694;&#26550;&#65292;&#20316;&#20026;&#19968;&#20010;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#65292;&#20351;&#29992;&#20195;&#30721;&#20316;&#20026;&#34892;&#21160;&#65292;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#35821;&#35328;&#21644;&#24179;&#21488;&#26080;&#20851;&#65292;&#20351;&#29992;&#29420;&#31435;&#30340;Docker&#29615;&#22659;&#25552;&#20379;&#23433;&#20840;&#21644;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;seq2seq&#32534;&#30721;&#26041;&#27861;&#24320;&#31665;&#21363;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;InterCode&#21019;&#24314;...
&lt;/p&gt;
&lt;p&gt;
Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#30693;&#35782;&#33976;&#39311;&#20026;&#22522;&#30784;&#65292;&#26088;&#22312;&#21152;&#36895;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#24320;&#21457;&#30693;&#35782;&#33976;&#39311;&#31574;&#30053;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#21152;&#36895;&#20102;&#20998;&#23376;GNNs&#65292;&#24182;&#22312;&#33021;&#37327;&#21644;&#21147;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14818</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21152;&#36895;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Molecular Graph Neural Networks via Knowledge Distillation. (arXiv:2306.14818v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#30693;&#35782;&#33976;&#39311;&#20026;&#22522;&#30784;&#65292;&#26088;&#22312;&#21152;&#36895;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#24320;&#21457;&#30693;&#35782;&#33976;&#39311;&#31574;&#30053;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#21152;&#36895;&#20102;&#20998;&#23376;GNNs&#65292;&#24182;&#22312;&#33021;&#37327;&#21644;&#21147;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#23545;&#20998;&#23376;&#21644;&#20998;&#23376;&#31995;&#32479;&#30340;&#24314;&#27169;&#26356;&#21152;&#20840;&#38754;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#27169;&#25311;&#30340;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#35813;&#39046;&#22495;&#36880;&#28176;&#21457;&#23637;&#21040;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#26368;&#20808;&#36827;&#30340;GNNs&#23545;&#35768;&#22810;&#22823;&#35268;&#27169;&#24212;&#29992;&#26469;&#35828;&#21464;&#24471;&#24456;&#38590;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#22312;&#21152;&#36895;&#20998;&#23376;GNNs&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;KD&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#23450;&#21521;&#21644;&#31561;&#21464;GNNs&#20013;&#38544;&#34255;&#34920;&#31034;&#30340;&#33976;&#39311;&#65292;&#24182;&#22312;&#33021;&#37327;&#21644;&#21147;&#39044;&#27979;&#30340;&#22238;&#24402;&#20219;&#21153;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#24072;&#29983;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21327;&#35758;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#23545;&#20854;&#26550;&#26500;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in graph neural networks (GNNs) have enabled more comprehensive modeling of molecules and molecular systems, thereby enhancing the precision of molecular property prediction and molecular simulations. Nonetheless, as the field has been progressing to bigger and more complex architectures, state-of-the-art GNNs have become largely prohibitive for many large-scale applications. In this paper, we explore the utility of knowledge distillation (KD) for accelerating molecular GNNs. To this end, we devise KD strategies that facilitate the distillation of hidden representations in directional and equivariant GNNs, and evaluate their performance on the regression task of energy and force prediction. We validate our protocols across different teacher-student configurations and datasets, and demonstrate that they can consistently boost the predictive accuracy of student models without any modifications to their architecture. Moreover, we conduct comprehensive optimization of vario
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#36845;&#20195;&#30340;&#21452;&#23618;&#33539;&#24335;&#65292;&#23558;&#36845;&#20195;&#24335;&#21452;&#23618;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#35299;&#32806;&#65292;&#24182;&#22238;&#31572;&#20102;&#20256;&#36882;&#20449;&#24687;&#12289;&#23433;&#20840;&#20248;&#21270;&#21644;&#21516;&#26102;&#36827;&#34892;&#22806;&#23618;&#20248;&#21270;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14479</link><description>&lt;p&gt;
&#20174;&#31574;&#30053;&#20013;&#35774;&#35745;&#65306;&#32447;&#19979;&#31574;&#30053;&#20248;&#21270;&#30340;&#20445;&#23432;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization. (arXiv:2306.14479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#36845;&#20195;&#30340;&#21452;&#23618;&#33539;&#24335;&#65292;&#23558;&#36845;&#20195;&#24335;&#21452;&#23618;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#35299;&#32806;&#65292;&#24182;&#22238;&#31572;&#20102;&#20256;&#36882;&#20449;&#24687;&#12289;&#23433;&#20840;&#20248;&#21270;&#21644;&#21516;&#26102;&#36827;&#34892;&#22806;&#23618;&#20248;&#21270;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36845;&#20195;&#24335;&#21452;&#23618;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#65288;&#20540;&#20272;&#35745;&#21644;&#31574;&#30053;&#25277;&#21462;&#65289;&#19982;&#32447;&#19979;&#35757;&#32451;&#38454;&#27573;&#35299;&#32806;&#65292;&#24418;&#25104;&#38750;&#36845;&#20195;&#30340;&#21452;&#23618;&#33539;&#24335;&#65292;&#24182;&#36991;&#20813;&#20102;&#20004;&#20010;&#23618;&#27425;&#30340;&#36845;&#20195;&#35823;&#24046;&#20256;&#25773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#31181;&#38750;&#36845;&#20195;&#30340;&#33539;&#24335;&#20801;&#35768;&#25105;&#20204;&#22312;&#35757;&#32451;&#20013;&#36827;&#34892;&#20869;&#23618;&#20248;&#21270;&#65288;&#20540;&#20272;&#35745;&#65289;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#20013;&#36827;&#34892;&#22806;&#23618;&#20248;&#21270;&#65288;&#31574;&#30053;&#25277;&#21462;&#65289;&#12290;&#33258;&#28982;&#22320;&#65292;&#36825;&#31181;&#33539;&#24335;&#25552;&#20986;&#20102;&#19977;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#38750;&#36845;&#20195;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#23545;&#24212;&#29289;&#65288;&#22914;&#22870;&#21169;&#26465;&#20214;&#31574;&#30053;&#65289;&#20013;&#27809;&#26377;&#24471;&#21040;&#23436;&#20840;&#22238;&#31572;&#65306;(q1)&#25105;&#20204;&#24212;&#35813;&#20174;&#20869;&#23618;&#21521;&#22806;&#23618;&#20256;&#36882;&#20160;&#20040;&#20449;&#24687;&#65311;(q2)&#24403;&#21033;&#29992;&#20256;&#36882;&#30340;&#20449;&#24687;&#36827;&#34892;&#23433;&#20840;/&#33258;&#20449;&#30340;&#22806;&#23618;&#20248;&#21270;&#26102;&#65292;&#25105;&#20204;&#24212;&#35813;&#27880;&#24847;&#20160;&#20040;&#65311;(q3)&#22312;&#27979;&#35797;&#26399;&#38388;&#21516;&#26102;&#36827;&#34892;&#22806;&#23618;&#20248;&#21270;&#26377;&#20160;&#20040;&#22909;&#22788;&#65311;&#21463;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#65288;MBO&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DROP&#65288;&#20174;&#31574;&#30053;&#20013;&#35774;&#35745;&#65289;&#65292;&#23427;&#23436;&#20840;&#22238;&#31572;&#20102;&#36825;&#19977;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we decouple the iterative bi-level offline RL (value estimation and policy extraction) from the offline training phase, forming a non-iterative bi-level paradigm and avoiding the iterative error propagation over two levels. Specifically, this non-iterative paradigm allows us to conduct inner-level optimization (value estimation) in training, while performing outer-level optimization (policy extraction) in testing. Naturally, such a paradigm raises three core questions that are not fully answered by prior non-iterative offline RL counterparts like reward-conditioned policy: (q1) What information should we transfer from the inner-level to the outer-level? (q2) What should we pay attention to when exploiting the transferred information for safe/confident outer-level optimization? (q3) What are the benefits of concurrently conducting outer-level optimization during testing? Motivated by model-based optimization (MBO), we propose DROP (design from policies), which fully answer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#28436;&#21069;&#21521;&#27169;&#22411;&#12289;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#31561;&#25163;&#27573;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.13104</link><description>&lt;p&gt;
&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses. (arXiv:2306.13104v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#28436;&#21069;&#21521;&#27169;&#22411;&#12289;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#31561;&#25163;&#27573;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21069;&#21015;&#33146;&#22312;&#24674;&#22797;&#22833;&#21435;&#30340;&#24863;&#23448;&#21151;&#33021;&#21644;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#35774;&#22791;&#20135;&#29983;&#30340;&#24863;&#35273;&#36890;&#24120;&#20284;&#20046;&#19981;&#33258;&#28982;&#25110;&#25197;&#26354;&#12290;&#26893;&#20837;&#22120;&#30340;&#30830;&#20999;&#20301;&#32622;&#21644;&#20010;&#20307;&#24863;&#30693;&#30340;&#24046;&#24322;&#23548;&#33268;&#21050;&#28608;&#21709;&#24212;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#20351;&#20010;&#24615;&#21270;&#21050;&#28608;&#20248;&#21270;&#25104;&#20026;&#20851;&#38190;&#25361;&#25112;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#21487;&#29992;&#20110;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#22122;&#22768;&#35266;&#23519;&#25968;&#25454;&#30340;&#24739;&#32773;&#19987;&#23646;&#21050;&#28608;&#21442;&#25968;&#65292;&#20294;&#23545;&#20110;&#39640;&#32500;&#21050;&#28608;&#19981;&#21487;&#34892;&#12290;&#32780;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#21050;&#28608;&#32534;&#30721;&#31574;&#30053;&#65292;&#20294;&#36890;&#24120;&#20551;&#35774;&#26377;&#20851;&#24739;&#32773;&#29305;&#23450;&#21464;&#21270;&#30340;&#23436;&#32654;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#23454;&#38469;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21453;&#28436;&#23558;&#30005;&#21050;&#28608;&#26144;&#23556;&#21040;&#35270;&#35273;&#24863;&#30693;&#30340;&#21069;&#21521;&#27169;&#22411;&#65292;&#35757;&#32451;&#28145;&#24230;&#32534;&#30721;&#22120;&#32593;&#32476;&#20197;&#20026;&#20219;&#20309;&#20010;&#20307;&#24739;&#32773;&#20135;&#29983;&#26368;&#20339;&#21050;&#28608;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#36873;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#65292;&#25104;&#21151;&#20351;&#30693;&#35273;&#21050;&#28608;&#26356;&#21152;&#36924;&#30495;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#26041;&#27861;&#22312;&#21160;&#29289;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuroprostheses show potential in restoring lost sensory function and enhancing human capabilities, but the sensations produced by current devices often seem unnatural or distorted. Exact placement of implants and differences in individual perception lead to significant variations in stimulus response, making personalized stimulus optimization a key challenge. Bayesian optimization could be used to optimize patient-specific stimulation parameters with limited noisy observations, but is not feasible for high-dimensional stimuli. Alternatively, deep learning models can optimize stimulus encoding strategies, but typically assume perfect knowledge of patient-specific variations. Here we propose a novel, practically feasible approach that overcomes both of these fundamental limitations. First, a deep encoder network is trained to produce optimal stimuli for any individual patient by inverting a forward model mapping electrical stimuli to visual percepts. Second, a preferential Bayesian opti
&lt;/p&gt;</description></item><item><title>SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13092</link><description>&lt;p&gt;
&#20174;&#26032;&#30340;&#35282;&#24230;&#21387;&#32553;ImageNet&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;SRe$^2$L
&lt;/p&gt;
&lt;p&gt;
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13092
&lt;/p&gt;
&lt;p&gt;
SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65292;&#31216;&#20026;Squeeze&#12289;Recover&#21644;Relabel&#65288;SRe$^2$L&#65289;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#20998;&#31163;&#20102;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#21452;&#23618;&#20248;&#21270;&#65292;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#35268;&#27169;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#22270;&#20687;&#20219;&#24847;&#20998;&#36776;&#29575;&#12289;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;Tiny-ImageNet&#21644;&#23436;&#25972;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312;50IPC&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;42.5&#65285;&#21644;60.8&#65285;&#30340;&#26368;&#39640;&#39564;&#35777;&#31934;&#24230;&#65292;&#36739;&#20043;&#21069;&#25152;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;14.5&#65285;&#21644;32.9&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Res&#19978;&#20063;&#27604;MTT&#24555;&#32422;52&#20493;(ConvNet-4)&#21644;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20110;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;CLASH&#65292;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.11839</link><description>&lt;p&gt;
&#26159;&#21542;&#24212;&#35813;&#20572;&#27490;&#65306;&#20855;&#26377;&#24322;&#36136;&#31181;&#32676;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations. (arXiv:2306.11839v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20110;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;CLASH&#65292;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23454;&#39564;&#30001;&#20110;&#27835;&#30103;&#36896;&#25104;&#24847;&#22806;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#22240;&#27492;&#24448;&#24448;&#38656;&#35201;&#25552;&#21069;&#20572;&#27490;&#12290;&#30446;&#21069;&#30830;&#23450;&#20309;&#26102;&#25552;&#21069;&#32456;&#27490;&#23454;&#39564;&#30340;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36866;&#29992;&#20110;&#24635;&#20307;&#25968;&#25454;&#65292;&#19981;&#32771;&#34385;&#27835;&#30103;&#25928;&#24212;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#29616;&#26377;&#26041;&#27861;&#22312;&#27835;&#30103;&#23545;&#23569;&#25968;&#21442;&#19982;&#32773;&#36896;&#25104;&#20260;&#23475;&#26102;&#24448;&#24448;&#26080;&#27861;&#20572;&#27490;&#23454;&#39564;&#12290;&#28982;&#21518;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20102;CLASH&#65292;&#36825;&#26159;&#39318;&#20010;&#24191;&#27867;&#36866;&#29992;&#20110;&#24322;&#36136;&#26089;&#26399;&#20572;&#27490;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;CLASH&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#20013;&#37117;&#33021;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20174;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#39640;&#25928;&#19988;&#33021;&#22312;&#36275;&#22815;&#35206;&#30422;&#25968;&#25454;&#30340;&#21306;&#22495;&#21644;&#36275;&#22815;&#36828;&#31163;&#25968;&#25454;&#30340;&#21306;&#22495;&#20013;&#20135;&#29983;&#25509;&#36817;&#30495;&#23454;&#21518;&#39564;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.11589</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20174;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent. (arXiv:2306.11589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20174;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#39640;&#25928;&#19988;&#33021;&#22312;&#36275;&#22815;&#35206;&#30422;&#25968;&#25454;&#30340;&#21306;&#22495;&#21644;&#36275;&#22815;&#36828;&#31163;&#25968;&#25454;&#30340;&#21306;&#22495;&#20013;&#20135;&#29983;&#25509;&#36817;&#30495;&#23454;&#21518;&#39564;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#26159;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#21644;&#39034;&#24207;&#20915;&#31574;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#20854;&#38656;&#35201;&#27714;&#35299;&#32447;&#24615;&#31995;&#32479;&#65292;&#27599;&#24403;&#25968;&#25454;&#38598;&#22823;&#23567;&#22686;&#21152;&#26102;&#20195;&#20215;&#26159;&#31435;&#26041;&#32423;&#21035;&#30340;&#19988;&#23545;&#26465;&#20214;&#25935;&#24863;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#20123;&#32447;&#24615;&#31995;&#32479;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#20302;&#26041;&#24046;&#30340;&#26368;&#20248;&#21270;&#30446;&#26631;&#20197;&#20174;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#24341;&#20837;&#28857;&#12290;&#20196;&#20154;&#24847;&#24819;&#19981;&#21040;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#19981;&#24555;&#36895;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36890;&#24120;&#20063;&#20250;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#38750;&#25910;&#25947;&#30340;&#38544;&#24335;&#20559;&#32622;&#30340;&#35889;&#29305;&#24449;&#26469;&#35299;&#37322;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20250;&#22312;&#36275;&#22815;&#35206;&#30422;&#25968;&#25454;&#30340;&#21306;&#22495;&#21644;&#36275;&#22815;&#36828;&#31163;&#25968;&#25454;&#30340;&#21306;&#22495;&#20013;&#20135;&#29983;&#25509;&#36817;&#30495;&#23454;&#21518;&#39564;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are a powerful framework for quantifying uncertainty and for sequential decision-making but are limited by the requirement of solving linear systems. In general, this has a cubic cost in dataset size and is sensitive to conditioning. We explore stochastic gradient algorithms as a computationally efficient method of approximately solving these linear systems: we develop low-variance optimization objectives for sampling from the posterior and extend these to inducing points. Counterintuitively, stochastic gradient descent often produces accurate predictions, even in cases where it does not converge quickly to the optimum. We explain this through a spectral characterization of the implicit bias from non-convergence. We show that stochastic gradient descent produces predictive distributions close to the true posterior both in regions with sufficient data coverage, and in regions sufficiently far away from the data. Experimentally, stochastic gradient descent achieves sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.11207</link><description>&lt;p&gt;
Quilt-1M: &#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#21152;&#36895;&#20351;&#24471;&#22312;&#32447;&#22270;&#20687;&#21644;&#25991;&#23383;&#25968;&#25454;&#22823;&#37327;&#28044;&#29616;&#65292;&#20294;&#21307;&#23398;&#39046;&#22495;&#65288;&#29305;&#21035;&#26159;&#30284;&#30151;&#32452;&#32455;&#23398;&#65289;&#31867;&#20284;&#30340;&#25968;&#25454;&#21364;&#24456;&#31232;&#23569;&#65292;&#36825;&#38459;&#30861;&#20102;&#21307;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#21033;&#29992;YouTube&#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#65292;&#20174;&#20013;&#36873;&#25321;&#20102; 1,087 &#23567;&#26102;&#30340;&#21307;&#23398;&#32452;&#32455;&#23398;&#35270;&#39057;&#65292;&#20197;&#27492;&#33258;&#21160;&#31579;&#36873;&#20986;&#20849;&#21253;&#21547; 768,826 &#20010;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#23383;&#23545;&#30340; Quilt &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10759</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#22270;&#34920;&#31034;&#31616;&#21270;&#21644;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#34920;&#31034;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#32534;&#30721;&#22120;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#27880;&#24847;&#21147;&#21487;&#20197;&#25429;&#25417;&#21040;&#37051;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#23545;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#23567;&#22411;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32487;&#25215;&#20102;Transformer&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#28145;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#26469;&#37319;&#29992;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#20110;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#20063;&#33021;&#22312;&#33410;&#28857;&#25968;&#37327;&#20174;&#21315;&#32423;&#21040;&#21313;&#20159;&#32423;&#30340;&#33539;&#22260;&#20869;&#24102;&#26469;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#40723;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20854;&#20013;&#20840;&#23616;&#27880;&#24847;&#21147;&#26159;&#19968;&#20010;&#38459;&#30861;&#21487;&#25193;&#23637;&#24615;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#26696;&#31216;&#20026;&#31616;&#21270;&#22270;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#21028;&#26029;&#23427;&#20204;&#26159;&#21542;&#21033;&#29992;&#20102;&#30456;&#21516;&#30340;&#20869;&#37096;&#36807;&#31243;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.10168</link><description>&lt;p&gt;
&#36229;&#36234;&#20960;&#20309;&#65306;&#20351;&#29992;&#21160;&#21147;&#30456;&#20284;&#24615;&#20998;&#26512;&#27604;&#36739;&#31070;&#32463;&#22238;&#36335;&#35745;&#31639;&#20013;&#30340;&#35745;&#31639;&#26102;&#38388;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis. (arXiv:2306.10168v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#21028;&#26029;&#23427;&#20204;&#26159;&#21542;&#21033;&#29992;&#20102;&#30456;&#21516;&#30340;&#20869;&#37096;&#36807;&#31243;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#21028;&#26029;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#22312;&#29305;&#23450;&#35745;&#31639;&#20013;&#21033;&#29992;&#20102;&#30456;&#21516;&#30340;&#20869;&#37096;&#36807;&#31243;&#65311;&#36825;&#20010;&#38382;&#39064;&#23545;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20010;&#23376;&#39046;&#22495;&#37117;&#24456;&#37325;&#35201;&#65292;&#21253;&#25324;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#26800;&#35299;&#37322;&#24615;&#21644;&#33041;&#26426;&#25509;&#21475;&#12290;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#26041;&#27861;&#27880;&#37325;&#28508;&#22312;&#29366;&#24577;&#30340;&#31354;&#38388;&#20960;&#20309;&#12290;&#28982;&#32780;&#65292;&#22312;&#24490;&#29615;&#32593;&#32476;&#20013;&#65292;&#35745;&#31639;&#26159;&#22312;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23618;&#38754;&#19978;&#23454;&#29616;&#30340;&#65292;&#23427;&#20204;&#19982;&#20960;&#20309;&#27809;&#26377;&#31616;&#21333;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#20851;&#31995;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#22312;&#21160;&#21147;&#23398;&#30340;&#23618;&#38754;&#19978;&#27604;&#36739;&#20004;&#20010;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#20351;&#29992;&#26368;&#36817;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26680;&#24515;&#29305;&#24449;&#30340;&#39640;&#32500;&#32447;&#24615;&#31995;&#32479;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;Procrustes&#20998;&#26512;&#30340;&#25193;&#23637;&#26041;&#27861;&#27604;&#36739;&#36825;&#20123;&#32447;&#24615;&#36817;&#20284;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#21521;&#37327;&#22330;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we tell whether two neural networks are utilizing the same internal processes for a particular computation? This question is pertinent for multiple subfields of both neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of neural dynamics, which do not have a simple one-to-one mapping with geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics. Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare these linear approximations via a novel extension of Procrustes Analysis that accounts for how vector fields c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09539</link><description>&lt;p&gt;
&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38656;&#35201;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#24615;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20026;&#36830;&#32493;&#20449;&#21495;&#35774;&#35745;&#30340;&#65292;&#20294;SSM&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;SSM&#20173;&#28982;&#33853;&#21518;&#20110;Transformers&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#23618;&#65292;&#23427;&#22312;&#20869;&#37096;&#32452;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#21270;&#30340;SSM&#23376;&#23618;&#21644;&#19968;&#20010;&#29992;&#20110;&#30701;&#26399;&#24207;&#21015;&#34920;&#31034;&#30340;&#22359;&#21464;&#25442;&#22120;&#23376;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#12289;&#23436;&#20840;&#21487;&#24182;&#34892;&#30340;&#38598;&#25104;SSM&#21644;&#22359;&#27880;&#24847;&#21147;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09526</link><description>&lt;p&gt;
&#27531;&#24046; Q &#23398;&#20064;&#65306;&#26080;&#38656;&#20215;&#20540;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#34892;&#20026;&#12290;&#24403;&#25163;&#24037;&#21046;&#20316;&#22870;&#21169;&#20989;&#25968;&#22256;&#38590;&#25110;&#30446;&#26631;&#26159;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#34892;&#20026;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#20294;&#26159;&#65292;&#23398;&#20064;&#30340;&#27169;&#20223;&#31574;&#30053;&#21482;&#33021;&#36981;&#24490;&#28436;&#31034;&#20013;&#30340;&#34892;&#20026;&#12290;&#22312;&#24212;&#29992;&#27169;&#20223;&#31574;&#30053;&#26102;&#65292;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#35201;&#27714;&#23450;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20173;&#24076;&#26395;&#23450;&#21046;&#30340;&#31574;&#30053;&#20445;&#25345;&#20854;&#27169;&#20223;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#31216;&#20026;&#31574;&#30053;&#23450;&#21046;&#12290;&#23427;&#23558;&#23398;&#20064;&#20219;&#21153;&#23450;&#20041;&#20026;&#35757;&#32451;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32487;&#25215;&#20808;&#21069;&#31574;&#30053;&#30340;&#29305;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#24378;&#21152;&#30340;&#19968;&#20123;&#38468;&#21152;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#30830;&#23450;&#20004;&#20010;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#32858;&#31867;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31867;&#26465;&#20214;&#19979;&#25552;&#20379;&#31867;&#21035;&#26465;&#20214;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#38024;&#23545;&#22810;&#20010;&#31867;&#21035;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#32463;&#39564;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09335</link><description>&lt;p&gt;
&#22810;&#31867;&#26465;&#20214;&#19979;&#30340;&#31867;&#21035;&#26465;&#20214;&#31526;&#21512;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Class-Conditional Conformal Prediction With Many Classes. (arXiv:2306.09335v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#32858;&#31867;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31867;&#26465;&#20214;&#19979;&#25552;&#20379;&#31867;&#21035;&#26465;&#20214;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#38024;&#23545;&#22810;&#20010;&#31867;&#21035;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#32463;&#39564;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#36793;&#32536;&#35206;&#30422;&#20445;&#35777;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#19968;&#20010;&#38543;&#26426;&#30340;&#27979;&#35797;&#28857;&#65292;&#31526;&#21512;&#24615;&#39044;&#27979;&#38598;&#21512;&#20197;&#29992;&#25143;&#36873;&#25321;&#30340;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#12290;&#22312;&#35768;&#22810;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#33719;&#24471;&#26356;&#24378;&#30340;&#20445;&#35777;&#8212;&#8212;&#23545;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#27979;&#35797;&#28857;&#65292;&#39044;&#27979;&#38598;&#20197;&#30456;&#21516;&#30340;&#29992;&#25143;&#36873;&#25321;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#12290;&#29616;&#26377;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#22312;&#27599;&#20010;&#31867;&#21035;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#26159;&#22823;&#37327;&#31867;&#21035;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32858;&#31867;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#20855;&#26377;&#8220;&#30456;&#20284;&#8221;&#31526;&#21512;&#24615;&#20998;&#25968;&#30340;&#31867;&#21035;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;&#28982;&#21518;&#22312;&#32858;&#31867;&#32423;&#21035;&#19978;&#25191;&#34892;&#31526;&#21512;&#24615;&#39044;&#27979;&#12290;&#22312;&#38024;&#23545;&#22810;&#20010;&#65288;&#22810;&#36798;1000&#65289;&#31867;&#21035;&#30340;&#22235;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#32858;&#31867;&#31526;&#21512;&#24615;&#36890;&#24120;&#22312;&#31867;&#26465;&#20214;&#35206;&#30422;&#21644;&#38598;&#21512;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard conformal prediction methods provide a marginal coverage guarantee, which means that for a random test point, the conformal prediction set contains the true label with a user-chosen probability. In many classification problems, we would like to obtain a stronger guarantee -- that for test points of a specific class, the prediction set contains the true label with the same user-chosen probability. Existing conformal prediction methods do not work well when there is a limited amount of labeled data per class, as is often the case in real applications where the number of classes is large. We propose a method called clustered conformal prediction, which clusters together classes that have "similar" conformal scores and then performs conformal prediction at the cluster level. Based on empirical evaluation across four image data sets with many (up to 1000) classes, we find that clustered conformal typically outperforms existing methods in terms of class-conditional coverage and set 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32422;&#26463;&#31070;&#32463;&#22330;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#26045;&#20219;&#24847;&#24494;&#20998;&#38454;&#30340;&#30828;&#32422;&#26463;&#12290;&#36890;&#36807;&#24212;&#29992;&#32447;&#24615;&#31639;&#23376;&#21040;&#31070;&#32463;&#22330;&#21450;&#20854;&#23548;&#25968;&#65292;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#26631;&#20934;&#27169;&#22411;&#22312;&#21463;&#38480;&#21046;&#24773;&#20917;&#19979;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08943</link><description>&lt;p&gt;
&#20855;&#26377;&#20219;&#24847;&#24494;&#20998;&#38454;&#30828;&#32422;&#26463;&#30340;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Neural Fields with Hard Constraints of Arbitrary Differential Order. (arXiv:2306.08943v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32422;&#26463;&#31070;&#32463;&#22330;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#26045;&#20219;&#24847;&#24494;&#20998;&#38454;&#30340;&#30828;&#32422;&#26463;&#12290;&#36890;&#36807;&#24212;&#29992;&#32447;&#24615;&#31639;&#23376;&#21040;&#31070;&#32463;&#22330;&#21450;&#20854;&#23548;&#25968;&#65292;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#26631;&#20934;&#27169;&#22411;&#22312;&#21463;&#38480;&#21046;&#24773;&#20917;&#19979;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#21508;&#31181;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#65292;&#20294;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#24378;&#21046;&#26045;&#21152;&#30828;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#20173;&#28982;&#19981;&#22826;&#25104;&#29087;&#12290;&#21463;&#21040;&#32593;&#26684;&#26080;&#32422;&#26463;&#25554;&#20540;&#21644;&#20854;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#30340;&#20809;&#35889;&#33394;&#25955;&#26041;&#27861;&#30340;&#20016;&#23500;&#25991;&#29486;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#22312;&#31070;&#32463;&#22330;&#19978;&#24378;&#21046;&#26045;&#21152;&#30828;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32422;&#26463;&#31070;&#32463;&#22330;&#65288;CNF&#65289;&#12290;&#32422;&#26463;&#21487;&#20197;&#25351;&#23450;&#20026;&#24212;&#29992;&#20110;&#31070;&#32463;&#22330;&#21450;&#20854;&#23548;&#25968;&#30340;&#32447;&#24615;&#31639;&#23376;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#29305;&#23450;&#30340;&#27169;&#22411;&#34920;&#31034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#26631;&#20934;&#27169;&#22411;&#21487;&#33021;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#22914;&#31995;&#32479;&#30340;&#26465;&#20214;&#12289;&#20869;&#23384;&#28040;&#32791;&#21644;&#22312;&#21463;&#38480;&#21046;&#26102;&#32593;&#32476;&#30340;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as Constrained Neural Fields (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MMD-FUSE&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#20869;&#26680;&#38598;&#21512;&#26368;&#22823;&#21270;&#22522;&#20110;MMD&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#21151;&#29575;&#65292;&#36991;&#20813;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#22312;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#21644;&#39640;&#32500;&#23454;&#38469;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#20854;&#36866;&#29992;&#24615;&#21644;&#21151;&#29575;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26680;&#26816;&#39564;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08777</link><description>&lt;p&gt;
MMD-FUSE: &#22312;&#19981;&#20998;&#21106;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21644;&#32452;&#21512;&#20869;&#26680;&#36827;&#34892;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting. (arXiv:2306.08777v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MMD-FUSE&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#20869;&#26680;&#38598;&#21512;&#26368;&#22823;&#21270;&#22522;&#20110;MMD&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#21151;&#29575;&#65292;&#36991;&#20813;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#22312;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#21644;&#39640;&#32500;&#23454;&#38469;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#20854;&#36866;&#29992;&#24615;&#21644;&#21151;&#29575;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26680;&#26816;&#39564;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#23450;&#20041;&#35813;&#26041;&#27861;&#30340;&#20869;&#26680;&#38598;&#21512;&#65292;&#26368;&#22823;&#21270;&#22522;&#20110;&#26368;&#22823;&#24179;&#22343;&#20559;&#24046;&#65288;MMD&#65289;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#21151;&#29575;&#12290; &#23545;&#20110;&#26377;&#38480;&#38598;&#21512;&#65292;&#36825;&#23601;&#32553;&#23567;&#20102;&#36890;&#36807;&#21152;&#26435;&#36719;&#26368;&#22823;&#20540;&#32452;&#21512;&#65288;&#26631;&#20934;&#21270;&#30340;&#65289;&#27599;&#20010;&#20869;&#26680;&#19979;&#30340;MMD&#20540;&#12290; &#23545;&#20110;&#38646;&#20551;&#35774;&#21644;&#22791;&#25321;&#20551;&#35774;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#32479;&#35745;&#37327;&#30340;&#25351;&#25968;&#27987;&#24230;&#19978;&#38480;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25968;&#25454;&#20381;&#36182;&#20294;&#19982;&#25490;&#21015;&#29420;&#31435;&#30340;&#26041;&#24335;&#36873;&#25321;&#36825;&#20123;&#20869;&#26680;&#65292;&#22312;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27979;&#35797;&#20013;&#36991;&#20813;&#25968;&#25454;&#20998;&#21106;&#12290; &#36825;&#31181;&#25216;&#26415;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#22522;&#20110;&#19968;&#33324;&#25490;&#21015;&#30340;MMD&#27979;&#35797;&#65292;&#24182;&#19988;&#21253;&#25324;&#20351;&#29992;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#31561;&#26080;&#30417;&#30563;&#27169;&#22411;&#23398;&#20064;&#30340;&#28145;&#24230;&#20869;&#26680;&#12290; &#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;MMD-FUSE&#27979;&#35797;&#22312;&#21512;&#25104;&#20302;&#32500;&#25968;&#25454;&#21644;&#29616;&#23454;&#19990;&#30028;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#21151;&#29575;&#34920;&#29616;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20869;&#26680;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), by adapting over the set of kernels used in defining it. For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum. Exponential concentration bounds are proved for our proposed statistics under the null and alternative. We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting. This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders. We highlight the applicability of our MMD-FUSE test on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.
&lt;/p&gt;</description></item><item><title>LargeST&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#22522;&#20934;&#65292;&#21253;&#25324;8600&#20010;&#20256;&#24863;&#22120;&#12289;&#35206;&#30422;5&#24180;&#26102;&#38388;&#21644;&#21253;&#25324;&#32454;&#33268;&#20803;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.08259</link><description>&lt;p&gt;
LargeST: &#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#20132;&#36890;&#39044;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting. (arXiv:2306.08259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08259
&lt;/p&gt;
&lt;p&gt;
LargeST&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#22522;&#20934;&#65292;&#21253;&#25324;8600&#20010;&#20256;&#24863;&#22120;&#12289;&#35206;&#30422;5&#24180;&#26102;&#38388;&#21644;&#21253;&#25324;&#32454;&#33268;&#20803;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#22312;&#26234;&#24935;&#22478;&#24066;&#39033;&#30446;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#27169;&#24335;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;LargeST&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;8600&#20010;&#20256;&#24863;&#22120;&#12289;&#35206;&#30422;5&#24180;&#26102;&#38388;&#21644;&#21253;&#25324;&#32454;&#33268;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;LargeST&#36827;&#34892;&#28145;&#20837;&#25968;&#25454;&#20998;&#26512;&#24182;&#28436;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#32780;&#35328;&#26159;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting plays a critical role in smart city initiatives and has experienced significant advancements thanks to the power of deep learning in capturing non-linear patterns of traffic data. However, the promising results achieved on current public datasets may not be applicable to practical scenarios due to limitations within these datasets. First, the limited sizes of them may not reflect the real-world scale of traffic networks. Second, the temporal coverage of these datasets is typically short, posing hurdles in studying long-term patterns and acquiring sufficient samples for training deep models. Third, these datasets often lack adequate metadata for sensors, which compromises the reliability and interpretability of the data. To mitigate these limitations, we introduce the LargeST benchmark dataset. It encompasses a total number of 8,600 sensors with a 5-year time coverage and includes comprehensive metadata. Using LargeST, we perform in-depth data analysis to extract dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07528</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65306;&#24378;&#21270;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26088;&#22312;&#36890;&#36807;&#24050;&#37096;&#32626;&#30340;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#20248;&#21270;&#25490;&#21517;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#32463;&#24120;&#23545;&#29992;&#25143;&#22914;&#20309;&#29983;&#25104;&#28857;&#20987;&#25968;&#25454;&#21363;&#28857;&#20987;&#27169;&#22411;&#36827;&#34892;&#20551;&#35774;&#65292;&#22240;&#27492;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#28857;&#20987;&#27169;&#22411;&#19987;&#38376;&#35843;&#25972;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25490;&#21517;&#36807;&#31243;&#22312;&#19968;&#33324;&#38543;&#26426;&#28857;&#20987;&#27169;&#22411;&#19979;&#32479;&#19968;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#31163;&#32447;RL&#25216;&#26415;&#36827;&#34892;&#38750;&#21516;&#31574;&#30053;LTR&#65292;&#24182;&#25552;&#20986;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MDP&#30340;&#19987;&#38376;&#21046;&#23450;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#21435;&#20559;&#20506;&#25216;&#26415;&#21644;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#32479;&#19968;&#23450;&#20041;&#21644;&#28548;&#28165;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#65292;&#36827;&#32780;&#25552;&#20379;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.07304</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#20840;&#38754;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation. (arXiv:2306.07304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#32479;&#19968;&#23450;&#20041;&#21644;&#28548;&#28165;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#65292;&#36827;&#32780;&#25552;&#20379;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#25104;&#20026;&#20102;&#19968;&#20123;&#26368;&#26377;&#21069;&#36884;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24110;&#21161;&#25105;&#20204;&#35299;&#37322;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#35797;&#22270;&#22312;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#20013;&#21457;&#29616;&#34987;&#38544;&#34255;&#22312;ANN&#28608;&#27963;&#30340;&#22797;&#26434;&#27169;&#24335;&#20013;&#30340;&#21487;&#29702;&#35299;&#30340;&#35270;&#35273;&#8220;&#27010;&#24565;&#8221;&#65306;&#65288;1&#65289;&#27010;&#24565;&#25552;&#21462;&#65292;&#65288;2&#65289;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#27493;&#39588;&#26159;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#20849;&#21516;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#20855;&#20307;&#23454;&#29616;&#37117;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20840;&#38754;&#23450;&#20041;&#21644;&#28548;&#28165;&#20102;&#36825;&#20004;&#20010;&#27493;&#39588;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#65306;&#65288;i&#65289;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#29616;&#20195;&#24402;&#22240;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#26469;&#25193;&#23637;&#21644;&#31995;&#32479;&#22320;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#21644;&#37325;&#35201;&#24615;&#35780;&#20272;&#25216;&#26415;&#65307;&#65288;iii&#65289;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual 'concepts' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that comprehensively defines and clarifies these two steps. This framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii) to derive theoretical guarantees regarding the optimality of such met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#35299;&#26512;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#30697;&#26469;&#23450;&#20041;&#21487;&#36870;&#22810;&#37325;&#38598;&#21512;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#35777;&#25454;&#23450;&#29702;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#37325;&#38598;&#21512;&#21644;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06529</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#35777;&#25454;&#23450;&#29702;&#65292;&#29992;&#20110;&#22810;&#37325;&#38598;&#21512;&#12289;&#24230;&#37327;&#21644;&#22270;&#30340;&#31070;&#32463;&#21487;&#36870;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem. (arXiv:2306.06529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#35299;&#26512;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#30697;&#26469;&#23450;&#20041;&#21487;&#36870;&#22810;&#37325;&#38598;&#21512;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#35777;&#25454;&#23450;&#29702;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#37325;&#38598;&#21512;&#21644;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36870;&#22810;&#37325;&#38598;&#21512;&#20989;&#25968;&#22312;&#22810;&#37325;&#38598;&#21512;&#21644;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#30740;&#31350;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29702;&#35770;&#19978;&#32771;&#34385;&#30340;&#21487;&#35777;&#26126;&#21487;&#36870;&#22810;&#37325;&#38598;&#21512;&#20989;&#25968;&#36890;&#24120;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#30697;&#65292;&#32780;&#23454;&#38469;&#20013;&#20351;&#29992;&#30340;&#22810;&#37325;&#38598;&#21512;&#20989;&#25968;&#20381;&#36182;&#20110;&#23578;&#26410;&#30740;&#31350;&#36807;&#22312;&#22810;&#37325;&#38598;&#21512;&#19978;&#30340;&#21487;&#36870;&#31070;&#32463;&#30697;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#31070;&#32463;&#32593;&#32476;&#30340;&#30697;&#30830;&#23454;&#23450;&#20041;&#20102;&#21487;&#36870;&#22810;&#37325;&#38598;&#21512;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#20351;&#29992;&#20102;&#19968;&#20010;&#35299;&#26512;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25152;&#38656;&#30340;&#30697;&#25968;&#37327;&#22522;&#26412;&#19978;&#26159;&#26368;&#20248;&#30340;&#65292;&#26368;&#22810;&#30456;&#24046;&#19968;&#20010;&#20056;&#27861;&#22240;&#23376;&#20026;&#20108;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#29420;&#31435;&#24341;&#20154;&#27880;&#30446;&#30340;&#8220;&#26377;&#38480;&#35777;&#25454;&#23450;&#29702;&#8221;&#12290;&#20316;&#20026;&#25105;&#20204;&#20027;&#35201;&#23450;&#29702;&#30340;&#25512;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20851;&#20110;&#22810;&#37325;&#38598;&#21512;&#21644;&#24230;&#37327;&#20989;&#25968;&#30340;&#26032;&#36817;&#20284;&#32467;&#26524;&#65292;&#24182;&#24471;&#21040;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#30340;&#20998;&#31163;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Injective multiset functions have a key role in the theoretical study of machine learning on multisets and graphs. Yet, there remains a gap between the provably injective multiset functions considered in theory, which typically rely on polynomial moments, and the multiset functions used in practice, which rely on $\textit{neural moments}$ $\unicode{x2014}$ whose injectivity on multisets has not been studied to date.  In this paper, we bridge this gap by showing that moments of neural networks do define injective multiset functions, provided that an analytic non-polynomial activation is used. The number of moments required by our theory is optimal essentially up to a multiplicative factor of two. To prove this result, we state and prove a $\textit{finite witness theorem}$, which is of independent interest.  As a corollary to our main theorem, we derive new approximation results for functions on multisets and measures, and new separation results for graph neural networks. We also provide
&lt;/p&gt;</description></item><item><title>ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.06446</link><description>&lt;p&gt;
ShiftAddViT&#65306;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#28151;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06446
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#32479;&#19968;&#39592;&#24178;&#12290;&#20294;&#26159;&#65292;ViTs&#20013;&#30340;&#27880;&#24847;&#21147;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30001;&#20110;&#23494;&#38598;&#30340;&#20056;&#27861;&#32780;&#19981;&#22815;&#39640;&#25928;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#20197;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#65288;&#20363;&#22914;&#20301;&#31227;&#21644;&#21152;&#27861;&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#31867;&#22411;&#30340;&#20943;&#23569;&#20056;&#27861;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ShiftAddViT&#65292;&#26088;&#22312;&#23454;&#29616;GPU&#19978;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26144;&#23556;&#20026;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#20108;&#36827;&#21046;&#30721;&#20043;&#21518;&#65292;&#37319;&#29992;&#21152;&#27861;&#26680;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20043;&#38388;&#30340;MatMul&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#21097;&#20313;&#30340;MLPs&#25110;&#32447;&#24615;&#23618;&#21017;&#37319;&#29992;&#20301;&#31227;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;TVM&#22312;GPU&#19978;&#23454;&#26045;&#24182;&#20248;&#21270;&#36825;&#20123;&#23450;&#21046;&#26680;&#65292;&#20197;&#23454;&#29616;&#23454;&#38469;&#30828;&#20214;&#37096;&#32626;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#22534;&#21472;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06253</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#28789;&#27963;&#30340;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#22534;&#21472;
&lt;/p&gt;
&lt;p&gt;
Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06253
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#22534;&#21472;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#29702;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#20960;&#20010;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25351;&#23450;&#22797;&#26434;&#30446;&#26631;&#12289;&#35268;&#21010;&#26410;&#26469;&#35266;&#23519;&#21644;&#34892;&#21160;&#65292;&#20197;&#21450;&#25209;&#35780;&#20854;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33021;&#21147;&#30340;&#32508;&#21512;&#38598;&#25104;&#22312;&#20445;&#25345;&#26368;&#22823;&#34920;&#36798;&#33021;&#21147;&#30340;&#21516;&#26102;&#20801;&#35768;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#36825;&#26500;&#25104;&#20102;&#31454;&#20105;&#24615;&#30340;&#31639;&#27861;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#22534;&#21472;&#65288;Decision Stacks&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#29420;&#31435;&#30340;&#29983;&#25104;&#27169;&#22411;&#27169;&#25311;&#20102;&#35266;&#27979;&#12289;&#22870;&#21169;&#21644;&#34892;&#21160;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20445;&#35777;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#22534;&#21472;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#33529;&#26524;&#21697;&#23581;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#20197;&#23454;&#29616;&#25112;&#30053;&#36951;&#25022;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#25112;&#30053;&#36951;&#25022;&#36817;&#20284;&#20110;&#26368;&#20339;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06250</link><description>&lt;p&gt;
&#25112;&#30053;&#24615;&#33529;&#26524;&#21697;&#23581;&#65306;&#24102;&#26377;&#19968;&#38754;&#24615;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Strategic Apple Tasting. (arXiv:2306.06250v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#33529;&#26524;&#21697;&#23581;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#20197;&#23454;&#29616;&#25112;&#30053;&#36951;&#25022;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#25112;&#30053;&#36951;&#25022;&#36817;&#20284;&#20110;&#26368;&#20339;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#65292;&#31639;&#27861;&#20915;&#31574;&#24448;&#24448;&#28041;&#21450;&#23558;&#20915;&#31574;&#20998;&#37197;&#32473;&#20855;&#26377;&#31574;&#30053;&#24615;&#20462;&#25913;&#20854;&#31639;&#27861;&#36755;&#20837;&#21160;&#26426;&#30340;&#20195;&#29702;&#12290;&#38500;&#20102;&#24212;&#23545;&#28608;&#21169;&#22240;&#32032;&#22806;&#65292;&#22312;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#36151;&#27454;&#21644;&#25307;&#32856;&#65289;&#20013;&#65292;&#20915;&#31574;&#32773;&#21482;&#35266;&#23519;&#21040;&#22312;&#20998;&#37197;&#31215;&#26497;&#20915;&#31574;&#32473;&#20195;&#29702;&#26102;&#30340;&#22238;&#39304;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#21453;&#39304;&#31216;&#20026;&#33529;&#26524;&#21697;&#23581;&#65288;&#25110;&#21333;&#21521;&#21453;&#39304;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#24773;&#22659;&#24418;&#24335;&#21270;&#20026;&#24102;&#26377;&#33529;&#26524;&#21697;&#23581;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#36127;&#36131;&#20154;&#20915;&#31574;&#19968;&#31995;&#21015; $T$ &#20010;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#30001;&#21487;&#34987;&#31574;&#30053;&#24615;&#20462;&#25913;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#20195;&#29702;&#25581;&#31034;&#20854;&#19978;&#19979;&#25991;&#26102;&#33719;&#24471;&#20122;&#32447;&#24615;&#30340;&#25112;&#30053;&#36951;&#25022;&#65292;&#21363;&#22914;&#26524;&#20195;&#29702;&#22312;&#25581;&#31034;&#20854;&#19978;&#19979;&#25991;&#26102;&#26159;&#30495;&#23454;&#30340;&#65292;&#21017;&#23558;&#36127;&#36131;&#20154;&#30340;&#34920;&#29616;&#19982;&#21518;&#35265;&#20043;&#26126;&#30340;&#26368;&#20339;&#22266;&#23450;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20135;&#29983; $\tilde{\mathcal{O}}(\sqrt{T})$ &#30340;&#25112;&#30053;&#36951;&#25022;&#65292;&#19982;&#26356;&#19968;&#33324;&#31867;&#22411;&#30340;&#21453;&#39304;&#25152;&#30693;&#30340;&#26368;&#20339;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic decision-making in high-stakes domains often involves assigning decisions to agents with incentives to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as apple tasting (or one-sided) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a principal makes decisions about a sequence of $T$ agents, each of which is represented by a context that may be strategically modified. Our goal is to achieve sublinear strategic regret, which compares the performance of the principal to that of the best fixed policy in hindsight, if the agents were truthful when revealing their contexts. Our main result is a learning algorithm which incurs $\tilde{\mathcal{O}}(\sqrt{T})$ strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FLSL&#26041;&#27861;&#65292;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#31867;&#31751;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.06203</link><description>&lt;p&gt;
&#29305;&#24449;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;FLSL
&lt;/p&gt;
&lt;p&gt;
FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FLSL&#26041;&#27861;&#65292;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#31867;&#31751;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;SimCLR&#12289;DINO&#12289;VICReg&#12289;MOCOv3&#65289;&#20027;&#35201;&#38024;&#23545;&#23454;&#20363;&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#19981;&#36866;&#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;Vision Transformers&#65288;ViT&#65289;&#30340;&#22522;&#30784;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#36807;&#31243;&#33021;&#22815;&#33391;&#22909;&#22320;&#19982;&#33258;&#28982;&#22270;&#20687;&#35821;&#20041;&#65288;&#20363;&#22914;&#29289;&#20307;&#21644;&#22330;&#26223;&#65289;&#23545;&#40784;&#12290;&#36890;&#36807;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#32423;&#29305;&#24449;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;FLSL&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FLSL&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#20174;&#22343;&#20540;&#28418;&#31227;&#21644;k-means&#30340;&#35282;&#24230;&#26500;&#24314;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLSL&#20419;&#36827;&#20102;&#26174;&#33879;&#30340;&#35821;&#20041;&#31867;&#31751;&#34920;&#31034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#30340;&#23884;&#20837;&#26041;&#26696;&#12290;FLSL&#30340;&#36816;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements 
&lt;/p&gt;</description></item><item><title>PoET&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#20309;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#19968;&#31995;&#21015;&#30456;&#20851;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#21487;&#20197;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#20309;&#20462;&#25913;</title><link>http://arxiv.org/abs/2306.06156</link><description>&lt;p&gt;
PoET: &#19968;&#31181;&#23558;&#34507;&#30333;&#36136;&#23478;&#26063;&#30475;&#20316;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PoET: A generative model of protein families as sequences-of-sequences. (arXiv:2306.06156v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06156
&lt;/p&gt;
&lt;p&gt;
PoET&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#20309;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#19968;&#31995;&#21015;&#30456;&#20851;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#21487;&#20197;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#20309;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#26159;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#21151;&#33021;&#30340;&#26032;&#34507;&#30333;&#36136;&#30340;&#33258;&#28982;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#35201;&#20040;&#38590;&#20197;&#25351;&#23548;&#20854;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#30340;&#34507;&#30333;&#36136;&#65292;&#35201;&#20040;&#24517;&#39035;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#22823;&#22411;&#22810;&#37325;&#24207;&#21015;&#27604;&#23545;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#20174;&#23478;&#26063;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34507;&#30333;&#36136;&#36827;&#21270;&#21464;&#25442;&#22120;&#65288;PoET&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20840;&#34507;&#30333;&#36136;&#23478;&#26063;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;&#22312;&#25968;&#21315;&#19975;&#20010;&#22825;&#28982;&#34507;&#30333;&#36136;&#24207;&#21015;&#31751;&#20043;&#38388;&#29983;&#25104;&#19968;&#31995;&#21015;&#30456;&#20851;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;PoET&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#34507;&#30333;&#36136;&#23478;&#26063;&#26465;&#20214;&#19979;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#24847;&#20462;&#25913;&#65292;&#32780;&#19988;&#21487;&#20197;&#20174;&#30701;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#22806;&#25512;&#65292;&#22312;&#23567;&#23478;&#26063;&#20013;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#36825;&#26159;&#36890;&#36807;&#29420;&#29305;&#30340;Transformer&#23618;&#23454;&#29616;&#30340;&#65307;&#25105;&#20204;&#27169;&#25311;&#20102;&#20196;&#29260;s
&lt;/p&gt;
&lt;p&gt;
Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\textbf{P}$r$\textbf{o}$tein $\textbf{E}$volutionary $\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#28085;&#30422;&#26680;&#24179;&#28369;&#30340;&#24378;&#24230;&#20989;&#25968;&#20272;&#35745;&#12289;&#26368;&#23567;&#21270;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#23398;&#20064;&#21644;&#24402;&#32435;&#26500;&#36896;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#32467;&#26500;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06155</link><description>&lt;p&gt;
&#24378;&#24230;&#36718;&#24275;&#25237;&#24433;&#65306;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#30340;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks. (arXiv:2306.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#28085;&#30422;&#26680;&#24179;&#28369;&#30340;&#24378;&#24230;&#20989;&#25968;&#20272;&#35745;&#12289;&#26368;&#23567;&#21270;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#23398;&#20064;&#21644;&#24402;&#32435;&#26500;&#36896;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#32467;&#26500;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24378;&#24230;&#36718;&#24275;&#25237;&#24433;&#8221;&#30340;&#26032;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#33410;&#28857;&#30340;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#65292;&#35813;&#21160;&#24577;&#32593;&#32476;&#30001;&#33410;&#28857;&#38598;&#21644;&#22312;&#36830;&#32493;&#26102;&#38388;&#20869;&#21457;&#29983;&#30340;&#30636;&#26102;&#20132;&#20114;&#20107;&#20214;&#30340;&#38598;&#21512;&#25152;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#36890;&#36807;&#26680;&#24179;&#28369;&#31561;&#26041;&#27861;&#20272;&#35745;&#33410;&#28857;&#23545;&#20043;&#38388;&#20132;&#20114;&#30340;&#24378;&#24230;&#20989;&#25968;&#65307;&#23398;&#20064;&#19968;&#20010;&#26368;&#23567;&#21270;&#26576;&#31181;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#65307;&#36890;&#36807;&#23398;&#20064;&#30340;&#25237;&#24433;&#24402;&#32435;&#26500;&#36896;&#20986;&#19981;&#26029;&#21457;&#23637;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#24182;&#20855;&#26377;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#28857;&#34920;&#31034;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#28857;&#19978;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#26500;&#24314;&#20102;&#20272;&#35745;&#29702;&#35770;&#26469;&#38416;&#26126;&#24179;&#28369;&#20316;&#20026;&#20559;&#24046;&#26041;&#24046;&#25240;&#34935;&#30340;&#20316;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#38543;&#30528;&#20449;&#22122;&#27604;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#24179;&#28369;&#31243;&#24230;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithmic framework, Intensity Profile Projection, for learning continuous-time representations of the nodes of a dynamic network, characterised by a node set and a collection of instantaneous interaction events which occur in continuous time. Our framework consists of three stages: estimating the intensity functions underlying the interactions between pairs of nodes, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and inductively constructing evolving node representations via the learned projection. We show that our representations preserve the underlying structure of the network, and are temporally coherent, meaning that node representations can be meaningfully compared at different points in time. We develop estimation theory which elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce smoothing as the signal-to-noise ratio increases on account of the algorithm `borrow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06101</link><description>&lt;p&gt;
Prodigy: &#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#38646;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#20013;&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20004;&#31181;&#25216;&#26415;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#35777;&#26126;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20197;&#20415;&#26368;&#20248;&#35774;&#32622;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#22522;&#20110;&#23398;&#20064;&#29575;&#33258;&#30001;&#30340;D-Adaptation&#26041;&#27861;&#30340;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;$O(\sqrt{\log(D/d_0)})$&#30340;&#22240;&#23376;&#25552;&#39640;&#20102;D-Adaptation&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$d_0$&#26159;$D$&#30340;&#21021;&#22987;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#24120;&#35265;&#30340;&#36923;&#36753;&#22238;&#24402;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;VGG11&#21644;ResNet-50&#12289;&#22312;Imagenet&#19978;&#35757;&#32451;&#30340;ViT&#12289;&#22312;IWSLT14&#19978;&#35757;&#32451;&#30340;LSTM&#12289;&#22312;Criteo&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DLRM&#12289;&#22312;Knee MRI&#25968;&#25454;&#38598;&#19978;&#30340;VarNet&#65292;&#20197;&#21450;&#22312;BookWiki&#19978;&#35757;&#32451;&#30340;RoBERTa&#21644;GPT transformer&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;D-Adaptation&#65292;&#24182;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#29992;&#20110;&#32570;&#20047;&#22240;&#26524;&#27169;&#22411;&#21644;&#30452;&#25509;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#24182;&#33021;&#25552;&#20379;&#20272;&#35745;&#32467;&#26524;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.05751</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#25512;&#36827;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Advancing Counterfactual Inference through Quantile Regression. (arXiv:2306.05751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#29992;&#20110;&#32570;&#20047;&#22240;&#26524;&#27169;&#22411;&#21644;&#30452;&#25509;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#24182;&#33021;&#25552;&#20379;&#20272;&#35745;&#32467;&#26524;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#21453;&#20107;&#23454;&#8220;&#20551;&#35774;&#8221;&#38382;&#39064;&#30340;&#33021;&#21147;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#22240;&#26524;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#36890;&#24120;&#20551;&#23450;&#23384;&#22312;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#26679;&#30340;&#22240;&#26524;&#27169;&#22411;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#29978;&#33267;&#19981;&#21487;&#36776;&#35782;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#22522;&#20110;&#65288;&#23398;&#20064;&#21040;&#30340;&#65289;&#23450;&#24615;&#22240;&#26524;&#32467;&#26500;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#32473;&#23450;&#22240;&#26524;&#27169;&#22411;&#29978;&#33267;&#19981;&#38656;&#35201;&#30452;&#25509;&#20272;&#35745;&#26465;&#20214;&#20998;&#24067;&#65292;&#23601;&#33021;&#36827;&#34892;&#21487;&#38752;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#21453;&#20107;&#23454;&#25512;&#29702;&#37325;&#26032;&#36716;&#21270;&#20026;&#19968;&#20010;&#25193;&#23637;&#20998;&#20301;&#25968;&#22238;&#24402;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#20351;&#24471;&#20272;&#35745;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#23545;&#26410;&#35265;&#25968;&#25454;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capacity to address counterfactual "what if" inquiries is crucial for understanding and making use of causal influences. Traditional counterfactual inference usually assumes a structural causal model is available. However, in practice, such a causal model is often unknown and may not be identifiable. This paper aims to perform reliable counterfactual inference based on the (learned) qualitative causal structure and observational data, without a given causal model or even directly estimating conditional distributions. We re-cast counterfactual reasoning as an extended quantile regression problem using neural networks. The approach is statistically more efficient than existing ones, and further makes it possible to develop the generalization ability of the estimated counterfactual outcome to unseen data and provide an upper bound on the generalization error. Experiment results on multiple datasets strongly support our theoretical claims.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#30340;&#26041;&#27861;&#65292;&#25968;&#25454;&#26174;&#31034;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05587</link><description>&lt;p&gt;
MC-NN&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types. (arXiv:2306.05587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#30340;&#26041;&#27861;&#65292;&#25968;&#25454;&#26174;&#31034;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#23545;&#20844;&#20849;&#21355;&#29983;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#23545;&#32769;&#24180;&#20154;&#12289;&#20799;&#31461;&#21644;&#24739;&#26377;&#28508;&#22312;&#30142;&#30149;&#30340;&#20154;&#26469;&#35828;&#26356;&#20026;&#20005;&#37325;&#12290;&#20005;&#37325;&#30149;&#20917;&#30340;&#21457;&#29983;&#65292;&#22914;&#32954;&#28814;&#65292;&#20984;&#26174;&#20102;&#39044;&#38450;&#27969;&#24863;&#20256;&#25773;&#30340;&#37325;&#35201;&#24615;&#12290;&#20934;&#30830;&#32780;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#23545;&#20110;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#34880;&#20957;&#32032;&#21644;&#31070;&#32463;&#27688;&#37240;&#37238;&#34507;&#30333;&#24207;&#21015;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#23436;&#25972;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#21508;&#31181;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#24207;&#21015;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26469;&#33258;&#23436;&#25972;&#21644;&#37096;&#20998;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#20855;&#26377;&#28508;&#21147;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenza poses a significant threat to public health, particularly among the elderly, young children, and people with underlying dis-eases. The manifestation of severe conditions, such as pneumonia, highlights the importance of preventing the spread of influenza. An accurate and cost-effective prediction of the host and antigenic sub-types of influenza A viruses is essential to addressing this issue, particularly in resource-constrained regions. In this study, we propose a multi-channel neural network model to predict the host and antigenic subtypes of influenza A viruses from hemagglutinin and neuraminidase protein sequences. Our model was trained on a comprehensive data set of complete protein sequences and evaluated on various test data sets of complete and incomplete sequences. The results demonstrate the potential and practicality of using multi-channel neural networks in predicting the host and antigenic subtypes of influenza A viruses from both full and partial protein sequence
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.05304</link><description>&lt;p&gt;
&#22270;&#19978;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimisation of Functions on Graphs. (arXiv:2306.05304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#19981;&#26029;&#28044;&#29616;&#25512;&#21160;&#20102;&#22312;&#22270;&#33410;&#28857;&#38598;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#22270;&#25628;&#32034;&#31639;&#27861;&#21487;&#29992;&#20110;&#27492;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#20851;&#20110;&#20989;&#25968;&#20540;&#30340;&#20449;&#24687;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31867;&#26377;&#21069;&#36884;&#30340;&#40657;&#30418;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#23427;&#24456;&#23569;&#34987;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#26032;&#39062;&#35774;&#32622;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20248;&#21270;&#22312;&#36890;&#29992;&#65292;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#30340;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#30340;&#20248;&#28857;&#12290;&#23616;&#37096;&#24314;&#27169;&#26041;&#27861;&#36827;&#19968;&#27493;&#20445;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has been scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05268</link><description>&lt;p&gt;
&#20998;&#35299;&#23545;&#27604;&#23398;&#20064;&#65306;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#29305;&#21035;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#21482;&#38656;&#37197;&#23545;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#26631;&#39064;&#25110;&#35270;&#39057;&#38899;&#39057;&#23545;&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#30784;&#26159;&#22810;&#35270;&#35282;&#20887;&#20313;&#30340;&#20551;&#35774;&#8212;&#8212;&#36328;&#27169;&#24577;&#38388;&#20849;&#20139;&#20449;&#24687;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#24517;&#35201;&#19988;&#36275;&#22815;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20063;&#21253;&#21547;&#22312;&#36328;&#27169;&#24577;&#21807;&#19968;&#21306;&#22495;&#20013;&#65306;&#19968;&#31181;&#20165;&#23384;&#22312;&#20110;&#19968;&#20010;&#27169;&#24577;&#20013;&#20294;&#19982;&#20219;&#21153;&#20173;&#28982;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22914;&#20309;&#23398;&#20064;&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#25429;&#33719;&#19982;&#19979;&#28216;&#20219;&#21153;&#30456;&#20851;&#30340;&#20849;&#20139;&#21644;&#21807;&#19968;&#20449;&#24687;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;FactorCL&#65292;&#20197;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;&#12290;FactorCL&#30340;&#22522;&#30784;&#26159;&#19977;&#20010;&#26032;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#23558;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20998;&#35299;&#20026;&#20849;&#20139;&#21644;&#21807;&#19968;&#34920;&#31034;&#65292;&#65288;2&#65289;&#38480;&#21046;&#20849;&#20139;&#21644;&#21807;&#19968;&#25104;&#20998;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#65288;3&#65289;&#20351;&#29992;&#22240;&#23376;&#27491;&#21017;&#21270;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#65292;&#22312;&#23384;&#22312;&#34394;&#20551;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#32452;&#30340;&#19981;&#24179;&#34913;&#21644;&#26131;&#20110;&#23398;&#20064;&#30340;&#34394;&#20551;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;PDE&#65292;&#23427;&#36880;&#27493;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#20854;&#26368;&#21155;&#32452;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#36234;&#20102;R&#27169;&#22411;&#31561;&#20854;&#23427;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04949</link><description>&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#25193;&#23637;&#25968;&#25454;&#23545;&#25239;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning with Progressive Data Expansion Against Spurious Correlation. (arXiv:2306.04949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#65292;&#22312;&#23384;&#22312;&#34394;&#20551;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#32452;&#30340;&#19981;&#24179;&#34913;&#21644;&#26131;&#20110;&#23398;&#20064;&#30340;&#34394;&#20551;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;PDE&#65292;&#23427;&#36880;&#27493;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#20854;&#26368;&#21155;&#32452;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#36234;&#20102;R&#27169;&#22411;&#31561;&#20854;&#23427;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#26131;&#20110;&#23398;&#20064;&#19982;&#30495;&#23454;&#26631;&#31614;&#26080;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#24050;&#26377;&#32447;&#24615;&#27169;&#22411;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#29702;&#35770;&#19978;&#26816;&#26597;&#20102;&#23384;&#22312;&#34394;&#20551;&#29305;&#24449;&#26102;&#20004;&#23618;&#38750;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25968;&#25454;&#32452;&#19981;&#24179;&#34913;&#21644;&#26131;&#20110;&#23398;&#20064;&#30340;&#34394;&#20551;&#29305;&#24449;&#21487;&#33021;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23548;&#33268;&#34394;&#20551;&#29305;&#24449;&#30340;&#25903;&#37197;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDE&#30340;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26368;&#24046;&#32452;&#24615;&#33021;&#12290;PDE&#20174;&#19968;&#32452;&#24179;&#34913;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#24320;&#22987;&#65292;&#24182;&#36880;&#27493;&#25193;&#23637;&#20854;&#22823;&#23567;&#20197;&#20419;&#36827;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#65288;&#20363;&#22914;R&#65289;&#19978;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have shown remarkable performance in various tasks, they are susceptible to learning non-generalizable spurious features rather than the core features that are genuinely correlated to the true label. In this paper, beyond existing analyses of linear models, we theoretically examine the learning process of a two-layer nonlinear convolutional neural network in the presence of spurious features. Our analysis suggests that imbalanced data groups and easily learnable spurious features can lead to the dominance of spurious features during the learning process. In light of this, we propose a new training algorithm called PDE that efficiently enhances the model's robustness for a better worst-group performance. PDE begins with a group-balanced subset of training data and progressively expands it to facilitate the learning of the core features. Experiments on synthetic and real-world benchmark datasets confirm the superior performance of our method on models such as R
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#31934;&#30830;&#26368;&#20248;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#26059;&#36716;&#23545;&#31216;&#30340;&#20849;&#20139;&#38543;&#26426;&#30721;&#20070;&#65292;&#24182;&#36890;&#36807;$k$-closest&#32534;&#30721;&#23454;&#29616;&#20102;&#38543;&#26426;&#26059;&#36716;&#30340;&#21333;&#32431;&#24418;$c$&#30340;&#31934;&#30830;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2306.04924</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22343;&#20540;&#20272;&#35745;&#20013;&#30340;&#36890;&#20449;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#30340;&#31934;&#30830;&#26368;&#20248;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation. (arXiv:2306.04924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#31934;&#30830;&#26368;&#20248;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#26059;&#36716;&#23545;&#31216;&#30340;&#20849;&#20139;&#38543;&#26426;&#30721;&#20070;&#65292;&#24182;&#36890;&#36807;$k$-closest&#32534;&#30721;&#23454;&#29616;&#20102;&#38543;&#26426;&#26059;&#36716;&#30340;&#21333;&#32431;&#24418;$c$&#30340;&#31934;&#30830;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#30456;&#21516;&#38382;&#39064;&#30340;\emph{&#38454;}-&#26368;&#20248;&#31639;&#27861;&#65288;&#21363;&#24403;&#25105;&#20204;&#33457;&#36153;&#26356;&#22810;&#27604;&#29305;&#26102;&#28176;&#36827;&#26368;&#20248;&#65289;&#65292;&#20294;&#22312;&#38750;&#28176;&#36827;&#35774;&#32622;&#19979;&#20173;&#28982;&#27809;&#26377;&#23454;&#29616;\emph{&#31934;&#30830;}&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#25551;&#36848;&#20102;&#22312;&#20849;&#20139;&#38543;&#26426;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;\emph{&#31934;&#30830;}-&#26368;&#20248;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#20960;&#20010;\emph{&#31934;&#30830;}&#26368;&#20248;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#20013;&#19968;&#20010;&#24517;&#35201;&#26465;&#20214;&#26159;&#21033;&#29992;&#26059;&#36716;&#23545;&#31216;&#30340;&#20849;&#20139;&#38543;&#26426;&#30721;&#20070;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#21270;&#26426;&#21046;&#65292;&#20854;&#20013;&#30721;&#20070;&#26159;&#38543;&#26426;&#26059;&#36716;&#30340;&#21333;&#32431;&#24418;&#8212;&#8212;&#28385;&#36275;\emph{&#31934;&#30830;}-&#26368;&#20248;&#30721;&#20070;&#30340;&#24517;&#35201;&#23646;&#24615;&#12290;&#35813;&#26426;&#21046;&#22522;&#20110;&#25105;&#20204;&#35777;&#26126;&#30340;$k$&#26368;&#36817;&#32534;&#30721;&#65292;&#23545;&#20110;&#38543;&#26426;&#26059;&#36716;&#30340;&#21333;&#32431;&#24418;$c$&#26469;&#35828;&#26159;\emph{&#31934;&#30830;}-&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the mean estimation problem under communication and local differential privacy constraints. While previous work has proposed \emph{order}-optimal algorithms for the same problem (i.e., asymptotically optimal as we spend more bits), \emph{exact} optimality (in the non-asymptotic setting) still has not been achieved. In this work, we take a step towards characterizing the \emph{exact}-optimal approach in the presence of shared randomness (a random variable shared between the server and the user) and identify several necessary conditions for \emph{exact} optimality. We prove that one of the necessary conditions is to utilize a rotationally symmetric shared random codebook. Based on this, we propose a randomization mechanism where the codebook is a randomly rotated simplex -- satisfying the necessary properties of the \emph{exact}-optimal codebook. The proposed mechanism is based on a $k$-closest encoding which we prove to be \emph{exact}-optimal for the randomly rotated simplex c
&lt;/p&gt;</description></item><item><title>SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04251</link><description>&lt;p&gt;
&#38543;&#26426;&#22349;&#32553;&#65306;&#22914;&#20309;&#21033;&#29992;&#26799;&#24230;&#22122;&#22768;&#20351;SGD&#21160;&#24577;&#36235;&#21521;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04251
&lt;/p&gt;
&lt;p&gt;
SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#19968;&#20010;&#24378;&#28872;&#38544;&#24335;&#20559;&#22909;&#65292;&#23427;&#23558;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#39537;&#21160;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#29420;&#31435;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#20559;&#22909;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#19981;&#21464;&#38598;&#65292;&#25110;&#32773;&#35828;&#26159;SGD&#26410;&#20462;&#25913;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31867;&#19981;&#21464;&#38598;&#65292;&#23427;&#20204;&#23545;&#24212;&#20110;&#29616;&#20195;&#26550;&#26500;&#20013;&#24120;&#35265;&#30340;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SGD&#22312;&#36825;&#20123;&#31616;&#21333;&#19981;&#21464;&#38598;&#26041;&#38754;&#20855;&#26377;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#25439;&#22833;&#26223;&#35266;&#22312;&#19981;&#21464;&#38598;&#21608;&#22260;&#30340;&#26354;&#29575;&#21644;&#38543;&#26426;&#26799;&#24230;&#24341;&#20837;&#30340;&#22122;&#22768;&#20043;&#38388;&#30340;&#31454;&#20105;&#24314;&#31435;&#20102;&#19968;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#22122;&#22768;&#27700;&#24179;&#20250;&#22686;&#24378;&#21560;&#24341;&#21147;&#65292;&#23548;&#33268;&#19982;&#38797;&#28857;&#25110;&#35757;&#32451;&#25439;&#22833;&#30340;&#23616;&#37096;&#26497;&#22823;&#20540;&#30456;&#20851;&#30340;&#21560;&#24341;&#19981;&#21464;&#38598;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.03819</link><description>&lt;p&gt;
LEACE&#65306;&#38381;&#21512;&#24418;&#24335;&#20013;&#30340;&#23436;&#32654;&#32447;&#24615;&#27010;&#24565;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#25830;&#38500;&#26088;&#22312;&#20174;&#34920;&#24449;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#29305;&#24449;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#65288;&#20363;&#22914;&#65292;&#38450;&#27490;&#20998;&#31867;&#22120;&#20351;&#29992;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#21644;&#21487;&#35299;&#37322;&#24615;&#65288;&#20363;&#22914;&#65292;&#21024;&#38500;&#27010;&#24565;&#20197;&#35266;&#23519;&#27169;&#22411;&#34892;&#20026;&#30340;&#21464;&#21270;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LEAst-squares&#27010;&#24565;&#25830;&#38500;&#65288;LEACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#22914;&#24191;&#27867;&#31867;&#21035;&#30340;&#33539;&#25968;&#25152;&#27979;&#37327;&#30340;&#37027;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21517;&#20026;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#30340;&#26032;&#26041;&#27861;&#23558;LEACE&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25830;&#38500;&#27599;&#20010;&#23618;&#20013;&#30340;&#30446;&#26631;&#27010;&#24565;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#20449;&#24687;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#21450;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/EleutherAI/concept-erasure&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.
&lt;/p&gt;</description></item><item><title>FAMO&#26159;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26041;&#24335;&#23454;&#29616;&#24179;&#34913;&#30340;&#20219;&#21153;&#25439;&#22833;&#20943;&#23569;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26799;&#24230;&#25805;&#20316;&#25216;&#26415;&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03792</link><description>&lt;p&gt;
FAMO&#65306;&#24555;&#36895;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FAMO: Fast Adaptive Multitask Optimization. (arXiv:2306.03792v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03792
&lt;/p&gt;
&lt;p&gt;
FAMO&#26159;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26041;&#24335;&#23454;&#29616;&#24179;&#34913;&#30340;&#20219;&#21153;&#25439;&#22833;&#20943;&#23569;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26799;&#24230;&#25805;&#20316;&#25216;&#26415;&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#38271;&#20037;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#22815;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20174;&#22810;&#26679;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#22810;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#25152;&#26377;&#20219;&#21153;&#30340;&#24179;&#22343;&#25439;&#22833;&#24212;&#29992;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#20250;&#30001;&#20110;&#26576;&#20123;&#20219;&#21153;&#30340;&#20005;&#37325;&#27424;&#20248;&#21270;&#32780;&#23548;&#33268;&#36739;&#24046;&#30340;&#22810;&#20219;&#21153;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#33719;&#24471;&#26356;&#24179;&#34913;&#30340;&#25439;&#22833;&#20943;&#23569;&#65292;&#20294;&#38656;&#35201;&#23384;&#20648;&#21644;&#35745;&#31639;&#25152;&#26377;&#20219;&#21153;&#30340;&#26799;&#24230;&#65288;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20026;O(k)&#65292;&#20854;&#20013;k&#26159;&#20219;&#21153;&#25968;&#37327;&#65289;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24555;&#36895;&#33258;&#36866;&#24212;&#22810;&#20219;&#21153;&#20248;&#21270;&#65288;FAMO&#65289;&#65292;&#19968;&#31181;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#65292;&#20197;O(1)&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20197;&#24179;&#34913;&#30340;&#26041;&#24335;&#20943;&#23569;&#20219;&#21153;&#25439;&#22833;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#22810;&#20219;&#21153;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FAMO&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26799;&#24230;&#25805;&#20316;&#25216;&#26415;&#30456;&#23218;&#32654;&#25110;&#26356;&#20248;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients ($\mathcal{O}(k)$ space and time where $k$ is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization FAMO, a dynamic weighting method that decreases task losses in a balanced way using $\mathcal{O}(1)$ space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation technique
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03552</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#21160;&#24577;&#20559;&#31227;&#30340;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#21463;&#21040;&#21160;&#24577;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#30340;&#29615;&#22659;&#21160;&#24577;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#29615;&#22659;&#21442;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26681;&#25454;&#20854;&#29615;&#22659;&#21442;&#25968;&#23558;&#24102;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20998;&#24320;&#20197;&#35757;&#32451;&#30456;&#24212;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#8220;&#29305;&#23450;&#22330;&#26223;&#8221;&#20351;&#29992;&#30340;&#65292;&#38024;&#23545;&#26576;&#20010;&#29615;&#22659;&#35757;&#32451;&#30340;&#31574;&#30053;&#19981;&#33021;&#20174;&#25910;&#38598;&#22312;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#25152;&#26377;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#21644;&#19981;&#21516;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#65292;&#24182;&#20174;&#20855;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#24577;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#36825;&#31181;&#20998;&#24067;&#29992;&#20110;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102; SRPO&#65288;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;&#65289;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SRPO &#22312;&#20855;&#26377;&#21160;&#24577;&#20559;&#31227;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38590;&#27665;&#23433;&#32622;&#20013;&#30340;&#38543;&#26426;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#24314;&#27169;&#31574;&#30053;&#65292;&#26368;&#32456;&#21457;&#29616;&#28151;&#21512;&#26041;&#27861;&#20855;&#26377;&#36739;&#24378;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02948</link><description>&lt;p&gt;
&#38590;&#27665;&#23433;&#32622;&#20013;&#30340;&#38543;&#26426;&#20998;&#24067;&#36716;&#31227;: &#24314;&#31435;&#20581;&#22766;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Distribution Shift in Refugee Placement: Strategies for Building Robust Models. (arXiv:2306.02948v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38590;&#27665;&#23433;&#32622;&#20013;&#30340;&#38543;&#26426;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#24314;&#27169;&#31574;&#30053;&#65292;&#26368;&#32456;&#21457;&#29616;&#28151;&#21512;&#26041;&#27861;&#20855;&#26377;&#36739;&#24378;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31639;&#27861;&#20998;&#37197;&#38590;&#27665;&#21644;&#23547;&#27714;&#24199;&#25252;&#32773;&#21040;&#20027;&#26426;&#22269;&#23478;&#30340;&#22320;&#28857;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#22312;&#32654;&#22269;&#21644;&#29790;&#22763;&#23454;&#26045;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#36807;&#21435;&#25269;&#36798;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#29992;&#20110;&#21305;&#37197;&#23478;&#24237;&#21040;&#20301;&#32622;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#19982;&#20998;&#37197;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#65289;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#25919;&#31574;&#30456;&#20851;&#30340;&#25972;&#21512;&#32467;&#26524;&#65292;&#22914;&#22312;&#19968;&#23450;&#26102;&#38388;&#21518;&#30340;&#23601;&#19994;&#29366;&#24577;&#12290;&#29616;&#26377;&#30340;&#23454;&#29616;&#21644;&#30740;&#31350;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#25919;&#31574;&#32467;&#26524;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#36825;&#20123;&#39044;&#27979;&#29992;&#20110;&#20998;&#37197;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#65292;&#23578;&#26410;&#34987;&#20808;&#21069;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24314;&#27169;&#31574;&#30053;&#65306;&#19978;&#36848;&#30340;&#26631;&#20934;&#26041;&#27861;&#12289;&#20351;&#29992;&#26356;&#26032;&#25968;&#25454;&#21644;&#20195;&#29702;&#32467;&#26524;&#30340;&#26041;&#27861;&#20197;&#21450;&#28151;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#28151;&#21512;&#26041;&#27861;&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#24369;&#20195;&#29702;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;-
&lt;/p&gt;
&lt;p&gt;
Algorithmic assignment of refugees and asylum seekers to locations within host countries has gained attention in recent years, with implementations in the US and Switzerland. These approaches use data on past arrivals to generate machine learning models that can be used (along with assignment algorithms) to match families to locations, with the goal of maximizing a policy-relevant integration outcome such as employment status after a certain duration. Existing implementations and research train models to predict the policy outcome directly, and use these predictions in the assignment procedure. However, the merits of this approach, particularly in non-stationary settings, has not been previously explored. This study proposes and compares three different modeling strategies: the standard approach described above, an approach that uses newer data and proxy outcomes, and a hybrid approach. We show that the hybrid approach is robust to both distribution shift and weak proxy relationships -
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Early-Exit&#32593;&#32476;&#20013;&#23454;&#29616;&#26465;&#20214;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#27169;&#22411;&#36716;&#21270;&#20026;&#30495;&#27491;&#30340;&#38543;&#26102;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.02652</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21046;&#26465;&#20214;&#21333;&#35843;&#24615;&#22312;Early-Exit&#32467;&#26500;&#20013;&#23454;&#29616;&#38543;&#26102;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity. (arXiv:2306.02652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Early-Exit&#32593;&#32476;&#20013;&#23454;&#29616;&#26465;&#20214;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#27169;&#22411;&#36716;&#21270;&#20026;&#30495;&#27491;&#30340;&#38543;&#26102;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#37096;&#32626;&#22312;&#35745;&#31639;&#39044;&#31639;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#12290;&#38543;&#26102;&#31639;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#36825;&#31181;&#29615;&#22659;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35745;&#31639;&#30340;&#20219;&#20309;&#26102;&#20505;&#37117;&#21487;&#20197;&#36755;&#20986;&#39044;&#27979;&#20540;&#65292;&#20854;&#36136;&#37327;&#26159;&#35745;&#31639;&#26102;&#38388;&#30340;&#20989;&#25968;&#12290;&#30001;&#20110;&#20854;&#33021;&#22815;&#22312;&#32593;&#32476;&#21508;&#20010;&#38454;&#27573;&#25552;&#20379;&#20013;&#38388;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#65292;Early-Exit&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26102;&#35745;&#31639;&#30340;&#32972;&#26223;&#19979;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#21069;&#30340;Early-Exit&#32593;&#32476;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#20219;&#20309;&#26102;&#20505;&#30340;&#35774;&#32622;&#65292;&#22240;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#39044;&#27979;&#36136;&#37327;&#19981;&#33021;&#20445;&#35777;&#38543;&#30528;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#20107;&#21518;&#20462;&#25913;&#65292;&#22522;&#20110;&#19987;&#23478;&#20056;&#31215;&#65292;&#40723;&#21169;Early-Exit&#32593;&#32476;&#36880;&#28176;&#21464;&#24471;&#33258;&#20449;&#12290;&#36825;&#36171;&#20104;&#20102;&#25105;&#20204;&#30340;&#28145;&#24230;&#27169;&#22411;&#26465;&#20214;&#21333;&#35843;&#24615;&#30340;&#29305;&#24615;&#8212;&#8212;&#36825;&#26159;&#23454;&#29616;&#30495;&#27491;&#38543;&#26102;&#20998;&#31867;&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern predictive models are often deployed to environments in which computational budgets are dynamic. Anytime algorithms are well-suited to such environments as, at any point during computation, they can output a prediction whose quality is a function of computation time. Early-exit neural networks have garnered attention in the context of anytime computation due to their capability to provide intermediate predictions at various stages throughout the network. However, we demonstrate that current early-exit networks are not directly applicable to anytime settings, as the quality of predictions for individual data points is not guaranteed to improve with longer computation. To address this shortcoming, we propose an elegant post-hoc modification, based on the Product-of-Experts, that encourages an early-exit network to become gradually confident. This gives our deep models the property of conditional monotonicity in the prediction quality -- an essential stepping stone towards truly an
&lt;/p&gt;</description></item><item><title>LambdaBeam&#26159;&#19968;&#31181;&#31070;&#32463;&#31243;&#24207;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20219;&#24847;lambda&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#21442;&#25968;&#20256;&#36882;&#32473;&#39640;&#38454;&#20989;&#25968;&#26469;&#35299;&#20915;&#20808;&#21069;&#31070;&#32463;&#25628;&#32034;&#21512;&#25104;&#26356;&#38271;&#19988;&#26356;&#36890;&#29992;&#31243;&#24207;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.02049</link><description>&lt;p&gt;
LambdaBeam&#65306;&#20855;&#26377;&#39640;&#38454;&#20989;&#25968;&#21644;Lambda&#30340;&#31070;&#32463;&#31243;&#24207;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas. (arXiv:2306.02049v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02049
&lt;/p&gt;
&lt;p&gt;
LambdaBeam&#26159;&#19968;&#31181;&#31070;&#32463;&#31243;&#24207;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20219;&#24847;lambda&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#21442;&#25968;&#20256;&#36882;&#32473;&#39640;&#38454;&#20989;&#25968;&#26469;&#35299;&#20915;&#20808;&#21069;&#31070;&#32463;&#25628;&#32034;&#21512;&#25104;&#26356;&#38271;&#19988;&#26356;&#36890;&#29992;&#31243;&#24207;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#26159;&#31243;&#24207;&#21512;&#25104;&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#25191;&#34892;&#32467;&#26524;&#37319;&#29992;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#20363;&#22914;&#19987;&#27880;&#20110;&#29305;&#23450;&#25628;&#32034;&#26041;&#21521;&#12290;&#20960;&#39033;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#31070;&#32463;&#27169;&#22411;&#22312;&#24341;&#23548;&#31243;&#24207;&#21512;&#25104;&#25628;&#32034;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#32570;&#28857;&#26159;&#26080;&#27861;&#22788;&#29702;&#36845;&#20195;&#24490;&#29615;&#12289;&#39640;&#38454;&#20989;&#25968;&#25110;lambda&#20989;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20808;&#21069;&#30340;&#31070;&#32463;&#25628;&#32034;&#21512;&#25104;&#26356;&#38271;&#19988;&#26356;&#36890;&#29992;&#30340;&#31243;&#24207;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;LambdaBeam&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#21487;&#20197;&#26500;&#24314;&#22312;&#32473;&#23450;DSL&#20869;&#32452;&#21512;&#25805;&#20316;&#30340;&#20219;&#24847;lambda&#20989;&#25968;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;lambda&#20989;&#25968;&#25191;&#34892;&#34892;&#20026;&#30340;&#35821;&#20041;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#32593;&#32476;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#36873;&#25321;&#35201;&#26500;&#24314;&#30340;lambda&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#21442;&#25968;&#20256;&#36882;&#32473;&#39640;&#38454;&#20989;&#25968;&#25191;&#34892;&#24490;&#29615;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LambdaBeam&#22312;&#31070;&#32463;&#12289;&#31526;&#21495;&#21644;LLM&#26041;&#27861;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search is an important technique in program synthesis that allows for adaptive strategies such as focusing on particular search directions based on execution results. Several prior works have demonstrated that neural models are effective at guiding program synthesis searches. However, a common drawback of those approaches is the inability to handle iterative loops, higher-order functions, or lambda functions, thus limiting prior neural searches from synthesizing longer and more general programs. We address this gap by designing a search algorithm called LambdaBeam that can construct arbitrary lambda functions that compose operations within a given DSL. We create semantic vector representations of the execution behavior of the lambda functions and train a neural policy network to choose which lambdas to construct during search, and pass them as arguments to higher-order functions to perform looping computations. Our experiments show that LambdaBeam outperforms neural, symbolic, and LLM-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#35270;&#39057;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#24418;&#24335;&#30340;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#19968;&#31995;&#21015;&#26377;&#36259;&#21457;&#29616;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.02014</link><description>&lt;p&gt;
&#25581;&#31034;&#35270;&#39057;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#38544;&#34255;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts. (arXiv:2306.02014v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#35270;&#39057;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#24418;&#24335;&#30340;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#19968;&#31995;&#21015;&#26377;&#36259;&#21457;&#29616;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#39057;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;VSSL&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#24418;&#24335;&#30340;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#30830;&#20999;&#34892;&#20026;&#21644;&#21160;&#24577;&#23578;&#26410;&#24471;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#30740;&#31350;&#20102;&#20845;&#31181;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65288;v-SimCLR&#12289;v-MoCo&#12289;v-BYOL&#12289;v-SimSiam&#12289;v-DINO&#12289;v-MAE&#65289;&#23545;&#20110;&#21508;&#31181;&#33258;&#28982;&#20998;&#24067;&#36716;&#31227;&#30340;&#34892;&#20026;&#65292;&#21363;&#65288;i&#65289;&#19978;&#19979;&#25991;&#36716;&#31227;&#12289;&#65288;ii&#65289;&#35270;&#35282;&#36716;&#31227;&#12289;&#65288;iii&#65289;&#34892;&#20026;&#32773;&#36716;&#31227;&#12289;&#65288;iv&#65289;&#26469;&#28304;&#36716;&#31227;&#12289;&#65288;v&#65289;&#23545;&#26410;&#30693;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#65288;&#38646;&#26679;&#26412;&#65289;&#21644;&#65288;vi&#65289;&#24320;&#25918;&#38598;&#35782;&#21035;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;17&#20010;&#22522;&#20934;&#23545;&#65292;&#20351;&#29992;&#29616;&#26377;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#19968;&#31995;&#21015;&#35780;&#20272;&#21327;&#35758;&#26469;&#23545;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;VSSL&#26041;&#27861;&#30340;&#19968;&#31995;&#21015;&#26377;&#36259;&#21457;&#29616;&#21644;&#26377;&#36259;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23613;&#31649;&#35270;&#39057;&#27169;&#22411;&#36890;&#24120;
&lt;/p&gt;
&lt;p&gt;
Video self-supervised learning (VSSL) has made significant progress in recent years. However, the exact behavior and dynamics of these models under different forms of distribution shift are not yet known. In this paper, we comprehensively study the behavior of six popular self-supervised methods (v-SimCLR, v-MoCo, v-BYOL, v-SimSiam, v-DINO, v-MAE) in response to various forms of natural distribution shift, i.e., (i) context shift, (ii) viewpoint shift, (iii) actor shift, (iv) source shift, (v) generalizability to unknown classes (zero-shot), and (vi) open-set recognition. To perform this extensive study, we carefully craft a test bed consisting of 17 in-distribution and out-of-distribution benchmark pairs using available public datasets and a series of evaluation protocols to stress-test the different methods under the intended shifts. Our study uncovers a series of intriguing findings and interesting behaviors of VSSL methods. For instance, we observe that while video models generally
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#19987;&#23478;&#30340;&#20215;&#20540;&#36229;&#20986;&#20102;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#27979;&#35797;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01646</link><description>&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#23457;&#26680;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Auditing for Human Expertise. (arXiv:2306.01646v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01646
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#30340;&#20215;&#20540;&#36229;&#20986;&#20102;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#27979;&#35797;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#65288;&#20363;&#22914;&#24739;&#32773;&#35786;&#26029;&#65289;&#36890;&#24120;&#30001;&#25509;&#21463;&#22521;&#35757;&#30340;&#20154;&#31867;&#19987;&#23478;&#22788;&#29702;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#33258;&#21160;&#21270;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#65292;&#19987;&#23478;&#21487;&#33021;&#36816;&#29992;&#24456;&#38590;&#24314;&#27169;&#30340;&#30452;&#35273;&#65292;&#24182;&#19988;/&#25110;&#32773;&#21487;&#20197;&#33719;&#21462;&#20449;&#24687;&#65288;&#20363;&#22914;&#19982;&#24739;&#32773;&#30340;&#20132;&#35848;&#65289;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65292;&#20154;&#31867;&#19987;&#23478;&#26159;&#21542;&#22686;&#21152;&#20102;&#26080;&#27861;&#34987;&#31639;&#27861;&#39044;&#27979;&#22120;&#25429;&#25417;&#21040;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20026;&#19968;&#20010;&#33258;&#28982;&#30340;&#20551;&#35774;&#26816;&#39564;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#26694;&#26550;&#25152;&#24378;&#35843;&#30340;&#37027;&#26679;&#65292;&#26816;&#27979;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#27604;&#31616;&#21333;&#27604;&#36739;&#19987;&#23478;&#39044;&#27979;&#20934;&#30830;&#24615;&#19982;&#29305;&#23450;&#23398;&#20064;&#31639;&#27861;&#20570;&#20986;&#30340;&#20934;&#30830;&#24615;&#26356;&#21152;&#24494;&#22937;&#12290;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#65292;&#27979;&#35797;&#19987;&#23478;&#39044;&#27979;&#26159;&#21542;&#22312;&#8220;&#29305;&#24449;&#8221;&#21487;&#29992;&#32780;&#26465;&#20214;&#19979;&#26159;&#21542;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#32479;&#35745;&#19978;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#25298;&#32477;&#34920;&#26126;&#20102;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30830;&#23454;&#22686;&#21152;&#20102;&#36229;&#20986;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor. We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (`features'). A rejection of our test thus suggests that huma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27880;&#24847;&#21147;&#25925;&#38556;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#26469;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer FFLMs&#32463;&#24120;&#20986;&#29616;&#25512;&#29702;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.00946</link><description>&lt;p&gt;
&#25581;&#31034;Attention&#25925;&#38556;&#30340;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Exposing Attention Glitches with Flip-Flop Language Modeling. (arXiv:2306.00946v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27880;&#24847;&#21147;&#25925;&#38556;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#26469;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer FFLMs&#32463;&#24120;&#20986;&#29616;&#25512;&#29702;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20160;&#20040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#36755;&#20986;&#20107;&#23454;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#38169;&#35823;&#30340;&#25512;&#29702;&#65311;&#36825;&#20123;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25191;&#34892;&#38271;&#38142;&#25512;&#29702;&#26102;&#65292;&#30446;&#21069;&#20284;&#20046;&#26159;&#20026;&#20102;&#23427;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#32508;&#21512;&#30693;&#35782;&#12289;&#35821;&#29992;&#21644;&#25277;&#35937;&#24605;&#32500;&#32780;&#24517;&#39035;&#20184;&#20986;&#30340;&#20195;&#20215;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20010;&#26681;&#26412;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#30830;&#23450;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#25925;&#38556;&#29616;&#35937;&#65292;&#20854;&#20013;Transformer&#26550;&#26500;&#30340;&#24402;&#32435;&#24615;&#20559;&#35265;&#38388;&#27463;&#24615;&#22320;&#26410;&#33021;&#25429;&#25417;&#21040;&#31283;&#20581;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#38548;&#31163;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65288;FFLM&#65289;&#65292;&#36825;&#26159;&#19968;&#32452;&#21442;&#25968;&#21270;&#21512;&#25104;&#22522;&#20934;&#65292;&#26088;&#22312;&#25506;&#32034;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#22806;&#25512;&#34892;&#20026;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#29983;&#25104;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#22312;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#20013;&#22797;&#21046;&#20108;&#36827;&#21046;&#31526;&#21495;&#65292;&#24573;&#30053;&#20013;&#38388;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#21457;&#29616;Transformer FFLMs&#22312;&#25512;&#29702;&#38169;&#35823;&#26041;&#38754;&#23384;&#22312;&#30528;&#38271;&#23614;&#29616;&#35937;&#65292;&#20854;&#20013;&#19968;&#20123;&#25105;&#20204;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#26680;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#19982;&#26684;&#29702;&#35770;&#24314;&#31435;&#20102;&#25968;&#23398;&#32852;&#31995;&#65292;&#20026;&#22686;&#24378;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00904</link><description>&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#27979;&#37327;&#65292;&#20998;&#21306;&#26684;&#21644;&#26680;&#27979;&#35797;&#29992;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions. (arXiv:2306.00904v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#26680;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#19982;&#26684;&#29702;&#35770;&#24314;&#31435;&#20102;&#25968;&#23398;&#32852;&#31995;&#65292;&#20026;&#22686;&#24378;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20381;&#36182;&#20110;&#25104;&#23545;&#20851;&#31995;&#30340;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#21508;&#31181;&#39046;&#22495;&#65288;&#22914;&#31038;&#20250;&#32463;&#27982;&#12289;&#29983;&#24577;&#25110;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#65289;&#20013;&#25214;&#21040;&#30340;&#22797;&#26434;&#22810;&#21464;&#37327;&#25968;&#25454;&#30340;&#23436;&#25972;&#32479;&#35745;&#32467;&#26500;&#12290;&#20004;&#20010;&#20197;&#19978;&#21464;&#37327;&#32452;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#20381;&#36182;&#20851;&#31995;&#22312;&#36825;&#20123;&#31995;&#32479;&#30340;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#21487;&#20197;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#36825;&#26679;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;$d$-order ($d \geq 2$)&#30456;&#20114;&#20316;&#29992;&#27979;&#37327;&#65292;&#20381;&#27425;&#21253;&#25324;&#21487;&#33021;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#20998;&#35299;&#65292;&#24182;&#23450;&#20041;&#20102;&#38750;&#21442;&#25968;&#12289;&#22522;&#20110;&#26680;&#30340;&#27979;&#35797;&#65292;&#20197;&#31995;&#32479;&#22320;&#30830;&#23450;$d$-order&#30456;&#20114;&#20316;&#29992;&#30340;&#32479;&#35745;&#26174;&#30528;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#26684;&#29702;&#35770;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#38416;&#26126;&#20102;&#30456;&#20114;&#20316;&#29992;&#24230;&#37327;&#30340;&#23548;&#20986;&#21450;&#20854;&#22797;&#21512;&#25490;&#21015;&#27979;&#35797;&#30340;&#28085;&#20041;&#65307;&#28548;&#28165;&#20102;&#21333;&#32431;&#22797;&#21512;&#20307;&#19982;&#26680;&#30697;&#38453;&#20013;&#24515;&#21270;&#30340;&#32852;&#31995;&#65307;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models that rely solely on pairwise relationships often fail to capture the complete statistical structure of the complex multivariate data found in diverse domains, such as socio-economic, ecological, or biomedical systems. Non-trivial dependencies between groups of more than two variables can play a significant role in the analysis and modelling of such systems, yet extracting such high-order interactions from data remains challenging. Here, we introduce a hierarchy of $d$-order ($d \geq 2$) interaction measures, increasingly inclusive of possible factorisations of the joint probability distribution, and define non-parametric, kernel-based tests to establish systematically the statistical significance of $d$-order interactions. We also establish mathematical links with lattice theory, which elucidate the derivation of the interaction measures and their composite permutation tests; clarify the connection of simplicial complexes with kernel matrix centring; and provide a means to enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#26410;&#30693;&#24178;&#39044;&#25968;&#25454;&#20013;&#25512;&#26029;&#38750;&#21442;&#25968;&#22240;&#26524;&#34920;&#36798;&#24335;&#23398;&#20064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#20004;&#20010;&#22240;&#26524;&#21464;&#37327;&#30340;&#22522;&#26412;&#35774;&#32622;&#20013;&#65292;&#26080;&#27861;&#28040;&#38500;&#19968;&#20123;&#30001;&#24178;&#39044;&#25968;&#25454;&#24341;&#36215;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00542</link><description>&lt;p&gt;
&#26410;&#30693;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#36798;&#24335;&#30340;&#38750;&#21442;&#25968;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Identifiability of Causal Representations from Unknown Interventions. (arXiv:2306.00542v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#26410;&#30693;&#24178;&#39044;&#25968;&#25454;&#20013;&#25512;&#26029;&#38750;&#21442;&#25968;&#22240;&#26524;&#34920;&#36798;&#24335;&#23398;&#20064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#20004;&#20010;&#22240;&#26524;&#21464;&#37327;&#30340;&#22522;&#26412;&#35774;&#32622;&#20013;&#65292;&#26080;&#27861;&#28040;&#38500;&#19968;&#20123;&#30001;&#24178;&#39044;&#25968;&#25454;&#24341;&#36215;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22240;&#26524;&#34920;&#36798;&#24335;&#23398;&#20064;&#65292;&#21363;&#20174;&#21464;&#37327;&#30340;&#39640;&#32500;&#20989;&#25968;&#65288;&#8220;&#28151;&#21512;&#29289;&#8221;&#65289;&#20013;&#25512;&#26029;&#28508;&#22312;&#30340;&#22240;&#26524;&#21464;&#37327;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#24369;&#30417;&#30563;&#65292;&#22914;&#21453;&#20107;&#23454;&#30340;&#24178;&#39044;&#35266;&#23519;&#25110;&#26102;&#38388;&#32467;&#26500;&#65307;&#23545;&#28151;&#21512;&#20989;&#25968;&#25110;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#26045;&#21152;&#38480;&#21046;&#65292;&#22914;&#32447;&#24615;&#65307;&#25110;&#38656;&#35201;&#37096;&#20998;&#20102;&#35299;&#29983;&#25104;&#36807;&#31243;&#65292;&#22914;&#22240;&#26524;&#22270;&#25110;&#24178;&#39044;&#30446;&#26631;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#22240;&#26524;&#27169;&#22411;&#21644;&#28151;&#21512;&#20989;&#25968;&#37117;&#26159;&#38750;&#21442;&#25968;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#23398;&#20064;&#20449;&#21495;&#37319;&#29992;&#26469;&#33258;&#22522;&#30784;&#22240;&#26524;&#27169;&#22411;&#20013;&#26410;&#30693;&#24178;&#39044;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#25110;&#29615;&#22659;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#22320;&#38754;&#30495;&#23454;&#28508;&#21464;&#37327;&#21450;&#20854;&#22240;&#26524;&#22270;&#37492;&#23450;&#20986;&#26469;&#65292;&#21516;&#26102;&#35299;&#20915;&#19968;&#32452;&#20174;&#24178;&#39044;&#25968;&#25454;&#26080;&#27861;&#28040;&#38500;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#22240;&#26524;&#21464;&#37327;&#30340;&#22522;&#26412;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions ("mixtures") of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or the intervention targets. We instead consider the general setting in which both the causal model and the mixing function are nonparametric. The learning signal takes the form of multiple datasets, or environments, arising from unknown interventions in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00354</link><description>&lt;p&gt;
&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23427;&#22312;&#21516;&#26102;&#28085;&#30422;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#21435;&#22122;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#25913;&#21892;&#25193;&#25955;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#22320;&#65292;MTL&#26377;&#26102;&#20250;&#23548;&#33268;&#20247;&#25152;&#21608;&#30693;&#30340;$\textit{&#36127;&#36801;&#31227;}$&#29616;&#35937;&#65292;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#32780;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#25193;&#25955;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;$\textbf{(O1)}$ &#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#24046;&#36317;&#21152;&#22823;&#65292;&#21435;&#22122;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#20146;&#21644;&#21147;&#20943;&#24369;&#65292; $\textbf{(O2)}$ &#22312;&#25193;&#25955;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#36127;&#36801;&#31227;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#36731;&#36127;&#36801;&#31227;&#26469;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12289;&#20855;&#20307;&#26159;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#26469;&#40723;&#21169;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#24182;&#20943;&#23569;&#20219;&#21153;&#24178;&#25200;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#21435;&#22122;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00219</link><description>&lt;p&gt;
&#25193;&#25955;&#30011;&#31508;&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#38480;&#21046;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#32463;&#24120;&#21253;&#21547;&#19981;&#33391;&#30340;&#20266;&#24433;&#25110;&#20854;&#20182;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#24494;&#35843;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#35201;&#20040;&#32791;&#26102;&#65288;&#25163;&#21160;&#32534;&#36753;&#65289;&#65292;&#35201;&#20040;&#20135;&#29983;&#19981;&#22815;&#23436;&#32654;&#30340;&#32467;&#26524;&#65288;&#20462;&#34917;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#25972;&#20307;&#22270;&#20687;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21464;&#21270;&#65288;&#21464;&#20307;&#36873;&#25321;&#21644;&#25552;&#31034;&#24494;&#35843;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24494;&#35843;AI&#21512;&#25104;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#24341;&#20837;&#20102;&#26032;&#30340;&#38543;&#26426;&#22122;&#22768;&#27169;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20854;&#20182;&#21306;&#22495;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23545;&#25351;&#23450;&#21306;&#22495;&#36827;&#34892;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#33402;&#26415;&#23478;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.00183</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#25193;&#25955;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#34920;&#31034;&#24050;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#21152;&#28145;&#20837;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#26159;&#22914;&#20309;&#34987;&#32534;&#30721;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#23618;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#23637;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#23545;&#20110;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#22823;&#23567;&#30340;&#20219;&#20309;&#38543;&#26426;&#23376;&#38598;&#31070;&#32463;&#20803;&#65292;&#37117;&#19982;&#23436;&#25972;&#23618;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#25972;&#20010;&#23618;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#65288;&#21253;&#25324;CNN&#21644;Transformer&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;ImageNet1k&#21644;ImageNet21k&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#26469;&#38477;&#20302;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#29702;&#35770;&#20998;&#26512;&#19982;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#21487;&#20197;&#20316;&#20026;&#20272;&#35745;&#27867;&#21270;&#38388;&#38553;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00169</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19981;&#19968;&#33268;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training. (arXiv:2306.00169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#29702;&#35770;&#20998;&#26512;&#19982;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#21487;&#20197;&#20316;&#20026;&#20272;&#35745;&#27867;&#21270;&#38388;&#38553;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#24456;&#39640;&#30340;&#34920;&#29616;&#21147;&#65292;&#23547;&#25214;&#20855;&#26377;&#23567;&#27867;&#21270;&#24046;&#36317;&#30340;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#24456;&#37325;&#35201;&#65288;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#35265;&#25968;&#25454;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#35757;&#32451;&#30340;&#38543;&#26426;&#24615;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#65292;&#20854;&#20013;&#27867;&#21270;&#38388;&#38553;&#30340;&#30028;&#38480;&#21462;&#20915;&#20110;&#25105;&#20204;&#31216;&#20043;&#20026;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#34920;&#26126;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#65292;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#24378;&#28872;&#39044;&#31034;&#30528;&#27867;&#21270;&#38388;&#38553;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#19981;&#19968;&#33268;&#24615;&#27604;&#25439;&#22833;&#21464;&#21270;&#30340;&#38160;&#24230;&#26356;&#21487;&#38752;&#22320;&#25351;&#31034;&#30528;&#27867;&#21270;&#38388;&#38553;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#30340;&#31639;&#27861;&#21487;&#20197;&#24102;&#26469;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#36824;&#20026;&#29616;&#26377;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#22914;&#20849;&#21516;&#33976;&#39311;&#21644;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks are highly expressive, it is important to find solutions with small generalization gap (the difference between the performance on the training data and unseen data). Focusing on the stochastic nature of training, we first present a theoretical analysis in which the bound of generalization gap depends on what we call inconsistency and instability of model outputs, which can be estimated on unlabeled data. Our empirical study based on this analysis shows that instability and inconsistency are strongly predictive of generalization gap in various settings. In particular, our finding indicates that inconsistency is a more reliable indicator of generalization gap than the sharpness of the loss landscape. Furthermore, we show that algorithmic reduction of inconsistency leads to superior performance. The results also provide a theoretical basis for existing methods such as co-distillation and ensemble.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Language augmented CLIP&#65288;LaCLIP&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#37325;&#20889;&#26469;&#22686;&#24378;CLIP&#35757;&#32451;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#37325;&#26032;&#20070;&#20889;&#19982;&#27599;&#20010;&#22270;&#20687;&#20851;&#32852;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.20088</link><description>&lt;p&gt;
&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#35821;&#35328;&#37325;&#20889;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving CLIP Training with Language Rewrites. (arXiv:2305.20088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Language augmented CLIP&#65288;LaCLIP&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#37325;&#20889;&#26469;&#22686;&#24378;CLIP&#35757;&#32451;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#37325;&#26032;&#20070;&#20889;&#19982;&#27599;&#20010;&#22270;&#20687;&#20851;&#32852;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#20351;&#29992;&#25104;&#23545;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#36716;&#31227;&#35270;&#35273;&#27169;&#22411;&#30340;&#26368;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;CLIP&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#21644;&#25463;&#24452;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;CLIP&#35757;&#32451;&#33539;&#24335;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#20165;&#24212;&#29992;&#20110;&#22270;&#20687;&#36755;&#20837;&#65292;&#32780;&#35821;&#35328;&#36755;&#20837;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#65292;&#38480;&#21046;&#20102;&#22810;&#26679;&#25991;&#26412;&#23545;&#30456;&#21516;&#22270;&#20687;&#30340;&#26292;&#38706;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Language augmented CLIP&#65288;LaCLIP&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#37325;&#20889;&#26469;&#22686;&#24378;CLIP&#35757;&#32451;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#37325;&#26032;&#20070;&#20889;&#19982;&#27599;&#20010;&#22270;&#20687;&#20851;&#32852;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#20123;&#37325;&#26032;&#20070;&#20889;&#30340;&#25991;&#26412;&#22312;&#21477;&#23376;&#32467;&#26500;&#21644;&#35789;&#27719;&#26041;&#38754;&#21576;&#29616;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#20851;&#38190;&#27010;&#24565;&#21644;&#24847;&#20041;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;LaCLIP&#38543;&#26426;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lattice&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#31574;&#30053;&#32593;&#32476;&#30340;&#28508;&#22312;&#29366;&#24577;&#20013;&#27880;&#20837;&#26102;&#38388;&#30456;&#20851;&#24615;&#22122;&#22768;&#65292;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#20316;&#29992;&#22120;&#31995;&#32479;&#23384;&#22312;&#30340;&#25506;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.20065</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Latent Exploration for Reinforcement Learning. (arXiv:2305.20065v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lattice&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#31574;&#30053;&#32593;&#32476;&#30340;&#28508;&#22312;&#29366;&#24577;&#20013;&#27880;&#20837;&#26102;&#38388;&#30456;&#20851;&#24615;&#22122;&#22768;&#65292;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#20316;&#29992;&#22120;&#31995;&#32479;&#23384;&#22312;&#30340;&#25506;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#36890;&#36807;&#25506;&#32034;&#21644;&#19982;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#31574;&#30053;&#12290;&#30001;&#20110;&#32500;&#24230;&#28798;&#38590;&#65292;&#23398;&#20064;&#23558;&#39640;&#32500;&#24863;&#30693;&#36755;&#20837;&#26144;&#23556;&#21040;&#36816;&#21160;&#36755;&#20986;&#30340;&#31574;&#30053;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#22914;SAC&#65292;PPO&#31561;&#65289;&#36890;&#36807;&#23545;&#20316;&#29992;&#21147;&#26045;&#21152;&#29420;&#31435;&#30340;&#39640;&#26031;&#22122;&#22768;&#26469;&#25506;&#32034;&#29615;&#22659;&#12290;&#23613;&#31649;&#36825;&#31181;&#38750;&#32467;&#26500;&#21270;&#30340;&#25506;&#32034;&#26041;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#35777;&#26126;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36807;&#21160;&#20316;&#31995;&#32479;&#26469;&#35828;&#21487;&#33021;&#19981;&#22815;&#20248;&#21270;&#12290;&#24403;&#22810;&#20010;&#20316;&#29992;&#22120;&#65288;&#22914;&#39532;&#36798;&#25110;&#32908;&#32905;&#65289;&#39537;&#21160;&#34892;&#20026;&#26102;&#65292;&#19981;&#30456;&#20851;&#30340;&#25200;&#21160;&#21487;&#33021;&#20250;&#20943;&#23569;&#24444;&#27492;&#30340;&#24433;&#21709;&#65292;&#25110;&#20197;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#20462;&#25913;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#36890;&#36807;&#24341;&#20837;&#21160;&#20316;&#25200;&#21160;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#24573;&#35270;&#20102;&#36328;&#20316;&#29992;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#26102;&#38388;&#30456;&#20851;&#25506;&#32034;&#65288;Lattice&#65289;&#65292;&#19968;&#31181;&#23558;&#26102;&#38388;&#30456;&#20851;&#22122;&#22768;&#27880;&#20837;&#21040;&#31574;&#30053;&#32593;&#32476;&#30340;&#28508;&#22312;&#29366;&#24577;&#20013;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning, agents learn policies by exploring and interacting with the environment. Due to the curse of dimensionality, learning policies that map high-dimensional sensory input to motor output is particularly challenging. During training, state of the art methods (SAC, PPO, etc.) explore the environment by perturbing the actuation with independent Gaussian noise. While this unstructured exploration has proven successful in numerous tasks, it can be suboptimal for overactuated systems. When multiple actuators, such as motors or muscles, drive behavior, uncorrelated perturbations risk diminishing each other's effect, or modifying the behavior in a task-irrelevant way. While solutions to introduce time correlation across action perturbations exist, introducing correlation across actuators has been largely ignored. Here, we propose LATent TIme-Correlated Exploration (Lattice), a method to inject temporally-correlated noise into the latent state of the policy network, which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNSSOR&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#28193;&#35757;&#32451;&#28151;&#21512;&#29289;&#26469;&#23454;&#29616;&#26080;&#30417;&#30563;&#31070;&#32463;&#35821;&#38899;&#20998;&#31163;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#27599;&#20010;&#28151;&#21512;&#20449;&#21495;&#20316;&#20026;&#32422;&#26463;&#65292;&#23558;&#20272;&#35745;&#30340;&#35828;&#35805;&#32773;&#22270;&#20687;&#21152;&#36215;&#26469;&#31561;&#20110;&#28151;&#21512;&#20449;&#21495;&#65292;&#20174;&#32780;&#32553;&#23567;&#35299;&#30340;&#33539;&#22260;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UNSSOR&#31639;&#27861;&#22312;&#35821;&#38899;&#20998;&#31163;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.20054</link><description>&lt;p&gt;
UNSSOR: &#36890;&#36807;&#21033;&#29992;&#36807;&#24230;&#35757;&#32451;&#28151;&#21512;&#29289;&#23454;&#29616;&#26080;&#30417;&#30563;&#31070;&#32463;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures. (arXiv:2305.20054v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNSSOR&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#28193;&#35757;&#32451;&#28151;&#21512;&#29289;&#26469;&#23454;&#29616;&#26080;&#30417;&#30563;&#31070;&#32463;&#35821;&#38899;&#20998;&#31163;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#27599;&#20010;&#28151;&#21512;&#20449;&#21495;&#20316;&#20026;&#32422;&#26463;&#65292;&#23558;&#20272;&#35745;&#30340;&#35828;&#35805;&#32773;&#22270;&#20687;&#21152;&#36215;&#26469;&#31561;&#20110;&#28151;&#21512;&#20449;&#21495;&#65292;&#20174;&#32780;&#32553;&#23567;&#35299;&#30340;&#33539;&#22260;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UNSSOR&#31639;&#27861;&#22312;&#35821;&#38899;&#20998;&#31163;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#21516;&#26102;&#35828;&#35805;&#32773;&#30340;&#28151;&#21709;&#26465;&#20214;&#19979;&#65292;&#27599;&#20010;&#40614;&#20811;&#39118;&#33719;&#21462;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#22810;&#20010;&#35828;&#35805;&#32773;&#28151;&#21512;&#20449;&#21495;&#12290;&#22312;&#36807;&#28193;&#26465;&#20214;&#19979;&#65292;&#21363;&#40614;&#20811;&#39118;&#25968;&#37327;&#36229;&#36807;&#35828;&#35805;&#32773;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23558;&#27599;&#20010;&#28151;&#21512;&#20449;&#21495;&#20316;&#20026;&#32422;&#26463;&#26469;&#32553;&#23567;&#21040;&#35828;&#35805;&#32773;&#22270;&#20687;&#30340;&#35299;&#65292;&#24182;&#23454;&#29616;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#65288;&#21363;&#20272;&#35745;&#30340;&#40614;&#20811;&#39118;&#19978;&#30340;&#35828;&#35805;&#32773;&#22270;&#20687;&#24212;&#35813;&#21152;&#36215;&#26469;&#31561;&#20110;&#28151;&#21512;&#20449;&#21495;&#65289;&#12290;&#22522;&#20110;&#36825;&#20010;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNSSOR&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#24230;&#35757;&#32451;&#28151;&#21512;&#29289;&#23454;&#29616;&#26080;&#30417;&#30563;&#31070;&#32463;&#35821;&#38899;&#20998;&#31163;&#12290;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#28151;&#21512;&#29289;&#36865;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20197;&#20026;&#27599;&#20010;&#35828;&#35805;&#32773;&#20135;&#29983;&#19968;&#20010;&#20013;&#38388;&#20272;&#35745;&#65292;&#23545;&#20272;&#35745;&#36827;&#34892;&#32447;&#24615;&#28388;&#27874;&#65292;&#24182;&#20248;&#21270;&#25439;&#22833;&#65292;&#20197;&#20351;&#27599;&#20010;&#40614;&#20811;&#39118;&#19978;&#30340;&#25152;&#26377;&#35828;&#35805;&#32773;&#30340;&#28388;&#27874;&#20272;&#35745;&#30456;&#21152;&#31561;&#20110;&#28151;&#21512;&#20449;&#21495;&#65292;&#28385;&#36275;&#19978;&#36848;&#32422;&#26463;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#20998;&#31163;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\textbf{u}$nsupervised $\textbf{n}$eural $\textbf{s}$peech $\textbf{s}$eparation by leveraging $\textbf{o}$ver-determined training mixtu$\textbf{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20174;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#22686;&#24191;&#36807;&#31243;&#25152;&#20135;&#29983;&#30340;&#24402;&#32435;&#20559;&#32622;&#19982;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30456;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#23545;&#29616;&#20195;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.19818</link><description>&lt;p&gt;
&#20809;&#35889;&#35856;&#27874;&#65306;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#36830;&#25509;&#35889;&#23884;&#20837;&#21644;&#30697;&#38453;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Spectal Harmonics: Bridging Spectral Embedding and Matrix Completion in Self-Supervised Learning. (arXiv:2305.19818v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20174;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#22686;&#24191;&#36807;&#31243;&#25152;&#20135;&#29983;&#30340;&#24402;&#32435;&#20559;&#32622;&#19982;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30456;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#23545;&#29616;&#20195;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#26041;&#27861;&#30001;&#20110;&#20854;&#22312;&#23398;&#20064;&#34920;&#31034;&#26102;&#20284;&#20046;&#21551;&#21457;&#24335;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#27809;&#26377;&#26126;&#26174;&#30340;&#26631;&#31614;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23562;&#37325;&#25968;&#25454;&#35821;&#20041;&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#65292;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30340;&#25991;&#29486;&#21457;&#34920;&#65292;&#35797;&#22270;&#24314;&#31435;&#36215;&#23545;&#29616;&#20195;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#19968;&#31995;&#21015;&#25439;&#22833;&#30340;&#36830;&#36143;&#19988;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20174;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#35282;&#24230;&#25552;&#20379;&#19968;&#20010;&#29702;&#35299;&#65292;&#24182;&#23558;&#22686;&#24191;&#36807;&#31243;&#25152;&#20135;&#29983;&#30340;&#24402;&#32435;&#20559;&#32622;&#19982;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30456;&#36830;&#25509;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#30340;&#32467;&#26524;&#65292;&#23545;&#29616;&#20195;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20197;&#21450;&#24433;&#21709;&#20854;&#19979;&#28216;&#24615;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised methods received tremendous attention thanks to their seemingly heuristic approach to learning representations that respect the semantics of the data without any apparent supervision in the form of labels. A growing body of literature is already being published in an attempt to build a coherent and theoretically grounded understanding of the workings of a zoo of losses used in modern self-supervised representation learning methods. In this paper, we attempt to provide an understanding from the perspective of a Laplace operator and connect the inductive bias stemming from the augmentation process to a low-rank matrix completion problem. To this end, we leverage the results from low-rank matrix completion to provide theoretical analysis on the convergence of modern SSL methods and a key property that affects their downstream performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#19968;&#31181;&#21517;&#20026;&#8220;&#38567;&#36947;&#8221;&#30340;&#29616;&#35937;&#65292;&#23427;&#22312;&#32593;&#32476;&#30340;&#35757;&#32451;&#26089;&#26399;&#23601;&#20986;&#29616;&#65292;&#24182;&#19988;&#23545;&#26368;&#32456;&#30340;&#25968;&#25454;&#34920;&#31034;&#36215;&#21040;&#20102;&#21387;&#32553;&#20316;&#29992;&#12290;&#20854;&#20013;&#65292;&#21021;&#22987;&#23618;&#26500;&#24314;&#20102;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#24418;&#24335;&#65292;&#32780;&#38543;&#21518;&#30340;&#23618;&#21387;&#32553;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#24182;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#38567;&#36947;&#20250;&#21066;&#24369;&#32593;&#32476;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19753</link><description>&lt;p&gt;
&#38567;&#36947;&#25928;&#24212;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25968;&#25454;&#34920;&#31034;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
The Tunnel Effect: Building Data Representations in Deep Neural Networks. (arXiv:2305.19753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#19968;&#31181;&#21517;&#20026;&#8220;&#38567;&#36947;&#8221;&#30340;&#29616;&#35937;&#65292;&#23427;&#22312;&#32593;&#32476;&#30340;&#35757;&#32451;&#26089;&#26399;&#23601;&#20986;&#29616;&#65292;&#24182;&#19988;&#23545;&#26368;&#32456;&#30340;&#25968;&#25454;&#34920;&#31034;&#36215;&#21040;&#20102;&#21387;&#32553;&#20316;&#29992;&#12290;&#20854;&#20013;&#65292;&#21021;&#22987;&#23618;&#26500;&#24314;&#20102;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#24418;&#24335;&#65292;&#32780;&#38543;&#21518;&#30340;&#23618;&#21387;&#32553;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#24182;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#38567;&#36947;&#20250;&#21066;&#24369;&#32593;&#32476;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#38395;&#21517;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#26356;&#28145;&#30340;&#32593;&#32476;&#38544;&#21547;&#30528;&#23545;&#26356;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#29992;&#20110;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#32593;&#32476;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#37096;&#20998;&#65292;&#23427;&#20204;&#23545;&#26368;&#32456;&#25968;&#25454;&#34920;&#31034;&#30340;&#24418;&#25104;&#36215;&#30528;&#19981;&#21516;&#30340;&#20316;&#29992;&#65306;&#26368;&#21021;&#30340;&#23618;&#26500;&#24314;&#20102;&#32447;&#24615;&#21487;&#20998;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#32780;&#38543;&#21518;&#30340;&#23618;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#38567;&#36947;&#8221;&#65289;&#21017;&#21387;&#32553;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#65292;&#24182;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#19981;&#22823;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#38567;&#36947;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20250;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26089;&#26399;&#20986;&#29616;&#65292;&#38567;&#36947;&#30340;&#28145;&#24230;&#21462;&#20915;&#20110;&#32593;&#32476;&#23481;&#37327;&#19982;&#20219;&#21153;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#38567;&#36947;&#20250;&#21066;&#24369;&#32593;&#32476;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely known for their remarkable effectiveness across various tasks, with the consensus that deeper networks implicitly learn more complex data representations. This paper shows that sufficiently deep networks trained for supervised image classification split into two distinct parts that contribute to the resulting data representations differently. The initial layers create linearly-separable representations, while the subsequent layers, which we refer to as \textit{the tunnel}, compress these representations and have a minimal impact on the overall performance. We explore the tunnel's behavior through comprehensive empirical studies, highlighting that it emerges early in the training process. Its depth depends on the relation between the network's capacity and task complexity. Furthermore, we show that the tunnel degrades out-of-distribution generalization and discuss its implications for continual learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#21644;&#26494;&#24347;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36825;&#23545;&#20110;RL&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#30340;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;&#20855;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.19562</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#29616;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#21644;&#26494;&#24347;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36825;&#23545;&#20110;RL&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#30340;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;&#20855;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#21487;&#22797;&#29616;&#24615;&#20316;&#20026;&#31639;&#27861;&#23646;&#24615;&#36827;&#34892;&#20102;&#25968;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#30340;&#24102;&#25240;&#25187;&#34920;&#26684;MDP&#30340;&#22522;&#26412;&#35774;&#32622;&#12290;&#21463;Impagliazzo&#31561;&#20154; [2022]&#30340;&#21551;&#21457;&#65292;&#22914;&#26524;&#22312;&#20869;&#37096;&#38543;&#26426;&#24615;&#30456;&#21516;&#26102;&#65292;RL&#31639;&#27861;&#22312;&#20174;&#29983;&#25104;&#22120;&#25277;&#21462;&#30340;&#20004;&#20010;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#26679;&#26412;&#19978;&#25191;&#34892;&#20004;&#27425;&#24182;&#36755;&#20986;&#23436;&#20840;&#30456;&#21516;&#30340;&#31574;&#30053;&#65292;&#21017;&#34920;&#31034;&#35813;RL&#31639;&#27861;&#26159;&#21487;&#22797;&#21046;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#26377;&#25928;&#30340;$\rho$-&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#29992;&#20110;$(\varepsilon,\delta)$-&#26368;&#20248;&#31574;&#30053;&#20272;&#35745;&#65292;&#20854;&#26679;&#26412;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026; $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$&#65292;&#20854;&#20013;$N$&#26159;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#25968;&#37327;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#23376;&#31867;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; $ \Omega\left(\frac {N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right) $ &#38454;&#30340;&#19979;&#38480;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Kalavasis&#31561;&#20154;[2019]&#25552;&#20986;&#30340;&#21487;&#22797;&#21046;&#24615;&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#20854;&#20013;&#20165;&#35201;&#27714;&#31639;&#27861;&#30340;&#36755;&#20986;&#25509;&#36817;&#22797;&#21046;&#31639;&#27861;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#20854;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026; $\widetilde O\left(\frac{N^5\cdot\log(1/\delta)}{(1-\gamma)^9\cdot\varepsilon^4\cdot\rho^2}\right)$&#65292;&#29992;&#20110;$(\varepsilon,\delta)$&#24847;&#20041;&#19979;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#36825;&#27604;&#20808;&#21069;&#19982;&#30456;&#20851;&#38382;&#39064;&#30340;&#30028;&#38480;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;RL&#31639;&#27861;&#35774;&#35745;&#21644;&#21487;&#37325;&#22797;&#24615;&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\rho$-replicable algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and time complexity $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#19968;&#29983;&#25104;&#27969;&#32593;&#32476;&#20013;&#32852;&#21512;&#24314;&#27169;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38750;&#31163;&#25955;&#26679;&#26412;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#23616;&#37096;&#27010;&#29575;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19366</link><description>&lt;p&gt;
&#21333;&#19968;&#29983;&#25104;&#27969;&#32593;&#32476;&#20013;&#30340;&#22270;&#32467;&#26500;&#19982;&#21442;&#25968;&#30340;&#32852;&#21512;&#36125;&#21494;&#26031;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network. (arXiv:2305.19366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#19968;&#29983;&#25104;&#27969;&#32593;&#32476;&#20013;&#32852;&#21512;&#24314;&#27169;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38750;&#31163;&#25955;&#26679;&#26412;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#23616;&#37096;&#27010;&#29575;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#26159;&#19968;&#31867;&#23545;&#31163;&#25955;&#21644;&#32467;&#26500;&#21270;&#26679;&#26412;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#26029;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#36793;&#32536;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#22312;&#38750;&#31163;&#25955;&#26679;&#26412;&#31354;&#38388;&#19978;&#23558;&#27492;&#26694;&#26550;&#25193;&#23637;&#21040;&#32852;&#21512;&#21518;&#39564;&#20998;&#24067;&#30340;&#24314;&#27169;&#65292;&#19981;&#20165;&#21253;&#25324;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#36824;&#32771;&#34385;&#20102;&#20854;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31574;&#30053;&#23398;&#20064;&#22312;NetHack&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20998;&#26512;&#20102;&#33719;&#32988;&#30340;&#31526;&#21495;&#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#21160;&#20316;&#23618;&#32423;&#30340;&#20248;&#21183;&#12289;&#31070;&#32463;&#26550;&#26500;&#30340;&#25913;&#36827;&#20197;&#21450;&#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30340;&#25972;&#21512;&#31561;&#26041;&#38754;&#21487;&#33021;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19240</link><description>&lt;p&gt;
NetHack&#24456;&#38590;&#34987;&#40657;&#23458;&#20837;&#20405;&#12290;
&lt;/p&gt;
&lt;p&gt;
NetHack is Hard to Hack. (arXiv:2305.19240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31574;&#30053;&#23398;&#20064;&#22312;NetHack&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20998;&#26512;&#20102;&#33719;&#32988;&#30340;&#31526;&#21495;&#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#21160;&#20316;&#23618;&#32423;&#30340;&#20248;&#21183;&#12289;&#31070;&#32463;&#26550;&#26500;&#30340;&#25913;&#36827;&#20197;&#21450;&#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30340;&#25972;&#21512;&#31561;&#26041;&#38754;&#21487;&#33021;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#31181;&#25511;&#21046;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20174;Atari&#28216;&#25103;&#21040;&#27169;&#25311;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#35270;&#37326;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22810;&#27169;&#24577;&#35266;&#27979;&#30340;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#65292;&#27604;&#22914;&#27969;&#34892;&#30340;&#22320;&#29282;&#25506;&#38505;&#28216;&#25103;NetHack&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#38754;&#20020;&#22256;&#38590;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;NeurIPS 2021 NetHack&#25361;&#25112;&#36187;&#34920;&#26126;&#65292;&#31526;&#21495;&#20195;&#29702;&#22312;&#20013;&#20301;&#28216;&#25103;&#24471;&#20998;&#19978;&#36229;&#36807;&#31070;&#32463;&#26041;&#27861;&#22235;&#20493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#24182;&#23545;NetHack&#30340;&#31070;&#32463;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#33719;&#32988;&#30340;&#31526;&#21495;&#20195;&#29702;&#65292;&#24182;&#25193;&#23637;&#20102;&#20854;&#20195;&#30721;&#24211;&#20197;&#36319;&#36394;&#20869;&#37096;&#31574;&#30053;&#36873;&#25321;&#65292;&#20197;&#29983;&#25104;&#20854;&#20013;&#19968;&#20010;&#26368;&#22823;&#30340;&#21487;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;(i) &#21160;&#20316;&#23618;&#32423;&#30340;&#20248;&#21183;&#65307;(ii) &#31070;&#32463;&#26550;&#26500;&#30340;&#25913;&#36827;&#65307;&#20197;&#21450; (iii) &#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#21387;&#32553;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.19185</link><description>&lt;p&gt;
Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#19979;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#21387;&#32553;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24120;&#35265;&#31867;&#22411;&#30340;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#23558;&#22352;&#26631;&#26144;&#23556;&#21040;&#20449;&#21495;&#20540;&#30340;&#20989;&#25968;&#65292;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#20301;&#32622;&#21040;RGB&#20540;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21151;&#33021;&#34920;&#31034;&#36827;&#34892;&#36229;&#25311;&#21512;&#65292;&#28982;&#21518;&#32534;&#30721;&#32593;&#32476;&#26435;&#37325;&#26469;&#21387;&#32553;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23558;&#31934;&#24230;&#37327;&#21270;&#21040;&#20302;&#27604;&#29305;&#20250;&#22823;&#24133;&#38477;&#20302;&#37325;&#26500;&#36136;&#37327;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36807;&#24230;&#25311;&#21512;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#36817;&#20284;&#21518;&#39564;&#26435;&#37325;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#37327;&#21270;&#21644;&#29109;&#32534;&#30721;&#23427;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#20808;&#39564;&#26435;&#37325;&#20998;&#24067;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#37319;&#29992;&#20027;&#21160;&#23610;&#23544;&#35843;&#25972;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31639;&#27861;LANCE&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#26469;&#21387;&#21147;&#27979;&#35797;&#35270;&#35273;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#22810;&#26679;&#12289;&#36924;&#30495;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#22270;&#20687;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#32534;&#36753;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;ImageNet&#20013;&#20808;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#23618;&#27425;&#27169;&#22411;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19164</link><description>&lt;p&gt;
LANCE&#65306;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images. (arXiv:2305.19164v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31639;&#27861;LANCE&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#26469;&#21387;&#21147;&#27979;&#35797;&#35270;&#35273;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#22810;&#26679;&#12289;&#36924;&#30495;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#22270;&#20687;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#32534;&#36753;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;ImageNet&#20013;&#20808;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#23618;&#27425;&#27169;&#22411;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#65288;LANCE&#65289;&#26469;&#23545;&#35757;&#32451;&#36807;&#30340;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#36817;&#35821;&#35328;&#24314;&#27169;&#21644;&#22522;&#20110;&#25991;&#26412;&#32534;&#36753;&#30340;&#22270;&#20687;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#19968;&#22871;&#22810;&#26679;&#65292;&#36924;&#30495;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#22270;&#20687;&#22686;&#21152;&#20102;&#19968;&#20010;IID&#27979;&#35797;&#38598;&#21512;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#32780;&#19968;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#32534;&#36753;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#25581;&#31034;ImageNet&#20013;&#20808;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#23618;&#27425;&#27169;&#22411;&#20559;&#24046;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/virajprabhu/lance&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pre-trained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet. Code is available at https://github.com/virajprabhu/lance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#26469;&#25581;&#31034;&#20219;&#21153;&#30340;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#24067;&#23572;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18402</link><description>&lt;p&gt;
&#31070;&#32463;&#38613;&#22609;&#65306;&#36890;&#36807;&#20462;&#21098;&#21644;&#32593;&#32476;&#20998;&#26512;&#25581;&#31034;&#20998;&#23618;&#27169;&#22359;&#21270;&#20219;&#21153;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis. (arXiv:2305.18402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#26469;&#25581;&#31034;&#20219;&#21153;&#30340;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#24067;&#23572;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30446;&#26631;&#20989;&#25968;&#21644;&#20219;&#21153;&#36890;&#24120;&#34920;&#29616;&#20026;&#20998;&#23618;&#27169;&#22359;&#21270;&#65292;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#20989;&#25968;&#20197;&#20998;&#23618;&#32452;&#32455;&#12290;&#36825;&#20123;&#23376;&#20989;&#25968;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#29305;&#24449;&#65306;&#23427;&#20204;&#26377;&#19968;&#32452;&#19981;&#21516;&#30340;&#36755;&#20837;&#65288;&#36755;&#20837;&#21487;&#20998;&#31163;&#24615;&#65289;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#23618;&#27425;&#20013;&#20316;&#20026;&#36755;&#20837;&#34987;&#37325;&#29992;&#65288;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#65289;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#31435;&#20102;&#20998;&#23618;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#23398;&#20064;&#25928;&#29575;&#12289;&#27867;&#21270;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#22914;&#20309;&#35782;&#21035;&#28508;&#22312;&#30340;&#23376;&#20989;&#25968;&#21450;&#20854;&#20998;&#23618;&#32467;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38613;&#22609;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#21098;&#21644;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#20197;&#25581;&#31034;&#23376;&#20989;&#25968;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#24067;&#23572;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#31070;&#32463;&#38613;&#22609;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20219;&#21153;&#30340;&#28508;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26356;&#23481;&#26131;&#34987;&#20154;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#30340;&#28508;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transferability. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an ap
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.18399</link><description>&lt;p&gt;
&#20851;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#35268;&#33539;&#21270;&#23545;&#21021;&#22987;&#21270;&#31561;&#36317;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010; Gram &#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#35813;&#30697;&#38453;&#21253;&#21547;&#19982;&#19968;&#25209;&#36755;&#20837;&#23545;&#24212;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#25104;&#23545;&#20869;&#31215;&#12290;&#22312;&#20960;&#31181;&#26550;&#26500;&#20013;&#65292;&#35266;&#23519;&#21040;&#22312;&#21021;&#22987;&#21270;&#26102;&#35813; Gram &#30697;&#38453;&#20250;&#38543;&#30528;&#28145;&#24230;&#21464;&#24471;&#36864;&#21270;&#65292;&#20174;&#32780;&#20005;&#37325;&#20943;&#32531;&#35757;&#32451;&#36895;&#24230;&#12290;&#35268;&#33539;&#21270;&#23618;&#22914;&#25209;&#22788;&#29702;&#35268;&#33539;&#21270;&#25110;&#23618;&#35268;&#33539;&#21270;&#65292;&#22312;&#38450;&#27490;&#31209;&#23849;&#28291;&#38382;&#39064;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#26080;&#27861;&#20840;&#38754;&#35206;&#30422;&#24191;&#27867;&#29992;&#20110; transformer &#20013;&#30340;&#23618;&#35268;&#33539;&#21270;&#21644;&#26377;&#38480;&#28145;&#24230;&#19979;&#35268;&#33539;&#21270;&#30340;&#37327;&#21270;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32467;&#21512;&#28608;&#27963;&#20989;&#25968;&#23618;&#20351;&#29992;&#30340;&#23618;&#35268;&#33539;&#21270;&#21487;&#20197;&#20351;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340; Gram &#30697;&#38453;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#24182;&#20351;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340; Hermite &#23637;&#24320;&#26469;&#37327;&#21270;&#36825;&#20010;&#36895;&#24230;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18395</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311;&#65306;&#38754;&#21521;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#21512;&#30693;&#35782;&#29702;&#35299;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#39640;&#19988;&#28041;&#21450;&#25968;&#25454;&#38544;&#31169;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;&#26631;&#35760;&#25968;&#25454;&#25110;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#26159;&#30001;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#25152;&#38656;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311; (KARD) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#30340;&#20381;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#37325;&#25490;&#22120;&#65292;&#29992;&#20110;&#33719;&#24471;&#19982;&#20381;&#25454;&#29983;&#25104;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;KARD&#22312;&#19977;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;LLMs&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;&#29616;&#35937;&#65292;&#21457;&#29616;&#31070;&#32463;&#20803;&#21487;&#20197;&#36827;&#34892;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24314;&#31435;&#36215;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27492;&#32467;&#26500;&#21487;&#34987;&#26377;&#25928;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.18390</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;
&lt;/p&gt;
&lt;p&gt;
Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;&#29616;&#35937;&#65292;&#21457;&#29616;&#31070;&#32463;&#20803;&#21487;&#20197;&#36827;&#34892;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24314;&#31435;&#36215;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27492;&#32467;&#26500;&#21487;&#34987;&#26377;&#25928;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#36825;&#26159;&#20154;&#33041;&#20013;&#24120;&#35265;&#30340;&#29305;&#28857;&#65292;&#34987;&#35748;&#20026;&#23545;&#20110;&#26222;&#36941;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20027;&#35201;&#32771;&#34385;&#20102;&#27169;&#22359;&#21270;&#30340;&#20004;&#20010;&#20027;&#35201;&#29305;&#24449;&#65306;&#65288;1&#65289;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#19987;&#19994;&#21270;&#65306;&#25105;&#20204;&#35780;&#20272;&#20102;&#27599;&#20010;&#31070;&#32463;&#20803;&#26159;&#21542;&#20027;&#35201;&#19987;&#19994;&#21270;&#20110;&#26576;&#19968;&#21151;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#26159;&#30340;&#12290;&#65288;2&#65289;&#22522;&#20110;&#21151;&#33021;&#32858;&#31867;&#30340;&#31070;&#32463;&#20803;&#20998;&#32452;&#65306;&#25105;&#20204;&#25506;&#31350;&#20102;&#23558;&#31070;&#32463;&#20803;&#25353;&#21151;&#33021;&#20998;&#32452;&#30340;&#32467;&#26500;&#23547;&#25214;&#26041;&#27861;&#65292;&#27599;&#20010;&#27169;&#22359;&#22343;&#20026;&#20854;&#30456;&#24212;&#21151;&#33021;&#24037;&#20316;&#12290;&#37492;&#20110;&#21487;&#33021;&#23384;&#22312;&#30340;&#22823;&#37327;&#32467;&#26500;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#20998;&#23618;&#19987;&#23478;&#27169;&#22411;&#36523;&#19978;&#65292;&#24182;&#23558;&#31070;&#32463;&#20803;&#21010;&#20998;&#20026;&#19987;&#23478;&#65292;&#36890;&#24120;&#20026;&#19981;&#21516;&#30340;&#36755;&#20837;&#28608;&#27963;&#19981;&#21516;&#30340;&#19987;&#23478;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21151;&#33021;&#19987;&#23478;&#65292;&#32858;&#38598;&#20102;&#26576;&#19968;&#21151;&#33021;&#30340;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#25200;&#21160;&#21151;&#33021;&#19987;&#23478;&#30340;&#28608;&#27963;&#26174;&#33879;&#24433;&#21709;&#20102;&#30456;&#24212;&#30340;f&#38190;
&lt;/p&gt;
&lt;p&gt;
This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding f
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#19981;&#33021;&#20165;&#20165;&#20381;&#38752;&#32622;&#20449;&#24230;&#65292;&#36824;&#24212;&#32771;&#34385;&#39044;&#27979;&#26679;&#26412;&#25110;&#31867;&#21035;&#30340;&#38750;&#20856;&#22411;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#20856;&#22411;&#36755;&#20837;&#25110;&#31867;&#21035;&#65292;&#27169;&#22411;&#39044;&#27979;&#26356;&#36807;&#20110;&#33258;&#20449;&#19988;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#23558;&#38750;&#20856;&#22411;&#24615;&#32435;&#20837;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18262</link><description>&lt;p&gt;
&#36229;&#36234;&#32622;&#20449;&#24230;&#65306;&#21487;&#38752;&#27169;&#22411;&#36824;&#24212;&#32771;&#34385;&#38750;&#20856;&#22411;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Confidence: Reliable Models Should Also Consider Atypicality. (arXiv:2305.18262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18262
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#19981;&#33021;&#20165;&#20165;&#20381;&#38752;&#32622;&#20449;&#24230;&#65292;&#36824;&#24212;&#32771;&#34385;&#39044;&#27979;&#26679;&#26412;&#25110;&#31867;&#21035;&#30340;&#38750;&#20856;&#22411;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#20856;&#22411;&#36755;&#20837;&#25110;&#31867;&#21035;&#65292;&#27169;&#22411;&#39044;&#27979;&#26356;&#36807;&#20110;&#33258;&#20449;&#19988;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#23558;&#38750;&#20856;&#22411;&#24615;&#32435;&#20837;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#32622;&#20449;&#24230;&#20197;&#39044;&#27979;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#32622;&#20449;&#24230;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#24403;&#36755;&#20837;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#24456;&#22909;&#30340;&#34920;&#31034;&#25110;&#32773;&#36755;&#20837; inherently &#26131;&#28151;&#28102;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#32473;&#20986;&#36739;&#20302;&#30340;&#32622;&#20449;&#24230;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26679;&#26412;&#25110;&#31867;&#21035;&#30340;&#38750;&#20856;&#22411;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#38750;&#20856;&#22411;&#24615;&#19982;&#35823;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#23545;&#20110;&#38750;&#20856;&#22411;&#30340;&#36755;&#20837;&#25110;&#38750;&#20856;&#22411;&#30340;&#31867;&#21035;&#65292;&#39044;&#27979;&#26356;&#21152;&#36807;&#20110;&#33258;&#20449;&#19988;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38750;&#20856;&#22411;&#24615;&#32435;&#20837;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#37492;&#21035;&#24615;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#38750;&#20856;&#22411;&#24615;&#22914;&#20309;&#25552;&#39640;&#19981;&#21516;&#32932;&#33394;&#32676;&#20307;&#30340;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While most machine learning models can provide confidence in their predictions, confidence is insufficient to understand a prediction's reliability. For instance, the model may have a low confidence prediction if the input is not well-represented in the training dataset or if the input is inherently ambiguous. In this work, we investigate the relationship between how atypical(rare) a sample or a class is and the reliability of a model's predictions. We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy. Using these insights, we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models. In a case study, we show that using atypicality improves the performance of a skin lesion classifier across different skin tone groups witho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17588</link><description>&lt;p&gt;
&#35786;&#26029;&#21464;&#21387;&#22120;&#65306;&#25581;&#31034;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290; (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#20026;&#20102;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#23433;&#20840;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#20351;&#29992;&#26377;&#38480;&#30340;&#20020;&#24202;&#35760;&#24405;&#23545;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#24494;&#35843;&#20197;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#29305;&#24449;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;SUFO&#21033;&#29992;&#19968;&#31995;&#21015;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;&#30417;&#30563;&#25506;&#32034;&#12289;&#26080;&#30417;&#30563;&#30456;&#20284;&#24615;&#20998;&#26512;&#12289;&#29305;&#24449;&#21160;&#24577;&#21644;&#24322;&#24120;&#20540;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#20851;&#20110;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#30495;&#23454;&#19990;&#30028;&#30149;&#29702;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;MedNLI&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;110M&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20998;&#20026;&#36890;&#29992;&#39046;&#22495;&#65288;BERT, TNLR&#65289;&#12289;&#28151;&#21512;&#39046;&#22495;&#65288;BioBERT, Clinical BioBERT&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#65288;PubMedBERT&#65289;&#32452;&#12290;&#25105;&#20204;&#30340;SUFO&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;(1)
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36172;&#21338;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#65292;&#33021;&#22815;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#24182;&#22788;&#29702;&#22240;&#20998;&#24067;&#28418;&#31227;&#23548;&#33268;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17570</link><description>&lt;p&gt;
&#36890;&#36807;&#36172;&#21338;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Auditing Fairness by Betting. (arXiv:2305.17570v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36172;&#21338;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#65292;&#33021;&#22815;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#24182;&#22788;&#29702;&#22240;&#20998;&#24067;&#28418;&#31227;&#23548;&#33268;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#12289;&#39640;&#25928;&#12289;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#23457;&#35745;&#24050;&#37096;&#32626;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#30456;&#27604;&#20043;&#21069;&#20381;&#36182;&#20110;&#22266;&#23450;&#26679;&#26412;&#37327;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#24207;&#36143;&#30340;&#65292;&#24182;&#20801;&#35768;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#36319;&#36394;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20063;&#20801;&#35768;&#25968;&#25454;&#36890;&#36807;&#27010;&#29575;&#31574;&#30053;&#36827;&#34892;&#25910;&#38598;&#65292;&#32780;&#19981;&#26159;&#20174;&#20154;&#21475;&#20013;&#22343;&#21248;&#37319;&#26679;&#12290;&#36825;&#20351;&#24471;&#23457;&#35745;&#21487;&#20197;&#22312;&#20026;&#20854;&#20182;&#30446;&#30340;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#12290;&#27492;&#22806;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#38543;&#26102;&#38388;&#25913;&#21464;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#23376;&#20154;&#32676;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22240;&#27169;&#22411;&#21464;&#26356;&#25110;&#22522;&#30784;&#20154;&#32676;&#21464;&#26356;&#23548;&#33268;&#30340;&#20998;&#24067;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110; anytime-valid &#25512;&#26029;&#21644;&#21338;&#24328;&#32479;&#35745;&#23398;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;"&#36890;&#36807;&#36172;&#21338;&#36827;&#34892;&#27979;&#35797;"&#26694;&#26550;&#12290;&#36825;&#20123;&#32852;&#31995;&#30830;&#20445;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#24555;&#36895;&#21644;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics-the "testing by betting" framework in particular. These connections ensure that our methods are interpretable, fast, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.17225</link><description>&lt;p&gt;
&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#30340;&#30446;&#26631;&#26159;&#20174;&#28151;&#21512;&#35266;&#27979;&#21040;&#30340;&#21464;&#37327;&#20013;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#32780;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#24378;&#30456;&#20851;&#24615;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#20197;&#21450;&#32534;&#30721;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26410;&#30693;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65292;&#31216;&#20026;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#12290;CauCA&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;ICA&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#23545;&#28508;&#22312;&#25104;&#20998;&#20043;&#38388;&#30340;&#22240;&#26524;&#20381;&#36182;&#24314;&#27169;&#65292;&#20063;&#26159;CRL&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#19982;CRL&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#65292;&#20165;&#20851;&#27880;&#20110;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#12290;&#25152;&#26377;&#20851;&#20110;CauCA&#22238;&#25910;&#22522;&#30784;&#30495;&#30456;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;CRL&#65292;&#32780;&#21487;&#33021;&#24615;&#32467;&#26524;&#21487;&#20197;&#20316;&#20026;&#25193;&#23637;CRL&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#23558;&#20174;&#23545;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#23454;&#26045;&#19981;&#21516;&#31867;&#22411;&#24178;&#39044;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#24449;CauCA&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; 3T &#26041;&#27861;&#65292;&#21363;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;3T &#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#23545;&#26816;&#32034;&#20219;&#21153;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16999</link><description>&lt;p&gt;
&#19977;&#22612;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#28789;&#27963;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Three Towers: Flexible Contrastive Learning with Pretrained Image Models. (arXiv:2305.16999v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; 3T &#26041;&#27861;&#65292;&#21363;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;3T &#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#23545;&#26816;&#32034;&#20219;&#21153;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19977;&#22612;&#65288;3T&#65289;&#8221;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#32435;&#20837;&#23545;&#27604;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#19982;&#36890;&#24120;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#19981;&#21516;&#65292;&#26368;&#36817;&#30340; LiT&#65288;Zhai &#31561;&#20154;&#65292;2022&#65289;&#34920;&#26126;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#23884;&#20837;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;LiT &#30452;&#25509;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26367;&#25442;&#22270;&#20687;&#22612;&#65292;&#25490;&#38500;&#20102;&#23545;&#22270;&#20687;&#22612;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#30340;&#20219;&#20309;&#28508;&#22312;&#22909;&#22788;&#12290;&#36890;&#36807; 3T&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#31574;&#30053;&#65292;&#20801;&#35768;&#22270;&#20687;&#22612;&#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19977;&#20010;&#22612;&#65292;&#20854;&#20013;&#21253;&#21547;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#65292;&#24182;&#40723;&#21169;&#35813;&#31532;&#19977;&#20010;&#22612;&#19982;&#20027;&#35201;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22612;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;3T &#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110; LiT &#21644; CLIP &#39118;&#26684;&#30340;&#20174;&#22836;&#24320;&#22987;&#23545;&#27604;&#23398;&#20064;&#22522;&#32447;&#12290;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;3T &#22312;&#20174;&#22836;&#24320;&#22987;&#22522;&#32447;&#30340;&#22522;&#30784;&#19978;&#21487;&#38752;&#22320;&#25913;&#21892;&#65292;&#34429;&#28982;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#21450; LiT&#65292;&#20294;&#20173;&#28982;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#26041;&#27861;&#20984;&#26174;&#20102;&#23558;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#27880;&#20837;&#21040;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#21033;&#29992;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits of contrastively training the image tower. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperform
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#26368;&#30495;&#23454;&#21644;&#39640;&#39118;&#38505;&#30340;&#24773;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20316;&#32773;&#39318;&#20808;&#26500;&#24314;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#22411;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;&#40657;&#30418;&#26597;&#35810;&#21487;&#20197;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16934</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Evaluating Adversarial Robustness of Large Vision-Language Models. (arXiv:2305.16934v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#26368;&#30495;&#23454;&#21644;&#39640;&#39118;&#38505;&#30340;&#24773;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20316;&#32773;&#39318;&#20808;&#26500;&#24314;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#22411;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;&#40657;&#30418;&#26597;&#35810;&#21487;&#20197;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;GPT-4&#22312;&#29983;&#25104;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#35270;&#35273;&#36755;&#20837;&#26041;&#38754;&#65292;&#20351;&#24471;&#20132;&#20114;&#26356;&#26377;&#21019;&#36896;&#21147;&#21644;&#36866;&#24212;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#29983;&#25104;&#21152;&#21095;&#20102;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#24494;&#22937;&#22320;&#25805;&#32437;&#26368;&#26131;&#21463;&#25915;&#20987;&#30340;&#27169;&#24577;&#65288;&#20363;&#22914;&#35270;&#35273;&#65289;&#25104;&#21151;&#36991;&#24320;&#25972;&#20010;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#26368;&#30495;&#23454;&#21644;&#39640;&#39118;&#38505;&#30340;&#24773;&#22659;&#19979;&#35780;&#20272;&#24320;&#28304;&#22823;&#22411;VLMs&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#31995;&#32479;&#65292;&#24182;&#35797;&#22270;&#27450;&#39575;&#27169;&#22411;&#36820;&#22238;&#30446;&#26631;&#21709;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;BLIP&#65289;&#26500;&#24314;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#36716;&#31227;&#21040;&#20854;&#20182;VLMs&#65288;&#22914;MiniGPT-4&#12289;LLaVA&#12289;UniDiffuser&#12289;BLIP-2&#21644;Img2Prompt&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#36825;&#20123;VLMs&#19978;&#36827;&#34892;&#40657;&#30418;&#26597;&#35810;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#20540;&#36845;&#20195;&#31639;&#27861;Anc-VI&#65292;&#37319;&#29992;&#20102;&#38170;&#23450;&#26426;&#21046;&#65292;&#21487;&#21152;&#36895;Bellman&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#24615;&#31639;&#23376;&#30340;&#35745;&#31639;&#12290;&#23545;&#20110;$\gamma\approx 1$&#25110;$\gamma=1$&#65292;Anc-VI&#36895;&#24230;&#20026;$\mathcal{O}(1/k)$&#65292;&#27604;&#26631;&#20934;VI&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2305.16569</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#23450;&#26426;&#21046;&#30340;&#20540;&#36845;&#20195;&#21152;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerating Value Iteration with Anchoring. (arXiv:2305.16569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#20540;&#36845;&#20195;&#31639;&#27861;Anc-VI&#65292;&#37319;&#29992;&#20102;&#38170;&#23450;&#26426;&#21046;&#65292;&#21487;&#21152;&#36895;Bellman&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#24615;&#31639;&#23376;&#30340;&#35745;&#31639;&#12290;&#23545;&#20110;$\gamma\approx 1$&#25110;$\gamma=1$&#65292;Anc-VI&#36895;&#24230;&#20026;$\mathcal{O}(1/k)$&#65292;&#27604;&#26631;&#20934;VI&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#36845;&#20195;(Value Iteration, VI)&#26159;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#22522;&#30784;&#65292;&#24050;&#30693;&#20854;&#25910;&#25947;&#36895;&#24230;&#20026;$\mathcal{O}(\gamma^k)$&#65292;&#20854;&#20013;$\gamma$&#26159;&#25240;&#25187;&#22240;&#23376;&#12290;&#28982;&#32780;&#65292;&#22312;VI&#35774;&#32622;&#20013;&#30340;&#26368;&#20248;&#36895;&#24230;&#23578;&#26410;&#30830;&#23450;&#65292;&#23547;&#27714;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;&#26426;&#21046;&#19968;&#30452;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#38170;&#23450;&#26426;&#21046;&#30340;VI&#21152;&#36895;&#31639;&#27861;&#65292;&#31216;&#20026;Anc-VI&#12290;&#19981;&#21516;&#20110;Nesterov&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;Anc-VI&#21487;&#20197;&#21152;&#36895;Bellman&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#24615;&#31639;&#23376;&#65292;&#36824;&#27604;&#26631;&#20934;VI&#26356;&#24555;&#22320;&#20943;&#23569;Bellman&#35823;&#24046;&#12290;&#23588;&#20854;&#26159;&#65292;&#23545;&#20110;$\gamma\approx 1$&#25110;&#29978;&#33267;$\gamma=1$&#65292;Anc-VI&#21576;&#29616;&#20986;$\mathcal{O}(1/k)$&#30340;&#36895;&#24230;&#65292;&#32780;&#26631;&#20934;VI&#22312;$\gamma\ge 1-1/k$&#26102;&#30340;&#36895;&#24230;&#20026;$\mathcal{O}(1)$&#65292;&#20854;&#20013;$k$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19982;&#19978;&#30028;&#21305;&#37197;&#30340;&#22797;&#26434;&#24615;&#19979;&#30028;&#65292;&#38500;&#20102;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;$4$&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;Anc-VI&#30340;&#21152;&#36895;&#36895;&#24230;&#30340;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;Anc-VI&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value Iteration (VI) is foundational to the theory and practice of modern reinforcement learning, and it is known to converge at a $\mathcal{O}(\gamma^k)$-rate, where $\gamma$ is the discount factor. Surprisingly, however, the optimal rate for the VI setup was not known, and finding a general acceleration mechanism has been an open problem. In this paper, we present the first accelerated VI for both the Bellman consistency and optimality operators. Our method, called Anc-VI, is based on an \emph{anchoring} mechanism (distinct from Nesterov's acceleration), and it reduces the Bellman error faster than standard VI. In particular, Anc-VI exhibits a $\mathcal{O}(1/k)$-rate for $\gamma\approx 1$ or even $\gamma=1$, while standard VI has rate $\mathcal{O}(1)$ for $\gamma\ge 1-1/k$, where $k$ is the iteration count. We also provide a complexity lower bound matching the upper bound up to a constant factor of $4$, thereby establishing optimality of the accelerated rate of Anc-VI. Finally, we sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#31639;&#27861;(TreeDSB)&#26469;&#35299;&#20915;&#22810;&#20803;&#26368;&#20248;&#36755;&#36816;(mOT)&#30340;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#39640;&#32500;&#35774;&#32622;&#22914;&#22270;&#20687;&#25554;&#20540;&#21644;&#36125;&#21494;&#26031;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.16557</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#31639;&#27861;&#22312;Wasserstein&#37325;&#24515;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-Based Diffusion Schr\"odinger Bridge with Applications to Wasserstein Barycenters. (arXiv:2305.16557v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#31639;&#27861;(TreeDSB)&#26469;&#35299;&#20915;&#22810;&#20803;&#26368;&#20248;&#36755;&#36816;(mOT)&#30340;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#39640;&#32500;&#35774;&#32622;&#22914;&#22270;&#20687;&#25554;&#20540;&#21644;&#36125;&#21494;&#26031;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26368;&#20248;&#36755;&#36816;(mOT)&#26159;&#26368;&#20248;&#36755;&#36816;(OT)&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#20854;&#26088;&#22312;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#30456;&#23545;&#20110;&#26576;&#20123;&#39044;&#20808;&#25351;&#23450;&#30340;&#36793;&#38469;&#20998;&#24067;&#30340;&#31215;&#20998;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26641;&#24418;&#20108;&#27425;&#25104;&#26412;&#30340;&#29109;&#29256;&#26412;&#65292;&#21363;&#19968;&#31181;&#21487;&#20197;&#20889;&#20316;&#26641;&#33410;&#28857;&#20043;&#38388;&#25104;&#23545;&#25104;&#26412;&#20989;&#25968;&#20043;&#21644;&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Tree-based Diffusion Schr\"odinger Bridge(TreeDSB)&#65292;&#36825;&#26159;&#25193;&#23637;&#20102;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;(DSB)&#31639;&#27861;&#30340;&#31639;&#27861;&#12290;TreeDSB&#23545;&#24212;&#20110;&#22810;&#20803;Sinkhorn&#31639;&#27861;&#30340;&#21160;&#24577;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#24212;&#29992;&#26159;&#35745;&#31639;Wasserstein&#37325;&#24515;&#65292;&#23427;&#21487;&#20197;&#34987;&#37325;&#26032;&#36716;&#21270;&#20026;&#22522;&#20110;&#26143;&#24418;&#26641;&#30340;mOT&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#39640;&#32500;&#35774;&#32622;&#65292;&#22914;&#22270;&#20687;&#25554;&#20540;&#21644;&#36125;&#21494;&#26031;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at minimizing the integral of a cost function with respect to a distribution with some prescribed marginals. In this paper, we consider an entropic version of mOT with a tree-structured quadratic cost, i.e., a function that can be written as a sum of pairwise cost functions between the nodes of a tree. To address this problem, we develop Tree-based Diffusion Schr\"odinger Bridge (TreeDSB), an extension of the Diffusion Schr\"odinger Bridge (DSB) algorithm. TreeDSB corresponds to a dynamic and continuous state-space counterpart of the multimarginal Sinkhorn algorithm. A notable use case of our methodology is to compute Wasserstein barycenters which can be recast as the solution of a mOT problem on a star-shaped tree. We demonstrate that our methodology can be applied in high-dimensional settings such as image interpolation and Bayesian fusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;SAM&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#19981;&#20165;&#33021;&#25552;&#21319;&#27867;&#21270;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#38477;&#20302;&#29305;&#24449;&#30340;&#31209;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#21644;&#20219;&#21153;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16292</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#23548;&#33268;&#20302;&#31209;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization Leads to Low-Rank Features. (arXiv:2305.16292v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16292
&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;SAM&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#19981;&#20165;&#33021;&#25552;&#21319;&#27867;&#21270;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#38477;&#20302;&#29305;&#24449;&#30340;&#31209;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#21644;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25439;&#22833;&#38160;&#24230;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#27867;&#21270;&#24615;&#33021;&#25552;&#21319;&#24050;&#34987;&#24191;&#27867;&#35748;&#21487;&#24182;&#25104;&#20026;&#20027;&#35201;&#21160;&#26426;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;SAM&#30340;&#21478;&#19968;&#20010;&#26377;&#36259;&#25928;&#26524;&#65306;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#23618;&#32423;&#19978;&#21457;&#29983;&#29305;&#24449;&#31209;&#38477;&#20302;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20302;&#31209;&#25928;&#26524;&#30340;&#24191;&#27867;&#24615;&#65306;&#36866;&#29992;&#20110;&#20840;&#36830;&#25509;&#32593;&#32476;&#12289;&#21367;&#31215;&#32593;&#32476;&#12289;&#35270;&#35273;&#21464;&#25442;&#22120;&#31561;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#12289;&#20998;&#31867;&#12289;&#35821;&#35328;&#22270;&#20687;&#23545;&#27604;&#35757;&#32451;&#31561;&#19981;&#21516;&#30446;&#26631;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#23618;&#32593;&#32476;&#20013;&#25552;&#20379;&#20102;&#20302;&#31209;&#29305;&#24449;&#20135;&#29983;&#30340;&#26426;&#21046;&#24615;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;SAM&#23558;&#22823;&#37327;&#30340;&#28608;&#27963;&#23436;&#20840;&#20462;&#21098;&#25481;&#65292;&#30452;&#25509;&#23548;&#33268;&#31209;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#36825;&#19968;&#25928;&#26524;&#65292;&#24182;&#26816;&#26597;&#20102;&#23427;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#20063;&#21487;&#33021;&#21457;&#29983;&#65292;&#23613;&#31649;&#25972;&#20307;&#31209;&#20250;&#26377;&#30053;&#24494;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoWG&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#26082;&#39640;&#25928;&#21448;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#20110;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.16284</link><description>&lt;p&gt;
DoWG&#23637;&#31034;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#29992;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method. (arXiv:2305.16284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoWG&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#26082;&#39640;&#25928;&#21448;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#20110;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#20248;&#21270;&#22120;&#65306;DoWG&#65288;Weighted Gradients&#30340;&#36317;&#31163;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#8212;&#8212;&#22312;&#19981;&#35843;&#25972;&#20219;&#20309;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21305;&#37197;&#20248;&#21270;&#20984;&#20248;&#21270;&#20013;&#26368;&#20248;&#35843;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30452;&#21040;&#23545;&#25968;&#22240;&#23376;&#65292;&#24182;&#19988;&#26159;&#36890;&#29992;&#30340;&#8212;&#8212;&#33258;&#21160;&#36866;&#24212;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#12290;&#19982;AdaGrad&#65292;Adam&#25110;DoG&#31561;&#27969;&#34892;&#31639;&#27861;&#35745;&#31639;&#24179;&#26041;&#26799;&#24230;&#30340;&#36816;&#34892;&#24179;&#22343;&#20540;&#19981;&#21516;&#65292;DoWG&#20445;&#25345;&#36816;&#34892;&#24179;&#22343;&#20540;&#30340;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#21152;&#26435;&#29256;&#26412;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;DoWG&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#30340;&#26080;&#21442;&#25968;&#65292;&#39640;&#25928;&#21644;&#36890;&#29992;&#31639;&#27861;&#12290;&#23427;&#36824;&#26159;&#31532;&#19968;&#20010;&#36866;&#24212;&#20110;&#24179;&#31283;&#20248;&#21270;&#30340;&#26080;&#21442;&#25968;AdaGrad&#26679;&#24335;&#31639;&#27861;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;DoWG&#22312;&#31283;&#23450;&#30340;&#36793;&#32536;&#35757;&#32451;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new easy-to-implement parameter-free gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is efficient -- matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal -- automatically adapting to both smooth and nonsmooth problems. While popular algorithms such as AdaGrad, Adam, or DoG compute a running average of the squared gradients, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To our best knowledge, DoWG is the first parameter-free, efficient, and universal algorithm that does not require backtracking search procedures. It is also the first parameter-free AdaGrad style algorithm that adapts to smooth optimization. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#31454;&#20105;&#23545;&#25163;&#30340;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.16272</link><description>&lt;p&gt;
&#22312;&#21327;&#21516;&#23398;&#20064;&#21644;&#20248;&#21270;&#20013;&#28608;&#21169;&#31454;&#20105;&#23545;&#25163;&#35802;&#23454;&#34892;&#20026;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Honesty among Competitors in Collaborative Learning and Optimization. (arXiv:2305.16272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#31454;&#20105;&#23545;&#25163;&#30340;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#27604;&#20165;&#21033;&#29992;&#21333;&#19968;&#25968;&#25454;&#28304;&#30340;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#28508;&#22312;&#30340;&#21442;&#19982;&#32773;&#26159;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#22914;&#27599;&#20010;&#37117;&#24076;&#26395;&#36890;&#36807;&#25552;&#20379;&#26368;&#20339;&#25512;&#33616;&#26469;&#21560;&#24341;&#23458;&#25143;&#30340;&#20844;&#21496;&#12290;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#19981;&#35802;&#23454;&#30340;&#26356;&#26032;&#65292;&#25439;&#23475;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#33021;&#30772;&#22351;&#21327;&#20316;&#30340;&#22909;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#36825;&#31181;&#20132;&#20114;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#20869;&#30740;&#31350;&#20102;&#20004;&#20010;&#23398;&#20064;&#20219;&#21153;&#65306;&#21333;&#36718;&#22343;&#20540;&#20272;&#35745;&#21644;&#24378;&#20984;&#30446;&#26631;&#30340;&#22810;&#36718; SGD&#12290;&#23545;&#20110;&#19968;&#31867;&#33258;&#28982;&#30340;&#21442;&#19982;&#32773;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#29702;&#24615;&#30340;&#23458;&#25143;&#20250;&#34987;&#28608;&#21169;&#24378;&#28872;&#22320;&#25805;&#32437;&#20182;&#20204;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#38450;&#27490;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#31181;&#36328;&#32500;&#24230;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22788;&#29702;&#19981;&#21516;&#32500;&#24230;&#25968;&#25454;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#21644;&#25554;&#20540;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16261</link><description>&lt;p&gt;
&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#30340;&#36328;&#32500;&#24230;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Trans-Dimensional Generative Modeling via Jump Diffusion Models. (arXiv:2305.16261v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#31181;&#36328;&#32500;&#24230;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22788;&#29702;&#19981;&#21516;&#32500;&#24230;&#25968;&#25454;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#21644;&#25554;&#20540;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#29366;&#24577;&#21644;&#23610;&#23544;&#65292;&#33258;&#28982;&#22320;&#22788;&#29702;&#19981;&#21516;&#32500;&#24230;&#30340;&#25968;&#25454;&#12290;&#35813;&#29983;&#25104;&#36807;&#31243;&#34987;&#23450;&#20041;&#20026;&#22312;&#19981;&#21516;&#32500;&#24230;&#31354;&#38388;&#20043;&#38388;&#36827;&#34892;&#36339;&#36291;&#25193;&#25955;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#20010;&#30772;&#22351;&#23610;&#23544;&#30340;&#21069;&#21521;&#22122;&#22768;&#36807;&#31243;&#65292;&#28982;&#21518;&#25512;&#23548;&#20986;&#19968;&#20010;&#21019;&#36896;&#23610;&#23544;&#30340;&#36870;&#21521;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#39062;&#30340;&#35777;&#25454;&#19979;&#30028;&#35757;&#32451;&#30446;&#26631;&#26469;&#23398;&#20064;&#36924;&#36817;&#35813;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#27169;&#25311;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#36870;&#21521;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#20540;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32852;&#21512;&#29983;&#25104;&#29366;&#24577;&#20540;&#21644;&#23610;&#23544;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#31181;&#22788;&#29702;&#19981;&#21516;&#32500;&#24230;&#25968;&#25454;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20998;&#23376;&#21644;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#22266;&#23450;&#32500;&#24230;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#26356;&#22909;&#30340;&#19982;&#27979;&#35797;&#26102;&#25193;&#25955;&#24341;&#23548;&#25554;&#20540;&#20219;&#21153;&#20860;&#23481;&#24615;&#21644;&#25913;&#36827;&#30340;&#25554;&#20540;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new class of generative models that naturally handle data of varying dimensionality by jointly modeling the state and dimension of each datapoint. The generative process is formulated as a jump diffusion process that makes jumps between different dimensional spaces. We first define a dimension destroying forward noising process, before deriving the dimension creating time-reversed generative process along with a novel evidence lower bound training objective for learning to approximate it. Simulating our learned approximation to the time-reversed generative process then provides an effective way of sampling data of varying dimensionality by jointly generating state values and dimensions. We demonstrate our approach on molecular and video datasets of varying dimensionality, reporting better compatibility with test-time diffusion guidance imputation tasks and improved interpolation capabilities versus fixed dimensional models that generate state values and dimensions separate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman&#26680;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#35270;&#39057;&#39044;&#27979;&#21644;&#20132;&#36890;&#39044;&#27979;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#22343;&#26377;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#23398;&#20064;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.16215</link><description>&lt;p&gt;
Koopman&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Koopman Kernel Regression. (arXiv:2305.16215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16215
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman&#26680;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#35270;&#39057;&#39044;&#27979;&#21644;&#20132;&#36890;&#39044;&#27979;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#22343;&#26377;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#23398;&#20064;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20915;&#31574;&#21046;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#24378;&#21270;&#23398;&#20064;&#65292;&#20381;&#36182;&#20110;&#27169;&#25311;&#22120;&#25110;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#24863;&#20852;&#36259;&#30340;&#37327;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#20363;&#22914;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#25110;&#31574;&#30053;&#30340;&#22870;&#21169;&#12290;&#36825;&#20123;&#22797;&#26434;&#29616;&#35937;&#30340;&#39044;&#27979;&#36890;&#24120;&#30001;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#21160;&#21147;&#31995;&#32479;&#25551;&#36848;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#22522;&#20110;&#20248;&#21270;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20351;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;Koopman&#31639;&#23376;&#29702;&#35770;&#36890;&#36807;&#36890;&#36807;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#25551;&#36848;&#39044;&#27979;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#31995;&#32479;&#20998;&#26512;&#21644;&#38271;&#26399;&#39044;&#27979;&#21464;&#24471;&#31616;&#21333;--&#21482;&#28041;&#21450;&#30697;&#38453;&#20056;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#32447;&#24615;&#31995;&#32479;&#36890;&#24120;&#26159;&#38750;&#24179;&#20961;&#30340;&#21644;&#26410;&#30693;&#30340;&#65292;&#38656;&#35201;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#20851;&#38190;&#30340;&#23398;&#20064;&#29702;&#35770;&#20445;&#35777;&#65292;&#22240;&#27492;&#25152;&#33719;&#24471;&#30340;&#27169;&#22411;&#22312;&#25968;&#25454;&#21644;&#32500;&#24230;&#22686;&#21152;&#26102;&#30340;&#34892;&#20026;&#36890;&#24120;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Koopman&#26680;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#30452;&#25509;&#20174;&#21382;&#21490;&#35266;&#23519;&#20013;&#23398;&#20064;&#21040;&#26410;&#26469;&#39044;&#27979;&#22312;Koopman&#31639;&#23376;&#31354;&#38388;&#20013;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20139;&#26377;&#21487;&#35777;&#26126;&#30340;&#23398;&#20064;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#21305;&#37197;&#65288;&#25110;&#20248;&#20110;&#65289;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#35270;&#39057;&#39044;&#27979;&#21644;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging. Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear dynamical systems. This makes system analysis and long-term predictions simple -- involving only matrix multiplications. However, the transformation to a linear system is generally non-trivial and unknown, requiring learning-based approaches. While there exists a variety of approaches, they usually lack crucial learning-theoretic guarantees, such that the behavior of the obtained models with increasing data and dimensionality is often unclear. We address the aforemention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Riemannian&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#21152;&#36895;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#20960;&#20309;&#20984;&#20248;&#21270;&#30340;&#26032;&#32467;&#26524;&#12290;&#36890;&#36807;&#21435;&#38500;&#20851;&#20110;&#36845;&#20195;&#22312;&#39044;&#23450;&#32039;&#33268;&#38598;&#21512;&#20869;&#30340;&#20551;&#35774;&#65292;&#23436;&#21892;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.16186</link><description>&lt;p&gt;
&#29992;&#20110;&#20445;&#35777;&#26377;&#30028;&#20960;&#20309;&#24809;&#32602;&#30340;Riemannian Min-Max&#20248;&#21270;&#21152;&#36895;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerated Methods for Riemannian Min-Max Optimization Ensuring Bounded Geometric Penalties. (arXiv:2305.16186v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Riemannian&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#21152;&#36895;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#20960;&#20309;&#20984;&#20248;&#21270;&#30340;&#26032;&#32467;&#26524;&#12290;&#36890;&#36807;&#21435;&#38500;&#20851;&#20110;&#36845;&#20195;&#22312;&#39044;&#23450;&#32039;&#33268;&#38598;&#21512;&#20869;&#30340;&#20551;&#35774;&#65292;&#23436;&#21892;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24418;&#24335;&#20026;$\min_x \max_y f(x, y)$&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;$f(x, y)$&#23450;&#20041;&#22312;&#20056;&#31215;Riemannian&#27969;&#24418;$\mathcal{M} \times \mathcal{N}$&#19978;&#65292;&#24182;&#19988;&#22312;$x$&#26041;&#21521;&#19978;$\mu_x$-&#24378;&#27979;&#22320;&#20984;&#65288;g-convex&#65289;&#65292;&#22312;$y$&#26041;&#21521;&#19978;$\mu_y$-&#24378;g-&#20985;&#65292;&#20854;&#20013;$\mu_x, \mu_y \geq 0$&#12290;&#24403;$f$&#26159;$(L_x, L_y, L_{xy})$-&#24179;&#28369;&#30340;&#65292;&#24182;&#19988;$\mathcal{M}$&#65292;$\mathcal{N}$&#26159;Hadamard&#27969;&#24418;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21152;&#36895;&#26041;&#27861;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20309;&#20984;&#20248;&#21270;&#30340;&#26032;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24230;&#37327;&#25237;&#24433;Riemannian&#26799;&#24230;&#19979;&#38477;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#36890;&#36807;&#20943;&#23567;&#20960;&#20309;&#24120;&#25968;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#21152;&#36895;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21435;&#38500;&#19968;&#31181;&#20851;&#20110;&#36845;&#20195;&#22312;&#39044;&#23450;&#32039;&#33268;&#38598;&#21512;&#20869;&#30340;&#20551;&#35774;&#65292;&#23436;&#25104;&#20102;&#20004;&#20010;&#20043;&#21069;&#24212;&#29992;&#20110;Riemannian min-max&#24773;&#20917;&#30340;&#24037;&#20316;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study optimization problems of the form $\min_x \max_y f(x, y)$, where $f(x, y)$ is defined on a product Riemannian manifold $\mathcal{M} \times \mathcal{N}$ and is $\mu_x$-strongly geodesically convex (g-convex) in $x$ and $\mu_y$-strongly g-concave in $y$, for $\mu_x, \mu_y \geq 0$. We design accelerated methods when $f$ is $(L_x, L_y, L_{xy})$-smooth and $\mathcal{M}$, $\mathcal{N}$ are Hadamard. To that aim we introduce new g-convex optimization results, of independent interest: we show global linear convergence for metric-projected Riemannian gradient descent and improve existing accelerated methods by reducing geometric constants. Additionally, we complete the analysis of two previous works applying to the Riemannian min-max case by removing an assumption about iterates staying in a pre-specified compact set.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#25112;&#30053;&#25968;&#25454;&#20849;&#20139;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24066;&#22330;&#26465;&#20214;&#23545;&#25968;&#25454;&#20849;&#20139;&#28608;&#21169;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16052</link><description>&lt;p&gt;
&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#30340;&#25112;&#30053;&#25968;&#25454;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Strategic Data Sharing between Competitors. (arXiv:2305.16052v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16052
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#25112;&#30053;&#25968;&#25454;&#20849;&#20139;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24066;&#22330;&#26465;&#20214;&#23545;&#25968;&#25454;&#20849;&#20139;&#28608;&#21169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21327;&#20316;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#32452;&#32455;&#20043;&#38388;&#30340;&#31169;&#23494;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20225;&#19994;&#22312;&#32771;&#34385;&#19982;&#31454;&#20105;&#23545;&#25163;&#20849;&#20139;&#25968;&#25454;&#26102;&#38754;&#20020;&#30528;&#22256;&#22659;&#8212;&#8212;&#34429;&#28982;&#21327;&#20316;&#21487;&#20197;&#25913;&#21892;&#20844;&#21496;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#20063;&#21487;&#33021;&#20351;&#31454;&#20105;&#23545;&#25163;&#21463;&#30410;&#65292;&#20174;&#32780;&#38477;&#20302;&#21033;&#28070;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#36825;&#31181;&#25968;&#25454;&#20849;&#20139;&#26435;&#34913;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#20998;&#21035;&#20195;&#34920;&#20225;&#19994;&#30340;&#29983;&#20135;&#20915;&#31574;&#12289;&#39069;&#22806;&#25968;&#25454;&#23545;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#20197;&#21450;&#25968;&#25454;&#20849;&#20139;&#21327;&#21830;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#20013;&#30340;&#20256;&#32479;&#24066;&#22330;&#27169;&#22411;&#30340;&#26694;&#26550;&#23454;&#20363;&#65292;&#20197;&#30830;&#23450;&#24433;&#21709;&#21327;&#20316;&#28608;&#21169;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#24066;&#22330;&#26465;&#20214;&#23545;&#25968;&#25454;&#20849;&#20139;&#28608;&#21169;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24066;&#22330;&#31454;&#20105;&#30340;&#20943;&#23569;&#65292;&#21363;&#20225;&#19994;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning techniques have significantly advanced in recent years, enabling private model training across multiple organizations. Despite this opportunity, firms face a dilemma when considering data sharing with competitors -- while collaboration can improve a company's machine learning model, it may also benefit competitors and hence reduce profits. In this work, we introduce a general framework for analyzing this data-sharing trade-off. The framework consists of three components, representing the firms' production decisions, the effect of additional data on model quality, and the data-sharing negotiation process, respectively. We then study an instantiation of the framework, based on a conventional market model from economic theory, to identify key factors that affect collaboration incentives. Our findings indicate a profound impact of market conditions on the data-sharing incentives. In particular, we find that reduced competition, in terms of the similarities between th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15944</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23558;&#24744;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#21487;&#29992;&#20316;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#25104;&#20026;&#30005;&#36335;&#24418;&#24335;--&#36825;&#26159;&#19968;&#31181;&#20801;&#35768;&#26377;&#25928;&#36793;&#38469;&#21270;&#30340;&#32422;&#26463;&#35745;&#31639;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#29983;&#25104;&#30005;&#36335;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#28608;&#27963;&#38480;&#21046;&#20026;&#38750;&#36127;&#25968;&#65292;&#21478;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#36755;&#20986;&#24179;&#26041;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#19981;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#33410;&#28857;&#36830;&#36793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30005;&#36335;&#26694;&#26550;&#20351;&#24471;MLE&#30340;&#31934;&#30830;&#23398;&#20064;&#12289;&#26032;&#19977;&#20803;&#32452;&#30340;&#26377;&#25928;&#25277;&#26679;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#24471;&#20197;&#28385;&#36275;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#30340;&#22270;&#19978;&#27604;&#21407;&#22987;&#30340;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.15759</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30452;&#25509;&#22312;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;DMs&#30340;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#32452;&#21512;&#24615;&#23646;&#24615;&#65292;&#22823;&#37327;&#22122;&#38899;&#27880;&#20837;&#21040;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#12290;LDMs&#20351;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20943;&#23569;&#21040;&#26356;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#35757;&#32451;DMs&#26356;&#21152;&#39640;&#25928;&#21644;&#24555;&#36895;&#12290;&#19982;[Ghalebikesabi&#31561;&#20154;&#65292;2023]&#39044;&#20808;&#29992;&#20844;&#20849;&#25968;&#25454;&#39044;&#35757;&#32451;DMs&#65292;&#28982;&#21518;&#20877;&#29992;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#20165;&#24494;&#35843;LDMs&#20013;&#19981;&#21516;&#23618;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#20197;&#33719;&#24471;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#25972;&#20010;DM&#24494;&#35843;&#65292;&#21487;&#20943;&#23569;&#22823;&#32422;96%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861; - &#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COT)&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#22522;&#20110;&#32463;&#39564;&#30340;&#21464;&#20307; - &#24102;&#38376;&#38480;&#30340;&#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COTT)&#65292;&#23427;&#20204;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20266;&#26631;&#31614;&#36716;&#31227;&#35823;&#24046;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.15640</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#34920;&#24449;&#21306;&#20998;&#20110;&#20998;&#24067;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Characterizing Out-of-Distribution Error via Optimal Transport. (arXiv:2305.15640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861; - &#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COT)&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#22522;&#20110;&#32463;&#39564;&#30340;&#21464;&#20307; - &#24102;&#38376;&#38480;&#30340;&#32622;&#20449;&#26368;&#20248;&#36755;&#36816;(COTT)&#65292;&#23427;&#20204;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20266;&#26631;&#31614;&#36716;&#31227;&#35823;&#24046;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#20013;&#65292;&#27809;&#22312;&#20998;&#24067;(out-of-distribution)&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#39044;&#27979;&#27169;&#22411;&#22312;&#27809;&#26631;&#31614;&#30340;o
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models, so methods of predicting a model's performance on OOD data without labels are important for machine learning safety. While a number of methods have been proposed by prior work, they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks. In this work, we identify pseudo-label shift, or the difference between the predicted and true OOD label distributions, as a key indicator to this underestimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo-label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25913;&#36827;&#20102;&#20855;&#26377;&#24378;&#21487;&#35266;&#23519;&#26080;&#21521;&#21453;&#39304;&#22270;&#30340;&#22312;&#32447;&#23398;&#20064;&#36951;&#25022;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;$\alpha$&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#19982;&#24378;&#30423;&#26696;&#20363;&#21644;&#19987;&#23478;&#26696;&#20363;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#24182;&#20013;&#38388;&#25554;&#20540;&#65292;&#35777;&#26126;&#36807;&#31243;&#20351;&#29992;&#20102;&#29305;&#23450;&#20540;$q \in [1/2, 1)$&#38543;$\alpha$&#21464;&#21270;&#30340;FTRL&#21644;$q$-Tsallis&#29109;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15383</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#21453;&#39304;&#22270;&#30340;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;Minimax&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
On the Minimax Regret for Online Learning with Feedback Graphs. (arXiv:2305.15383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25913;&#36827;&#20102;&#20855;&#26377;&#24378;&#21487;&#35266;&#23519;&#26080;&#21521;&#21453;&#39304;&#22270;&#30340;&#22312;&#32447;&#23398;&#20064;&#36951;&#25022;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;$\alpha$&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#19982;&#24378;&#30423;&#26696;&#20363;&#21644;&#19987;&#23478;&#26696;&#20363;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#24182;&#20013;&#38388;&#25554;&#20540;&#65292;&#35777;&#26126;&#36807;&#31243;&#20351;&#29992;&#20102;&#29305;&#23450;&#20540;$q \in [1/2, 1)$&#38543;$\alpha$&#21464;&#21270;&#30340;FTRL&#21644;$q$-Tsallis&#29109;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#20855;&#26377;&#24378;&#21487;&#35266;&#23519;&#26080;&#21521;&#21453;&#39304;&#22270;&#30340;&#22312;&#32447;&#23398;&#20064;&#36951;&#25022;&#30340;&#19978;&#19979;&#30028;&#12290;&#35813;&#38382;&#39064;&#30340;&#24050;&#30693;&#26368;&#20248;&#19978;&#30028;&#20026;$\mathcal {O}\bigl(\sqrt{\alpha T\ln K}\bigr)$&#65292;&#20854;&#20013;$K$&#26159;&#34892;&#21160;&#25968;&#37327;&#65292;$\alpha$&#26159;&#22270;&#30340;&#29420;&#31435;&#25968;&#65292;$T$&#26159;&#26102;&#38388;&#33539;&#22260;&#12290; $\sqrt{\ln K}$&#22240;&#23376;&#22312;$\alpha=1$&#65288;&#19987;&#23478;&#26696;&#20363;&#65289;&#26102;&#34987;&#35748;&#20026;&#26159;&#24517;&#35201;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;$\alpha=K$&#65288;&#24378;&#30423;&#26696;&#20363;&#65289;&#26102;&#65292;Minimax&#29575;&#20026;$\Theta\bigl(\sqrt{KT}\bigr)$&#65292;&#24182;&#19988;&#24050;&#30693;&#23545;&#20110;&#20219;&#20309;$\alpha$&#65292;&#19979;&#30028;&#20026;$\Omega\bigl(\sqrt{\alpha T}\bigr)$&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#19978;&#30028;$\mathcal {O}\bigl(\sqrt{\alpha T(1+\ln(K/\alpha))}\bigr)$&#36866;&#29992;&#20110;&#20219;&#20309;$\alpha$&#65292;&#19982;&#24378;&#30423;&#21644;&#19987;&#23478;&#30340;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#24182;&#25554;&#20540;&#20013;&#38388;&#26696;&#20363;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20855;&#26377;&#29305;&#23450;&#20540;$q \in [1/2, 1)$&#38543;$\alpha$&#21464;&#21270;&#30340;FTRL&#21644;$q$-Tsallis&#29109;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we improve on the upper and lower bounds for the regret of online learning with strongly observable undirected feedback graphs. The best known upper bound for this problem is $\mathcal{O}\bigl(\sqrt{\alpha T\ln K}\bigr)$, where $K$ is the number of actions, $\alpha$ is the independence number of the graph, and $T$ is the time horizon. The $\sqrt{\ln K}$ factor is known to be necessary when $\alpha = 1$ (the experts case). On the other hand, when $\alpha = K$ (the bandits case), the minimax rate is known to be $\Theta\bigl(\sqrt{KT}\bigr)$, and a lower bound $\Omega\bigl(\sqrt{\alpha T}\bigr)$ is known to hold for any $\alpha$. Our improved upper bound $\mathcal{O}\bigl(\sqrt{\alpha T(1+\ln(K/\alpha))}\bigr)$ holds for any $\alpha$ and matches the lower bounds for bandits and experts, while interpolating intermediate cases. To prove this result, we use FTRL with $q$-Tsallis entropy for a carefully chosen value of $q \in [1/2, 1)$ that varies with $\alpha$. The analysis of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#20351;&#29992;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#23454;&#29616;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.15349</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Black-Box Variational Inference Converges. (arXiv:2305.15349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15349
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#20351;&#29992;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#23454;&#29616;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20063;&#31216;&#20026;&#33945;&#29305;&#21345;&#32599;&#21464;&#20998;&#25512;&#26029;&#12290;&#23613;&#31649;&#26089;&#26399;&#30340;&#30740;&#31350;&#21482;&#38024;&#23545;&#31616;&#21270;&#29256;&#26412;&#30340;BBVI&#36827;&#34892;&#20102;&#30740;&#31350;&#65288;&#20363;&#22914;&#65292;&#26377;&#30028;&#22495;&#12289;&#26377;&#30028;&#25903;&#25345;&#12289;&#20165;&#38024;&#23545;&#23610;&#24230;&#36827;&#34892;&#20248;&#21270;&#31561;&#65289;&#65292;&#20294;&#25105;&#20204;&#30340;&#35774;&#32622;&#19981;&#38656;&#35201;&#20219;&#20309;&#36825;&#26679;&#30340;&#31639;&#27861;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#23545;&#25968;&#24179;&#28369;&#21518;&#39564;&#23494;&#24230;&#65292;&#26080;&#35770;&#26159;&#21542;&#24378;&#23545;&#25968;&#20985;&#24615;&#20197;&#21450;&#20301;&#32622;-&#23610;&#24230;&#21464;&#20998;&#26063;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#65292;&#29305;&#21035;&#26159;&#21464;&#20998;&#36817;&#20284;&#23610;&#24230;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36816;&#34892;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#32416;&#27491;&#36825;&#20123;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24050;&#30693;&#30340;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36817;&#31471;SGD&#19982;&#20854;&#20182;&#26631;&#20934;&#30340;BBVI&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#29702;&#35770;&#32467;&#35770;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide the first convergence guarantee for full black-box variational inference (BBVI), also known as Monte Carlo variational inference. While preliminary investigations worked on simplified versions of BBVI (e.g., bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. Our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. Also, our analysis reveals that certain algorithm design choices commonly employed in practice, particularly, nonlinear parameterizations of the scale of the variational approximation, can result in suboptimal convergence rates. Fortunately, running BBVI with proximal stochastic gradient descent fixes these limitations, and thus achieves the strongest known convergence rate guarantees. We evaluate this theoretical insight by comparing proximal SGD against other standard implementations of BBVI on large-scale
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65288;EBFlow&#65289;&#65292;&#36890;&#36807;&#24471;&#20998;&#21305;&#37197;&#30446;&#26631;&#20248;&#21270;&#20351;&#20854;&#35757;&#32451;&#26356;&#39640;&#25928;&#65292;&#21516;&#26102;&#24320;&#21457;&#19968;&#20123;&#25216;&#26415;&#22686;&#24378;EBFlow&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#23454;&#35777;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15267</link><description>&lt;p&gt;
&#35757;&#32451;&#22522;&#20110;&#33021;&#37327;&#30340;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#30340;&#24471;&#20998;&#21305;&#37197;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Training Energy-Based Normalizing Flow with Score-Matching Objectives. (arXiv:2305.15267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65288;EBFlow&#65289;&#65292;&#36890;&#36807;&#24471;&#20998;&#21305;&#37197;&#30446;&#26631;&#20248;&#21270;&#20351;&#20854;&#35757;&#32451;&#26356;&#39640;&#25928;&#65292;&#21516;&#26102;&#24320;&#21457;&#19968;&#20123;&#25216;&#26415;&#22686;&#24378;EBFlow&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#27969;&#27169;&#22411;&#21644;&#33021;&#37327;&#27169;&#22411;&#21442;&#25968;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#24402;&#19968;&#21270;&#27969;&#24314;&#27169;&#26041;&#27861;&#65288;EBFlow&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24471;&#20998;&#21305;&#37197;&#30446;&#26631;&#20248;&#21270;EBFlow&#65292;&#21487;&#20197;&#23436;&#20840;&#36991;&#24320;&#32447;&#24615;&#21464;&#25442;&#30340;&#38597;&#21487;&#27604;&#34892;&#21015;&#24335;&#35745;&#31639;&#12290;&#36825;&#20351;&#24471;EBFlow&#22312;&#26500;&#24314;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#26102;&#20351;&#29992;&#20219;&#24847;&#32447;&#24615;&#23618;&#65292;&#32780;&#19981;&#20250;&#20351;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#30340;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}(D^2L)$&#22686;&#21152;&#21040;$\mathcal{O}(D^3L)$&#65292;&#20854;&#20013;$L$&#20026;&#23618;&#25968;&#65292;$D$&#20026;&#36755;&#20837;&#32500;&#24230;&#12290;&#36825;&#20351;&#24471;EBFlow&#30340;&#35757;&#32451;&#27604;&#24120;&#29992;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;&#38500;&#20102;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#20998;&#20540;&#21305;&#37197;&#26041;&#27861;&#30340;&#20998;&#26512;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#20197;&#22686;&#24378;EBFlow&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish a connection between the parameterization of flow-based and energy-based generative models, and present a new flow-based modeling approach called energy-based normalizing flow (EBFlow). We demonstrate that by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be entirely bypassed. This feature enables the use of arbitrary linear layers in the construction of flow-based models without increasing the computational time complexity of each training iteration from $\mathcal{O}(D^2L)$ to $\mathcal{O}(D^3L)$ for an $L$-layered model that accepts $D$-dimensional inputs. This makes the training of EBFlow more efficient than the commonly-adopted maximum likelihood training method. In addition to the reduction in runtime, we enhance the training stability and empirical performance of EBFlow through a number of techniques developed based on our analysis on the score-matching methods. The experimental
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#21160;&#37327;&#21487;&#20197;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35823;&#24046;&#21453;&#39304;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#25209;&#27425;&#22823;&#23567;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15155</link><description>&lt;p&gt;
&#21160;&#37327;&#34987;&#35777;&#26126;&#21487;&#20197;&#25913;&#21892;&#35823;&#24046;&#21453;&#39304;&#65281;
&lt;/p&gt;
&lt;p&gt;
Momentum Provably Improves Error Feedback!. (arXiv:2305.15155v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15155
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#21160;&#37327;&#21487;&#20197;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35823;&#24046;&#21453;&#39304;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#25209;&#27425;&#22823;&#23567;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#23384;&#22312;&#36739;&#39640;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#29616;&#20195;&#31639;&#27861;&#24635;&#26159;&#20381;&#36182;&#20110;&#26377;&#25439;&#21387;&#32553;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#21152;&#22788;&#29702;&#65292;&#21387;&#32553;&#24341;&#36215;&#30340;&#35823;&#24046;&#20250;&#20256;&#25773;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#19981;&#31283;&#23450;&#34892;&#20026;&#65292;&#21253;&#25324;&#25351;&#25968;&#32423;&#21457;&#25955;&#12290;&#22823;&#32422;&#21313;&#24180;&#21069;&#65292;Seide&#31561;&#20154;[2014]&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EF14&#30340;&#38169;&#35823;&#21453;&#39304;&#65288;EF&#65289;&#26426;&#21046;&#65292;&#20316;&#20026;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26497;&#20854;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;EF&#39046;&#22495;&#22312;&#31639;&#27861;&#21644;&#29702;&#35770;&#26041;&#38754;&#26377;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#23545;&#38382;&#39064;&#30340;&#29702;&#35299;&#36824;&#36828;&#26410;&#23436;&#21892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#32463;&#20856;&#30340;&#38750;&#20984;&#35774;&#32622;&#20013;&#65292;&#25152;&#26377;&#24050;&#30693;&#30340;EF&#21464;&#31181;&#37117;&#20381;&#36182;&#20110;&#38750;&#24120;&#22823;&#30340;&#25209;&#27425;&#22823;&#23567;&#25165;&#33021;&#25910;&#25947;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26159;&#31105;&#27490;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#22320;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#28040;&#38500;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#23558;Polyak&#30340;&#21160;&#37327;&#24212;&#29992;&#21040;&#26368;&#26032;&#30340;i
&lt;/p&gt;
&lt;p&gt;
Due to the high communication overhead when training machine learning models in a distributed environment, modern algorithms invariably rely on lossy communication compression. However, when untreated, the errors caused by compression propagate, and can lead to severely unstable behavior, including exponential divergence. Almost a decade ago, Seide et al [2014] proposed an error feedback (EF) mechanism, which we refer to as EF14, as an immensely effective heuristic for mitigating this issue. However, despite steady algorithmic and theoretical advances in the EF field in the last decade, our understanding is far from complete. In this work we address one of the most pressing issues. In particular, in the canonical nonconvex setting, all known variants of EF rely on very large batch sizes to converge, which can be prohibitive in practice. We propose a surprisingly simple fix which removes this issue both theoretically, and in practice: the application of Polyak's momentum to the latest i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15121</link><description>&lt;p&gt;
&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#36827;&#34892;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#36229;&#36234;&#20010;&#20307;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Individual Input for Deep Anomaly Detection on Tabular Data. (arXiv:2305.15121v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#65288;NPTs&#65289;&#30340;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#22522;&#20110;&#37325;&#26500;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;NPT&#26469;&#37325;&#26500;&#27491;&#24120;&#26679;&#26412;&#30340;&#36974;&#34109;&#29305;&#24449;&#12290;&#20197;&#38750;&#21442;&#25968;&#21270;&#26041;&#24335;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25972;&#20010;&#35757;&#32451;&#38598;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#22312;&#29983;&#25104;&#24322;&#24120;&#24471;&#20998;&#26102;&#37325;&#26500;&#36974;&#34109;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#25104;&#21151;&#32467;&#21512;&#29305;&#24449;&#20043;&#38388;&#21644;&#26679;&#26412;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;31&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#24471;&#20998;&#21644;AUROC&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is crucial in various domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train the NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features during to generate an anomaly score. To the best of our knowledge, our proposed method is the first to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. We evaluate our method on an extensive benchmark of 31 tabular datasets and demonstrate that our approach outperforms existing state-of-the-art methods based on the F1-score and AUROC by a signifi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#28508;&#22312;&#34920;&#31034;&#30340;&#22359;&#23616;&#37096;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#22359;&#24182;&#24341;&#20837;&#21453;&#39304;&#32593;&#32476;&#65292;&#22312;&#35299;&#20915;&#38145;&#23450;&#21644;&#26435;&#37325;&#20256;&#36755;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24182;&#34892;&#21270;&#21644;&#27700;&#24179;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14974</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#28508;&#22312;&#34920;&#31034;&#30340;&#22359;&#23616;&#37096;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Block-local learning with probabilistic latent representations. (arXiv:2305.14974v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#28508;&#22312;&#34920;&#31034;&#30340;&#22359;&#23616;&#37096;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#22359;&#24182;&#24341;&#20837;&#21453;&#39304;&#32593;&#32476;&#65292;&#22312;&#35299;&#20915;&#38145;&#23450;&#21644;&#26435;&#37325;&#20256;&#36755;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24182;&#34892;&#21270;&#21644;&#27700;&#24179;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#38656;&#35201;&#36890;&#36807;&#32593;&#32476;&#36827;&#34892;&#39034;&#24207;&#26356;&#26032;&#65292;&#23548;&#33268;&#20102;&#38145;&#23450;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#21453;&#21521;&#20256;&#25773;&#20381;&#36182;&#20110;&#21069;&#21521;&#26435;&#37325;&#30697;&#38453;&#30340;&#36716;&#32622;&#26469;&#35745;&#31639;&#26356;&#26032;&#65292;&#23548;&#33268;&#20102;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#20256;&#36755;&#38382;&#39064;&#12290;&#38145;&#23450;&#21644;&#26435;&#37325;&#20256;&#36755;&#38382;&#39064;&#38459;&#30861;&#20102;&#35757;&#32451;&#36807;&#31243;&#30340;&#39640;&#25928;&#24182;&#34892;&#21270;&#21644;&#27700;&#24179;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21453;&#39304;&#32593;&#32476;&#65292;&#20174;&#30446;&#26631;&#21521;&#21518;&#20256;&#25773;&#20449;&#24687;&#20197;&#25552;&#20379;&#36741;&#21161;&#30340;&#23616;&#37096;&#25439;&#22833;&#12290;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#21487;&#20197;&#24182;&#34892;&#36827;&#34892;&#65292;&#24182;&#19988;&#20351;&#29992;&#19981;&#21516;&#30340;&#26435;&#37325;&#38598;&#65292;&#35299;&#20915;&#20102;&#38145;&#23450;&#21644;&#26435;&#37325;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#20110;&#19968;&#31181;&#32479;&#35745;&#35299;&#37322;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#32593;&#32476;&#22359;&#30340;&#36755;&#20986;&#28608;&#27963;&#35270;&#20026;&#27010;&#29575;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquitous backpropagation algorithm requires sequential updates through the network introducing a locking problem. In addition, back-propagation relies on the transpose of forward weight matrices to compute updates, introducing a weight transport problem across the network. Locking and weight transport are problems because they prevent efficient parallelization and horizontal scaling of the training process. We propose a new method to address both these problems and scale up the training of large models. Our method works by dividing a deep neural network into blocks and introduces a feedback network that propagates the information from the targets backwards to provide auxiliary local losses. Forward and backward propagation can operate in parallel and with different sets of weights, addressing the problems of locking and weight transport. Our approach derives from a statistical interpretation of training that treats output activations of network blocks as parameters of probability
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23547;&#27714;&#35299;&#20915;&#20551;&#26032;&#38395;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27867;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12289;&#28201;&#24230;&#12289;&#25552;&#31034;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#38469;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;LIAR-New&#65292;&#20026;&#20449;&#24687;&#30495;&#23454;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2305.14928</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#38752;&#30340;&#20551;&#26032;&#38395;&#32531;&#35299;&#65306;&#27867;&#21270;&#65292;&#19981;&#30830;&#23450;&#24615;&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23547;&#27714;&#35299;&#20915;&#20551;&#26032;&#38395;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27867;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12289;&#28201;&#24230;&#12289;&#25552;&#31034;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#38469;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;LIAR-New&#65292;&#20026;&#20449;&#24687;&#30495;&#23454;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#31038;&#20250;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23578;&#26410;&#25214;&#21040;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20851;&#27880;&#27867;&#21270;&#65292;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#26080;&#27861;&#23436;&#32654;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26356;&#23454;&#29992;&#30340;&#24037;&#20855;&#26469;&#35780;&#20272;&#20449;&#24687;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;GPT-4&#22312;&#22810;&#20010;&#35774;&#23450;&#21644;&#35821;&#35328;&#20013;&#21487;&#20197;&#32988;&#36807;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#32034;&#27867;&#21270;&#65292;&#25581;&#31034;&#20102;GPT-4&#21644;RoBERTa-large&#22312;&#22833;&#25928;&#27169;&#24335;&#19978;&#30340;&#24046;&#24322;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#19981;&#21487;&#33021;&#30340;&#20363;&#23376;&#24182;&#26174;&#33879;&#25913;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#65292;&#28201;&#24230;&#65292;&#25552;&#31034;&#65292;&#29256;&#26412;&#25511;&#21046;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#30340;&#32467;&#26524;&#65292;&#27599;&#20010;&#32467;&#26524;&#37117;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20855;&#26377;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#21644;&#21487;&#34892;&#24615;&#26631;&#31614;&#30340;LIAR-New&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#26631;&#35821;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#22788;&#29702;&#26356;&#21152;&#22797;&#26434;&#30340;&#36755;&#20837;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#26576;&#20123;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#24674;&#22797;&#24615;&#33021;&#33021;&#22815;&#39044;&#27979;&#22810;&#27169;&#24577;&#21305;&#37197;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14897</link><description>&lt;p&gt;
&#25991;&#26412;&#32534;&#30721;&#22120;&#38480;&#21046;&#20102;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Text encoders bottleneck compositionality in contrastive vision-language models. (arXiv:2305.14897v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#26631;&#35821;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#22788;&#29702;&#26356;&#21152;&#22797;&#26434;&#30340;&#36755;&#20837;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#26576;&#20123;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#24674;&#22797;&#24615;&#33021;&#33021;&#22815;&#39044;&#27979;&#22810;&#27169;&#24577;&#21305;&#37197;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VL&#65289;&#22914;CLIP&#20351;&#29992;&#21333;&#19968;&#21521;&#37327;&#34920;&#31034;&#26631;&#39064;&#12290;&#22312;&#36825;&#20010;&#29942;&#39048;&#20013;&#22833;&#21435;&#20102;&#22810;&#23569;&#20851;&#20110;&#35821;&#35328;&#30340;&#20449;&#24687;&#65311;&#25105;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;CompPrompts&#65292;&#36825;&#26159;&#19968;&#32452;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#22270;&#20687;&#26631;&#39064;&#65292;VL&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25429;&#25417;&#21040;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#23545;&#35937;&#65292;&#21040;&#23545;&#35937;+&#23646;&#24615;&#65292;&#21040;&#22810;&#20010;&#20114;&#21160;&#23545;&#35937;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#24674;&#22797;&#25506;&#38024;&#65292;&#26088;&#22312;&#20174;&#20960;&#20010;VL&#27169;&#22411;&#29983;&#25104;&#30340;&#21333;&#19968;&#21521;&#37327;&#25991;&#26412;&#34920;&#31034;&#20013;&#37325;&#24314;&#26631;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#22270;&#20687;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;CLIP&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21253;&#25324;&#23545;&#35937;&#20851;&#31995;&#12289;&#23646;&#24615;-&#23545;&#35937;&#20851;&#32852;&#12289;&#35745;&#25968;&#21644;&#21542;&#23450;&#65307;2&#65289;&#19968;&#20123;&#25991;&#26412;&#32534;&#30721;&#22120;&#27604;&#20854;&#20182;&#32534;&#30721;&#22120;&#35201;&#22909;&#24471;&#22810;&#65307;3&#65289;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#24674;&#22797;&#24615;&#33021;&#39044;&#27979;&#20102;ControlledImCaps&#19978;&#30340;&#22810;&#27169;&#24577;&#21305;&#37197;&#24615;&#33021;&#65306;&#36825;&#26159;&#25105;&#20204;&#25910;&#38598;&#21644;&#21457;&#24067;&#30340;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP's text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multi-modal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JTFT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20943;&#23567;&#35745;&#31639;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20302;&#31209;&#27880;&#24847;&#23618;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14649</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32852;&#21512;&#26102;&#39057;&#22495;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting. (arXiv:2305.14649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JTFT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20943;&#23567;&#35745;&#31639;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20302;&#31209;&#27880;&#24847;&#23618;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#21512;&#26102;&#39057;&#22495;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#38656;&#27714;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#39057;&#29575;&#30340;&#31232;&#30095;&#24615;&#65292;&#22312;&#39057;&#22495;&#26377;&#25928;&#22320;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#38500;&#20102;&#39057;&#29575;&#22495;&#34920;&#31034;&#22806;&#65292;&#26368;&#36817;&#30340;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#20063;&#34987;&#30452;&#25509;&#32534;&#30721;&#22312;&#26102;&#38388;&#22495;&#20013;&#65292;&#20197;&#22686;&#24378;&#23398;&#20064;&#23616;&#37096;&#20851;&#31995;&#24182;&#20943;&#36731;&#38750;&#24179;&#31283;&#24615;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;JTFT&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#22240;&#20026;&#20869;&#37096;&#34920;&#31034;&#30340;&#38271;&#24230;&#20445;&#25345;&#29420;&#31435;&#20110;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#31209;&#27880;&#24847;&#23618;&#65292;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#38450;&#27490;&#30001;&#20110;&#26102;&#38388;&#21644;&#36890;&#36947;&#24314;&#27169;&#30340;&#32416;&#32544;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#32423;&#12290; &#23545;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;JTFT&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance predicting performance while minimizing computational demands, this paper introduces a joint time-frequency domain Transformer (JTFT) for multivariate forecasting. The method exploits the sparsity of time series in the frequency domain using a small number of learnable frequencies to extract temporal dependencies effectively. Alongside the frequency domain representation, a fixed number of the most recent data points are directly encoded in the time domain, bolstering the learning of local relationships and mitigating the adverse effects of non-stationarity. JTFT achieves linear complexity since the length of the internal representation remains independent of the input sequence length. Additionally, a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies and prevent performance degradation due to the entanglement of temporal and channel-wise modeling. Experiments conducted on six real-world datasets demonstrate that JTFT outperforms state
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#25490;&#24207;&#30340;&#27169;&#25311;&#26657;&#20934;&#65288;SBC&#65289;&#30340;&#28789;&#27963;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27979;&#35797;&#32479;&#35745;&#37327;&#65292;&#24182;&#35745;&#31639;&#20986;&#20174;&#20998;&#31867;&#20934;&#30830;&#24230;&#20013;&#35745;&#31639;&#20986;&#30340;&#35823;&#26657;&#20934;&#21457;&#25955;&#24230;&#24230;&#37327;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#37325;&#26816;&#39564;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14593</link><description>&lt;p&gt;
&#21028;&#21035;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Discriminative calibration. (arXiv:2305.14593v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#25490;&#24207;&#30340;&#27169;&#25311;&#26657;&#20934;&#65288;SBC&#65289;&#30340;&#28789;&#27963;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27979;&#35797;&#32479;&#35745;&#37327;&#65292;&#24182;&#35745;&#31639;&#20986;&#20174;&#20998;&#31867;&#20934;&#30830;&#24230;&#20013;&#35745;&#31639;&#20986;&#30340;&#35823;&#26657;&#20934;&#21457;&#25955;&#24230;&#24230;&#37327;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#37325;&#26816;&#39564;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26816;&#39564;&#36125;&#21494;&#26031;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#24120;&#24120;&#20351;&#29992;&#22522;&#20110;&#25490;&#24207;&#30340;&#27169;&#25311;&#26657;&#20934;&#65288;SBC&#65289;&#12290;&#28982;&#32780;&#65292;SBC &#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65306;&#27979;&#35797;&#32479;&#35745;&#37327;&#30053;&#26174;&#38543;&#24847;&#65292;&#20132;&#20114;&#24615;&#38590;&#20197;&#26816;&#26597;&#65292;&#22810;&#37325;&#26816;&#39564;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#24182;&#19988;&#24471;&#21040;&#30340; P &#20540;&#19981;&#26159;&#19968;&#31181;&#21457;&#25955;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#31867;&#26041;&#27861;&#26367;&#25442;&#36793;&#32536;&#25490;&#24207;&#26816;&#39564;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27979;&#35797;&#32479;&#35745;&#37327;&#12290;&#35813;&#24230;&#37327;&#36890;&#24120;&#20855;&#26377;&#27604; SBC &#25490;&#21517;&#26816;&#39564;&#26356;&#39640;&#30340;&#32479;&#35745;&#21151;&#25928;&#65292;&#24182;&#36820;&#22238;&#20174;&#20998;&#31867;&#20934;&#30830;&#24230;&#35745;&#31639;&#20986;&#30340;&#21487;&#35299;&#37322;&#30340;&#35823;&#26657;&#20934;&#21457;&#25955;&#24230;&#24230;&#37327;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#24212;&#23545;&#26080;&#38656;&#20284;&#28982;&#25512;&#26029;&#25110;&#20256;&#32479;&#25512;&#26029;&#26041;&#27861;&#65288;&#22914;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#25110;&#21464;&#20998;&#25512;&#26029;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#32479;&#35745;&#23398;&#21551;&#21457;&#24335;&#29305;&#24449;&#28436;&#31034;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#23454;&#29616;&#65292;&#24182;&#29992;&#25968;&#20540;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To check the accuracy of Bayesian computations, it is common to use rank-based simulation-based calibration (SBC). However, SBC has drawbacks: The test statistic is somewhat ad-hoc, interactions are difficult to examine, multiple testing is a challenge, and the resulting p-value is not a divergence metric. We propose to replace the marginal rank test with a flexible classification approach that learns test statistics from data. This measure typically has a higher statistical power than the SBC rank test and returns an interpretable divergence measure of miscalibration, computed from classification accuracy. This approach can be used with different data generating processes to address likelihood-free inference or traditional inference methods like Markov chain Monte Carlo or variational inference. We illustrate an automated implementation using neural networks and statistically-inspired features, and validate the method with numerical and real data experiments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#39044;&#26465;&#20214;&#21644;&#36153;&#33293;&#23572;&#33258;&#36866;&#24212; Langevin &#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#19988;&#22312;&#39640;&#32500;&#20013;&#38750;&#24120;&#24378;&#20581;&#30340;&#33258;&#36866;&#24212; MCMC &#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14442</link><description>&lt;p&gt;
&#26368;&#20248;&#39044;&#26465;&#20214;&#21644;&#36153;&#33293;&#23572;&#33258;&#36866;&#24212; Langevin &#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Optimal Preconditioning and Fisher Adaptive Langevin Sampling. (arXiv:2305.14442v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14442
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#39044;&#26465;&#20214;&#21644;&#36153;&#33293;&#23572;&#33258;&#36866;&#24212; Langevin &#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#19988;&#22312;&#39640;&#32500;&#20013;&#38750;&#24120;&#24378;&#20581;&#30340;&#33258;&#36866;&#24212; MCMC &#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#26368;&#22823;&#21270;&#39044;&#26399;&#24179;&#26041;&#36339;&#36291;&#36317;&#31163;&#65292;&#20026; Langevin &#25193;&#25955;&#23450;&#20041;&#20102;&#26368;&#20248;&#39044;&#26465;&#20214;&#12290;&#36825;&#23548;&#33268;&#26368;&#20248;&#39044;&#26465;&#20214;&#20026;&#21453;&#36153;&#33293;&#23572;&#20449;&#24687;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20854;&#20013;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;&#22312;&#30446;&#26631;&#19979;&#24179;&#22343;&#23545;&#25968;&#30446;&#26631;&#26799;&#24230;&#30340;&#22806;&#31215;&#12290;&#25105;&#20204;&#23558;&#27492;&#32467;&#26524;&#24212;&#29992;&#20110; Metropolis &#35843;&#25972; Langevin &#31639;&#27861; (MALA)&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#20174;&#31639;&#27861;&#36816;&#34892;&#20135;&#29983;&#30340;&#26799;&#24230;&#21382;&#21490;&#20013;&#23398;&#20064;&#39044;&#26465;&#20214;&#30340;&#35745;&#31639;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212; MCMC &#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#39640;&#32500;&#20013;&#38750;&#24120;&#24378;&#20581;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26631;&#20934;&#33258;&#36866;&#24212; MCMC &#23398;&#20064;&#39044;&#26465;&#20214;&#21644;&#20301;&#32622;&#30456;&#20851;&#30340; Riemann &#27969;&#24418; MALA &#37319;&#26679;&#22120;&#30340;&#23494;&#20999;&#30456;&#20851;&#30340;&#33258;&#36866;&#24212; MALA &#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define an optimal preconditioning for the Langevin diffusion by analytically maximizing the expected squared jumped distance. This yields as the optimal preconditioning an inverse Fisher information covariance matrix, where the covariance matrix is computed as the outer product of log target gradients averaged under the target. We apply this result to the Metropolis adjusted Langevin algorithm (MALA) and derive a computationally efficient adaptive MCMC scheme that learns the preconditioning from the history of gradients produced as the algorithm runs. We show in several experiments that the proposed algorithm is very robust in high dimensions and significantly outperforms other methods, including a closely related adaptive MALA scheme that learns the preconditioning with standard adaptive MCMC as well as the position-dependent Riemannian manifold MALA sampler.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EPNS&#30340;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31995;&#32479;&#28436;&#21270;&#20013;&#29983;&#25104;&#31561;&#21464;&#20998;&#24067;&#65292;&#24182;&#22312;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14286</link><description>&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#27169;&#25311;&#22120;&#29992;&#20110;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics. (arXiv:2305.14286v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EPNS&#30340;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31995;&#32479;&#28436;&#21270;&#20013;&#29983;&#25104;&#31561;&#21464;&#20998;&#24067;&#65292;&#24182;&#22312;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27491;&#22312;&#25104;&#20026;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#27169;&#25311;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#20540;&#26041;&#27861;&#19981;&#21487;&#34892;&#25110;&#35745;&#31639;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#30830;&#23450;&#24615;&#31070;&#32463;&#27169;&#25311;&#22120;&#20013;&#24341;&#20837;&#22495;&#23545;&#31216;&#24615;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#20854;&#31934;&#30830;&#24615;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#23545;&#31216;&#24615;&#32435;&#20837;&#21487;&#20197;&#27169;&#25311;&#38543;&#26426;&#29616;&#35937;&#30340;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#22120;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31561;&#21464;&#36712;&#36857;&#20998;&#24067;&#32780;&#19981;&#26159;&#31561;&#21464;&#20989;&#25968;&#36924;&#36817;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#65288;EPNS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31561;&#21464;&#20998;&#24067;&#31995;&#32479;&#28436;&#21270;&#30340;&#33258;&#22238;&#24402;&#27010;&#29575;&#24314;&#27169;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;EPNS&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38543;&#26426;N&#20307;&#31995;&#32479;&#21644;&#38543;&#26426;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EPNS&#22312;p&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are emerging as a tool for scalable data-driven simulation of high-dimensional dynamical systems, especially in settings where numerical methods are infeasible or computationally expensive. Notably, it has been shown that incorporating domain symmetries in deterministic neural simulators can substantially improve their accuracy, sample efficiency, and parameter efficiency. However, to incorporate symmetries in probabilistic neural simulators that can simulate stochastic phenomena, we need a model that produces equivariant distributions over trajectories, rather than equivariant function approximations. In this paper, we propose Equivariant Probabilistic Neural Simulation (EPNS), a framework for autoregressive probabilistic modeling of equivariant distributions over system evolutions. We use EPNS to design models for a stochastic n-body system and stochastic cellular dynamics. Our results show that EPNS considerably outperforms existing neural network-based methods for p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14257</link><description>&lt;p&gt;
&#20998;&#23618;&#25552;&#31034;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#35266;&#23519;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#24635;&#26159;&#25226;\emph{&#23436;&#25972;}&#35266;&#23519;&#65288;&#20363;&#22914;&#32593;&#39029;&#65289;&#25918;&#21040;&#25552;&#31034;&#20013;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#30340;\emph{&#21387;&#32553;}&#21644;\emph{&#30456;&#20851;}&#30340;&#35266;&#23519;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;\summ&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;\actor&#25552;&#31034;&#26681;&#25454;&#24635;&#32467;&#30340;&#35266;&#23519;&#39044;&#27979;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#23588;&#20854;&#23637;&#31034;&#20102;&#23427;&#22312;&#22797;&#26434;&#30340;&#32593;&#32476;&#23548;&#33322;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#23436;&#25972;&#30340;&#35266;&#23519;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#19978;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;6.2\%&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PEQA&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#37327;&#21270;LLM&#30340;&#20248;&#28857;&#12290;&#36890;&#36807;&#20165;&#26356;&#26032;&#37327;&#21270;&#23610;&#24230;&#65292;PEQA&#21487;&#20197;&#39640;&#25928;&#22320;&#24494;&#35843;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20943;&#23569;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2305.14152</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;4&#20301;&#25972;&#25968;&#37327;&#21270;&#23454;&#29616;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. (arXiv:2305.14152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PEQA&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#37327;&#21270;LLM&#30340;&#20248;&#28857;&#12290;&#36890;&#36807;&#20165;&#26356;&#26032;&#37327;&#21270;&#23610;&#24230;&#65292;PEQA&#21487;&#20197;&#39640;&#25928;&#22320;&#24494;&#35843;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20943;&#23569;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#30528;&#22312;&#24494;&#35843;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30001;&#20110;&#20854;&#39640;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#25104;&#26412;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#20294;&#39044;&#35757;&#32451;LLM&#26435;&#37325;&#26412;&#36523;&#30340;&#22823;&#23567;&#20173;&#28982;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#37327;&#21270;&#25216;&#26415;&#34987;&#24191;&#27867;&#25552;&#20986;&#26469;&#32531;&#35299;&#20869;&#23384;&#38656;&#27714;&#21644;&#21152;&#24555;LLM&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#22823;&#22810;&#25968;&#36825;&#20123;&#25216;&#26415;&#37117;&#26159;&#38024;&#23545;&#37096;&#32626;&#38454;&#27573;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#37327;&#21270;&#24863;&#30693;&#36866;&#24212;&#65288;PEQA&#65289;-&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;PEFT&#30340;&#20248;&#28857;&#19982;&#37327;&#21270;LLM&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#20165;&#26356;&#26032;&#37327;&#21270;&#23610;&#24230;&#65292;PEQA&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#37327;&#21270;LLM&#65292;&#30830;&#20445;&#24179;&#31283;&#30340;&#20219;&#21153;&#36716;&#25442;&#12290;&#19982;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#24182;&#34892;&#65292;PEQA&#26174;&#30528;&#20943;&#23569;&#20102;&#19982;&#20248;&#21270;&#22120;&#29366;&#24577;&#30456;&#20851;&#30340;&#20869;&#23384;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.14076</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent. (arXiv:2305.14076v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD)&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#22522;&#20110;&#31890;&#23376;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#12290;&#23613;&#31649;&#20854;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#29702;&#35299;SVGD&#30340;&#29702;&#35770;&#23646;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#21463;&#27492;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#21452;&#32447;&#24615;&#26680;&#23558;SVGD&#25237;&#24433;&#21040;&#39640;&#26031;&#20998;&#24067;&#26063;&#20013;&#65292;&#21363;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029; (GVI) &#19982; SVGD&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#22343;&#22330; PDE &#21644;&#31163;&#25955;&#31890;&#23376;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22270;&#20687;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#19968;&#20010;&#26032;&#30340;&#20195;&#25968;&#24658;&#31561;&#24335;&#65292;&#35813;&#31561;&#24335;&#23558;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#30340;&#36153;&#24076;&#23572;&#20449;&#24687;&#30697;&#38453;&#19982;&#31890;&#23376;&#22343;&#21248;&#20998;&#24067;&#30340;&#36153;&#24076;&#23572;&#20449;&#24687;&#30697;&#38453;&#30456;&#20851;&#32852;&#12290;&#36825;&#20010;&#31561;&#24335;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#36879;&#35270; GVI with SVGD &#22312;&#22343;&#22330;&#21644;&#31890;&#23376;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#24615;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in ti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#21270;&#26041;&#27861;&#29992;&#20110;&#29109;&#20146;&#21644;&#21147;&#19979;&#30340;&#38477;&#32500;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23545;&#31216;&#21270;&#36807;&#31243;&#20013;&#30340;&#29109;&#21644;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13797</link><description>&lt;p&gt;
SNEkhorn: &#23545;&#31216;&#29109;&#20146;&#21644;&#21147;&#19979;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities. (arXiv:2305.13797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13797
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#21270;&#26041;&#27861;&#29992;&#20110;&#29109;&#20146;&#21644;&#21147;&#19979;&#30340;&#38477;&#32500;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23545;&#31216;&#21270;&#36807;&#31243;&#20013;&#30340;&#29109;&#21644;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#21152;&#26435;&#22270;&#26469;&#32534;&#30721;&#25968;&#25454;&#38598;&#20013;&#26679;&#26412;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#29109;&#20146;&#21644;&#21147;&#65288;EAs&#65289;&#26159;&#36825;&#31867;&#22270;&#30340;&#29305;&#20363;&#65292;&#23427;&#36890;&#24120;&#29992;&#20110;&#27969;&#24418;&#23398;&#20064;&#31639;&#27861; t-SNE &#20013;&#12290;&#20026;&#20102;&#20445;&#35777;&#23545;&#19981;&#21516;&#37319;&#26679;&#23494;&#24230;&#30340;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;EAs &#25353;&#19968;&#23450;&#26041;&#24335;&#23545;&#27599;&#20010;&#26679;&#26412;&#20998;&#37197;&#19968;&#20010;&#26680;&#24102;&#23485;&#21442;&#25968;&#65292;&#20197;&#20351;&#24471;&#20146;&#21644;&#21147;&#30697;&#38453;&#20013;&#27599;&#19968;&#34892;&#30340;&#29109;&#37117;&#20445;&#25345;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#25351;&#25968;&#21442;&#25968;&#19979;&#12290;EAs&#20855;&#26377;&#19981;&#23545;&#31216;&#24615;&#21644;&#25353;&#34892;&#38543;&#26426;&#24615;&#65292;&#20294;&#26159;&#22312;&#32463;&#36807;&#21551;&#21457;&#24335;&#23545;&#31216;&#21270;&#26041;&#27861;&#22788;&#29702;&#21518;&#65292;&#21448;&#34987;&#29992;&#20110;&#38477;&#32500;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;EAs&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#24418;&#24335;&#65292;&#35270;&#20854;&#20316;&#20026;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#23545;&#31216;&#21270;&#65292;&#24182;&#19988;&#21487;&#29992;&#21452;&#37325;&#19978;&#21319;&#27861;&#39640;&#25928;&#35745;&#31639;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#20146;&#21644;&#21147;&#30697;&#38453;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;&#23545;&#31216;&#21270;&#25152;&#24102;&#26469;&#30340;&#29109;&#21644;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches in machine learning rely on a weighted graph to encode the similarities between samples in a dataset. Entropic affinities (EAs), which are notably used in the popular Dimensionality Reduction (DR) algorithm t-SNE, are particular instances of such graphs. To ensure robustness to heterogeneous sampling densities, EAs assign a kernel bandwidth parameter to every sample in such a way that the entropy of each row in the affinity matrix is kept constant at a specific value, whose exponential is known as perplexity. EAs are inherently asymmetric and row-wise stochastic, but they are used in DR approaches after undergoing heuristic symmetrization methods that violate both the row-wise constant entropy and stochasticity properties. In this work, we uncover a novel characterization of EA as an optimal transport problem, allowing a natural symmetrization that can be computed efficiently using dual ascent. The corresponding novel affinity matrix derives advantages from symmetric do
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12467</link><description>&lt;p&gt;
&#29702;&#35299;ReLU&#32593;&#32476;&#30340;&#22810;&#38454;&#27573;&#20248;&#21270;&#21160;&#24577;&#21644;&#20016;&#23500;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#32463;&#24120;&#34920;&#29616;&#20986;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#21644;&#25439;&#22833;&#30340;&#38750;&#20984;&#24615;&#20026;&#29702;&#35770;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#29305;&#23450;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25429;&#33719;&#20102;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#21040;&#26368;&#32456;&#25910;&#25947;&#30340;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#12290;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#20063;&#21487;&#20197;&#34987;&#31934;&#30830;&#22320;&#35782;&#21035;&#21644;&#29702;&#35770;&#19978;&#25429;&#33719;&#65292;&#20363;&#22914;...
&lt;/p&gt;
&lt;p&gt;
The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#23398;&#20064;&#29702;&#35770;&#21644;&#24212;&#29992;&#27010;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;Wasserstein&#31283;&#23450;&#24615;&#30028;&#38480;&#30340;&#32479;&#19968;&#25351;&#21335;&#65292;&#24182;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#24378;&#20984;&#25439;&#22833;&#21644;&#24102;&#28155;&#21152;&#22122;&#22768;&#30340;&#38750;&#20984;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.12056</link><description>&lt;p&gt;
&#65288;&#24102;&#22122;&#22768;&#30340;&#65289;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26102;&#38388;&#22343;&#21248;Wasserstein&#31283;&#23450;&#24615;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic Gradient Descent. (arXiv:2305.12056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#23398;&#20064;&#29702;&#35770;&#21644;&#24212;&#29992;&#27010;&#29575;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;Wasserstein&#31283;&#23450;&#24615;&#30028;&#38480;&#30340;&#32479;&#19968;&#25351;&#21335;&#65292;&#24182;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#24378;&#20984;&#25439;&#22833;&#21644;&#24102;&#28155;&#21152;&#22122;&#22768;&#30340;&#38750;&#20984;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#31283;&#23450;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#27010;&#24565;&#65292;&#23545;&#20110;&#25512;&#23548;&#23454;&#36341;&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#38480;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;&#36807;&#21435;&#21313;&#24180;&#24050;&#32463;&#35265;&#35777;&#20102;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#25152;&#24212;&#29992;&#30340;&#19981;&#21516;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#30028;&#38480;&#30340;&#22686;&#21152;&#12290;&#34429;&#28982;&#36825;&#20123;&#30028;&#38480;&#29031;&#20142;&#20102;&#20248;&#21270;&#31639;&#27861;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#20294;&#27599;&#20010;&#26696;&#20363;&#30340;&#20998;&#26512;&#36890;&#24120;&#38656;&#35201;&#19981;&#21516;&#30340;&#35777;&#26126;&#25216;&#26415;&#21644;&#26174;&#33879;&#19981;&#21516;&#30340;&#25968;&#23398;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#29702;&#35770;&#21644;&#24212;&#29992;&#27010;&#29575;&#20043;&#38388;&#24314;&#31435;&#20102;&#26032;&#30340;&#32852;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35777;&#26126;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;Wasserstein&#31283;&#23450;&#24615;&#30028;&#38480;&#30340;&#32479;&#19968;&#25351;&#21335;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#19978;&#38416;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#24378;&#20984;&#25439;&#22833;&#21644;&#24102;&#28155;&#21152;&#22122;&#22768;&#30340;&#38750;&#20984;&#25439;&#22833;&#30340;&#26102;&#38388;&#22343;&#21248;&#31283;&#23450;&#24615;&#30028;&#38480;&#65288;&#21363;&#65292;&#30028;&#38480;&#19981;&#38543;&#36845;&#20195;&#27425;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#65289;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#19982;&#20808;&#21069;&#25991;&#29486;&#30456;&#20284;&#30340;&#32467;&#26524;&#25110;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic stability is an important notion that has proven powerful for deriving generalization bounds for practical algorithms. The last decade has witnessed an increasing number of stability bounds for different algorithms applied on different classes of loss functions. While these bounds have illuminated various properties of optimization algorithms, the analysis of each case typically required a different proof technique with significantly different mathematical tools. In this study, we make a novel connection between learning theory and applied probability and introduce a unified guideline for proving Wasserstein stability bounds for stochastic optimization algorithms. We illustrate our approach on stochastic gradient descent (SGD) and we obtain time-uniform stability bounds (i.e., the bound does not increase with the number of iterations) for strongly convex losses and non-convex losses with additive noise, where we recover similar results to the prior art or extend them to mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#25252;&#20080;&#23478;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#65292;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11362</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#29289;&#21697;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Item Pricing. (arXiv:2305.11362v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#25252;&#20080;&#23478;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#65292;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20445;&#25252;&#20080;&#26041;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#23427;&#19982;&#20080;&#26041;&#30340;&#36755;&#20837;&#23545;&#65288;&#21830;&#21697;&#36873;&#25321;&#21644;&#20986;&#20215;&#65289;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;(regret)&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25351;&#25968;&#26435;&#37325;&#20803;&#31639;&#27861;&#65292;&#36890;&#36807;&#23567;&#30340;&#38543;&#26426;&#25200;&#21160;&#32531;&#35299;&#20102;&#25910;&#30410;&#20989;&#25968;&#19981;&#36830;&#32493;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#19982;&#25351;&#25968;&#26426;&#21046;&#30340;&#32467;&#26500;&#30456;&#20284;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22266;&#26377;&#22320;&#20445;&#35777;&#20102;&#24046;&#20998;&#38544;&#31169;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#21040;&#36880;&#36718;&#31574;&#30053;&#24615;&#20986;&#20215;&#30340;&#24773;&#20917;&#12290;&#20869;&#22312;&#30340;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26368;&#23567;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#30830;&#20445;&#20854;&#20111;&#25439;&#23376;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the problem of revenue maximization in a repeated, unlimited supply item-pricing auction while preserving buyer privacy. We present a novel algorithm that provides differential privacy with respect to the buyer's input pair: item selection and bid. Notably, our algorithm is the first to offer a sublinear $O(\sqrt{T}\log{T})$ regret with a privacy guarantee. Our method is based on an exponential weights meta-algorithm, and we mitigate the issue of discontinuities in revenue functions via small random perturbations. As a result of its structural similarity to the exponential mechanism, our method inherently secures differential privacy. We also extend our algorithm to accommodate scenarios where buyers strategically bid over successive rounds. The inherent differential privacy allows us to adapt our algorithm with minimal modification to ensure a sublinear regret in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#21487;&#33021;&#36973;&#21463;&#21040;&#30340;&#26631;&#31614;&#25200;&#21160;&#25915;&#20987;&#24773;&#20917;&#65292;&#24471;&#20986;&#25915;&#20987;&#24378;&#24230;&#36229;&#36807;&#20020;&#30028;&#38408;&#20540;&#26102;&#23398;&#20064;&#20934;&#30830;&#29575;&#23558;&#20986;&#29616;&#19981;&#36830;&#32493;&#36716;&#21464;&#30340;&#32467;&#35770;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#22312;&#22797;&#26434;&#32467;&#26500;&#23398;&#20064;&#22120;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11132</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#32773;&#30340;&#25915;&#20987;&#65306;&#19968;&#39033;&#25945;&#24072;-&#23398;&#29983;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Attacks on Online Learners: a Teacher-Student Analysis. (arXiv:2305.11132v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#21487;&#33021;&#36973;&#21463;&#21040;&#30340;&#26631;&#31614;&#25200;&#21160;&#25915;&#20987;&#24773;&#20917;&#65292;&#24471;&#20986;&#25915;&#20987;&#24378;&#24230;&#36229;&#36807;&#20020;&#30028;&#38408;&#20540;&#26102;&#23398;&#20064;&#20934;&#30830;&#29575;&#23558;&#20986;&#29616;&#19981;&#36830;&#32493;&#36716;&#21464;&#30340;&#32467;&#35770;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#22312;&#22797;&#26434;&#32467;&#26500;&#23398;&#20064;&#22120;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65306;&#25968;&#25454;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;&#25991;&#29486;&#30740;&#31350;&#20102;&#23545;&#24050;&#32463;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27979;&#35797;&#26102;&#30340;&#25915;&#20987;&#24773;&#20917;&#65292;&#20294;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#25915;&#20987;&#24773;&#20917;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#32773;&#21487;&#33021;&#23384;&#22312;&#30340;&#26631;&#31614;&#25200;&#21160;&#25915;&#20987;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#24182;&#38024;&#23545;&#31616;&#21333;&#32447;&#24615;&#23398;&#20064;&#22120;&#30340;&#31283;&#24577;&#33719;&#24471;&#20102;&#20998;&#26512;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#35777;&#26126;&#65292;&#24403;&#25915;&#20987;&#24378;&#24230;&#36229;&#36807;&#20020;&#30028;&#38408;&#20540;&#26102;&#65292;&#23398;&#20064;&#22120;&#30340;&#20934;&#30830;&#29575;&#20250;&#20986;&#29616;&#19981;&#36830;&#32493;&#30340;&#36716;&#21464;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#23545;&#22797;&#26434;&#32467;&#26500;&#30340;&#23398;&#20064;&#22120;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#39564;&#35777;&#20102;&#29702;&#35770;&#20998;&#26512;&#30340;&#27934;&#35265;&#24182;&#25581;&#31034;&#20102;&#36973;&#21463;&#25915;&#20987;&#30340;&#23398;&#20064;&#22120;&#30340;&#26032;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are famously vulnerable to adversarial attacks: small ad-hoc perturbations of the data that can catastrophically alter the model predictions. While a large literature has studied the case of test-time attacks on pre-trained models, the important case of attacks in an online learning setting has received little attention so far. In this work, we use a control-theoretical perspective to study the scenario where an attacker may perturb data labels to manipulate the learning dynamics of an online learner. We perform a theoretical analysis of the problem in a teacher-student setup, considering different attack strategies, and obtaining analytical results for the steady state of simple linear learners. These results enable us to prove that a discontinuous transition in the learner's accuracy occurs when the attack strength exceeds a critical threshold. We then study empirically attacks on learners with complex architectures using real data, confirming the insights of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10626</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#19990;&#30028;&#27169;&#22411;&#65306;&#23454;&#20307;&#32463;&#39564;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#22788;&#29702;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#31616;&#21333;&#25512;&#29702;&#21644;&#35268;&#21010;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#29702;&#35299;&#29289;&#20307;&#27704;&#24658;&#25110;&#35268;&#21010;&#23478;&#24237;&#27963;&#21160;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110; LM &#20165;&#21463;&#20070;&#38754;&#35821;&#35328;&#35757;&#32451;&#65292;&#32570;&#23569;&#24517;&#35201;&#30340;&#23454;&#20307;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378; LM &#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#19968;&#33324;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#37096;&#32626;&#19968;&#20010;&#34701;&#20837;&#23454;&#20307;&#32463;&#39564;&#30340;&#20195;&#29702;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#30340;&#20223;&#30495;&#22120;(VirtualHome)&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#32463;&#39564;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#32463;&#39564;&#24494;&#35843; LM &#65292;&#20197;&#25945;&#25480;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21010;&#21644;&#23436;&#25104;&#30446;&#26631;&#12289;&#29289;&#20307;&#27704;&#24658;&#21644;&#36319;&#36394;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21033;&#29992;&#20854;&#20182;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102; LM &#22312;&#19968;&#31995;&#21015;&#29289;&#29702;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#24182;&#32463;&#24120;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#30693;&#35782;&#36755;&#20986;&#12290;&#36890;&#36807;&#23545;14&#31181;GLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#65292;&#21457;&#29616;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#32780;&#23545;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10519</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#30693;&#35782;&#36755;&#20986;&#12290;&#36890;&#36807;&#23545;14&#31181;GLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#65292;&#21457;&#29616;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#32780;&#23545;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#23637;&#31034;&#20102;&#23384;&#20648;&#20107;&#23454;&#30693;&#35782;&#21644;&#39640;&#25928;&#22238;&#31572;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;GLM&#26159;&#21542;&#22987;&#32456;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#30340;&#31572;&#26696;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#28508;&#21464;&#37327;&#21644;KaRR&#24230;&#37327;&#25351;&#23548;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#24230;&#37327;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#24418;&#24335;&#19978;&#30340;&#36830;&#32493;&#27010;&#29575;&#37327;&#21270;&#20854;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;14&#31181;GLM&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21253;&#25324;LLaMA&#12289;Alpaca&#12289;OPT&#21644;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#28085;&#30422;&#20102;600&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#24378;&#30456;&#20851;&#24615;&#65288;0.43 Kendall's $\tau$&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#25903;&#26550;&#32467;&#26500;&#30340;GLM&#30340;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#19988;&#22312;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#25345;&#32493;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models (GLMs) have demonstrated capabilities to store factual knowledge and answer queries efficiently. Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model's knowledge by computing its continuous probability across diverse text forms. We conduct a comprehensive comparison of knowledge across 14 GLMs using our framework, including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment encompasses 600 relation types and exhibits a strong correlation (0.43 Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge in GLMs with the same backbone architecture adheres to the scaling law, and that tuning on instruction-following data may compromise the model's ability to generate factually correct text consistently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#30340;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#23567;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.09956</link><description>&lt;p&gt;
&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Adversarial Consistency of Surrogate Risks for Binary Classification. (arXiv:2305.09956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#30340;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29992;&#20110;&#40065;&#26834;&#20108;&#20998;&#31867;&#30340;&#20195;&#29702;&#39118;&#38505;&#30340;&#19968;&#33268;&#24615;&#12290;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23398;&#20064;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#35797;&#22270;&#22312;&#27599;&#20010;&#31034;&#20363;&#21487;&#20197;&#22312;&#23567;&#29699;&#20869;&#34987;&#24694;&#24847;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#26399;&#26395;&#30340;$0$-$1$&#25439;&#22833;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#23436;&#25972;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#36825;&#20123;&#38598;&#26159;&#8220;&#19968;&#33268;&#8221;&#30340;&#65292;&#21363;&#21487;&#20197;&#26367;&#25442;$0$-$1$&#25439;&#22833;&#32780;&#19981;&#24433;&#21709;&#21407;&#22987;&#23545;&#25239;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#24207;&#21015;&#30340;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#29992;&#20110;$\rho$-margin&#25439;&#22833;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#30340;&#37327;&#21270;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#26126;&#26174;&#36739;&#23567;&#65292;&#22312;&#26631;&#20934;&#35774;&#32622;&#20013;&#65292;&#35768;&#22810;&#24120;&#35265;&#30340;&#20195;&#29702;&#37117;&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the consistency of surrogate risks for robust binary classification. It is common to learn robust classifiers by adversarial training, which seeks to minimize the expected $0$-$1$ loss when each example can be maliciously corrupted within a small ball. We give a simple and complete characterization of the set of surrogate loss functions that are \emph{consistent}, i.e., that can replace the $0$-$1$ loss without affecting the minimizing sequences of the original adversarial risk, for any data distribution. We also prove a quantitative version of adversarial consistency for the $\rho$-margin loss. Our results reveal that the class of adversarially consistent surrogates is substantially smaller than in the standard setting, where many common surrogates are known to be consistent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;k-&#21311;&#21517;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#33021;&#28304;&#28040;&#32791;&#36739;&#23569;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#33021;&#28304;&#28040;&#32791;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07116</link><description>&lt;p&gt;
k-&#21311;&#21517;&#21644;&#21512;&#25104;&#25968;&#25454;&#25216;&#26415;&#30340;&#33021;&#37327;&#25104;&#26412;&#21644;&#26426;&#22120;&#23398;&#20064;&#20934;&#30830;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;k-&#21311;&#21517;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#33021;&#28304;&#28040;&#32791;&#36739;&#23569;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#33021;&#28304;&#28040;&#32791;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#19982;&#38544;&#31169;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#20851;&#30340;&#24840;&#21457;&#22686;&#38271;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#27431;&#30431;&#39041;&#24067;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#24182;&#25215;&#35834;&#20102;&#32511;&#33394;&#21327;&#35758;&#12290;&#22823;&#37327;&#30740;&#31350;&#25506;&#31350;&#20102;&#36816;&#29992;&#21311;&#21517;&#25968;&#25454;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#25928;&#21644;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#31350;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;k-&#21311;&#21517;&#12290;&#30001;&#20110;&#21512;&#25104;&#25968;&#25454;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#27492;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20934;&#30830;&#24615;&#65306;a&#65289;&#23558;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#24212;&#29992;&#20110;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;b&#65289;&#22312;&#30456;&#20851;&#38544;&#31169;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65306;k-&#21311;&#21517;&#21270;&#65288;&#20351;&#29992;&#27867;&#21270;&#21644;&#25233;&#21046;&#65289;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#21450;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27599;&#20010;&#27169;&#22411;&#37117;&#22312;&#27599;&#20010;&#38544;&#31169;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32463;&#36807;k-&#21311;&#21517;&#21270;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#28040;&#32791;&#30340;&#33021;&#37327;&#36739;&#23569;&#65292;&#19982;&#22312;&#38750;&#21311;&#21517;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;k-&#21311;&#21517;&#21270;&#36807;&#31243;&#20013;&#28040;&#32791;&#30340;&#33021;&#37327;&#38750;&#24120;&#21487;&#35266;&#65292;&#22312;&#35780;&#20272;&#20854;&#26377;&#29992;&#24615;&#26102;&#24517;&#39035;&#23558;&#20854;&#32771;&#34385;&#22312;&#20869;&#12290;&#21512;&#25104;&#25968;&#25454;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#22312;&#28040;&#32791;&#26356;&#23569;&#33021;&#28304;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#38750;&#21311;&#21517;&#21270;&#25968;&#25454;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#21450;&#20854;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.06807</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Information Design in Multi-Agent Reinforcement Learning. (arXiv:2305.06807v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#21450;&#20854;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#20223;&#20154;&#31867;&#21644;&#21160;&#29289;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#23454;&#38469;&#29615;&#22659;&#20013;&#23384;&#22312;&#20854;&#20182;&#26377;&#33258;&#24049;&#30446;&#26631;&#30340;&#26234;&#33021;&#20307;&#65292;&#23427;&#20204;&#20250;&#36866;&#24212;&#22320;&#19982;&#33258;&#24049;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#25104;&#21151;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#38656;&#35201;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#20197;&#20351;&#23427;&#20204;&#30340;&#34892;&#20026;&#26356;&#26377;&#30410;&#12290;&#20449;&#24687;&#35774;&#35745;&#26159;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#19968;&#32452;RL&#20195;&#29702;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) mimics how humans and animals interact with the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receivers are willing to respect. We formulate the Markov signaling game, and develop the notions 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05803</link><description>&lt;p&gt;
&#22522;&#20110;Segment Anything Model (SAM)&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;(WSSS)&#30001;&#20110;&#20854;&#19982;&#20687;&#32032;&#32423;&#27880;&#37322;&#30456;&#27604;&#30340;&#20302;&#25104;&#26412;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31867;&#28608;&#27963;&#22270;(CAM)&#29983;&#25104;&#20687;&#32032;&#32423;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;CAM&#32463;&#24120;&#36973;&#21463;&#23616;&#37096;&#28608;&#27963;&#30340;&#38480;&#21046;-&#21482;&#28608;&#27963;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#23545;&#35937;&#21306;&#22495;&#21644;&#34394;&#20551;&#30340;&#28608;&#27963;-&#19981;&#24517;&#35201;&#22320;&#28608;&#27963;&#29289;&#20307;&#21608;&#22260;&#30340;&#32972;&#26223;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Segment Anything Model (SAM)&#22686;&#24378;CAM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;SAM&#26159;&#19968;&#20010;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#27573;&#33853;&#30340;&#24378;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#36825;&#20123;&#21306;&#22495;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#23450;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#20449;&#21495;&#26469;&#36873;&#25321;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#25968;Vononoi&#25439;&#22833;&#20989;&#25968;&#24182;&#24314;&#31435;&#20102;MLE&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#35299;&#20915;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;Softmax&#38376;&#25511;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#38376;&#25511;&#19982;&#39640;&#26031;&#20998;&#24067;&#20013;&#30340;&#19987;&#23478;&#20989;&#25968;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#26159;&#19968;&#20010;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03288</link><description>&lt;p&gt;
&#35299;&#23494;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;Softmax&#38376;&#25511;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Demystifying Softmax Gating in Gaussian Mixture of Experts. (arXiv:2305.03288v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#25968;Vononoi&#25439;&#22833;&#20989;&#25968;&#24182;&#24314;&#31435;&#20102;MLE&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#35299;&#20915;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;Softmax&#38376;&#25511;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#38376;&#25511;&#19982;&#39640;&#26031;&#20998;&#24067;&#20013;&#30340;&#19987;&#23478;&#20989;&#25968;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#26159;&#19968;&#20010;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;Softmax&#38376;&#25511;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#38271;&#26399;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#19977;&#20010;&#22522;&#26412;&#29702;&#35770;&#25361;&#25112;&#19982;Softmax&#38376;&#25511;&#30456;&#20851;&#65306;&#65288;i&#65289;&#21482;&#33021;&#35782;&#21035;&#21442;&#25968;&#30340;&#24179;&#31227;&#65307;&#65288;ii&#65289;Softmax&#38376;&#25511;&#21644;&#39640;&#26031;&#20998;&#24067;&#20013;&#19987;&#23478;&#20989;&#25968;&#20043;&#38388;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20869;&#22312;&#30456;&#20114;&#20316;&#29992;&#65307;&#65288;iii&#65289;Softmax&#38376;&#25511;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26465;&#20214;&#23494;&#24230;&#30340;&#20998;&#23376;&#21644;&#20998;&#27597;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#21442;&#25968;Vononoi&#25439;&#22833;&#20989;&#25968;&#24182;&#24314;&#31435;MLE&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#24403;&#19987;&#23478;&#25968;&#37327;&#26410;&#30693;&#19988;&#36229;&#39069;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;MLE&#30340;&#36895;&#29575;&#19982;&#19968;&#32452;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#21487;&#35299;&#24615;&#38382;&#39064;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating: (i) the identifiability only up to the translation of the parameters; (ii) the intrinsic interaction via partial differential equation between the softmax gating and the expert functions in Gaussian distribution; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Vononoi loss functions among parameters and establishing the convergence rates of the maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the number of experts is unknown and over-specified, our findings show a connection between the rate of MLE and a solvability problem of a system of polynomial equations.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#19988;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02252</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Algorithm for Learning with Unknown Distribution Drift. (arXiv:2305.02252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02252
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#19988;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#23398;&#20064;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#36890;&#29992;&#25216;&#26415;&#12290;&#32473;&#23450;&#19968;&#20010;&#20174;&#28418;&#31227;&#20998;&#24067;&#30340;&#26368;&#21518;$T$&#27493;&#20013;&#29420;&#31435;&#35266;&#27979;&#21040;&#30340;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;$T$&#26102;&#21051;&#19981;&#21152;&#21306;&#20998;&#22320;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#38656;&#35201;&#20851;&#20110;&#28418;&#31227;&#22823;&#23567;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#26679;&#26412;&#25968;&#25454;&#12290;&#22312;&#19981;&#26126;&#30830;&#20272;&#35745;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#23398;&#20064;&#30340;&#20989;&#25968;&#26063;&#30340;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and analyze a general technique for learning with an unknown distribution drift. Given a sequence of independent observations from the last $T$ steps of a drifting distribution, our algorithm agnostically learns a family of functions with respect to the current distribution at time $T$. Unlike previous work, our technique does not require prior knowledge about the magnitude of the drift. Instead, the algorithm adapts to the sample data. Without explicitly estimating the drift, the algorithm learns a family of functions with almost the same error as a learning algorithm that knows the magnitude of the drift in advance. Furthermore, since our algorithm adapts to the data, it can guarantee a better learning error than an algorithm that relies on loose bounds on the drift.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500; DAFNO&#65292;&#21487;&#20197;&#23398;&#20064;&#24102;&#26377;&#19981;&#35268;&#21017;&#20960;&#20309;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22495;&#30340;&#20195;&#29702;&#12290;&#36890;&#36807;&#23558;&#24179;&#28369;&#21270;&#30340;&#29305;&#24449;&#20989;&#25968;&#32435;&#20837; FNOs &#30340;&#31215;&#20998;&#23618;&#26550;&#26500;&#20013;&#65292;&#24182;&#21033;&#29992; FFT &#26469;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#65292;&#20197;&#26126;&#30830;&#30340;&#26041;&#24335;&#23558;&#20960;&#20309;&#20449;&#24687;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#65292;DAFNO &#30456;&#23545;&#20110;&#22522;&#32447;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00478</link><description>&lt;p&gt;
&#22495;&#19981;&#21487;&#30693;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Domain Agnostic Fourier Neural Operators. (arXiv:2305.00478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00478
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500; DAFNO&#65292;&#21487;&#20197;&#23398;&#20064;&#24102;&#26377;&#19981;&#35268;&#21017;&#20960;&#20309;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22495;&#30340;&#20195;&#29702;&#12290;&#36890;&#36807;&#23558;&#24179;&#28369;&#21270;&#30340;&#29305;&#24449;&#20989;&#25968;&#32435;&#20837; FNOs &#30340;&#31215;&#20998;&#23618;&#26550;&#26500;&#20013;&#65292;&#24182;&#21033;&#29992; FFT &#26469;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#65292;&#20197;&#26126;&#30830;&#30340;&#26041;&#24335;&#23558;&#20960;&#20309;&#20449;&#24687;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#65292;DAFNO &#30456;&#23545;&#20110;&#22522;&#32447;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNOs&#65289;&#33021;&#22815;&#23398;&#20064;&#22312;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#26144;&#23556;&#65292;&#26368;&#36817;&#24050;&#25104;&#20026;&#23398;&#20064;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#21709;&#24212;&#30340;&#28909;&#38376;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#33391;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;FNOs &#20381;&#36182;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442; (FFT)&#65292;&#35813;&#21464;&#25442;&#20165;&#38480;&#20110;&#30697;&#24418;&#22495;&#19978;&#30340;&#24314;&#27169;&#38382;&#39064;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#26679;&#30340;&#38480;&#21046;&#65292;&#20801;&#35768; FFT &#22312;&#19981;&#35268;&#21017;&#20960;&#20309;&#20197;&#21450;&#25299;&#25169;&#21464;&#21270;&#20013;&#20351;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22495;&#19981;&#21487;&#30693;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376; (DAFNO)&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#19981;&#35268;&#21017;&#20960;&#20309;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22495;&#30340;&#20195;&#29702;&#30340;&#26032;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#24179;&#28369;&#21270;&#30340;&#29305;&#24449;&#20989;&#25968;&#32435;&#20837; FNOs &#30340;&#31215;&#20998;&#23618;&#26550;&#26500;&#20013;&#65292;&#24182;&#21033;&#29992; FFT &#26469;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#65292;&#20197;&#20415;&#20197;&#26126;&#30830;&#30340;&#26041;&#24335;&#23558;&#20960;&#20309;&#20449;&#24687;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;DAFNO &#30456;&#23545;&#20110;&#22522;&#32447;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier neural operators (FNOs) can learn highly nonlinear mappings between function spaces, and have recently become a popular tool for learning responses of complex physical systems. However, to achieve good accuracy and efficiency, FNOs rely on the Fast Fourier transform (FFT), which is restricted to modeling problems on rectangular domains. To lift such a restriction and permit FFT on irregular geometries as well as topology changes, we introduce domain agnostic Fourier neural operator (DAFNO), a novel neural operator architecture for learning surrogates with irregular geometries and evolving domains. The key idea is to incorporate a smoothed characteristic function in the integral layer architecture of FNOs, and leverage FFT to achieve rapid computations, in such a way that the geometric information is explicitly encoded in the architecture. In our empirical evaluation, DAFNO has achieved state-of-the-art accuracy as compared to baseline neural operator models on two benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#32463;&#36807;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#19988;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00418</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effectiveness of Large Language Models in Generating Unit Tests. (arXiv:2305.00418v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#32463;&#36807;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#19988;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#27880;&#37322;&#12289;&#29616;&#26377;&#20195;&#30721;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#26469;&#29983;&#25104;&#20195;&#30721;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#19977;&#20010;&#29983;&#25104;&#27169;&#22411;&#65288;CodeGen&#12289;Codex&#21644;GPT-3.5&#65289;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#22522;&#20934;&#65288;HumanEval&#21644;Evosuite SF110&#65289;&#26469;&#35843;&#26597;&#29615;&#22659;&#29983;&#25104;&#23545;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#26681;&#25454;&#32534;&#35793;&#29575;&#12289;&#27979;&#35797;&#27491;&#30830;&#24615;&#12289;&#35206;&#30422;&#29575;&#21644;&#27979;&#35797;&#21619;&#36947;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Codex&#27169;&#22411;&#22312;HumanEval&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;80%&#30340;&#35206;&#30422;&#29575;&#65292;&#20294;&#22312;EvoSuite SF110&#22522;&#20934;&#20013;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#36229;&#36807;2%&#30340;&#35206;&#30422;&#29575;&#12290;&#29983;&#25104;&#30340;&#27979;&#35797;&#36824;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#65292;&#27604;&#22914;&#37325;&#22797;&#30340;&#26029;&#35328;&#21644;&#31354;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models (CodeGen, Codex, and GPT-3.5) can generate test cases. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861; ACDC&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14997</link><description>&lt;p&gt;
&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861; ACDC&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20498;&#25512;&#20102;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#34892;&#20026;&#12290;&#36825;&#20123;&#21457;&#29616;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#30740;&#31350;&#32773;&#30340;&#30452;&#35273;&#65292;&#36825;&#20351;&#24471;&#24212;&#29992;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#20102;&#35299;&#24403;&#21069;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#22797;&#26434;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#26680;&#24515;&#24037;&#20316;&#27969;&#31243;&#38750;&#24120;&#30456;&#20284;&#12290;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#65292;&#35825;&#21457;&#25152;&#38656;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#23558;&#32593;&#32476;&#20998;&#20026;&#36866;&#24403;&#30340;&#25277;&#35937;&#21333;&#20803;&#65292;&#26367;&#25442;&#36825;&#20123;&#21333;&#20803;&#30340;&#28608;&#27963;&#20197;&#30830;&#23450;&#21738;&#20123;&#21442;&#19982;&#20102;&#34892;&#20026;&#65292;&#28982;&#21518;&#35299;&#37322;&#36825;&#20123;&#21333;&#20803;&#23454;&#26045;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#38598;&#12289;&#24230;&#37327;&#21644;&#24453;&#30740;&#31350;&#30340;&#21333;&#20803;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#29702;&#35299;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#21306;&#22495;&#30340;&#21151;&#33021;&#21644;&#23427;&#20204;&#32452;&#25104;&#30340;&#30005;&#36335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#65288;ACDC&#65289;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has reverse-engineered nontrivial behaviors of transformer models. These contributions required considerable effort and researcher intuition, which makes it difficult to apply the same methods to understand the complex behavior that current models display. At their core however, the workflow for these discoveries is surprisingly similar. Researchers create a data set and metric that elicit the desired model behavior, subdivide the network into appropriate abstract units, replace activations of those units to identify which are involved in the behavior, and then interpret the functions that these units implement. By varying the data set, metric, and units under investigation, researchers can understand the functionality of each neural network region and the circuits they compose. This work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to automate the identification of the important units in the network. Given a model's comput
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;ERM&#26412;&#36136;&#19978;&#21516;&#26102;&#23398;&#20064;&#20102;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;ERM&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#36136;&#37327;&#24433;&#21709;&#20102;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#65292;&#26410;&#33021;&#25429;&#33719;&#25152;&#26377;&#28508;&#22312;&#30340;&#26377;&#29992;&#29305;&#24449;&#23558;&#38480;&#21046;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11327</link><description>&lt;p&gt;
&#25506;&#32034;&#22806;&#37096;&#20998;&#24067;&#24191;&#20041;&#21270;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Feature Learning in Out-of-Distribution Generalization. (arXiv:2304.11327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11327
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;ERM&#26412;&#36136;&#19978;&#21516;&#26102;&#23398;&#20064;&#20102;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;ERM&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#36136;&#37327;&#24433;&#21709;&#20102;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#65292;&#26410;&#33021;&#25429;&#33719;&#25152;&#26377;&#28508;&#22312;&#30340;&#26377;&#29992;&#29305;&#24449;&#23558;&#38480;&#21046;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#24191;&#20041;&#21270;&#30340;&#22833;&#36133;&#65292;&#24120;&#35265;&#30340;&#35299;&#37322;&#26159;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#27169;&#22411;&#23398;&#20064;&#21040;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#32780;&#19981;&#26159;&#26399;&#26395;&#30340;&#19981;&#21464;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#20960;&#39033;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#31181;&#35299;&#37322;&#65292;&#21457;&#29616;&#28145;&#24230;&#32593;&#32476;&#21487;&#33021;&#24050;&#32463;&#23398;&#21040;&#20102;&#36275;&#22815;&#22909;&#30340;&#29305;&#24449;&#36827;&#34892;OOD&#24191;&#20041;&#21270;&#12290;&#36825;&#22330;&#36777;&#35770;&#25193;&#23637;&#21040;&#20102;&#35768;&#22810;OOD&#24191;&#20041;&#21270;&#20219;&#21153;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#32452;&#32455;&#21644;OOD&#24615;&#33021;&#30456;&#20851;&#24615;&#20013;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#20284;&#20046;&#30456;&#20114;&#30683;&#30462;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;ERM&#26412;&#36136;&#19978;&#21516;&#26102;&#23398;&#20064;&#20102;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;ERM&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#36136;&#37327;&#26174;&#33879;&#24433;&#21709;&#20102;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#65292;&#22240;&#20026;OOD&#23545;&#35937;&#24456;&#23569;&#23398;&#20064;&#21040;&#26032;&#21151;&#33021;&#12290;&#26410;&#33021;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#25429;&#33719;&#25152;&#26377;&#28508;&#22312;&#30340;&#26377;&#29992;&#29305;&#24449;&#23558;&#36827;&#19968;&#27493;&#38480;&#21046;&#26368;&#32456;&#30340;OOD&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common explanation for the failure of out-of-distribution (OOD) generalization is that the model trained with empirical risk minimization (ERM) learns spurious features instead of the desired invariant features. However, several recent studies challenged this explanation and found that deep networks may have already learned sufficiently good features for OOD generalization. The debate extends to the in-distribution and OOD performance correlations along with training or fine-tuning neural nets across a variety of OOD generalization tasks. To understand these seemingly contradicting phenomena, we conduct a theoretical investigation and find that ERM essentially learns both spurious features and invariant features. On the other hand, the quality of learned features during ERM pre-training significantly affects the final OOD performance, as OOD objectives rarely learn new features. Failing to capture all the underlying useful features during pre-training will further limit the final OOD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#25554;&#20540;&#21644;&#23383;&#20856;&#23398;&#20064;&#30340;&#27844;&#28431;&#23450;&#20301;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;Modena&#26696;&#20363;&#24471;&#21040;&#20102;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10932</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#25554;&#20540;&#30340;&#27700;&#32593;&#27844;&#28431;&#23450;&#20301;&#20013;&#30340;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Dictionaries from Physical-Based Interpolation for Water Network Leak Localization. (arXiv:2304.10932v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#25554;&#20540;&#21644;&#23383;&#20856;&#23398;&#20064;&#30340;&#27844;&#28431;&#23450;&#20301;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;Modena&#26696;&#20363;&#24471;&#21040;&#20102;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#20272;&#35745;&#21644;&#23398;&#20064;&#30340;&#27844;&#28431;&#23450;&#20301;&#26041;&#27861;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#30001;&#25554;&#20540;&#26041;&#26696;&#22788;&#29702;&#65292;&#31532;&#20108;&#20010;&#38454;&#27573;&#32771;&#34385;&#23383;&#20856;&#23398;&#20064;&#12290;&#26032;&#25552;&#20986;&#30340;&#25554;&#20540;&#25216;&#26415;&#21033;&#29992;&#20102;&#27700;&#21147;&#36830;&#25509;&#30340;&#29289;&#29702;&#23398;&#21407;&#29702;&#65292;&#36830;&#25509;&#30456;&#37051;&#33410;&#28857;&#30340;&#28082;&#21387;&#22836;&#12290;&#21478;&#22806;&#65292;&#27531;&#24046;&#30452;&#25509;&#34987;&#25554;&#20540;&#32780;&#19981;&#26159;&#28082;&#21387;&#22836;&#20540;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#33879;&#21517;&#26696;&#20363;(Modena)&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26032;&#30340;&#25554;&#20540;&#26041;&#27861;&#22312;&#25554;&#20540;&#35823;&#24046;(&#32771;&#34385;&#29366;&#24577;&#21644;&#27531;&#24046;&#20272;&#35745;)&#21644;&#21518;&#39564;&#23450;&#20301;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a leak localization methodology based on state estimation and learning. The first is handled by an interpolation scheme, whereas dictionary learning is considered for the second stage. The novel proposed interpolation technique exploits the physics of the interconnections between hydraulic heads of neighboring nodes in water distribution networks. Additionally, residuals are directly interpolated instead of hydraulic head values. The results of applying the proposed method to a well-known case study (Modena) demonstrated the improvements of the new interpolation method with respect to a state-of-the-art approach, both in terms of interpolation error (considering state and residual estimation) and posterior localization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#29109;&#27491;&#21017;&#21270;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#30340;&#38590;&#24230;&#65292;&#24182;&#26681;&#25454;&#26679;&#26412;&#38590;&#24230;&#24809;&#32602;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.10127</link><description>&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#26679;&#26412;&#38590;&#24230;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Sample Difficulty from Pre-trained Models for Reliable Prediction. (arXiv:2304.10127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#29109;&#27491;&#21017;&#21270;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#30340;&#38590;&#24230;&#65292;&#24182;&#26681;&#25454;&#26679;&#26412;&#38590;&#24230;&#24809;&#32602;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#21644;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#23427;&#20204;&#26469;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#39044;&#27979;&#21487;&#38752;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#22312;&#22266;&#26377;&#26679;&#26412;&#38590;&#24230;&#21644;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20570;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#26679;&#26412;&#38590;&#24230;&#24863;&#30693;&#30340;&#29109;&#27491;&#21017;&#21270;&#26469;&#25351;&#23548;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#65292;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#19979;&#28216;&#35757;&#32451;&#38598;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#39640;&#26031;&#24314;&#27169;&#21644;&#30456;&#23545;&#39532;&#27663;&#36317;&#31163;&#30340;&#35745;&#31639;&#26469;&#27979;&#37327;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#38590;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#30340;&#38590;&#24230;&#33258;&#36866;&#24212;&#22320;&#24809;&#32602;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#21516;&#26102;&#25552;&#39640;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained models have achieved remarkable success in a variety of scenarios and applications, but how to leverage them to improve the prediction reliability of downstream models is undesirably under-explored. Moreover, modern neural networks have been found to be poorly calibrated and make overconfident predictions regardless of inherent sample difficulty and data uncertainty. To address this issue, we propose to utilize large-scale pre-trained models to guide downstream model training with sample difficulty-aware entropy regularization. Pre-trained models that have been exposed to large-scale datasets and do not overfit the downstream training classes enable us to measure each training sample difficulty via feature-space Gaussian modeling and relative Mahalanobis distance computation. Importantly, by adaptively penalizing overconfident prediction based on the sample's difficulty, we simultaneously improve accuracy and uncertainty calibration on various challenging benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;SVRS&#21644;AccSVRS&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20854;&#20013;&#65292;AccSVRS&#31639;&#27861;&#23454;&#29616;&#20102;&#23436;&#20840;&#26080;&#24179;&#28369;&#24615;&#65292;&#36890;&#20449;&#22797;&#26434;&#24230;&#26356;&#26159;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07504</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#22343;&#20108;&#38454;&#30456;&#20284;&#24615;&#30340;&#38543;&#26426;&#20998;&#24067;&#24335;&#20248;&#21270;&#65306;&#31639;&#27861;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis. (arXiv:2304.07504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;SVRS&#21644;AccSVRS&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20854;&#20013;&#65292;AccSVRS&#31639;&#27861;&#23454;&#29616;&#20102;&#23436;&#20840;&#26080;&#24179;&#28369;&#24615;&#65292;&#36890;&#20449;&#22797;&#26434;&#24230;&#26356;&#26159;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;$n$&#20010;&#23458;&#25143;&#31471;&#30340;&#26377;&#38480;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#28385;&#36275;&#27969;&#34892;&#30340;$\delta$-&#30456;&#20284;&#24615;&#26465;&#20214;&#21644;$\mu$-&#24378;&#20984;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65306;SVRS&#21644;AccSVRS&#65292;&#21551;&#21457;&#33258;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#38750;&#21152;&#36895;&#30340;SVRS&#26041;&#27861;&#32467;&#21512;&#20102;&#26799;&#24230;&#28369;&#21160;&#21644;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;$\tilde{\gO}(n {+} \sqrt{n}\delta/\mu)$&#65292;&#19982;&#29616;&#26377;&#30340;&#38750;&#21152;&#36895;&#31639;&#27861;&#30456;&#27604;&#26377;&#25152;&#25552;&#39640;&#12290;&#24212;&#29992;Katyusha X&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;AccSVRS&#30340;&#30452;&#25509;&#21152;&#36895;&#23454;&#38469;&#29256;&#26412;&#65292;&#20854;&#23436;&#20840;&#26080;&#24179;&#28369;&#24615;&#65292;&#36890;&#20449;&#22797;&#26434;&#24230;&#20026;$\tilde{\gO}(n {+} n^{3/4}\sqrt{\delta/\mu})$&#65292;&#22312;&#30149;&#24577;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#25509;&#36817;&#21305;&#37197;&#30340;&#19979;&#30028;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;AccSVRS&#26041;&#27861;&#30340;&#32039;&#23494;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study finite-sum distributed optimization problems with $n$-clients under popular $\delta$-similarity condition and $\mu$-strong convexity. We propose two new algorithms: SVRS and AccSVRS motivated by previous works. The non-accelerated SVRS method combines the techniques of gradient-sliding and variance reduction, which achieves superior communication complexity $\tilde{\gO}(n {+} \sqrt{n}\delta/\mu)$ compared to existing non-accelerated algorithms. Applying the framework proposed in Katyusha X, we also build a direct accelerated practical version named AccSVRS with totally smoothness-free $\tilde{\gO}(n {+} n^{3/4}\sqrt{\delta/\mu})$ communication complexity that improves upon existing algorithms on ill-conditioning cases. Furthermore, we show a nearly matched lower bound to verify the tightness of our AccSVRS method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.07056</link><description>&lt;p&gt;
&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#24863;&#30693;&#36136;&#37327;&#35780;&#20272;&#65306;&#22522;&#20934;&#21644;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#38656;&#27714;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#20351;&#24471;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#28151;&#21512;&#35270;&#39057;&#32534;&#30721;&#33539;&#22260;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#25197;&#26354;&#31867;&#22411;&#30340;&#26497;&#22823;&#22810;&#26679;&#24615;&#65292;&#20174;&#20256;&#32479;&#30340;&#28151;&#21512;&#32534;&#30721;&#26694;&#26550;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#32473;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#36825;&#26159;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547; 3,240 &#20010;&#21387;&#32553;&#30340;&#38754;&#37096;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#22810;&#20010;&#21387;&#32553;&#32423;&#21035;&#65292;&#36825;&#20123;&#29255;&#27573;&#26469;&#33258; 135 &#20010;&#28304;&#35270;&#39057;&#65292;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23398;&#20064;&#28436;&#31034;&#20013;&#34892;&#20026;&#31354;&#38388;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#20182;&#24418;&#24577;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#20195;&#29702;&#30340;&#28436;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.03833</link><description>&lt;p&gt;
&#23398;&#20064;&#28436;&#31034;&#20013;&#30340;&#34892;&#20026;&#31354;&#38388;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bridging Action Space Mismatch in Learning from Demonstrations. (arXiv:2304.03833v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23398;&#20064;&#28436;&#31034;&#20013;&#34892;&#20026;&#31354;&#38388;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#20182;&#24418;&#24577;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#20195;&#29702;&#30340;&#28436;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28436;&#31034;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#25945;&#24072;&#28436;&#31034;&#36798;&#21040;&#29305;&#23450;&#30340;&#30446;&#30340;&#65292;&#20294;&#26159;&#24403;&#25945;&#24072;&#29992;&#20110;&#28436;&#31034;&#30340;&#34892;&#20026;&#31354;&#38388;&#19982;&#23398;&#29983;&#19981;&#21516;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#34892;&#20026;&#31354;&#38388;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24418;&#24577;&#36866;&#24212;&#65288;MAIL&#65289;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#29983;&#20195;&#29702;&#26681;&#25454;&#20854;&#20182;&#24418;&#24577;&#26174;&#30528;&#19981;&#21516;&#30340;&#20195;&#29702;&#30340;&#28436;&#31034;&#36827;&#34892;&#35757;&#32451;&#12290; MAIL&#21487;&#20197;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#21482;&#35201;&#36825;&#20123;&#28436;&#31034;&#25552;&#20379;&#20102;&#19968;&#20123;&#25351;&#24341;&#65292;&#20197;&#36798;&#21040;&#39044;&#26399;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20316;&#32773;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23478;&#24237;&#24067;&#26009;&#25805;&#20316;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;MAIL&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;DRY CLOTH&#20219;&#21153; - &#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#24067;&#26009;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from demonstrations (LfD) methods guide learning agents to a desired solution using demonstrations from a teacher. While some LfD methods can handle small mismatches in the action spaces of the teacher and student, here we address the case where the teacher demonstrates the task in an action space that can be substantially different from that of the student -- thereby inducing a large action space mismatch. We bridge this gap with a framework, Morphological Adaptation in Imitation Learning (MAIL), that allows training an agent from demonstrations by other agents with significantly different morphologies (from the student or each other). MAIL is able to learn from suboptimal demonstrations, so long as they provide some guidance towards a desired solution. We demonstrate MAIL on challenging household cloth manipulation tasks and introduce a new DRY CLOTH task -- cloth manipulation in 3D task with obstacles. In these tasks, we train a visual control policy for a robot with one en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#23398;&#20064;&#26041;&#27861;&#20197;&#25269;&#24481;&#27979;&#35797;&#26102;&#25915;&#20987;&#21644;&#20998;&#24067;&#20559;&#31227;&#65292;&#22312;&#27979;&#35797;&#26102;&#24341;&#20837;&#20102;&#26032;&#30340;&#21487;&#38752;&#24615;&#20445;&#38556;&#26041;&#27861;&#65292;&#30830;&#20445;&#39044;&#27979;&#32467;&#26524;&#27491;&#30830;&#12290;&#21516;&#26102;&#65292;&#35813;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#20219;&#24847;&#27979;&#35797;&#28857;&#65292;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03370</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#23398;&#20064;&#26041;&#27861;&#24212;&#23545;&#27979;&#35797;&#26102;&#25915;&#20987;&#19982;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Reliable Learning for Test-time Attacks and Distribution Shift. (arXiv:2304.03370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#23398;&#20064;&#26041;&#27861;&#20197;&#25269;&#24481;&#27979;&#35797;&#26102;&#25915;&#20987;&#21644;&#20998;&#24067;&#20559;&#31227;&#65292;&#22312;&#27979;&#35797;&#26102;&#24341;&#20837;&#20102;&#26032;&#30340;&#21487;&#38752;&#24615;&#20445;&#38556;&#26041;&#27861;&#65292;&#30830;&#20445;&#39044;&#27979;&#32467;&#26524;&#27491;&#30830;&#12290;&#21516;&#26102;&#65292;&#35813;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#20219;&#24847;&#27979;&#35797;&#28857;&#65292;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32463;&#24120;&#34987;&#29992;&#20110;&#21363;&#20351;&#32463;&#36807;&#31934;&#24515;&#33719;&#24471;&#30340;&#35757;&#32451;&#25968;&#25454;&#20063;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#26082;&#21487;&#33021;&#26159;&#30001;&#20110;&#27979;&#35797;&#26102;&#30340;&#8220;&#23545;&#25239;&#24615;&#8221;&#25915;&#20987;&#65292;&#20063;&#21487;&#33021;&#26159;&#22240;&#20026;&#8220;&#33258;&#28982;&#8221;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#38024;&#23545;&#27979;&#35797;&#26102;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#19968;&#31181;&#26032;&#39062;&#30340;&#31283;&#20581;&#24615;&#21487;&#38752;&#24615;&#20445;&#35777;&#26041;&#27861;&#65292;&#35201;&#27714;&#23398;&#20064;&#22120;&#36755;&#20986;&#19968;&#20010;&#21487;&#38752;&#21322;&#24452; $\eta$ &#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24847;&#21619;&#30528;&#21482;&#35201;&#23545;&#25163;&#27809;&#26377;&#25200;&#21160;&#27979;&#35797;&#28857;&#36229;&#36807;&#36317;&#31163; $\eta$&#65292;&#23427;&#30340;&#39044;&#27979;&#32467;&#26524;&#23601;&#26159;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#20219;&#24847;&#27979;&#35797;&#28857;&#19978;&#37117;&#33021;&#36755;&#20986;&#26368;&#20339;&#21487;&#38752;&#24615;&#21322;&#24452;&#30340;&#26368;&#20248;&#23398;&#20064;&#22120;&#65292;&#24182;&#19988;&#29305;&#24449;&#21270;&#20102;&#21487;&#38752;&#21306;&#22495;&#21363;&#21487;&#36798;&#21040;&#32473;&#23450;&#21487;&#38752;&#24615;&#21322;&#24452;&#30340;&#28857;&#38598;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#21487;&#38752;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#27979;&#35797;&#28857;&#21487;&#33021;&#26469;&#33258;&#20110;&#19968;&#20010;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#20219;&#24847;&#20998;&#24067; $Q$&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are often used in environments which are not captured accurately even by the most carefully obtained training data, either due to the possibility of `adversarial' test-time attacks, or on account of `natural' distribution shift. For test-time attacks, we introduce and analyze a novel robust reliability guarantee, which requires a learner to output predictions along with a reliability radius $\eta$, with the meaning that its prediction is guaranteed to be correct as long as the adversary has not perturbed the test point farther than a distance $\eta$. We provide learners that are optimal in the sense that they always output the best possible reliability radius on any test point, and we characterize the reliable region, i.e. the set of points where a given reliability radius is attainable. We additionally analyze reliable learners under distribution shift, where the test points may come from an arbitrary distribution Q different from the training distribution 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03271</link><description>&lt;p&gt;
&#20351;AI&#8220;&#21475;&#28212;&#8221;&#20943;&#23569;&#30340;&#26041;&#27861;&#65306;&#25581;&#31034;&#21644;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#31192;&#23494;&#27700;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#20687;GPT-3&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24050;&#32463;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21516;&#31561;&#37325;&#35201;&#19988;&#24040;&#22823;&#30340;AI&#27169;&#22411;&#27700;&#21360;&#23578;&#26410;&#24341;&#36215;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#20363;&#22914;&#65292;&#22312;&#24494;&#36719;&#26368;&#20808;&#36827;&#30340;&#32654;&#22269;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;GPT-3&#21487;&#20197;&#30452;&#25509;&#28040;&#32791;70&#19975;&#21319;&#28165;&#27905;&#28129;&#27700;&#65288;&#30456;&#24403;&#20110;&#29983;&#20135;370&#36742;&#23453;&#39532;&#27773;&#36710;&#25110;320&#36742;&#29305;&#26031;&#25289;&#30005;&#21160;&#27773;&#36710;&#65289;&#65292;&#22914;&#26524;&#22312;&#24494;&#36719;&#30340;&#20122;&#27954;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20010;&#27700;&#28040;&#32791;&#37327;&#23558;&#22686;&#21152;&#19977;&#20493;&#65292;&#20294;&#36825;&#26679;&#30340;&#20449;&#24687;&#19968;&#30452;&#34987;&#20445;&#23494;&#12290;&#36825;&#26497;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#28129;&#27700;&#30701;&#32570;&#24050;&#25104;&#20026;&#22312;&#20154;&#21475;&#36805;&#36895;&#22686;&#38271;&#12289;&#27700;&#36164;&#28304;&#20943;&#23569;&#21644;&#32769;&#21270;&#30340;&#27700;&#22522;&#30784;&#35774;&#26045;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25152;&#26377;&#20154;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27700;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#65292;&#32780;&#19988;&#24212;&#35813;&#65292;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20197;&#36523;&#20316;&#21017;&#35299;&#20915;&#33258;&#24049;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25968;&#20985;&#37319;&#26679;&#30340;&#26597;&#35810;&#19979;&#30028;&#65292;&#22312;&#24378;&#23545;&#25968;&#20985;&#21644;&#23545;&#25968;&#20809;&#28369;&#20998;&#24067;&#20013;&#37319;&#26679;&#38656;&#35201; $\Omega(\log \kappa)$ &#26597;&#35810;&#65292;&#22312;&#37319;&#26679;&#39640;&#26031;&#20998;&#24067;&#20013;&#38656;&#35201; $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ &#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2304.02599</link><description>&lt;p&gt;
&#23545;&#25968;&#20985;&#37319;&#26679;&#30340;&#26597;&#35810;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Query lower bounds for log-concave sampling. (arXiv:2304.02599v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25968;&#20985;&#37319;&#26679;&#30340;&#26597;&#35810;&#19979;&#30028;&#65292;&#22312;&#24378;&#23545;&#25968;&#20985;&#21644;&#23545;&#25968;&#20809;&#28369;&#20998;&#24067;&#20013;&#37319;&#26679;&#38656;&#35201; $\Omega(\log \kappa)$ &#26597;&#35810;&#65292;&#22312;&#37319;&#26679;&#39640;&#26031;&#20998;&#24067;&#20013;&#38656;&#35201; $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ &#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#23545;&#25968;&#20985;&#37319;&#26679;&#22312;&#31639;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#30456;&#24212;&#30340;&#35777;&#26126;&#27492;&#20219;&#21153;&#30340;&#19979;&#30028;&#30340;&#38382;&#39064;&#20173;&#28982;&#24456;&#38590;&#65292;&#20197;&#21069;&#21482;&#30693;&#36947;&#22312;&#19968;&#32500;&#20013;&#23384;&#22312;&#36739;&#23567;&#30340;&#19979;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20197;&#19979;&#26597;&#35810;&#19979;&#30028;&#65306;&#65288;1&#65289;&#22312;&#32500;&#24230; $d\ge 2$&#20013;&#20174;&#24378;&#23545;&#25968;&#20985;&#21644;&#23545;&#25968;&#20809;&#28369;&#20998;&#24067;&#20013;&#37319;&#26679;&#38656;&#35201; $\Omega(\log \kappa)$ &#26597;&#35810;&#65292;&#36825;&#22312;&#20219;&#20309;&#22266;&#23450;&#32500;&#24230;&#19978;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#65288;2&#65289;&#20174;&#39640;&#26031;&#20998;&#24067;&#20013;&#37319;&#26679;&#38656;&#35201; $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ &#26597;&#35810;&#65288;&#22240;&#27492;&#20063;&#36866;&#29992;&#20110;&#22312;&#32500;&#25968; $d$ &#20013;&#37319;&#26679;&#19968;&#33324;&#30340;&#23545;&#25968;&#20985;&#21644;&#20809;&#28369;&#20998;&#24067;&#65289;&#65292;&#36825;&#23545;&#20110;&#39640;&#26031;&#31867;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#36825;&#37324; $\kappa$ &#26159;&#30446;&#26631;&#20998;&#24067;&#30340;&#26465;&#20214;&#25968;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#65288;1&#65289;&#19968;&#31181;&#22810;&#23610;&#24230;&#26500;&#36896;&#65292;&#21463;&#21040;&#20102;&#20851;&#20110;&#35856;&#25391;&#20998;&#26512;&#20013;&#30340;Kakeya&#29468;&#24819;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#32422;&#31616;&#65292;&#35777;&#26126;&#20102;&#22359;Krylov&#31639;&#27861;&#22312;&#27492;&#38382;&#39064;&#20013;&#26159;&#26368;&#20339;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log-concave sampling has witnessed remarkable algorithmic advances in recent years, but the corresponding problem of proving lower bounds for this task has remained elusive, with lower bounds previously known only in dimension one. In this work, we establish the following query lower bounds: (1) sampling from strongly log-concave and log-smooth distributions in dimension $d\ge 2$ requires $\Omega(\log \kappa)$ queries, which is sharp in any constant dimension, and (2) sampling from Gaussians in dimension $d$ (hence also from general log-concave and log-smooth distributions in dimension $d$) requires $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ queries, which is nearly sharp for the class of Gaussians. Here $\kappa$ denotes the condition number of the target distribution. Our proofs rely upon (1) a multiscale construction inspired by work on the Kakeya conjecture in harmonic analysis, and (2) a novel reduction that demonstrates that block Krylov algorithms are optimal for this probl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;X&#23556;&#32447;CT&#22270;&#20687;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#36719;&#20214;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#36816;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20960;&#20309;&#26694;&#26550;&#30340;&#26041;&#27861;&#26816;&#27979;&#8220;&#19981;&#21487;&#35265;&#8221;&#30340;&#30899;&#22696;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#38590;&#20197;&#36798;&#21040;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02084</link><description>&lt;p&gt;
&#12298;EduceLab-Scrolls&#65306;&#21033;&#29992;X&#23556;&#32447;CT&#20174;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#20013;&#21487;&#39564;&#35777;&#22320;&#24674;&#22797;&#25991;&#26412;&#12299;
&lt;/p&gt;
&lt;p&gt;
EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT. (arXiv:2304.02084v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;X&#23556;&#32447;CT&#22270;&#20687;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#36719;&#20214;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#36816;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20960;&#20309;&#26694;&#26550;&#30340;&#26041;&#27861;&#26816;&#27979;&#8220;&#19981;&#21487;&#35265;&#8221;&#30340;&#30899;&#22696;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#38590;&#20197;&#36798;&#21040;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#23436;&#25972;&#36719;&#20214;&#31649;&#36947;&#12290;&#36825;&#20010;&#22686;&#24378;&#30340;&#34394;&#25311;&#23637;&#24320;&#27969;&#27700;&#32447;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#23558;&#19977;&#32500;&#21644;&#20108;&#32500;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;EduceLab-Scrolls&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#20108;&#21313;&#24180;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21162;&#21147;&#12290;EduceLab-Scrolls&#21253;&#21547;&#20102;&#19968;&#32452;&#23567;&#30862;&#29255;&#21644;&#23436;&#25972;&#21367;&#36724;&#30340;&#20307;&#31215;X&#23556;&#32447;CT&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#21547;&#29992;&#20110;&#30417;&#30563;&#35757;&#32451;&#27833;&#22696;&#26816;&#27979;&#27169;&#22411;&#30340;&#20108;&#32500;&#22270;&#20687;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#21367;&#36724;&#30862;&#29255;&#30340;&#39057;&#35889;&#29031;&#29255;&#19982;&#30456;&#21516;&#30862;&#29255;&#30340;X&#23556;&#32447;CT&#22270;&#20687;&#23545;&#20934;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#26426;&#22120;&#23398;&#20064;&#30340;&#22270;&#20687;&#31354;&#38388;&#21644;&#27169;&#24577;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#31181;&#23545;&#20934;&#20801;&#35768;&#26377;&#30417;&#30563;&#22320;&#23398;&#20064;&#26816;&#27979;X&#23556;&#32447;CT&#20013;&#8220;&#38544;&#24418;&#8221;&#30899;&#22696;&#30340;&#20219;&#21153;&#65292;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#26469;&#35828;&#20063;&#26159;&#8220;&#19981;&#21487;&#33021;&#8221;&#30340;&#20219;&#21153;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#40784;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a complete software pipeline for revealing the hidden texts of the Herculaneum papyri using X-ray CT images. This enhanced virtual unwrapping pipeline combines machine learning with a novel geometric framework linking 3D and 2D images. We also present EduceLab-Scrolls, a comprehensive open dataset representing two decades of research effort on this problem. EduceLab-Scrolls contains a set of volumetric X-ray CT images of both small fragments and intact, rolled scrolls. The dataset also contains 2D image labels that are used in the supervised training of an ink detection model. Labeling is enabled by aligning spectral photography of scroll fragments with X-ray CT images of the same fragments, thus creating a machine-learnable mapping between image spaces and modalities. This alignment permits supervised learning for the detection of "invisible" carbon ink in X-ray CT, a task that is "impossible" even for human expert labelers. To our knowledge, this is the first aligned datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#24314;&#27169;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#21253;&#21547;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00200</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#30340;&#31890;&#23376;&#31995;&#32479;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion map particle systems for generative modeling. (arXiv:2304.00200v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#24314;&#27169;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#21253;&#21547;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#21644;Laplacian&#35843;&#25972;&#30340;Wasserstein&#26799;&#24230;&#19979;&#38477;&#65288;LAWGD&#65289;&#12290;&#25193;&#25955;&#26144;&#23556;&#34987;&#29992;&#26469;&#20174;&#26679;&#26412;&#20013;&#36817;&#20284;Langevin&#25193;&#25955;&#36807;&#31243;&#30340;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#23398;&#20064;&#28508;&#22312;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#24418;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LAWGD&#33021;&#22815;&#22312;&#21512;&#36866;&#30340;&#26680;&#20989;&#25968;&#36873;&#25321;&#19979;&#39640;&#25928;&#22320;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#35745;&#31639;&#29983;&#25104;&#22120;&#30340;&#35889;&#36924;&#36817;&#26469;&#26500;&#36896;&#26680;&#20989;&#25968;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20855;&#26377;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel diffusion map particle system (DMPS) for generative modeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient descent (LAWGD). Diffusion maps are used to approximate the generator of the Langevin diffusion process from samples, and hence to learn the underlying data-generating manifold. On the other hand, LAWGD enables efficient sampling from the target distribution given a suitable choice of kernel, which we construct here via a spectral approximation of the generator, computed with diffusion maps. Numerical experiments show that our method outperforms others on synthetic datasets, including examples with manifold structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#21160;&#24577;&#33976;&#39311;&#24182;&#20248;&#21270;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23545;&#24212;&#30340;&#26381;&#21153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.00083</link><description>&lt;p&gt;
Fides&#65306;&#19968;&#31181;&#21033;&#29992;&#23433;&#20840;&#25191;&#34892;&#29615;&#22659;&#23545;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#32467;&#26524;&#39564;&#35777;&#30340;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#21160;&#24577;&#33976;&#39311;&#24182;&#20248;&#21270;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23545;&#24212;&#30340;&#26381;&#21153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25935;&#24863;&#39046;&#22495;&#30340;&#37096;&#32626;&#23548;&#33268;&#20102;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#37325;&#35270;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22810;&#26041;&#35745;&#31639;&#21644;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#32473;&#23454;&#26102;&#24212;&#29992;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#65292;&#20854;&#20013;&#37319;&#29992;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#19968;&#31181;&#23454;&#26102;&#39564;&#35777;&#27169;&#22411;&#26469;&#36739;&#23569;&#22320;&#28040;&#32791;&#31354;&#38388;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#36816;&#34892;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as autonomous driving, integrity verification of the outsourced ML workload is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time validation of outsourced ML workloads. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergenc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#25216;&#26415;&#65292;&#24182;&#19988;&#23450;&#20041;&#20102;&#19968;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24230;&#37327;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17618</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#25277;&#35937;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version]. (arXiv:2303.17618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17618
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#30340;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#25216;&#26415;&#65292;&#24182;&#19988;&#23450;&#20041;&#20102;&#19968;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24230;&#37327;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#19988;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#31995;&#32479;&#25277;&#35937;&#33258;&#36866;&#24212;&#32454;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#26681;&#25454;&#26410;&#26469;&#36755;&#20986;&#30340;&#35266;&#23519;&#23558;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26159;&#21160;&#24577;&#22320;&#20197;&#19981;&#23545;&#31216;&#30340;&#26041;&#24335;&#26500;&#24314;&#30340;&#12290;&#20026;&#20102;&#23398;&#20064;&#26368;&#20248;&#32467;&#26500;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#24247;&#25176;&#27931;&#32500;&#22855;&#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36866;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#20294;&#19981;&#21463;&#38480;&#20110;&#27492;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#19978;&#36848;&#24230;&#37327;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#21487;&#33021;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#35813;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#20256;&#32479;&#30340;&#32447;&#24615;&#35268;&#21010;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive refinement procedure for smart, and scalable abstraction of dynamical systems. Our technique relies on partitioning the state space depending on the observation of future outputs. However, this knowledge is dynamically constructed in an adaptive, asymmetric way. In order to learn the optimal structure, we define a Kantorovich-inspired metric between Markov chains, and we use it as a loss function. Our technique is prone to data-driven frameworks, but not restricted to.  We also study properties of the above mentioned metric between Markov chains, which we believe could be of application for wider purpose. We propose an algorithm to approximate it, and we show that our method yields a much better computational complexity than using classical linear programming techniques.
&lt;/p&gt;</description></item><item><title>Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2303.17503</link><description>&lt;p&gt;
Pgx:&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#30340;&#24182;&#34892;&#28216;&#25103;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17503
&lt;/p&gt;
&lt;p&gt;
Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pgx&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#26827;&#30424;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#12290;&#30001;&#20110;JAX&#30340;&#33258;&#21160;&#21521;&#37327;&#21270;&#21644;&#21363;&#26102;&#32534;&#35793;&#21151;&#33021;&#65292;Pgx&#26131;&#20110;&#22312;GPU/TPU&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;A100 GPU&#19978;&#30340;Pgx&#27169;&#25311;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290;Pgx&#23454;&#29616;&#20102;&#34987;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#28216;&#25103;&#65292;&#22914;Backgammon&#65292;Shogi&#21644;Go&#12290; Pgx&#21487;&#22312;https://github.com/sotetsuk/pgx&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17475</link><description>&lt;p&gt;
&#36229;&#36234;&#36127;&#37319;&#26679;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#23884;&#20837;&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31867;&#20284;&#20110;Word2Vec&#31639;&#27861;&#20013;&#24341;&#20837;&#24182;&#22312;&#22810;&#20010;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#20248;&#21270;&#35745;&#31639;&#30340;&#29942;&#39048;&#26159;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#35745;&#31639;&#65292;&#36825;&#38656;&#35201;&#19982;&#26679;&#26412;&#22823;&#23567;&#21576;&#20108;&#27425;&#27604;&#20363;&#30340;&#25805;&#20316;&#25968;&#12290;&#36825;&#31181;&#22797;&#26434;&#24230;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25152;&#20197;&#36127;&#37319;&#26679;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19982;&#26679;&#26412;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#30340;&#26102;&#38388;&#20869;&#33719;&#24471;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36127;&#37319;&#26679;&#20250;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#30340;&#26159;&#19982;&#26368;&#21021;&#25552;&#20986;&#30340;&#19981;&#21516;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#20174;&#32780;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#36127;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;$C^k$&#65288;&#22312;&#23454;&#21464;&#37327;&#24847;&#20041;&#19979;&#65289;&#30340;&#20989;&#25968;&#65292;&#20351;&#29992;&#20855;&#26377;&#21333;&#23618;&#38544;&#34255;&#23618;&#21644;$m$&#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#38169;&#35823;&#29575;$m^{-k/(2n)}$&#23558;&#20854;&#36924;&#36817;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#36873;&#21462;&#26435;&#20540;$\sigma_j,b_j\in\mathbb{C}$&#21644;$\rho_j\in\mathbb{C}^n$&#23545;$f$&#36830;&#32493;&#65292;&#37027;&#20040;&#33719;&#24471;&#30340;&#36924;&#36817;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.16813</link><description>&lt;p&gt;
&#27973;&#23618;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#23545;$C^k$-&#20989;&#25968;&#30340;&#26368;&#20248;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Optimal approximation of $C^k$-functions using shallow complex-valued neural networks. (arXiv:2303.16813v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;$C^k$&#65288;&#22312;&#23454;&#21464;&#37327;&#24847;&#20041;&#19979;&#65289;&#30340;&#20989;&#25968;&#65292;&#20351;&#29992;&#20855;&#26377;&#21333;&#23618;&#38544;&#34255;&#23618;&#21644;$m$&#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#38169;&#35823;&#29575;$m^{-k/(2n)}$&#23558;&#20854;&#36924;&#36817;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#36873;&#21462;&#26435;&#20540;$\sigma_j,b_j\in\mathbb{C}$&#21644;$\rho_j\in\mathbb{C}^n$&#23545;$f$&#36830;&#32493;&#65292;&#37027;&#20040;&#33719;&#24471;&#30340;&#36924;&#36817;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#27973;&#23618;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#23545;&#22797;&#31435;&#26041;&#20307;&#19978;$C^k$&#65288;&#22312;&#23454;&#21464;&#37327;&#24847;&#20041;&#19979;&#65289;&#30340;&#20989;&#25968;&#36827;&#34892;&#36924;&#36817;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#21333;&#23618;&#38544;&#34255;&#23618;&#21644;$m$&#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;&#24418;&#22914;$z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$&#30340;&#32593;&#32476;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#36825;&#31181;&#24418;&#24335;&#30340;&#20989;&#25968;&#36924;&#36817;$C^k \left(\Omega_n;\mathbb{C}\right)$&#20013;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24403;$m\to\infty$&#26102;&#35823;&#24046;&#20026;$m^{-k/(2n)}$.&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#36873;&#21462;&#26435;&#20540;$\sigma_j,b_j\in\mathbb{C}$&#21644;$\rho_j\in\mathbb{C}^n$&#23545;$f$&#36830;&#32493;&#24182;&#19988;&#22312;&#36825;&#31181;&#36830;&#32493;&#24615;&#20551;&#35774;&#19979;&#33719;&#24471;&#30340;&#36924;&#36817;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a quantitative result for the approximation of functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube $\Omega_n := [-1,1]^n +i[-1,1]^n\subseteq \mathbb{C}^n$ using shallow complex-valued neural networks. Precisely, we consider neural networks with a single hidden layer and $m$ neurons, i.e., networks of the form $z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$ and show that one can approximate every function in $C^k \left( \Omega_n; \mathbb{C}\right)$ using a function of that form with error of the order $m^{-k/(2n)}$ as $m \to \infty$, provided that the activation function $\phi: \mathbb{C} \to \mathbb{C}$ is smooth but not polyharmonic on some non-empty open set. Furthermore, we show that the selection of the weights $\sigma_j, b_j \in \mathbb{C}$ and $\rho_j \in \mathbb{C}^n$ is continuous with respect to $f$ and prove that the derived rate of approximation is optimal under this continuity assumption. We also discuss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#30340;&#23616;&#37096;&#32447;&#24615;&#21270;&#27010;&#29575;&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29305;&#24449;&#21270;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16698</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#30340;&#23616;&#37096;&#32447;&#24615;&#21270;&#27010;&#29575;&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic inverse optimal control with local linearization for non-linear partially observable systems. (arXiv:2303.16698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#30340;&#23616;&#37096;&#32447;&#24615;&#21270;&#27010;&#29575;&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29305;&#24449;&#21270;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29305;&#24449;&#21270;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#35201;&#27714;&#24050;&#30693;&#25511;&#21046;&#20449;&#21495;&#65292;&#25110;&#32773;&#20165;&#38480;&#20110;&#23436;&#20840;&#21487;&#35266;&#27979;&#25110;&#32447;&#24615;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#29575;&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#30340;&#20002;&#22833;&#25511;&#21046;&#20449;&#21495;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#35813;&#26041;&#27861;&#32479;&#19968;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#30340;&#24863;&#35273;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;&#22122;&#22768;&#29305;&#24449;&#30340;&#26174;&#24335;&#27169;&#22411;&#20197;&#21450;&#23616;&#37096;&#32447;&#24615;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#36817;&#20284;&#20284;&#28982;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#29256;&#26412;&#30340;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#65292;&#23548;&#33322;&#20219;&#21153;&#21644;&#25163;&#21160;&#36798;&#21040;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#21487;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#21040;&#24863;&#35273;&#36816;&#21160;&#31070;&#32463;&#31185;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse optimal control methods can be used to characterize behavior in sequential decision-making tasks. Most existing work, however, requires the control signals to be known, or is limited to fully-observable or linear systems. This paper introduces a probabilistic approach to inverse optimal control for stochastic non-linear systems with missing control signals and partial observability that unifies existing approaches. By using an explicit model of the noise characteristics of the sensory and control systems of the agent in conjunction with local linearization techniques, we derive an approximate likelihood for the model parameters, which can be computed within a single forward pass. We evaluate our proposed method on stochastic and partially observable version of classic control tasks, a navigation task, and a manual reaching task. The proposed method has broad applicability, ranging from imitation learning to sensorimotor neuroscience.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;PDExplain&#65292;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#65292;&#26497;&#22823;&#22320;&#21327;&#21161;&#20102;&#24314;&#31435;&#29289;&#29702;&#31185;&#23398;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#29616;&#35937;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.15827</link><description>&lt;p&gt;
PDExplain&#65306;PDEs &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24773;&#22659;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PDExplain: Contextual Modeling of PDEs in the Wild. (arXiv:2303.15827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15827
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PDExplain&#65292;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#65292;&#26497;&#22823;&#22320;&#21327;&#21161;&#20102;&#24314;&#31435;&#29289;&#29702;&#31185;&#23398;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#29616;&#35937;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;PDExplain&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#25805;&#20316;&#21592;&#23450;&#20041;&#30340;PDE&#23478;&#26063;&#30340;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#23478;&#26063;&#30340;&#19968;&#33324;&#24418;&#24335;&#36827;&#34892;&#39304;&#36865;&#12290;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#29616;&#35937;&#20013;&#25910;&#38598;&#21040;&#30340;&#26368;&#23567;&#26679;&#26412;&#65292;&#20854;&#20013;&#26679;&#26412;&#19982; PDE &#23478;&#26063;&#30456;&#20851;&#65292;&#20294;&#19981;&#19968;&#23450;&#23646;&#20110;&#35757;&#32451;&#38454;&#27573;&#30475;&#21040;&#30340;&#20855;&#20307; PDE &#38598;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31639;&#27861;&#22914;&#20309;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;PDE&#30340;&#21487;&#35299;&#37322;&#24418;&#24335;&#65292;&#36825;&#31181;&#29305;&#24449;&#21487;&#20197;&#21327;&#21161;&#36890;&#36807;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#26469;&#23545;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32771;&#23519;&#20102;&#20854;&#22312;&#39044;&#27979;&#35823;&#24046;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an explainable method for solving Partial Differential Equations by using a contextual scheme called PDExplain. During the training phase, our method is fed with data collected from an operator-defined family of PDEs accompanied by the general form of this family. In the inference phase, a minimal sample collected from a phenomenon is provided, where the sample is related to the PDE family but not necessarily to the set of specific PDEs seen in the training phase. We show how our algorithm can predict the PDE solution for future timesteps. Moreover, our method provides an explainable form of the PDE, a trait that can assist in modelling phenomena based on data in physical sciences. To verify our method, we conduct extensive experimentation, examining its quality both in terms of prediction error and explainability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#20998;&#20026;&#37327;&#23376;&#24182;&#23545;&#37327;&#23376;&#36827;&#34892;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#65292;&#21487;&#20197;&#35299;&#37322;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#33258;&#21160;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26679;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13506</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Quantization Model of Neural Scaling. (arXiv:2303.13506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#20998;&#20026;&#37327;&#23376;&#24182;&#23545;&#37327;&#23376;&#36827;&#34892;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#65292;&#21487;&#20197;&#35299;&#37322;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#33258;&#21160;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26679;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#23450;&#24459;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#35299;&#37322;&#20102;&#35266;&#23519;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#24130;&#24459;&#19979;&#38477;&#20197;&#21450;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#20986;&#29616;&#26032;&#33021;&#21147;&#30340;&#31361;&#28982;&#31361;&#30772;&#12290;&#25105;&#20204;&#20174;&#25152;&#35859;&#30340;&#8220;&#37327;&#21270;&#20551;&#35774;&#8221;&#20013;&#25512;&#23548;&#20986;&#36825;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#21151;&#33021;&#34987;&#37327;&#21270;&#20026;&#31163;&#25955;&#22359;&#65288;&#8220;&#37327;&#23376;&#8221;&#65289;&#12290;&#25105;&#20204;&#22312;&#38477;&#24207;&#23398;&#20064;&#39057;&#29575;&#20013;&#23398;&#20064;&#37327;&#23376;&#65292;&#24182;&#34920;&#26126;&#24403;&#37327;&#23376;&#34987;&#20197;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#26102;&#65292;&#22312;&#20351;&#29992;&#39057;&#29575;&#20013;&#20351;&#29992;&#24130;&#24459;&#21487;&#20197;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#36825;&#20010;&#39044;&#27979;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32553;&#25918;&#26354;&#32447;&#22914;&#20309;&#20998;&#35299;&#12290;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#65292;&#25105;&#20204;&#33258;&#21160;&#21457;&#29616;&#22810;&#26679;&#30340;&#27169;&#22411;&#33021;&#21147;&#65288;&#37327;&#23376;&#65289;&#65292;&#24182;&#21457;&#29616;&#23545;&#24212;&#23376;&#38382;&#39064;&#30340;&#20998;&#24067;&#19982;&#25105;&#20204;&#29702;&#35770;&#39044;&#27979;&#30340;&#31070;&#32463;&#32553;&#25918;&#25351;&#25968;&#20135;&#29983;&#20102;&#20860;&#23481;&#24615;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the $\textit{Quantization Model}$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the $\textit{Quantization Hypothesis}$, where learned network capabilities are quantized into discrete chunks ($\textit{quanta}$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Latent Slot Diffusion (LSD)&#27169;&#22411;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#20063;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#30417;&#30563;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.10834</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10834
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Latent Slot Diffusion (LSD)&#27169;&#22411;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#20063;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#30417;&#30563;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#30340;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#31361;&#26174;&#20102;&#24378;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#30340;&#25972;&#21512;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Latent Slot Diffusion (LSD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#30446;&#26631;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23558;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#26367;&#25442;&#20026;&#20197;&#23545;&#35937;&#27133;&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#27169;&#22411;&#65307;&#20854;&#27425;&#65292;&#23427;&#20063;&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#20687;&#25991;&#26412;&#36825;&#26679;&#30340;&#30417;&#30563;&#27880;&#37322;&#32780;&#33021;&#22815;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#23545;&#35937;&#20013;&#24515;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#39318;&#27425;&#22312;FFHQ&#25968;&#25454;&#38598;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09716</link><description>&lt;p&gt;
&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#25919;&#31574;&#36845;&#20195;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#20004;&#20010;&#38454;&#27573;: &#23398;&#20064;&#38454;&#27573;&#21644;&#35268;&#21010;&#38454;&#27573;&#12290;&#22312;&#26631;&#20934;MDPs&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20215;&#20540;&#36845;&#20195;&#25110;&#31574;&#30053;&#36845;&#20195;&#26469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#36845;&#20195;&#21464;&#20307;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#30340;&#22522;&#20934;&#20197;&#21450;&#19968;&#31181;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#21644;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#12290;&#30740;&#31350;&#27169;&#25311;&#20102;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#24341;&#20837;&#30340;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25439;&#22833;&#26816;&#26597;&#26469;&#26816;&#27979;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06999</link><description>&lt;p&gt;
&#36890;&#36807;&#25439;&#22833;&#26816;&#26597;&#22312;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Identifying Label Errors in Object Detection Datasets by Loss Inspection. (arXiv:2303.06999v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#30340;&#22522;&#20934;&#20197;&#21450;&#19968;&#31181;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#21644;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#12290;&#30740;&#31350;&#27169;&#25311;&#20102;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#24341;&#20837;&#30340;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25439;&#22833;&#26816;&#26597;&#26469;&#26816;&#27979;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#26631;&#27880;&#26159;&#19968;&#20010;&#26543;&#29157;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22312;&#27880;&#37322;&#36807;&#31243;&#20013;&#24456;&#23481;&#26131;&#24341;&#20837;&#38169;&#35823;&#65292;&#24182;&#19988;&#22312;&#23457;&#26680;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20934;&#30830;&#24230;&#20302;&#19979;&#30340;&#22522;&#20934;&#21644;&#22522;&#20110;&#22122;&#22768;&#26631;&#31614;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#38477;&#20302;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#30340;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#30340;&#22522;&#20934;&#20197;&#21450;&#19968;&#20010;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#21644;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#24050;&#32463;&#26631;&#35760;&#33391;&#22909;&#30340;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#27169;&#25311;&#20102;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#24341;&#20837;&#30340;&#26631;&#31614;&#38169;&#35823;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#20551;&#35774;&#24050;&#32463;&#25552;&#20379;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#24182;&#32771;&#34385;&#20004;&#20010;&#38454;&#27573;&#30340;&#20998;&#31867;&#25439;&#22833;&#21644;&#22238;&#24402;&#25439;&#22833;&#30340;&#24635;&#21644;&#12290;&#36825;&#20123;&#25439;&#22833;&#22522;&#20110;&#39044;&#27979;&#21644;&#21253;&#25324;&#27169;&#25311;&#26631;&#31614;&#38169;&#35823;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#35745;&#31639;&#65292;&#26088;&#22312;&#26816;&#27979;&#21518;&#32773;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19977;&#20010;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#19968;&#20010;&#26080;&#28145;&#24230;&#23398;&#20064;&#30340;&#26420;&#32032;&#26041;&#27861;&#65292;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Labeling datasets for supervised object detection is a dull and time-consuming task. Errors can be easily introduced during annotation and overlooked during review, yielding inaccurate benchmarks and performance degradation of deep neural networks trained on noisy labels. In this work, we for the first time introduce a benchmark for label error detection methods on object detection datasets as well as a label error detection method and a number of baselines. We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detection datasets. For our label error detection method we assume a two-stage object detector to be given and consider the sum of both stages' classification and regression losses. The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at detecting the latter. We compare our method to three baselines: a naive one without deep learning, the object detector'
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06833</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#20989;&#25968;&#20540;&#26597;&#25214;&#20854;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;&#20110;&#29983;&#25104;&#26041;&#31243;&#24207;&#21015;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#33719;&#30410;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#27604;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20851;&#27880;&#30340;&#26159;&#20511;&#37492;&#25991;&#26412;&#29983;&#25104;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26041;&#31243;&#30340;&#29305;&#23450;&#30446;&#26631;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TPSR&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#34701;&#20837;&#21040;Transformer&#35299;&#30721;&#36807;&#31243;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#21516;&#65292;TPSR&#20801;&#35768;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#65288;&#22914;&#25311;&#21512;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65289;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#36827;&#34892;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#30340;&#24847;&#20041;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;MST-based&#21010;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#38750;MST&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;MST&#26041;&#27861;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.05679</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#36827;&#34892;&#32858;&#31867;&#65306;&#33021;&#26377;&#22810;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Clustering with minimum spanning trees: How good can it be?. (arXiv:2303.05679v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#36827;&#34892;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#30340;&#24847;&#20041;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;MST-based&#21010;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#38750;MST&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;MST&#26041;&#27861;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#22312;&#35768;&#22810;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#20379;&#26041;&#20415;&#30340;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#24182;&#19988;&#35745;&#31639;&#30456;&#23545;&#36739;&#24555;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;MST&#22312;&#20302;&#32500;&#31354;&#38388;&#30340;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#20013;&#30340;&#24847;&#20041;&#31243;&#24230;&#12290;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#65288;oracle&#65289;&#31639;&#27861;&#19982;&#22823;&#37327;&#22522;&#20934;&#25968;&#25454;&#30340;&#19987;&#23478;&#26631;&#31614;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#19978;&#38480;&#65292;&#25105;&#20204;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#19981;&#26159;&#25552;&#20986;&#21478;&#19968;&#20010;&#21482;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31639;&#27861;&#65292;&#32780;&#26159;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#26368;&#26032;MST-based&#21010;&#20998;&#26041;&#26696;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#38750;MST&#31639;&#27861;&#65292;&#22914;k-means&#65292;&#39640;&#26031;&#28151;&#21512;&#65292;&#35889;&#32858;&#31867;&#65292;Birch&#65292;&#22522;&#20110;&#23494;&#24230;&#21644;&#32463;&#20856;&#23618;&#27425;&#32858;&#31867;&#31243;&#24207;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#36824;&#26159;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum spanning trees (MSTs) provide a convenient representation of datasets in numerous pattern recognition activities. Moreover, they are relatively fast to compute. In this paper, we quantify the extent to which they can be meaningful in partitional data clustering tasks in low-dimensional spaces. By identifying the upper bounds for the agreement between the best (oracle) algorithm and the expert labels from a large battery of benchmark data, we discover that MST methods are overall very competitive. Next, instead of proposing yet another algorithm that performs well on a limited set of examples, we review, study, extend, and generalise existing, state-of-the-art MST-based partitioning schemes. This leads to a few new and noteworthy approaches. Overall, Genie and the information-theoretic methods often outperform the non-MST algorithms such as k-means, Gaussian mixtures, spectral clustering, Birch, density-based, and classical hierarchical agglomerative procedures. Nevertheless, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20132;&#36890;&#37327;&#20272;&#35745;&#20013;&#30340;&#19981;&#30830;&#23450;&#21644;&#38750;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#38754;&#20132;&#36890;&#37327;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.05660</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#30456;&#20851;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#23454;&#29616;&#20132;&#36890;&#37327;&#20272;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#65306;&#35299;&#20915;&#19981;&#30830;&#23450;&#21644;&#38750;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards better traffic volume estimation: Tackling both underdetermined and non-equilibrium problems via a correlation-adaptive graph convolution network. (arXiv:2303.05660v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20132;&#36890;&#37327;&#20272;&#35745;&#20013;&#30340;&#19981;&#30830;&#23450;&#21644;&#38750;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#38754;&#20132;&#36890;&#37327;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#37327;&#26159;&#20132;&#36890;&#31649;&#29702;&#21644;&#25511;&#21046;&#25552;&#20379;&#32454;&#31890;&#24230;&#20449;&#24687;&#19981;&#21487;&#25110;&#32570;&#30340;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#20256;&#24863;&#22120;&#30340;&#26377;&#38480;&#37096;&#32626;&#65292;&#33719;&#21462;&#20840;&#38754;&#30340;&#20132;&#36890;&#37327;&#20449;&#24687;&#24182;&#19981;&#23481;&#26131;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#29305;&#23450;&#26041;&#27861;&#30340;&#25972;&#20307;&#20272;&#35745;&#20934;&#30830;&#24615;&#19978;&#65292;&#24573;&#30053;&#20102;&#20132;&#36890;&#37327;&#20272;&#35745;&#30340;&#22522;&#26412;&#25361;&#25112;&#65292;&#22240;&#27492;&#22312;&#19968;&#20123;&#20851;&#38190;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20132;&#36890;&#37327;&#20272;&#35745;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;: (1) &#30001;&#26410;&#26816;&#27979;&#21040;&#30340;&#34892;&#21160;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#20132;&#36890;&#27969;&#65292;&#20197;&#21450; (2) &#30001;&#25317;&#22581;&#20256;&#25773;&#24341;&#36215;&#30340;&#38750;&#24179;&#34913;&#20132;&#36890;&#27969;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#26080;&#27169;&#22411;&#30340;&#21644;&#30456;&#20851;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20934;&#30830;&#30340;&#20840;&#38754;&#20132;&#36890;&#37327;&#20272;&#35745;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#20102;&#37327;&#21270;&#20132;&#36890;&#36895;&#24230;&#21644;&#27969;&#37327;&#20043;&#38388;&#30340;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#24314;&#31435;&#20132;&#36890;&#27969;&#22270;&#30340;&#30456;&#20851;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic volume is an indispensable ingredient to provide fine-grained information for traffic management and control. However, due to limited deployment of traffic sensors, obtaining full-scale volume information is far from easy. Existing works on this topic primarily focus on improving the overall estimation accuracy of a particular method and ignore the underlying challenges of volume estimation, thereby having inferior performances on some critical tasks. This paper studies two key problems with regard to traffic volume estimation: (1) underdetermined traffic flows caused by undetected movements, and (2) non-equilibrium traffic flows arise from congestion propagation. Here we demonstrate a graph-based deep learning method that can offer a data-driven, model-free and correlation adaptive approach to tackle the above issues and perform accurate network-wide traffic volume estimation. Particularly, in order to quantify the dynamic and nonlinear relationships between traffic speed and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05479</link><description>&lt;p&gt;
Cal-QL: &#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#39044;&#20808;&#35757;&#32451;&#29992;&#20110;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#31574;&#30053;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#26377;&#38480;&#20114;&#21160;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22312;&#32447;&#24494;&#35843;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#23432;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26377;&#25928;&#30340;&#21021;&#22987;&#21270;&#65292;&#24182;&#20351;&#20854;&#20855;&#22791;&#24555;&#36895;&#30340;&#22312;&#32447;&#24494;&#35843;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;Cal-QL&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20302;&#20272;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#21516;&#26102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#22312;&#21512;&#29702;&#30340;&#33539;&#22260;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24182;&#19988;&#20063;&#33021;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27491;&#30830;&#12289;&#38169;&#35823;&#21644;&#22806;&#22312;&#31561;&#21464;&#24615;&#30340;&#26222;&#36941;&#29702;&#35770;&#65292;&#36890;&#36807;&#36880;&#28857;&#23450;&#20041;&#37327;&#21270;&#20102;&#20989;&#25968;&#34920;&#29616;&#30340;&#27599;&#31181;&#31867;&#22411;&#31561;&#21464;&#24615;&#30340;&#31243;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#27491;&#30830;&#25110;&#22806;&#22312;&#23545;&#31216;&#24615;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290; (230&#23383;&#31526;)</title><link>http://arxiv.org/abs/2303.04745</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#27491;&#30830;&#12289;&#38169;&#35823;&#21644;&#22806;&#22312;&#31561;&#21464;&#24615;&#30340;&#26222;&#36941;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A General Theory of Correct, Incorrect, and Extrinsic Equivariance. (arXiv:2303.04745v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27491;&#30830;&#12289;&#38169;&#35823;&#21644;&#22806;&#22312;&#31561;&#21464;&#24615;&#30340;&#26222;&#36941;&#29702;&#35770;&#65292;&#36890;&#36807;&#36880;&#28857;&#23450;&#20041;&#37327;&#21270;&#20102;&#20989;&#25968;&#34920;&#29616;&#30340;&#27599;&#31181;&#31867;&#22411;&#31561;&#21464;&#24615;&#30340;&#31243;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#27491;&#30830;&#25110;&#22806;&#22312;&#23545;&#31216;&#24615;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290; (230&#23383;&#31526;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31561;&#21464;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20551;&#35774;&#22320;&#38754;&#30495;&#30456;&#20989;&#25968;&#22312;&#25972;&#20010;&#22495;&#19978;&#26159;&#23545;&#31216;&#30340;&#65292;&#19982;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#21305;&#37197;&#12290;&#31561;&#21464;&#23398;&#20064;&#25991;&#29486;&#20013;&#32570;&#23569;&#30340;&#19968;&#22359;&#26159;&#22312;&#22495;&#20013;&#20165;&#37096;&#20998;&#23384;&#22312;&#23545;&#31216;&#24615;&#26102;&#31561;&#21464;&#32593;&#32476;&#30340;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#30340;&#26222;&#36941;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27491;&#30830;&#12289;&#38169;&#35823;&#21644;&#22806;&#22312;&#31561;&#21464;&#24615;&#30340;&#36880;&#28857;&#23450;&#20041;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36830;&#32493;&#22320;&#37327;&#21270;&#20989;&#25968;&#26174;&#31034;&#30340;&#27599;&#31181;&#31867;&#22411;&#31561;&#21464;&#24615;&#30340;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#27491;&#30830;&#25110;&#22806;&#22312;&#23545;&#31216;&#24615;&#30340;&#21508;&#31181;&#31243;&#24230;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#37096;&#20998;&#19981;&#27491;&#30830;&#23545;&#31216;&#24615;&#30340;&#20998;&#31867;&#25110;&#22238;&#24402;&#35774;&#32622;&#20013;&#19981;&#21464;&#25110;&#31561;&#21464;&#32593;&#32476;&#23384;&#22312;&#38169;&#35823;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#22806;&#22312;&#31561;&#21464;&#24615;&#30340;&#28508;&#22312;&#26377;&#23475;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#23454;&#39564;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although equivariant machine learning has proven effective at many tasks, success depends heavily on the assumption that the ground truth function is symmetric over the entire domain matching the symmetry in an equivariant neural network. A missing piece in the equivariant learning literature is the analysis of equivariant networks when symmetry exists only partially in the domain. In this work, we present a general theory for such a situation. We propose pointwise definitions of correct, incorrect, and extrinsic equivariance, which allow us to quantify continuously the degree of each type of equivariance a function displays. We then study the impact of various degrees of incorrect or extrinsic symmetry on model error. We prove error lower bounds for invariant or equivariant networks in classification or regression settings with partially incorrect symmetry. We also analyze the potentially harmful effects of extrinsic equivariance. Experiments validate these results in three different 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#32467;&#26500;&#21270;&#36755;&#20986;&#38382;&#39064;&#20135;&#29983;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#36828;&#36229;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.04132</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65306;SynthIE&#21644;&#20449;&#24687;&#25552;&#21462;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#32467;&#26500;&#21270;&#36755;&#20986;&#38382;&#39064;&#20135;&#29983;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#36828;&#36229;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;LLM&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#30340;&#20219;&#21153;&#65292;&#20063;&#21487;&#20197;&#21512;&#25104;&#29983;&#25104;&#26377;&#29992;&#30340;&#25968;&#25454;&#65306;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#31034;LLM&#22312;&#21453;&#21521;&#26041;&#21521;&#19978;&#25191;&#34892;&#20219;&#21153;&#65292;&#36890;&#36807;&#20026;&#30446;&#26631;&#36755;&#20986;&#32467;&#26500;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;&#21033;&#29992;&#20219;&#21153;&#22256;&#38590;&#24230;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#22797;&#26434;&#20219;&#21153;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#39046;&#22495;&#38590;&#20197;&#25910;&#38598;&#21040;&#30495;&#23454;&#25968;&#25454;&#65292;&#33267;&#20170;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#12290;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#35777;&#26126;&#20854;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#24182;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#65288;220M&#21644;770M&#21442;&#25968;&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#31216;&#20026;SynthIE&#65292;&#20197;&#36828;&#36828;&#36229;&#36807;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#27700;&#24179;&#65288;&#20855;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03944</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#26041;&#27861;&#22312;&#38750;&#20984;&#19979;&#23618;&#21452;&#23618;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Momentum-Based Gradient Methods for Bilevel Optimization with Nonconvex Lower-Level. (arXiv:2303.03944v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20004;&#32423;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#65292;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#36229;&#21442;&#25968;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#19979;&#23618;&#38382;&#39064;&#20026;&#38750;&#20984;&#26102;&#65292;&#21452;&#23618;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19978;&#23618;&#38382;&#39064;&#21644;&#19979;&#23618;&#38382;&#39064;&#22343;&#20026;&#38750;&#20984;&#38382;&#39064;&#65292;&#24182;&#19988;&#19979;&#23618;&#38382;&#39064;&#28385;&#36275;Polyak-Lojasiewicz (PL)&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#39640;&#25928;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization is a popular two-level hierarchical optimization, which has been widely applied to many machine learning tasks such as hyperparameter learning, meta learning and continual learning. Although many bilevel optimization methods recently have been developed, the bilevel methods are not well studied when the lower-level problem is nonconvex. To fill this gap, in the paper, we study a class of nonconvex bilevel optimization problems, where both upper-level and lower-level problems are nonconvex, and the lower-level problem satisfies Polyak-{\L}ojasiewicz (PL) condition. We propose an efficient momentum-based gradient bilevel method (MGBiO) to solve these deterministic problems. Meanwhile, we propose a class of efficient momentum-based stochastic gradient bilevel methods (MSGBiO and VR-MSGBiO) to solve these stochastic problems. Moreover, we provide a useful convergence analysis framework for our methods. Specifically, under some mild conditions, we prove that our MGBiO m
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#32858;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23458;&#25143;&#29305;&#23450;&#23646;&#24615;&#25512;&#26029;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#21253;&#25324;&#24046;&#20998;&#38544;&#31169;&#21644;&#23433;&#20840;&#32858;&#21512;&#65292;&#20294;&#23427;&#20204;&#37117;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#20445;&#25252;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.03908</link><description>&lt;p&gt;
&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#23433;&#20840;&#32858;&#21512;&#30340;&#23458;&#25143;&#29305;&#23450;&#23646;&#24615;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Client-specific Property Inference against Secure Aggregation in Federated Learning. (arXiv:2303.03908v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03908
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#32858;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23458;&#25143;&#29305;&#23450;&#23646;&#24615;&#25512;&#26029;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#21253;&#25324;&#24046;&#20998;&#38544;&#31169;&#21644;&#23433;&#20840;&#32858;&#21512;&#65292;&#20294;&#23427;&#20204;&#37117;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#20445;&#25252;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#20043;&#38388;&#21327;&#20316;&#35757;&#32451;&#20849;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#21327;&#35843;&#35757;&#32451;&#30340;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#21327;&#35843;&#12290;&#23613;&#31649;&#22312;&#32852;&#37030;&#35757;&#32451;&#26399;&#38388;&#20165;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#25110;&#20854;&#20182;&#27169;&#22411;&#26356;&#26032;&#65292;&#32780;&#19981;&#26159;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#20294;&#35768;&#22810;&#25915;&#20987;&#34920;&#26126;&#20173;&#28982;&#26377;&#21487;&#33021;&#25512;&#26029;&#20986;&#25935;&#24863;&#20449;&#24687;&#65292;&#22914;&#25104;&#21592;&#36523;&#20221;&#12289;&#23646;&#24615;&#25110;&#21442;&#19982;&#32773;&#25968;&#25454;&#30340;&#23436;&#20840;&#37325;&#24314;&#12290;&#23613;&#31649;&#24046;&#20998;&#38544;&#31169;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#38544;&#31169;&#25915;&#20987;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20063;&#22240;&#20854;&#23545;&#25928;&#29992;&#30340;&#36127;&#38754;&#24433;&#21709;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#21478;&#19968;&#20010;&#21487;&#33021;&#30340;&#38450;&#24481;&#26159;&#20351;&#29992;&#23433;&#20840;&#32858;&#21512;&#65292;&#23427;&#20801;&#35768;&#26381;&#21153;&#22120;&#20165;&#35775;&#38382;&#32858;&#21512;&#30340;&#26356;&#26032;&#65292;&#32780;&#19981;&#26159;&#27599;&#20010;&#21333;&#29420;&#30340;&#26356;&#26032;&#65292;&#24182;&#19988;&#30001;&#20110;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#36136;&#37327;&#65292;&#22240;&#27492;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#32467;&#21512;&#30001;&#27599;&#36718;&#20013;&#19981;&#21516;&#32452;&#21512;&#30340;&#23458;&#25143;&#29983;&#25104;&#30340;&#32858;&#21512;&#26356;&#26032;&#21487;&#33021;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has become a widely used paradigm for collaboratively training a common model among different participants with the help of a central server that coordinates the training. Although only the model parameters or other model updates are exchanged during the federated training instead of the participant's data, many attacks have shown that it is still possible to infer sensitive information such as membership, property, or outright reconstruction of participant data. Although differential privacy is considered an effective solution to protect against privacy attacks, it is also criticized for its negative effect on utility. Another possible defense is to use secure aggregation which allows the server to only access the aggregated update instead of each individual one, and it is often more appealing because it does not degrade model quality. However, combining only the aggregated updates, which are generated by a different composition of clients in every round, may still 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02265</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#20154;&#31867;&#20114;&#21160;&#26159;&#26368;&#22797;&#26434;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#20154;&#31867;&#24448;&#24448;&#30001;&#20110;&#22797;&#26434;&#30340;&#20559;&#35265;&#32780;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20195;&#29702;&#26368;&#32456;&#20250;&#24433;&#21709;&#36825;&#20123;&#20154;&#25152;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24433;&#21709;&#26469;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#23637;&#24320;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#19982;&#20154;&#21592;&#36827;&#34892;&#22312;&#32447;&#22521;&#35757;&#65288;&#36825;&#24448;&#24448;&#22826;&#26114;&#36149;&#21644;&#19981;&#23433;&#20840;&#65289;&#65292;&#20063;&#19981;&#20551;&#35774;&#26377;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#20219;&#21153;&#22870;&#21169;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#23398;&#20064;&#32452;&#21512;&#34892;&#20026;&#30340;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#23548;&#33268;&#26356;&#29702;&#24819;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#34892;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#21487;&#20197;&#23398;&#20064;&#31574;&#30053;&#26469;&#24433;&#21709;&#21644;&#25913;&#21892;&#20154;&#31867;&#34892;&#20026;&#65292;&#23613;&#31649;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#26399;&#26395;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22312;&#25968;&#25454;&#20849;&#20139;&#20013;&#20445;&#25252;&#25688;&#35201;&#32479;&#35745;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#34913;&#37327;&#38544;&#31169;&#39118;&#38505;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35777;&#26126;&#20102;&#38544;&#31169;&#21644;&#22833;&#30495;&#20043;&#38388;&#30340;&#26435;&#34913;&#23384;&#22312;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31867;&#36866;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#37327;&#21270;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19982;&#19979;&#30028;&#21305;&#37197;&#65292;&#26368;&#32456;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;-&#22833;&#30495;&#26435;&#34913;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.02014</link><description>&lt;p&gt;
&#25968;&#25454;&#20849;&#20139;&#20013;&#30340;&#25688;&#35201;&#32479;&#35745;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Summary Statistic Privacy in Data Sharing. (arXiv:2303.02014v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02014
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22312;&#25968;&#25454;&#20849;&#20139;&#20013;&#20445;&#25252;&#25688;&#35201;&#32479;&#35745;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#34913;&#37327;&#38544;&#31169;&#39118;&#38505;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35777;&#26126;&#20102;&#38544;&#31169;&#21644;&#22833;&#30495;&#20043;&#38388;&#30340;&#26435;&#34913;&#23384;&#22312;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31867;&#36866;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#37327;&#21270;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19982;&#19979;&#30028;&#21305;&#37197;&#65292;&#26368;&#32456;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;-&#22833;&#30495;&#26435;&#34913;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#25968;&#25454;&#25345;&#26377;&#32773;&#24076;&#26395;&#19982;&#25509;&#25910;&#32773;&#20849;&#20139;&#25968;&#25454;&#65292;&#21516;&#26102;&#21448;&#19981;&#36879;&#38706;&#25968;&#25454;&#20998;&#24067;&#30340;&#26576;&#20123;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24179;&#22343;&#20540;&#65292;&#26631;&#20934;&#24046;&#65289;&#30340;&#24773;&#26223;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#36890;&#36807;&#38543;&#26426;&#21270;&#26426;&#21046;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25688;&#35201;&#32479;&#35745;&#38544;&#31169;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#27492;&#31867;&#26426;&#21046;&#30340;&#38544;&#31169;&#39118;&#38505;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22522;&#20110;&#23545;&#20110;&#25915;&#20987;&#32773;&#22312;&#26576;&#20010;&#38408;&#20540;&#20869;&#29468;&#27979;&#20998;&#24067;&#31192;&#23494;&#30340;&#26368;&#22351;&#24773;&#20917;&#27010;&#29575;&#12290;&#23558;&#22833;&#30495;&#23450;&#20041;&#20026;&#30495;&#23454;&#25968;&#25454;&#19982;&#21457;&#24067;&#25968;&#25454;&#20043;&#38388;&#30340;&#26368;&#22351;&#24773;&#20917;Wasserstein-1&#36317;&#31163;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38544;&#31169;&#21644;&#22833;&#30495;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#19979;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#37327;&#21270;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35813;&#37327;&#21270;&#26426;&#21046;&#30340;&#38544;&#31169;-&#22833;&#30495;&#26435;&#34913;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19982;&#25105;&#20204;&#30340;&#19979;&#30028;&#21305;&#37197;&#65292;&#26368;&#22810;&#30456;&#24046;&#19968;&#20123;&#36739;&#23567;&#30340;&#24120;&#25968;&#22240;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#37327;&#21270;&#26426;&#21046;&#23454;&#29616;&#26356;&#22909;&#30340;&#38544;&#31169;-&#22833;&#30495;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a setting where a data holder wishes to share data with a receiver, without revealing certain summary statistics of the data distribution (e.g., mean, standard deviation). It achieves this by passing the data through a randomization mechanism. We propose summary statistic privacy, a metric for quantifying the privacy risk of such a mechanism based on the worst-case probability of an adversary guessing the distributional secret within some threshold. Defining distortion as a worst-case Wasserstein-1 distance between the real and released data, we prove lower bounds on the tradeoff between privacy and distortion. We then propose a class of quantization mechanisms that can be adapted to different data distributions. We show that the quantization mechanism's privacy-distortion tradeoff matches our lower bounds under certain regimes, up to small constant factors. Finally, we demonstrate on real-world datasets that the proposed quantization mechanisms achieve better privacy-distorti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#30740;&#31350;&#20102;&#22312;ImageNet&#19978;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#21457;&#29616;&#36890;&#36807;&#36731;&#24494;&#25913;&#21464;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#26696;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20462;&#25913;&#21518;&#30340;ConvNeXt&#22312;&#24050;&#35265;&#23041;&#32961;&#27169;&#22411;&#19979;&#33719;&#24471;&#20102;&#26368;&#40065;&#26834;&#30340;&#32467;&#26524;&#65292;&#32780;ViT + ConvStem&#22312;&#26410;&#35265;&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;&#27867;&#21270;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.01870</link><description>&lt;p&gt;
&#23545;&#20110;ImageNet&#30340;&#23545;&#25239;&#35757;&#32451;&#20877;&#25506;&#65306;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#36328;&#23041;&#32961;&#27169;&#22411;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models. (arXiv:2303.01870v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#30740;&#31350;&#20102;&#22312;ImageNet&#19978;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#21457;&#29616;&#36890;&#36807;&#36731;&#24494;&#25913;&#21464;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#26696;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20462;&#25913;&#21518;&#30340;ConvNeXt&#22312;&#24050;&#35265;&#23041;&#32961;&#27169;&#22411;&#19979;&#33719;&#24471;&#20102;&#26368;&#40065;&#26834;&#30340;&#32467;&#26524;&#65292;&#32780;ViT + ConvStem&#22312;&#26410;&#35265;&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;&#27867;&#21270;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23545;&#20110;ResNet&#26550;&#26500;&#21644;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#22914;CIFAR&#26469;&#35828;&#65292;&#23545;&#25239;&#35757;&#32451;&#24050;&#26377;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;ImageNet&#32780;&#35328;&#65292;&#20102;&#35299;&#36739;&#23569;&#12290;&#37492;&#20110;&#26368;&#36817;&#26377;&#20851;transformers&#26159;&#21542;&#27604;&#21367;&#31215;&#32593;&#32476;&#26356;&#22362;&#22266;&#30340;&#20105;&#35770;&#65292;&#25105;&#20204;&#37325;&#26032;&#30740;&#31350;&#20102;&#22312;ImageNet&#19978;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#27604;&#36739;&#20102;ViTs&#21644;ConvNeXts&#30340;&#24615;&#33021;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#26550;&#26500;&#30340;&#24494;&#23567;&#25913;&#21464;&#65292;&#23588;&#20854;&#26159;&#29992;ConvStem&#26367;&#25442;PatchStem&#20197;&#21450;&#35757;&#32451;&#26041;&#26696;&#65292;&#23545;&#25152;&#33719;&#24471;&#30340;&#40065;&#26834;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#36825;&#20123;&#25913;&#21464;&#19981;&#20165;&#25552;&#39640;&#20102;&#22312;&#24050;&#35265;$\ell_\infty$&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#26356;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#23545;&#26410;&#35265;$\ell_1/\ell_2$&#25915;&#20987;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20462;&#25913;&#21518;&#30340;ConvNeXt&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#21644;FLOPs&#33539;&#22260;&#20869;&#33719;&#24471;&#20102;&#26368;&#40065;&#26834;&#30340;$\ell_\infty$&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#30340;ViT + ConvStem&#22312;&#26410;&#35265;&#23041;&#32961;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen $\ell_\infty$-threat model, but even more so improve generalization to unseen $\ell_1/\ell_2$-attacks. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust $\ell_\infty$-models across different ranges of model parameters and FLOPs, while our ViT + ConvStem yields the best generalization to unseen threat models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;-GlucoSynth&#65292;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#26469;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#65292;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#23433;&#20840;&#30340;&#21516;&#26102;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;.</title><link>http://arxiv.org/abs/2303.01621</link><description>&lt;p&gt;
GlucoSynth&#65306;&#29983;&#25104;&#24046;&#20998;&#31169;&#26377;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces. (arXiv:2303.01621v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;-GlucoSynth&#65292;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#26469;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#65292;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#23433;&#20840;&#30340;&#21516;&#26102;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#31169;&#26377;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#30340;&#38382;&#39064;&#65292;&#36825;&#20010;&#20219;&#21153;&#21487;&#25512;&#24191;&#21040;&#35768;&#22810;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#34880;&#31958;&#25968;&#25454;&#30340;&#20808;&#22825;&#29305;&#24449;&#65292;&#20063;&#26080;&#27861;&#22312;&#19981;&#20005;&#37325;&#38477;&#20302;&#21512;&#25104;&#25968;&#25454;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20219;&#20309;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GlucoSynth&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;GAN&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#34880;&#31958;&#36712;&#36857;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#32771;&#34385;&#26102;&#24207;&#21160;&#24577;&#30340;&#21516;&#26102;&#65292;&#20445;&#30041;&#36712;&#36857;&#20013;motif&#65288;&#34880;&#31958;&#20107;&#20214;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;120&#19975;&#26465;&#34880;&#31958;&#36712;&#36857;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65307;GlucoSynth&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAP-IQ&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20219;&#24847;&#38454;Shapley&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#36924;&#36817;&#36136;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.01179</link><description>&lt;p&gt;
SHAP-IQ: &#20219;&#24847;&#38454;Shapley interaction&#30340;&#32479;&#19968;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SHAP-IQ: Unified Approximation of any-order Shapley Interactions. (arXiv:2303.01179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAP-IQ&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20219;&#24847;&#38454;Shapley&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#36924;&#36817;&#36136;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30740;&#31350;&#20013;&#65292;Shapley&#20540;&#65288;SV&#65289;&#36890;&#24120;&#34987;&#24212;&#29992;&#20110;&#30830;&#23450;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290; Shapley interaction indices&#23558;SV&#25193;&#23637;&#20026;&#23450;&#20041;&#20219;&#24847;&#38454;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#24471;&#20998;&#12290;&#23450;&#20041;&#29420;&#29305;&#30340;Shapley interaction index&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#36804;&#20170;&#20026;&#27490;&#24050;&#32463;&#25552;&#20986;&#20102;&#19977;&#20010;&#23450;&#20041;&#65292;&#20854;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#25152;&#36873;&#25321;&#30340;&#20844;&#29702;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#23450;&#20041;&#37117;&#38656;&#35201;&#29305;&#23450;&#30340;&#36924;&#36817;&#25216;&#26415;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#37319;&#26679;&#30340;&#26377;&#25928;&#36924;&#36817;&#26041;&#27861;SHAPley Interaction Quantification&#65288;SHAP-IQ&#65289;&#65292;&#20197;&#35745;&#31639;&#20219;&#24847;&#22522;&#25968;&#20132;&#20114;&#25351;&#25968;&#65288;CII&#65289;&#30340;Shapley&#20114;&#21160;&#12290;&#21363;&#28385;&#36275;&#32447;&#24615;&#12289;&#23545;&#31216;&#21644;&#34394;&#25311;&#20844;&#29702;&#30340;&#20132;&#20114;&#25351;&#25968;&#12290;SHAP-IQ&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#20026;&#20854;&#36924;&#36817;&#36136;&#37327;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#28857;&#20272;&#35745;&#30340;&#26041;&#24046;&#20272;&#35745;&#12290;&#23545;&#20110;SV&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#36924;&#36817;&#26041;&#27861;&#19982;&#31934;&#30830;&#35745;&#31639;&#19968;&#33268;&#12290;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;SHAP-IQ&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature importance scores for any black box model. Shapley interaction indices extend the SV to define any-order feature interaction scores. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our app
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10850</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#24320;&#21457;&#23545;&#35805;&#31649;&#29702;&#65288;DM&#65289;&#20195;&#29702;&#65292;&#23454;&#29616;&#38750;&#30446;&#26631;&#23548;&#21521;&#65292;&#36827;&#34892;&#23500;&#26377;&#20869;&#23481;&#30340;&#23545;&#35805;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#23545;&#35805;&#32842;&#22825;&#26426;&#22120;&#20154;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#22312;&#32447;&#25506;&#32034;&#20197;&#26377;&#25928;&#23398;&#20064;&#65292;&#32780;&#25910;&#38598;&#26032;&#39062;&#30340;&#20154;&#26426;&#20132;&#20114;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#19981;&#23433;&#20840;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38754;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#26102;&#21464;&#24471;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20197;&#35789;&#32423;&#21035;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289; - &#19968;&#31181;&#25429;&#25417;&#22810;&#26679;&#35821;&#20041;&#65292;&#29983;&#25104;&#21453;&#26144;&#19981;&#21516;&#24847;&#22270;&#30340;&#35805;&#35821;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#31649;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;MoE-LM&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#34892;&#21160;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#25968;&#25454;&#20998;&#25968;&#38543;&#26426;&#21453;&#21521;&#36807;&#31243;&#26159;&#19968;&#20010;&#38789;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#30340;DPM&#65292;&#26377;&#25928;&#20943;&#23567;&#27169;&#22411;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#22686;&#21152;&#27169;&#22411;&#20284;&#28982;&#30340;&#19979;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#26657;&#20934;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2302.10688</link><description>&lt;p&gt;
&#20851;&#20110;&#26657;&#20934;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Calibrating Diffusion Probabilistic Models. (arXiv:2302.10688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#25968;&#25454;&#20998;&#25968;&#38543;&#26426;&#21453;&#21521;&#36807;&#31243;&#26159;&#19968;&#20010;&#38789;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#30340;DPM&#65292;&#26377;&#25928;&#20943;&#23567;&#27169;&#22411;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#22686;&#21152;&#27169;&#22411;&#20284;&#28982;&#30340;&#19979;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#26657;&#20934;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPM&#65289;&#22312;&#21508;&#31181;&#29983;&#25104;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;DPM&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#36880;&#28176;&#25193;&#25955;&#25968;&#25454;&#20998;&#24067;&#30340;&#27491;&#21521;&#36807;&#31243;&#21644;&#19968;&#20010;&#20174;&#26102;&#38388;&#30456;&#20851;&#25968;&#25454;&#20998;&#25968;&#20013;&#24674;&#22797;&#25968;&#25454;&#20998;&#24067;&#30340;&#38543;&#26426;&#21453;&#21521;&#36807;&#31243;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#25968;&#25454;&#20998;&#25968;&#30340;&#38543;&#26426;&#21453;&#21521;&#36807;&#31243;&#26159;&#19968;&#20010;&#38789;&#65292;&#20174;&#20013;&#21487;&#20197;&#23548;&#20986;&#25968;&#25454;&#20998;&#25968;&#30340;&#38598;&#20013;&#30028;&#21644;&#38543;&#26426;&#20572;&#27490;&#23450;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#20219;&#24847;&#39044;&#20808;&#35757;&#32451;&#30340;DPM&#65292;&#20197;&#20943;&#23567;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#24182;&#22240;&#27492;&#22686;&#21152;&#27169;&#22411;&#20284;&#28982;&#30340;&#19979;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21508;&#31181;&#27169;&#22411;&#21442;&#25968;&#21270;&#19979;&#30340;&#19968;&#33324;&#26657;&#20934;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#26657;&#20934;&#26041;&#27861;&#20165;&#25191;&#34892;&#19968;&#27425;&#65292;&#24182;&#19988;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110;https://github.com/thudzj/Cal&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion probabilistic models (DPMs) have achieved promising results in diverse generative tasks. A typical DPM framework includes a forward process that gradually diffuses the data distribution and a reverse process that recovers the data distribution from time-dependent data scores. In this work, we observe that the stochastic reverse process of data scores is a martingale, from which concentration bounds and the optional stopping theorem for data scores can be derived. Then, we discover a simple way for calibrating an arbitrary pretrained DPM, with which the score matching loss can be reduced and the lower bounds of model likelihood can consequently be increased. We provide general calibration guidelines under various model parametrizations. Our calibration method is performed only once and the resulting models can be used repeatedly for sampling. We conduct experiments on multiple datasets to empirically validate our proposal. Our code is at https://github.com/thudzj/Cal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#19981;&#21516;&#24212;&#29992;&#65288;&#20559;&#35265;&#28040;&#38500;&#12289;&#28151;&#28102;&#35299;&#20915;&#12289;&#38544;&#31169;&#20445;&#25252;&#65289;&#36951;&#24536;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#30340;&#36951;&#24536;&#23450;&#20041;&#21644;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;SCRUB&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#24212;&#29992;&#30340;&#36951;&#24536;&#36136;&#37327;&#24230;&#37327;&#19978;&#22987;&#32456;&#26159;&#39030;&#32423;&#34920;&#29616;&#32773;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09880</link><description>&lt;p&gt;
&#36808;&#21521;&#26080;&#30028;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Towards Unbounded Machine Unlearning. (arXiv:2302.09880v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#19981;&#21516;&#24212;&#29992;&#65288;&#20559;&#35265;&#28040;&#38500;&#12289;&#28151;&#28102;&#35299;&#20915;&#12289;&#38544;&#31169;&#20445;&#25252;&#65289;&#36951;&#24536;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#30340;&#36951;&#24536;&#23450;&#20041;&#21644;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;SCRUB&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#24212;&#29992;&#30340;&#36951;&#24536;&#36136;&#37327;&#24230;&#37327;&#19978;&#22987;&#32456;&#26159;&#39030;&#32423;&#34920;&#29616;&#32773;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#36951;&#24536;&#26159;&#25351;&#20174;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#8220;&#31227;&#38500;&#8221;&#20854;&#35757;&#32451;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#21450;&#26102;&#65292;&#24182;&#19988;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#35299;&#38500;&#20559;&#35265;&#65288;RB&#65289;&#12289;&#28040;&#38500;&#28151;&#28102;&#65288;RC&#65289;&#65288;&#30001;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#26631;&#31614;&#25968;&#25454;&#24341;&#36215;&#65289;&#65292;&#20197;&#21450;&#20801;&#35768;&#29992;&#25143;&#34892;&#20351;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65288;UP&#65289;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#31687;&#30740;&#31350;&#19981;&#21516;&#24212;&#29992;&#65288;RB&#12289;RC&#12289;UP&#65289;&#30340;&#36951;&#24536;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25105;&#20204;&#35748;&#20026;&#27599;&#20010;&#24212;&#29992;&#37117;&#26377;&#33258;&#24049;&#30340;&#24536;&#35760;&#38656;&#27714;&#12289;&#24536;&#35760;&#23450;&#20041;&#21644;&#19982;&#24536;&#35760;&#36136;&#37327;&#30456;&#20851;&#30340;&#25351;&#26631;&#12290;&#23545;&#20110;UP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36951;&#24536;&#30340;&#26032;&#39062;&#36866;&#24212;&#24615;&#24378;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;SCRUB&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36951;&#24536;&#31639;&#27861;&#65292;&#22312;RB&#12289;RC&#21644;UP&#30340;&#19981;&#21516;&#24212;&#29992;&#30456;&#20851;&#24230;&#37327;&#25351;&#26631;&#19978;&#22987;&#32456;&#26159;&#24536;&#35760;&#36136;&#37327;&#30340;&#39030;&#32423;&#34920;&#29616;&#32773;&#12290;&#21516;&#26102;&#65292;SCRUB&#36824;&#22312;&#34913;&#37327;&#27169;&#24335;&#30340;&#24230;&#37327;&#25351;&#26631;&#19978;&#22987;&#32456;&#26159;&#39030;&#32423;&#34920;&#29616;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine unlearning is the problem of `removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their `right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for `forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36924;&#36817;&#27861;&#65292;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.09267</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#27861;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation Approaches to Group Distributionally Robust Optimization. (arXiv:2302.09267v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36924;&#36817;&#27861;&#65292;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#65292;&#30446;&#30340;&#26159;&#23398;&#20064;&#19968;&#20010;&#33021;&#22312;$m$&#20010;&#19981;&#21516;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;GDRO&#24314;&#27169;&#20026;&#38543;&#26426;&#20984;&#20985;&#38797;&#28857;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;$m$&#20010;&#26679;&#26412;&#30340;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#27861;(SMD)&#65292;&#33021;&#22815;&#23454;&#29616;$O(m(\log m)/\epsilon ^2)$&#20010;&#26679;&#26412;&#30340;&#22797;&#26434;&#24230;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#35299;&#65292;&#36825;&#19982;$\Omega(m/\epsilon ^2)$&#30340;&#19979;&#30028;&#24819;&#21305;&#37197;&#65292;&#38500;&#20102;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;GDRO&#26500;&#36896;&#20026;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#31616;&#21333;&#22320;&#25191;&#34892;SMD&#65292;&#21478;&#19968;&#20010;&#25191;&#34892;&#19968;&#31181;&#29992;&#20110;&#38750;&#26126;&#26174;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#21487;&#20197;&#20174;&#27599;&#20010;&#20998;&#24067;&#20013;&#32472;&#21046;&#30340;&#26679;&#26412;&#25968;&#37327;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates group distributionally robust optimization (GDRO), with the purpose to learn a model that performs well over $m$ different distributions. First, we formulate GDRO as a stochastic convex-concave saddle-point problem, and demonstrate that stochastic mirror descent (SMD), using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$ sample complexity for finding an $\epsilon$-optimal solution, which matches the $\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make use of techniques from online learning to reduce the number of samples required in each round from $m$ to $1$, keeping the same sample complexity. Specifically, we cast GDRO as a two-players game where one player simply performs SMD and the other executes an online algorithm for non-oblivious multi-armed bandits. Next, we consider a more practical scenario where the number of samples that can be drawn from each distribution is different, and propose a novel formulation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26102;&#22495;&#23567;&#27874;&#22270;&#31070;&#32463;&#32593;&#32476;(FTWGNN)&#65292;&#29992;&#20110;&#21487;&#38752;&#21644;&#21450;&#26102;&#22320;&#23545;&#20154;&#33041;&#21644;&#20132;&#36890;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.08643</link><description>&lt;p&gt;
&#24555;&#36895;&#26102;&#22495;&#23567;&#27874;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast Temporal Wavelet Graph Neural Networks. (arXiv:2302.08643v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26102;&#22495;&#23567;&#27874;&#22270;&#31070;&#32463;&#32593;&#32476;(FTWGNN)&#65292;&#29992;&#20110;&#21487;&#38752;&#21644;&#21450;&#26102;&#22320;&#23545;&#20154;&#33041;&#21644;&#20132;&#36890;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#20449;&#21495;&#39044;&#27979;&#22312;&#20247;&#22810;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#31070;&#32463;&#31185;&#23398;&#21644;&#20132;&#36890;&#39046;&#22495;&#12290;&#30001;&#20110;&#32593;&#32476;&#30340;&#39640;&#24230;&#22797;&#26434;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#38750;&#32447;&#24615;&#30340;&#26102;&#24577;&#21160;&#24577;&#65292;&#36825;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#26041;&#20415;&#21487;&#38752;&#21644;&#21450;&#26102;&#22320;&#23545;&#20154;&#33041;&#21644;&#20132;&#36890;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26102;&#22495;&#23567;&#27874;&#22270;&#31070;&#32463;&#32593;&#32476;(FTWGNN)&#65292;&#23427;&#23545;&#22522;&#20110;&#26102;&#24207;&#25968;&#25454;&#21644;&#22522;&#30784;&#22270;&#32467;&#26500;&#30340;&#23398;&#20064;&#20219;&#21153;&#26082;&#39640;&#25928;&#21448;&#33410;&#30465;&#20869;&#23384;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#31163;&#25955;&#31354;&#38388;&#23567;&#27874;&#29702;&#35770;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#37319;&#29992;&#22810;&#20998;&#36776;&#29575;&#30697;&#38453;&#20998;&#35299;(MMF) (Kondor et al., 2014)&#26469;&#20998;&#35299;&#39640;&#23494;&#24230;&#22270;&#32467;&#26500;&#65292;&#24182;&#35745;&#31639;&#30456;&#24212;&#30340;&#31232;&#30095;&#23567;&#27874;&#22522;&#65292;&#20174;&#32780;&#26500;&#24314;&#24555;&#36895;&#23567;&#27874;&#21367;&#31215;&#20316;&#20026;&#25105;&#20204;&#26032;&#22411;&#26550;&#26500;&#30340;&#26680;&#24515;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;PEMS-BAY&#12289;METR-LA&#20132;&#36890;&#25968;&#25454;&#38598;&#21644;AJILE12 ECoG&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FTWGNN&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal signals forecasting plays an important role in numerous domains, especially in neuroscience and transportation. The task is challenging due to the highly intricate spatial structure, as well as the non-linear temporal dynamics of the network. To facilitate reliable and timely forecast for the human brain and traffic networks, we propose the Fast Temporal Wavelet Graph Neural Networks (FTWGNN) that is both time- and memory-efficient for learning tasks on timeseries data with the underlying graph structure, thanks to the theories of multiresolution analysis and wavelet theory on discrete spaces. We employ Multiresolution Matrix Factorization (MMF) (Kondor et al., 2014) to factorize the highly dense graph structure and compute the corresponding sparse wavelet basis that allows us to construct fast wavelet convolution as the backbone of our novel architecture. Experimental results on real-world PEMS-BAY, METR-LA traffic datasets and AJILE12 ECoG dataset show that FTWGNN is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;DP-SGD&#30340;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;&#20102;&#22914;&#20309;&#35774;&#32622;&#38544;&#31169;&#21442;&#25968;&#20197;&#20445;&#25252;&#20813;&#21463;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#25915;&#20987;&#30340;&#19978;&#38480;&#21644;&#21305;&#37197;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2302.07225</link><description>&lt;p&gt;
DP-SGD&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30028;&#38480;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bounding Training Data Reconstruction in DP-SGD. (arXiv:2302.07225v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;DP-SGD&#30340;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;&#20102;&#22914;&#20309;&#35774;&#32622;&#38544;&#31169;&#21442;&#25968;&#20197;&#20445;&#25252;&#20813;&#21463;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#25915;&#20987;&#30340;&#19978;&#38480;&#21644;&#21305;&#37197;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#36890;&#24120;&#34987;&#35299;&#37322;&#20026;&#23545;&#25239;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#20445;&#25252;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#21482;&#38656;&#35201;&#20445;&#25252;&#20813;&#21463;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#37027;&#20040;&#31169;&#26377;&#27169;&#22411;&#30340;&#25928;&#29992;&#21487;&#20197;&#25913;&#21892;&#65292;&#22240;&#20026;&#20445;&#25252;&#20813;&#21463;&#36825;&#20123;&#26356;&#26377;&#37326;&#24515;&#30340;&#25915;&#20987;&#38656;&#35201;&#26356;&#23569;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#22312;DP-SGD&#30340;&#19978;&#19979;&#25991;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;DP-SGD&#30340;&#20219;&#20309;&#37325;&#24314;&#25915;&#20987;&#25104;&#21151;&#30340;&#19978;&#38480;&#20197;&#21450;&#19982;&#25105;&#20204;&#36793;&#30028;&#39044;&#27979;&#30456;&#21305;&#37197;&#30340;&#25915;&#20987;&#12290;&#36825;&#20004;&#20010;&#32467;&#26524;&#20026;&#35774;&#32622;DP-SGD&#30340;&#38544;&#31169;&#21442;&#25968;&#20197;&#20445;&#25252;&#20813;&#21463;&#37325;&#24314;&#25915;&#20987;&#24320;&#36767;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but instead only wants to protect against training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24191;&#20041;&#20809;&#28369;&#24615;&#26465;&#20214;&#19979;&#30340;&#36817;&#20284;&#26368;&#20248;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;&#20102;&#22823;&#25209;&#37327;&#22823;&#23567;&#21644;&#21482;&#22522;&#20110;&#39044;&#26399;&#36895;&#29575;&#30340;&#25910;&#25947;&#30028;&#38480;&#20004;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06032</link><description>&lt;p&gt;
&#36817;&#20284;&#26368;&#20248;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#22312;&#24191;&#20041;&#20809;&#28369;&#24615;&#19979;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Non-Convex Stochastic Optimization under Generalized Smoothness. (arXiv:2302.06032v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24191;&#20041;&#20809;&#28369;&#24615;&#26465;&#20214;&#19979;&#30340;&#36817;&#20284;&#26368;&#20248;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;&#20102;&#22823;&#25209;&#37327;&#22823;&#23567;&#21644;&#21482;&#22522;&#20110;&#39044;&#26399;&#36895;&#29575;&#30340;&#25910;&#25947;&#30028;&#38480;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#20809;&#28369;&#26465;&#20214;&#65292;$(L_{0},L_{1})$-&#20809;&#28369;&#24615;&#65292;&#22312;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#20013;&#37117;&#26356;&#21152;&#29616;&#23454;&#65292;&#36825;&#36890;&#36807;&#32463;&#39564;&#21644;&#29702;&#35770;&#35777;&#25454;&#37117;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#26368;&#36817;&#30340;&#20004;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;$O(\epsilon^{-3})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20197;&#33719;&#24471;$O(\epsilon)$-&#31283;&#23450;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#32773;&#37117;&#38656;&#35201;&#19968;&#20010;&#22823;&#25209;&#37327;&#30340;&#22823;&#23567;&#65292;&#20854;&#25968;&#37327;&#32423;&#26159;$\mathrm{poly}(\epsilon^{-1})$&#65292;&#36825;&#19981;&#20165;&#22312;&#35745;&#31639;&#19978;&#24456;&#36127;&#25285;&#65292;&#20063;&#19981;&#36866;&#29992;&#20110;&#27969;&#24335;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29616;&#26377;&#30340;&#25910;&#25947;&#30028;&#38480;&#21482;&#23545;&#39044;&#26399;&#36895;&#29575;&#36827;&#34892;&#20102;&#24314;&#31435;&#65292;&#36825;&#26159;&#19981;&#36275;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21333;&#27425;&#36816;&#34892;&#26102;&#27809;&#26377;&#25552;&#20379;&#26377;&#29992;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;STORM&#31639;&#27861;&#30340;&#19968;&#20010;&#31616;&#21333;&#21464;&#20307;&#26469;&#21516;&#26102;&#35299;&#20915;&#21069;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;$(L_{0},L_{1})$-&#20809;&#28369;&#24615;&#21644;&#20223;&#23556;&#22411;&#22122;&#22768;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#30340;&#39640;&#27010;&#29575;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20026;$O(\log(1/(\delta\epsilon))\epsilon^{-3})$&#65292;&#20854;&#20013;$\delta\i
&lt;/p&gt;
&lt;p&gt;
The generalized smooth condition, $(L_{0},L_{1})$-smoothness, has triggered people's interest since it is more realistic in many optimization problems shown by both empirical and theoretical evidence. Two recent works established the $O(\epsilon^{-3})$ sample complexity to obtain an $O(\epsilon)$-stationary point. However, both require a large batch size on the order of $\mathrm{ploy}(\epsilon^{-1})$, which is not only computationally burdensome but also unsuitable for streaming applications. Additionally, these existing convergence bounds are established only for the expected rate, which is inadequate as they do not supply a useful performance guarantee on a single run. In this work, we solve the prior two problems simultaneously by revisiting a simple variant of the STORM algorithm. Specifically, under the $(L_{0},L_{1})$-smoothness and affine-type noises, we establish the first near-optimal $O(\log(1/(\delta\epsilon))\epsilon^{-3})$ high-probability sample complexity where $\delta\i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#24191;&#24179;&#22343;Lipschitz&#24179;&#28369;&#24615;&#21040;H&#246;lder&#24179;&#28369;&#24615;&#65292;&#24471;&#21040;&#20102;&#20851;&#20110;&#24179;&#22343;H&#246;lder&#24179;&#28369;&#24615;&#30340;&#19978;&#19979;&#39118;&#38505;&#30028;&#65292;&#26368;&#20248;&#30340;&#19979;&#30028;&#23545;&#25968;&#22240;&#23376;&#26368;&#22810;&#24046;&#19968;&#20010;&#65292;&#25552;&#20379;&#20102;&#29420;&#31435;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.06005</link><description>&lt;p&gt;
&#24179;&#22343;H&#246;lder&#24179;&#28369;&#24230;&#19979;&#30340;&#36817;&#20284;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-optimal learning with average H\"older smoothness. (arXiv:2302.06005v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#24191;&#24179;&#22343;Lipschitz&#24179;&#28369;&#24615;&#21040;H&#246;lder&#24179;&#28369;&#24615;&#65292;&#24471;&#21040;&#20102;&#20851;&#20110;&#24179;&#22343;H&#246;lder&#24179;&#28369;&#24615;&#30340;&#19978;&#19979;&#39118;&#38505;&#30028;&#65292;&#26368;&#20248;&#30340;&#19979;&#30028;&#23545;&#25968;&#22240;&#23376;&#26368;&#22810;&#24046;&#19968;&#20010;&#65292;&#25552;&#20379;&#20102;&#29420;&#31435;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;Ashlagi&#31561;&#20154;&#65288;COLT 2021&#65289;&#25552;&#20986;&#30340;&#24179;&#22343;Lipschitz&#24179;&#28369;&#24615;&#27010;&#24565;&#25512;&#24191;&#21040;H&#246;lder&#24179;&#28369;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20851;&#20110;&#24179;&#22343;H&#246;lder&#24179;&#28369;&#24615;&#30340;&#19978;&#19979;&#39118;&#38505;&#30028;&#65292;&#36825;&#20123;&#30028;&#30340;&#36895;&#29575;&#29978;&#33267;&#22312;&#24179;&#22343;Lipschitz&#24179;&#28369;&#24615;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#20043;&#21069;&#24050;&#30693;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19979;&#30028;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#26368;&#22810;&#24046;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#26497;&#23567;&#20540;&#29575;&#12290;&#20174;&#31639;&#27861;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#25105;&#20204;&#23545;&#24179;&#22343;&#24179;&#28369;&#24230;&#30340;&#23450;&#20041;&#26159;&#38024;&#23545;&#26410;&#30693;&#30340;&#22522;&#30784;&#20998;&#24067;&#30340;&#65292;&#22240;&#27492;&#23398;&#20064;&#32773;&#27809;&#26377;&#20989;&#25968;&#31867;&#30340;&#26174;&#24335;&#34920;&#31034;&#65292;&#26080;&#27861;&#25191;&#34892;ERM&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29420;&#31435;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize the notion of average Lipschitz smoothness proposed by Ashlagi et al. (COLT 2021) by extending it to H\"older smoothness. This measure of the "effective smoothness" of a function is sensitive to the underlying distribution and can be dramatically smaller than its classic "worst-case H\"older constant. We consider both the realizable and the agnostic (noisy) regression settings, proving upper and lower risk bounds in terms of the average H\"older smoothness; these rates improve upon both previously known rates even in the special case of average Lipschitz smoothness. Moreover, our lower bound is tight in the realizable setting up to log factors, thus we establish the minimax rate. From an algorithmic perspective, since our notion of average smoothness is defined with respect to the unknown underlying distribution, the learner does not have an explicit representation of the function class, hence is unable to execute ERM. Nevertheless, we provide distinct learning algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21333;&#20010;&#29992;&#25143;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#29992;&#25143;&#27963;&#21160;&#30340;&#35782;&#21035;&#65292;&#26377;&#26395;&#29992;&#20110;&#20154;&#26426;&#21327;&#20316;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2302.05763</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36741;&#21161;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#23454;&#29616;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications. (arXiv:2302.05763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21333;&#20010;&#29992;&#25143;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#29992;&#25143;&#27963;&#21160;&#30340;&#35782;&#21035;&#65292;&#26377;&#26395;&#29992;&#20110;&#20154;&#26426;&#21327;&#20316;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#36880;&#28176;&#20851;&#27880;&#22810;&#26041;&#38754;&#22330;&#26223;&#65292;&#21363;&#26426;&#22120;&#20154;&#19982;&#22810;&#20010;&#20154;&#29992;&#25143;&#21516;&#26102;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290; &#28982;&#32780;&#65292;&#22312;&#20154;&#26426;&#21327;&#20316;&#26041;&#38754;&#65292;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22788;&#29702;&#27492;&#31867;&#21512;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38656;&#35201;&#30340;&#25968;&#25454;&#27604;&#20856;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#35774;&#32622;&#20013;&#26356;&#19981;&#21487;&#34892;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#20108;&#20803;&#20154;&#26426;&#21327;&#20316;&#24212;&#29992;&#30340;&#24182;&#34892;&#20219;&#21153;&#22330;&#26223;&#65292;&#24182;&#25552;&#35758;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25910;&#38598;&#19982;&#22810;&#29992;&#25143;&#27963;&#21160;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#21363;&#25910;&#38598;&#19982;&#21333;&#20010;&#29992;&#25143;&#30456;&#20851;&#30340;&#25968;&#25454;&#24182;&#22312;&#21518;&#22788;&#29702;&#20013;&#21512;&#24182;&#23427;&#20204;&#65292;&#20197;&#20943;&#23569;&#20135;&#29983;&#25104;&#21452;&#35774;&#32622;&#24405;&#21046;&#30340;&#21162;&#21147;&#12290;&#25910;&#38598;&#20102;&#21333;&#20010;&#29992;&#25143;&#30340;&#27963;&#21160;&#19977;&#32500;&#39592;&#26550;&#23039;&#21183;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#25104;&#19968;&#23545;&#26469;&#39564;&#35777;&#35813;&#35821;&#21477;&#65292;&#38543;&#21518;&#65292;&#36825;&#20123;&#25968;&#25454;&#28857;&#34987;&#29992;&#20110;&#20998;&#21035;&#35757;&#32451;&#30001;LSTM&#32593;&#32476;&#21644;VAE &#28151;&#21512;&#32780;&#25104;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-robot interaction (HRI) research is progressively addressing multi-party scenarios, where a robot interacts with more than one human user at the same time. Conversely, research is still at an early stage for human-robot collaboration. The use of machine learning techniques to handle such type of collaboration requires data that are less feasible to produce than in a typical HRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRC applications. Based upon these concepts, this study also proposes an alternative way of gathering data regarding multi-user activity, by collecting data related to single users and merging them in post-processing, to reduce the effort involved in producing recordings of pair settings. To validate this statement, 3D skeleton poses of activity of single users were collected and merged in pairs. After this, such datapoints were used to separately train a long short-term memory (LSTM) network and a variational autoencoder (VAE) composed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05666</link><description>&lt;p&gt;
Jaccard&#24230;&#37327;&#25439;&#22833;&#65306;&#20351;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Jaccard&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoU&#25439;&#22833;&#26159;&#30452;&#25509;&#20248;&#21270;Jaccard&#25351;&#25968;&#30340;&#26367;&#20195;&#21697;&#12290;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#23558;IoU&#25439;&#22833;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#65292;&#19982;&#20165;&#20248;&#21270;&#20687;&#32032;&#25439;&#22833;&#65288;&#22914;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#30456;&#27604;&#65292;&#23545;&#20110;Jaccard&#25351;&#25968;&#27979;&#37327;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#26174;&#30528;&#30340;IoU&#25439;&#22833;&#26159;&#36719;Jaccard&#25439;&#22833;&#21644;Lovasz-Softmax&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25439;&#22833;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#26631;&#31614;&#19981;&#20860;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#65292;&#23427;&#20204;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;&#36719;&#26631;&#31614;&#20860;&#23481;&#65292;&#19982;&#36719;Jaccard&#25439;&#22833;&#30456;&#21516;&#12290;&#20351;&#29992;JMLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#27969;&#34892;&#30340;&#36719;&#26631;&#31614;&#29992;&#20363;&#65306;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#19977;&#20010;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;Cityscapes&#12289;PASCAL VOC&#21644;DeepGlobe Land&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;DeepGlobe Land&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#22122;&#22768;&#36807;&#31243;&#30340;&#26143;&#24418;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24191;&#27867;&#36866;&#29992;&#20110;&#25351;&#25968;&#26063;&#20013;&#30340;&#22810;&#31181;&#20998;&#24067;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#32422;&#26463;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.05259</link><description>&lt;p&gt;
&#26143;&#24418;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Star-Shaped Denoising Diffusion Probabilistic Models. (arXiv:2302.05259v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05259
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#22122;&#22768;&#36807;&#31243;&#30340;&#26143;&#24418;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24191;&#27867;&#36866;&#29992;&#20110;&#25351;&#25968;&#26063;&#20013;&#30340;&#22810;&#31181;&#20998;&#24067;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#32422;&#26463;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22823;&#22810;&#23616;&#38480;&#20110;&#39640;&#26031;&#21644;&#31163;&#25955;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26143;&#24418;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;SS-DDPM&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#22122;&#22768;&#36807;&#31243;&#30340;&#27169;&#22411;&#12290;&#22312;&#39640;&#26031;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#31561;&#25928;&#20110;&#39532;&#23572;&#21487;&#22827;DDPM&#12290;&#28982;&#32780;&#65292;&#23427;&#21487;&#20197;&#23450;&#20041;&#21644;&#36866;&#29992;&#20110;&#20219;&#24847;&#22122;&#22768;&#20998;&#24067;&#65292;&#24182;&#19988;&#23545;&#20110;&#33853;&#22312;&#25351;&#25968;&#26063;&#20013;&#30340;&#24191;&#27867;&#20998;&#24067;&#65292;&#23427;&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#37197;&#26041;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;Beta&#65292;von Mises-Fisher&#65292;Dirichlet&#65292;Wishart&#31561;&#20998;&#24067;&#30340;&#25193;&#25955;&#26679;&#24335;&#27169;&#22411;&#65292;&#24403;&#25968;&#25454;&#20301;&#20110;&#32422;&#26463;&#27969;&#24418;&#19978;&#26102;&#29305;&#21035;&#26377;&#29992;&#65292;&#20363;&#22914;&#21333;&#20301;&#29699;&#65292;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#31354;&#38388;&#65292;&#27010;&#29575;&#21333;&#32431;&#24418;&#31561;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#24456;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods based on Denoising Diffusion Probabilistic Models (DDPM) became a ubiquitous tool in generative modeling. However, they are mostly limited to Gaussian and discrete diffusion processes. We propose Star-Shaped Denoising Diffusion Probabilistic Models (SS-DDPM), a model with a non-Markovian diffusion-like noising process. In the case of Gaussian distributions, this model is equivalent to Markovian DDPMs. However, it can be defined and applied with arbitrary noising distributions, and admits efficient training and sampling algorithms for a wide range of distributions that lie in the exponential family. We provide a simple recipe for designing diffusion-like models with distributions like Beta, von Mises--Fisher, Dirichlet, Wishart and others, which can be especially useful when data lies on a constrained manifold such as the unit sphere, the space of positive semi-definite matrices, the probabilistic simplex, etc. We evaluate the model in different settings and find it competitive 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#24322;&#24120;&#31283;&#20581;Gromov-Wasserstein&#26041;&#27861;&#65288;RGW&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20048;&#35266;&#25200;&#21160;&#30340;&#36793;&#38469;&#32422;&#26463;&#21644;&#20351;&#29992;Bregman&#36817;&#31471;&#20132;&#26367;&#32447;&#24615;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;GW&#36317;&#31163;&#23545;&#24322;&#24120;&#20540;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04610</link><description>&lt;p&gt;
&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#24322;&#24120;&#31283;&#20581;Gromov-Wasserstein&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Outlier-Robust Gromov-Wasserstein for Graph Data. (arXiv:2302.04610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#24322;&#24120;&#31283;&#20581;Gromov-Wasserstein&#26041;&#27861;&#65288;RGW&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20048;&#35266;&#25200;&#21160;&#30340;&#36793;&#38469;&#32422;&#26463;&#21644;&#20351;&#29992;Bregman&#36817;&#31471;&#20132;&#26367;&#32447;&#24615;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;GW&#36317;&#31163;&#23545;&#24322;&#24120;&#20540;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gromov-Wasserstein&#65288;GW&#65289;&#36317;&#31163;&#26159;&#19968;&#31181;&#22312;&#19981;&#21516;&#24230;&#37327;&#31354;&#38388;&#19978;&#27604;&#36739;&#21644;&#23545;&#40784;&#27010;&#29575;&#20998;&#24067;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;GW&#24050;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23545;&#40784;&#24322;&#26500;&#25968;&#25454;&#30340;&#20027;&#35201;&#24314;&#27169;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;GW&#36317;&#31163;&#23545;&#24322;&#24120;&#20540;&#38750;&#24120;&#25935;&#24863;&#65292;&#22914;&#26524;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#23558;&#24322;&#24120;&#20540;&#19982;&#20854;&#20182;&#26679;&#26412;&#36171;&#20104;&#30456;&#21516;&#30340;&#26435;&#37325;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#36739;&#22823;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#31283;&#20581;&#30340;GW&#36317;&#31163;&#31216;&#20026;RGW&#12290;RGW&#22312;&#22522;&#20110;Kullback-Leibler&#25955;&#24230;&#30340;&#27169;&#31946;&#38598;&#21512;&#20013;&#24341;&#20837;&#20102;&#20048;&#35266;&#25200;&#21160;&#30340;&#36793;&#38469;&#32422;&#26463;&#12290;&#20026;&#20102;&#26356;&#26041;&#20415;&#22320;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;RGW&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#21033;&#29992;Bregman&#36817;&#31471;&#20132;&#26367;&#32447;&#24615;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#19988;&#29702;&#35770;&#21487;&#35777;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;RGW&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gromov-Wasserstein (GW) distance is a powerful tool for comparing and aligning probability distributions supported on different metric spaces. Recently, GW has become the main modeling technique for aligning heterogeneous data for a wide range of graph learning tasks. However, the GW distance is known to be highly sensitive to outliers, which can result in large inaccuracies if the outliers are given the same weight as other samples in the objective function. To mitigate this issue, we introduce a new and robust version of the GW distance called RGW. RGW features optimistically perturbed marginal constraints within a Kullback-Leibler divergence-based ambiguity set. To make the benefits of RGW more accessible in practice, we develop a computationally efficient and theoretically provable procedure using Bregman proximal alternating linearized minimization algorithm. Through extensive experimentation, we validate our theoretical results and demonstrate the effectiveness of RGW on real-wor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#20272;&#35745;&#36830;&#32493;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#25919;&#31574;&#30456;&#20851;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#23545;PM2.5&#30340;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#36827;&#34892;&#20462;&#35746;&#21518;&#30340;&#20581;&#24247;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.02560</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22240;&#26524;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;: &#22312;&#32654;&#22269;&#35780;&#20272;&#26356;&#20005;&#26684;&#30340;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#30340;&#20581;&#24247;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US. (arXiv:2302.02560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#20272;&#35745;&#36830;&#32493;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#25919;&#31574;&#30456;&#20851;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#23545;PM2.5&#30340;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#36827;&#34892;&#20462;&#35746;&#21518;&#30340;&#20581;&#24247;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#31574;&#30740;&#31350;&#20013;&#65292;&#20272;&#35745;&#36830;&#32493;&#24615;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#26159;&#26368;&#20851;&#38190;&#30340;&#20998;&#26512;&#20219;&#21153;&#20043;&#19968;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20559;&#31227;-&#21709;&#24212;&#20989;&#25968;&#65288;SRF&#65289;&#20272;&#35745;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#28041;&#21450;&#24378;&#20581;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#23454;&#29616;&#65292;&#29992;&#20110;SRF&#20272;&#35745;&#12290;&#21463;&#20844;&#20849;&#21355;&#29983;&#20013;&#30340;&#20851;&#38190;&#25919;&#31574;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21450;&#20854;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#24378;&#20581;&#24615;&#21644;&#25928;&#29575;&#20445;&#35777;&#30340;SRF&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#20197;&#20272;&#35745;&#23558;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#26368;&#36817;&#25552;&#35758;&#20174;12 &#956;g/m&#179;&#25913;&#20026;9 &#956;g/m&#179;&#30340;PM2.5&#30340;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#30340;&#20462;&#35746;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39318;&#27425;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
In policy research, one of the most critical analytic tasks is to estimate the causal effect of a policy-relevant shift to the distribution of a continuous exposure/treatment on an outcome of interest. We call this problem shift-response function (SRF) estimation. Existing neural network methods involving robust causal-effect estimators lack theoretical guarantees and practical implementations for SRF estimation. Motivated by a key policy-relevant question in public health, we develop a neural network method and its theoretical underpinnings to estimate SRFs with robustness and efficiency guarantees. We then apply our method to data consisting of 68 million individuals and 27 million deaths across the U.S. to estimate the causal effect from revising the US National Ambient Air Quality Standards (NAAQS) for PM 2.5 from 12 $\mu g/m^3$ to 9 $\mu g/m^3$. This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate, for the first time, the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#32039;&#23494;&#26497;&#23567;&#39118;&#38505;&#30028;&#65292;&#24182;&#25512;&#24191;&#20102;&#20808;&#21069;&#20851;&#20110;&#23545;&#28418;&#31227;&#30340;&#26080;&#30693;&#23398;&#20064;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.02460</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Density Estimation under Distribution Drift. (arXiv:2302.02460v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02460
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#32039;&#23494;&#26497;&#23567;&#39118;&#38505;&#30028;&#65292;&#24182;&#25512;&#24191;&#20102;&#20808;&#21069;&#20851;&#20110;&#23545;&#28418;&#31227;&#30340;&#26080;&#30693;&#23398;&#20064;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#39547;&#28857;&#28418;&#31227;&#35774;&#32622;&#19979;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12290;&#32473;&#23450;&#26469;&#33258;&#19968;&#31995;&#21015;&#38543;&#26102;&#38388;&#36880;&#28176;&#21464;&#21270;&#30340;&#20998;&#24067;&#30340;&#29420;&#31435;&#26679;&#26412;&#24207;&#21015;&#65292;&#30446;&#26631;&#26159;&#35745;&#31639;&#24403;&#21069;&#20998;&#24067;&#30340;&#26368;&#20339;&#20272;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#24179;&#28369;&#23494;&#24230;&#30340;&#32039;&#23494;&#26497;&#23567;&#39118;&#38505;&#30028;&#65292;&#20854;&#20013;&#26497;&#23567;&#20540;&#26159;&#23545;&#25152;&#26377;&#21487;&#33021;&#20272;&#35745;&#30340;&#26368;&#23567;&#20540;&#65292;&#32780;&#26497;&#22823;&#20540;&#26159;&#23545;&#28385;&#36275;&#28418;&#31227;&#32422;&#26463;&#30340;&#25152;&#26377;&#21487;&#33021;&#20998;&#24067;&#30340;&#26368;&#22823;&#20540;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#28418;&#31227;&#27169;&#22411;&#65292;&#24182;&#25512;&#24191;&#20102;&#20808;&#21069;&#20851;&#20110;&#23545;&#28418;&#31227;&#30340;&#26080;&#30693;&#23398;&#20064;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study nonparametric density estimation in non-stationary drift settings. Given a sequence of independent samples taken from a distribution that gradually changes in time, the goal is to compute the best estimate for the current distribution. We prove tight minimax risk bounds for both discrete and continuous smooth densities, where the minimum is over all possible estimates and the maximum is over all possible distributions that satisfy the drift constraints. Our technique handles a broad class of drift models, and generalizes previous results on agnostic learning under drift.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26377;&#25928;&#40065;&#26834;&#24615;&#30340;&#26032;&#25351;&#26631;&#65292;&#36890;&#36807;&#25511;&#21046;&#25152;&#26377;&#27169;&#22411;&#30340;&#35757;&#32451;&#20998;&#24067;&#30340;&#22810;&#20010;ID&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#20272;&#35745;&#30340;&#26377;&#25928;&#40065;&#26834;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#37322;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;ImageNet&#30340;CLIP&#26679;&#24335;&#38646;&#26679;&#26412;&#27169;&#22411;&#25152;&#23637;&#31034;&#20986;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.01381</link><description>&lt;p&gt;
&#19982;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#30340;&#27169;&#22411;&#30340;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#30340;&#26377;&#25928;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Effective Robustness against Natural Distribution Shifts for Models with Different Training Data. (arXiv:2302.01381v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26377;&#25928;&#40065;&#26834;&#24615;&#30340;&#26032;&#25351;&#26631;&#65292;&#36890;&#36807;&#25511;&#21046;&#25152;&#26377;&#27169;&#22411;&#30340;&#35757;&#32451;&#20998;&#24067;&#30340;&#22810;&#20010;ID&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#20272;&#35745;&#30340;&#26377;&#25928;&#40065;&#26834;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#37322;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;ImageNet&#30340;CLIP&#26679;&#24335;&#38646;&#26679;&#26412;&#27169;&#22411;&#25152;&#23637;&#31034;&#20986;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#26377;&#25928;&#40065;&#26834;&#24615;"&#34913;&#37327;&#20102;&#36229;&#20986;&#30001;&#20110;&#22312;&#20998;&#24067;&#65288;ID&#65289;&#24615;&#33021;&#39044;&#27979;&#30340;&#39069;&#22806;&#30340;&#31163;&#22495;&#65288;OOD&#65289;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#26377;&#25928;&#40065;&#26834;&#24615;&#35780;&#20272;&#36890;&#24120;&#20351;&#29992;&#21333;&#20010;&#27979;&#35797;&#38598;&#65288;&#22914;ImageNet&#65289;&#26469;&#35780;&#20272;ID&#20934;&#30830;&#24615;&#12290;&#24403;&#35780;&#20272;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#22312;LAION&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65289;&#26102;&#65292;&#36825;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25511;&#21046;&#20102;&#25152;&#26377;&#35780;&#20272;&#27169;&#22411;&#30340;&#35757;&#32451;&#20998;&#24067;&#25152;&#28085;&#30422;&#30340;&#22810;&#20010;ID&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#22312;&#23384;&#22312;&#20855;&#26377;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#30340;&#27169;&#22411;&#26102;&#33021;&#22815;&#26356;&#22909;&#22320;&#20272;&#35745;&#26377;&#25928;&#40065;&#26834;&#24615;&#12290;&#23427;&#36824;&#21487;&#20197;&#35299;&#37322;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;ImageNet&#30340;CLIP&#26679;&#24335;&#38646;&#26679;&#26412;&#27169;&#22411;&#23637;&#29616;&#20986;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#40065;&#26834;&#24615;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
"Effective robustness" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageN
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.00878</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22871;&#32034;&#65306;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#23454;&#29616;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The contextual lasso: Sparse linear models via deep neural networks. (arXiv:2302.00878v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26631;&#20934;&#24037;&#20855;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#19978;&#19979;&#25991;&#22871;&#32034;&#26159;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#23427;&#23558;&#36755;&#20837;&#29305;&#24449;&#20998;&#25104;&#21487;&#35299;&#37322;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#20004;&#32452;&#65292;&#24182;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#36827;&#34892;&#31232;&#30095;&#25311;&#21512;&#65292;&#21516;&#26102;&#20854;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#38656;&#21442;&#25968;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse linear models are a gold standard tool for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31934;&#32454;&#20998;&#31867;&#20013;&#20943;&#23567;&#38169;&#35823;&#20005;&#37325;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31895;&#31890;&#24230;&#20998;&#31867;&#22120;&#36827;&#34892;&#27979;&#35797;&#26399;&#20462;&#27491;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#23618;&#32423;&#26469;&#25552;&#39640;&#31934;&#32454;&#20998;&#31867;&#24615;&#33021;&#65292;&#22312;iNaturalist-19&#21644;tieredImageNet-H&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.00368</link><description>&lt;p&gt;
&#20351;&#29992;&#31895;&#31890;&#24230;&#20998;&#31867;&#22120;&#36827;&#34892;&#31934;&#32454;&#20998;&#31867;&#30340;&#27979;&#35797;&#26399;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification. (arXiv:2302.00368v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31934;&#32454;&#20998;&#31867;&#20013;&#20943;&#23567;&#38169;&#35823;&#20005;&#37325;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31895;&#31890;&#24230;&#20998;&#31867;&#22120;&#36827;&#34892;&#27979;&#35797;&#26399;&#20462;&#27491;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#23618;&#32423;&#26469;&#25552;&#39640;&#31934;&#32454;&#20998;&#31867;&#24615;&#33021;&#65292;&#22312;iNaturalist-19&#21644;tieredImageNet-H&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20943;&#23567;&#31934;&#32454;&#20998;&#31867;&#38169;&#35823;&#20005;&#37325;&#24615;&#30340;&#38382;&#39064;&#12290;&#31934;&#32454;&#20998;&#31867;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#20934;&#30830;&#26631;&#27880;&#12290;&#30456;&#27604;&#36739;&#32780;&#35328;&#65292;&#20154;&#31867;&#22312;&#36827;&#34892;&#31895;&#31890;&#24230;&#20998;&#31867;&#26102;&#29305;&#21035;&#25797;&#38271;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#30456;&#23545;&#36739;&#20302;&#30340;&#19987;&#19994;&#27700;&#24179;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Ensembles (HiE)&#30340;&#21518;&#26399;&#32416;&#27491;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#31614;&#23618;&#32423;&#26469;&#25913;&#21892;&#22312;&#27979;&#35797;&#26399;&#38388;&#20351;&#29992;&#31895;&#31890;&#24230;&#39044;&#27979;&#30340;&#31934;&#32454;&#20998;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#20165;&#35201;&#27714;&#21494;&#33410;&#28857;&#30340;&#29238;&#33410;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#24179;&#22343;&#38169;&#35823;&#20005;&#37325;&#24615;&#65292;&#21516;&#26102;&#22312;iNaturalist-19&#21644;tieredImageNet-H&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;&#65292;&#23454;&#29616;&#20102;&#36825;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#33879;&#38477;&#20302;&#38169;&#35823;&#20005;&#37325;&#24615;&#30340;&#21516;&#26102;&#65292;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;top-1&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of reducing mistake severity for fine-grained classification. Fine-grained classification can be challenging, mainly due to the requirement of domain expertise for accurate annotation. However, humans are particularly adept at performing coarse classification as it requires relatively low levels of expertise. To this end, we present a novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions. By only requiring the parents of leaf nodes, our method significantly reduces avg. mistake severity while improving top-1 accuracy on the iNaturalist-19 and tieredImageNet-H datasets, achieving a new state-of-the-art on both benchmarks. We also investigate the efficacy of our approach in the semi-supervised setting. Our approach brings notable gains in top-1 accuracy while significantly decreasing the severity of mista
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#20855;&#26377;&#31867;&#20284;&#30340;&#20960;&#20309;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#38543;&#30528;&#23618;&#32423;&#30340;&#31227;&#21160;&#65292;&#23427;&#20204;&#22312;&#26368;&#21021;&#30340;&#20960;&#23618;&#20013;&#21464;&#24471;&#39640;&#32500;&#65292;&#28982;&#21518;&#22312;&#20013;&#38388;&#23618;&#20013;&#26174;&#33879;&#25910;&#32553;&#65292;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#37096;&#20998;&#65292;&#20445;&#25345;&#24658;&#23450;&#25110;&#24418;&#25104;&#31532;&#20108;&#20010;&#27973;&#23792;&#12290;&#22312;&#31532;&#19968;&#20010;&#23792;&#20540;&#32467;&#26463;&#26102;&#65292;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20449;&#24687;&#34987;&#26356;&#22909;&#22320;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2302.00294</link><description>&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#30340;&#20960;&#20309;&#23398;
&lt;/p&gt;
&lt;p&gt;
The geometry of hidden representations of large transformer models. (arXiv:2302.00294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00294
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#20855;&#26377;&#31867;&#20284;&#30340;&#20960;&#20309;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#38543;&#30528;&#23618;&#32423;&#30340;&#31227;&#21160;&#65292;&#23427;&#20204;&#22312;&#26368;&#21021;&#30340;&#20960;&#23618;&#20013;&#21464;&#24471;&#39640;&#32500;&#65292;&#28982;&#21518;&#22312;&#20013;&#38388;&#23618;&#20013;&#26174;&#33879;&#25910;&#32553;&#65292;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#37096;&#20998;&#65292;&#20445;&#25345;&#24658;&#23450;&#25110;&#24418;&#25104;&#31532;&#20108;&#20010;&#27973;&#23792;&#12290;&#22312;&#31532;&#19968;&#20010;&#23792;&#20540;&#32467;&#26463;&#26102;&#65292;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20449;&#24687;&#34987;&#26356;&#22909;&#22320;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#26159;&#29992;&#20110;&#33258;&#30417;&#30563;&#25968;&#25454;&#20998;&#26512;&#30340;&#24378;&#22823;&#26550;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#21253;&#25324;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#22312;&#20869;&#30340;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#32467;&#26500;&#36890;&#36807;&#19968;&#20010;&#34920;&#31034;&#19982;&#19979;&#19968;&#20010;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#31995;&#21015;&#21464;&#25442;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#36825;&#20123;&#34920;&#31034;&#30340;&#20960;&#20309;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#23618;&#32423;&#31227;&#21160;&#26102;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20998;&#26512;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#21644;&#37051;&#23621;&#32452;&#25104;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#22312;&#34507;&#30333;&#36136;&#35821;&#35328;&#20219;&#21153;&#21644;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#19978;&#30340;Transformer&#27169;&#22411;&#20013;&#65292;&#34920;&#31034;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#28436;&#21270;&#12290;&#22312;&#26368;&#21021;&#30340;&#20960;&#23618;&#20013;&#65292;&#25968;&#25454;&#27969;&#24418;&#25193;&#23637;&#65292;&#21464;&#24471;&#39640;&#32500;&#65292;&#28982;&#21518;&#22312;&#20013;&#38388;&#23618;&#20013;&#26174;&#33879;&#25910;&#32553;&#12290;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#37096;&#20998;&#65292;ID&#20445;&#25345;&#22823;&#33268;&#24658;&#23450;&#25110;&#24418;&#25104;&#31532;&#20108;&#20010;&#27973;&#23792;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20449;&#24687;&#22312;&#31532;&#19968;&#20010;&#23792;&#20540;&#32467;&#26463;&#26102;&#26356;&#22909;&#22320;&#34920;&#36798;&#65292;&#36825;&#19968;&#29616;&#35937;&#21487;&#20197;&#34987;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large transformers are powerful architectures used for self-supervised data analysis across various data types, including protein sequences, images, and text. In these models, the semantic structure of the dataset emerges from a sequence of transformations between one representation and the next. We characterize the geometric and statistical properties of these representations and how they change as we move through the layers. By analyzing the intrinsic dimension (ID) and neighbor composition, we find that the representations evolve similarly in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic information of the dataset is better expressed at the end of the first peak, and this phenomenon can be ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#32422;&#26463;&#20989;&#25968;&#20026;&#20984;&#25110;&#24369;&#20984;&#65292;&#22312;&#21482;&#20351;&#29992;&#21333;&#29615;&#36335;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13314</link><description>&lt;p&gt;
&#21333;&#29615;&#36335;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#27714;&#35299;&#38750;&#20809;&#28369;&#24369;&#20984;&#20989;&#25968;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;Oracle&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization. (arXiv:2301.13314v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#32422;&#26463;&#20989;&#25968;&#20026;&#20984;&#25110;&#24369;&#20984;&#65292;&#22312;&#21482;&#20351;&#29992;&#21333;&#29615;&#36335;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24369;&#20984;&#19988;&#32422;&#26463;&#20026;&#20984;&#25110;&#24369;&#20984;&#30340;&#38750;&#20984;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#27714;&#35299;&#27492;&#31867;&#38382;&#39064;&#65292;&#23427;&#26159;&#19968;&#31181;&#30452;&#35266;&#26131;&#23454;&#29616;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#22312;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#24050;&#30693;&#20854;Oracle&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#39033;&#38024;&#23545;&#38750;&#20984;&#38382;&#39064;&#30340;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;Oracle&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#38024;&#23545;&#30340;&#38382;&#39064;&#26159;&#27714;&#24471;&#20960;&#20046;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#20998;&#21035;&#23545;&#32422;&#26463;&#20026;&#20984;&#21644;&#24369;&#20984;&#30340;&#24773;&#24418;&#36827;&#34892;&#35752;&#35770;&#12290;&#19982;&#29616;&#26377;&#30340;&#21452;&#29615;&#36335;&#26041;&#27861;&#30456;&#27604;&#65292;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#21487;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#38382;&#39064;&#65292;&#20165;&#20351;&#29992;&#21333;&#29615;&#36335;&#21363;&#21487;&#23454;&#29616;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#20869;&#37096;&#36845;&#20195;&#27425;&#25968;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a non-convex constrained optimization problem, where the objective function is weakly convex and the constraint function is either convex or weakly convex. To solve this problem, we consider the classical switching subgradient method, which is an intuitive and easily implementable first-order method whose oracle complexity was only known for convex problems. This paper provides the first analysis on the oracle complexity of the switching subgradient method for finding a nearly stationary point of non-convex problems. Our results are derived separately for convex and weakly convex constraints. Compared to existing approaches, especially the double-loop methods, the switching gradient method can be applied to non-smooth problems and achieves the same complexity using only a single loop, which saves the effort on tuning the number of inner iterations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#31574;&#30053;&#21644;&#26041;&#27861;&#35299;&#20915;&#20102;&#35777;&#26126;&#28145;&#24230;&#32593;&#32476;&#31283;&#20581;&#24615;&#30340;&#38590;&#28857;&#65292;&#24341;&#20837;&#20102;Linear ResNet&#26550;&#26500;&#21644;Efficient Margin MAximization&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#31283;&#20581;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12549</link><description>&lt;p&gt;
&#28145;&#24230;&#23610;&#24230;&#65306;&#22312;ImageNet&#19978;&#23454;&#29616;&#31283;&#20581;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Scaling in Depth: Unlocking Robustness Certification on ImageNet. (arXiv:2301.12549v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#31574;&#30053;&#21644;&#26041;&#27861;&#35299;&#20915;&#20102;&#35777;&#26126;&#28145;&#24230;&#32593;&#32476;&#31283;&#20581;&#24615;&#30340;&#38590;&#28857;&#65292;&#24341;&#20837;&#20102;Linear ResNet&#26550;&#26500;&#21644;Efficient Margin MAximization&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#31283;&#20581;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;Lipschitz&#26041;&#27861;&#22312;&#30830;&#23450;&#24615;&#20445;&#35777;&#19979;&#33021;&#22815;&#23454;&#29616;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#30340;&#25215;&#35834;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20165;&#38480;&#20110;&#23545;&#20302;&#32500;&#25968;&#25454;&#65292;&#20363;&#22914;CIFAR-10&#30340;&#21069;&#39304;&#21367;&#31215;&#32593;&#32476;&#65288;ConvNet&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#21487;&#35777;&#26126;&#30340;&#31283;&#20581;&#35757;&#32451;&#25193;&#23637;&#21040;&#26356;&#22823;&#12289;&#26356;&#28145;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;&#35777;&#26126;&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#35745;&#31639;ResNet&#21644;ViT&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#27531;&#24046;&#22359;&#30340;Lipschitz&#30028;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#24120;&#35268;ResNet&#30340;Lipschitz&#24120;&#25968;&#36793;&#30028;&#30340;&#24555;&#36895;&#26041;&#27861;&#24448;&#24448;&#19981;&#20934;&#30830;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#27531;&#24046;&#22359;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;\emph{Linear ResNet} (LiResNet)&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;\emph{Efficient Margin MAximization} (EMMA)&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21516;&#26102;&#24809;&#32602;&#26469;&#33258;\emph{&#25152;&#26377;}&#31867;&#21035;&#30340;&#26368;&#22351;&#24773;&#20917;&#23545;&#25239;&#24615;&#31034;&#20363;&#31283;&#23450;&#31283;&#20581;&#35757;&#32451;&#12290;&#36825;&#20123;&#36129;&#29486;&#20849;&#21516;&#20135;&#29983;&#20102;&#26032;&#30340;\emph{&#26368;&#20808;&#36827;}&#30340;&#31283;&#20581;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promise of Lipschitz-based methods for provably-robust deep learning with deterministic guarantees, current state-of-the-art results are limited to feed-forward Convolutional Networks (ConvNets) on low-dimensional data, such as CIFAR-10. This paper investigates strategies for expanding certifiably robust training to larger, deeper models. A key challenge in certifying deep networks is efficient calculation of the Lipschitz bound for residual blocks found in ResNet and ViT architectures. We show that fast ways of bounding the Lipschitz constant for conventional ResNets are loose, and show how to address this by designing a new residual block, leading to the \emph{Linear ResNet} (LiResNet) architecture. We then introduce \emph{Efficient Margin MAximization} (EMMA), a loss function that stabilizes robust training by simultaneously penalizing worst-case adversarial examples from \emph{all} classes. Together, these contributions yield new \emph{state-of-the-art} robust accuracy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26680;&#25216;&#24039;&#23558;&#32047;&#35745;&#37327;&#25193;&#23637;&#21040;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#26032;&#30340;&#36890;&#29992;&#32479;&#35745;&#37327;&#12290;&#36229;&#36234;&#19968;&#38454;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#21644;&#26368;&#23567;&#30340;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2301.12466</link><description>&lt;p&gt;
&#26680;&#21270;&#32047;&#35745;&#37327;&#65306;&#36229;&#36234;&#26680;&#22343;&#20540;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Kernelized Cumulants: Beyond Kernel Mean Embeddings. (arXiv:2301.12466v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26680;&#25216;&#24039;&#23558;&#32047;&#35745;&#37327;&#25193;&#23637;&#21040;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#26032;&#30340;&#36890;&#29992;&#32479;&#35745;&#37327;&#12290;&#36229;&#36234;&#19968;&#38454;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#21644;&#26368;&#23567;&#30340;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;$d$&#32500;&#23454;&#25968;&#31354;&#38388;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#32047;&#35745;&#37327;&#26159;&#19968;&#31181;&#26367;&#20195;&#30697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#26041;&#24046;&#20272;&#35745;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#21033;&#29992;&#24352;&#37327;&#20195;&#25968;&#30340;&#24037;&#20855;&#23558;&#32047;&#35745;&#37327;&#25193;&#23637;&#21040;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#65292;&#24182;&#36890;&#36807;&#26680;&#25216;&#24039;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#26159;&#21487;&#34892;&#30340;&#12290;&#36825;&#20123;&#26680;&#21270;&#32047;&#35745;&#37327;&#25552;&#20379;&#20102;&#19968;&#32452;&#26032;&#30340;&#36890;&#29992;&#32479;&#35745;&#37327;&#65307;&#32463;&#20856;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#24076;&#23572;&#20271;&#29305;-&#26045;&#23494;&#29305;&#29420;&#31435;&#24615;&#20934;&#21017;&#26159;&#25105;&#20204;&#19968;&#33324;&#26500;&#36896;&#20013;&#30340;&#19968;&#38454;&#23545;&#35937;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65288;&#20351;&#29992;&#21512;&#25104;&#12289;&#29615;&#22659;&#21644;&#27969;&#37327;&#25968;&#25454;&#20998;&#26512;&#65289;&#35770;&#35777;&#20102;&#36229;&#36234;&#19968;&#38454;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#20197;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26368;&#23567;&#30340;&#39069;&#22806;&#24320;&#38144;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In $\mathbb R^d$, it is well-known that cumulants provide an alternative to moments that can achieve the same goals with numerous benefits such as lower variance estimators. In this paper we extend cumulants to reproducing kernel Hilbert spaces (RKHS) using tools from tensor algebras and show that they are computationally tractable by a kernel trick. These kernelized cumulants provide a new set of all-purpose statistics; the classical maximum mean discrepancy and Hilbert-Schmidt independence criterion arise as the degree one objects in our general construction. We argue both theoretically and empirically (on synthetic, environmental, and traffic data analysis) that going beyond degree one has several advantages and can be achieved with the same computational complexity and minimal overhead in our experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.12321</link><description>&lt;p&gt;
&#31070;&#32463;&#20851;&#31995;&#22270;&#65306;&#35782;&#21035;&#26631;&#31614;&#22122;&#38899;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#21644;&#28165;&#29702;&#25968;&#25454;&#26159;&#26500;&#24314;&#20581;&#22766;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23384;&#22312;&#22797;&#26434;&#38382;&#39064;&#65292;&#22914;&#26631;&#31614;&#38169;&#35823;&#12289;&#27424;&#34920;&#31034;&#21644;&#24322;&#24120;&#20540;&#65292;&#22240;&#27492;&#22312;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#36825;&#19968;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20851;&#31995;&#22270;&#32467;&#26500;&#26469;&#26816;&#27979;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25552;&#20379;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#28857;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20316;&#20026;&#20132;&#20114;&#24335;&#35786;&#26029;&#25968;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26631;&#31614;&#38169;&#35823;&#21644;&#31163;&#32676;&#20540;/&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;U&#24418;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#19982;&#20154;&#31867;&#23545;&#40784;&#24182;&#38750;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2301.11990</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#25903;&#25345;&#40065;&#26834;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Alignment with human representations supports robust few-shot learning. (arXiv:2301.11990v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11990
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;U&#24418;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#19982;&#20154;&#31867;&#23545;&#40784;&#24182;&#38750;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#20851;&#24515;AI&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#19990;&#30028;&#34920;&#24449;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#20998;&#26512;&#65292;&#24314;&#35758;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#24230;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#20043;&#38388;&#24212;&#35813;&#23384;&#22312;&#19968;&#20010;U&#24418;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;491&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24615;&#33021;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20010;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#34920;&#26126;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#20110;&#23545;&#25239;&#25915;&#20987;&#21644;&#22495;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20154;&#31867;&#23545;&#40784;&#24448;&#24448;&#26159;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#12289;&#40065;&#26834;&#24615; &#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#30340;&#26465;&#20214;&#38598;&#26469;&#34920;&#24449;&#21644;&#23398;&#20064;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#21644;&#26465;&#20214;&#38598;&#36739;&#22823;&#26102;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;k-&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#22312;&#19981;&#33021;&#21033;&#29992;&#25152;&#26377;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#26102;&#20173;&#28982;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.09028</link><description>&lt;p&gt;
&#29992;&#23567;&#30340;&#26465;&#20214;&#38598;&#34920;&#24449;&#21644;&#23398;&#20064;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Characterization and Learning of Causal Graphs with Small Conditioning Sets. (arXiv:2301.09028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#30340;&#26465;&#20214;&#38598;&#26469;&#34920;&#24449;&#21644;&#23398;&#20064;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#21644;&#26465;&#20214;&#38598;&#36739;&#22823;&#26102;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;k-&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#22312;&#19981;&#33021;&#21033;&#29992;&#25152;&#26377;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#26102;&#20173;&#28982;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#36890;&#36807;&#31995;&#32479;&#22320;&#27979;&#35797;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#30340;&#19968;&#37096;&#20998;&#32467;&#26500;&#12290;&#36825;&#20123;&#31639;&#27861;&#65292;&#22914;PC&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#65292;&#20381;&#36182;&#20110;&#30001;Pearl&#25552;&#20986;&#30340;&#25152;&#35859;&#22240;&#26524;&#22270;&#31561;&#20215;&#31867;&#30340;&#22270;&#24418;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#24448;&#24448;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#24456;&#24555;&#22833;&#21435;&#32479;&#35745;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#24403;&#26465;&#20214;&#38598;&#24456;&#22823;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65292;&#22312;&#40065;&#26834;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#23558;&#26465;&#20214;&#38598;&#30340;&#22823;&#23567;&#19978;&#38480;&#35774;&#32622;&#20026;&#26576;&#20010;&#25972;&#25968; k&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22240;&#26524;&#22270;&#31561;&#20215;&#31867;&#30340;&#22270;&#24418;&#34920;&#24449;&#22312;&#25105;&#20204;&#19981;&#33021;&#21033;&#29992;&#25152;&#26377;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#26102;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102; k-&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#27010;&#24565;&#65306;&#22914;&#26524;&#20004;&#20010;&#22240;&#26524;&#22270;&#24471;&#21040;&#30456;&#21516;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#65292;&#23427;&#20204;&#26159; k-&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint-based causal discovery algorithms learn part of the causal graph structure by systematically testing conditional independences observed in the data. These algorithms, such as the PC algorithm and its variants, rely on graphical characterizations of the so-called equivalence class of causal graphs proposed by Pearl. However, constraint-based causal discovery algorithms struggle when data is limited since conditional independence tests quickly lose their statistical power, especially when the conditioning set is large. To address this, we propose using conditional independence tests where the size of the conditioning set is upper bounded by some integer $k$ for robust causal discovery. The existing graphical characterizations of the equivalence classes of causal graphs are not applicable when we cannot leverage all the conditional independence statements. We first define the notion of $k$-Markov equivalence: Two causal graphs are $k$-Markov equivalent if they entail the same c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#38382;&#39064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#65292;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20027;&#23548;&#30528;&#20854;&#24615;&#33021;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21452;&#23792;&#39118;&#38505;&#26354;&#32447;&#30456;&#21453;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#26368;&#20248;&#37327;&#23376;&#31070;&#32463;&#20998;&#31867;&#22120;&#19982;Helstrom&#36793;&#30028;&#21644;&#31561;&#35282;&#32039;&#26694;&#20043;&#38388;&#23384;&#22312;&#20869;&#22312;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#35282;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.01597</link><description>&lt;p&gt;
&#25581;&#31034;&#22522;&#20110;&#38382;&#39064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31867;&#20998;&#31867;&#19978;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Demystify Problem-Dependent Power of Quantum Neural Networks on Multi-Class Classification. (arXiv:2301.01597v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#38382;&#39064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#65292;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20027;&#23548;&#30528;&#20854;&#24615;&#33021;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21452;&#23792;&#39118;&#38505;&#26354;&#32447;&#30456;&#21453;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#26368;&#20248;&#37327;&#23376;&#31070;&#32463;&#20998;&#31867;&#22120;&#19982;Helstrom&#36793;&#30028;&#21644;&#31561;&#35282;&#32039;&#26694;&#20043;&#38388;&#23384;&#22312;&#20869;&#22312;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#35282;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#24050;&#25104;&#20026;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#19968;&#20123;&#20351;&#29992;&#29305;&#23450;&#32534;&#30721;&#26041;&#27861;&#30340;QNNs&#21487;&#20197;&#36890;&#36807;&#32463;&#20856;&#20195;&#29702;&#26377;&#25928;&#22320;&#27169;&#25311;&#65292;&#32780;&#20855;&#26377;&#37327;&#23376;&#35760;&#24518;&#30340;&#20854;&#20182;QNNs&#21487;&#33021;&#27604;&#32463;&#20856;&#20998;&#31867;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#37327;&#23376;&#31070;&#32463;&#20998;&#31867;&#22120;&#65288;QCs&#65289;&#22312;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#38382;&#39064;&#30456;&#20851;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#26399;&#26395;&#39118;&#38505;&#30340;&#20998;&#26512;&#65292;&#35813;&#25351;&#26631;&#32508;&#21512;&#32771;&#34385;&#20102;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#25439;&#22833;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#39318;&#20808;&#65292;&#35757;&#32451;&#25439;&#22833;&#20027;&#23548;&#30528;&#21151;&#25928;&#65292;&#32780;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#65307;&#31532;&#20108;&#65292;QCs&#32463;&#21382;U&#24418;&#39118;&#38505;&#26354;&#32447;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21452;&#23792;&#39118;&#38505;&#26354;&#32447;&#30456;&#21453;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#26368;&#20248;QCs&#19982;Helstrom&#36793;&#30028;&#21644;&#31561;&#35282;&#32039;&#26694;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#36890;&#36807;&#37327;&#23376;&#24453;&#27979;&#35797;&#26679;&#26412;&#19982;&#26368;&#20248;QCs&#20043;&#38388;&#30340;&#26368;&#23567;&#35282;&#24230;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#38382;&#39064;&#30340;&#37327;&#23376;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs) have become an important tool for understanding the physical world, but their advantages and limitations are not fully understood. Some QNNs with specific encoding methods can be efficiently simulated by classical surrogates, while others with quantum memory may perform better than classical classifiers. Here we systematically investigate the problem-dependent power of quantum neural classifiers (QCs) on multi-class classification tasks. Through the analysis of expected risk, a measure that weighs the training loss and the generalization error of a classifier jointly, we identify two key findings: first, the training loss dominates the power rather than the generalization ability; second, QCs undergo a U-shaped risk curve, in contrast to the double-descent risk curve of deep neural classifiers. We also reveal the intrinsic connection between optimal QCs and the Helstrom bound and the equiangular tight frame. Using these findings, we propose a method that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14424</link><description>&lt;p&gt;
&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#19968;&#31867;&#29992;&#20110;&#39640;&#25928;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#38469;&#20013;&#65292;&#27969;&#36890;&#24120;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#21487;&#36870;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#38142;; &#20026;&#20102;&#20415;&#20110;&#35757;&#32451;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#23545;&#27969;&#36712;&#36857;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#29305;&#27530;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;Jordan-Kinderleherer-Otto (JKO)&#26041;&#26696;&#21551;&#21457;&#30340;&#31070;&#32463;ODE&#27969;&#32593;&#32476;&#65292;&#23427;&#20801;&#35768;&#26377;&#25928;&#22320;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#26080;&#38656;&#37319;&#26679;SDE&#36712;&#36857;&#25110;&#20998;&#25968;&#21305;&#37197;&#25110;&#21464;&#20998;&#23398;&#20064;&#30340;&#20869;&#24490;&#29615;&#12290;&#30001;&#20110;JKO&#26041;&#26696;&#23637;&#24320;&#20102;&#26799;&#24230;&#27969;&#30340;&#21160;&#24577;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33258;&#28982;&#22320;&#36880;&#20010;&#22534;&#21472;&#27531;&#24046;&#32593;&#32476;&#22359;&#65292;&#38477;&#20302;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#36827;&#34892;&#31471;&#21040;&#31471;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flow is a class of deep generative models for efficient sampling and density estimation. In practice, the flow often appears as a chain of invertible neural network blocks; to facilitate training, existing works have regularized flow trajectories and designed special network architectures. The current paper develops a neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise training of the residual blocks without sampling SDE trajectories or inner loops of score matching or variational learning. As the JKO scheme unfolds the dynamic of gradient flow, the proposed model naturally stacks residual network blocks one by one, reducing the memory load and difficulty in performing end-to-end deep flow network training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the trajectory in probability space, which improves the model training efficiency and accuracy in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#23616;&#25910;&#25947;&#24182;&#28040;&#38500;&#26497;&#38480;&#29615;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;DSGDA &#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12978</link><description>&lt;p&gt;
&#21452;&#37325;&#24179;&#28369;GDA&#65306;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#20840;&#23616;&#25910;&#25947;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization. (arXiv:2212.12978v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#23616;&#25910;&#25947;&#24182;&#28040;&#38500;&#26497;&#38480;&#29615;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;DSGDA &#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#19981;&#33021;&#20445;&#35777;&#20840;&#23616;&#25910;&#25947;&#65292;&#29978;&#33267;&#20250;&#36973;&#21463;&#26497;&#38480;&#29615;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#31216;&#20026;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#23427;&#33021;&#22815;&#33258;&#28982;&#22320;&#24179;&#34913;&#21407;&#22987;&#19982;&#23545;&#20598;&#26356;&#26032;&#65292;&#24182;&#19988;&#23558;&#26497;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;-&#38750;&#20985;&#20363;&#23376;&#20013;&#30340;&#26497;&#38480;&#29615;&#28040;&#38500;&#65292;&#21253;&#25324; Forsaken&#65292;Bilinearly-coupled minimax&#65292;Sixth-order polynomial &#21644; PolarGame&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#22312;&#19968;&#20010;&#21333;&#20391;&#30340; $\theta\in(0,1)$ Kurdyka-\L{}ojasiewicz&#26465;&#20214;&#65288;&#25110;&#20984;&#21407;&#22987;/&#20985;&#23545;&#20598;&#20989;&#25968;&#65289;&#19979;&#65292;DSGDA &#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#28216;&#25103;&#24179;&#34913;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230; $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$&#65288;&#25110; $\mathcal{O}(\epsilon^{-4})$&#65289;&#65292;&#36825;&#20123;&#19982;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonconvex-nonconcave minimax optimization has received intense attention over the last decade due to its broad applications in machine learning. Unfortunately, most existing algorithms cannot be guaranteed to converge globally and even suffer from limit cycles. To address this issue, we propose a novel single-loop algorithm called doubly smoothed gradient descent ascent method (DSGDA), which naturally balances the primal and dual updates. The proposed DSGDA can get rid of limit cycles in various challenging nonconvex-nonconcave examples in the literature, including Forsaken, Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further show that under an one-sided Kurdyka-\L{}ojasiewicz condition with exponent $\theta\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a game-stationary point with an iteration complexity of $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ (resp. $\mathcal{O}(\epsilon^{-4})$). These match the best results for single-loop al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#26080;&#27861;&#37327;&#21270;&#36817;&#20284;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.12474</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#24212;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. (arXiv:2212.12474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#26080;&#27861;&#37327;&#21270;&#36817;&#20284;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#19968;&#31867;&#37325;&#35201;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26426;&#26800;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#29289;&#29702;&#36807;&#31243;&#65292;&#20363;&#22914;&#28909;&#20256;&#23548;&#12289;&#30005;&#30913;&#23398;&#21644;&#27874;&#20256;&#25773;&#31561;&#12290;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#31163;&#25955;&#21270;&#30340;&#19987;&#38376;&#25968;&#20540;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#20123;&#27714;&#35299;&#22120;&#36890;&#24120;&#20351;&#29992;&#26410;&#30693;&#27169;&#22411;&#21442;&#25968;&#30340;&#20272;&#35745;&#20540;&#20197;&#21450;&#22914;&#26524;&#21487;&#29992;&#30340;&#35805;&#65292;&#29289;&#29702;&#27979;&#37327;&#20540;&#29992;&#20110;&#21021;&#22987;&#21270;&#12290;&#36825;&#20123;&#27714;&#35299;&#22120;&#32463;&#24120;&#23884;&#20837;&#21040;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#26356;&#22823;&#30340;&#31185;&#23398;&#27169;&#22411;&#20013;&#65292;&#22240;&#27492;&#35823;&#24046;&#37327;&#21270;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#24573;&#30053;&#21442;&#25968;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;&#19968;&#33268;&#24615;&#30340;&#20272;&#35745;&#20540;&#65292;&#20197;&#29992;&#20110;&#35745;&#31639;&#20854;&#22266;&#26377;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27714;&#35299;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#37322;&#20026;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#23450;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25512;&#24191;&#65292;&#35813;&#23450;&#29702;&#36866;&#29992;&#20110;&#36890;&#36807;&#20219;&#24847;&#30028;&#38754;&#36827;&#34892;&#35266;&#23519;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#29992;&#20110;&#32422;&#26463;&#22240;&#26524;&#39640;&#26031;&#36807;&#31243;&#20013;&#32463;&#39564;&#21327;&#26041;&#24046;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20998;&#35299;&#65292;&#29992;&#20110;&#24314;&#31435;&#39640;&#26031;&#20108;&#27425;&#24418;&#24335;&#30340;&#21333;&#20391;&#23614;&#37096;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#21521;&#37327;&#33258;&#22238;&#24402;&#30340;&#26368;&#23567;&#20108;&#20056;&#35782;&#21035;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.09508</link><description>&lt;p&gt;
&#20851;&#20110;&#22240;&#26524;&#39640;&#26031;&#36807;&#31243;&#32463;&#39564;&#21327;&#26041;&#24046;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A note on the smallest eigenvalue of the empirical covariance of causal Gaussian processes. (arXiv:2212.09508v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#29992;&#20110;&#32422;&#26463;&#22240;&#26524;&#39640;&#26031;&#36807;&#31243;&#20013;&#32463;&#39564;&#21327;&#26041;&#24046;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20998;&#35299;&#65292;&#29992;&#20110;&#24314;&#31435;&#39640;&#26031;&#20108;&#27425;&#24418;&#24335;&#30340;&#21333;&#20391;&#23614;&#37096;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#21521;&#37327;&#33258;&#22238;&#24402;&#30340;&#26368;&#23567;&#20108;&#20056;&#35782;&#21035;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#29992;&#20110;&#32422;&#26463;&#22240;&#26524;&#39640;&#26031;&#36807;&#31243;&#20013;&#32463;&#39564;&#21327;&#26041;&#24046;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#12290;&#22312;&#35777;&#26126;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22240;&#26524;&#20998;&#35299;&#26469;&#24314;&#31435;&#39640;&#26031;&#20108;&#27425;&#24418;&#24335;&#30340;&#21333;&#20391;&#23614;&#37096;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21482;&#20351;&#29992;&#20102;&#20851;&#20110;&#39640;&#26031;&#20998;&#24067;&#21644;&#24182;&#38598;&#30028;&#30340;&#22522;&#26412;&#20107;&#23454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#20363;&#23376;&#65292;&#20026;&#21521;&#37327;&#33258;&#22238;&#24402;&#30340;&#26368;&#23567;&#20108;&#20056;&#35782;&#21035;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple proof for bounding the smallest eigenvalue of the empirical covariance in a causal Gaussian process. Along the way, we establish a one-sided tail inequality for Gaussian quadratic forms using a causal decomposition. Our proof only uses elementary facts about the Gaussian distribution and the union bound. We conclude with an example in which we provide a performance guarantee for least squares identification of a vector autoregression.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#30740;&#31350;&#20102;&#22870;&#21169;&#25512;&#26029;&#23545;&#38169;&#35823;&#20154;&#31867;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#21487;&#33021;&#26500;&#24314;&#23567;&#30340;&#23545;&#25239;&#24615;&#20559;&#24046;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.04717</link><description>&lt;p&gt;
&#20851;&#20110;&#22870;&#21169;&#25512;&#26029;&#23545;&#38169;&#35823;&#20154;&#31867;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Sensitivity of Reward Inference to Misspecified Human Models. (arXiv:2212.04717v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#30740;&#31350;&#20102;&#22870;&#21169;&#25512;&#26029;&#23545;&#38169;&#35823;&#20154;&#31867;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#21487;&#33021;&#26500;&#24314;&#23567;&#30340;&#23545;&#25239;&#24615;&#20559;&#24046;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#26159;&#19982;&#20215;&#20540;&#23545;&#40784;&#23494;&#20999;&#30456;&#20851;&#30340;&#20869;&#23481; - &#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#19982;&#25105;&#20204;&#20154;&#31867;&#23454;&#38469;&#24819;&#35201;&#30340;&#19968;&#33268;&#12290;&#20294;&#36825;&#38656;&#35201;&#24314;&#31435;&#20154;&#31867;&#30340;&#34892;&#20026;&#27169;&#22411;&#12290;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#35748;&#30693;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30740;&#31350;&#65292;&#33719;&#24471;&#20934;&#30830;&#30340;&#20154;&#31867;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#23545;&#20110;&#22870;&#21169;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#26377;&#22810;&#37325;&#35201;&#65311;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#27169;&#22411;&#20013;&#23384;&#22312;&#23567;&#38169;&#35823;&#23601;&#20250;&#23548;&#33268;&#25512;&#26029;&#30340;&#28798;&#38590;&#24615;&#38169;&#35823;&#65292;&#37027;&#20040;&#22870;&#21169;&#23398;&#20064;&#30340;&#25972;&#20010;&#26694;&#26550;&#20284;&#20046;&#27880;&#23450;&#22833;&#36133;&#65292;&#22240;&#20026;&#25105;&#20204;&#27704;&#36828;&#26080;&#27861;&#25317;&#26377;&#23436;&#32654;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#38543;&#30528;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#20445;&#35777;&#22870;&#21169;&#30340;&#20934;&#30830;&#24615;&#20063;&#20250;&#25552;&#39640;&#65292;&#36825;&#23558;&#35777;&#26126;&#22312;&#27169;&#22411;&#26041;&#38754;&#20570;&#26356;&#22810;&#24037;&#20316;&#30340;&#30410;&#22788;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23454;&#23637;&#31034;&#20102;&#26500;&#24314;&#23567;&#30340;&#23545;&#25239;&#24615;&#20559;&#24046;&#26159;&#21487;&#33021;&#30340;
&lt;/p&gt;
&lt;p&gt;
Inferring reward functions from human behavior is at the center of value alignment - aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#20540;&#20989;&#25968;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22788;&#29702;&#20102;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#26681;&#25454;&#20445;&#23432;&#31243;&#24230;&#21160;&#24577;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.04607</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32622;&#20449;&#24230;&#26465;&#20214;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Confidence-Conditioned Value Functions for Offline Reinforcement Learning. (arXiv:2212.04607v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#20540;&#20989;&#25968;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22788;&#29702;&#20102;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#26681;&#25454;&#20445;&#23432;&#31243;&#24230;&#21160;&#24577;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25215;&#35834;&#33021;&#22815;&#20165;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26114;&#36149;&#30340;&#22312;&#32447;&#20132;&#20114;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#31163;&#32447;RL&#26041;&#27861;&#24517;&#39035;&#22788;&#29702;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#20445;&#23432;&#25110;&#19979;&#38480;&#20540;&#20989;&#25968;&#65292;&#23427;&#20204;&#20302;&#20272;&#20102;&#36229;&#20986;&#20998;&#24067;&#30340;&#34892;&#20026;&#30340;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#65306;&#22312;&#36825;&#20123;&#20540;&#20989;&#25968;&#19978;&#20248;&#21270;&#30340;&#31574;&#30053;&#21482;&#33021;&#26681;&#25454;&#22266;&#23450;&#30340;&#12289;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#20445;&#23432;&#31243;&#24230;&#26469;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#22312;&#35757;&#32451;&#26102;&#23398;&#20064;&#19981;&#21516;&#20445;&#23432;&#31243;&#24230;&#30340;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#19968;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;&#26102;&#21160;&#24577;&#36873;&#25321;&#20854;&#20013;&#20043;&#19968;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21487;&#20197;&#24471;&#21040;&#32531;&#35299;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#38468;&#21152;&#26465;&#20214;&#30340;&#20540;&#20989;&#25968;&#65292;&#36825;&#20123;&#20540;&#20989;&#25968;&#20381;&#36182;&#20110;&#20445;&#23432;&#31243;&#24230;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32622;&#20449;&#24230;&#26465;&#20214;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of out-of-distribution (OOD) actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Be
&lt;/p&gt;</description></item><item><title>&#23558;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#19982;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20960;&#20309;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#34507;&#30333;&#36136;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.03447</link><description>&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integration of Pre-trained Protein Language Models into Geometric Deep Learning Networks. (arXiv:2212.03447v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03447
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#19982;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20960;&#20309;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#34507;&#30333;&#36136;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#23398;&#20064;&#22823;&#22411;&#29983;&#29289;&#20998;&#23376;&#30340;&#19977;&#32500;&#32467;&#26500;&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32467;&#26500;&#25968;&#25454;&#25968;&#37327;&#26377;&#38480;&#65292;&#20854;&#26377;&#25928;&#24615;&#21463;&#21040;&#24456;&#22823;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38024;&#23545;&#20016;&#23500;&#30340;&#19968;&#32500;&#24207;&#21015;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#21147;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#26377;&#20960;&#20010;&#23581;&#35797;&#23558;&#36825;&#20123;&#19981;&#21516;&#30340;&#34507;&#30333;&#36136;&#27169;&#24577;&#32452;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#21319;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20294;&#26410;&#33021;&#32473;&#20986;&#23545;&#20854;&#20248;&#21183;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#23558;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#20960;&#20309;&#32593;&#32476;&#20013;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#34507;&#30333;&#36136;&#34920;&#24449;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#25324;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12289;&#27169;&#22411;&#36136;&#37327;&#35780;&#20272;&#12289;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#21018;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning has recently achieved great success in non-Euclidean domains, and learning on 3D structures of large biomolecules is emerging as a distinct research area. However, its efficacy is largely constrained due to the limited quantity of structural data. Meanwhile, protein language models trained on substantial 1D sequences have shown burgeoning capabilities with scale in a broad range of applications. Several previous studies consider combining these different protein modalities to promote the representation power of geometric neural networks, but fail to present a comprehensive understanding of their benefits. In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#35268;&#36991;&#26426;&#21046;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#21644;&#36991;&#20813;&#28798;&#38590;&#24615;&#32467;&#26524;&#30340;&#39118;&#38505;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.00124</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20998;&#24067;&#20559;&#31227;&#30340;&#39118;&#38505;&#35268;&#36991;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion. (arXiv:2212.00124v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#35268;&#36991;&#26426;&#21046;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#21644;&#36991;&#20813;&#28798;&#38590;&#24615;&#32467;&#26524;&#30340;&#39118;&#38505;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36866;&#29992;&#20110;&#22312;&#32447;&#25506;&#32034;&#19981;&#21487;&#34892;&#30340;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;&#22312;&#36825;&#31181;&#39046;&#22495;&#20013;&#65292;&#20915;&#31574;&#24212;&#32771;&#34385;&#21040;&#28798;&#38590;&#24615;&#32467;&#26524;&#30340;&#39118;&#38505;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20915;&#31574;&#24212;&#35813;&#26159;&#39118;&#38505;&#35268;&#36991;&#30340;&#12290;&#31163;&#32447;RL&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#65292;&#21363;&#30830;&#20445;&#31574;&#30053;&#35775;&#38382;&#30340;&#29366;&#24577;-&#25805;&#20316;&#23545;&#38752;&#36817;&#25968;&#25454;&#38598;&#20013;&#30340;&#29366;&#24577;-&#25805;&#20316;&#23545;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#31163;&#32447;RL&#25216;&#26415;&#65288;&#20197;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#65289;&#19982;&#39118;&#38505;&#25935;&#24863;&#22411;RL&#31639;&#27861;&#65288;&#20197;&#23454;&#29616;&#39118;&#38505;&#35268;&#36991;&#65289;&#30456;&#32467;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#39118;&#38505;&#35268;&#36991;&#26426;&#21046;&#20316;&#20026;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27169;&#22411;&#38598;&#21512;&#26469;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#39118;&#38505;&#35268;&#36991;&#30340;&#31574;&#30053;&#65292;&#36991;&#20813;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#20026;&#12290;&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#39118;&#38505;&#35268;&#36991;&#21487;&#20197;&#38450;&#27490;&#20998;&#24067;&#20559;&#31227;&#65292;&#22240;&#20026;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#20013;&#26410;&#28085;&#30422;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#22788;&#29702;&#36825;&#20123;&#25361;&#25112;&#30340;&#24037;&#20316;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be risk-averse. An additional challenge of offline RL is avoiding distributional shift, i.e. ensuring that state-action pairs visited by the policy remain near those in the dataset. Previous works on risk in offline RL combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address both of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36817;&#20284;&#35745;&#31639;&#32447;&#24615;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#20043;&#38388;&#30456;&#20132;&#21644;&#24046;&#24322;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#27604;&#20004;&#20010;&#27169;&#22411;&#30340;&#30456;&#20132;&#31354;&#38388;&#21644;&#38598;&#21512;&#24046;&#24322;&#65292;&#24182;&#29983;&#25104;&#26032;&#30340;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.16314</link><description>&lt;p&gt;
&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#36817;&#20284;&#35745;&#31639;&#32447;&#24615;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20132;&#21644;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Approximating Intersections and Differences Between Linear Statistical Shape Models Using Markov Chain Monte Carlo. (arXiv:2211.16314v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36817;&#20284;&#35745;&#31639;&#32447;&#24615;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#20043;&#38388;&#30456;&#20132;&#21644;&#24046;&#24322;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#27604;&#20004;&#20010;&#27169;&#22411;&#30340;&#30456;&#20132;&#31354;&#38388;&#21644;&#38598;&#21512;&#24046;&#24322;&#65292;&#24182;&#29983;&#25104;&#26032;&#30340;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23545;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411; (SSMs) &#30340;&#27604;&#36739;&#36890;&#24120;&#20165;&#22522;&#20110;&#24615;&#33021;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914;&#32039;&#33268;&#24615;&#12289;&#27867;&#21270;&#24615;&#25110;&#29305;&#24322;&#24615;&#65289;&#36827;&#34892;&#12290;&#23454;&#38469;&#24418;&#29366;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#25110;&#24046;&#24322;&#26080;&#27861;&#21487;&#35270;&#21270;&#25110;&#37327;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#25152;&#36328;&#36234;&#30340;&#65288;&#36229;&#26925;&#29699;&#24418;&#65289;&#21487;&#20801;&#35768;&#24418;&#29366;&#22495;&#20043;&#38388;&#30340;&#36817;&#20284;&#30456;&#20132;&#31354;&#38388;&#21644;&#38598;&#21512;&#24046;&#24322;&#65292;&#23545;&#27604;&#20004;&#20010;&#32447;&#24615;SSM&#22312;&#23494;&#38598;&#23545;&#24212;&#19978;&#36827;&#34892;&#23450;&#24615;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36817;&#20284;&#35745;&#31639;&#22788;&#20110;&#30456;&#20132;&#31354;&#38388;&#20013;&#30340;&#24418;&#29366;&#20998;&#24067;&#65292;&#24182;&#38543;&#21518;&#24212;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#23545;&#21518;&#39564;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#26368;&#32456;&#24471;&#21040;&#30456;&#20132;&#31354;&#38388;&#30340;&#26032;SSM&#12290;&#25105;&#20204;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#20272;&#35745;&#32447;&#24615;SSM&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25152;&#24471;&#31354;&#38388;&#19981;&#20877;&#26159;&#20984;&#30340;&#65292;&#25105;&#20204;&#19981;&#20877;&#24212;&#29992;PCA&#65292;&#32780;&#26159;&#20351;&#29992;&#21518;&#39564;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
To date, the comparison of Statistical Shape Models (SSMs) is often solely performance-based, carried out by means of simplistic metrics such as compactness, generalization, or specificity. Any similarities or differences between the actual shape spaces can neither be visualized nor quantified. In this paper, we present a new method to qualitatively compare two linear SSMs in dense correspondence by computing approximate intersection spaces and set-theoretic differences between the (hyper-ellipsoidal) allowable shape domains spanned by the models. To this end, we approximate the distribution of shapes lying in the intersection space using Markov chain Monte Carlo and subsequently apply Principal Component Analysis (PCA) to the posterior samples, eventually yielding a new SSM of the intersection space. We estimate differences between linear SSMs in a similar manner; here, however, the resulting spaces are no longer convex and we do not apply PCA but instead use the posterior samples for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#23427;&#21033;&#29992;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#26469;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#20855;&#26377;&#35782;&#21035;&#28508;&#22312;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#22823;&#33041;&#30340;&#32593;&#32476;&#24418;&#24335;&#65292;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#33041;&#32593;&#32476;&#20197;&#22270;&#24418;&#26041;&#24335;&#34920;&#31034;&#65292;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.12421</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#31070;&#32463;&#31185;&#23398;&#65306;&#20851;&#20110;&#25968;&#25454;&#25910;&#38598;&#19982;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Network Neuroscience: On Data Collection and Benchmark. (arXiv:2211.12421v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#23427;&#21033;&#29992;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#26469;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#20855;&#26377;&#35782;&#21035;&#28508;&#22312;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#22823;&#33041;&#30340;&#32593;&#32476;&#24418;&#24335;&#65292;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#33041;&#32593;&#32476;&#20197;&#22270;&#24418;&#26041;&#24335;&#34920;&#31034;&#65292;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#34987;&#29992;&#20110;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#19988;&#22312;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#27663;&#30151;&#12289;&#24085;&#37329;&#26862;&#30151;&#21644;&#33258;&#38381;&#30151;&#31561;&#28508;&#22312;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#20197;&#33041;&#32593;&#32476;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#22823;&#33041;&#30340;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#20316;&#20026;&#19968;&#20010;&#22270;&#24418;&#34920;&#31034;&#30340;&#33041;&#32593;&#32476;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#25506;&#32034;&#12290;&#20854;&#20013;&#20027;&#35201;&#30340;&#22256;&#38590;&#22312;&#20110;&#22797;&#26434;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and quality collection of functional human brain \emph{network} data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps 
&lt;/p&gt;</description></item><item><title>MelHuBERT&#26159;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;&#29256;HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#34920;&#29616;&#65292;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#21644;33.5%&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2211.09944</link><description>&lt;p&gt;
MelHuBERT: &#19968;&#31181;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;HuBERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MelHuBERT: A simplified HuBERT on Mel spectrograms. (arXiv:2211.09944v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09944
&lt;/p&gt;
&lt;p&gt;
MelHuBERT&#26159;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;&#29256;HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#34920;&#29616;&#65292;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#21644;33.5%&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#22810;&#20010;GPU&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20005;&#37325;&#38480;&#21046;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20943;&#23569;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;HuBERT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#25104;&#21151;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#25913;&#36827;&#24182;&#31616;&#21270;&#20102;&#20960;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MelHuBERT&#22312;&#38899;&#32032;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#22343;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#65292;&#25110;&#31561;&#25928;&#22320;&#27599;&#31186;&#35821;&#38899;&#33410;&#30465;&#20102;33.5%&#30340;MACs&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/nervjack2/MelHuBERT&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33391;&#22909;&#30340;&#23454;&#36341;&#26469;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#20013;&#30340;&#38480;&#21046;&#65292;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#30340;&#25915;&#20987;&#29305;&#28857;&#21644;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.09565</link><description>&lt;p&gt;
&#22312;&#35780;&#20272;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#33391;&#22909;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Towards Good Practices in Evaluating Transfer Adversarial Attacks. (arXiv:2211.09565v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33391;&#22909;&#30340;&#23454;&#36341;&#26469;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#20013;&#30340;&#38480;&#21046;&#65292;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#30340;&#25915;&#20987;&#29305;&#28857;&#21644;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#22312;&#30495;&#23454;&#19990;&#30028;&#12289;&#40657;&#30418;&#23376;&#22330;&#26223;&#20013;&#24341;&#21457;&#20102;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#35780;&#20272;&#20013;&#23384;&#22312;&#20004;&#20010;&#24120;&#35265;&#38480;&#21046;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#23454;&#38469;&#36827;&#23637;&#24456;&#38590;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#19981;&#21516;&#26041;&#27861;&#24448;&#24448;&#27809;&#26377;&#36827;&#34892;&#31995;&#32479;&#21644;&#20844;&#27491;&#30340;&#19968;&#23545;&#19968;&#27604;&#36739;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#21482;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#65292;&#32780;&#21478;&#19968;&#20010;&#20851;&#38190;&#30340;&#25915;&#20987;&#23646;&#24615;&#8212;&#8212;&#38544;&#34109;&#24615;&#21017;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33391;&#22909;&#30340;&#23454;&#36341;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#19988;&#25105;&#20204;&#23545;&#36716;&#31227;&#25915;&#20987;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#23545;ImageNet&#19978;9&#31181;&#38450;&#24481;&#25514;&#26045;&#30340;23&#31181;&#20195;&#34920;&#24615;&#25915;&#20987;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#29616;&#26377;&#25915;&#20987;&#20998;&#20026;&#20116;&#22823;&#31867;&#65292;&#36825;&#26679;&#21487;&#20197;&#36827;&#34892;&#31995;&#32479;&#30340;&#31867;&#21035;&#20998;&#26512;&#12290;&#36825;&#20123;&#20998;&#26512;&#24471;&#20986;&#20102;&#26032;&#30340;&#21457;&#29616;&#65292;&#29978;&#33267;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#30830;&#23450;&#25105;&#20204;&#30340;&#25915;&#20987;&#20840;&#38754;&#35780;&#20272;&#20013;&#30340;&#26368;&#20339;&#25915;&#20987;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#29305;&#21035;&#27880;&#24847;&#20102;&#38544;&#34109;&#24615;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer adversarial attacks raise critical security concerns in real-world, black-box scenarios. However, the actual progress of this field is difficult to assess due to two common limitations in existing evaluations. First, different methods are often not systematically and fairly evaluated in a one-to-one comparison. Second, only transferability is evaluated but another key attack property, stealthiness, is largely overlooked. In this work, we design good practices to address these limitations, and we present the first comprehensive evaluation of transfer attacks, covering 23 representative attacks against 9 defenses on ImageNet. In particular, we propose to categorize existing attacks into five categories, which enables our systematic category-wise analyses. These analyses lead to new findings that even challenge existing knowledge and also help determine the optimal attack hyperparameters for our attack-wise comprehensive evaluation. We also pay particular attention to stealthines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer&#12290;&#36890;&#36807;&#25913;&#36827;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.13148</link><description>&lt;p&gt;
&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformers over Directed Acyclic Graphs. (arXiv:2210.13148v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer&#12290;&#36890;&#36807;&#25913;&#36827;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Transformer&#27169;&#22411;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#33021;&#21147;&#23398;&#20064;&#36229;&#20986;&#24120;&#35268;&#22270;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#21040;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#20027;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22270;&#30340;&#32467;&#26500;&#20559;&#24046;&#27880;&#20837;&#21040;Transformer&#30340;&#26550;&#26500;&#20013;&#65292;&#24182;&#38024;&#23545;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#25552;&#20986;&#20102;&#19968;&#20123;&#36866;&#24212;&#24615;&#30340;&#26550;&#26500;&#25913;&#36827;&#65306;&#65288;1&#65289;&#19968;&#20010;&#27604;&#24120;&#35268;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#26356;&#39640;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21516;&#26102;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;DAGs&#30340;&#32467;&#26500;&#65292;&#65288;2&#65289;&#19968;&#20010;&#23545;DAG&#30340;&#20559;&#24207;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#34917;&#20805;&#20102;&#21069;&#32773;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20174;&#23545;&#28304;&#20195;&#30721;&#22270;&#30340;&#20998;&#31867;&#21040;&#23545;&#24341;&#29992;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have recently gained popularity in graph representation learning as they have the potential to learn complex relationships beyond the ones captured by regular graph neural networks. The main research question is how to inject the structural bias of graphs into the transformer architecture, and several proposals have been made for undirected molecular graphs and, recently, also for larger network graphs. In this paper, we study transformers over directed acyclic graphs (DAGs) and propose architecture adaptations tailored to DAGs: (1) An attention mechanism that is considerably more efficient than the regular quadratic complexity of transformers and at the same time faithfully captures the DAG structure, and (2) a positional encoding of the DAG's partial order, complementing the former. We rigorously evaluate our approach over various types of tasks, ranging from classifying source code graphs to nodes in citation networks, and show that it is effective in two importan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2210.13011</link><description>&lt;p&gt;
&#35770;&#22810;&#21160;&#20316;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
On Many-Actions Policy Gradient. (arXiv:2210.13011v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#22810;&#21160;&#20316;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#23427;&#20915;&#23450;&#20102;&#24403;&#19982;&#27604;&#20363;&#25193;&#23637;&#36712;&#36857;&#30340;&#21333;&#21160;&#20316;&#20195;&#29702;&#30456;&#27604;&#65292;&#22810;&#21160;&#20316;SPG&#20135;&#29983;&#27604;&#36739;&#20302;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#65288;MBMA&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;SPG&#32972;&#26223;&#19979;&#21033;&#29992;&#21160;&#24577;&#27169;&#22411;&#36827;&#34892;&#22810;&#21160;&#20316;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#21160;&#20316;SPG&#23454;&#29616;&#25152;&#28041;&#21450;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#22411;&#27169;&#25311;&#30340;&#22238;&#21512;&#20013;&#25552;&#20379;&#19982;SPG&#30456;&#24403;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MBMA&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#32467;&#26500;&#19982;&#29702;&#35770;&#39044;&#27979;&#30340;&#30456;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#21160;&#20316;&#29615;&#22659;&#20013;&#65292;MBMA&#19982;&#26080;&#27169;&#22411;&#65292;&#22810;&#21160;&#20316;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#22522;&#32447;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the variance of stochastic policy gradients (SPGs) with many action samples per state. We derive a many-actions optimality condition, which determines when many-actions SPG yields lower variance as compared to a single-action agent with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA), an approach leveraging dynamics models for many-actions sampling in the context of SPG. MBMA addresses issues associated with existing implementations of many-actions SPG and yields lower bias and comparable variance to SPG estimated from states in model-simulated rollouts. We find that MBMA bias and variance structure matches that predicted by theory. As a result, MBMA achieves improved sample efficiency and higher returns on a range of continuous action environments as compared to model-free, many-actions, and model-based on-policy SPG baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30740;&#31350;&#24322;&#26500;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#26412;&#22320;&#27169;&#22411;&#26469;&#25191;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2210.12974</link><description>&lt;p&gt;
&#30740;&#31350;&#24322;&#26500;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Investigating Neuron Disturbing in Fusing Heterogeneous Neural Networks. (arXiv:2210.12974v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30740;&#31350;&#24322;&#26500;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#26412;&#22320;&#27169;&#22411;&#26469;&#25191;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22312;&#19981;&#21516;&#20301;&#32622;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#25104;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#30340;&#30452;&#25509;&#23454;&#29616;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#22312;&#34701;&#21512;&#20960;&#20046;&#30456;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#22312;&#23454;&#39564;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#34987;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;&#26412;&#25991;&#20174;&#36125;&#21494;&#26031;&#35266;&#28857;&#32467;&#21512;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#20803;&#24178;&#25200;&#30340;&#29616;&#35937;&#65292;&#21363;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#30340;&#31070;&#32463;&#20803;&#30456;&#20114;&#24178;&#25200;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#26412;&#22320;&#27169;&#22411;&#26469;&#25191;&#34892;&#39044;&#27979;&#30340;&#23454;&#39564;&#26041;&#27861;&#65292;&#31216;&#20026;AMS&#65292;&#20197;&#25490;&#38500;&#31070;&#32463;&#20803;&#24178;&#25200;&#24182;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMS&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#27604;&#19968;&#33324;&#30340;&#27169;&#22411;&#34701;&#21512;&#21644;&#38598;&#25104;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing deep learning models trained on separately located clients into a global model in a one-shot communication round is a straightforward implementation of Federated Learning. Although current model fusion methods are shown experimentally valid in fusing neural networks with almost identical architectures, they are rarely theoretically analyzed. In this paper, we reveal the phenomenon of neuron disturbing, where neurons from heterogeneous local models interfere with each other mutually. We give detailed explanations from a Bayesian viewpoint combining the data heterogeneity among clients and properties of neural networks. Furthermore, to validate our findings, we propose an experimental method that excludes neuron disturbing and fuses neural networks via adaptively selecting a local model, called AMS, to execute the prediction according to the input. The experiments demonstrate that AMS is more robust in data heterogeneity than general model fusion and ensemble methods. This implies
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12770</link><description>&lt;p&gt;
&#20851;&#20110;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#25968;&#25454;&#21463;&#38480;&#24494;&#35843;&#20013;&#23427;&#20204;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20351;&#29992;&#20174;&#19968;&#33324;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23558;&#20854;&#24494;&#35843;&#21040;&#29305;&#23450;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#65292;&#24182;&#20351;&#29992;&#26032;&#20219;&#21153;&#20013;&#21487;&#29992;&#30340;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#24494;&#35843;&#65292;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#23454;&#36341;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20855;&#20307;&#26159;&#22312;&#33647;&#29289;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;Transformer&#27169;&#22411;&#21644;&#36890;&#36807;&#24494;&#35843;BERT-based LLMs&#65288;&#21253;&#25324;BERT-base&#12289;BioBERT&#21644;ClinicalBERT&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#25193;&#23637;&#27169;&#22411;&#19982;&#24102;&#26377;CRF&#23618;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;CRF&#23618;&#23545;&#25152;&#26377;&#31070;&#32463;&#27169;&#22411;&#37117;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#24433;&#21709;&#65307;2&#65289;&#22312;&#20351;&#29992;&#23439;&#24179;&#22343;F1&#23545;BIO-strict&#36328;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#24494;&#35843;&#30340;LLMs&#33719;&#24471;&#20102;0.83+&#30340;&#24471;&#20998;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;TransformerCRF&#27169;&#22411;&#24471;&#20998;&#20026;0.78+&#65292;&#35777;&#26126;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BHMC&#30340;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#23450;&#20041;&#20102;&#32422;&#26463;&#30340;&#40654;&#26364;&#27969;&#24418;&#20013;&#36827;&#34892;&#26080;&#20559;&#37319;&#26679;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#30340;&#36807;&#28388;&#27493;&#39588;involution checking step&#12290;</title><link>http://arxiv.org/abs/2210.11925</link><description>&lt;p&gt;
&#33258;&#20849;&#36717;&#38556;&#30861;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#27931;&#30340;&#26080;&#20559;&#32422;&#26463;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo. (arXiv:2210.11925v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BHMC&#30340;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#23450;&#20041;&#20102;&#32422;&#26463;&#30340;&#40654;&#26364;&#27969;&#24418;&#20013;&#36827;&#34892;&#26080;&#20559;&#37319;&#26679;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#30340;&#36807;&#28388;&#27493;&#39588;involution checking step&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38556;&#30861;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#32599;(BHMC)&#65292;&#23427;&#26159;HMC&#31639;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#20174;&#24102;&#26377;&#33258;&#20849;&#36717;&#38556;&#30861;&#24230;&#37327;&#30340;&#27969;&#24418;&#20013;&#30340;Gibbs&#20998;&#24067;&#960;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#21253;&#21547;&#24230;&#37327;&#30340;Hamiltonian&#21160;&#21147;&#23398;&#12290;&#22240;&#27492;&#65292;&#23427;&#21253;&#21547;&#23450;&#20041;&#27969;&#24418;&#30340;&#32422;&#26463;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#20854;&#24213;&#23618;&#20960;&#20309;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#30456;&#24212;&#30340;Hamilton&#21160;&#21147;&#23398;&#26159;&#36890;&#36807;&#19981;&#21487;&#20998;&#31163;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#26469;&#23450;&#20041;&#30340;&#65292;&#19982;&#27431;&#20960;&#37324;&#24471;&#24773;&#20917;&#30456;&#21453;&#12290;&#36825;&#24847;&#21619;&#30528;&#23558;HMC&#25512;&#24191;&#21040;&#40654;&#26364;&#27969;&#24418;&#20013;&#20250;&#20135;&#29983;&#19981;&#21487;&#36991;&#20813;&#30340;&#20559;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#28388;&#27493;&#39588;&#65292;&#31216;&#20026;&#8220;involution&#26816;&#26597;&#27493;&#39588;&#8221;&#12290;&#35813;&#27493;&#39588;&#22312;&#20004;&#20010;BHMC&#29256;&#26412;&#8212;&#8212;&#36830;&#32493;BHMC(c-BHMC)&#21644;&#25968;&#20540;BHMC(n-BHMC)&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#26032;&#31639;&#27861;&#29983;&#25104;&#21487;&#36870;Markov&#38142;&#19988;&#26080;&#20559;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Barrier Hamiltonian Monte Carlo (BHMC), a version of the HMC algorithm which aims at sampling from a Gibbs distribution $\pi$ on a manifold $\mathrm{M}$, endowed with a Hessian metric $\mathfrak{g}$ derived from a self-concordant barrier. Our method relies on Hamiltonian dynamics which comprises $\mathfrak{g}$. Therefore, it incorporates the constraints defining $\mathrm{M}$ and is able to exploit its underlying geometry. However, the corresponding Hamiltonian dynamics is defined via non separable Ordinary Differential Equations (ODEs) in contrast to the Euclidean case. It implies unavoidable bias in existing generalization of HMC to Riemannian manifolds. In this paper, we propose a new filter step, called "involution checking step", to address this problem. This step is implemented in two versions of BHMC, coined continuous BHMC (c-BHMC) and numerical BHMC (n-BHMC) respectively. Our main results establish that these two new algorithms generate reversible Mark
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#22266;&#26377;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09943</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20559;&#35265;&#32531;&#35299;&#65306;&#26356;&#20844;&#24179;&#30340;&#26550;&#26500;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition. (arXiv:2210.09943v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09943
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#22266;&#26377;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#24191;&#27867;&#24212;&#29992;&#20110;&#25191;&#27861;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#20294;&#23427;&#20204;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#32500;&#24230;&#19978;&#23384;&#22312;&#20559;&#35265;&#12290;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#65292;&#27169;&#22411;&#20559;&#35265;&#28304;&#20110;&#26377;&#20559;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#20197;&#24448;&#20851;&#20110;&#20559;&#35265;&#32531;&#35299;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#24809;&#32602;&#39033;&#20197;&#38450;&#27490;&#20559;&#35265;&#24433;&#21709;&#27169;&#22411;&#65292;&#25110;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#21518;&#22788;&#29702;&#20197;&#28040;&#38500;&#20559;&#35265;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#23454;&#38469;&#19978;&#26681;&#28304;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#12290;&#22522;&#20110;&#36825;&#19968;&#37325;&#26032;&#23450;&#20041;&#65292;&#25105;&#20204;&#39318;&#27425;&#36827;&#34892;&#20102;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#25628;&#32034;&#36755;&#20986;&#20102;&#19968;&#31995;&#21015;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#39640;&#24615;&#33021;&#26550;&#26500;&#21644;&#29616;&#26377;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race. Conventional wisdom dictates that model biases arise from biased training data. As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition. In our work, we discover that biases are actually inherent to neural network architectures themselves. Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20915;&#31574;&#21382;&#21490;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#37327;&#21270;&#20381;&#36182;&#31243;&#24230;&#30340;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2210.09903</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#38480;&#21046;&#20869;&#23384;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Convex Optimization with Unbounded Memory. (arXiv:2210.09903v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20915;&#31574;&#21382;&#21490;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#37327;&#21270;&#20381;&#36182;&#31243;&#24230;&#30340;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26159;&#22312;&#32447;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#32773;&#30340;&#25439;&#22833;&#19981;&#20165;&#21462;&#20915;&#20110;&#24403;&#21069;&#30340;&#20915;&#31574;&#65292;&#36824;&#21462;&#20915;&#20110;&#30452;&#21040;&#37027;&#20010;&#26102;&#38388;&#28857;&#30340;&#25152;&#26377;&#20915;&#31574;&#21382;&#21490;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;OCO&#30340;&#25193;&#23637;&#26694;&#26550;&#65292;&#8220;&#20855;&#26377;&#26080;&#38480;&#21046;&#20869;&#23384;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#8221;&#65292;&#26469;&#25429;&#25417;&#23545;&#36807;&#21435;&#20915;&#31574;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#65292;$H_p$&#65292;&#23427;&#37327;&#21270;&#20102;$p$&#38454;&#24433;&#21709;&#30340;&#26368;&#22823;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework, ``Online Convex Optimization with Unbounded Memory'', that captures long-term dependence on past decisions. We introduce the notion of $p$-effective memory capacity, $H_p$, that quantifies the maximum influence of p
&lt;/p&gt;</description></item><item><title>ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.05845</link><description>&lt;p&gt;
ConSpec: &#31361;&#20986;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05845
&lt;/p&gt;
&lt;p&gt;
ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#25104;&#21151;&#24448;&#24448;&#21462;&#20915;&#20110;&#22810;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#22312;&#26102;&#38388;&#19978;&#30456;&#36317;&#36739;&#36828;&#65292;&#19982;&#26368;&#32456;&#22870;&#21169;&#20063;&#30456;&#36317;&#29978;&#36828;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20381;&#36182;Bellman&#26041;&#31243;&#65292;&#24456;&#38590;&#35782;&#21035;&#36825;&#20123;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20851;&#38190;&#27493;&#39588;&#12290;&#36825;&#20010;&#31639;&#27861;&#34987;&#31216;&#20026;&#23545;&#27604;&#20869;&#30465;&#65288;ConSpec&#65289;&#65292;&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;ConSpec&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#19982;&#36825;&#20123;&#21407;&#22411;&#20043;&#19968;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#12290;ConSpec&#20013;&#30340;&#21407;&#22411;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;&#23427;&#20204;&#20351;&#24471;&#33021;&#22815;&#36805;&#36895;&#35782;&#21035;&#25152;&#26377;&#20851;&#38190;&#27493;&#39588;&#65307;&#65288;2&#65289;&#23427;&#20204;&#20197;&#23481;&#26131;&#35299;&#37322;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20351;&#24471;&#22312;&#24863;&#35273;&#29305;&#24449;&#25913;&#21464;&#26102;&#21487;&#20197;&#36827;&#34892;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#12290;&#19982;&#20854;&#20182;&#24403;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#32500;&#24230;&#30340;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2210.04802</link><description>&lt;p&gt;
SimSCOOD: Fine-tuned&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#30340;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models. (arXiv:2210.04802v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#32500;&#24230;&#30340;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#25968;&#25454;&#38598;&#24050;&#32463;&#36234;&#26469;&#36234;&#23481;&#26131;&#22320;&#29992;&#20110;&#39044;&#35757;&#32451;&#28304;&#20195;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24494;&#35843;&#38454;&#27573;&#26469;&#35828;&#65292;&#33719;&#21462;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#20805;&#20998;&#35206;&#30422;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20195;&#30721;&#20998;&#24067;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#20219;&#21153;&#29305;&#23450;&#24615;&#21644;&#26377;&#38480;&#30340;&#26631;&#27880;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36951;&#24536;&#20197;&#21069;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#20102;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#20986;&#29616;&#24847;&#22806;&#24773;&#20917;&#65292;&#36825;&#23578;&#26410;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31995;&#32479;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#19981;&#21516;&#32500;&#24230;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#30340;&#21508;&#31181;&#36229;&#20998;&#24067;&#22330;&#26223;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22330;&#26223;&#20013;&#24494;&#35843;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#65288;&#21253;&#25324;&#20840;&#24494;&#35843;&#21644;&#20302;&#31209;&#36866;&#24212;&#24494;&#35843;&#26041;&#27861;&#65289;&#19979;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#21508;&#20010;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#24179;&#31283;&#20998;&#24067;&#26469;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20248;&#21270;&#24615;&#33021;&#21644;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.04317</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Spectral Approach to Item Response Theory. (arXiv:2210.04317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#24179;&#31283;&#20998;&#24067;&#26469;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20248;&#21270;&#24615;&#33021;&#21644;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Rasch&#27169;&#22411;&#26159;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#20013;&#26368;&#22522;&#30784;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#25945;&#32946;&#27979;&#35797;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39033;&#30446;&#20272;&#35745;&#31639;&#27861;&#65292;&#26680;&#24515;&#26159;&#35745;&#31639;&#22312;&#39033;&#30446;-&#39033;&#30446;&#22270;&#19978;&#23450;&#20041;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#24179;&#31283;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Rasch model is one of the most fundamental models in \emph{item response theory} and has wide-ranging applications from education testing to recommendation systems. In a universe with $n$ users and $m$ items, the Rasch model assumes that the binary response $X_{li} \in \{0,1\}$ of a user $l$ with parameter $\theta^*_l$ to an item $i$ with parameter $\beta^*_i$ (e.g., a user likes a movie, a student correctly solves a problem) is distributed as $\Pr(X_{li}=1) = 1/(1 + \exp{-(\theta^*_l - \beta^*_i)})$. In this paper, we propose a \emph{new item estimation} algorithm for this celebrated model (i.e., to estimate $\beta^*$). The core of our algorithm is the computation of the stationary distribution of a Markov chain defined on an item-item graph. We complement our algorithmic contributions with finite-sample error guarantees, the first of their kind in the literature, showing that our algorithm is consistent and enjoys favorable optimality properties. We discuss practical modification
&lt;/p&gt;</description></item><item><title>VoLTA&#26159;&#19968;&#31181;&#37319;&#29992;&#24369;&#30417;&#30563;&#23545;&#40784;&#31574;&#30053;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#29305;&#24449;&#19978;&#36827;&#34892;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#22270;&#20687;&#29702;&#35299;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#36793;&#30028;&#26694;&#26631;&#27880;&#12290;</title><link>http://arxiv.org/abs/2210.04135</link><description>&lt;p&gt;
VoLTA: &#24369;&#30417;&#30563;&#26412;&#22320;&#29305;&#24449;&#23545;&#40784;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer
&lt;/p&gt;
&lt;p&gt;
VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment. (arXiv:2210.04135v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04135
&lt;/p&gt;
&lt;p&gt;
VoLTA&#26159;&#19968;&#31181;&#37319;&#29992;&#24369;&#30417;&#30563;&#23545;&#40784;&#31574;&#30053;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#29305;&#24449;&#19978;&#36827;&#34892;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#22270;&#20687;&#29702;&#35299;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#36793;&#30028;&#26694;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#22312;&#21508;&#31181;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#19979;&#28216;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;VLP&#26041;&#27861;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;-&#25991;&#26412;&#26694;&#25968;&#25454;&#22312;&#31934;&#32454;&#21270;&#30340;&#21306;&#22495;&#32423;&#20219;&#21153;&#65288;&#22914;&#30446;&#26631;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#37197;&#20197;&#20934;&#30830;&#30340;&#36793;&#30028;&#26694;&#26631;&#27880;&#26114;&#36149;&#19988;&#38590;&#20197;&#22823;&#35268;&#27169;&#37319;&#38598;&#21644;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VoLTA&#65288;&#20351;&#29992;&#24369;&#30417;&#30563;&#26412;&#22320;&#29305;&#24449;&#23545;&#40784;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;VLP&#33539;&#24335;&#65292;&#21482;&#20351;&#29992;&#22270;&#20687;-&#26631;&#39064;&#25968;&#25454;&#65292;&#20294;&#21487;&#20197;&#23454;&#29616;&#32454;&#31890;&#24230;&#21306;&#22495;&#32423;&#22270;&#20687;&#29702;&#35299;&#65292;&#36991;&#20813;&#20351;&#29992;&#26114;&#36149;&#30340;&#26694;&#26631;&#27880;&#12290;VoLTA&#37319;&#29992;&#22522;&#20110;&#22270;&#20248;&#21270;&#20256;&#36755;&#30340;&#24369;&#30417;&#30563;&#26412;&#22320;&#22270;&#20687;&#22359;&#21644;&#25991;&#26412;&#26631;&#35760;&#23545;&#40784;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#26174;&#24335;&#12289;&#33258;&#26631;&#20934;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#20302;&#32423;&#21305;&#37197;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-mo
&lt;/p&gt;</description></item><item><title>Learnware&#33539;&#24335;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20805;&#20998;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#38590;&#24230;&#39640;&#12289;&#25968;&#25454;&#23433;&#20840;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.03647</link><description>&lt;p&gt;
Learnware: &#23567;&#27169;&#22411;&#23454;&#29616;&#22823;&#20316;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learnware: Small Models Do Big. (arXiv:2210.03647v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03647
&lt;/p&gt;
&lt;p&gt;
Learnware&#33539;&#24335;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20805;&#20998;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#38590;&#24230;&#39640;&#12289;&#25968;&#25454;&#23433;&#20840;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#25216;&#33021;&#39640;&#12289;&#36830;&#32493;&#23398;&#20064;&#38590;&#12289;&#36951;&#24536;&#39118;&#38505;&#22823;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#19987;&#26377;&#20449;&#24687;&#27844;&#38706;&#31561;&#38382;&#39064;&#65292;&#32780;&#36807;&#21435;&#30340;&#22823;&#27169;&#22411;&#33539;&#24335;&#34429;&#28982;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#65292;&#20294;&#24182;&#26410;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21453;&#32780;&#25104;&#20026;&#20005;&#37325;&#30340;&#30899;&#25490;&#25918;&#28304;&#12290;&#35813;&#25991;&#27010;&#36848;&#20102;Learnware&#33539;&#24335;&#65292;&#35753;&#29992;&#25143;&#19981;&#38656;&#35201;&#20174;&#22836;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24076;&#26395;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20854;&#20013;&#20851;&#38190;&#26159;&#35268;&#33539;&#65292;&#21487;&#20197;&#20351;&#35757;&#32451;&#30340;&#27169;&#22411;&#24471;&#21040;&#20805;&#20998;&#37492;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20307;&#36924;&#36817;&#30340;&#26041;&#27861;&#26469;&#20998;&#35299;&#38750;&#36127;&#24352;&#37327;&#65292;&#36890;&#36807;&#33021;&#37327;&#24314;&#27169;&#26469;&#36991;&#20813;&#20840;&#23616;&#20248;&#21270;&#21644;&#30446;&#26631;&#31209;&#36873;&#25321;&#30340;&#22256;&#38590;&#65292;&#21487;&#36890;&#36807;&#32771;&#34385;&#27169;&#24335;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;; &#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.15338</link><description>&lt;p&gt;
&#38750;&#36127;&#24352;&#37327;&#30340;&#22810;&#20307;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Many-body Approximation for Non-negative Tensors. (arXiv:2209.15338v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15338
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20307;&#36924;&#36817;&#30340;&#26041;&#27861;&#26469;&#20998;&#35299;&#38750;&#36127;&#24352;&#37327;&#65292;&#36890;&#36807;&#33021;&#37327;&#24314;&#27169;&#26469;&#36991;&#20813;&#20840;&#23616;&#20248;&#21270;&#21644;&#30446;&#26631;&#31209;&#36873;&#25321;&#30340;&#22256;&#38590;&#65292;&#21487;&#36890;&#36807;&#32771;&#34385;&#27169;&#24335;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;; &#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#20998;&#35299;&#38750;&#36127;&#24352;&#37327;&#65292;&#31216;&#20026;&#22810;&#20307;&#36924;&#36817;&#12290;&#20256;&#32479;&#30340;&#20998;&#35299;&#26041;&#27861;&#20551;&#35774;&#34920;&#31034;&#20855;&#26377;&#20302;&#31209;&#24615;&#65292;&#23548;&#33268;&#20840;&#23616;&#20248;&#21270;&#21644;&#30446;&#26631;&#31209;&#36873;&#25321;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#36890;&#36807;&#24352;&#37327;&#30340;&#33021;&#37327;&#24314;&#27169;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20854;&#20013;&#24352;&#37327;&#21644;&#20854;&#27169;&#24335;&#20998;&#21035;&#23545;&#24212;&#20110;&#27010;&#29575;&#20998;&#24067;&#21644;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#27169;&#24335;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;&#65292;&#21487;&#20197;&#27604;&#31209;&#26356;&#30452;&#35266;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#35270;&#21270;&#20026;&#24352;&#37327;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#22810;&#20307;&#36924;&#36817;&#21644;&#20302;&#31209;&#36924;&#36817;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#24352;&#37327;&#23436;&#25104;&#21644;&#36924;&#36817;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an alternative approach to decompose non-negative tensors, called many-body approximation. Traditional decomposition methods assume low-rankness in the representation, resulting in difficulties in global optimization and target rank selection. We avoid these problems by energy-based modeling of tensors, where a tensor and its mode correspond to a probability distribution and a random variable, respectively. Our model can be globally optimized in terms of the KL divergence minimization by taking the interaction between variables, i.e. modes, into account that can be tuned more intuitively than ranks. Furthermore, we visualize interactions between modes as tensor networks and reveal a nontrivial relationship between many-body approximation and low-rank approximation. We demonstrate the effectiveness of our approach in tensor completion and approximation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#27969;&#24418;&#30340;&#28508;&#31354;&#38388;&#26469;&#25913;&#36827;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;GM-VAE&#26041;&#27861;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.15217</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#30340;&#21452;&#26354; VAE
&lt;/p&gt;
&lt;p&gt;
Hyperbolic VAE via Latent Gaussian Distributions. (arXiv:2209.15217v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15217
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#27969;&#24418;&#30340;&#28508;&#31354;&#38388;&#26469;&#25913;&#36827;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;GM-VAE&#26041;&#27861;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#26031;&#27969;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(GM-VAE)&#65292;&#20854;&#28508;&#31354;&#38388;&#30001;&#19968;&#32452;&#39640;&#26031;&#20998;&#24067;&#32452;&#25104;&#12290;&#24050;&#30693;&#19968;&#32500;&#39640;&#26031;&#20998;&#24067;&#38598;&#21512;&#22312; Fisher &#20449;&#24687;&#24230;&#37327;&#19979;&#24418;&#25104;&#20102;&#19968;&#20010;&#21452;&#26354;&#31354;&#38388;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#26031;&#27969;&#24418;&#12290;&#20026;&#20102;&#23398;&#20064;&#20855;&#26377;&#39640;&#26031;&#27969;&#24418;&#30340; VAE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110; Kullback-Leibler &#25955;&#24230;&#30340;&#20266;&#39640;&#26031;&#27969;&#24418;&#27491;&#24577;&#20998;&#24067;&#65292;&#23427;&#26159;&#23545;&#24179;&#26041; Fisher-Rao &#36317;&#31163;&#30340;&#23616;&#37096;&#36817;&#20284;&#65292;&#29992;&#20110;&#23450;&#20041;&#28508;&#31354;&#38388;&#19978;&#30340;&#23494;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; GM-VAE &#22312;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65306;&#22270;&#20687;&#25968;&#25454;&#38598;&#23494;&#24230;&#20272;&#35745;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29615;&#22659;&#24314;&#27169;&#12290;GM-VAE &#22312;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;&#21452;&#26354;&#21644;&#27431;&#20960;&#37324;&#24471; VAE &#30340;&#21464;&#20307;&#65292;&#24182;&#22312;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent space consists of a set of Gaussian distributions. It is known that the set of the univariate Gaussian distributions with the Fisher information metric form a hyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed with the Gaussian manifolds, we propose a pseudo-Gaussian manifold normal distribution based on the Kullback-Leibler divergence, a local approximation of the squared Fisher-Rao distance, to define a density over the latent space. In experiments, we demonstrate the efficacy of GM-VAE on two different tasks: density estimation of image datasets and environment modeling in model-based reinforcement learning. GM-VAE outperforms the other variants of hyperbolicand Euclidean-VAEs on density estimation tasks and shows competitive performance in model-based reinforcement learning. We observe that our model provides strong numerical stability, addressing a common limitation reported 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22823;&#30340;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#32479;&#19968;Lipschitz&#21442;&#25968;&#30340;&#25509;&#36817;&#26368;&#20248;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.07403</link><description>&lt;p&gt;
&#20855;&#26377;&#22823;&#30340;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#30340;&#31169;&#26377;&#38543;&#26426;&#20248;&#21270;&#65306;&#65288;&#38750;&#20809;&#28369;&#65289;&#20984;&#25439;&#22833;&#30340;&#26368;&#20248;&#36895;&#29575;&#21450;&#20854;&#23545;&#38750;&#20984;&#25439;&#22833;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses. (arXiv:2209.07403v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22823;&#30340;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#32479;&#19968;Lipschitz&#21442;&#25968;&#30340;&#25509;&#36817;&#26368;&#20248;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#21487;&#33021;&#38750;&#24120;&#22823;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38543;&#26426;&#20248;&#21270;&#65288;SO&#65289;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;DP SO&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#25439;&#22833;&#22312;&#25152;&#26377;&#25968;&#25454;&#28857;&#19978;&#26159;&#22343;&#21248;Lipschitz&#36830;&#32493;&#30340;&#65288;&#21363;&#38543;&#26426;&#26799;&#24230;&#22312;&#25152;&#26377;&#25968;&#25454;&#28857;&#19978;&#37117;&#26377;&#30028;&#65289;&#12290;&#34429;&#28982;&#36825;&#31181;&#20551;&#35774;&#24456;&#26041;&#20415;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#24754;&#35266;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#20013;&#65292;&#30001;&#20110;&#24322;&#24120;&#20540;&#65292;&#25439;&#22833;&#22312;&#25152;&#26377;&#25968;&#25454;&#28857;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#65288;&#32479;&#19968;&#65289;Lipschitz&#21442;&#25968;&#21487;&#33021;&#38750;&#24120;&#22823;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;DP SO&#30340;&#35823;&#24046;&#30028;&#38480;&#19982;&#25439;&#22833;&#30340;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#25104;&#27604;&#20363;&#65292;&#23558;&#20250;&#26159;&#31354;&#27934;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#25509;&#36817;&#26368;&#20248;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#65292;&#19981;&#20381;&#36182;&#20110;&#25439;&#22833;&#30340;&#32479;&#19968;Lipschitz&#21442;&#25968;&#12290;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;Wang&#31561;&#20154;&#65292;2020; Kamath&#31561;&#20154;&#65292;2022&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20551;&#35774;&#38543;&#26426;&#26799;&#24230;&#20855;&#26377;&#26377;&#30028;&#30340;k&#38454;&#30697;
&lt;/p&gt;
&lt;p&gt;
We study differentially private (DP) stochastic optimization (SO) with loss functions whose worst-case Lipschitz parameter over all data points may be extremely large. To date, the vast majority of work on DP SO assumes that the loss is uniformly Lipschitz continuous over data (i.e. stochastic gradients are uniformly bounded over all data points). While this assumption is convenient, it often leads to pessimistic excess risk bounds. In many practical problems, the worst-case (uniform) Lipschitz parameter of the loss over all data points may be extremely large due to outliers. In such cases, the error bounds for DP SO, which scale with the worst-case Lipschitz parameter of the loss, are vacuous. To address these limitations, this work provides near-optimal excess risk bounds that do not depend on the uniform Lipschitz parameter of the loss. Building on a recent line of work (Wang et al., 2020; Kamath et al., 2022), we assume that stochastic gradients have bounded $k$-th order moments fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33041;&#30005;&#22270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#26816;&#27979;&#39550;&#39542;&#21592;&#21980;&#30561;&#29366;&#24577;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#26159;&#24615;&#33021;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26377;&#26356;&#39640;&#30340;f1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.04048</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#33041;&#30005;&#22270;&#30740;&#31350;&#39550;&#39542;&#20013;&#30340;&#21980;&#30561;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Studying Drowsiness Detection Performance while Driving through Scalable Machine Learning Models using Electroencephalography. (arXiv:2209.04048v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33041;&#30005;&#22270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#26816;&#27979;&#39550;&#39542;&#21592;&#21980;&#30561;&#29366;&#24577;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#26159;&#24615;&#33021;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26377;&#26356;&#39640;&#30340;f1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223; / &#24341;&#35328;&#65306;&#39550;&#39542;&#21592;&#21980;&#30561;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20063;&#26159;&#20132;&#36890;&#20107;&#25925;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26816;&#27979;&#39550;&#39542;&#21592;&#30340;&#21980;&#30561;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#20351;&#29992;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#30340;&#21980;&#30561;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#26377;&#24517;&#35201;&#30740;&#31350;&#36866;&#29992;&#20110;&#34987;&#35797;&#32676;&#20307;&#30340;&#21487;&#25193;&#23637;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;- &#26041;&#27861;&#65306;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#21644;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#29305;&#24449;&#65292;&#29992;&#20110;&#26816;&#27979;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#21980;&#30561;&#29366;&#24577;&#12290;&#20351;&#29992;SEED-VIG&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#22312;&#20010;&#20307;&#21644;&#32676;&#20307;&#19978;&#30340;&#34920;&#29616;&#12290;- &#32467;&#26524;&#65306;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#22312;&#20010;&#20307;&#39550;&#39542;&#21592;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22312;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20855;&#26377;78&#65285;&#30340;f1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
- Background / Introduction: Driver drowsiness is a significant concern and one of the leading causes of traffic accidents. Advances in cognitive neuroscience and computer science have enabled the detection of drivers' drowsiness using Brain-Computer Interfaces (BCIs) and Machine Learning (ML). However, the literature lacks a comprehensive evaluation of drowsiness detection performance using a heterogeneous set of ML algorithms, and it is necessary to study the performance of scalable ML models suitable for groups of subjects. - Methods: To address these limitations, this work presents an intelligent framework employing BCIs and features based on electroencephalography for detecting drowsiness in driving scenarios. The SEED-VIG dataset is used to evaluate the best-performing models for individual subjects and groups. - Results: Results show that Random Forest (RF) outperformed other models used in the literature, such as Support Vector Machine (SVM), with a 78% f1-score for individual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21709;&#24212;&#34987;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#21464;&#21305;&#37197;&#23646;&#24615;&#65288;IMP&#65289;&#20316;&#20026;&#19968;&#31181;&#25429;&#25417;&#24178;&#39044;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.10027</link><description>&lt;p&gt;
&#22312;&#21709;&#24212;&#19978;&#36827;&#34892;&#19968;&#33324;&#24178;&#39044;&#30340;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Invariant Representations under General Interventions on the Response. (arXiv:2208.10027v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21709;&#24212;&#34987;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#21464;&#21305;&#37197;&#23646;&#24615;&#65288;IMP&#65289;&#20316;&#20026;&#19968;&#31181;&#25429;&#25417;&#24178;&#39044;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#25910;&#38598;&#29305;&#24449;&#21644;&#21709;&#24212;&#23545;&#30340;&#35266;&#23519;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#24517;&#39035;&#23558;&#23398;&#20064;&#21040;&#30340;&#39044;&#27979;&#22120;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26159;&#37319;&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#26469;&#25551;&#36848;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#36981;&#24490;&#19981;&#21464;&#24615;&#21407;&#21017;&#65292;&#21363;&#32473;&#23450;&#39044;&#27979;&#22120;&#30340;&#26465;&#20214;&#20998;&#24067;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#21407;&#21017;&#21487;&#33021;&#34987;&#36829;&#21453;&#65292;&#29305;&#21035;&#26159;&#22312;&#21709;&#24212;&#34987;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#26159;&#21542;&#20173;&#28982;&#21487;&#33021;&#35782;&#21035;&#20854;&#20182;&#24418;&#24335;&#30340;&#19981;&#21464;&#24615;&#20197;&#20419;&#36827;&#39044;&#27979;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#24182;&#24341;&#20837;&#19981;&#21464;&#21305;&#37197;&#23646;&#24615;&#65288;IMP&#65289;&#65292;&#36825;&#26159;&#36890;&#36807;&#38468;&#21152;&#29305;&#24449;&#26469;&#25429;&#25417;&#24178;&#39044;&#30340;&#19968;&#20010;&#26126;&#30830;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#21478;&#19968;&#31181;&#24418;&#24335;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become increasingly common nowadays to collect observations of feature and response pairs from different environments. As a consequence, one has to apply learned predictors to data with a different distribution due to distribution shifts. One principled approach is to adopt the structural causal models to describe training and test models, following the invariance principle which says that the conditional distribution of the response given its predictors remains the same across environments. However, this principle might be violated in practical settings when the response is intervened. A natural question is whether it is still possible to identify other forms of invariance to facilitate prediction in unseen environments. To shed light on this challenging scenario, we focus on linear structural causal models (SCMs) and introduce invariant matching property (IMP), an explicit relation to capture interventions through an additional feature, leading to an alternative form of invari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#20998;&#31867;&#12289;&#24403;&#21069;&#36235;&#21183;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#35813;&#35843;&#26597;&#24378;&#35843;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.03392</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#20998;&#31867;&#12289;&#24403;&#21069;&#36235;&#21183;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions. (arXiv:2208.03392v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#20998;&#31867;&#12289;&#24403;&#21069;&#36235;&#21183;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#35813;&#35843;&#26597;&#24378;&#35843;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#23398;&#24212;&#29992;&#39046;&#22495;&#24050;&#25104;&#20026;&#35774;&#35745;&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#35786;&#26029;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#26395;&#36884;&#24452;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#23398;&#24212;&#29992;&#39046;&#22495;&#21463;&#21040;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22312;&#25913;&#21892;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#26222;&#36941;&#38754;&#20020;&#30528;&#37319;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#30103;&#24212;&#29992;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#28385;&#36275;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#26381;&#21153;&#36136;&#37327;&#26631;&#20934;&#30340;&#33392;&#24040;&#20219;&#21153;&#12290;&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#20351;&#24471;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#24050;&#25104;&#20026;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#25955;&#30340;&#36793;&#32536;&#32593;&#32476;&#20013;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#20197;&#20445;&#25252;&#38544;&#31169;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of the IoT, AI and ML/DL algorithms, the landscape of data-driven medical applications has emerged as a promising avenue for designing robust and scalable diagnostic and prognostic models from medical data. Consequently, the realm of data-driven medical applications has garnered significant attention spanning academia and industry, ushering in marked enhancements in healthcare delivery quality. Despite these strides, the adoption of AI-driven medical applications remains hindered by formidable challenges, including the arduous task of meeting security, privacy, and quality of service (QoS) standards. Recent developments in federated learning have made it possible to train complex machine-learned models in a distributed manner and has become an active research domain, particularly processing the medical data at the edge of the network in a decentralized way to preserve privacy and address security concerns. To this end, this survey paper highlights the current and future
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#34880;&#20957;&#32032;&#24207;&#21015;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#22312;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#24207;&#21015;&#30340;&#36215;&#28304;&#12290;</title><link>http://arxiv.org/abs/2207.13842</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30740;&#31350;&#34880;&#20957;&#32032;&#24207;&#21015;&#22312;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences. (arXiv:2207.13842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#34880;&#20957;&#32032;&#24207;&#21015;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#22312;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#24207;&#21015;&#30340;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#30149;&#27602;&#31361;&#21464;&#36805;&#36895;&#65292;&#23545;&#20844;&#20247;&#20581;&#24247;&#65292;&#29305;&#21035;&#26159;&#33030;&#24369;&#32676;&#20307;&#26500;&#25104;&#23041;&#32961;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#22312;&#19981;&#21516;&#29289;&#31181;&#20043;&#38388;&#24341;&#21457;&#36807;&#22823;&#27969;&#34892;&#12290;&#30830;&#23450;&#30149;&#27602;&#30340;&#36215;&#28304;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#38450;&#27490;&#30123;&#24773;&#30340;&#20256;&#25773;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#30149;&#27602;&#24207;&#21015;&#30340;&#24555;&#36895;&#20934;&#30830;&#39044;&#27979;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30495;&#23454;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#20197;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#30001;&#20110;&#34880;&#20957;&#32032;&#26159;&#20813;&#30123;&#21453;&#24212;&#20013;&#30340;&#20027;&#35201;&#34507;&#30333;&#36136;&#65292;&#21482;&#20351;&#29992;&#34880;&#20957;&#32032;&#24207;&#21015;&#65292;&#24182;&#20197;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#27979;&#30149;&#27602;&#24207;&#21015;&#36215;&#28304;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#22312;&#36739;&#39640;&#20998;&#31867;&#27700;&#24179;&#19978;&#22823;&#32422;&#26377;99.54&#65285;&#30340;AUCPR&#65292;98.01&#65285;&#30340;F1&#24471;&#20998;&#21644;96.60&#65285;&#30340;MCC&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenza viruses mutate rapidly and can pose a threat to public health, especially to those in vulnerable groups. Throughout history, influenza A viruses have caused pandemics between different species. It is important to identify the origin of a virus in order to prevent the spread of an outbreak. Recently, there has been increasing interest in using machine learning algorithms to provide fast and accurate predictions for viral sequences. In this study, real testing data sets and a variety of evaluation metrics were used to evaluate machine learning algorithms at different taxonomic levels. As hemagglutinin is the major protein in the immune response, only hemagglutinin sequences were used and represented by position-specific scoring matrix and word embedding. The results suggest that the 5-grams-transformer neural network is the most effective algorithm for predicting viral sequence origins, with approximately 99.54% AUCPR, 98.01% F1 score and 96.60% MCC at a higher classification l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#26631;&#35760;&#30340;&#19981;&#30830;&#23450;&#31034;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#24341;&#20837;&#28151;&#28102;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#31034;&#20363;&#20998;&#20026;&#20998;&#24067;&#22806;&#12289;&#36793;&#30028;&#21644;&#39640;&#20998;&#24067;&#35823;&#20998;&#31867;&#21306;&#22495;&#30340;&#19977;&#31867;&#65292;&#26412;&#30740;&#31350;&#20026;&#35780;&#20272;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.05161</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#26631;&#35760;&#26159;&#20160;&#20040;&#65311;&#28508;&#22312;&#23494;&#24230;&#27169;&#22411;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization. (arXiv:2207.05161v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#26631;&#35760;&#30340;&#19981;&#30830;&#23450;&#31034;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#24341;&#20837;&#28151;&#28102;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#31034;&#20363;&#20998;&#20026;&#20998;&#24067;&#22806;&#12289;&#36793;&#30028;&#21644;&#39640;&#20998;&#24067;&#35823;&#20998;&#31867;&#21306;&#22495;&#30340;&#19977;&#31867;&#65292;&#26412;&#30740;&#31350;&#20026;&#35780;&#20272;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#23545;&#20110;&#21019;&#24314;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#33021;&#22815;&#26631;&#35760;&#21487;&#30097;&#26679;&#26412;&#30340;UQ&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#31350;&#31455;&#35782;&#21035;&#20102;&#20160;&#20040;&#20869;&#23481;&#24448;&#24448;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20998;&#31867;&#20219;&#21153;&#20013;&#34987;UQ&#26041;&#27861;&#26631;&#35760;&#20026;&#19981;&#30830;&#23450;&#30340;&#31034;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#28102;&#23494;&#24230;&#30697;&#38453;&#8212;&#8212;&#22522;&#20110;&#26680;&#30340;&#35823;&#20998;&#31867;&#23494;&#24230;&#30340;&#36817;&#20284;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23558;&#34987;&#32473;&#23450;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#35782;&#21035;&#20026;&#21487;&#30097;&#26679;&#26412;&#30340;&#31034;&#20363;&#20998;&#20026;&#19977;&#31867;&#65306;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#12289;&#36793;&#30028;&#65288;Bnd&#65289;&#26679;&#26412;&#21644;&#22788;&#20110;&#39640;&#20998;&#24067;&#35823;&#20998;&#31867;&#21306;&#22495;&#65288;IDM&#65289;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#35780;&#20272;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods in classification tasks. We introduce the confusion density matrix -- a kernel-based approximation of the misclassification density -- and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20845;&#31181;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21463;&#27745;&#26579;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110;&#25968;&#25454;&#27745;&#26579;&#38750;&#24120;&#25935;&#24863;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#26032;&#27169;&#22411;&#26102;&#23545;&#20110;&#25968;&#25454;&#25200;&#21160;&#30340;&#33258;&#25105;&#38450;&#24481;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03576</link><description>&lt;p&gt;
&#28145;&#24230;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems. (arXiv:2207.03576v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20845;&#31181;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21463;&#27745;&#26579;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110;&#25968;&#25454;&#27745;&#26579;&#38750;&#24120;&#25935;&#24863;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#26032;&#27169;&#22411;&#26102;&#23545;&#20110;&#25968;&#25454;&#25200;&#21160;&#30340;&#33258;&#25105;&#38450;&#24481;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24314;&#31435;&#23433;&#20840;&#35745;&#31639;&#26426;&#32593;&#32476;&#30340;&#28508;&#22312;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#12290;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#27604;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#24320;&#22987;&#24191;&#27867;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25968;&#25454;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#25104;&#20026;&#20102;&#25915;&#20987;&#32773;&#30340;&#28508;&#22312;&#30446;&#26631;&#12290;&#25968;&#25454;&#27602;&#21270;&#25110;&#27745;&#26579;&#22522;&#26412;&#19978;&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#27450;&#39575;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24120;&#29992;&#25216;&#26415;&#20043;&#19968;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20845;&#31181;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21463;&#27745;&#26579;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#23545;&#25968;&#25454;&#27745;&#26579;&#25935;&#24863;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#22312;&#24320;&#21457;&#26032;&#27169;&#22411;&#26102;&#33258;&#25105;&#38450;&#24481;&#23545;&#20110;&#25968;&#25454;&#25200;&#21160;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, advances in deep learning have been observed in various fields, including computer vision, natural language processing, and cybersecurity. Machine learning (ML) has demonstrated its ability as a potential tool for anomaly detection-based intrusion detection systems to build secure computer networks. Increasingly, ML approaches are widely adopted than heuristic approaches for cybersecurity because they learn directly from data. Data is critical for the development of ML systems, and becomes potential targets for attackers. Basically, data poisoning or contamination is one of the most common techniques used to fool ML models through data. This paper evaluates the robustness of six recent deep learning algorithms for intrusion detection on contaminated data. Our experiments suggest that the state-of-the-art algorithms used in this study are sensitive to data contamination and reveal the importance of self-defense against data perturbation when developing novel models, especially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25216;&#26415;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#35282;&#24230;&#20986;&#21457;&#65292;&#39318;&#27425;&#35843;&#26597;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#20559;&#35265;&#26469;&#28304;&#30340;&#20998;&#31867;&#27861;&#21644;&#30001;&#20854;&#24341;&#36215;&#30340;&#27495;&#35270;&#31867;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#20013;&#19981;&#20844;&#24179;&#32467;&#26524;&#30340;&#22330;&#26223;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2207.03444</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fairness and Bias in Robot Learning. (arXiv:2207.03444v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25216;&#26415;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#35282;&#24230;&#20986;&#21457;&#65292;&#39318;&#27425;&#35843;&#26597;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#20559;&#35265;&#26469;&#28304;&#30340;&#20998;&#31867;&#27861;&#21644;&#30001;&#20854;&#24341;&#36215;&#30340;&#27495;&#35270;&#31867;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#20013;&#19981;&#20844;&#24179;&#32467;&#26524;&#30340;&#22330;&#26223;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26174;&#33879;&#22686;&#24378;&#20102;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#20154;&#31867;&#29615;&#22659;&#20013;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#24182;&#36866;&#24212;&#25105;&#20204;&#19981;&#30830;&#23450;&#30340;&#30495;&#23454;&#19990;&#30028;&#12290;&#26368;&#36817;&#21508;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#37325;&#22797;&#20154;&#31867;&#30340;&#20559;&#35265;&#65292;&#24182;&#22240;&#27492;&#23548;&#33268;&#20855;&#26377;&#27495;&#35270;&#24615;&#30340;&#32467;&#26524;&#12290;&#38543;&#30528;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#25191;&#34892;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#65292;&#20102;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#24433;&#21709;&#20197;&#38450;&#27490;&#23545;&#26576;&#20123;&#20154;&#32676;&#20135;&#29983;&#24847;&#22806;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#25361;&#25112;&#30340;&#36328;&#23398;&#31185;&#35282;&#24230;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#39318;&#27425;&#35843;&#26597;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20559;&#35265;&#26469;&#28304;&#30340;&#20998;&#31867;&#27861;&#21644;&#30001;&#23427;&#20204;&#24341;&#36215;&#30340;&#27495;&#35270;&#31867;&#22411;&#12290;&#36890;&#36807;&#19981;&#21516;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#20844;&#24179;&#32467;&#26524;&#30340;&#22330;&#26223;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has significantly enhanced the abilities of robots, enabling them to perform a wide range of tasks in human environments and adapt to our uncertain real world. Recent works in various machine learning domains have highlighted the importance of accounting for fairness to ensure that these algorithms do not reproduce human biases and consequently lead to discriminatory outcomes. With robot learning systems increasingly performing more and more tasks in our everyday lives, it is crucial to understand the influence of such biases to prevent unintended behavior toward certain groups of people. In this work, we present the first survey on fairness in robot learning from an interdisciplinary perspective spanning technical, ethical, and legal challenges. We propose a taxonomy for sources of bias and the resulting types of discrimination due to them. Using examples from different robot learning domains, we examine scenarios of unfair outcomes and strategies to mitigate them. We
&lt;/p&gt;</description></item><item><title>SCOPE &#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21322;&#30417;&#30563;&#23545;&#27604;&#24322;&#24120;&#20540;&#21076;&#38500;&#30340;&#20266;&#26368;&#22823;&#26399;&#26395;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#28151;&#28102;&#38169;&#35823;&#26469;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.14261</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#27604;&#24322;&#24120;&#20540;&#21076;&#38500;&#30340;&#20266;&#26368;&#22823;&#26399;&#26395;&#21270;&#26041;&#27861;&#65288;SCOPE&#65289;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE). (arXiv:2206.14261v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14261
&lt;/p&gt;
&lt;p&gt;
SCOPE &#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21322;&#30417;&#30563;&#23545;&#27604;&#24322;&#24120;&#20540;&#21076;&#38500;&#30340;&#20266;&#26368;&#22823;&#26399;&#26395;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#28151;&#28102;&#38169;&#35823;&#26469;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#23558;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#19982;&#36739;&#22823;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;&#20986;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21322;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20266;&#26631;&#31614;&#12289;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20266;&#26631;&#31614;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28151;&#28102;&#30340;&#24433;&#21709;&#65292;&#22312;&#26089;&#26399;&#36845;&#20195;&#20013;&#65292;&#38169;&#35823;&#30340;&#20266;&#26631;&#31614;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#26631;&#31614;&#65292;&#23548;&#33268;&#27169;&#22411;&#21152;&#24378;&#20854;&#20808;&#21069;&#30340;&#20559;&#35265;&#65292;&#36827;&#32780;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#24378;&#22823;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#21322;&#30417;&#30563;&#23545;&#27604;&#24322;&#24120;&#20540;&#21076;&#38500;&#30340;&#20266;&#26368;&#22823;&#26399;&#26395;&#21270;&#26041;&#27861;&#65288;SCOPE&#65289;&#26469;&#25233;&#21046;&#28151;&#28102;&#38169;&#35823;&#12290;&#20687;&#22522;&#26412;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#19968;&#26679;&#65292;SCOPE&#19982;&#26368;&#22823;&#26399;&#26395;&#21270;&#65288;EM&#65289;&#30456;&#20851;&#65292;EM&#26159;&#19968;&#31181;&#28508;&#21464;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#29702;&#35299;&#32858;&#31867;&#20551;&#35774;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#19982;&#22522;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning is the problem of training an accurate predictive model by combining a small labeled dataset with a presumably much larger unlabeled dataset. Many methods for semi-supervised deep learning have been developed, including pseudolabeling, consistency regularization, and contrastive learning techniques. Pseudolabeling methods however are highly susceptible to confounding, in which erroneous pseudolabels are assumed to be true labels in early iterations, thereby causing the model to reinforce its prior biases and thereby fail to generalize to strong predictive performance. We present a new approach to suppress confounding errors through a method we describe as Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE). Like basic pseudolabeling, SCOPE is related to Expectation Maximization (EM), a latent variable framework which can be extended toward understanding cluster-assumption deep semi-supervised algorithms. However, unlike basic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IVRE&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#22312;IVRE&#20013;&#35774;&#32622;&#19981;&#30830;&#23450;&#30340;&#21160;&#20316;-&#25928;&#26524;&#23545;&#65292;&#35201;&#27714;&#20195;&#29702;&#30830;&#23450;&#23545;&#35937;&#30340;&#35282;&#33394;&#65292;&#24182;&#40723;&#21169;&#20195;&#29702;&#22522;&#20110;&#35266;&#23519;&#25552;&#20986;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2206.09203</link><description>&lt;p&gt;
&#12298;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20132;&#20114;&#24335;&#35270;&#35273;&#25512;&#29702;&#12299;
&lt;/p&gt;
&lt;p&gt;
Interactive Visual Reasoning under Uncertainty. (arXiv:2206.09203v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IVRE&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#22312;IVRE&#20013;&#35774;&#32622;&#19981;&#30830;&#23450;&#30340;&#21160;&#20316;-&#25928;&#26524;&#23545;&#65292;&#35201;&#27714;&#20195;&#29702;&#30830;&#23450;&#23545;&#35937;&#30340;&#35282;&#33394;&#65292;&#24182;&#40723;&#21169;&#20195;&#29702;&#22522;&#20110;&#35266;&#23519;&#25552;&#20986;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#20043;&#19968;&#26159;&#36890;&#36807;&#29983;&#25104;&#20551;&#35774;&#24182;&#36890;&#36807;&#31215;&#26497;&#35797;&#39564;&#26469;&#36805;&#36895;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#36935;&#21040;&#20276;&#38543;&#30528;&#27169;&#31946;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#29616;&#35937;&#26102;&#65292;&#20154;&#31867;&#23545;&#25968;&#25454;&#25552;&#20986;&#20551;&#35774;&#65292;&#36890;&#36807;&#35266;&#23519;&#36827;&#34892;&#25512;&#29702;&#65292;&#36890;&#36807;&#23454;&#39564;&#26469;&#27979;&#35797;&#20182;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#22312;&#19981;&#19968;&#33268;&#20986;&#29616;&#26102;&#20462;&#27491;&#21629;&#39064;&#12290;&#36825;&#20123;&#36845;&#20195;&#36807;&#31243;&#25345;&#32493;&#21040;&#24213;&#23618;&#26426;&#21046;&#21464;&#24471;&#28165;&#26224;&#20026;&#27490;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;IVRE&#65288;&#35835;&#20316;"ivory"&#65289;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;IVRE&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#29615;&#22659;&#65292;&#22260;&#32469;Blicket&#26816;&#27979;&#30340;&#20016;&#23500;&#22330;&#26223;&#23637;&#24320;&#12290;IVRE&#20013;&#30340;&#20195;&#29702;&#34987;&#25918;&#32622;&#22312;&#20855;&#26377;&#21508;&#31181;&#27169;&#31946;&#30340;&#21160;&#20316;-&#25928;&#26524;&#23545;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#34987;&#35201;&#27714;&#30830;&#23450;&#27599;&#20010;&#23545;&#35937;&#30340;&#35282;&#33394;&#12290;&#20182;&#20204;&#34987;&#40723;&#21169;&#22522;&#20110;&#35266;&#23519;&#25552;&#20986;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#20182;&#20204;&#30340;&#20551;&#35774;&#65292;&#24182;&#31215;&#26497;&#25910;&#38598;&#26032;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental cognitive abilities of humans is to quickly resolve uncertainty by generating hypotheses and testing them via active trials. Encountering a novel phenomenon accompanied by ambiguous cause-effect relationships, humans make hypotheses against data, conduct inferences from observation, test their theory via experimentation, and correct the proposition if inconsistency arises. These iterative processes persist until the underlying mechanism becomes clear. In this work, we devise the IVRE (pronounced as "ivory") environment for evaluating artificial agents' reasoning ability under uncertainty. IVRE is an interactive environment featuring rich scenarios centered around Blicket detection. Agents in IVRE are placed into environments with various ambiguous action-effect pairs and asked to determine each object's role. They are encouraged to propose effective and efficient experiments to validate their hypotheses based on observations and actively gather new information. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.07550</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#35825;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#36215;&#28304;&#20110;&#21746;&#23398;&#25506;&#32034;&#65292;&#20851;&#27880;&#20010;&#20307;&#22312;&#24605;&#32771;&#12289;&#24773;&#24863;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#26085;&#24120;&#21512;&#20316;&#30340;&#31038;&#20132;&#26426;&#22120;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#65306;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#25317;&#26377;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20010;&#24615;&#65311;&#22914;&#26524;&#26159;&#65292;&#25105;&#20204;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#65311;&#36827;&#19968;&#27493;&#22320;&#65292;&#22312;&#27492;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#22914;&#20309;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#20010;&#24615;&#24211;(Machine Personality Inventory, MPI)&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#30340;&#20010;&#24615;&#12290;MPI&#36981;&#24490;&#26631;&#20934;&#21270;&#30340;&#20010;&#24615;&#27979;&#35797;&#65292;&#22522;&#20110;&#20116;&#22240;&#32032;&#20154;&#26684;&#29702;&#35770;&#21644;&#20154;&#26684;&#35780;&#20272;&#24211;&#24314;&#31435;&#12290;&#36890;&#36807;&#29992;MPI&#31995;&#32479;&#22320;&#35780;&#20272;LLM&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;LLM&#30340;&#20010;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#20010;&#24615;&#25552;&#31034;(Personality Prompting, P^2)&#26041;&#27861;&#65292;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLMs&#20855;&#26377;&#29305;&#23450;&#30340;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#39640;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#30340;&#23398;&#20064;&#25928;&#26524;&#26356;&#22909;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2206.02341</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#36827;&#34892;&#22797;&#26434;&#36816;&#21160;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Complex Locomotion Skill Learning via Differentiable Physics. (arXiv:2206.02341v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#39640;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#30340;&#23398;&#20064;&#25928;&#26524;&#26356;&#22909;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#29289;&#29702;&#33021;&#22815;&#36890;&#36807;&#26377;&#25928;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26469;&#33719;&#24471;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#21482;&#33021;&#25552;&#20379;&#20855;&#26377;&#26377;&#38480;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36755;&#20986;&#32479;&#19968;&#30340;&#20855;&#26377;&#26356;&#39640;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#25552;&#39640;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#23545;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#21253;&#25324;&#21608;&#26399;&#28608;&#27963;&#20989;&#25968;&#21644;&#23450;&#21046;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25209;&#22788;&#29702;&#21644;Adam&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22797;&#26434;&#36816;&#21160;&#20219;&#21153;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#22312;&#21487;&#24494;&#20998;&#30340;&#36136;&#28857;&#24377;&#31783;&#21644;&#26448;&#26009;&#28857;&#27861;&#65288;MPM&#65289;&#27169;&#25311;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36827;&#34892;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#20219;&#21153;&#21644;&#22810;&#20010;&#26426;&#22120;&#20154;&#35774;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#23398;&#20064;&#26694;&#26550;&#27604;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable physics enables efficient gradient-based optimizations of neural network (NN) controllers. However, existing work typically only delivers NN controllers with limited capability and generalizability. We present a practical learning framework that outputs unified NN controllers capable of tasks with significantly improved complexity and diversity. To systematically improve training robustness and efficiency, we investigated a suite of improvements over the baseline approach, including periodic activation functions, and tailored loss functions. In addition, we find our adoption of batching and an Adam optimizer effective in training complex locomotion tasks. We evaluate our framework on differentiable mass-spring and material point method (MPM) simulations, with challenging locomotion tasks and multiple robot designs. Experiments show that our learning framework, based on differentiable physics, delivers better results than reinforcement learning and converges much faster. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.15677</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#21487;&#24494;&#22686;&#24378;&#25216;&#26415;&#25913;&#21892;&#20102;GAN&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#22686;&#24378;&#25216;&#26415;&#38544;&#24335;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#19981;&#21464;&#24615;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#30001;&#25968;&#25454;&#36716;&#25442;&#24341;&#36215;&#30340;&#26631;&#31614;&#31354;&#38388;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#21028;&#21035;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26368;&#32456;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21464;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#32487;&#25215;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#21487;&#20197;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#30340;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#39044;&#27979;&#30446;&#26631;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#36824;&#40723;&#21169;&#29983;&#25104;&#22120;&#23545;&#25239;&#22320;&#29983;&#25104;&#20854;&#22686;&#24378;&#21442;&#25968;&#21487;&#20197;&#34987;&#21028;&#21035;&#22120;&#20934;&#30830;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#37327;&#21644;&#26356;&#39640;&#25928;&#30340;&#21028;&#21035;&#22120;&#65292;&#25552;&#39640;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
&lt;/p&gt;</description></item><item><title>DELTA &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#26469;&#20943;&#23569;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#25152;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20197;&#32531;&#35299;&#29616;&#26377;&#25277;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23427;&#32771;&#34385;&#20102;&#23458;&#25143;&#31471;&#30340;&#22810;&#26679;&#24615;&#21644;&#23616;&#37096;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#25152;&#38656;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20195;&#34920;&#24615;&#23458;&#25143;&#31471;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DELTA &#21487;&#20197;&#20248;&#20110;&#20854;&#20182;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#24182;&#21152;&#36895;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2205.13925</link><description>&lt;p&gt;
DELTA: &#22810;&#26679;&#21270;&#23458;&#25143;&#25277;&#26679;&#29992;&#20110;&#24555;&#36895;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DELTA: Diverse Client Sampling for Fasting Federated Learning. (arXiv:2205.13925v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13925
&lt;/p&gt;
&lt;p&gt;
DELTA &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#26469;&#20943;&#23569;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#25152;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20197;&#32531;&#35299;&#29616;&#26377;&#25277;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23427;&#32771;&#34385;&#20102;&#23458;&#25143;&#31471;&#30340;&#22810;&#26679;&#24615;&#21644;&#23616;&#37096;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#25152;&#38656;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20195;&#34920;&#24615;&#23458;&#25143;&#31471;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DELTA &#21487;&#20197;&#20248;&#20110;&#20854;&#20182;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#24182;&#21152;&#36895;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20197;&#39640;&#25928;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#19981;&#21512;&#36866;&#30340;&#23458;&#25143;&#31471;&#25277;&#26679;&#26041;&#26696;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#20986;&#19981;&#20855;&#20195;&#34920;&#24615;&#23376;&#38598;&#65292;&#20174;&#32780;&#23548;&#33268;&#27169;&#22411;&#26356;&#26032;&#30340;&#26174;&#33879;&#26041;&#24046;&#21644;&#25910;&#25947;&#36895;&#24230;&#30340;&#20943;&#24930;&#12290;&#29616;&#26377;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#33021;&#20250;&#26377;&#20559;&#24046;&#65292;&#25110;&#32773;&#21487;&#20197;&#36827;&#19968;&#27493;&#20248;&#21270;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; DELTA&#65292;&#36825;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#12290;DELTA &#21051;&#30011;&#20102;&#23458;&#25143;&#31471;&#30340;&#22810;&#26679;&#24615;&#21644;&#23616;&#37096;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#25152;&#38656;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20195;&#34920;&#24615;&#23458;&#25143;&#31471;&#12290;&#27492;&#22806;&#65292;DELTA &#26159;&#19968;&#31181;&#32463;&#36807;&#35777;&#26126;&#30340;&#26368;&#20248;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#65292;&#22312;&#20943;&#23569;&#30001;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#24341;&#36215;&#30340;&#26041;&#24046;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#20026;&#35299;&#20915;&#20840;&#23458;&#25143;&#31471;&#26799;&#24230;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#29256;&#26412;&#30340; DELTA&#65292;&#23427;&#21462;&#20915;&#20110;&#21487;&#29992;&#23458;&#25143;&#31471;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial client participation has been widely adopted in Federated Learning (FL) to reduce the communication burden efficiently. However, an inadequate client sampling scheme can lead to the selection of unrepresentative subsets, resulting in significant variance in model updates and slowed convergence. Existing sampling methods are either biased or can be further optimized for faster convergence.In this paper, we present DELTA, an unbiased sampling scheme designed to alleviate these issues. DELTA characterizes the effects of client diversity and local variance, and samples representative clients with valuable information for global model updates. In addition, DELTA is a proven optimal unbiased sampling scheme that minimizes variance caused by partial client participation and outperforms other unbiased sampling schemes in terms of convergence. Furthermore, to address full-client gradient dependence,we provide a practical version of DELTA depending on the available clients' information, 
&lt;/p&gt;</description></item><item><title>INSPIRE&#26159;&#19968;&#31181;&#38754;&#21521;&#23494;&#38598;WLAN&#20013;&#31354;&#38388;&#22797;&#29992;&#30340;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26080;&#32447;&#20256;&#36755;&#21442;&#25968;&#30340;&#20248;&#21270;&#65292;&#25552;&#39640;WLAN&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.10184</link><description>&lt;p&gt;
INSPIRE&#65306;&#38754;&#21521;&#23494;&#38598;WLAN&#20013;&#31354;&#38388;&#22797;&#29992;&#30340;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
INSPIRE: Distributed Bayesian Optimization for ImproviNg SPatIal REuse in Dense WLANs. (arXiv:2204.10184v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10184
&lt;/p&gt;
&lt;p&gt;
INSPIRE&#26159;&#19968;&#31181;&#38754;&#21521;&#23494;&#38598;WLAN&#20013;&#31354;&#38388;&#22797;&#29992;&#30340;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26080;&#32447;&#20256;&#36755;&#21442;&#25968;&#30340;&#20248;&#21270;&#65292;&#25552;&#39640;WLAN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WLANs&#24050;&#32463;&#21462;&#20195;&#26377;&#32447;&#32593;&#32476;&#25104;&#20026;&#36830;&#25509;&#35774;&#22791;&#21040;&#20114;&#32852;&#32593;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#20294;&#30001;&#20110;&#26080;&#32447;&#30005;&#39057;&#35889;&#31354;&#38388;&#26377;&#38480;&#65292;&#23481;&#26131;&#20986;&#29616;&#24615;&#33021;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;IEEE 802.11ax&#21644;&#38543;&#21518;&#30340;&#20462;&#27491;&#26696;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#26080;&#32447;&#20256;&#36755;&#20013;&#20004;&#20010;&#20851;&#38190;&#21442;&#25968;&#65288;&#20256;&#36755;&#21151;&#29575;&#21644;&#28789;&#25935;&#24230;&#38408;&#20540;&#65289;&#30340;&#21160;&#24577;&#26356;&#26032;&#65292;&#25552;&#39640;&#26080;&#32447;&#20449;&#36947;&#30340;&#31354;&#38388;&#22797;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;INSPIRE&#30340;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#25191;&#34892;&#26412;&#22320;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20197;&#25552;&#39640;WLAN&#20013;&#30340;&#31354;&#38388;&#22797;&#29992;&#12290;INSPIRE&#19981;&#23545;WLAN&#30340;&#25299;&#25169;&#20570;&#20986;&#26126;&#30830;&#20551;&#35774;&#65292;&#38738;&#30544;&#26080;&#32447;&#25509;&#20837;&#28857;&#30340;&#26080;&#31169;&#34892;&#20026;&#65292;&#20351;&#20854;&#25214;&#21040;&#36866;&#24403;&#30340;&#20256;&#36755;&#21151;&#29575;&#21644;&#28789;&#25935;&#24230;&#38408;&#20540;&#21442;&#25968;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;WLAN&#30340;&#8220;&#26368;&#22823;&#21033;&#30410;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;ns-3&#27169;&#25311;&#22120;&#21644;&#20004;&#20010;&#31034;&#20363;&#34920;&#26126;&#20102;INSPIRE&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#31574;&#30053;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
WLANs, which have overtaken wired networks to become the primary means of connecting devices to the Internet, are prone to performance issues due to the scarcity of space in the radio spectrum. As a response, IEEE 802.11ax and subsequent amendments aim at increasing the spatial reuse of a radio channel by allowing the dynamic update of two key parameters in wireless transmission: the transmission power (TX_POWER) and the sensitivity threshold (OBSS_PD). In this paper, we present INSPIRE, a distributed solution performing local Bayesian optimizations based on Gaussian processes to improve the spatial reuse in WLANs. INSPIRE makes no explicit assumptions about the topology of WLANs and favors altruistic behaviors of the access points, leading them to find adequate configurations of their TX_POWER and OBSS_PD parameters for the "greater good" of the WLANs. We demonstrate the superiority of INSPIRE over other state-of-the-art strategies using the ns-3 simulator and two examples inspired by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#26102;&#38388;&#20013;&#20855;&#26377;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#21644;&#38543;&#26426;&#24377;&#24615;&#30340;&#26368;&#20248;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#32447;&#24615;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#20540;&#31639;&#27861;&#26469;&#22788;&#29702;&#38750;&#32447;&#24615;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#21697;&#22312;&#21442;&#25968;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28789;&#27963;&#21487;&#20280;&#32553;&#24615;&#65292;&#23545;&#20110;&#31934;&#30830;&#26657;&#20934;&#20215;&#26684;&#24433;&#21709;&#21644;&#24377;&#24615;&#31561;&#21442;&#25968;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2204.08581</link><description>&lt;p&gt;
&#20851;&#20110;&#21442;&#25968;&#21270;&#26368;&#20248;&#25191;&#34892;&#21644;&#26426;&#22120;&#23398;&#20064;&#26367;&#20195;&#21697;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Parametric Optimal Execution and Machine Learning Surrogates. (arXiv:2204.08581v3 [q-fin.TR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#26102;&#38388;&#20013;&#20855;&#26377;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#21644;&#38543;&#26426;&#24377;&#24615;&#30340;&#26368;&#20248;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#32447;&#24615;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#20540;&#31639;&#27861;&#26469;&#22788;&#29702;&#38750;&#32447;&#24615;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#21697;&#22312;&#21442;&#25968;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28789;&#27963;&#21487;&#20280;&#32553;&#24615;&#65292;&#23545;&#20110;&#31934;&#30830;&#26657;&#20934;&#20215;&#26684;&#24433;&#21709;&#21644;&#24377;&#24615;&#31561;&#21442;&#25968;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#21644;&#38543;&#26426;&#24377;&#24615;&#30340;&#31163;&#25955;&#26102;&#38388;&#26368;&#20248;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#32447;&#24615;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#30340;&#38381;&#24335;&#36882;&#24402;&#20844;&#24335;&#65292;&#25193;&#23637;&#20102;Obizhaeva&#21644;Wang&#65288;2013&#24180;&#65292;&#12298;&#37329;&#34701;&#24066;&#22330;&#26434;&#24535;&#12299;&#65289;&#30340;&#30830;&#23450;&#24615;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#21644;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#20540;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;Bouchaud&#31561;&#20154;&#65288;2004&#24180;&#65292;&#12298;&#37327;&#21270;&#37329;&#34701;&#12299;&#65289;&#25552;&#20986;&#30340;&#38750;&#32447;&#24615;&#30636;&#26102;&#20215;&#26684;&#24433;&#21709;&#30340;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#29992;&#20110;&#20215;&#20540;&#20989;&#25968;&#21644;&#21453;&#39304;&#25511;&#21046;&#12290;NN&#21151;&#33021;&#36924;&#36817;&#22120;&#30340;&#28789;&#27963;&#21487;&#20280;&#32553;&#24615;&#20351;&#24471;&#21442;&#25968;&#21270;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#23558;&#20960;&#20010;&#27169;&#22411;&#25110;&#24066;&#22330;&#21442;&#25968;&#20316;&#20026;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#37096;&#20998;&#12290;&#31934;&#30830;&#26657;&#20934;&#20215;&#26684;&#24433;&#21709;&#12289;&#24377;&#24615;&#31561;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#27492;&#20102;&#35299;&#25191;&#34892;&#31574;&#30053;&#23545;&#36825;&#20123;&#21442;&#25968;&#30340;&#25935;&#24863;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate optimal order execution problems in discrete time with instantaneous price impact and stochastic resilience. First, in the setting of linear transient price impact we derive a closed-form recursion for the optimal strategy, extending the deterministic results from Obizhaeva and Wang (J Financial Markets, 2013). Second, we develop a numerical algorithm based on dynamic programming and deep learning for the case of nonlinear transient price impact as proposed by Bouchaud et al. (Quant. Finance, 2004). Specifically, we utilize an actor-critic framework that constructs two neural-network (NN) surrogates for the value function and the feedback control. The flexible scalability of NN functional approximators enables parametric learning, i.e., incorporating several model or market parameters as part of the input space. Precise calibration of price impact, resilience, etc., is known to be extremely challenging and hence it is critical to understand sensitivity of the execution p
&lt;/p&gt;</description></item><item><title>Speculative Decoding&#26159;&#19968;&#31181;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#26080;&#25439;&#21152;&#36895;&#30340;&#32763;&#35793;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#23427;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#39564;&#35777;&#27169;&#22411;&#30830;&#20445;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;&#36890;&#36807;&#25512;&#27979;&#35299;&#30721;&#21644;&#39564;&#35777;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#19981;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2203.16487</link><description>&lt;p&gt;
Speculative Decoding: &#26080;&#25439;&#21152;&#36895;&#33258;&#22238;&#24402;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding: Lossless Speedup of Autoregressive Translation. (arXiv:2203.16487v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16487
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding&#26159;&#19968;&#31181;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#26080;&#25439;&#21152;&#36895;&#30340;&#32763;&#35793;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#23427;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#39564;&#35777;&#27169;&#22411;&#30830;&#20445;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;&#36890;&#36807;&#25512;&#27979;&#35299;&#30721;&#21644;&#39564;&#35777;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#19981;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20043;&#21069;&#19968;&#20123;&#29306;&#29298;&#32763;&#35793;&#36136;&#37327;&#21152;&#36895;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Speculative Decoding&#65288;SpecDec&#65289;-&#19968;&#31181;&#21463;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#25512;&#27979;&#25191;&#34892;&#21551;&#21457;&#30340;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#23427;&#32467;&#21512;&#20102;AT&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#21508;&#33258;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;SpecDec&#39318;&#20808;&#20351;&#29992;NAT&#27169;&#22411;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#65288;&#21363;&#35299;&#30721;&#65289;&#19979;&#19968;&#20010;k&#20010;&#26631;&#35760;&#65292;&#28982;&#21518;&#20351;&#29992;AT&#27169;&#22411;&#39564;&#35777;&#36825;&#20123;&#26631;&#35760;&#65292;&#21482;&#26377;&#36890;&#36807;&#39564;&#35777;&#30340;&#39044;&#27979;&#26631;&#35760;&#25165;&#20250;&#34987;&#25509;&#21463;&#20316;&#20026;&#35299;&#30721;&#32467;&#26524;&#65292;&#20197;&#30830;&#20445;&#20854;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;NAT&#30340;&#25512;&#27979;&#21644;AT&#30340;&#39564;&#35777;&#20043;&#38388;&#30340;&#21327;&#20316;&#20351;&#24471;&#35299;&#30721;&#36895;&#24230;&#22823;&#22823;&#25552;&#39640;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#25512;&#27979;&#35299;&#30721;&#25152;&#25903;&#25345;&#30340;&#24182;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;4&#20010;&#26631;&#20934;WMT&#32763;&#35793;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#35777;&#23454;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422; $k$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from some previous work accelerating autoregressive translation (AT) at the sacrifice of quality, we propose Speculative Decoding (SpecDec) -a novel decoding paradigm inspired by speculative execution in computer architecture, which combines respective advantages of AT and non-autoregressive translation (NAT) for lossless speedup of translation. At each decoding step, SpecDec first speculatively drafts (i.e. decodes) next $k$ tokens with an NAT model and then verifies them with an AT model, where only the drafted tokens passing the verification are accepted as decoded tokens for guaranteeing its translation result is exactly the same as AT. The collaboration of NAT drafting and AT verification leads to a much higher decoding speed without quality loss due to parallel computing enabled by speculative decoding.  We conduct experiments in 4 standard WMT translation benchmarks and confirm the vanilla SpecDec yields exactly the same results as AT greedy decoding with an around $
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#23398;&#20064;&#22240;&#26524;Transformer&#65288;TLT&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#22122;&#22768;&#22270;&#20687;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;"&#23384;&#22312;&#22122;&#22768;"&#30340;&#20449;&#24687;&#20316;&#20026;&#22788;&#29702;&#36755;&#20837;&#65292;&#24182;&#32852;&#21512;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#65292;TLT&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#28508;&#22312;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2203.15529</link><description>&lt;p&gt;
&#22788;&#29702;&#23398;&#20064;&#22240;&#26524;Transformer&#29992;&#20110;&#22122;&#22768;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Treatment Learning Causal Transformer for Noisy Image Classification. (arXiv:2203.15529v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#23398;&#20064;&#22240;&#26524;Transformer&#65288;TLT&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#22122;&#22768;&#22270;&#20687;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;"&#23384;&#22312;&#22122;&#22768;"&#30340;&#20449;&#24687;&#20316;&#20026;&#22788;&#29702;&#36755;&#20837;&#65292;&#24182;&#32852;&#21512;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#65292;TLT&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#28508;&#22312;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#39030;&#32423;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#35270;&#35273;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#25506;&#32034;&#21644;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#21644;&#20854;&#20851;&#32852;&#26631;&#31614;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#30340;&#19968;&#20010;&#23454;&#38469;&#25361;&#25112;&#26159;&#23427;&#20204;&#22312;"&#22122;&#22768;"&#25968;&#25454;&#23545;&#25239;&#19979;&#24615;&#33021;&#19979;&#38477;&#65292;&#22122;&#22768;&#25968;&#25454;&#30001;&#20110;&#19981;&#21516;&#24773;&#20917;&#24341;&#36215;&#65292;&#27604;&#22914;&#34394;&#20551;&#30456;&#20851;&#24615;&#12289;&#26080;&#20851;&#32972;&#26223;&#12289;&#39046;&#22495;&#36716;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;"&#23384;&#22312;&#22122;&#22768;"&#30340;&#20108;&#36827;&#21046;&#20449;&#24687;&#20316;&#20026;&#22788;&#29702;&#36755;&#20837;&#21040;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#21463;&#22240;&#26524;&#21464;&#20998;&#25512;&#26029;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#22788;&#29702;&#23398;&#20064;&#22240;&#26524;Transformer&#65288;TLT&#65289;&#65292;&#23427;&#20351;&#29992;&#28508;&#22312;&#30340;&#29983;&#25104;&#27169;&#22411;&#20174;&#24403;&#21069;&#35266;&#27979;&#36755;&#20837;&#20013;&#20272;&#35745;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#20197;&#36827;&#34892;&#22122;&#22768;&#22270;&#20687;&#20998;&#31867;&#12290;&#26681;&#25454;&#20272;&#35745;&#30340;&#22122;&#22768;&#27700;&#24179;&#65288;&#24314;&#27169;&#20026;&#20108;&#36827;&#21046;&#22788;&#29702;&#22240;&#23376;&#65289;&#65292;TLT&#20998;&#37197;&#30456;&#24212;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Current top-notch deep learning (DL) based vision models are primarily based on exploring and exploiting the inherent correlations between training data samples and their associated labels. However, a known practical challenge is their degraded performance against "noisy" data, induced by different circumstances such as spurious correlations, irrelevant contexts, domain shift, and adversarial attacks. In this work, we incorporate this binary information of "existence of noise" as treatment into image classification tasks to improve prediction accuracy by jointly estimating their treatment effects. Motivated from causal variational inference, we propose a transformer-based architecture, Treatment Learning Causal Transformer (TLT), that uses a latent generative model to estimate robust feature representations from current observational input for noise image classification. Depending on the estimated noise level (modeled as a binary treatment factor), TLT assigns the corresponding inferen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2203.03897</link><description>&lt;p&gt;
&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29992;&#20110;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#22411;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#23884;&#20837;&#65292;&#24182;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23398;&#20064;&#21040;&#30340;&#22810;&#27169;&#22411;&#23884;&#20837;&#30340;&#20998;&#26512;&#30456;&#23545;&#36739;&#23569;&#65292;&#23884;&#20837;&#30340;&#21487;&#36716;&#31227;&#24615;&#26377;&#24453;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;CLIP&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20445;&#30041;&#20102;&#20998;&#31163;&#30340;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#23545;&#40784;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#24494;&#35843;&#20043;&#21518;&#65292;CLIP&#20173;&#28982;&#20445;&#25345;&#30528;&#36739;&#24046;&#30340;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#21487;&#33021;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#40065;&#26834;&#34920;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23884;&#20837;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#22312;&#36229;&#29699;&#38754;&#19978;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multi-modal models, such as CLIP, provide transferable embeddings and show promising results in diverse applications. However, the analysis of learned multi-modal embeddings is relatively unexplored, and the embedding transferability can be improved. In this work, we observe that CLIP holds separated embedding subspaces for two different modalities, and then we investigate it through the lens of uniformity-alignment to measure the quality of learned representation. Both theoretically and empirically, we show that CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack of alignment and uniformity might restrict the transferability and robustness of embeddings. To this end, we devise a new fine-tuning method for robust representation equipping better alignment and uniformity. First, we propose a Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to generate hard negative samples on the hypersphere. Then, we fine-tune the model on har
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\phi$-&#31163;&#25955;&#24230;&#30340;&#20998;&#24067;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.02128</link><description>&lt;p&gt;
&#22522;&#20110;$\phi$-&#31163;&#25955;&#24230;&#30340;&#20998;&#24067;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Bayesian Optimization with $\phi$-divergences. (arXiv:2203.02128v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\phi$-&#31163;&#25955;&#24230;&#30340;&#20998;&#24067;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#30740;&#31350;&#22240;&#20854;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#35768;&#22810;&#31995;&#32479;&#20013;&#19981;&#21487;&#36991;&#20813;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#23427;&#38754;&#20020;&#30528;&#22810;&#26041;&#38754;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20165;&#26377;&#23569;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36825;&#20010;&#26041;&#21521;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\phi$-&#31163;&#25955;&#24230;&#30340;&#20998;&#24067;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al. (2020), which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question Can one devise a computationally tractable algorithm for solving this DRO-BO problem? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in $\phi$-divergences, which subsumes many popular choices, such as the $\chi^2$-divergence, Total Variation, and the extant Kullback-Lei
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#31639;&#27861;MROT&#65292;&#29992;&#20110;&#25913;&#36827;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#21644;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MROT&#22312;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#21644;&#26448;&#26009;&#21560;&#38468;&#36873;&#25321;&#31561;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#20855;&#26377;&#21152;&#36895;&#21457;&#29616;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#26032;&#29289;&#36136;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2202.06208</link><description>&lt;p&gt;
&#29992;&#24230;&#37327;&#23398;&#20064;&#22686;&#24378;&#30340;&#26368;&#20248;&#20256;&#36755;&#25913;&#36827;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Molecular Representation Learning with Metric Learning-enhanced Optimal Transport. (arXiv:2202.06208v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#31639;&#27861;MROT&#65292;&#29992;&#20110;&#25913;&#36827;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#21644;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MROT&#22312;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#21644;&#26448;&#26009;&#21560;&#38468;&#36873;&#25321;&#31561;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#20855;&#26377;&#21152;&#36895;&#21457;&#29616;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#26032;&#29289;&#36136;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21270;&#23398;&#21644;&#29983;&#29289;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26377;&#38480;&#25110;&#24322;&#26500;&#12290;&#29616;&#26377;&#30340;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26410;&#33021;&#32771;&#34385;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#31639;&#27861;MROT&#65292;&#29992;&#20110;&#22686;&#24378;&#20998;&#23376;&#22238;&#24402;&#38382;&#39064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;MROT&#36890;&#36807;&#27979;&#37327;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#36317;&#31163;&#24230;&#37327;&#21644;&#20256;&#36755;&#35745;&#21010;&#19978;&#30340;&#21518;&#39564;&#26041;&#24046;&#27491;&#21017;&#21270;&#65292;&#26469;&#23398;&#20064;&#25968;&#25454;&#30340;&#36830;&#32493;&#26631;&#31614;&#65292;&#20197;&#24357;&#21512;&#21270;&#23398;&#39046;&#22495;&#30340;&#24046;&#36317;&#12290;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#30340;&#22522;&#26412;&#21270;&#23398;&#22238;&#24402;&#20219;&#21153;&#65292;&#21253;&#25324;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#21644;&#26448;&#26009;&#21560;&#38468;&#36873;&#25321;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;MROT&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#21152;&#36895;&#21457;&#29616;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#26032;&#29289;&#36136;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training data are usually limited or heterogeneous in many chemical and biological applications. Existing machine learning models for chemistry and materials science fail to consider generalizing beyond training domains. In this article, we develop a novel optimal transport-based algorithm termed MROT to enhance their generalization capability for molecular regression problems. MROT learns a continuous label of the data by measuring a new metric of domain distances and a posterior variance regularization over the transport plan to bridge the chemical domain gap. Among downstream tasks, we consider basic chemical regression tasks in unsupervised and semi-supervised settings, including chemical property prediction and materials adsorption selection. Extensive experiments show that MROT significantly outperforms state-of-the-art models, showing promising potential in accelerating the discovery of new substances with desired properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#39057;&#27604;&#29305;&#27969;&#20803;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#21387;&#32553;&#35270;&#39057;&#36136;&#37327;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#26356;&#39640;&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21387;&#32553;&#35270;&#39057;&#30340;&#36824;&#21407;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2202.00011</link><description>&lt;p&gt;
&#21033;&#29992;&#27604;&#29305;&#27969;&#20803;&#25968;&#25454;&#24555;&#36895;&#12289;&#20934;&#30830;&#12289;&#27867;&#21270;&#22320;&#21387;&#32553;&#35270;&#39057;&#36136;&#37327;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement. (arXiv:2202.00011v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#39057;&#27604;&#29305;&#27969;&#20803;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#21387;&#32553;&#35270;&#39057;&#36136;&#37327;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#26356;&#39640;&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21387;&#32553;&#35270;&#39057;&#30340;&#36824;&#21407;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21387;&#32553;&#26159;&#29616;&#20195;&#20114;&#32852;&#32593;&#30340;&#26680;&#24515;&#21151;&#33021;&#65292;&#28085;&#30422;&#20174;&#31038;&#20132;&#23186;&#20307;&#21040;&#35270;&#39057;&#20250;&#35758;&#31561;&#25216;&#26415;&#12290;&#34429;&#28982;&#35270;&#39057;&#21387;&#32553;&#19981;&#26029;&#25104;&#29087;&#65292;&#20294;&#22312;&#35768;&#22810;&#21387;&#32553;&#35774;&#32622;&#20013;&#65292;&#36136;&#37327;&#25439;&#22833;&#20173;&#28982;&#24456;&#26126;&#26174;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#23545;&#20110;&#22312;&#24102;&#23485;&#21463;&#38480;&#25110;&#19981;&#31283;&#23450;&#30340;&#36830;&#25509;&#19978;&#39640;&#25928;&#20256;&#36755;&#35270;&#39057;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#21033;&#29992;&#23884;&#20837;&#35270;&#39057;&#27604;&#29305;&#27969;&#20013;&#30340;&#24213;&#23618;&#32467;&#26500;&#21644;&#21160;&#24577;&#20449;&#24687;&#65292;&#20026;&#21387;&#32553;&#35270;&#39057;&#24674;&#22797;&#32454;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#21387;&#32553;&#26657;&#27491;&#26041;&#27861;&#25552;&#39640;&#20102;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#23454;&#29616;&#26356;&#39640;&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#65292;&#19982;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#39057;&#21387;&#32553;&#26041;&#27861;&#22312;&#36895;&#29575;-&#22833;&#30495;&#27604;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#19978;&#36827;&#34892;&#37327;&#21270;&#25968;&#25454;&#26465;&#20214;&#21270;&#65292;&#36825;&#21487;&#36731;&#26494;&#22320;&#20174;&#27604;&#29305;&#27969;&#20013;&#33719;&#21462;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#21333;&#19968;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19981;&#21516;&#30340;&#21387;&#32553;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video compression is a central feature of the modern internet powering technologies from social media to video conferencing. While video compression continues to mature, for many compression settings, quality loss is still noticeable. These settings nevertheless have important applications to the efficient transmission of videos over bandwidth constrained or otherwise unstable connections. In this work, we develop a deep learning architecture capable of restoring detail to compressed videos which leverages the underlying structure and motion information embedded in the video bitstream. We show that this improves restoration accuracy compared to prior compression correction methods and is competitive when compared with recent deep-learning-based video compression methods on rate-distortion while achieving higher throughput. Furthermore, we condition our model on quantization data which is readily available in the bitstream. This allows our single model to handle a variety of different c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#31639;&#27861;LESS&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;&#38543;&#26426;&#28857;&#20026;&#20013;&#24515;&#30340;&#23376;&#38598;&#24182;&#35757;&#32451;&#23616;&#37096;&#39044;&#27979;&#22120;&#65292;&#28982;&#21518;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#32452;&#21512;&#39044;&#27979;&#22120;&#24471;&#21040;&#25972;&#20307;&#39044;&#27979;&#22120;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#26126;&#65292;LESS&#26159;&#19968;&#31181;&#26377;&#31454;&#20105;&#21147;&#19988;&#39640;&#25928;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.06251</link><description>&lt;p&gt;
&#23398;&#20064;&#19982;&#23376;&#38598;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Learning with Subset Stacking. (arXiv:2112.06251v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#31639;&#27861;LESS&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;&#38543;&#26426;&#28857;&#20026;&#20013;&#24515;&#30340;&#23376;&#38598;&#24182;&#35757;&#32451;&#23616;&#37096;&#39044;&#27979;&#22120;&#65292;&#28982;&#21518;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#32452;&#21512;&#39044;&#27979;&#22120;&#24471;&#21040;&#25972;&#20307;&#39044;&#27979;&#22120;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#26126;&#65292;LESS&#26159;&#19968;&#31181;&#26377;&#31454;&#20105;&#21147;&#19988;&#39640;&#25928;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20174;&#19968;&#32452;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#36755;&#20837;&#21464;&#37327;&#19982;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#39044;&#27979;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#24322;&#36136;&#34892;&#20026;&#30340;&#32676;&#20307;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#29983;&#25104;&#20197;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#28857;&#20026;&#20013;&#24515;&#30340;&#23376;&#38598;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#23376;&#38598;&#35757;&#32451;&#19968;&#20010;&#23616;&#37096;&#39044;&#27979;&#22120;&#12290;&#28982;&#21518;&#36825;&#20123;&#39044;&#27979;&#22120;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#25972;&#20307;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#23558;&#27492;&#31639;&#27861;&#31216;&#20026;&#8220;&#23398;&#20064;&#19982;&#23376;&#38598;&#21472;&#21152;&#8221;&#25110;LESS&#65292;&#22240;&#20026;&#23427;&#31867;&#20284;&#20110;&#21472;&#21152;&#22238;&#24402;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;LESS&#19982;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65292;LESS&#26159;&#19968;&#31181;&#26377;&#31454;&#20105;&#21147;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LESS&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#20063;&#38750;&#24120;&#39640;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#24182;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new regression algorithm that learns from a set of input-output pairs. Our algorithm is designed for populations where the relation between the input variables and the output variable exhibits a heterogeneous behavior across the predictor space. The algorithm starts with generating subsets that are concentrated around random points in the input space. This is followed by training a local predictor for each subset. Those predictors are then combined in a novel way to yield an overall predictor. We call this algorithm ``LEarning with Subset Stacking'' or LESS, due to its resemblance to the method of stacking regressors. We compare the testing performance of LESS with state-of-the-art methods on several datasets. Our comparison shows that LESS is a competitive supervised learning method. Moreover, we observe that LESS is also efficient in terms of computation time and it allows a straightforward parallel implementation.
&lt;/p&gt;</description></item><item><title>CubeTR&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#38271;&#24207;&#21015;&#21160;&#20316;&#21644;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20174;&#20219;&#24847;&#36215;&#22987;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#19987;&#19994;&#20154;&#21592;&#35299;&#20915;&#26041;&#26696;&#38271;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2111.06036</link><description>&lt;p&gt;
CubeTR: &#20351;&#29992;Transformer&#23398;&#20064;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CubeTR: Learning to Solve The Rubiks Cube Using Transformers. (arXiv:2111.06036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06036
&lt;/p&gt;
&lt;p&gt;
CubeTR&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#38271;&#24207;&#21015;&#21160;&#20316;&#21644;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20174;&#20219;&#24847;&#36215;&#22987;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#19987;&#19994;&#20154;&#21592;&#35299;&#20915;&#26041;&#26696;&#38271;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Transformer&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#65292;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#31181;&#39046;&#22495;&#12290;&#26368;&#36817;&#25552;&#20986;&#23558;Transformer&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#20854;&#37325;&#26032;&#23450;&#20041;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30456;&#27604;&#65292;&#39764;&#26041;&#38382;&#39064;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39764;&#26041;&#26377;&#30528;&#25968;&#20197;&#21315;&#19975;&#35745;&#30340;&#21487;&#33021;&#32452;&#21512;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#35299;&#20915;&#30340;&#29366;&#24577;&#65292;&#36825;&#23548;&#33268;&#20102;&#26497;&#24230;&#31232;&#30095;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;CubeTR&#20851;&#27880;&#20110;&#38271;&#24207;&#21015;&#30340;&#21160;&#20316;&#65292;&#24182;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;CubeTR&#33021;&#22815;&#20174;&#20219;&#24847;&#36215;&#22987;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#65292;&#32463;&#36807;&#31227;&#21160;&#35268;&#33539;&#21270;&#21518;&#65292;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#38271;&#24230;&#39044;&#26399;&#23558;&#38750;&#24120;&#25509;&#36817;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#31639;&#27861;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#38271;&#24230;&#12290;CubeTR&#20026;&#23398;&#20064;&#31639;&#27861;&#22312;&#26356;&#39640;&#32500;&#24230;&#39764;&#26041;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its first appearance, transformers have been successfully used in wide ranging domains from computer vision to natural language processing. Application of transformers in Reinforcement Learning by reformulating it as a sequence modelling problem was proposed only recently. Compared to other commonly explored reinforcement learning problems, the Rubiks cube poses a unique set of challenges. The Rubiks cube has a single solved state for quintillions of possible configurations which leads to extremely sparse rewards. The proposed model CubeTR attends to longer sequences of actions and addresses the problem of sparse rewards. CubeTR learns how to solve the Rubiks cube from arbitrary starting states without any human prior, and after move regularisation, the lengths of solutions generated by it are expected to be very close to those given by algorithms used by expert human solvers. CubeTR provides insights to the generalisability of learning algorithms to higher dimensional cubes and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#25209;&#37327;&#21453;&#39304;&#30340;Lipschitz&#36138;&#23146;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLiN&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#26368;&#20248;&#22320;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#36798;&#21040;&#20102;&#26368;&#20248;&#36951;&#25022;&#29575;&#65292;&#24182;&#20165;&#20351;&#29992;&#26368;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2110.09722</link><description>&lt;p&gt;
&#24102;&#25209;&#37327;&#21453;&#39304;&#30340;Lipschitz&#36138;&#23146;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lipschitz Bandits with Batched Feedback. (arXiv:2110.09722v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.09722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#25209;&#37327;&#21453;&#39304;&#30340;Lipschitz&#36138;&#23146;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLiN&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#26368;&#20248;&#22320;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#36798;&#21040;&#20102;&#26368;&#20248;&#36951;&#25022;&#29575;&#65292;&#24182;&#20165;&#20351;&#29992;&#26368;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24102;&#26377;&#25209;&#37327;&#21453;&#39304;&#30340;Lipschitz&#36138;&#23146;&#38382;&#39064;&#65292;&#20854;&#20013;&#26399;&#26395;&#22870;&#21169;&#26159;Lipschitz&#30340;&#65292;&#22870;&#21169;&#35266;&#27979;&#32467;&#26524;&#20197;&#25209;&#37327;&#24418;&#24335;&#20256;&#36798;&#32473;&#29609;&#23478;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#31639;&#27861;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BLiN&#65288;Batched Lipschitz Narrowing&#65289;&#65292;&#21487;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;Lipschitz&#22870;&#21169;&#30340;&#24102;&#26377;$d_z$&#20493;&#32553;&#25918;&#32500;&#24230;&#30340;$T$&#27493;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#36798;&#21040;&#20102;&#26368;&#20248;&#65288;&#38500;&#23545;&#25968;&#22240;&#23376;&#22806;&#65289;&#30340;&#36951;&#25022;&#29575;$ \widetilde{\mathcal{O}}\left(T^{\frac{d_z+1}{d_z+2}}\right)$&#65292;&#20165;&#20351;&#29992;$ \mathcal{O} \left( \log\log T\right) $&#25209;&#27425;&#12290;&#25105;&#20204;&#36824;&#20026;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#19979;&#30028;&#24847;&#21619;&#30528;&#20219;&#20309;&#31639;&#27861;&#35201;&#36798;&#21040;&#26368;&#20248;&#36951;&#25022;&#65292;&#24517;&#39035;&#20351;&#29992;$\Omega(\log\log T)$&#25209;&#27425;&#12290;&#22240;&#27492;&#65292;BLiN&#20351;&#29992;&#26368;&#23567;&#30340;&#36890;&#20449;&#23454;&#29616;&#20102;&#26368;&#20248;&#36951;&#25022;&#29575;&#65288;&#38500;&#23545;&#25968;&#22240;&#23376;&#22806;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study Lipschitz bandit problems with batched feedback, where the expected reward is Lipschitz and the reward observations are communicated to the player in batches. We introduce a novel landscape-aware algorithm, called Batched Lipschitz Narrowing (BLiN), that optimally solves this problem. Specifically, we show that for a $T$-step problem with Lipschitz reward of zooming dimension $d_z$, our algorithm achieves theoretically optimal (up to logarithmic factors) regret rate $\widetilde{\mathcal{O}}\left(T^{\frac{d_z+1}{d_z+2}}\right)$ using only $ \mathcal{O} \left( \log\log T\right) $ batches. We also provide complexity analysis for this problem. Our theoretical lower bound implies that $\Omega(\log\log T)$ batches are necessary for any algorithm to achieve the optimal regret. Thus, BLiN achieves optimal regret rate (up to logarithmic factors) using minimal communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.03894</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification. (arXiv:2110.03894v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#37325;&#32534;&#31243;&#65288;AR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#65288;SCR&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;AR-SCR&#31995;&#32479;&#12290;AR&#36807;&#31243;&#26088;&#22312;&#20462;&#25913;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#22768;&#23398;&#20449;&#21495;&#65292;&#20197;&#37325;&#26032;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;SCR&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#37325;&#32534;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26631;&#31614;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;AR&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#26144;&#23556;&#25216;&#26415;&#26469;&#23545;&#40784;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#25216;&#26415;&#19982;&#21407;&#22987;AR&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20302;&#36164;&#28304;SCR&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;AR-SCR&#31995;&#32479;&#65292;&#21253;&#25324;&#38463;&#25289;&#20271;&#35821;&#12289;&#31435;&#38518;&#23451;&#35821;&#21644;&#35328;&#35821;&#38556;&#30861;&#24615;&#26222;&#36890;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;AM&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#30340;AR-SCR&#31995;&#32479;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#19988;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#25552;&#28860;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#24341;&#39046;&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#24212;&#29992;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2109.09658</link><description>&lt;p&gt;
FUTURE-AI:&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#20849;&#35782;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.09658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#25552;&#28860;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#24341;&#39046;&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#24212;&#29992;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#31995;&#32479;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#25512;&#21160;&#20102;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#25972;&#20010;&#20215;&#20540;&#38142;&#19978;&#30340;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#22270;&#20687;&#37325;&#24314;&#12289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#12290;&#23613;&#31649;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#35768;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#25285;&#24515;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#35748;&#20026;&#20854;&#22797;&#26434;&#12289;&#19981;&#36879;&#26126;&#12289;&#38590;&#20197;&#29702;&#35299;&#12289;&#38590;&#20197;&#24212;&#29992;&#21644;&#38590;&#20197;&#22312;&#20851;&#38190;&#20020;&#24202;&#24212;&#29992;&#20013;&#24314;&#31435;&#20449;&#20219;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25285;&#24551;&#21644;&#39118;&#38505;&#65292;&#20294;&#30446;&#21069;&#23578;&#27809;&#26377;&#20855;&#20307;&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#26469;&#24341;&#23548;&#26410;&#26469;&#21307;&#23398;&#24433;&#20687;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20197;&#22686;&#21152;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#37319;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#31215;&#32047;&#30340;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#31934;&#36873;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#30103;&#26723;&#26696;&#27169;&#22411;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#23558;&#24739;&#32773;&#30149;&#21490;&#34920;&#31034;&#20026;&#30142;&#30149;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#21442;&#25968;&#65292;&#21487;&#20197;&#25104;&#21151;&#23558;&#21307;&#30103;&#30693;&#35782;&#36716;&#31227;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#27169;&#22411;&#35757;&#32451;&#20110;&#36229;&#36807;&#19968;&#30334;&#19975;&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#22312;&#35786;&#26029;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#21478;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#20004;&#20010;&#22522;&#20110;&#35813;&#26723;&#26696;&#27169;&#22411;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#25581;&#31034;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20551;&#35774;&#30340;&#26032;&#39062;&#26041;&#27861;&#21644;&#20174;&#26723;&#26696;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24739;&#32773;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2107.03913</link><description>&lt;p&gt;
&#21307;&#30103;&#26723;&#26696;&#27169;&#22411;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#31185;&#23398;&#21644;&#23454;&#36341;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Medical Profile Model: Scientific and Practical Applications in Healthcare. (arXiv:2107.03913v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#30103;&#26723;&#26696;&#27169;&#22411;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#23558;&#24739;&#32773;&#30149;&#21490;&#34920;&#31034;&#20026;&#30142;&#30149;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#21442;&#25968;&#65292;&#21487;&#20197;&#25104;&#21151;&#23558;&#21307;&#30103;&#30693;&#35782;&#36716;&#31227;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#27169;&#22411;&#35757;&#32451;&#20110;&#36229;&#36807;&#19968;&#30334;&#19975;&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#22312;&#35786;&#26029;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#21478;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#20004;&#20010;&#22522;&#20110;&#35813;&#26723;&#26696;&#27169;&#22411;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#25581;&#31034;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20551;&#35774;&#30340;&#26032;&#39062;&#26041;&#27861;&#21644;&#20174;&#26723;&#26696;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24739;&#32773;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#24739;&#32773;&#30149;&#21490;&#34920;&#31034;&#20026;&#30142;&#30149;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23884;&#20837;&#31354;&#38388;&#36824;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#21442;&#25968;&#65292;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#24191;&#20041;&#30340;&#24739;&#32773;&#20010;&#20154;&#26723;&#26696;&#65292;&#24182;&#25104;&#21151;&#23558;&#21307;&#30103;&#30693;&#35782;&#36716;&#31227;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36825;&#31181;&#21307;&#30103;&#26723;&#26696;&#27169;&#22411;&#24050;&#22312;&#36229;&#36807;&#19968;&#30334;&#19975;&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#35814;&#32454;&#30340;&#27169;&#22411;&#20998;&#26512;&#21450;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#22312;&#35786;&#26029;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#24320;&#21457;&#30340;&#26723;&#26696;&#27169;&#22411;&#30340;&#20004;&#20010;&#24212;&#29992;&#65306;&#39318;&#20808;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Harbinger&#30142;&#30149;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#25581;&#31034;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20551;&#35774;&#65292;&#24182;&#22312;&#27969;&#34892;&#30149;&#23398;&#30740;&#31350;&#35774;&#35745;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30410;&#22788;&#12290;&#20854;&#27425;&#65292;&#20174;&#26723;&#26696;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#24739;&#32773;&#23884;&#20837;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The paper researches the problem of representation learning for electronic health records. We present the patient histories as temporal sequences of diseases for which embeddings are learned in an unsupervised setup with a transformer-based neural network model. Additionally the embedding space includes demographic parameters which allow the creation of generalized patient profiles and successful transfer of medical knowledge to other domains. The training of such a medical profile model has been performed on a dataset of more than one million patients. Detailed model analysis and its comparison with the state-of-the-art method show its clear advantage in the diagnosis prediction task. Further, we show two applications based on the developed profile model. First, a novel Harbinger Disease Discovery method allowing to reveal disease associated hypotheses and potentially are beneficial in the design of epidemiological studies. Second, the patient embeddings extracted from the profile mod
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;XAI&#21487;&#29992;&#20110;&#35777;&#26126;&#36741;&#21161;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2106.14186</link><description>&lt;p&gt;
&#19968;&#31181;&#24212;&#29992;XAI&#26041;&#27861;&#26816;&#27979;DCIS&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An XAI Approach to Deep Learning Models in the Detection of DCIS. (arXiv:2106.14186v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.14186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;XAI&#21487;&#29992;&#20110;&#35777;&#26126;&#36741;&#21161;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#30830;&#23454;&#21487;&#20197;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#23545;&#20020;&#24202;&#31038;&#21306;&#20869;&#36741;&#21161;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23454;&#26045;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The results showed that XAI could indeed be used as a proof of concept to begin discussions on the implementation of assistive AI systems within the clinical community.
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20219;&#21153;&#26159;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22312;&#36830;&#32493;&#20248;&#21270;&#20013;&#20351;&#29992;&#30340;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#21463;&#38480;&#20110;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#19982;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#19968;&#33268;&#30340;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2106.02835</link><description>&lt;p&gt;
&#35770;&#36830;&#32493;&#20248;&#21270;&#19979;&#22522;&#20110;&#29109;&#25439;&#22833;&#20989;&#25968;&#22312;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02835
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20219;&#21153;&#26159;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22312;&#36830;&#32493;&#20248;&#21270;&#20013;&#20351;&#29992;&#30340;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#21463;&#38480;&#20110;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#19982;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#19968;&#33268;&#30340;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;NOTEARS&#30340;&#38750;&#32452;&#21512;&#26377;&#21521;&#26080;&#29615;&#32422;&#26463;&#26041;&#27861;&#23558;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#30340;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#22312;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#19979;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#22914;&#26524;&#35813;&#20551;&#35774;&#19981;&#25104;&#31435;&#65292;&#23427;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#30340;&#36829;&#21453;&#23558;&#38459;&#30861;&#22240;&#26524;&#26041;&#21521;&#30340;&#35782;&#21035;&#65292;&#20351;&#22240;&#26524;&#26041;&#21521;&#23436;&#20840;&#30001;&#22240;&#26524;&#24378;&#24230;&#20197;&#21450;&#32447;&#24615;&#24773;&#20917;&#19979;&#22122;&#22768;&#26041;&#24046;&#21644;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#24378;&#38750;&#39640;&#26031;&#22122;&#22768;&#30830;&#23450;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#19979;&#65292;&#22312;&#29702;&#35770;&#19978;&#19982;&#20284;&#28982;&#20998;&#25968;&#19968;&#33268;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery from observational data is an important but challenging task in many scientific fields. Recently, a method with non-combinatorial directed acyclic constraint, called NOTEARS, formulates the causal structure learning problem as a continuous optimization problem using least-square loss. Though the least-square loss function is well justified under the standard Gaussian noise assumption, it is limited if the assumption does not hold. In this work, we theoretically show that the violation of the Gaussian noise assumption will hinder the causal direction identification, making the causal orientation fully determined by the causal strength as well as the variances of noises in the linear case and by the strong non-Gaussian noises in the nonlinear case. Consequently, we propose a more general entropy-based loss that is theoretically consistent with the likelihood score under any noise distribution. We run extensive empirical evaluations on both synthetic data and real-world d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19977;&#31181;&#30913;&#24615;&#34180;&#33180;&#30340;&#20648;&#23618;&#35745;&#31639;&#65292;&#35813;&#35745;&#31639;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#20869;&#37096;&#21160;&#21147;&#23398;&#23454;&#29616;&#38750;&#32447;&#24615;&#25237;&#24433;&#65292;&#25552;&#21319;&#20102;&#27169;&#24335;&#35782;&#21035;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.12700</link><description>&lt;p&gt;
&#30913;&#24615;&#34180;&#33180;&#30340;&#20648;&#23618;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing with Magnetic Thin Films. (arXiv:2101.12700v2 [cs.ET] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.12700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19977;&#31181;&#30913;&#24615;&#34180;&#33180;&#30340;&#20648;&#23618;&#35745;&#31639;&#65292;&#35813;&#35745;&#31639;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#20869;&#37096;&#21160;&#21147;&#23398;&#23454;&#29616;&#38750;&#32447;&#24615;&#25237;&#24433;&#65292;&#25552;&#21319;&#20102;&#27169;&#24335;&#35782;&#21035;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#26159;&#21463;&#21040;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#25216;&#26415;&#25512;&#21160;&#30340;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#30340;&#21151;&#33021;&#21644;&#33021;&#25928;&#36828;&#36828;&#19981;&#21450;&#29983;&#29289;&#31995;&#32479;&#12290;&#21463;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21551;&#21457;&#65292;&#26032;&#30340;&#38750;&#20256;&#32479;&#35745;&#31639;&#30828;&#20214;&#20986;&#29616;&#20102;&#65292;&#20855;&#26377;&#21033;&#29992;&#33258;&#28982;&#29616;&#35937;&#21644;&#25552;&#39640;&#25928;&#29575;&#30340;&#28508;&#21147;&#65292;&#31867;&#20284;&#20110;&#29983;&#29289;&#31995;&#32479;&#12290;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#36890;&#36807;&#21508;&#31181;&#38750;&#20256;&#32479;&#31995;&#32479;&#65288;&#21253;&#25324;&#20809;&#23398;&#21644;&#35760;&#24518;&#30005;&#38459;&#31995;&#32479;&#65289;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;&#20648;&#23618;&#35745;&#31639;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#20869;&#37096;&#21160;&#21147;&#23398;&#23558;&#20219;&#21153;&#36755;&#20837;&#36827;&#34892;&#38750;&#32447;&#24615;&#25237;&#24433;&#21040;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#36755;&#20986;&#23618;&#32467;&#21512;&#36825;&#20123;&#29305;&#24449;&#26469;&#25191;&#34892;&#35782;&#21035;&#27169;&#24335;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#36827;&#34892;&#22806;&#37096;&#20449;&#21495;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in artificial intelligence are driven by technologies inspired by the brain, but these technologies are orders of magnitude less powerful and energy efficient than biological systems. Inspired by the nonlinear dynamics of neural networks, new unconventional computing hardware has emerged with the potential to exploit natural phenomena and gain efficiency, in a similar manner to biological systems. Physical reservoir computing demonstrates this with a variety of unconventional systems, from optical-based to memristive systems. Reservoir computers provide a nonlinear projection of the task input into a high-dimensional feature space by exploiting the system's internal dynamics. A trained readout layer then combines features to perform tasks, such as pattern recognition and time-series analysis. Despite progress, achieving state-of-the-art performance without external signal processing to the reservoir remains challenging. Here we perform an initial exploration of three magnetic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CcGAN&#65289;&#65292;&#39318;&#20010;&#29992;&#20110;&#22522;&#20110;&#36830;&#32493;&#26631;&#37327;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#32463;&#39564;cGAN&#25439;&#22833;&#21644;&#25552;&#20986;&#26032;&#30340;&#26631;&#31614;&#36755;&#20837;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22238;&#24402;&#26631;&#31614;&#26465;&#20214;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.07466</link><description>&lt;p&gt;
&#36830;&#32493;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65306;&#21019;&#26032;&#30340;&#32463;&#39564;&#25439;&#22833;&#21644;&#26631;&#31614;&#36755;&#20837;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous Conditional Generative Adversarial Networks: Novel Empirical Losses and Label Input Mechanisms. (arXiv:2011.07466v9 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CcGAN&#65289;&#65292;&#39318;&#20010;&#29992;&#20110;&#22522;&#20110;&#36830;&#32493;&#26631;&#37327;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#32463;&#39564;cGAN&#25439;&#22833;&#21644;&#25552;&#20986;&#26032;&#30340;&#26631;&#31614;&#36755;&#20837;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22238;&#24402;&#26631;&#31614;&#26465;&#20214;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CcGAN&#65289;&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#22522;&#20110;&#36830;&#32493;&#26631;&#37327;&#26465;&#20214;&#65288;&#31216;&#20026;&#22238;&#24402;&#26631;&#31614;&#65289;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#26465;&#20214;GAN&#65288;cGAN&#65289;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#20998;&#31867;&#26465;&#20214;&#65288;&#20363;&#22914;&#31867;&#26631;&#31614;&#65289;&#65307;&#23545;&#20110;&#22238;&#24402;&#26631;&#31614;&#30340;&#26465;&#20214;&#29983;&#25104;&#21017;&#22312;&#25968;&#23398;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#24341;&#21457;&#20102;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#65288;P1&#65289;&#30001;&#20110;&#26576;&#20123;&#22238;&#24402;&#26631;&#31614;&#21487;&#33021;&#27809;&#26377;&#30495;&#23454;&#22270;&#20687;&#65292;&#26368;&#23567;&#21270;&#29616;&#26377;&#30340;&#32463;&#39564;cGAN&#25439;&#22833;&#65288;&#20063;&#31216;&#20026;&#32463;&#39564;cGAN&#25439;&#22833;&#65289;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#65307;&#65288;P2&#65289;&#30001;&#20110;&#22238;&#24402;&#26631;&#31614;&#26159;&#36830;&#32493;&#30340;&#19988;&#26080;&#38480;&#22810;&#65292;&#20256;&#32479;&#30340;&#26631;&#31614;&#36755;&#20837;&#26041;&#27861;&#19981;&#36866;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;CcGAN&#36890;&#36807;&#20998;&#21035;&#65288;S1&#65289;&#37325;&#26032;&#26500;&#24314;&#29616;&#26377;&#30340;&#32463;&#39564;cGAN&#25439;&#22833;&#20197;&#36866;&#24212;&#36830;&#32493;&#22330;&#26223;&#65307;&#20197;&#21450;&#65288;S2&#65289;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26631;&#31614;&#36755;&#20837;&#65288;NLI&#65289;&#26041;&#27861;&#21644;&#19968;&#31181;&#25913;&#36827;&#30340;&#26631;&#31614;&#36755;&#20837;&#65288;ILI&#65289;&#26041;&#27861;&#23558;&#22238;&#24402;&#26631;&#31614;&#34701;&#20837;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (eg, class labels); conditioning on regression labels is mathematically distinct and raises two fundamental problems:(P1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (aka empirical cGAN losses) often fails in practice;(P2) Since regression labels are scalar and infinitely many, conventional label input methods are not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a naive label input (NLI) method and an improved label input (ILI) method to incorporate regression labels into
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23398;&#20064;&#25351;&#23548;&#33258;&#19979;&#32780;&#19978;&#25628;&#32034;&#30340;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#20013;&#38388;&#20540;&#30340;&#32452;&#21512;&#65292;&#21033;&#29992;&#20013;&#38388;&#31243;&#24207;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#20855;&#20307;&#20540;&#30340;&#29305;&#24615;&#36827;&#34892;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2007.14381</link><description>&lt;p&gt;
BUSTLE: &#36890;&#36807;&#23398;&#20064;&#25351;&#23548;&#30340;&#33258;&#19979;&#32780;&#19978;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration. (arXiv:2007.14381v3 [cs.PL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.14381
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23398;&#20064;&#25351;&#23548;&#33258;&#19979;&#32780;&#19978;&#25628;&#32034;&#30340;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#20013;&#38388;&#20540;&#30340;&#32452;&#21512;&#65292;&#21033;&#29992;&#20013;&#38388;&#31243;&#24207;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#20855;&#20307;&#20540;&#30340;&#29305;&#24615;&#36827;&#34892;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21512;&#25104;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#24222;&#22823;&#30340;&#31243;&#24207;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#30340;&#22256;&#38590;&#12290;&#20154;&#31867;&#31243;&#24207;&#21592;&#36890;&#24120;&#36890;&#36807;&#32534;&#20889;&#23376;&#31243;&#24207;&#24182;&#20998;&#26512;&#20854;&#20013;&#38388;&#32467;&#26524;&#26469;&#20197;&#36866;&#24403;&#30340;&#26041;&#24335;&#32452;&#21512;&#23427;&#20204;&#26469;&#35299;&#20915;&#32534;&#20889;&#22797;&#26434;&#31243;&#24207;&#30340;&#20219;&#21153;&#12290;&#22312;&#27492;&#30452;&#35273;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#26469;&#24341;&#23548;&#33258;&#19979;&#32780;&#19978;&#30340;&#31243;&#24207;&#25628;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#26102;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32771;&#34385;&#20013;&#38388;&#20540;&#30340;&#32452;&#21512;&#26469;&#25351;&#23548;&#25628;&#32034;&#12290;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#32452;&#21512;&#65292;&#22240;&#20026;&#26377;&#20960;&#20010; emergent properties&#12290;&#39318;&#20808;&#65292;&#22312;&#33258;&#19979;&#32780;&#19978;&#30340;&#25628;&#32034;&#20013;&#65292;&#21487;&#20197;&#25191;&#34892;&#20013;&#38388;&#31243;&#24207;&#65292;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#37492;&#20110;&#36825;&#20123;&#25191;&#34892;&#30340;&#20855;&#20307;&#20540;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#23646;&#24615;&#31614;&#21517;&#30340;&#24037;&#20316;&#30340;&#20016;&#23500;&#29305;&#24615;&#12290;&#26368;&#21518;&#65292;&#33258;&#19979;&#32780;&#19978;&#30340;&#25628;&#32034;&#20351;&#31995;&#32479;&#22312;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#39034;&#24207;&#19978;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#22870;&#21169;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#24207;&#21015;&#38271;&#24230;&#20559;&#24046;&#21644;&#26463;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#20851;&#38190;&#35789;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1909.09485</link><description>&lt;p&gt;
BSDAR: &#22522;&#20110;&#27880;&#24847;&#21147;&#22870;&#21169;&#30340;&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
BSDAR: Beam Search Decoding with Attention Reward in Neural Keyphrase Generation. (arXiv:1909.09485v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1909.09485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#22870;&#21169;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#24207;&#21015;&#38271;&#24230;&#20559;&#24046;&#21644;&#26463;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#20851;&#38190;&#35789;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#20004;&#20010;&#24120;&#35265;&#35299;&#30721;&#38382;&#39064;&#65306;&#24207;&#21015;&#38271;&#24230;&#20559;&#24046;&#21644;&#26463;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#32423;&#21644;ngram&#32423;&#22870;&#21169;&#20989;&#25968;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;&#31574;&#30053;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#32422;&#26463;&#21644;&#20248;&#21270;Seq2Seq&#25512;&#29702;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#31616;&#21333;&#30340;&#25552;&#26696;&#21487;&#20197;&#20811;&#26381;&#31639;&#27861;&#23545;&#36739;&#30701;&#21644;&#20960;&#20046;&#30456;&#21516;&#30340;&#24207;&#21015;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#28304;&#25991;&#26412;&#20013;&#23384;&#22312;&#21644;&#19981;&#23384;&#22312;&#30340;&#20851;&#38190;&#35789;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study mainly investigates two common decoding problems in neural keyphrase generation: sequence length bias and beam diversity. To tackle the problems, we introduce a beam search decoding strategy based on word-level and ngram-level reward function to constrain and refine Seq2Seq inference at test time. Results show that our simple proposal can overcome the algorithm bias to shorter and nearly identical sequences, resulting in a significant improvement of the decoding performance on generating keyphrases that are present and absent in source text.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;18&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23545;&#21508;&#31181;&#26410;&#39044;&#26009;&#21040;&#30340;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#30340;&#26032;&#22522;&#20934;&#12290;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#31574;&#30053;&#65292;&#21487;&#20197;&#24110;&#21161;&#20811;&#26381;&#35757;&#32451;&#26399;&#38388;&#26410;&#32771;&#34385;&#21040;&#30340;&#23545;&#25163;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#23558;&#20026;&#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#65292;&#20419;&#36827;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/1908.08016</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#39044;&#26009;&#21040;&#30340;&#23545;&#25163;&#27979;&#35797;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Testing Robustness Against Unforeseen Adversaries. (arXiv:1908.08016v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1908.08016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;18&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23545;&#21508;&#31181;&#26410;&#39044;&#26009;&#21040;&#30340;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#30340;&#26032;&#22522;&#20934;&#12290;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#31574;&#30053;&#65292;&#21487;&#20197;&#24110;&#21161;&#20811;&#26381;&#35757;&#32451;&#26399;&#38388;&#26410;&#32771;&#34385;&#21040;&#30340;&#23545;&#25163;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#23558;&#20026;&#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#65292;&#20419;&#36827;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#29616;&#23454;&#19990;&#30028;&#30340;&#23545;&#25239;&#29615;&#22659;&#26102;&#65292;&#38450;&#24481;&#32773;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#22826;&#21487;&#33021;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#25163;&#24456;&#21487;&#33021;&#20351;&#29992;&#36924;&#30495;&#30340;&#23545;&#25239;&#25197;&#26354;&#65292;&#32780;&#19981;&#38480;&#20110;&#23567;&#30340;L_p&#32422;&#26463;&#25200;&#21160;&#12290;&#20026;&#20102;&#32553;&#23567;&#30740;&#31350;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;18&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21019;&#24314;&#20102;ImageNet-UA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#21508;&#31181;&#26410;&#39044;&#26009;&#21040;&#30340;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#30340;&#26032;&#22522;&#20934;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#26469;&#35782;&#21035;&#19968;&#31995;&#21015;&#33021;&#22815;&#24110;&#21161;&#20811;&#26381;&#36825;&#31181;&#27867;&#21270;&#24046;&#36317;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#21457;&#29616;&#20102;&#21487;&#20197;&#25552;&#39640;&#23545;&#26410;&#39044;&#26009;&#21040;&#30340;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#25216;&#26415;&#30340;&#20016;&#23500;&#31354;&#38388;&#12290;&#25105;&#20204;&#24076;&#26395;ImageNet-UA&#30340;&#26356;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24615;&#23558;&#25104;&#20026;&#37027;&#20123;&#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#26368;&#22351;&#24773;&#20917;&#30340;&#40065;&#26834;&#24615;&#30340;&#20154;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#20419;&#36827;&#24320;&#21457;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#30475;&#19981;&#21040;&#30340;&#25915;&#20987;&#20013;&#36827;&#34892;&#27867;&#21270;&#30340;&#26356;&#24378;&#22823;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
When considering real-world adversarial settings, defenders are unlikely to have access to the full range of deployment-time adversaries during training, and adversaries are likely to use realistic adversarial distortions that will not be limited to small L_p-constrained perturbations. To narrow in on this discrepancy between research and reality we introduce eighteen novel adversarial attacks, which we use to create ImageNet-UA, a new benchmark for evaluating model robustness against a wide range of unforeseen adversaries. We make use of our benchmark to identify a range of defense strategies which can help overcome this generalization gap, finding a rich space of techniques which can improve unforeseen robustness. We hope the greater variety and realism of ImageNet-UA will make it a useful tool for those working on real-world worst-case robustness, enabling development of more robust defenses which can generalize beyond attacks seen during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#25197;&#26354;&#27010;&#29575;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;UCB&#31639;&#27861;&#20026;&#22522;&#30784;&#12289;&#32771;&#34385;&#20102;&#22870;&#21169;&#25197;&#26354;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/1611.10283</link><description>&lt;p&gt;
&#21152;&#26435;&#36172;&#21338;&#26426;&#25110;&#32773;&#65306;&#36172;&#21338;&#26426;&#22914;&#20309;&#23398;&#20064;&#39044;&#26399;&#20043;&#22806;&#30340;&#25197;&#26354;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Weighted bandits or: How bandits learn distorted values that are not expected. (arXiv:1611.10283v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1611.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#25197;&#26354;&#27010;&#29575;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;UCB&#31639;&#27861;&#20026;&#22522;&#30784;&#12289;&#32771;&#34385;&#20102;&#22870;&#21169;&#25197;&#26354;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#29992;&#20110;&#35299;&#37322;&#24120;&#35265;&#20559;&#31163;&#20256;&#32479;&#39044;&#26399;&#20215;&#20540;&#20559;&#22909;&#30340;&#20154;&#31867;&#20915;&#31574;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24102;&#26377;&#25197;&#26354;&#27010;&#29575;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65306;&#32463;&#20856;&#30340;K&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#21442;&#25968;&#21270;&#36172;&#21338;&#26426;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#23545;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;&#23545;&#20110;K&#33218;&#36172;&#21338;&#26426;&#20197;&#21450;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#21040;&#19978;&#32622;&#20449;&#30028;(UCB)&#31639;&#27861;&#21551;&#21457;&#12289;&#21253;&#21547;&#22870;&#21169;&#25197;&#26354;&#24182;&#19988;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;K&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#39044;&#26399;&#21518;&#24724;&#30340;&#19978;&#30028;&#65292;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#31639;&#27861;&#30340;&#27425;&#32447;&#24615;&#20248;&#21270;&#39034;&#24207;&#12290;&#23545;&#20110;&#32447;&#24615;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#21518;&#24724;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#26159;&#27425;&#32447;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the reward distributions: the classic $K$-armed bandit and the linearly parameterized bandit settings. We consider the aforementioned problems in the regret minimization as well as best arm identification framework for multi-armed bandits. For the regret minimization setting in $K$-armed as well as linear bandit problems, we propose algorithms that are inspired by Upper Confidence Bound (UCB) algorithms, incorporate reward distortions, and exhibit sublinear regret. For the $K$-armed bandit setting, we derive an upper bound on the expected regret for our proposed algorithm, and then we prove a matching lower bound to establish the order-optimality of our algorithm. For the linearly parameterized setting, our algorithm achieves a regret upper bound that is of the 
&lt;/p&gt;</description></item></channel></rss>