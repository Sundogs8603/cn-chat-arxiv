<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10429</link><description>&lt;p&gt;
DoReMi: &#20248;&#21270;&#25968;&#25454;&#28151;&#21512;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10429
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#28151;&#21512;&#27604;&#20363;&#65288;&#20363;&#22914;&#65292;&#32500;&#22522;&#30334;&#31185;&#12289;&#22270;&#20070;&#12289;&#32593;&#39029;&#25991;&#26412;&#65289;&#26497;&#22823;&#22320;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoReMi&#30340;Domain Reweighting with Minimax Optimization&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;Group DRO&#65289;&#35757;&#32451;&#19968;&#20010;&#23567;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65288;&#28151;&#21512;&#27604;&#20363;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#22495;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#30340;&#65292;&#20840;&#23610;&#23544;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DoReMi&#22312;&#19968;&#20010;280M&#21442;&#25968;&#30340;&#20195;&#29702;&#27169;&#22411;&#19978;&#65292;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#35757;&#32451;&#19968;&#20010;8B&#21442;&#25968;&#27169;&#22411;&#65288;30&#20493;&#22823;&#65289;&#30340;&#22495;&#26435;&#37325;&#12290;&#22312;The Pile&#19978;&#65292;&#21363;&#20351;&#22312;&#20943;&#23567;&#19968;&#20123;&#22495;&#30340;&#27604;&#37325;&#26102;&#65292;DoReMi&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#22495;&#30340;perplexity&#12290;&#30456;&#27604;&#20351;&#29992;The Pile&#30340;&#40664;&#35748;&#22495;&#26435;&#37325;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;DoReMi&#23558;&#24179;&#22343;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;6.5%&#65292;&#24182;&#20351;&#29992;2.6&#20493;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;&#22312;GLaM&#25968;&#25454;&#38598;&#19978;&#65292;DoReMi&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;4.7%&#65288;&#27425;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65289;&#30340;few-shot&#20934;&#30830;&#24230;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#19979;&#25552;&#39640;&#20102;9.0%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25512;&#26029;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10427</link><description>&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#21152;&#36895;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10427
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25512;&#26029;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#38480;&#21046;&#20102;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25928;&#29575;&#12290;&#31038;&#21306;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#24456;&#26114;&#36149;&#24182;&#19988;&#38656;&#35201;&#25913;&#21464;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#25512;&#23548;&#20986;&#35299;&#30721;&#36895;&#24230;&#21644;&#32763;&#35793;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#20174;&#35299;&#30721;&#31639;&#27861;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#26631;&#20934;&#30340;&#36138;&#24515;&#33258;&#22238;&#24402;&#35299;&#30721;&#36716;&#21270;&#20026;&#24182;&#34892;&#35299;&#30721;&#65292;&#24182;&#21033;&#29992;&#38597;&#20811;&#27604;&#21644;&#39640;&#26031;-&#22622;&#24503;&#23572;&#36845;&#20195;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#24182;&#34892;&#21270;&#35299;&#30721;&#30456;&#23545;&#20110;&#26631;&#20934;&#33258;&#22238;&#24402;&#35299;&#30721;&#21487;&#25552;&#39640;&#36798;38&#65285;&#30340;&#36895;&#24230;&#65292;&#24403;&#25193;&#23637;&#27169;&#22411;&#26102;&#65292;&#36895;&#24230;&#20960;&#20046;&#25552;&#39640;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling t
&lt;/p&gt;</description></item><item><title>ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10424</link><description>&lt;p&gt;
ZeroFlow: &#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#24555;&#36895;&#38646;&#26631;&#31614;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10424
&lt;/p&gt;
&lt;p&gt;
ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#20272;&#35745;&#26159;&#25551;&#36848;&#36830;&#32493;&#28857;&#20113;&#20043;&#38388;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#30340;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#38656;&#35201;&#25968;&#21313;&#31186;&#30340;&#26102;&#38388;&#65292;&#20351;&#20854;&#26080;&#27861;&#20316;&#20026;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65289;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20803;&#20351;&#29992;&#12290;&#21069;&#21521;&#20256;&#36882;&#26041;&#27861;&#30456;&#23545;&#24555;&#36895;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#30340;&#36816;&#34892;&#26102;&#38388;&#22312;&#25968;&#21313;&#33267;&#25968;&#30334;&#27627;&#31186;&#20043;&#38388;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#26694;&#26550; Scene Flow via Distillation&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340; ZeroFlow&#65292;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#65292;&#22312;&#22823;&#35268;&#27169;&#28857;&#20113;&#19978;&#23454;&#26102;&#29983;&#25104;&#22330;&#26223;&#27969;&#20272;&#35745;&#32467;&#26524;&#65292;&#21516;&#26102;&#36136;&#37327;&#31454;&#20105;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102; ZeroFlow
&lt;/p&gt;
&lt;p&gt;
Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#30340;Tsukamoto&#31070;&#32463;&#27169;&#31946;&#27169;&#22411;&#65292;&#22312;&#20845;&#20010;&#20174;&#33016;&#36879;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#32441;&#29702;&#29305;&#24449;&#30340;&#24110;&#21161;&#19979;&#65292;&#23558;Covid-19&#30142;&#30149;&#20174;&#27491;&#24120;&#21644;&#32954;&#28814;&#30149;&#20363;&#20013;&#21306;&#20998;&#20986;&#26469;&#65292;&#24182;&#23454;&#39564;&#34920;&#26126;&#20854;&#26816;&#27979;&#31934;&#24230;&#39640;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10421</link><description>&lt;p&gt;
&#22522;&#20110;&#28436;&#21270;&#30340;Tsukamoto&#31070;&#32463;&#27169;&#31946;&#27169;&#22411;&#22312;&#22810;&#31867;&#21035;Covid 19&#33016;&#36879;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evolving Tsukamoto Neuro Fuzzy Model for Multiclass Covid 19 Classification with Chest X Ray Images. (arXiv:2305.10421v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#30340;Tsukamoto&#31070;&#32463;&#27169;&#31946;&#27169;&#22411;&#65292;&#22312;&#20845;&#20010;&#20174;&#33016;&#36879;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#32441;&#29702;&#29305;&#24449;&#30340;&#24110;&#21161;&#19979;&#65292;&#23558;Covid-19&#30142;&#30149;&#20174;&#27491;&#24120;&#21644;&#32954;&#28814;&#30149;&#20363;&#20013;&#21306;&#20998;&#20986;&#26469;&#65292;&#24182;&#23454;&#39564;&#34920;&#26126;&#20854;&#26816;&#27979;&#31934;&#24230;&#39640;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#21475;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#36816;&#29992;&#20154;&#24037;&#26234;&#33021;&#20316;&#20986;&#24555;&#36895;&#20915;&#31574;&#30340;&#38656;&#27714;&#65292;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30149;&#30151;&#26816;&#27979;&#27169;&#22411;&#21644;&#24322;&#24120;&#35782;&#21035;&#31995;&#32479;&#24050;&#32463;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#21307;&#30103;&#35786;&#26029;&#27700;&#24179;&#12290;&#30001;&#20110;Covid-19&#24050;&#25104;&#20026;&#19990;&#30028;&#19978;&#26368;&#20005;&#37325;&#30340;&#30142;&#30149;&#20043;&#19968;&#65292;&#24320;&#21457;&#33258;&#21160;&#30340;Covid-19&#26816;&#27979;&#26694;&#26550;&#26377;&#21161;&#20110;&#21307;&#29983;&#35786;&#26029;&#30142;&#30149;&#24182;&#24555;&#36895;&#25552;&#20379;&#27491;&#30830;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Covid 19&#26816;&#27979;&#26694;&#26550;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;Tsukamoto&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#21306;&#20998;Covid 19&#30142;&#30149;&#21644;&#27491;&#24120;&#21450;&#32954;&#28814;&#30149;&#20363;&#12290;&#19982;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#26368;&#23567;&#20108;&#20056;&#27861;&#35843;&#25972;&#31070;&#32463;&#27169;&#31946;&#27169;&#22411;&#30340;&#21442;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#28436;&#21270;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#29483;&#32676;&#31639;&#27861;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#20845;&#20010;&#20174;&#33016;&#36879;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#32441;&#29702;&#29305;&#24449;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#30142;&#30149;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;&#31934;&#24230;&#19978;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Du e to rapid population growth and the need to use artificial intelligence to make quick decisions, developing a machine learning-based disease detection model and abnormality identification system has greatly improved the level of medical diagnosis Since COVID-19 has become one of the most severe diseases in the world, developing an automatic COVID-19 detection framework helps medical doctors in the diagnostic process of disease and provides correct and fast results. In this paper, we propose a machine lear ning based framework for the detection of Covid 19. The proposed model employs a Tsukamoto Neuro Fuzzy Inference network to identify and distinguish Covid 19 disease from normal and pneumonia cases. While the traditional training methods tune the parameters of the neuro-fuzzy model by gradient-based algorithms and recursive least square method, we use an evolutionary-based optimization, the Cat swarm algorithm to update the parameters. In addition, six texture features extracted f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19981;&#21516;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#23450;&#20041;&#21644;&#38543;&#26426;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Lasso&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10413</link><description>&lt;p&gt;
&#20351;&#29992;Lasso&#30340;&#31614;&#21517;&#19968;&#33268;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Consistency of Signatures Using Lasso. (arXiv:2305.10413v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19981;&#21516;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#23450;&#20041;&#21644;&#38543;&#26426;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Lasso&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31614;&#21517;&#21464;&#25442;&#26159;&#36830;&#32493;&#21644;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36845;&#20195;&#36335;&#24452;&#31215;&#20998;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#38750;&#32447;&#24615;&#36890;&#36807;&#32447;&#24615;&#21270;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#26356;&#25509;&#36817;&#24067;&#26391;&#36816;&#21160;&#25110;&#20855;&#26377;&#36739;&#24369;&#36328;&#32500;&#24230;&#30456;&#20851;&#24615;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#31614;&#21517;&#23450;&#20041;&#20026;It\^o&#31215;&#20998;&#30340;Lasso&#22238;&#24402;&#26356;&#20855;&#19968;&#33268;&#24615;&#65307;&#23545;&#20110;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#31614;&#21517;&#23450;&#20041;&#20026;Stratonovich&#31215;&#20998;&#22312;Lasso&#22238;&#24402;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#32479;&#35745;&#25512;&#26029;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#21644;&#38543;&#26426;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signature transforms are iterated path integrals of continuous and discrete-time time series data, and their universal nonlinearity linearizes the problem of feature selection. This paper revisits the consistency issue of Lasso regression for the signature transform, both theoretically and numerically. Our study shows that, for processes and time series that are closer to Brownian motion or random walk with weaker inter-dimensional correlations, the Lasso regression is more consistent for their signatures defined by It\^o integrals; for mean reverting processes and time series, their signatures defined by Stratonovich integrals have more consistency in the Lasso regression. Our findings highlight the importance of choosing appropriate definitions of signatures and stochastic models in statistical inference and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Wasserstein&#26799;&#24230;&#27969;&#26469;&#20248;&#21270;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;$L^2$-Wasserstein&#36317;&#31163;&#26469;&#32422;&#26463;&#31574;&#30053;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#20248;&#21270;&#30340;&#31283;&#23450;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10411</link><description>&lt;p&gt;
Wasserstein&#26799;&#24230;&#27969;&#29992;&#20110;&#20248;&#21270;&#39640;&#26031;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies. (arXiv:2305.10411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Wasserstein&#26799;&#24230;&#27969;&#26469;&#20248;&#21270;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;$L^2$-Wasserstein&#36317;&#31163;&#26469;&#32422;&#26463;&#31574;&#30053;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#20248;&#21270;&#30340;&#31283;&#23450;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#26102;&#36890;&#24120;&#20381;&#36182;&#20110;&#20197;&#21069;&#23398;&#20064;&#21040;&#30340;&#36816;&#21160;&#31574;&#30053;&#24211;&#12290;&#24403;&#38754;&#20020;&#26410;&#30693;&#20219;&#21153;&#26465;&#20214;&#25110;&#20986;&#29616;&#26032;&#20219;&#21153;&#35201;&#27714;&#26102;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#30456;&#24212;&#22320;&#35843;&#25972;&#23427;&#20204;&#30340;&#36816;&#21160;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31574;&#30053;&#20248;&#21270;&#26159;&#23558;&#26426;&#22120;&#20154;&#31574;&#30053;&#20316;&#20026;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#30340;&#20989;&#25968;&#36866;&#24212;&#30340;&#8220;&#20107;&#23454;&#19978;&#8221;&#30340;&#33539;&#20363;&#12290;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#36816;&#21160;&#31574;&#30053;&#20855;&#26377;&#29305;&#23450;&#30340;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#32463;&#24120;&#34987;&#24573;&#30053;&#22312;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#31574;&#30053;&#30340;&#32467;&#26500;&#65292;&#23558;&#31574;&#30053;&#20248;&#21270;&#20316;&#20026;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#36827;&#34892;&#25237;&#24433;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;(GMMs)&#65292;&#24182;&#23558;&#31574;&#30053;&#20248;&#21270;&#26500;&#25104;GMMs&#31354;&#38388;&#19978;&#30340;Wassertein&#26799;&#24230;&#27969;&#12290;&#36825;&#33258;&#28982;&#22320;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;GMMs&#20043;&#38388;&#30340;$L^2$-Wasserstein&#36317;&#31163;&#32422;&#26463;&#31574;&#30053;&#26356;&#26032;&#65292;&#20197;&#22686;&#24378;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#30340;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#23548;&#26799;&#24230;&#26356;&#26032;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#36817;&#31471;&#28857;&#31639;&#27861;&#65292;&#20801;&#35768;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#37327;&#30340;&#28151;&#21512;&#25104;&#20998;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#25913;&#36827;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#20943;&#23569;&#20102;&#21487;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities. When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the \emph{de facto} paradigm to adapt robot policies as a function of task-specific objectives. Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms. We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wassertein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the $L^2$-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#28508;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#30340;&#21453;&#38382;&#39064;&#12290;&#23454;&#29616;&#20102;&#37325;&#24314;&#29702;&#35770;&#21160;&#21147;&#23398;&#37327;&#30340;&#20840;&#23616;&#20998;&#24067;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#23398;&#21518;&#39564;&#20998;&#24067;&#36981;&#23432;&#24050;&#30693;&#29289;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.10399</link><description>&lt;p&gt;
&#39640;&#33021;&#29289;&#29702;&#21453;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#28508;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics. (arXiv:2305.10399v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#28508;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#30340;&#21453;&#38382;&#39064;&#12290;&#23454;&#29616;&#20102;&#37325;&#24314;&#29702;&#35770;&#21160;&#21147;&#23398;&#37327;&#30340;&#20840;&#23616;&#20998;&#24067;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#23398;&#21518;&#39564;&#20998;&#24067;&#36981;&#23432;&#24050;&#30693;&#29289;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#65288;LHC&#65289;&#19978;&#30340;&#39640;&#33021;&#30896;&#25758;&#25552;&#20379;&#20102;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#24453;&#35299;&#20915;&#38382;&#39064;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#37327;&#32467;&#26524;&#33021;&#22815;&#19982;&#29305;&#23450;&#29702;&#35770;&#39044;&#27979;&#25110;&#26469;&#33258;&#20854;&#20182;&#25506;&#27979;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20043;&#21069;&#65292;&#24517;&#39035;&#32416;&#27491;&#25506;&#27979;&#22120;&#30340;&#24433;&#21709;&#12290;&#35299;&#20915;&#23558;&#25506;&#27979;&#22120;&#35266;&#27979;&#26144;&#23556;&#21040;&#22522;&#30784;&#30896;&#25758;&#30340;&#29702;&#35770;&#37327;&#30340;&#21453;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;LHC&#35768;&#22810;&#29289;&#29702;&#20998;&#26512;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#36817;&#20284;&#21453;&#26144;&#26144;&#23556;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#26550;&#26500;&#65292;&#31216;&#20026;&#28508;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#26368;&#26032;&#30340;&#29983;&#25104;&#33402;&#26415;&#26041;&#27861;&#30340;&#28508;&#22312;&#23398;&#20064;&#19982;&#31471;&#21040;&#31471;&#21464;&#20998;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#29992;&#20110;&#37325;&#24314;&#29702;&#35770;&#21160;&#21147;&#23398;&#37327;&#30340;&#20840;&#23616;&#20998;&#24067;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#30830;&#20445;&#25152;&#23398;&#21518;&#39564;&#20998;&#24067;&#36981;&#23432;&#24050;&#30693;&#29289;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-energy collisions at the Large Hadron Collider (LHC) provide valuable insights into open questions in particle physics. However, detector effects must be corrected before measurements can be compared to certain theoretical predictions or measurements from other detectors. Methods to solve this \textit{inverse problem} of mapping detector observations to theoretical quantities of the underlying collision are essential parts of many physics analyses at the LHC. We investigate and compare various generative deep learning methods to approximate this inverse mapping. We introduce a novel unified architecture, termed latent variation diffusion models, which combines the latent learning of cutting-edge generative art approaches with an end-to-end variational framework. We demonstrate the effectiveness of this approach for reconstructing global distributions of theoretical kinematic quantities, as well as for ensuring the adherence of the learned posterior distributions to known physics c
&lt;/p&gt;</description></item><item><title>RelationMatch&#26159;&#19968;&#31181;&#21033;&#29992;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21305;&#37197;&#25209;&#20869;&#20851;&#31995;&#65292;&#26377;&#25928;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10397</link><description>&lt;p&gt;
RelationMatch&#65306;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#25209;&#20869;&#20851;&#31995;&#21305;&#37197;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
RelationMatch: Matching In-batch Relationships for Semi-supervised Learning. (arXiv:2305.10397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10397
&lt;/p&gt;
&lt;p&gt;
RelationMatch&#26159;&#19968;&#31181;&#21033;&#29992;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21305;&#37197;&#25209;&#20869;&#20851;&#31995;&#65292;&#26377;&#25928;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#38598;&#20013;&#22312;&#26469;&#33258;&#30456;&#21516;&#26469;&#28304;&#30340;&#25104;&#23545;&#25968;&#25454;&#28857;&#30340;&#39044;&#27979;&#23545;&#20934;&#19978;&#65292;&#24182;&#24573;&#30053;&#20102;&#27599;&#20010;&#25209;&#27425;&#20869;&#30340;&#28857;&#38388;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;RelationMatch&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#26469;&#21457;&#25496;&#25209;&#20869;&#20851;&#31995;&#12290;&#36890;&#36807;&#24212;&#29992;MCE&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;FixMatch&#21644;FlexMatch&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20165;&#20351;&#29992;40&#20010;&#26631;&#31614;&#30340;STL-10&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30456;&#23545;&#20110;FlexMatch&#26377;15.21&#65285;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;MCE&#24212;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning has achieved notable success by leveraging very few labeled data and exploiting the wealth of information derived from unlabeled data. However, existing algorithms usually focus on aligning predictions on paired data points augmented from an identical source, and overlook the inter-point relationships within each batch. This paper introduces a novel method, RelationMatch, which exploits in-batch relationships with a matrix cross-entropy (MCE) loss function. Through the application of MCE, our proposed method consistently surpasses the performance of established state-of-the-art methods, such as FixMatch and FlexMatch, across a variety of vision datasets. Notably, we observed a substantial enhancement of 15.21% in accuracy over FlexMatch on the STL-10 dataset using only 40 labels. Moreover, we apply MCE to supervised learning scenarios, and observe consistent improvements as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#31232;&#30095;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26159;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2305.10391</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#30340;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#31232;&#30095;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26159;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29305;&#24449;&#35013;&#39280;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#22312;&#31232;&#30095;&#35774;&#32622;&#19979;&#65292;&#21363;&#33410;&#28857;&#30340;&#39044;&#26399;&#24230;&#25968;&#20026;&#33410;&#28857;&#25968;&#30340;O(1)&#26102;&#12290;&#36825;&#26679;&#30340;&#22270;&#36890;&#24120;&#34987;&#31216;&#20026;&#26412;&#22320;&#26641;&#29366;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21483;&#20570;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#27010;&#24565;&#65292;&#24182;&#26681;&#25454;&#36825;&#20010;&#26631;&#20934;&#35745;&#31639;&#20102;&#20855;&#26377;&#20219;&#24847;&#33410;&#28857;&#29305;&#24449;&#21644;&#36793;&#36830;&#25509;&#20998;&#24067;&#30340;&#30456;&#24403;&#19968;&#33324;&#30340;&#32479;&#35745;&#25968;&#25454;&#27169;&#22411;&#30340;&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#35813;&#26368;&#20248;&#20998;&#31867;&#22120;&#21487;&#20197;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23454;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#20102;&#35813;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#22312;&#19968;&#20010;&#24050;&#32463;&#30740;&#31350;&#20805;&#20998;&#30340;&#32479;&#35745;&#27169;&#22411;&#19978;&#20174;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20302;&#22270;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20339;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#25554;&#20540;&#20110;&#26631;&#20934;MLP&#21644;&#19968;&#31181;&#20856;&#22411;&#30340;c&#26550;&#26500;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;&#35748;&#35777;&#38450;&#24481;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#35777;&#26126;&#20102;&#24191;&#20041;&#38388;&#38553;&#26159;&#19968;&#20010;&#33391;&#22909;&#30340;&#39044;&#27979;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.10388</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35748;&#35777;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#25171;&#30772;&#29616;&#26377;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Raising the Bar for Certified Adversarial Robustness with Diffusion Models. (arXiv:2305.10388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;&#35748;&#35777;&#38450;&#24481;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#35777;&#26126;&#20102;&#24191;&#20041;&#38388;&#38553;&#26159;&#19968;&#20010;&#33391;&#22909;&#30340;&#39044;&#27979;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#25552;&#20379;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24418;&#24335;&#20445;&#35777;&#65292;&#20351;&#20854;&#27604;&#32463;&#39564;&#26041;&#27861;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#21487;&#38752;&#65292;&#21518;&#32773;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#20250;&#34987;&#26410;&#30693;&#25915;&#20987;&#21066;&#24369;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21487;&#23454;&#29616;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#21463;&#38480;&#65292;&#24050;&#25104;&#20026;&#23454;&#38469;&#37319;&#29992;&#30340;&#29942;&#39048;&#12290;Gowal&#31561;&#20154;&#21644;Wang&#31561;&#20154;&#24050;&#32463;&#34920;&#26126;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#65292;&#26412;&#25991;&#35777;&#26126;&#31867;&#20284;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30830;&#23450;&#24615;&#35748;&#35777;&#38450;&#24481;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#24314;&#35758;&#65292;&#20197;&#25193;&#23637;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35265;&#35299;&#20043;&#19968;&#26159;&#65292;&#21363;&#65292;&#24191;&#20041;&#38388;&#38553;&#65292;&#21363;&#21407;&#22987;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26159;&#20351;&#29992;&#39069;&#22806;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#40065;&#26834;&#24615;&#25913;&#21892;&#24133;&#24230;&#30340;&#33391;&#22909;&#39044;&#27979;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. One of our main insights is that the generalization gap, i.e., the difference between the training and test accuracy of the original model, is a good predictor of the magnitude of the robustness improvement when using additional g
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Logit&#30340;&#38598;&#25104;&#27169;&#22411;&#33976;&#39311;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#65288;epistemic&#65289;&#21644;&#25968;&#25454;&#65288;aleatoric&#65289;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20219;&#21153;&#33021;&#22815;&#25552;&#39640;student&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.10384</link><description>&lt;p&gt;
&#22522;&#20110;Logit&#30340;&#38598;&#25104;&#20998;&#24067;&#33976;&#39311;&#22312;&#33258;&#22238;&#24402;&#24207;&#21015;&#30340;&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties. (arXiv:2305.10384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Logit&#30340;&#38598;&#25104;&#27169;&#22411;&#33976;&#39311;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#65288;epistemic&#65289;&#21644;&#25968;&#25454;&#65288;aleatoric&#65289;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20219;&#21153;&#33021;&#22815;&#25552;&#39640;student&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#36890;&#24120;&#38750;&#24120;&#39640;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#30340;&#38598;&#25104;&#20998;&#24067;&#33976;&#39311;&#65288;Ensemble Distribution Distillation&#65292;EDD&#65289;&#26041;&#27861;&#12290;EDD&#26088;&#22312;&#23558;&#26114;&#36149;&#30340;&#65288;teacher&#65289;&#38598;&#25104;&#27169;&#22411;&#30340;&#20248;&#36234;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#21387;&#32553;&#21040;&#26356;&#20415;&#23452;&#30340;&#65288;student&#65289;&#21333;&#19968;&#27169;&#22411;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20445;&#30041;&#20102;&#23558;&#30693;&#35782;&#65288;&#35748;&#30693;&#65289;&#21644;&#25968;&#25454;&#65288;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#30340;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#27010;&#29575;&#31354;&#38388;&#26041;&#27861;&#23545;&#20110;&#22823;&#35789;&#27719;&#37327;&#30340;&#20219;&#21153;&#26469;&#35828;&#19981;&#26131;&#25193;&#23637;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#32763;&#35793;&#20219;&#21153;&#30340;&#29616;&#20195;Transformers&#27169;&#22411;&#20013;&#65292;&#23545;&#38598;&#25104;&#27169;&#22411;&#30340;logits&#36827;&#34892;&#24314;&#27169;&#27604;&#23545;softmax&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;student&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently and reliably estimating uncertainty is an important objective in deep learning. It is especially pertinent to autoregressive sequence tasks, where training and inference costs are typically very high. However, existing research has predominantly focused on tasks with static data such as image classification. In this work, we investigate Ensemble Distribution Distillation (EDD) applied to large-scale natural language sequence-to-sequence data. EDD aims to compress the superior uncertainty performance of an expensive (teacher) ensemble into a cheaper (student) single model. Importantly, the ability to separate knowledge (epistemic) and data (aleatoric) uncertainty is retained. Existing probability-space approaches to EDD, however, are difficult to scale to large vocabularies. We show, for modern transformer architectures on large-scale translation tasks, that modelling the ensemble logits, instead of softmax probabilities, leads to significantly better students. Moreover, the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10379</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#31526;&#21495;&#22238;&#24402;&#20013;&#20027;&#21160;&#23398;&#20064;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Active Learning in Symbolic Regression Performance with Physical Constraints. (arXiv:2305.10379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26159;&#19968;&#31181;&#23558;&#31526;&#21495;&#26041;&#31243;&#25311;&#21512;&#21040;&#25968;&#25454;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#31616;&#27905;&#26131;&#25026;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;SR&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#12290;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;SR&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#25552;&#20986;&#19979;&#19968;&#27493;&#23454;&#39564;&#12290;&#29289;&#29702;&#32422;&#26463;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#25913;&#21892;&#25152;&#24314;&#35758;&#30340;&#26041;&#31243;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;SR&#25152;&#38656;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary symbolic regression (SR) fits a symbolic equation to data, which gives a concise interpretable model. We explore using SR as a method to propose which data to gather in an active learning setting with physical constraints. SR with active learning proposes which experiments to do next. Active learning is done with query by committee, where the Pareto frontier of equations is the committee. The physical constraints improve proposed equations in very low data settings. These approaches reduce the data required for SR and achieves state of the art results in data required to rediscover known equations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#21033;&#29992;&#36817;&#36229;&#22768;&#27874;&#29305;&#27931;&#20234;&#26408;&#39532;&#23545;&#20122;&#39532;&#36874;Alexa&#35821;&#38899;&#26381;&#21153;&#30340;&#20027;&#35201;&#19981;&#21487;&#21548;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#20225;&#19994;&#12289;&#31227;&#21160;&#21644;&#24037;&#25511;&#31995;&#32479;&#30340;&#25915;&#20987;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.10358</link><description>&lt;p&gt;
NUANCE: &#32593;&#32476;&#36890;&#20449;&#29615;&#22659;&#19979;&#21033;&#29992;&#36817;&#36229;&#22768;&#27874;&#36827;&#34892;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NUANCE: Near Ultrasound Attack On Networked Communication Environments. (arXiv:2305.10358v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#21033;&#29992;&#36817;&#36229;&#22768;&#27874;&#29305;&#27931;&#20234;&#26408;&#39532;&#23545;&#20122;&#39532;&#36874;Alexa&#35821;&#38899;&#26381;&#21153;&#30340;&#20027;&#35201;&#19981;&#21487;&#21548;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#20225;&#19994;&#12289;&#31227;&#21160;&#21644;&#24037;&#25511;&#31995;&#32479;&#30340;&#25915;&#20987;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#36817;&#36229;&#22768;&#27874;&#29305;&#27931;&#20234;&#26408;&#39532;&#23545;&#20122;&#39532;&#36874;Alexa&#35821;&#38899;&#26381;&#21153;&#30340;&#20027;&#35201;&#19981;&#21487;&#21548;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#30528;&#37325;&#34920;&#24449;&#20102;&#25915;&#20987;&#38754;&#24182;&#32771;&#23519;&#20102;&#21457;&#20986;&#19981;&#21487;&#21548;&#35821;&#38899;&#25351;&#20196;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#23558;&#27599;&#20010;&#25915;&#20987;&#21521;&#37327;&#26144;&#23556;&#21040;MITRE ATT&#65286;CK&#30697;&#38453;&#20013;&#30340;&#19968;&#31181;&#31574;&#30053;&#25110;&#25216;&#26415;&#65292;&#28085;&#30422;&#20225;&#19994;&#12289;&#31227;&#21160;&#21644;&#24037;&#25511;&#31995;&#32479;&#65288;ICS&#65289;&#26694;&#26550;&#12290;&#23454;&#39564;&#28041;&#21450;&#29983;&#25104;&#21644;&#35843;&#26597;50&#20010;&#36817;&#36229;&#22768;&#27874;&#38899;&#39057;&#20197;&#35780;&#20272;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#26410;&#32463;&#22788;&#29702;&#30340;&#25351;&#20196;&#20855;&#26377;100&#65285;&#30340;&#25104;&#21151;&#29575;&#65292;&#22788;&#29702;&#21518;&#30340;&#25351;&#20196;&#23454;&#29616;&#20102;58&#65285;&#30340;&#24635;&#20307;&#25104;&#21151;&#29575;&#12290;&#35813;&#31995;&#32479;&#24615;&#26041;&#27861;&#21050;&#28608;&#20102;&#20197;&#21069;&#26410;&#24471;&#21040;&#35299;&#20915;&#30340;&#25915;&#20987;&#38754;&#65292;&#30830;&#20445;&#20102;&#20840;&#38754;&#30340;&#26816;&#27979;&#21644;&#25915;&#20987;&#35774;&#35745;&#65292;&#24182;&#23558;&#27599;&#20010;ATT&#65286;CK&#26631;&#35782;&#31526;&#19982;&#27979;&#35797;&#36807;&#30340;&#38450;&#24481;&#26041;&#27861;&#25645;&#37197;&#65292;&#20026;&#24555;&#36895;&#21709;&#24212;&#25552;&#20379;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36873;&#39033;&#12290;&#20027;&#35201;&#21457;&#29616;&#25581;&#31034;&#20102;&#35813;&#25915;&#20987;&#26041;&#27861;&#37319;&#29992;&#21333;&#36793;&#24102;&#24133;&#24230;&#35843;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates a primary inaudible attack vector on Amazon Alexa voice services using near ultrasound trojans and focuses on characterizing the attack surface and examining the practical implications of issuing inaudible voice commands. The research maps each attack vector to a tactic or technique from the MITRE ATT&amp;CK matrix, covering enterprise, mobile, and Industrial Control System (ICS) frameworks. The experiment involved generating and surveying fifty near-ultrasonic audios to assess the attacks' effectiveness, with unprocessed commands having a 100% success rate and processed ones achieving a 58% overall success rate. This systematic approach stimulates previously unaddressed attack surfaces, ensuring comprehensive detection and attack design while pairing each ATT&amp;CK Identifier with a tested defensive method, providing attack and defense tactics for prompt-response options. The main findings reveal that the attack method employs Single Upper Sideband Amplitude Modulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#26080;&#27491;&#20132;&#21270;&#26041;&#27861;&#20316;&#20026;&#35889;&#32858;&#31867;&#38477;&#32500;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#29305;&#24449;&#20540;&#20272;&#35745;&#65292;&#22312;&#32858;&#31867;&#36136;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#22343;&#20248;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#36866;&#21512;&#20110;&#24182;&#34892;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2305.10356</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#27491;&#20132;&#21270;&#26041;&#27861;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering via Orthogonalization-Free Methods. (arXiv:2305.10356v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#26080;&#27491;&#20132;&#21270;&#26041;&#27861;&#20316;&#20026;&#35889;&#32858;&#31867;&#38477;&#32500;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#29305;&#24449;&#20540;&#20272;&#35745;&#65292;&#22312;&#32858;&#31867;&#36136;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#22343;&#20248;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#36866;&#21512;&#20110;&#24182;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35889;&#32858;&#31867;&#30340;&#38477;&#32500;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#38656;&#35201;&#26114;&#36149;&#30340;&#29305;&#24449;&#20540;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#26368;&#20248;&#21270;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#28388;&#27874;&#22120;&#24182;&#25552;&#20986;&#20351;&#29992;&#22235;&#31181;&#26080;&#27491;&#20132;&#21270;&#26041;&#27861;&#20316;&#20026;&#35889;&#32858;&#31867;&#20013;&#30340;&#38477;&#32500;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#21033;&#29992;&#20219;&#20309;&#27491;&#20132;&#21270;&#26041;&#27861;&#65292;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#19981;&#21487;&#20280;&#32553;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#26500;&#36896;&#20102;&#36275;&#22815;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#26368;&#22810;&#26159;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#29305;&#24449;&#31354;&#38388;&#30340;&#21152;&#26435;&#25913;&#21464;&#12290;&#25105;&#20204;&#22312;&#25968;&#20540;&#19978;&#20551;&#35774;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#21033;&#29992;&#31934;&#30830;&#29305;&#24449;&#20540;&#20294;&#38656;&#35201;&#26114;&#36149;&#29305;&#24449;&#20540;&#20272;&#35745;&#30340;&#29702;&#24819;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#22312;&#32858;&#31867;&#36136;&#37327;&#19978;&#31561;&#25928;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#36136;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#24130;&#36845;&#20195;&#30340;&#26041;&#27861;&#21644;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#12290;&#19982;&#22522;&#20110;&#24130;&#36845;&#20195;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Signal Filter used as dimensionality reduction in spectral clustering usually requires expensive eigenvalue estimation. We analyze the filter in an optimization setting and propose to use four orthogonalization-free methods by optimizing objective functions as dimensionality reduction in spectral clustering. The proposed methods do not utilize any orthogonalization, which is known as not well scalable in a parallel computing environment. Our methods theoretically construct adequate feature space, which is, at most, a weighted alteration to the eigenspace of a normalized Laplacian matrix. We numerically hypothesize that the proposed methods are equivalent in clustering quality to the ideal Graph Signal Filter, which exploits the exact eigenvalue needed without expensive eigenvalue estimation. Numerical results show that the proposed methods outperform Power Iteration-based methods and Graph Signal Filter in clustering quality and computation cost. Unlike Power Iteration-based meth
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#31995;&#32479;&#32570;&#20047;&#23545;&#36816;&#21160;&#24341;&#36215;&#30340;&#34880;&#31958;&#25668;&#21462;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#29702;&#27169;&#22411;&#21644;Siamese&#32593;&#32476;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#21033;&#29992;&#22810;&#20010;&#29983;&#29702;&#20449;&#21495;&#27969;&#36827;&#34892;&#36816;&#21160;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10353</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#36816;&#21160;&#26816;&#27979;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Learning Approach for Exercise Detection in Type 1 Diabetes Patients. (arXiv:2305.10353v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10353
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#31995;&#32479;&#32570;&#20047;&#23545;&#36816;&#21160;&#24341;&#36215;&#30340;&#34880;&#31958;&#25668;&#21462;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#29702;&#27169;&#22411;&#21644;Siamese&#32593;&#32476;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#21033;&#29992;&#22810;&#20010;&#29983;&#29702;&#20449;&#21495;&#27969;&#36827;&#34892;&#36816;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1&#22411;&#31958;&#23615;&#30149;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#30142;&#30149;&#65292;&#20351;&#24739;&#32773;&#26080;&#27861;&#35843;&#33410;&#34880;&#31958;&#27700;&#24179;&#65292;&#23548;&#33268;&#21508;&#31181;&#21307;&#30103;&#24182;&#21457;&#30151;&#12290;&#20154;&#24037;&#33008;&#33146;&#31995;&#32479;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#20316;&#20026;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#27169;&#25311;&#33008;&#33146;&#30340;&#34892;&#20026;&#21644;&#35843;&#33410;&#34880;&#31958;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20154;&#24037;&#33008;&#33146;&#31995;&#32479;&#32570;&#20047;&#23545;&#36816;&#21160;&#24341;&#36215;&#30340;&#34880;&#31958;&#25668;&#21462;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#25345;&#32493;4&#21040;8&#20010;&#23567;&#26102;&#12290;&#36825;&#31181;&#26080;&#33021;&#20250;&#23548;&#33268;&#20302;&#34880;&#31958;&#65292;&#22914;&#26524;&#19981;&#21152;&#27835;&#30103;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20005;&#37325;&#21518;&#26524;&#65292;&#21253;&#25324;&#27515;&#20129;&#12290;&#29616;&#26377;&#30340;&#36816;&#21160;&#26816;&#27979;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#21333;&#19968;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#35201;&#20040;&#20351;&#29992;&#19981;&#20934;&#30830;&#30340;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#26816;&#27979;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#19981;&#22826;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#29702;&#27169;&#22411;&#21644;Siamese&#32593;&#32476;&#65292;&#21033;&#29992;&#22810;&#20010;&#29983;&#29702;&#20449;&#21495;&#27969;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#36816;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Type 1 diabetes is a serious disease in which individuals are unable to regulate their blood glucose levels, leading to various medical complications. Artificial pancreas (AP) systems have been developed as a solution for type 1 diabetic patients to mimic the behavior of the pancreas and regulate blood glucose levels. However, current AP systems lack detection capabilities for exercise-induced glucose intake, which can last up to 4 to 8 hours. This incapability can lead to hypoglycemia, which if left untreated, could have serious consequences, including death. Existing exercise detection methods are either limited to single sensor data or use inaccurate models for exercise detection, making them less effective in practice. In this work, we propose an ensemble learning framework that combines a data-driven physiological model and a Siamese network to leverage multiple physiological signal streams for exercise detection with high accuracy. To evaluate the effectiveness of our proposed ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#22312;&#26497;&#20302;&#39057;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#20013;&#26816;&#27979;&#19981;&#21516;&#23478;&#30005;&#30340;&#23384;&#22312;/&#32570;&#22833;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......</title><link>http://arxiv.org/abs/2305.10352</link><description>&lt;p&gt;
&#37319;&#29992;&#26497;&#20302;&#39057;&#26234;&#33021;&#30005;&#34920;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23478;&#30005;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Appliance Detection Using Very Low-Frequency Smart Meter Time Series. (arXiv:2305.10352v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#22312;&#26497;&#20302;&#39057;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#20013;&#26816;&#27979;&#19981;&#21516;&#23478;&#30005;&#30340;&#23384;&#22312;/&#32570;&#22833;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26234;&#33021;&#30005;&#34920;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20197;&#25913;&#21892;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#30340;&#31649;&#29702;&#65292;&#36825;&#20123;&#30005;&#34920;&#36890;&#24120;&#20197;&#26497;&#20302;&#30340;&#39057;&#29575;&#65288;&#27599;30&#20998;&#38047;&#65289;&#25910;&#38598;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#21521;&#23458;&#25143;&#35745;&#36153;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#30340;&#24314;&#35758;&#65292;&#19979;&#19968;&#27493;&#26159;&#26816;&#27979;&#23458;&#25143;&#25317;&#26377;&#30340;&#23478;&#30005;&#65292;&#30001;&#20110;&#26497;&#20302;&#30340;&#35745;&#37327;&#35835;&#25968;&#39057;&#29575;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#23478;&#30005;&#26816;&#27979;&#38382;&#39064;&#21487;&#20197;&#34987;&#35270;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#36825;&#20010;&#20855;&#20307;&#30340;&#38382;&#39064;&#24182;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#26368;&#26032;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#22312;&#26497;&#20302;&#39057;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#20013;&#26816;&#27979;&#19981;&#21516;&#23478;&#30005;&#30340;&#23384;&#22312;/&#32570;&#22833;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;5&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;13&#20010;&#26102;&#24207;&#20998;&#31867;&#22120;&#26816;&#27979;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, smart meters have been widely adopted by electricity suppliers to improve the management of the smart grid system. These meters usually collect energy consumption data at a very low frequency (every 30min), enabling utilities to bill customers more accurately. To provide more personalized recommendations, the next step is to detect the appliances owned by customers, which is a challenging problem, due to the very-low meter reading frequency. Even though the appliance detection problem can be cast as a time series classification problem, with many such classifiers having been proposed in the literature, no study has applied and compared them on this specific problem. This paper presents an in-depth evaluation and comparison of state-of-the-art time series classifiers applied to detecting the presence/absence of diverse appliances in very low-frequency smart meter data. We report results with five real datasets. We first study the impact of the detection quality of 13 di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#28304;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#22312;&#25152;&#26377;&#29983;&#29289;&#20449;&#21495;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;BIOT&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20196;&#29260;&#21270;&#20026;&#32479;&#19968;&#30340;&#8220;&#29983;&#29289;&#20449;&#21495;&#21477;&#23376;&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36328;&#25968;&#25454;&#28304;&#30340;&#23398;&#20064;&#65292;&#22788;&#29702;&#21508;&#31181;&#26684;&#24335;&#30340;&#29983;&#29289;&#20449;&#21495;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10351</link><description>&lt;p&gt;
BIOT&#65306;&#22312;&#37326;&#22806;&#36328;&#25968;&#25454;&#28304;&#29983;&#29289;&#20449;&#21495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BIOT: Cross-data Biosignal Learning in the Wild. (arXiv:2305.10351v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#28304;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#22312;&#25152;&#26377;&#29983;&#29289;&#20449;&#21495;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;BIOT&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20196;&#29260;&#21270;&#20026;&#32479;&#19968;&#30340;&#8220;&#29983;&#29289;&#20449;&#21495;&#21477;&#23376;&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36328;&#25968;&#25454;&#28304;&#30340;&#23398;&#20064;&#65292;&#22788;&#29702;&#21508;&#31181;&#26684;&#24335;&#30340;&#29983;&#29289;&#20449;&#21495;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20449;&#21495;&#65292;&#22914;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#65292;&#22312;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;&#22810;&#31181;&#25968;&#25454;&#26684;&#24335;&#21644;&#36136;&#37327;&#29305;&#24449;&#12290;&#24403;&#21069;&#30340;&#29983;&#29289;&#20449;&#21495;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#20020;&#24202;&#35774;&#32622;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#24320;&#21457;&#22522;&#20110;&#22810;&#20010;&#25968;&#25454;&#28304;&#36827;&#34892;&#35757;&#32451;&#24182;&#21487;&#22312;&#19981;&#21516;&#22522;&#30784;&#29983;&#29289;&#20449;&#21495;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#20811;&#26381;&#19982;&#21508;&#31181;&#26684;&#24335;&#30340;&#29983;&#29289;&#20449;&#21495;&#30456;&#20851;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#19981;&#21305;&#37197;&#30340;&#36890;&#36947;&#65292;&#21487;&#21464;&#30340;&#37319;&#26679;&#38271;&#24230;&#21644;&#26222;&#36941;&#23384;&#22312;&#30340;&#32570;&#22833;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Biosignal Transformer&#27169;&#22411;&#65288;&#31616;&#31216;\method&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;\method&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20196;&#29260;&#21270;&#20026;&#32479;&#19968;&#30340;&#8220;&#29983;&#29289;&#20449;&#21495;&#21477;&#23376;&#8221;&#26469;&#23454;&#29616;&#36890;&#36947;&#19981;&#21305;&#37197;&#12289;&#38271;&#24230;&#21487;&#21464;&#21644;&#32570;&#22833;&#20540;&#26222;&#36941;&#23384;&#22312;&#30340;&#36328;&#25968;&#25454;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#36890;&#36947;&#20196;&#29260;&#21270;&#20026;&#22266;&#23450;&#38271;&#24230;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#36890;&#36947;&#20043;&#38388;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#24418;&#25104;&#23436;&#25972;&#30340;&#29983;&#29289;&#20449;&#21495;&#21477;&#23376;&#12290;&#22312;EEG&#20998;&#31867;&#21644;EEG&#36716;&#35821;&#38899;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\method). The proposed \method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified "biosignal sentences". Specifically, we tokenize each channel into fixed-le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#8220;&#22810;&#20803;&#23431;&#23449;&#8221;&#33539;&#24335;&#65292;&#20854;&#20013;&#26377;&#20960;&#20010;&#25968;&#23383;&#23402;&#29983;&#20307;&#35797;&#22270;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#20445;&#30495;&#24230;&#19979;&#25429;&#25417;&#30495;&#23454;&#19990;&#30028;&#65292;&#36890;&#36807;&#36827;&#34892;&#33258;&#23398;&#20064;&#20197;&#22686;&#24378;&#22522;&#20110;DL&#30340;&#23454;&#26102;&#20915;&#31574;&#12290;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#21152;&#36895;&#31227;&#21160;&#29615;&#22659;&#20013;&#30340;&#26041;&#21521;&#27874;&#26463;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2305.10350</link><description>&lt;p&gt;
&#36793;&#32536;&#22810;&#20803;&#23431;&#23449;&#65306;&#20132;&#20114;&#30495;&#23454;&#19990;&#30028;&#21644;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#26080;&#32447;&#27874;&#26463;&#25104;&#24418;
&lt;/p&gt;
&lt;p&gt;
Multiverse at the Edge: Interacting Real World and Digital Twins for Wireless Beamforming. (arXiv:2305.10350v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#8220;&#22810;&#20803;&#23431;&#23449;&#8221;&#33539;&#24335;&#65292;&#20854;&#20013;&#26377;&#20960;&#20010;&#25968;&#23383;&#23402;&#29983;&#20307;&#35797;&#22270;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#20445;&#30495;&#24230;&#19979;&#25429;&#25417;&#30495;&#23454;&#19990;&#30028;&#65292;&#36890;&#36807;&#36827;&#34892;&#33258;&#23398;&#20064;&#20197;&#22686;&#24378;&#22522;&#20110;DL&#30340;&#23454;&#26102;&#20915;&#31574;&#12290;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#21152;&#36895;&#31227;&#21160;&#29615;&#22659;&#20013;&#30340;&#26041;&#21521;&#27874;&#26463;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20808;&#36827;&#30340;&#20223;&#30495;&#36719;&#20214;&#21644;&#26222;&#36941;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21019;&#24314;&#19968;&#20010;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#35768;&#22810;&#22797;&#26434;&#20132;&#20114;&#21644;&#32467;&#26524;&#23494;&#20999;&#30456;&#20284;&#30340;&#25968;&#23383;&#19990;&#30028;&#26159;&#21487;&#33021;&#30340;&#12290;&#36825;&#31181;&#22522;&#20110;&#36719;&#20214;&#30340;&#19982;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#30340;&#23454;&#20307;&#23494;&#20999;&#27169;&#25311;&#30340;&#23454;&#20307;&#34987;&#31216;&#20026;&#8220;&#25968;&#23383;&#23402;&#29983;&#20307;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36710;&#36733;&#27627;&#31859;&#27874;&#27573;&#26080;&#32447;&#30005;&#30340;&#23402;&#29983;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21152;&#36895;&#31227;&#21160;&#29615;&#22659;&#20013;&#30340;&#26041;&#21521;&#27874;&#26463;&#36873;&#25321;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#23454;&#20363;&#21270;&#19968;&#20010;&#21333;&#19968;&#30340;&#23402;&#29983;&#20307;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#22810;&#20803;&#23431;&#23449;&#8221;&#33539;&#24335;&#65292;&#26377;&#20960;&#20010;&#21487;&#33021;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#35797;&#22270;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#20445;&#30495;&#24230;&#19979;&#25429;&#25417;&#30495;&#23454;&#19990;&#30028;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25551;&#36848;&#65288;i&#65289;&#36710;&#36742;&#19978;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#30830;&#23450;&#30001;&#20110;&#35745;&#31639;&#21644;&#24310;&#36831;&#38480;&#21046;&#24517;&#39035;&#20351;&#29992;&#21738;&#20010;&#23402;&#29983;&#20307;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#19968;&#31181;&#21033;&#29992;&#22810;&#20803;&#23431;&#23449;&#24341;&#23548;&#30340;&#20809;&#26463;&#32467;&#26524;&#26469;&#22686;&#24378;&#22522;&#20110;DL&#30340;&#23454;&#26102;&#20915;&#31574;&#30340;&#33258;&#23398;&#20064;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating a digital world that closely mimics the real world with its many complex interactions and outcomes is possible today through advanced emulation software and ubiquitous computing power. Such a software-based emulation of an entity that exists in the real world is called a 'digital twin'. In this paper, we consider a twin of a wireless millimeter-wave band radio that is mounted on a vehicle and show how it speeds up directional beam selection in mobile environments. To achieve this, we go beyond instantiating a single twin and propose the 'Multiverse' paradigm, with several possible digital twins attempting to capture the real world at different levels of fidelity. Towards this goal, this paper describes (i) a decision strategy at the vehicle that determines which twin must be used given the computational and latency limitations, and (ii) a self-learning scheme that uses the Multiverse-guided beam outcomes to enhance DL-based decision-making in the real world over time. Our work
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21463;&#35797;&#32773;&#30340;&#26080;&#23545;&#29031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#30005;&#20449;&#21495;&#22788;&#29702;&#65292;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#25110;&#36127;&#23545;&#65292;&#24182;&#22312;ECG&#22788;&#29702;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;SSL&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.10347</link><description>&lt;p&gt;
&#22522;&#20110;&#21463;&#35797;&#32773;&#30340;&#26080;&#23545;&#29031;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#24515;&#30005;&#20449;&#21495;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Subject-based Non-contrastive Self-Supervised Learning for ECG Signal Processing. (arXiv:2305.10347v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21463;&#35797;&#32773;&#30340;&#26080;&#23545;&#29031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#30005;&#20449;&#21495;&#22788;&#29702;&#65292;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#25110;&#36127;&#23545;&#65292;&#24182;&#22312;ECG&#22788;&#29702;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;SSL&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#24515;&#33039;&#30149;&#25968;&#23383;&#20581;&#24247;&#25216;&#26415;&#35774;&#35745;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#33258;&#21160;&#25552;&#21462;ECG&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#12290;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#21151;&#29992;&#20110;&#35782;&#21035;&#20449;&#21495;&#20013;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#22914;&#33410;&#24459;&#38556;&#30861;&#65288;&#24515;&#24459;&#22833;&#24120;&#65289;&#30340;&#26816;&#27979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#21487;&#29992;&#20110;&#25552;&#21462;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#25152;&#26377;&#29305;&#24449;&#12290;&#27169;&#22411;&#22312;&#27809;&#26377;&#29305;&#23450;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#20174;&#25968;&#25454;&#26412;&#36523;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#36866;&#24212;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#65292;&#26368;&#36817;&#25253;&#36947;&#20102;&#20960;&#31181;ECG&#22788;&#29702;&#30340;SSL&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;SSL&#26041;&#27861;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#25110;&#36127;&#23545;&#65292;&#36825;&#38480;&#21046;&#20102;&#35813;&#26041;&#27861;&#20165;&#26597;&#25214;&#20004;&#20010;ECG&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#21363;&#21516;&#19968;&#20449;&#21495;&#30340;&#20004;&#20010;&#29256;&#26412;&#25110;&#26469;&#33258;&#19981;&#21516;&#24739;&#32773;&#30340;&#20004;&#20010;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21463;&#35797;&#32773;&#30340;&#38750;&#23545;&#29031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;ECG&#20449;&#21495;&#22788;&#29702;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25968;&#25454;&#22686;&#24378;&#25110;&#36127;&#23545;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23558;ECG&#35760;&#24405;&#35270;&#20026;&#26469;&#33258;&#29305;&#23450;&#21463;&#35797;&#32773;&#30340;&#27979;&#37327;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#27604;&#36739;&#20004;&#20010;&#19981;&#21516;&#30340;&#20449;&#21495;&#26469;&#23398;&#20064;ECG&#20449;&#21495;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;ECG&#22788;&#29702;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;SSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting information from the electrocardiography (ECG) signal is an essential step in the design of digital health technologies in cardiology. In recent years, several machine learning (ML) algorithms for automatic extraction of information in ECG have been proposed. Supervised learning methods have successfully been used to identify specific aspects in the signal, like detection of rhythm disorders (arrhythmias). Self-supervised learning (SSL) methods, on the other hand, can be used to extract all the features contained in the data. The model is optimized without any specific goal and learns from the data itself. By adapting state-of-the-art computer vision methodologies to the signal processing domain, a few SSL approaches have been reported recently for ECG processing. However, such SSL methods require either data augmentation or negative pairs, which limits the method to only look for similarities between two ECG inputs, either two versions of the same signal or two signals from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;G-Adapter&#30340;&#38754;&#21521;&#22270;&#24418;Transformer&#32593;&#32476;&#30340;&#32467;&#26500;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#19968;&#32452;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#23558;PEFT&#25216;&#26415;&#24212;&#29992;&#20110;GTNs&#24182;&#38750;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.10329</link><description>&lt;p&gt;
G-Adapter: &#38754;&#21521;&#22270;&#24418;Transformer&#32593;&#32476;&#30340;&#32467;&#26500;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks. (arXiv:2305.10329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;G-Adapter&#30340;&#38754;&#21521;&#22270;&#24418;Transformer&#32593;&#32476;&#30340;&#32467;&#26500;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#19968;&#32452;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#23558;PEFT&#25216;&#26415;&#24212;&#29992;&#20110;GTNs&#24182;&#38750;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#24494;&#35843;&#25972;&#20010;&#27169;&#22411;&#21442;&#25968;&#20256;&#36882;&#21040;&#21508;&#20010;&#19979;&#28216;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#21644;&#19979;&#28216;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#33539;&#20363;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#30528;&#35745;&#31639;&#28040;&#32791;&#21644;&#20869;&#23384;&#21344;&#29992;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;&#22914;Adapter&#12289;LoRA&#12289;BitFit&#65289;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#26395;&#36890;&#36807;&#20165;&#26356;&#26032;&#19968;&#37096;&#20998;&#21442;&#25968;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#22312;&#22270;&#24418;Transformer&#32593;&#32476;&#65288;GTNs&#65289;&#19979;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#19981;&#22815;&#24191;&#27867;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#22522;&#20110;&#22270;&#24418;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#23558;PEFT&#25216;&#26415;&#24212;&#29992;&#20110;GTNs&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become a popular paradigm to transfer the knowledge of large-scale pre-trained models to various downstream tasks via fine-tuning the entire model parameters. However, with the growth of model scale and the rising number of downstream tasks, this paradigm inevitably meets the challenges in terms of computation consumption and memory footprint issues. Recently, Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a promising paradigm to alleviate these concerns by updating only a portion of parameters. Despite these PEFTs having demonstrated satisfactory performance in natural language processing, it remains under-explored for the question of whether these techniques could be transferred to graph-based tasks with Graph Transformer Networks (GTNs). Therefore, in this paper, we fill this gap by providing extensive benchmarks with traditional PEFTs on a range of graph-based downstream tasks. Our empirical study shows that it is sub-optimal to directly transfer 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;CNN&#35299;&#20915;&#20102;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Guided Backpropagation&#33719;&#24471;&#20102;CNN&#26816;&#27979;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.10319</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic Photo Orientation Detection with Convolutional Neural Networks. (arXiv:2305.10319v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;CNN&#35299;&#20915;&#20102;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Guided Backpropagation&#33719;&#24471;&#20102;CNN&#26816;&#27979;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#35299;&#20915;&#30830;&#23450;&#28040;&#36153;&#32773;&#29031;&#29255;&#27491;&#30830;&#26041;&#21521;(0&#176;, 90&#176;, 180&#176;&#21644;270&#176;)&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#23545;&#20110;&#27169;&#25311;&#29031;&#29255;&#30340;&#25968;&#23383;&#21270;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#26356;&#22256;&#38590;&#30340;&#28040;&#36153;&#32773;&#29031;&#29255;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;(Guided Backpropagation)&#26469;&#33719;&#24471;&#20851;&#20110;CNN&#22914;&#20309;&#26816;&#27979;&#29031;&#29255;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#24182;&#35299;&#37322;&#20854;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply convolutional neural networks (CNN) to the problem of image orientation detection in the context of determining the correct orientation (from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is especially important for digitazing analog photographs. We substantially improve on the published state of the art in terms of the performance on one of the standard datasets, and test our system on a more difficult large dataset of consumer photos. We use Guided Backpropagation to obtain insights into how our CNN detects photo orientation, and to explain its mistakes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaModulation&#30340;&#23569;&#20219;&#21153;Few-Shot Learning&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#35843;&#21046;&#25209;&#37327;&#24402;&#19968;&#21270;&#21442;&#25968;&#20197;&#22686;&#21152;&#20803;&#35757;&#32451;&#20219;&#21153;&#30340;&#23494;&#24230;&#12290;&#26412;&#26041;&#27861;&#36890;&#36807;&#21464;&#20998;MetaModulation&#20171;&#32461;&#20102;&#23398;&#20064;&#21464;&#20998;&#29305;&#24449;&#23618;&#27425;&#65292;&#21487;&#20197;&#32771;&#34385;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#26356;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.10309</link><description>&lt;p&gt;
MetaModulation&#65306;&#22312;&#23569;&#20219;&#21153;&#24773;&#20917;&#19979;&#23398;&#20064;&#21464;&#20998;&#29305;&#24449;&#23618;&#27425;&#30340;Few-Shot Learning&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks. (arXiv:2305.10309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaModulation&#30340;&#23569;&#20219;&#21153;Few-Shot Learning&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#35843;&#21046;&#25209;&#37327;&#24402;&#19968;&#21270;&#21442;&#25968;&#20197;&#22686;&#21152;&#20803;&#35757;&#32451;&#20219;&#21153;&#30340;&#23494;&#24230;&#12290;&#26412;&#26041;&#27861;&#36890;&#36807;&#21464;&#20998;MetaModulation&#20171;&#32461;&#20102;&#23398;&#20064;&#21464;&#20998;&#29305;&#24449;&#23618;&#27425;&#65292;&#21487;&#20197;&#32771;&#34385;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#26356;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#33021;&#19981;&#23481;&#26131;&#24471;&#21040;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaModulation&#30340;&#23569;&#20219;&#21153;Few-Shot Learning&#26041;&#27861;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#35843;&#21046;&#25209;&#37327;&#24402;&#19968;&#21270;&#21442;&#25968;&#20197;&#22686;&#21152;&#20803;&#35757;&#32451;&#20219;&#21153;&#30340;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#21508;&#20010;&#32593;&#32476;&#23618;&#27425;&#20462;&#25913;&#21442;&#25968;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#23618;&#27425;&#65292;&#20197;&#22686;&#21152;&#20219;&#21153;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#32771;&#34385;&#26377;&#38480;&#30340;&#35757;&#32451;&#20219;&#21153;&#25152;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;MetaModulation&#65292;&#20854;&#20013;&#35843;&#21046;&#21442;&#25968;&#34987;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21464;&#20998;MetaModulation&#20171;&#32461;&#20102;&#23398;&#20064;&#21464;&#20998;&#29305;&#24449;&#23618;&#27425;&#65292;&#35813;&#26041;&#27861;&#35843;&#21046;&#25152;&#26377;&#23618;&#27425;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#32771;&#34385;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#26356;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#26412;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning algorithms are able to learn a new task using previously learned knowledge, but they often require a large number of meta-training tasks which may not be readily available. To address this issue, we propose a method for few-shot learning with fewer tasks, which we call MetaModulation. The key idea is to use a neural network to increase the density of the meta-training tasks by modulating batch normalization parameters during meta-training. Additionally, we modify parameters at various network levels, rather than just a single layer, to increase task diversity. To account for the uncertainty caused by the limited training tasks, we propose a variational MetaModulation where the modulation parameters are treated as latent variables. We also introduce learning variational feature hierarchies by the variational MetaModulation, which modulates features at all layers and can consider task uncertainty and generate more diverse tasks. The ablation studies illustrate the advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;Random Continuous Embedding&#65292;RCE&#65289;&#65292;&#33021;&#22815;&#25552;&#39640; Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#22823;&#24133;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10308</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#32771;&#34385;&#34920;&#26684;&#25968;&#25454;&#25968;&#25454;&#22686;&#24378;&#30340;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;
Rethinking Data Augmentation for Tabular Data in Deep Learning. (arXiv:2305.10308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;Random Continuous Embedding&#65292;RCE&#65289;&#65292;&#33021;&#22815;&#25552;&#39640; Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#22823;&#24133;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#26684;&#24335;&#12290;&#34429;&#28982;&#22312;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#26641;&#24418;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65307;&#20294;&#26368;&#36817;&#30340;&#25991;&#29486;&#25253;&#21578;&#31216;&#65292;Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;&#22312;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#20027;&#23548;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29420;&#29305;&#32467;&#26500;&#21644;&#39640;&#22797;&#26434;&#24615;&#65292;&#34920;&#26684;&#25968;&#25454;&#30340;&#25968;&#25454;&#22686;&#24378;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#27169;&#22411;&#32467;&#26500;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#19968;&#36215;&#25552;&#20986;&#12290;&#22240;&#27492;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23545;&#27604;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;RCE&#65289;&#65292;&#36890;&#36807;&#21521;&#36830;&#32493;&#21464;&#37327;&#27880;&#20837;&#22122;&#22768;&#26469;&#29983;&#25104;&#22686;&#24378;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126; RCE &#22312;&#20351;&#29992; Transformer-based &#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26102;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#31579;&#36873;&#30740;&#31350;&#20197;&#26174;&#31034; RCE &#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126; RCE &#20351; Transformer-based &#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is the most widely used data format in machine learning (ML). While tree-based methods outperform DL-based methods in supervised learning, recent literature reports that self-supervised learning with Transformer-based models outperforms tree-based methods. In the existing literature on self-supervised learning for tabular data, contrastive learning is the predominant method. In contrastive learning, data augmentation is important to generate different views. However, data augmentation for tabular data has been difficult due to the unique structure and high complexity of tabular data. In addition, three main components are proposed together in existing methods: model structure, self-supervised learning methods, and data augmentation. Therefore, previous works have compared the performance without comprehensively considering these components, and it is not clear how each component affects the actual performance.  In this study, we focus on data augmentation to address these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19981;&#21516;&#26041;&#27861;&#20272;&#35745;&#38146;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#30005;&#27744;&#24615;&#33021;&#21442;&#25968;&#20934;&#30830;&#39044;&#27979;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;</title><link>http://arxiv.org/abs/2305.10298</link><description>&lt;p&gt;
&#38024;&#23545;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;&#65289;&#30340;&#21097;&#20313;&#23551;&#21629;&#21644;SOH&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of Remaining Useful Life and SOH of Lithium Ion Batteries (For EV Vehicles). (arXiv:2305.10298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19981;&#21516;&#26041;&#27861;&#20272;&#35745;&#38146;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#30005;&#27744;&#24615;&#33021;&#21442;&#25968;&#20934;&#30830;&#39044;&#27979;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#21253;&#25324;&#20415;&#25658;&#24335;&#30005;&#23376;&#35774;&#22791;&#12289;&#30005;&#21160;&#27773;&#36710;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#23384;&#20648;&#31995;&#32479;&#12290;&#20934;&#30830;&#20272;&#35745;&#36825;&#20123;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#23545;&#20110;&#30830;&#20445;&#20854;&#26368;&#20339;&#24615;&#33021;&#65292;&#39044;&#38450;&#24847;&#22806;&#25925;&#38556;&#20197;&#21450;&#38477;&#20302;&#32500;&#25252;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#20272;&#35745;&#38146;&#31163;&#23376;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21253;&#25324;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12289;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#20197;&#21450;&#28151;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#30005;&#27744;&#24615;&#33021;&#21442;&#25968;&#65292;&#21253;&#25324;&#30005;&#21387;&#12289;&#30005;&#27969;&#21644;&#28201;&#24230;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38146;&#31163;&#23376;&#30005;&#27744;&#21608;&#26399;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lithium-ion batteries are widely used in various applications, including portable electronic devices, electric vehicles, and renewable energy storage systems. Accurately estimating the remaining useful life of these batteries is crucial for ensuring their optimal performance, preventing unexpected failures, and reducing maintenance costs. In this paper, we present a comprehensive review of the existing approaches for estimating the remaining useful life of lithium-ion batteries, including data-driven methods, physics-based models, and hybrid approaches. We also propose a novel approach based on machine learning techniques for accurately predicting the remaining useful life of lithium-ion batteries. Our approach utilizes various battery performance parameters, including voltage, current, and temperature, to train a predictive model that can accurately estimate the remaining useful life of the battery. We evaluate the performance of our approach on a dataset of lithium-ion battery cycles
&lt;/p&gt;</description></item><item><title>DualFL&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20855;&#20307;&#23545;&#20598;&#24418;&#24335;&#35299;&#20915;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#20102;&#21363;&#20351;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#20063;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10294</link><description>&lt;p&gt;
DualFL&#65306;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#30340;Federated Learning&#31639;&#27861;&#21450;&#22312;&#19968;&#33324;&#20984;&#24773;&#24418;&#19979;&#21152;&#36895;&#36890;&#35759;
&lt;/p&gt;
&lt;p&gt;
DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime. (arXiv:2305.10294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10294
&lt;/p&gt;
&lt;p&gt;
DualFL&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20855;&#20307;&#23545;&#20598;&#24418;&#24335;&#35299;&#20915;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#20102;&#21363;&#20351;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#20063;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DualFL&#65288;Dualized Federated Learning&#65289;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#30340;&#29305;&#23450;&#23545;&#20598;&#24418;&#24335;&#12290;DualFL&#22312;&#19981;&#21516;&#30340;&#20809;&#28369;&#24615;&#21644;&#24378;&#20984;&#24615;&#35774;&#32622;&#19979;&#23454;&#29616;&#36890;&#35759;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#27714;&#35299;&#22120;&#65292;&#21363;&#20351;&#26159;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#65292;&#20063;&#21487;&#20197;&#20445;&#25345;&#20854;&#26368;&#20339;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;DualFL&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#36890;&#35759;&#21152;&#36895;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20351;&#25104;&#26412;&#20989;&#25968;&#26082;&#38750;&#20809;&#28369;&#20063;&#38750;&#24378;&#20984;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;DualFL&#30340;&#23454;&#38469;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#23545;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel training algorithm called DualFL (Dualized Federated Learning), for solving a distributed optimization problem in federated learning. Our approach is based on a specific dual formulation of the federated learning problem. DualFL achieves communication acceleration under various settings on smoothness and strong convexity of the problem. Moreover, it theoretically guarantees the use of inexact local solvers, preserving its optimal communication complexity even with inexact local solutions. DualFL is the first federated learning algorithm that achieves communication acceleration, even when the cost function is either nonsmooth or non-strongly convex. Numerical results demonstrate that the practical performance of DualFL is comparable to those of state-of-the-art federated learning algorithms, and it is robust with respect to hyperparameter tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#36890;&#36807;&#28151;&#21512;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#26631;&#31614;&#26469;&#22686;&#24378;&#26679;&#26412;&#30340;&#31574;&#30053;&#12290;&#26032;&#30340;&#20998;&#31867;&#22120;&#26159;&#36755;&#20837;&#23545;&#20998;&#31867;&#22120;&#21521;&#37327;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20351;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10293</link><description>&lt;p&gt;
&#26080;&#38480;&#31867;&#21035;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Infinite Class Mixup. (arXiv:2305.10293v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#36890;&#36807;&#28151;&#21512;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#26631;&#31614;&#26469;&#22686;&#24378;&#26679;&#26412;&#30340;&#31574;&#30053;&#12290;&#26032;&#30340;&#20998;&#31867;&#22120;&#26159;&#36755;&#20837;&#23545;&#20998;&#31867;&#22120;&#21521;&#37327;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20351;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup &#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#25554;&#20540;&#36755;&#20837;&#21644;&#26631;&#31614;&#30340;&#35757;&#32451;&#23545;&#26469;&#22686;&#21152;&#39069;&#22806;&#30340;&#26679;&#26412;&#12290; Mixup &#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12289;&#32593;&#32476;&#26657;&#20934;&#21644;&#36229;&#20986;&#20998;&#24067;&#27010;&#25324;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;Mixup&#30340;&#19968;&#20010;&#22522;&#30707;&#26159;&#32593;&#32476;&#22312;&#31867;&#21035;&#20043;&#38388;&#23398;&#20064;&#32447;&#24615;&#34892;&#20026;&#27169;&#24335;&#65292;&#20294;&#23427;&#21482;&#26159;&#38388;&#25509;&#22320;&#36890;&#36807;&#27010;&#29575;&#32423;&#21035;&#36827;&#34892;&#36755;&#20986;&#25554;&#20540;&#32780;&#24378;&#21046;&#25191;&#34892;&#12290;&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#28151;&#21512;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#28151;&#21512;&#27599;&#20010;&#28151;&#21512;&#23545;&#30340;&#26631;&#31614;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#27599;&#20010;&#22686;&#24378;&#30340;&#26679;&#26412;&#30340;&#30446;&#26631;&#23450;&#20041;&#20026;&#19968;&#20010;&#21807;&#19968;&#30340;&#26032;&#20998;&#31867;&#22120;&#65292;&#20854;&#21442;&#25968;&#26159;&#36755;&#20837;&#23545;&#30340;&#20998;&#31867;&#22120;&#21521;&#37327;&#30340;&#32447;&#24615;&#25554;&#20540;&#12290;&#25152;&#26377;&#21487;&#33021;&#20998;&#31867;&#22120;&#30340;&#31354;&#38388;&#26159;&#36830;&#32493;&#30340;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#22120;&#23545;&#20043;&#38388;&#30340;&#25152;&#26377;&#25554;&#20540;&#12290;&#20026;&#20102;&#20351;&#20248;&#21270;&#21487;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23545;&#27604;&#26080;&#38480;&#31867;&#28151;&#21512;&#25439;&#22833;&#65292;&#20854;&#20013;&#25105;&#20204;&#23545;&#27599;&#20010;&#26679;&#26412;&#23545;&#21521;&#25152;&#26377;&#20854;&#20182;&#26679;&#26412;&#23545;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a widely adopted strategy for training deep networks, where additional samples are augmented by interpolating inputs and labels of training pairs. Mixup has shown to improve classification performance, network calibration, and out-of-distribution generalisation. While effective, a cornerstone of Mixup, namely that networks learn linear behaviour patterns between classes, is only indirectly enforced since the output interpolation is performed at the probability level. This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair. We propose to define the target of each augmented sample as a uniquely new classifier, whose parameters are a linear interpolation of the classifier vectors of the input pair. The space of all possible classifiers is continuous and spans all interpolations between classifier pairs. To make optimisation tractable, we propose a dual-contrastive Infinite Class Mixup loss, where we contrast 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#19977;&#38454;&#27573;&#28151;&#21512;RL&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22870;&#21169;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22312;&#32447;&#21644;&#31163;&#32447;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#32454;&#35843;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10282</link><description>&lt;p&gt;
&#8220;&#26080;&#20219;&#20309;&#22870;&#21169;&#20449;&#24687;&#30340;&#32454;&#35843; Fine-tuning:&#22522;&#20110;&#28151;&#21512;&#22686;&#24378;&#23398;&#20064;&#30340;&#21487;&#35777;&#26126;&#32479;&#35745;&#20248;&#21183;&#8221;
&lt;/p&gt;
&lt;p&gt;
Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning. (arXiv:2305.10282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#19977;&#38454;&#27573;&#28151;&#21512;RL&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22870;&#21169;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22312;&#32447;&#21644;&#31163;&#32447;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#32454;&#35843;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28151;&#21512;&#29615;&#22659;&#20013;&#36827;&#34892;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;(RL)&#65292;&#35813;&#29615;&#22659;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#31163;&#32447;&#25968;&#25454;&#38598;&#24182;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#12290;&#20854;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#21033;&#29992;&#22312;&#32447;&#25968;&#25454;&#25910;&#38598;&#26469;&#21152;&#24378;&#21644;&#34917;&#20805;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#31574;&#30053;&#32454;&#35843;&#12290;&#26412;&#25991;&#20511;&#37492;&#20102;&#26368;&#36817;&#30340;&#26080;&#22870;&#21169;&#25506;&#32034;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;RL &#30340;&#36827;&#23637;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#28151;&#21512;RL&#31639;&#27861;&#65292;&#20854;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20165;&#20351;&#29992;&#31163;&#32447;RL &#21644;&#20165;&#20351;&#29992;&#22312;&#32447;RL &#30340;&#26368;&#20339;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#22870;&#21169;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26159;&#22522;&#20110;&#19968;&#20010;&#26032;&#27010;&#24565;&#8212;&#8212;&#21333;&#31574;&#30053;&#23616;&#37096;&#38598;&#20013;&#24615;&#30340;&#65292;&#35813;&#27010;&#24565;&#25429;&#25417;&#20102;&#20998;&#24067;&#19981;&#21305;&#37197;&#21644;&#35206;&#30422;&#38169;&#35823;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25351;&#23548;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies tabular reinforcement learning (RL) in the hybrid setting, which assumes access to both an offline dataset and online interactions with the unknown environment. A central question boils down to how to efficiently utilize online data collection to strengthen and complement the offline dataset and enable effective policy fine-tuning. Leveraging recent advances in reward-agnostic exploration and model-based offline RL, we design a three-stage hybrid RL algorithm that beats the best of both worlds -- pure offline RL and pure online RL -- in terms of sample complexities. The proposed algorithm does not require any reward information during data collection. Our theory is developed based on a new notion called single-policy partial concentrability, which captures the trade-off between distribution mismatch and miscoverage and guides the interplay between offline and online data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.10272</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Package Manipulation via Learned Metrics of Pick Success. (arXiv:2305.10272v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#21487;&#20197;&#38477;&#20302;&#29289;&#27969;&#25104;&#26412;&#65292;&#26368;&#32456;&#38477;&#20302;&#28040;&#36153;&#21697;&#20215;&#26684;&#65292;&#25552;&#39640;&#20132;&#36135;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#23545;&#21171;&#21160;&#21147;&#27874;&#21160;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#20294;&#22823;&#22810;&#25968;&#26159;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#12290;&#20174;&#26434;&#20081;&#30340;&#22534;&#22534;&#20013;&#25361;&#36873;&#29289;&#21697;&#31561;&#20219;&#21153;&#30452;&#21040;&#26368;&#36817;&#25165;&#21464;&#24471;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#30340;Robot Induction&#65288;Robin&#65289;&#32676;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#35813;&#32676;&#21033;&#29992;&#22312;&#23454;&#38469;&#29983;&#20135;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#22312;&#36229;&#36807;394K&#20010;&#25342;&#21462;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#23427;&#29992;&#20110;&#25226;&#27599;&#22825;&#39640;&#36798;5&#30334;&#19975;&#20010;&#21253;&#35065;&#36827;&#34892;&#20102;&#20998;&#31163;&#65292;&#26412;&#25991;&#30340;&#35780;&#20272;&#26399;&#38388;&#25805;&#20316;&#20102;&#36229;&#36807;2&#20159;&#20010;&#21253;&#35065;&#12290;&#24320;&#21457;&#30340;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#23454;&#26102;&#25490;&#21517;&#21508;&#31181;&#25342;&#21462;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#39640;&#25104;&#21151;&#29575;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.  This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.  The developed learned pick quality measure ranks various pick alternatives in real-time and p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#26041;&#27861;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10267</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
State Representation Learning Using an Unbalanced Atlas. (arXiv:2305.10267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#26041;&#27861;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35828;&#35748;&#20026;&#65292;&#39640;&#32500;&#25968;&#25454;&#36890;&#24120;&#20301;&#20110;&#36739;&#20302;&#32500;&#30340;&#27969;&#24418;&#19978;&#65292;&#24182;&#19988;&#21033;&#29992;&#35813;&#27969;&#24418;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#21487;&#20197;&#20135;&#29983;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#25216;&#26415;&#29992;&#20110;&#38477;&#32500;&#65292;&#20294;&#23427;&#20204;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#12290;&#26368;&#36817;&#30340;MSIMCLR&#26041;&#27861;&#23558;&#27969;&#24418;&#32534;&#30721;&#19982;SimCLR&#30456;&#32467;&#21512;&#65292;&#20294;&#38656;&#35201;&#26497;&#20302;&#30340;&#30446;&#26631;&#32534;&#30721;&#32500;&#24230;&#25165;&#33021;&#32988;&#36807;SimCLR&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#35843;&#25972;&#26102;&#31354;DeepInfomax&#65288;ST-DIM&#65289;&#26694;&#26550;&#20197;&#19982;&#25105;&#20204;&#25552;&#35758;&#30340;UA&#27169;&#24335;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#37319;&#29992;&#20005;&#35880;&#30340;&#31185;&#23398;&#26041;&#27861;&#26469;&#31934;&#24515;&#30740;&#31350;&#21644;&#35774;&#35745;&#20102;&#20351;&#29992;UA&#30340;DeepInfomax&#65288;DIM-UA&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold hypothesis posits that high-dimensional data often lies on a lower-dimensional manifold and that utilizing this manifold as the target space yields more efficient representations. While numerous traditional manifold-based techniques exist for dimensionality reduction, their application in self-supervised learning has witnessed slow progress. The recent MSIMCLR method combines manifold encoding with SimCLR but requires extremely low target encoding dimensions to outperform SimCLR, limiting its applicability. This paper introduces a novel learning paradigm using an unbalanced atlas (UA), capable of surpassing state-of-the-art self-supervised learning approaches. We meticulously investigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA) method by systematically adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align with our proposed UA paradigm, employing rigorous scientific methodologies throughout the process. The efficacy of DIM-UA is demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#19979;&#28216;&#20219;&#21153;&#65292;&#24320;&#21457;&#20102;&#38160;&#24230;&#19982;&#25968;&#25454;&#20301;&#31227;&#24863;&#30693;&#29702;&#35770;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;(SSA-CLR)&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#38160;&#24230;&#26368;&#23567;&#21270;&#19982;&#20613;&#37324;&#21494;&#21464;&#25442;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#32531;&#35299;&#29702;&#24819;&#20998;&#24067;&#19982;&#23454;&#38469;&#20998;&#24067;&#38388;&#30340;&#25968;&#25454;&#20301;&#31227;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.10252</link><description>&lt;p&gt;
&#38160;&#24230;&#19982;&#20301;&#31227;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sharpness &amp; Shift-Aware Self-Supervised Learning. (arXiv:2305.10252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#19979;&#28216;&#20219;&#21153;&#65292;&#24320;&#21457;&#20102;&#38160;&#24230;&#19982;&#25968;&#25454;&#20301;&#31227;&#24863;&#30693;&#29702;&#35770;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;(SSA-CLR)&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#38160;&#24230;&#26368;&#23567;&#21270;&#19982;&#20613;&#37324;&#21494;&#21464;&#25442;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#32531;&#35299;&#29702;&#24819;&#20998;&#24067;&#19982;&#23454;&#38469;&#20998;&#24067;&#38388;&#30340;&#25968;&#25454;&#20301;&#31227;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#20197;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#25991;&#32771;&#34385;&#20998;&#31867;&#20316;&#20026;&#31532;&#20108;&#38454;&#27573;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#20005;&#23494;&#30340;&#29702;&#35770;&#26469;&#23454;&#29616;&#24433;&#21709;&#35813;&#20998;&#31867;&#20219;&#21153;&#30340;&#38544;&#21547;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#38160;&#24230;&#24863;&#30693;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#26377;&#21033;&#20110;&#31532;&#20108;&#38454;&#27573;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#29702;&#35770;&#24320;&#21457;&#20013;&#30340;&#29702;&#24819;&#20998;&#24067;&#19982;&#23454;&#38469;&#23454;&#29616;&#20013;&#30340;&#23454;&#38469;&#20998;&#24067;&#20043;&#38388;&#30340;&#25968;&#25454;&#20301;&#31227;&#20063;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#35813;&#20998;&#31867;&#20219;&#21153;&#12290;&#36827;&#19968;&#27493;&#21033;&#29992;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#38160;&#24230;&#21644;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#23454;&#29616;&#20013;&#20998;&#24067;&#20301;&#31227;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38160;&#24230;&#21644;&#20301;&#31227;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;(SSA-CLR)&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning aims to extract meaningful features from unlabeled data for further downstream tasks. In this paper, we consider classification as a downstream task in phase 2 and develop rigorous theories to realize the factors that implicitly influence the general loss of this classification task. Our theories signify that sharpness-aware feature extractors benefit the classification task in phase 2 and the existing data shift between the ideal (i.e., the ideal one used in theory development) and practical (i.e., the practical one used in implementation) distributions to generate positive pairs also remarkably affects this classification task. Further harvesting these theoretical findings, we propose to minimize the sharpness of the feature extractor and a new Fourier-based data augmentation technique to relieve the data shift in the distributions generating positive pairs, reaching Sharpness &amp; Shift-Aware Contrastive Learning (SSA-CLR). We conduct extensive experiments to v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IoT&#33021;&#37327;&#26381;&#21153;&#30340;&#33021;&#37327;&#25439;&#22833;&#39044;&#27979;&#26694;&#26550;ELP&#65292;&#22522;&#20110;Easeformer&#31639;&#27861;&#29992;&#20110;&#39044;&#27979;&#20849;&#20139;&#33021;&#37327;&#29615;&#22659;&#20013;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#30005;&#27744;&#30005;&#37327;&#65292;&#36827;&#32780;&#20272;&#31639;&#33021;&#37327;&#25439;&#22833;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10238</link><description>&lt;p&gt;
IoT&#33021;&#37327;&#26381;&#21153;&#20013;&#33021;&#37327;&#25439;&#22833;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Energy Loss Prediction in IoT Energy Services. (arXiv:2305.10238v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IoT&#33021;&#37327;&#26381;&#21153;&#30340;&#33021;&#37327;&#25439;&#22833;&#39044;&#27979;&#26694;&#26550;ELP&#65292;&#22522;&#20110;Easeformer&#31639;&#27861;&#29992;&#20110;&#39044;&#27979;&#20849;&#20139;&#33021;&#37327;&#29615;&#22659;&#20013;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#30005;&#27744;&#30005;&#37327;&#65292;&#36827;&#32780;&#20272;&#31639;&#33021;&#37327;&#25439;&#22833;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33021;&#37327;&#25439;&#22833;&#39044;&#27979;&#65288;ELP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#20139;&#20247;&#21253;&#33021;&#28304;&#26381;&#21153;&#20013;&#30340;&#33021;&#37327;&#25439;&#22833;&#12290;&#20247;&#21253;&#26080;&#32447;&#33021;&#28304;&#26381;&#21153;&#26159;&#19968;&#31181;&#26032;&#39062;&#21644;&#20415;&#21033;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23454;&#29616;&#38468;&#36817;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#26222;&#36941;&#20805;&#30005;&#12290;&#22240;&#27492;&#65292;&#25429;&#25417;&#26080;&#32447;&#33021;&#28304;&#20998;&#20139;&#25439;&#22833;&#23545;&#20110;&#25104;&#21151;&#37096;&#32626;&#26377;&#25928;&#30340;&#33021;&#28304;&#26381;&#21153;&#32452;&#21512;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Easeformer&#65292;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20849;&#20139;&#33021;&#37327;&#29615;&#22659;&#20013;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#30005;&#27744;&#30005;&#37327;&#12290;&#39044;&#27979;&#30340;&#30005;&#27744;&#30005;&#37327;&#29992;&#20110;&#20272;&#31639;&#33021;&#37327;&#25439;&#22833;&#12290;&#19968;&#31995;&#21015;&#23454;&#39564;&#34987;&#36827;&#34892;&#26469;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#26080;&#32447;&#33021;&#28304;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel Energy Loss Prediction(ELP) framework that estimates the energy loss in sharing crowdsourced energy services. Crowdsourcing wireless energy services is a novel and convenient solution to enable the ubiquitous charging of nearby IoT devices. Therefore, capturing the wireless energy sharing loss is essential for the successful deployment of efficient energy service composition techniques. We propose Easeformer, a novel attention-based algorithm to predict the battery levels of IoT devices in a crowdsourced energy sharing environment. The predicted battery levels are used to estimate the energy loss. A set of experiments were conducted to demonstrate the feasibility and effectiveness of the proposed framework. We conducted extensive experiments on real wireless energy datasets to demonstrate that our framework significantly outperforms existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.10235</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#30340;&#38544;&#34255;&#39118;&#38505;&#65306;&#20851;&#20110;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#24320;&#25918;&#24335;&#29615;&#22659;&#65288;&#22914;API&#12289;&#24320;&#28304;&#27169;&#22411;&#21644;&#25554;&#20214;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#32570;&#20047;&#20840;&#38754;&#35752;&#35770;&#21644;&#20998;&#26512;&#28508;&#22312;&#39118;&#38505;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#20294;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;LLMs&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#26469;&#22788;&#29702;&#22823;&#37327;&#26597;&#35810;/&#21709;&#24212;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;OPT&#22312;&#20869;&#30340;&#20027;&#27969;LLMs&#36827;&#34892;&#20102;100&#22810;&#19975;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#26680;&#24515;&#21253;&#25324;&#25968;&#25454;&#21407;&#35821;&#65292;&#38543;&#21518;&#26159;&#33258;&#21160;&#35299;&#37322;&#22120;&#65292;&#35780;&#20272;&#36825;&#20123;LLMs&#22312;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#24230;&#37327;&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#12289;&#20063;&#35768;&#26159;&#19981;&#24184;&#30340;&#32467;&#35770;&#65292;&#36825;&#20123;&#32467;&#35770;&#30456;&#24403;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#30340;&#31751;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#20294;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#35777;&#26126;&#20102;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.10229</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65306;&#20174;&#32858;&#31867;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Exploring Inductive Biases in Contrastive Learning: A Clustering Perspective. (arXiv:2305.10229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#30340;&#31751;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#20294;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#35777;&#26126;&#20102;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#25968;&#25454;&#32452;&#32455;&#30340;&#24046;&#24322;&#65292;&#37325;&#28857;&#20851;&#27880;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#30456;&#23545;&#23616;&#37096;&#23494;&#24230;&#65288;RLD&#65289;&#65292;&#29992;&#20110;&#23450;&#37327;&#27979;&#37327;&#31751;&#20869;&#30340;&#23616;&#37096;&#23494;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35270;&#35273;&#31034;&#20363;&#65292;&#20197;&#31361;&#20986;&#23616;&#37096;&#23494;&#38598;&#31751;&#21644;&#20840;&#23616;&#23494;&#38598;&#31751;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#36890;&#36807;&#23545;&#27604;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#24418;&#25104;&#30340;&#31751;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#32780;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#26469;&#35777;&#26126;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#32467;&#26463;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the differences in data organization between contrastive and supervised learning methods, focusing on the concept of locally dense clusters. We introduce a novel metric, Relative Local Density (RLD), to quantitatively measure local density within clusters. Visual examples are provided to highlight the distinctions between locally dense clusters and globally dense ones. By comparing the clusters formed by contrastive and supervised learning, we reveal that contrastive learning generates locally dense clusters without global density, while supervised learning creates clusters with both local and global density. We further explore the use of a Graph Convolutional Network (GCN) classifier as an alternative to linear classifiers for handling locally dense clusters. Finally, we utilize t-SNE visualizations to substantiate the differences between the features generated by contrastive and supervised learning methods. We conclude by proposing future research directions, 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#23567;&#30340;&#30772;&#22351;&#33410;&#28857;&#27604;&#20363;&#19979;&#21363;&#21487;&#36798;&#21040;Kesten-Stigum&#20020;&#30028;&#28857;&#30340;&#24369;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2305.10227</link><description>&lt;p&gt;
&#22312;&#33410;&#28857;&#30772;&#22351;&#19979;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#36798;&#21040;Kesten-Stigum&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Reaching Kesten-Stigum Threshold in the Stochastic Block Model under Node Corruptions. (arXiv:2305.10227v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10227
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#23567;&#30340;&#30772;&#22351;&#33410;&#28857;&#27604;&#20363;&#19979;&#21363;&#21487;&#36798;&#21040;Kesten-Stigum&#20020;&#30028;&#28857;&#30340;&#24369;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#30772;&#22351;&#38543;&#26426;&#22359;&#27169;&#22411;&#19979;&#30340;&#24378;&#20581;&#31038;&#21306;&#26816;&#27979;&#65292;&#20854;&#20013;&#23545;&#20110;$n$&#20010;&#39030;&#28857;&#20013;&#30340;&#19968;&#20010;&#37096;&#20998;&#20855;&#26377;&#20219;&#24847;&#20462;&#25913;&#36793;&#32536;&#26435;&#37325;&#30340;&#25932;&#25163;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#23567;&#30340;&#30772;&#22351;&#33410;&#28857;&#27604;&#20363;&#19979;&#21363;&#21487;&#36798;&#21040;Kesten-Stigum&#20020;&#30028;&#28857;&#30340;&#24369;&#24674;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20043;&#21069;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#24378;&#20581;&#24615;&#31639;&#27861;&#65292;&#22312;&#25509;&#36817;Kesten-Stigum&#20020;&#30028;&#28857;&#26102;&#20063;&#20250;&#34987;&#33410;&#28857;&#30772;&#22351;&#25932;&#25163;&#25915;&#30772;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#25193;&#23637;&#21040;$Z_2$&#21516;&#27493;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23384;&#22312;&#31867;&#20284;&#30340;&#24378;&#30772;&#22351;&#25932;&#25163;&#27874;&#21160;&#26102;&#20063;&#33021;&#36798;&#21040;&#26368;&#20248;&#24674;&#22797;&#38408;&#20540;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;&#35777;&#26126;&#65292;&#21033;&#29992;&#20027;&#23376;&#30697;&#38453;&#30340;Grothendieck&#33539;&#25968;&#25512;&#20986;&#30340;&#25512;&#20986;&#12288;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study robust community detection in the context of node-corrupted stochastic block model, where an adversary can arbitrarily modify all the edges incident to a fraction of the $n$ vertices. We present the first polynomial-time algorithm that achieves weak recovery at the Kesten-Stigum threshold even in the presence of a small constant fraction of corrupted nodes. Prior to this work, even state-of-the-art robust algorithms were known to break under such node corruption adversaries, when close to the Kesten-Stigum threshold.  We further extend our techniques to the $Z_2$ synchronization problem, where our algorithm reaches the optimal recovery threshold in the presence of similar strong adversarial perturbations.  The key ingredient of our algorithm is a novel identifiability proof that leverages the push-out effect of the Grothendieck norm of principal submatrices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#35782;&#21035;&#20844;&#20849;&#25968;&#25454;&#38598;WISDM&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#38477;&#20302;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#21644;&#20449;&#20219;&#24230;&#12290;&#36890;&#36807;&#20462;&#22797;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10222</link><description>&lt;p&gt;
rWISDM&#65306;&#20462;&#22797;&#21518;&#30340;&#34892;&#20026;&#35782;&#21035;&#20844;&#20849;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
rWISDM: Repaired WISDM, a Public Dataset for Human Activity Recognition. (arXiv:2305.10222v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#35782;&#21035;&#20844;&#20849;&#25968;&#25454;&#38598;WISDM&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#38477;&#20302;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#21644;&#20449;&#20219;&#24230;&#12290;&#36890;&#36807;&#20462;&#22797;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#24050;&#25104;&#20026;&#36817;&#24180;&#26469;&#31185;&#23398;&#30740;&#31350;&#30340;&#20142;&#28857;&#65292;&#22240;&#20026;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#31454;&#25216;&#27604;&#36187;&#12289;&#26234;&#24935;&#22478;&#24066;&#21644;&#26234;&#33021;&#23478;&#23621;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#19987;&#27880;&#20110;&#22788;&#29702;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#29992;&#25143;&#20173;&#22312;&#30097;&#24785;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26159;&#21542;&#21487;&#20449;&#12290;&#20449;&#20219;&#20027;&#35201;&#21462;&#20915;&#20110;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#25110;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35843;&#26597;HAR&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#20010;&#21512;&#36866;&#30340;&#24403;&#21069;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#24182;&#36873;&#25321;WISDM&#36827;&#34892;&#25105;&#20204;&#23545;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#34429;&#28982;WISDM&#21457;&#24067;&#30340;&#35268;&#26684;&#31526;&#21512;&#25105;&#20204;&#30340;&#22522;&#26412;&#35201;&#27714;&#65288;&#22914;&#22823;&#22411;&#12289;&#24179;&#34913;&#12289;&#21487;&#22810;&#30828;&#20214;&#25805;&#20316;&#65289;&#65292;&#20294;&#22312;&#20998;&#26512;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#38544;&#34255;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#38477;&#20302;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#21644;&#25972;&#20307;&#20449;&#20219;&#24230;&#12290;&#36890;&#36807;&#35782;&#21035;&#38382;&#39064;&#24182;&#20462;&#22797;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#29992;&#20110;&#20462;&#22797;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) has become a spotlight in recent scientific research because of its applications in various domains such as healthcare, athletic competitions, smart cities, and smart home. While researchers focus on the methodology of processing data, users wonder if the Artificial Intelligence (AI) methods used for HAR can be trusted. Trust depends mainly on the reliability or robustness of the system. To investigate the robustness of HAR systems, we analyzed several suitable current public datasets and selected WISDM for our investigation of Deep Learning approaches. While the published specification of WISDM matched our fundamental requirements (e.g., large, balanced, multi-hardware), several hidden issues were found in the course of our analysis. These issues reduce the performance and the overall trust of the classifier. By identifying the problems and repairing the dataset, the performance of the classifier was increased. This paper presents the methods by which 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#30340;&#21487;&#20998;&#24615;&#21644;&#31163;&#25955;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;S&amp;S&#27604;&#30340;&#26377;&#25928;SVM&#27491;&#21017;&#21270;&#21442;&#25968;&#12289;&#26680;&#20989;&#25968;&#21644;&#26680;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;&#65292;&#34920;&#29616;&#36739;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.10219</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#20998;&#24615;&#21644;&#31163;&#25955;&#24230;&#27604;&#30340;SVM&#27491;&#21017;&#21270;&#21442;&#25968;&#12289;&#26680;&#20989;&#25968;&#21644;&#26680;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Separability and Scatteredness (S&amp;S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection. (arXiv:2305.10219v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#30340;&#21487;&#20998;&#24615;&#21644;&#31163;&#25955;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;S&amp;S&#27604;&#30340;&#26377;&#25928;SVM&#27491;&#21017;&#21270;&#21442;&#25968;&#12289;&#26680;&#20989;&#25968;&#21644;&#26680;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;&#65292;&#34920;&#29616;&#36739;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#24322;&#24120;&#20540;&#26816;&#27979;&#12290;SVM&#38656;&#35201;&#35843;&#25972;&#27491;&#21017;&#21270;&#21442;&#25968;&#65288;RP&#65289;&#26469;&#25511;&#21046;&#27169;&#22411;&#23481;&#37327;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#36807;&#31243;&#23545;&#19968;&#31995;&#21015;&#22791;&#36873;RP&#36827;&#34892;&#27604;&#36739;&#20197;&#25214;&#21040;&#26368;&#20339;RP&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;SVM&#20351;&#29992;&#26680;&#20989;&#25968;&#65292;&#22312;&#26680;&#20989;&#25968;&#30340;&#32593;&#26684;&#20013;&#36873;&#25321;&#19968;&#32452;&#20855;&#26377;&#19968;&#32452;&#21442;&#25968;&#30340;&#26680;&#20989;&#25968;&#12290;RP&#21644;&#26680;&#32593;&#26684;&#30340;&#26368;&#20339;&#36873;&#25321;&#26159;&#36890;&#36807;CV&#30340;&#32593;&#26684;&#25628;&#32034;&#33719;&#24471;&#30340;&#12290;&#36890;&#36807;&#38543;&#26426;&#20998;&#26512;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#34892;&#20026;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;SVM&#24615;&#33021;&#21487;&#20197;&#24314;&#27169;&#20026;&#25968;&#25454;&#30340;&#21487;&#20998;&#24615;&#21644;&#31163;&#25955;&#24230;&#65288;S&amp;S&#65289;&#30340;&#20989;&#25968;&#12290;&#21487;&#20998;&#24615;&#26159;&#31867;&#21035;&#20043;&#38388;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#31163;&#25955;&#24230;&#26159;&#25968;&#25454;&#28857;&#30340;&#20256;&#25773;&#27604;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#38128;&#38142;&#25439;&#22833;&#25104;&#26412;&#20989;&#25968;&#65292;S&amp;S&#27604;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;RP&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;S&amp;S&#27604;&#30340;&#39640;&#25928;&#36873;&#25321;&#26680;&#20989;&#25968;&#21450;&#20854;&#21442;&#25968;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#26041;&#27861;&#20855;&#26377;&#26356;&#23569;&#30340;&#38656;&#35201;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#19988;&#24615;&#33021;&#20248;&#24322;&#25110;&#21487;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning the regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels where a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through the grid-search of CV. By stochastically analyzing the behavior of the regularization parameter, this work shows that the SVM performance can be modeled as a function of separability and scatteredness (S&amp;S) of the data. Separability is a measure of the distance between classes, and scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#38543;&#26426;LSTM&#27169;&#22411;&#65292;&#21463;&#21040;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25506;&#35752;&#22312;&#38750;&#37327;&#23376;&#26694;&#26550;&#19979;&#25509;&#36817;QML&#30340;&#25253;&#21578;&#25104;&#21151;&#21644;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2305.10212</link><description>&lt;p&gt;
&#19968;&#31181;&#21463;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#22411;&#38543;&#26426;LSTM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Novel Stochastic LSTM Model Inspired by Quantum Machine Learning. (arXiv:2305.10212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#38543;&#26426;LSTM&#27169;&#22411;&#65292;&#21463;&#21040;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25506;&#35752;&#22312;&#38750;&#37327;&#23376;&#26694;&#26550;&#19979;&#25509;&#36817;QML&#30340;&#25253;&#21578;&#25104;&#21151;&#21644;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;(QML)&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;QML&#31639;&#27861;&#19981;&#20165;&#21487;&#20197;&#20687;&#20256;&#32479;&#31639;&#27861;&#19968;&#26679;&#21457;&#25381;&#20854;&#21151;&#33021;&#65292;&#32780;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#21487;&#20197;&#36229;&#36234;&#20854;&#34920;&#29616;&#12290;&#22312;&#20247;&#22810;&#30740;&#31350;&#20013;&#65292;&#35768;&#22810;QML&#27169;&#22411;&#21033;&#29992;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;(VQA)&#30005;&#36335;&#65292;&#22240;&#20026;&#20854;&#35268;&#27169;&#36890;&#24120;&#36275;&#22815;&#23567;&#65292;&#21487;&#20197;&#19982;NISQ&#35774;&#22791;&#20860;&#23481;&#65292;&#24182;&#19988;&#29992;&#20110;&#20248;&#21270;&#30005;&#36335;&#21442;&#25968;&#30340;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#29087;&#24713;&#20110;&#26426;&#22120;&#23398;&#20064;(ML)&#12290;&#23613;&#31649;&#36825;&#20123;&#32467;&#26524;&#22312;&#37327;&#23376;&#26426;&#22120;&#26356;&#26131;&#20110;&#20351;&#29992;&#30340;&#26102;&#20195;&#65292;&#24102;&#26469;&#20102;&#26377;&#36259;&#30340;&#21069;&#26223;&#65292;&#20294;&#22914;&#26524;&#36890;&#36807;&#38750;&#37327;&#23376;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#37027;&#20040;&#20174;&#19994;&#32773;&#21487;&#33021;&#20250;&#33719;&#24471;&#26356;&#36817;&#26399;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21463;&#21464;&#20998;&#37327;&#23376;LSTM&#27169;&#22411;&#21551;&#21457;&#30340;&#38543;&#26426;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#20197;&#35797;&#22270;&#22312;&#38750;&#37327;&#23376;&#26694;&#26550;&#20013;&#25509;&#36817;QML&#30340;&#25253;&#21578;&#25104;&#21151;&#21644;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Works in quantum machine learning (QML) over the past few years indicate that QML algorithms can function just as well as their classical counterparts, and even outperform them in some cases. Among the corpus of recent work, many current QML models take advantage of variational quantum algorithm (VQA) circuits, given that their scale is typically small enough to be compatible with NISQ devices and the method of automatic differentiation for optimizing circuit parameters is familiar to machine learning (ML). While the results bear interesting promise for an era when quantum machines are more readily accessible, if one can achieve similar results through non-quantum methods then there may be a more near-term advantage available to practitioners. To this end, the nature of this work is to investigate the utilization of stochastic methods inspired by a variational quantum version of the long short-term memory (LSTM) model in an attempt to approach the reported successes in performance and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#20877;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#29992;&#20110;&#28857;&#20113;ReID&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#21644;&#35266;&#27979;&#28857;&#23494;&#24230;&#30340;&#22686;&#21152;&#65292;&#28857;&#20113;ReID&#30340;&#34920;&#29616;&#36880;&#28176;&#25509;&#36817;&#20110;&#22270;&#20687;ReID&#12290;</title><link>http://arxiv.org/abs/2305.10210</link><description>&lt;p&gt;
&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#30446;&#26631;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Towards Object Re-Identification from Point Clouds for 3D MOT. (arXiv:2305.10210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#20877;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#29992;&#20110;&#28857;&#20113;ReID&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#21644;&#35266;&#27979;&#28857;&#23494;&#24230;&#30340;&#22686;&#21152;&#65292;&#28857;&#20113;ReID&#30340;&#34920;&#29616;&#36880;&#28176;&#25509;&#36817;&#20110;&#22270;&#20687;ReID&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#20174;&#21098;&#35009;&#30340;&#28857;&#20113;&#35266;&#27979;&#20013;&#21305;&#37197;&#23545;&#35937;&#23545;&#65288;&#20363;&#22914;&#20351;&#29992;&#20854;&#39044;&#27979;&#30340;&#19977;&#32500;&#36793;&#30028;&#26694;&#65289;&#26469;&#35299;&#20915;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#19978;&#30340;&#23545;&#35937;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#38382;&#39064;&#12290; &#25105;&#20204;&#19981;&#20851;&#24515;&#19977;&#32500;MOT&#30340;SOTA&#24615;&#33021;&#65292;&#32780;&#26159;&#36861;&#27714;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#22312;&#23454;&#38469;&#30340;&#36319;&#36394;&#26816;&#27979;&#29615;&#22659;&#20013;&#65292;&#19982;&#22270;&#29255;&#20013;&#30340;ReID&#30456;&#27604;&#65292;&#26469;&#33258;&#28857;&#20113;&#30340;&#23545;&#35937;ReID&#30340;&#34920;&#29616;&#22914;&#20309;&#65311; &#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36830;&#25509;&#21040;&#20219;&#20309;&#38598;&#21512;&#25110;&#24207;&#21015;&#22788;&#29702;&#39592;&#24178;&#65288;&#20363;&#22914;PointNet&#25110;ViT&#65289;&#30340;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#65292;&#20026;&#20004;&#31181;&#27169;&#24577;&#21019;&#36896;&#21487;&#27604;&#36739;&#30340;&#23545;&#35937;ReID&#32593;&#32476;&#23478;&#26063;&#12290;&#22312;&#23402;&#29983;&#26679;&#24335;&#19979;&#36816;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28857;&#20113;ReID&#32593;&#32476;&#21487;&#20197;&#22312;&#23454;&#26102;&#65288;10 hz&#65289;&#20013;&#36827;&#34892;&#25968;&#21315;&#20010;&#25104;&#23545;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#34920;&#29616;&#38543;&#30528;&#26356;&#39640;&#30340;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#32780;&#25552;&#39640;&#65292;&#24182;&#22312;&#35266;&#27979;&#36275;&#22815;&#23494;&#38598;&#26102;&#25509;&#36817;&#22270;&#20687;ReID&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the problem of object re-identification (ReID) in a 3D multi-object tracking (MOT) context, by learning to match pairs of objects from cropped (e.g., using their predicted 3D bounding boxes) point cloud observations. We are not concerned with SOTA performance for 3D MOT, however. Instead, we seek to answer the following question: In a realistic tracking by-detection context, how does object ReID from point clouds perform relative to ReID from images? To enable such a study, we propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in siamese style, our proposed point-cloud ReID networks can make thousands of pairwise comparisons in real-time (10 hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Ad
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20849;&#20139; Attention &#36755;&#20837;&#32467;&#26500;&#20294;&#19981;&#38480;&#20110; Attention &#35745;&#31639;&#30340;&#27169;&#22411;&#31354;&#38388;&#65292;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#35299;&#65292;&#36825;&#26159; Attention &#26080;&#27861;&#39640;&#25928;&#36924;&#36817;&#30340;&#65292;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#32780;&#35328;&#20063;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.10203</link><description>&lt;p&gt;
&#24102;&#26377;&#24847;&#22270;&#30340;&#38190;&#20540;&#26597;&#35810;&#27169;&#22411;&#30340;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring the Space of Key-Value-Query Models with Intention. (arXiv:2305.10203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20849;&#20139; Attention &#36755;&#20837;&#32467;&#26500;&#20294;&#19981;&#38480;&#20110; Attention &#35745;&#31639;&#30340;&#27169;&#22411;&#31354;&#38388;&#65292;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#35299;&#65292;&#36825;&#26159; Attention &#26080;&#27861;&#39640;&#25928;&#36924;&#36817;&#30340;&#65292;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#32780;&#35328;&#20063;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#26368;&#26032;&#31361;&#30772;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#27880;&#24847;&#21147;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20854;&#36755;&#20837;&#30340;&#32467;&#26500;&#65288;&#21253;&#21547;&#38190;&#65292;&#20540;&#21644;&#26597;&#35810;&#65289;&#20197;&#21450;&#36825;&#19977;&#20010;&#37096;&#20998;&#22914;&#20309;&#36827;&#34892;&#32452;&#21512;&#30340;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20849;&#20139;&#19978;&#36848;&#36755;&#20837;&#32467;&#26500;&#20294;&#19981;&#20165;&#38480;&#20110;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#27169;&#22411;&#31354;&#38388;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#31354;&#38388;&#20026;&#38190;-&#20540;-&#26597;&#35810;&#65288;KVQ&#65289;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;Attention&#26080;&#27861;&#39640;&#25928;&#36924;&#36817;&#12289;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#31665;&#23454;&#29616;&#65292;&#24182;&#35299;&#20915;&#31038;&#21306;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#30340;&#20854;&#20182;&#21487;&#22534;&#21472;&#27169;&#22411;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26631;&#20934;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#28385;&#36275;&#36825;&#20123;&#23646;&#24615;&#12290;&#33021;&#22815;&#35745;&#31639;&#36825;&#20010;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#19981;&#20165;&#20016;&#23500;&#20102;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#35745;&#31639;&#38598;&#21512;&#65292;&#32780;&#19988;&#36824;&#34987;&#35777;&#26126;&#26159;&#32447;&#24615;&#22238;&#24402;&#30340;&#19968;&#20010;&#20005;&#26684;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based models have been a key element of many recent breakthroughs in deep learning. Two key components of Attention are the structure of its input (which consists of keys, values and queries) and the computations by which these three are combined. In this paper we explore the space of models that share said input structure but are not restricted to the computations of Attention. We refer to this space as Keys-Values-Queries (KVQ) Space. Our goal is to determine whether there are any other stackable models in KVQ Space that Attention cannot efficiently approximate, which we can implement with our current deep learning toolbox and that solve problems that are interesting to the community. Maybe surprisingly, the solution to the standard least squares problem satisfies these properties. A neural network module that is able to compute this solution not only enriches the set of computations that a neural network can represent but is also provably a strict generalisation of Linear 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#33410;&#24459;&#26469;&#31232;&#30095;&#33033;&#20914;&#32593;&#32476;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#21644;&#35745;&#31639;&#33021;&#37327;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.10191</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#33410;&#24459;&#31232;&#30095;&#33033;&#20914;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sparsifying Spiking Networks through Local Rhythms. (arXiv:2305.10191v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#33410;&#24459;&#26469;&#31232;&#30095;&#33033;&#20914;&#32593;&#32476;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#21644;&#35745;&#31639;&#33021;&#37327;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#30830;&#35748;&#65292;&#22312;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#27599;&#23618;&#20135;&#29983;&#30340;&#35768;&#22810;&#20540;&#37117;&#26159;&#38646;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#23637;&#31034;&#20102;&#33033;&#20914;&#22411;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#20449;&#24687;&#38450;&#27490;&#20256;&#36755;&#20195;&#34920;&#25509;&#36817;&#38646;&#30340;&#33033;&#20914;&#65292;&#20174;&#32780;&#20943;&#23569;&#36825;&#20123;&#32593;&#32476;&#25152;&#38656;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#33021;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#36824;&#23637;&#31034;&#20102;&#29983;&#29289;&#35266;&#23519;&#21040;&#30340;&#33033;&#20914;&#33410;&#24459;&#30340;&#26032;&#39062;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been well-established that within conventional neural networks, many of the values produced at each layer are zero. In this work, I demonstrate that spiking neural networks can prevent the transmission of spikes representing values close to zero using local information. This can reduce the amount of energy required for communication and computation in these networks while preserving accuracy. Additionally, this demonstrates a novel application of biologically observed spiking rhythms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#20195;&#27493;&#20849;&#20139;&#24179;&#21488;&#20013;&#21160;&#24577;&#26465;&#20214;&#20998;&#20301;&#27835;&#30103;&#25928;&#26524;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20010;&#20307;CQTE&#20043;&#21644;&#26469;&#31616;&#21270;&#21160;&#24577;CQTE&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.10187</link><description>&lt;p&gt;
&#35780;&#20272;&#20855;&#26377;&#24212;&#29992;&#20110;&#20195;&#27493;&#20849;&#20139;&#30340;&#21160;&#24577;&#26465;&#20214;&#20998;&#20301;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Evaluating Dynamic Conditional Quantile Treatment Effects with Applications in Ridesharing. (arXiv:2305.10187v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#20195;&#27493;&#20849;&#20139;&#24179;&#21488;&#20013;&#21160;&#24577;&#26465;&#20214;&#20998;&#20301;&#27835;&#30103;&#25928;&#26524;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20010;&#20307;CQTE&#20043;&#21644;&#26469;&#31616;&#21270;&#21160;&#24577;CQTE&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#31185;&#25216;&#20844;&#21496;&#65292;&#22914;Google&#12289;Uber&#21644;Didi&#65292;&#21033;&#29992;&#22312;&#32447;&#23454;&#39564;&#65288;&#20063;&#31216;&#20026;A / B&#27979;&#35797;&#65289;&#35780;&#20272;&#26032;&#25919;&#31574;&#19982;&#29616;&#26377;&#25919;&#31574;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#19978;&#65292;&#20294;&#20559;&#26012;&#21644;&#37325;&#23614;&#30340;&#32467;&#26524;&#20998;&#24067;&#24773;&#20917;&#21487;&#33021;&#20250;&#21463;&#30410;&#20110;&#20854;&#20182;&#26631;&#20934;&#65292;&#20363;&#22914;&#20998;&#20301;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#26469;&#33258;&#20195;&#27493;&#20849;&#20139;&#24179;&#21488;&#30340;&#25968;&#25454;&#26102;&#65292;&#35780;&#20272;&#21160;&#24577;&#20998;&#20301;&#27835;&#30103;&#25928;&#26524;&#65288;QTE&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#28041;&#21450;&#36328;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#26469;&#35745;&#31639;&#22312;&#27835;&#30103;&#29420;&#31435;&#20110;&#29305;&#24449;&#30340;&#26465;&#20214;&#19979;&#30340;QTE&#12290;&#22312;&#29305;&#23450;&#30340;&#27169;&#22411;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21160;&#24577;&#26465;&#20214;QTE&#65288;CQTE&#65289;&#31561;&#20110;&#26102;&#38388;&#19978;&#30340;&#20010;&#20307;CQTE&#20043;&#21644;&#65292;&#23613;&#31649;&#32047;&#31215;&#22870;&#21169;&#30340;&#26465;&#20214;&#20998;&#20301;&#25968;&#19981;&#19968;&#23450;&#31561;&#20110;&#20010;&#20307;&#22870;&#21169;&#30340;&#26465;&#20214;&#20998;&#20301;&#25968;&#20043;&#21644;&#12290;&#36825;&#19968;&#20851;&#38190;&#27934;&#23519;&#21147;&#26174;&#33879;&#31616;&#21270;&#20102;&#21160;&#24577;CQTE&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many modern tech companies, such as Google, Uber, and Didi, utilize online experiments (also known as A/B testing) to evaluate new policies against existing ones. While most studies concentrate on average treatment effects, situations with skewed and heavy-tailed outcome distributions may benefit from alternative criteria, such as quantiles. However, assessing dynamic quantile treatment effects (QTE) remains a challenge, particularly when dealing with data from ride-sourcing platforms that involve sequential decision-making across time and space. In this paper, we establish a formal framework to calculate QTE conditional on characteristics independent of the treatment. Under specific model assumptions, we demonstrate that the dynamic conditional QTE (CQTE) equals the sum of individual CQTEs across time, even though the conditional quantile of cumulative rewards may not necessarily equate to the sum of conditional quantiles of individual rewards. This crucial insight significantly strea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25972;&#25968;&#35268;&#21010;&#30340;&#20132;&#26367;&#20248;&#21270;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#24067;&#23572;&#30697;&#38453;&#20998;&#35299;&#30340;NP&#38590;&#39064;&#65292;&#24182;&#19988;&#20351;&#29992;&#21478;&#19968;&#20010;&#25972;&#25968;&#35268;&#21010;&#23558;&#22810;&#20010;&#35299;&#32452;&#21512;&#25104;&#26368;&#20248;&#35299;&#65292;&#23454;&#39564;&#34920;&#26126;&#31639;&#27861;&#22312;&#20013;&#31561;&#35268;&#27169;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.10185</link><description>&lt;p&gt;
&#24067;&#23572;&#30697;&#38453;&#20998;&#35299;&#30340;&#25972;&#25968;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for Boolean Matrix Factorization using Integer Programming. (arXiv:2305.10185v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25972;&#25968;&#35268;&#21010;&#30340;&#20132;&#26367;&#20248;&#21270;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#24067;&#23572;&#30697;&#38453;&#20998;&#35299;&#30340;NP&#38590;&#39064;&#65292;&#24182;&#19988;&#20351;&#29992;&#21478;&#19968;&#20010;&#25972;&#25968;&#35268;&#21010;&#23558;&#22810;&#20010;&#35299;&#32452;&#21512;&#25104;&#26368;&#20248;&#35299;&#65292;&#23454;&#39564;&#34920;&#26126;&#31639;&#27861;&#22312;&#20013;&#31561;&#35268;&#27169;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#30697;&#38453;&#20998;&#35299;&#65288;BMF&#65289;&#23558;&#19968;&#20010;&#32473;&#23450;&#30340;&#20108;&#36827;&#21046;&#36755;&#20837;&#30697;&#38453;&#36817;&#20284;&#34920;&#31034;&#20026;&#20004;&#20010;&#26356;&#23567;&#30340;&#20108;&#36827;&#21046;&#22240;&#23376;&#30340;&#20056;&#31215;&#12290;&#30456;&#23545;&#20110;&#20351;&#29992;&#26631;&#20934;&#31639;&#26415;&#30340;&#20108;&#36827;&#21046;&#30697;&#38453;&#20998;&#35299;&#65292;BMF&#20351;&#29992;&#24067;&#23572;OR&#21644;&#24067;&#23572;AND&#25805;&#20316;&#36827;&#34892;&#30697;&#38453;&#20056;&#31215;&#36816;&#31639;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20302;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;BMF&#26159;&#19968;&#20010;NP&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#20248;&#21270;&#65288;AO&#65289;&#31574;&#30053;&#65292;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#65288;IP&#65289;&#35299;&#20915;BMF&#20013;&#19968;&#20010;&#22240;&#23376;&#30697;&#38453;&#30340;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#31181;&#21021;&#22987;&#21270;&#22240;&#23376;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21478;&#19968;&#20010;IP&#23558;BMF&#30340;&#22810;&#20010;&#35299;&#32452;&#21512;&#21040;&#26368;&#20248;&#35299;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#31181;&#26032;&#31639;&#27861;&#65306;&#20351;&#29992;AO&#29983;&#25104;&#22810;&#20010;&#35299;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#20197;&#26368;&#20248;&#30340;&#26041;&#24335;&#32452;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#21487;&#22312;GitLab&#19978;&#33719;&#24471;&#65289;&#22312;&#20013;&#31561;&#35268;&#27169;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. As opposed to binary matrix factorization which uses standard arithmetic, BMF uses the Boolean OR and Boolean AND operations to perform matrix products, which leads to lower reconstruction errors. BMF is an NP-hard problem. In this paper, we first propose an alternating optimization (AO) strategy that solves the subproblem in one factor matrix in BMF using an integer program (IP). We also provide two ways to initialize the factors within AO. Then, we show how several solutions of BMF can be combined optimally using another IP. This allows us to come up with a new algorithm: it generates several solutions using AO and then combines them in an optimal way. Experiments show that our algorithms (available on gitlab) outperform the state of the art on medium-scale problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#25289;&#32461;&#33945;&#38598;&#21512;&#20013;&#20934;&#30830;&#24615;&#31867;&#20284;&#30340;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#29305;&#24449;&#20132;&#20114;&#20998;&#25968;&#65288;FIS&#65289;&#26469;&#26816;&#27979;&#29305;&#24449;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#30456;&#36739;&#20110;&#20174;&#21333;&#20010;&#39044;&#20808;&#25351;&#23450;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#29305;&#24449;&#20132;&#20114;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#26356;&#20026;&#21487;&#38752;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.10181</link><description>&lt;p&gt;
&#25506;&#32034;&#25289;&#32461;&#33945;&#38598;&#21512;&#20013;&#29305;&#24449;&#20132;&#20114;&#24471;&#20998;&#30340;&#20113;
&lt;/p&gt;
&lt;p&gt;
Exploring the cloud of feature interaction scores in a Rashomon set. (arXiv:2305.10181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#25289;&#32461;&#33945;&#38598;&#21512;&#20013;&#20934;&#30830;&#24615;&#31867;&#20284;&#30340;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#29305;&#24449;&#20132;&#20114;&#20998;&#25968;&#65288;FIS&#65289;&#26469;&#26816;&#27979;&#29305;&#24449;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#30456;&#36739;&#20110;&#20174;&#21333;&#20010;&#39044;&#20808;&#25351;&#23450;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#29305;&#24449;&#20132;&#20114;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#26356;&#20026;&#21487;&#38752;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30340;&#26680;&#24515;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20010;&#39044;&#27979;&#27169;&#22411;&#20013;&#26816;&#27979;&#21644;&#37327;&#21270;&#29305;&#24449;&#20132;&#20114;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#20174;&#21333;&#20010;&#39044;&#20808;&#25351;&#23450;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20132;&#20114;&#21487;&#33021;&#19981;&#21487;&#20449;&#65292;&#22240;&#20026;&#65306;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#19981;&#20250;&#20445;&#30041;&#30495;&#23454;&#30340;&#29305;&#24449;&#20132;&#20114;&#65292;&#23384;&#22312;&#22810;&#20010;&#22312;&#29305;&#24449;&#20132;&#20114;&#24378;&#24230;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#20294;&#20934;&#30830;&#24615;&#30456;&#36817;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20934;&#30830;&#24615;&#36817;&#20284;&#30456;&#31561;&#30340;&#39044;&#27979;&#27169;&#22411;&#31867;&#20013;&#25506;&#32034;&#29305;&#24449;&#20132;&#20114;&#24378;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#25289;&#32461;&#33945;&#38598;&#21512;&#30340;&#19978;&#19979;&#25991;&#20013;&#24341;&#20837;&#20102;&#29305;&#24449;&#20132;&#20114;&#20998;&#25968;&#65288;FIS&#65289;&#65292;&#34920;&#31034;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#23454;&#29616;&#31867;&#20284;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#38598;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#27169;&#22411;&#31867;&#20013;&#30340;FIS&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23637;&#31034;&#20102;FIS&#30340;&#23646;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19982;&#21333;&#20010;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20132;&#20114;&#29305;&#24449;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions among features are central to understanding the behavior of machine learning models. Recent research has made significant strides in detecting and quantifying feature interactions in single predictive models. However, we argue that the feature interactions extracted from a single pre-specified model may not be trustworthy since: a well-trained predictive model may not preserve the true feature interactions and there exist multiple well-performing predictive models that differ in feature interaction strengths. Thus, we recommend exploring feature interaction strengths in a model class of approximately equally accurate predictive models. In this work, we introduce the feature interaction score (FIS) in the context of a Rashomon set, representing a collection of models that achieve similar accuracy on a given task. We propose a general and practical algorithm to calculate the FIS in the model class. We demonstrate the properties of the FIS via synthetic data and draw connecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;GCSL&#31639;&#27861;&#30340;&#26041;&#27861;&#65292; TraIL&#21033;&#29992;&#36712;&#36857;&#20013;&#30340;&#20449;&#24687;&#39044;&#27979;&#23376;&#30446;&#26631;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10171</link><description>&lt;p&gt;
&#24102;&#23376;&#30446;&#26631;&#39044;&#27979;&#30340;&#30446;&#26631;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Supervised Learning with Sub-Goal Prediction. (arXiv:2305.10171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;GCSL&#31639;&#27861;&#30340;&#26041;&#27861;&#65292; TraIL&#21033;&#29992;&#36712;&#36857;&#20013;&#30340;&#20449;&#24687;&#39044;&#27979;&#23376;&#30446;&#26631;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;&#30417;&#30563;&#23398;&#20064;(GCSL)&#65292;&#20197;&#22788;&#29702;&#30446;&#26631;&#26465;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;GCSL&#22522;&#20110;&#20107;&#21518;&#23398;&#20064;&#21407;&#21017;&#65306;&#36890;&#36807;&#35266;&#23519;&#20808;&#21069;&#25191;&#34892;&#30340;&#36712;&#36857;&#20013;&#35775;&#38382;&#30340;&#29366;&#24577;&#24182;&#23558;&#20854;&#35270;&#20026;&#36798;&#21040;&#30340;&#30446;&#26631;&#65292;GCSL&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#30456;&#24212;&#30340;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;GCSL&#20165;&#23398;&#20064;&#30446;&#26631;&#26465;&#20214;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#20002;&#24323;&#20854;&#20182;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#26159;&#65292;&#21516;&#26679;&#30340;&#20107;&#21518;&#23398;&#20064;&#21407;&#21017;&#21487;&#29992;&#20110;&#20174;&#21516;&#19968;&#36712;&#36857;&#23398;&#20064;&#39044;&#27979;&#30446;&#26631;&#26465;&#20214;&#30340;&#23376;&#30446;&#26631;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Trajectory Iterative Learner(TraIL)&#65292;&#36825;&#26159;GCSL&#30340;&#25193;&#23637;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#36712;&#36857;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#21160;&#20316;&#21644;&#23376;&#30446;&#26631;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;TraIL&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#35774;&#32622;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20960;&#20010;&#27969;&#34892;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#23558;GCSL&#20013;&#30340;&#30495;&#23454;&#30446;&#26631;&#26367;&#25442;&#20026;TraIL&#23376;&#30446;&#26631;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a simple yet effective algorithm -- goal-conditioned supervised-learning (GCSL) -- was proposed to tackle goal-conditioned reinforcement-learning. GCSL is based on the principle of hindsight learning: by observing states visited in previously executed trajectories and treating them as attained goals, GCSL learns the corresponding actions via supervised learning. However, GCSL only learns a goal-conditioned policy, discarding other information in the process. Our insight is that the same hindsight principle can be used to learn to predict goal-conditioned sub-goals from the same trajectory. Based on this idea, we propose Trajectory Iterative Learner (TraIL), an extension of GCSL that further exploits the information in a trajectory, and uses it for learning to predict both actions and sub-goals. We investigate the settings in which TraIL can make better use of the data, and discover that for several popular problem settings, replacing real goals in GCSL with predicted TraIL su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TwinGP&#30340;&#26032;&#30340;&#20840;&#23616;-&#23616;&#37096;&#36817;&#20284;&#26694;&#26550;&#65292;&#20351;&#29992;&#23376;&#38598;&#25968;&#25454;&#26041;&#27861;&#65292;&#24182;&#23558;&#30456;&#20851;&#20989;&#25968;&#24314;&#27169;&#20026;&#20840;&#23616;&#21644;&#23616;&#37096;&#26680;&#30340;&#32452;&#21512;&#12290;TwinGP&#22312;&#35745;&#31639;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;GP&#24314;&#27169;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10158</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#30340;&#20840;&#23616;-&#23616;&#37096;&#36817;&#20284;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Global-Local Approximation Framework for Large-Scale Gaussian Process Modeling. (arXiv:2305.10158v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TwinGP&#30340;&#26032;&#30340;&#20840;&#23616;-&#23616;&#37096;&#36817;&#20284;&#26694;&#26550;&#65292;&#20351;&#29992;&#23376;&#38598;&#25968;&#25454;&#26041;&#27861;&#65292;&#24182;&#23558;&#30456;&#20851;&#20989;&#25968;&#24314;&#27169;&#20026;&#20840;&#23616;&#21644;&#23616;&#37096;&#26680;&#30340;&#32452;&#21512;&#12290;TwinGP&#22312;&#35745;&#31639;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;GP&#24314;&#27169;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#35268;&#27169;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24314;&#27169;&#26694;&#26550;&#12290;&#19982;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#35299;&#20915;&#31934;&#30830;GP&#24314;&#27169;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#36817;&#20284;&#30456;&#21453;&#65292;&#25105;&#20204;&#22312;&#26500;&#24314;&#36817;&#20284;&#26102;&#37319;&#29992;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#23376;&#38598;&#25968;&#25454;&#26041;&#27861;&#65292;&#20854;&#20013;&#23376;&#38598;&#26159;&#19968;&#20010;&#26088;&#22312;&#25429;&#25417;&#25968;&#25454;&#20840;&#23616;&#36235;&#21183;&#30340;&#20840;&#23616;&#28857;&#38598;&#30340;&#24182;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#25429;&#25417;&#32473;&#23450;&#27979;&#35797;&#20301;&#32622;&#21608;&#22260;&#23616;&#37096;&#36235;&#21183;&#30340;&#23616;&#37096;&#28857;&#38598;&#12290;&#30456;&#20851;&#20989;&#25968;&#20063;&#34987;&#24314;&#27169;&#20026;&#20840;&#23616;&#21644;&#23616;&#37096;&#26680;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24615;&#33021;&#65292;&#21363;TwinGP&#65292;&#22312;&#35745;&#31639;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;GP&#24314;&#27169;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel framework for large-scale Gaussian process (GP) modeling. Contrary to the global, and local approximations proposed in the literature to address the computational bottleneck with exact GP modeling, we employ a combined global-local approach in building the approximation. Our framework uses a subset-of-data approach where the subset is a union of a set of global points designed to capture the global trend in the data, and a set of local points specific to a given testing location to capture the local trend around the testing location. The correlation function is also modeled as a combination of a global, and a local kernel. The performance of our framework, which we refer to as TwinGP, is on par or better than the state-of-the-art GP modeling methods at a fraction of their computational cost.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\partial$-CROWN&#30340;&#26694;&#26550;&#65292;&#20197;&#20445;&#35777;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20855;&#26377;&#20840;&#23616;&#27491;&#30830;&#24615;&#30340;&#26368;&#22351;&#21097;&#20313;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#33719;&#24471;&#26377;&#25928;&#35777;&#20070;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10157</link><description>&lt;p&gt;
&#35777;&#26126;&#27491;&#30830;&#24615;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Provably Correct Physics-Informed Neural Networks. (arXiv:2305.10157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\partial$-CROWN&#30340;&#26694;&#26550;&#65292;&#20197;&#20445;&#35777;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20855;&#26377;&#20840;&#23616;&#27491;&#30830;&#24615;&#30340;&#26368;&#22351;&#21097;&#20313;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#33719;&#24471;&#26377;&#25928;&#35777;&#20070;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#26410;&#33021;&#20445;&#35777;PINN&#22312;&#25972;&#20010;&#26102;&#31354;&#22495;&#20869;&#30340;&#26368;&#22351;&#21097;&#20313;&#35823;&#24046;&#65292;&#36825;&#26159;&#31867;&#20284;&#20110;&#25968;&#23383;&#27714;&#35299;&#22120;&#30340;&#20844;&#24046;&#30340;&#19968;&#31181;&#24230;&#37327;&#65292;&#32780;&#26159;&#38598;&#20013;&#20110;&#22312;&#19968;&#32452;&#36755;&#20837;&#19978;&#36890;&#36807;&#28857;&#23545;&#28857;&#27604;&#36739;&#26469;&#24471;&#21040;&#35299;&#20915;&#26041;&#26696;&#21644;&#27714;&#35299;&#22120;&#24471;&#21040;&#35299;&#20915;&#26041;&#26696;&#30340;&#32467;&#26524;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19981;&#33021;&#35748;&#20026;&#22312;&#19968;&#32452;&#26377;&#38480;&#28857;&#19978;&#30340;&#27979;&#35797;&#23601;&#36275;&#20197;&#20351;&#24471;&#37096;&#32626;&#25104;&#31435;&#65292;&#22240;&#20026;&#22312;&#21478;&#19968;&#32452;&#28857;&#19978;&#24615;&#33021;&#21487;&#33021;&#22823;&#19981;&#30456;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#25972;&#20010;&#36755;&#20837;&#22495;&#30340;PINN&#22522;&#20110;&#20844;&#24046;&#30340;&#27491;&#30830;&#24615;&#26465;&#20214;&#12290;&#20026;&#20102;&#39564;&#35777;&#23427;&#20204;&#30340;&#26377;&#25928;&#31243;&#24230;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;$\partial$-CROWN&#65306;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#39640;&#25928;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#12289;&#21518;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#38480;&#21046;PINN&#30340;&#21097;&#20313;&#35823;&#24046;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23427;&#22312;&#33719;&#24471;&#32039;&#23494;&#35777;&#20070;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work provides promising evidence that Physics-informed neural networks (PINN) can efficiently solve partial differential equations (PDE). However, previous works have failed to provide guarantees on the worst-case residual error of a PINN across the spatio-temporal domain - a measure akin to the tolerance of numerical solvers - focusing instead on point-wise comparisons between their solution and the ones obtained by a solver on a set of inputs. In real-world applications, one cannot consider tests on a finite set of points to be sufficient grounds for deployment, as the performance could be substantially worse on a different set. To alleviate this issue, we establish tolerance-based correctness conditions for PINNs over the entire input domain. To verify the extent to which they hold, we introduce $\partial$-CROWN: a general, efficient and scalable post-training framework to bound PINN residual errors. We demonstrate its effectiveness in obtaining tight certificates by applying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#12290; &#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#19977;&#32500;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.10133</link><description>&lt;p&gt;
Lingo3DMol:&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model. (arXiv:2305.10133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#12290; &#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#19977;&#32500;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25512;&#21160;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#22791;&#21463;&#30633;&#30446;&#12290; &#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#20108;&#32500;&#32467;&#26500;&#20013;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#32780;&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21017;&#21487;&#20197;&#30452;&#25509;&#20135;&#29983;&#20855;&#26377;&#20934;&#30830;&#19977;&#32500;&#22352;&#26631;&#30340;&#20998;&#23376;&#12290;&#21463;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21475;&#34955;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#29983;&#25104;&#19977;&#32500;&#22352;&#26631;&#30340;&#33021;&#21147;&#12290; &#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#25968;&#25454;&#19981;&#36275;&#65292;&#22240;&#27492;&#35774;&#35745;&#20102;&#19968;&#31181;&#25200;&#21160;&#21644;&#24674;&#22797;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#22823;&#37327;&#30340;&#23567;&#20998;&#23376;&#25968;&#25454;&#12290; &#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#24418;&#24335;&#65292;&#21363;&#24102;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#22352;&#26631;&#30340;&#22522;&#20110;&#29255;&#27573;&#30340;SMILES&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#23376;&#25299;&#25169;&#32467;&#26500;&#21644;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#12290;&#26368;&#32456;&#65292;CrossDocked&#21644;DUD-E&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#35780;&#20272;&#21644;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design powered by deep generative models have attracted increasing research interest in recent years. Language models have demonstrated a robust capacity for generating valid molecules in 2D structures, while methods based on geometric deep learning can directly produce molecules with accurate 3D coordinates. Inspired by both methods, this article proposes a pocket-based 3D molecule generation method that leverages the language model with the ability to generate 3D coordinates. High quality protein-ligand complex data are insufficient; hence, a perturbation and restoration pre-training task is designed that can utilize vast amounts of small-molecule data. A new molecular representation, a fragment-based SMILES with local and global coordinates, is also presented, enabling the language model to learn molecular topological structures and spatial position information effectively. Ultimately, CrossDocked and DUD-E dataset is employed for evaluation and additional metri
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#36951;&#24536;&#26041;&#27861;&#65292;&#21363;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#25351;&#23450;&#28040;&#38500;&#21738;&#20123;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.10120</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#36951;&#24536;&#65306;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models. (arXiv:2305.10120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10120
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#36951;&#24536;&#26041;&#27861;&#65292;&#21363;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#25351;&#23450;&#28040;&#38500;&#21738;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#12289;&#35823;&#23548;&#25110;&#19981;&#24403;&#20869;&#23481;&#30340;&#25285;&#24551;&#12290;&#21463;&#27492;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25345;&#32493;&#23398;&#20064;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21487;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#20197;&#25351;&#23450;&#35813;&#22914;&#20309;&#36951;&#24536;&#19968;&#20010;&#27010;&#24565;&#12290;&#36873;&#25321;&#24615;&#36951;&#24536;&#21487;&#24212;&#29992;&#20110;&#21464;&#20998;&#20284;&#28982;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#12290;&#19981;&#21516;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35825;&#23548;&#36951;&#24536;&#21508;&#31181;&#27010;&#24565;&#65292;&#20174;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#25972;&#20010;&#31867;&#21035;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21517;&#20154;&#21644;&#35064;&#20307;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;https://github.com/clear-nus/selective-amnesia&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#31216;&#20854;&#20026;Gap Filler (GaFi)&#27969;&#31243;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.10118</link><description>&lt;p&gt;
&#26550;&#36215;&#26725;&#26753;&#65306;&#36890;&#36807;&#21518;&#22788;&#29702;&#25216;&#26415;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques. (arXiv:2305.10118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#31216;&#20854;&#20026;Gap Filler (GaFi)&#27969;&#31243;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#21644;&#27880;&#37322;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29983;&#25104;&#26367;&#20195;&#25110;&#22686;&#24378;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20854;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#38543;&#21518;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#65306;&#21160;&#24577;&#26679;&#26412;&#36807;&#28388;&#65292;&#21160;&#24577;&#25968;&#25454;&#38598;&#22238;&#25910;&#21644;&#25193;&#23637;&#25216;&#24039;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; Gap Filler (GaFi)&#8221;&#30340;&#27969;&#31243;&#65292;&#22312;&#26368;&#20339;&#21644;&#21327;&#35843;&#30340;&#26041;&#24335;&#19979;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring and annotating suitable datasets for training deep learning models is challenging. This often results in tedious and time-consuming efforts that can hinder research progress. However, generative models have emerged as a promising solution for generating synthetic datasets that can replace or augment real-world data. Despite this, the effectiveness of synthetic data is limited by their inability to fully capture the complexity and diversity of real-world data. To address this issue, we explore the use of Generative Adversarial Networks to generate synthetic datasets for training classifiers that are subsequently evaluated on real-world images. To improve the quality and diversity of the synthetic dataset, we propose three novel post-processing techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which applies these techniques in an optimal and coordinated manner to maximise classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#20809;&#29255;&#19978;&#26816;&#27979;&#20986;&#19971;&#31181;&#29305;&#23450;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#35780;&#20272;&#22270;&#20687;&#30340;&#20845;&#21517;&#25918;&#23556;&#31185;&#21307;&#24072;&#12290;</title><link>http://arxiv.org/abs/2305.10116</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21487;&#38752;&#22320;&#35782;&#21035;&#33016;&#37096;X&#20809;&#24322;&#24120;&#27169;&#24335;&#21527;&#65311;&#19968;&#39033;&#22810;&#35835;&#32773;&#30740;&#31350;&#65292;&#26816;&#26597;AI&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#19968;&#20010;&#26376;&#30340;&#23454;&#26045;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Deep Learning Reliably Recognize Abnormality Patterns on Chest X-rays? A Multi-Reader Study Examining One Month of AI Implementation in Everyday Radiology Clinical Practice. (arXiv:2305.10116v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#20809;&#29255;&#19978;&#26816;&#27979;&#20986;&#19971;&#31181;&#29305;&#23450;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#35780;&#20272;&#22270;&#20687;&#30340;&#20845;&#21517;&#25918;&#23556;&#31185;&#21307;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26816;&#27979;&#31639;&#27861;&#65288;DLAD&#65292;Carebot AI CXR&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#23450;&#20301;&#33016;&#37096;X&#32447;&#29255;&#19978;&#30340;&#19971;&#31181;&#29305;&#23450;&#25918;&#23556;&#23398;&#21457;&#29616;&#65288;&#32954;&#19981;&#24352;&#65288;ATE&#65289;&#65292;&#23454;&#21464;&#65288;CON&#65289;&#65292;&#33016;&#33108;&#31215;&#28082;&#65288;EFF&#65289;&#65292;&#32954;&#37096;&#30149;&#21464;&#65288;LES&#65289;&#65292;&#30382;&#19979;&#27668;&#32959;&#65288;SCE&#65289;&#65292;&#24515;&#33039;&#25193;&#22823;&#65288;CMG&#65289;&#65292;&#27668;&#33016;&#65288;PNO&#65289;&#65289;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;956&#24352;&#33016;&#37096;X&#32447;&#29255;&#65292;&#24182;&#23558;DLAD&#30340;&#24615;&#33021;&#19982;&#22312;&#21307;&#38498;&#29615;&#22659;&#19979;&#35780;&#20272;&#22270;&#20687;&#30340;&#20845;&#21517;&#21333;&#20010;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#21363;&#20351;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;DLAD&#20063;&#21462;&#24471;&#20102;&#39640;&#28789;&#25935;&#24230;&#65288;ATE 1.000&#65288;0.624-1.000&#65289;&#65292;CON 0.864&#65288;0.671-0.956&#65289;&#65292;EFF 0.953&#65288;0.887-0.983&#65289;&#65292;LES 0.905&#65288;0.715-0.978&#65289;&#65292;SCE 1.000&#65288;0.366-1.000&#65289;&#65292;CMG 0.837&#65288;0.711-0.917&#65289;&#65292;PNO 0.875&#65288;0.538-0.986&#65289;&#65289;&#65288;&#26368;&#20302;&#65306;ATE 0.000&#65288;0.000-0.376&#65289;&#65292;CON 0.182&#65288;0.070-0.382&#65289;&#65292;EFF 0.400&#65288;0.302-0.506&#65289;&#65292;LES 0.238&#65288;0.103-0.448&#65289;&#65292;SCE 0.000&#65288;0.000-0.634&#65289;&#65292;CMG 0.347&#65288;0.228-0.486&#65289;&#65292;PNO 0.375&#65288;0.134-0.691&#65289;&#65292;&#26368;&#39640;&#65306;ATE 1.000&#65288;0.624-1.000&#65289;&#65292;CON 0.864&#65288;0.671-0.956&#65289;&#65292;EFF 0.953&#65288;0.887-0.983&#65289;&#65292;LES 0.905&#65288;0.715-0.978&#65289;&#65292;SCE 1.000&#65288;0.366-1.000&#65289;&#65292;CMG 0.837&#65288;0.711-0.917&#65289;&#65292;PNO 0.875&#65288;0.538-0.986&#65289;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we developed a deep-learning-based automatic detection algorithm (DLAD, Carebot AI CXR) to detect and localize seven specific radiological findings (atelectasis (ATE), consolidation (CON), pleural effusion (EFF), pulmonary lesion (LES), subcutaneous emphysema (SCE), cardiomegaly (CMG), pneumothorax (PNO)) on chest X-rays (CXR). We collected 956 CXRs and compared the performance of the DLAD with that of six individual radiologists who assessed the images in a hospital setting. The proposed DLAD achieved high sensitivity (ATE 1.000 (0.624-1.000), CON 0.864 (0.671-0.956), EFF 0.953 (0.887-0.983), LES 0.905 (0.715-0.978), SCE 1.000 (0.366-1.000), CMG 0.837 (0.711-0.917), PNO 0.875 (0.538-0.986)), even when compared to the radiologists (LOWEST: ATE 0.000 (0.000-0.376), CON 0.182 (0.070-0.382), EFF 0.400 (0.302-0.506), LES 0.238 (0.103-0.448), SCE 0.000 (0.000-0.634), CMG 0.347 (0.228-0.486), PNO 0.375 (0.134-0.691), HIGHEST: ATE 1.000 (0.624-1.000), CON 0.864 (0.671-0.956), E
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;STOIC&#25968;&#25454;&#38598;&#23545;COVID-19&#33016;&#37096;CT&#25195;&#25551;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#65292;&#24182;&#37319;&#29992;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10115</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#26041;&#27861;&#36827;&#34892;COVID-19&#33016;&#37096;CT&#25195;&#25551;&#30340;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Deep Learning Approach for COVID-19 Severity Prediction Using Chest CT Scans. (arXiv:2305.10115v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;STOIC&#25968;&#25454;&#38598;&#23545;COVID-19&#33016;&#37096;CT&#25195;&#25551;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#65292;&#24182;&#37319;&#29992;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#23556;&#32447;&#24191;&#27867;&#29992;&#20110;COVID-19&#31579;&#26597;&#65292;&#20294;&#26159;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#25104;&#20687;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;STOIC&#25968;&#25454;&#38598;&#23545;COVID-19&#33016;&#37096;CT&#25195;&#25551;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20999;&#29255;&#20989;&#25968;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#26102;&#38388;&#25968;&#25454;&#22686;&#24378;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#65292;&#32467;&#21512;&#24378;&#22823;&#30340;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65292;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#21487;&#27604;&#25311;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;STOIC2021 COVID-19 AI&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#22235;&#21517;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/aleemsidra/stoic2021-baseline-finalphase-main
&lt;/p&gt;
&lt;p&gt;
Chest X-rays have been widely used for COVID-19 screening; however, 3D computed tomography (CT) is a more effective modality. We present our findings on COVID-19 severity prediction from chest CT scans using the STOIC dataset. We developed an ensemble deep learning based model that incorporates multiple neural networks to improve predictions. To address data imbalance, we used slicing functions and data augmentation. We further improved performance using test time data augmentation. Our approach which employs a simple yet effective ensemble of deep learning-based models with strong test time augmentations, achieved results comparable to more complex methods and secured the fourth position in the STOIC2021 COVID-19 AI Challenge. Our code is available on online: at: https://github.com/aleemsidra/stoic2021- baseline-finalphase-main.
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35780;&#20272;&#31232;&#30095;&#30697;&#38453;&#20808;&#39564;&#20013;&#24402;&#19968;&#21270;&#22240;&#23376;&#30340;&#38646;&#28857;&#26469;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#22411;&#25968;&#20540;&#26041;&#27861;&#65292;&#24182;&#22312;&#22320;&#38754;&#30495;&#23454;&#31232;&#30095;&#30697;&#38453;&#37325;&#24314;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10114</link><description>&lt;p&gt;
&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#20013;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Automatic Hyperparameter Tuning in Sparse Matrix Factorization. (arXiv:2305.10114v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10114
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35780;&#20272;&#31232;&#30095;&#30697;&#38453;&#20808;&#39564;&#20013;&#24402;&#19968;&#21270;&#22240;&#23376;&#30340;&#38646;&#28857;&#26469;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#22411;&#25968;&#20540;&#26041;&#27861;&#65292;&#24182;&#22312;&#22320;&#38754;&#30495;&#23454;&#31232;&#30095;&#30697;&#38453;&#37325;&#24314;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#22522;&#20110;&#20960;&#20010;&#36817;&#20284;&#65292;&#36890;&#36807;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#24471;&#21040;&#20102;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#20808;&#39564;&#30340;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#30340;&#20998;&#26512;&#35299;&#12290;&#22522;&#20110;&#27492;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35780;&#20272;&#31232;&#30095;&#30697;&#38453;&#20808;&#39564;&#20013;&#24402;&#19968;&#21270;&#22240;&#23376;&#30340;&#38646;&#28857;&#26469;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#22411;&#25968;&#20540;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22320;&#38754;&#30495;&#23454;&#31232;&#30095;&#30697;&#38453;&#37325;&#24314;&#20013;&#34920;&#29616;&#20986;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of hyperparameter tuning in sparse matrix factorization under Bayesian framework. In the prior work, an analytical solution of sparse matrix factorization with Laplace prior was obtained by variational Bayes method under several approximations. Based on this solution, we propose a novel numerical method of hyperparameter tuning by evaluating the zero point of normalization factor in sparse matrix prior. We also verify that our method shows excellent performance for ground-truth sparse matrix reconstruction by comparing it with the widely-used algorithm of sparse principal component analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;AI&#26041;&#27861;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#26469;&#33258;&#21160;&#21270;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#30340;&#21512;&#35268;&#24615;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#24322;&#24120;&#21644;&#38169;&#35823;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10113</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;AI&#29992;&#20110;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#30340;&#21512;&#35268;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic AI for Compliance Checking of Electrical Control Panels. (arXiv:2305.10113v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;AI&#26041;&#27861;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#26469;&#33258;&#21160;&#21270;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#30340;&#21512;&#35268;&#24615;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#24322;&#24120;&#21644;&#38169;&#35823;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25903;&#25345;&#21644;&#25913;&#21892;&#26234;&#33021;&#21046;&#36896;&#21644;&#24037;&#19994;4.0&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#20351;&#39046;&#22495;&#19987;&#23478;&#25163;&#21160;&#25191;&#34892;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#35780;&#20272;&#20135;&#21697;&#19982;&#30456;&#23545;&#21407;&#29702;&#22270;&#30340;&#31526;&#21512;&#24615;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#29305;&#23450;&#30340;&#24037;&#19994;&#22330;&#26223;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#30340;&#21512;&#35268;&#24615;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#30340;&#32452;&#21512;&#65292;&#21363;&#20351;&#21482;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#26368;&#32456;&#20135;&#21697;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24322;&#24120;&#21644;&#38169;&#35823;&#12290;&#36890;&#36807;&#24847;&#22823;&#21033;&#19968;&#23478;&#20174;&#20107;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#29983;&#20135;&#30340;&#20844;&#21496;&#25552;&#20379;&#30340;&#23454;&#38469;&#27979;&#35797;&#26696;&#20363;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence plays a main role in supporting and improving smart manufacturing and Industry 4.0, by enabling the automation of different types of tasks manually performed by domain experts. In particular, assessing the compliance of a product with the relative schematic is a time-consuming and prone-to-error process. In this paper, we address this problem in a specific industrial scenario. In particular, we define a Neuro-Symbolic approach for automating the compliance verification of the electrical control panels. Our approach is based on the combination of Deep Learning techniques with Answer Set Programming (ASP), and allows for identifying possible anomalies and errors in the final product even when a very limited amount of training data is available. The experiments conducted on a real test case provided by an Italian Company operating in electrical control panel production demonstrate the effectiveness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;TweetGage&#65292;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#34920;&#31034;&#21457;&#24067;&#24086;&#23376;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#26469;&#39044;&#27979;&#29992;&#25143;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20114;&#21160;&#65292;&#30456;&#23545;&#20854;&#20182;&#30740;&#31350;&#21482;&#32771;&#34385;&#24086;&#23376;&#25991;&#26412;&#21644;&#21457;&#24067;&#29992;&#25143;&#31561;&#22240;&#32032;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10103</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29305;&#20114;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predicting Tweet Engagement with Graph Neural Networks. (arXiv:2305.10103v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;TweetGage&#65292;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#34920;&#31034;&#21457;&#24067;&#24086;&#23376;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#26469;&#39044;&#27979;&#29992;&#25143;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20114;&#21160;&#65292;&#30456;&#23545;&#20854;&#20182;&#30740;&#31350;&#21482;&#32771;&#34385;&#24086;&#23376;&#25991;&#26412;&#21644;&#21457;&#24067;&#29992;&#25143;&#31561;&#22240;&#32032;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#26159;&#26368;&#37325;&#35201;&#30340;&#22312;&#32447;&#20869;&#23481;&#20998;&#20139;&#24179;&#21488;&#20043;&#19968;&#65292;&#39044;&#27979;&#21457;&#24067;&#20869;&#23481;&#30340;&#20114;&#21160;&#24773;&#20917;&#26159;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#23454;&#29616;&#30408;&#21033;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#35748;&#20026;&#21457;&#24067;&#30340;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#20063;&#26159;&#20114;&#21160;&#25968;&#22686;&#38271;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TweetGage&#65292;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#26469;&#34920;&#31034;&#24086;&#23376;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#39044;&#27979;&#29992;&#25143;&#30340;&#20114;&#21160;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#25105;&#20204;&#38024;&#23545;Twitter&#24179;&#21488;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#33391;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social Networks represent one of the most important online sources to share content across a world-scale audience. In this context, predicting whether a post will have any impact in terms of engagement is of crucial importance to drive the profitable exploitation of these media. In the literature, several studies address this issue by leveraging direct features of the posts, typically related to the textual content and the user publishing it. In this paper, we argue that the rise of engagement is also related to another key component, which is the semantic connection among posts published by users in social media. Hence, we propose TweetGage, a Graph Neural Network solution to predict the user engagement based on a novel graph-based model that represents the relationships among posts. To validate our proposal, we focus on the Twitter platform and perform a thorough experimental campaign providing evidence of its quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#21644;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.10089</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization. (arXiv:2305.10089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#21644;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20013;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#27169;&#20223;&#19987;&#23478;&#30340;&#22870;&#21169;&#20540;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35789;&#20856;&#24207;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#65292;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#35753;&#23398;&#20064;&#32773;&#30340;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#30417;&#30563;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#35299;&#20915;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10071</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#38382;&#39064;&#65306;&#26080;&#30417;&#30563;&#30340;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cold PAWS: Unsupervised class discovery and the cold-start problem. (arXiv:2305.10071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#30417;&#30563;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#35299;&#20915;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#38598;&#24120;&#24120;&#26159;&#19968;&#39033;&#33392;&#33510;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30740;&#31350;&#34920;&#26126;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#26631;&#31614;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#22914;&#20309;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#39318;&#27425;&#36873;&#25321;&#20449;&#24687;&#22270;&#20687;&#23376;&#38598;&#36827;&#34892;&#26631;&#35760;&#30340;&#25361;&#25112;&#65292;&#21363;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;CIFAR10&#12289;Imagenette&#12289;DeepWeeds&#21644;EuroSAT&#65289;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#24403;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#26102;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#22343;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;d&#26041;&#38754;&#33719;&#24471;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the field of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the first instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#37096;&#20998;&#21644;&#32858;&#31867;&#37096;&#20998;&#65292;&#24182;&#21033;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;&#25216;&#26415;&#35299;&#37322;&#30456;&#20132;&#21306;&#22495;&#30340;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.10060</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#30417;&#30563;&#32858;&#31867;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#30340;XAI
&lt;/p&gt;
&lt;p&gt;
XAI for Self-supervised Clustering of Wireless Spectrum Activity. (arXiv:2305.10060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#37096;&#20998;&#21644;&#32858;&#31867;&#37096;&#20998;&#65292;&#24182;&#21033;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;&#25216;&#26415;&#35299;&#37322;&#30456;&#20132;&#21306;&#22495;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#35859;&#30340;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#21253;&#25324;&#26080;&#32447;&#36890;&#20449;&#39046;&#22495;&#65292;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#36825;&#19968;&#36235;&#21183;&#20013;&#65292;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20284;&#20046;&#26159;&#39046;&#22495;&#30456;&#20851;&#20998;&#31867;&#38382;&#39064;&#26368;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#23427;&#20204;&#34987;&#35777;&#26126;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#19981;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#20316;&#20026;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#12290;&#33258;&#30417;&#30563;&#26550;&#26500;&#20986;&#29616;&#20316;&#20026;&#19968;&#20010;&#20943;&#23569;&#25152;&#38656;&#26631;&#35760;&#25968;&#25454;&#35268;&#27169;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#35299;&#37322;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#28145;&#24230;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#30001;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#34920;&#31034;&#23398;&#20064;&#37096;&#20998;&#21644;&#32858;&#31867;&#37096;&#20998;&#32452;&#25104;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#37096;&#20998;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;&#26469;&#35299;&#37322;&#30456;&#20132;&#21306;&#22495;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The so-called black-box deep learning (DL) models are increasingly used in classification tasks across many scientific disciplines, including wireless communications domain. In this trend, supervised DL models appear as most commonly proposed solutions to domain-related classification problems. Although they are proven to have unmatched performance, the necessity for large labeled training data and their intractable reasoning, as two major drawbacks, are constraining their usage. The self-supervised architectures emerged as a promising solution that reduces the size of the needed labeled data, but the explainability problem remains. In this paper, we propose a methodology for explaining deep clustering, self-supervised learning architectures comprised of a representation learning part based on a Convolutional Neural Network (CNN) and a clustering part. For the state of the art representation learning part, our methodology employs Guided Backpropagation to interpret the regions of inter
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#28151;&#21512;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#12289;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#36873;&#25321;&#20174;ATM&#20107;&#20214;&#26085;&#24535;&#20013;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#65292;&#29992;&#20110;&#26089;&#26399;ATM&#25925;&#38556;&#39044;&#27979;&#65292;&#22312;&#30495;&#23454;&#30340;ATM&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.10059</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#28151;&#21512;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;ATM&#25925;&#38556;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A hybrid feature learning approach based on convolutional kernels for ATM fault prediction using event-log data. (arXiv:2305.10059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#28151;&#21512;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#12289;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#36873;&#25321;&#20174;ATM&#20107;&#20214;&#26085;&#24535;&#20013;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#65292;&#29992;&#20110;&#26089;&#26399;ATM&#25925;&#38556;&#39044;&#27979;&#65292;&#22312;&#30495;&#23454;&#30340;ATM&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#32500;&#25252;&#65288;PdM&#65289;&#26041;&#27861;&#26088;&#22312;&#22312;&#35774;&#22791;&#25925;&#38556;&#20043;&#21069;&#23433;&#25490;&#32500;&#25252;&#24037;&#20316;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#33258;&#21160;&#21462;&#27454;&#26426;&#65288;ATM&#65289;&#30340;&#26089;&#26399;&#25925;&#38556;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#26426;&#22120;&#26131;&#21463;&#21508;&#31181;&#19981;&#21487;&#39044;&#27979;&#30340;&#25925;&#38556;&#24433;&#21709;&#12290;ATM&#36890;&#36807;&#29983;&#25104;&#22823;&#37327;&#30340;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#26469;&#36319;&#36394;&#25191;&#34892;&#29366;&#24577;&#65292;&#36825;&#20123;&#25968;&#25454;&#25910;&#38598;&#19982;&#25925;&#38556;&#20107;&#20214;&#26080;&#20851;&#30340;&#31995;&#32479;&#28040;&#24687;&#12290; &#22522;&#20110;&#20107;&#20214;&#26085;&#24535;&#39044;&#27979;&#25925;&#38556;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#22312;&#20110;&#25552;&#21462;&#21487;&#33021;&#34920;&#31034;&#21363;&#23558;&#21457;&#29983;&#25925;&#38556;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#30446;&#21069;&#27491;&#22312;PdM&#20013;&#20351;&#29992;&#65292;&#20854;&#20013;&#20174;&#26368;&#23567;&#22788;&#29702;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#20294;&#26159;&#65292;&#20173;&#28982;&#23384;&#22312;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#20174;&#22522;&#20110;&#20107;&#20214;&#26085;&#24535;&#30340;&#25968;&#25454;&#20013;&#23548;&#20986;&#30456;&#20851;&#29305;&#24449;&#30340;&#31354;&#30333;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;MiniROCKET&#65289;&#29992;&#20110;&#26089;&#26399;ATM&#25925;&#38556;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#31181;&#28151;&#21512;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#12289;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#36873;&#25321;&#20174;ATM&#20107;&#20214;&#26085;&#24535;&#20013;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#23545;&#30495;&#23454;&#30340;ATM&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive Maintenance (PdM) methods aim to facilitate the scheduling of maintenance work before equipment failure. In this context, detecting early faults in automated teller machines (ATMs) has become increasingly important since these machines are susceptible to various types of unpredictable failures. ATMs track execution status by generating massive event-log data that collect system messages unrelated to the failure event. Predicting machine failure based on event logs poses additional challenges, mainly in extracting features that might represent sequences of events indicating impending failures. Accordingly, feature learning approaches are currently being used in PdM, where informative features are learned automatically from minimally processed sensor data. However, a gap remains to be seen on how these approaches can be exploited for deriving relevant features from event-log-based data. To fill this gap, we present a predictive model based on a convolutional kernel (MiniROCKET
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;&#26053;&#34892;&#26102;&#38388;&#39044;&#27979;&#65292;&#20351;&#29992;&#20102;&#30830;&#23450;&#24615;&#25302;&#25341;&#27169;&#22411;&#21644;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#36828;&#31243;&#30417;&#27979;&#21644;&#29616;&#22330;&#25968;&#25454;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10057</link><description>&lt;p&gt;
&#29289;&#29702;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;&#26053;&#34892;&#26102;&#38388;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-driven machine learning for the prediction of coronal mass ejections' travel times. (arXiv:2305.10057v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;&#26053;&#34892;&#26102;&#38388;&#39044;&#27979;&#65292;&#20351;&#29992;&#20102;&#30830;&#23450;&#24615;&#25302;&#25341;&#27169;&#22411;&#21644;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#36828;&#31243;&#30417;&#27979;&#21644;&#29616;&#22330;&#25968;&#25454;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;&#65288;CMEs&#65289;&#26159;&#25351;&#20174;&#22826;&#38451;&#20885;&#23618;&#20013;&#23558;&#31561;&#31163;&#23376;&#20307;&#21644;&#30913;&#22330;&#21095;&#28872;&#22320;&#25243;&#23556;&#21040;&#22826;&#38451;&#31995;&#20013;&#30340;&#36807;&#31243;&#12290;CMEs&#22312;&#31185;&#23398;&#19978;&#20855;&#26377;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21442;&#19982;&#20102;&#34920;&#24449;&#27963;&#36291;&#22826;&#38451;&#30340;&#29289;&#29702;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;CMEs&#22240;&#20854;&#23545;&#31354;&#38388;&#22825;&#27668;&#30340;&#24433;&#21709;&#32780;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#22320;&#30913;&#39118;&#26292;&#30456;&#20851;&#24182;&#19988;&#21487;&#33021;&#24341;&#21457;&#22826;&#38451;&#39640;&#33021;&#31890;&#23376;&#27969;&#30340;&#20135;&#29983;&#12290;&#22312;&#36825;&#20010;&#31354;&#38388;&#22825;&#27668;&#26694;&#26550;&#19979;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#26469;&#39044;&#27979;CMEs&#26053;&#34892;&#26102;&#38388;&#65292;&#20854;&#20013;&#21033;&#29992;&#30830;&#23450;&#24615;&#25302;&#25341;&#27169;&#22411;&#26469;&#25913;&#21892;&#32423;&#32852;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20197;&#20379;&#36828;&#31243;&#30417;&#27979;&#21644;&#29616;&#22330;&#25968;&#25454;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;AI&#26550;&#26500;&#20013;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#26174;&#30528;&#25552;&#39640;&#20102;&#26053;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronal Mass Ejections (CMEs) correspond to dramatic expulsions of plasma and magnetic field from the solar corona into the heliosphere. CMEs are scientifically relevant because they are involved in the physical mechanisms characterizing the active Sun. However, more recently CMEs have attracted attention for their impact on space weather, as they are correlated to geomagnetic storms and may induce the generation of Solar Energetic Particles streams. In this space weather framework, the present paper introduces a physics-driven artificial intelligence (AI) approach to the prediction of CMEs travel time, in which the deterministic drag-based model is exploited to improve the training phase of a cascade of two neural networks fed with both remote sensing and in-situ data. This study shows that the use of physical information in the AI architecture significantly improves both the accuracy and the robustness of the travel time prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;RF&#31639;&#27861;&#30340;1&#27493;&#21644;2&#27493;&#26368;&#20248;&#21152;&#26435;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26469;&#22788;&#29702;&#39044;&#27979;&#24615;&#33021;&#24046;&#24322;&#38382;&#39064;&#65292;&#35777;&#26126;&#28176;&#36817;&#26368;&#20248;&#65292;&#24182;&#36827;&#34892;&#20102;&#25968;&#25454;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.10042</link><description>&lt;p&gt;
&#26368;&#20248;&#21152;&#26435;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Optimal Weighted Random Forests. (arXiv:2305.10042v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;RF&#31639;&#27861;&#30340;1&#27493;&#21644;2&#27493;&#26368;&#20248;&#21152;&#26435;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26469;&#22788;&#29702;&#39044;&#27979;&#24615;&#33021;&#24046;&#24322;&#38382;&#39064;&#65292;&#35777;&#26126;&#28176;&#36817;&#26368;&#20248;&#65292;&#24182;&#36827;&#34892;&#20102;&#25968;&#25454;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#26862;&#26519; (RF) &#31639;&#27861;&#30340;&#20248;&#21270;&#21152;&#26435;&#26041;&#27861;&#65292;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#27493;&#26368;&#20248;&#21152;&#26435;&#38543;&#26426;&#26862;&#26519; (1step-WRF$_\mathrm{opt}$) &#21644;&#20004;&#27493;&#26368;&#20248;&#21152;&#26435;&#38543;&#26426;&#26862;&#26519; (2steps-WRF$_\mathrm{opt}$)&#65292;&#36890;&#36807;&#21152;&#26435;&#36873;&#25321;&#20934;&#21017;&#26469;&#32452;&#21512;&#22522;&#26412;&#23398;&#20064;&#22120;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20316;&#32773;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#26159;&#28176;&#36817;&#26368;&#20248;&#30340;&#65292;&#21363;&#24471;&#21040;&#30340;&#24179;&#26041;&#25439;&#22833;&#21644;&#39118;&#38505;&#19982;&#19981;&#21487;&#34892;&#20294;&#26368;&#20339;&#27169;&#22411;&#24179;&#22343;&#20272;&#35745;&#37327;&#30340;&#30456;&#23545;&#24046;&#24322;&#28176;&#36817;&#31561;&#21516;&#12290;&#26368;&#21518;&#65292;&#20316;&#32773;&#20351;&#29992;&#25968;&#25454;&#30740;&#31350;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The random forest (RF) algorithm has become a very popular prediction method for its great flexibility and promising accuracy. In RF, it is conventional to put equal weights on all the base learners (trees) to aggregate their predictions. However, the predictive performances of different trees within the forest can be very different due to the randomization of the embedded bootstrap sampling and feature selection. In this paper, we focus on RF for regression and propose two optimal weighting algorithms, namely the 1 Step Optimal Weighted RF (1step-WRF$_\mathrm{opt}$) and 2 Steps Optimal Weighted RF (2steps-WRF$_\mathrm{opt}$), that combine the base learners through the weights determined by weight choice criteria. Under some regularity conditions, we show that these algorithms are asymptotically optimal in the sense that the resulting squared loss and risk are asymptotically identical to those of the infeasible but best possible model averaging estimator. Numerical studies conducted on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHoP&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#39640;&#38454;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#20986;&#39640;&#38454;&#23548;&#25968;&#35268;&#21017;&#65292;&#24182;&#23558;&#32593;&#32476;&#23637;&#24320;&#25104;&#27888;&#21202;&#32423;&#25968;&#65292;&#25552;&#20379;&#19968;&#20010;&#26126;&#30830;&#30340;PDE&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.10033</link><description>&lt;p&gt;
SHoP&#65306;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#27714;&#35299;&#39640;&#38454;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
SHoP: A Deep Learning Framework for Solving High-order Partial Differential Equations. (arXiv:2305.10033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHoP&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#39640;&#38454;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#20986;&#39640;&#38454;&#23548;&#25968;&#35268;&#21017;&#65292;&#24182;&#23558;&#32593;&#32476;&#23637;&#24320;&#25104;&#27888;&#21202;&#32423;&#25968;&#65292;&#25552;&#20379;&#19968;&#20010;&#26126;&#30830;&#30340;PDE&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#19968;&#30452;&#26159;&#35745;&#31639;&#31185;&#23398;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20063;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#30740;&#31350;&#30340;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36890;&#29992;&#36924;&#36817;&#24615;&#65292;&#22240;&#27492;&#24191;&#27867;&#29992;&#20110;&#36924;&#36817;PDE&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#38454;&#23548;&#25968;&#30340;&#35745;&#31639;&#31934;&#24230;&#19981;&#36275;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#39640;&#38454;PDE&#65292;&#24182;&#19988;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#26159;&#19968;&#20010;&#27809;&#26377;&#26126;&#30830;&#35299;&#37322;&#30340;&#40657;&#30418;&#23376;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHoP&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#38454;PDE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#38454;&#23548;&#25968;&#35268;&#21017;&#65292;&#20197;&#20415;&#24555;&#36895;&#20934;&#30830;&#22320;&#33719;&#24471;&#23548;&#25968;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25226;&#32593;&#32476;&#23637;&#24320;&#25104;&#27888;&#21202;&#32423;&#25968;&#65292;&#20026;PDE&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#32500;&#24230;&#30340;&#22235;&#20010;&#39640;&#38454;PDE&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#32780;&#20934;&#30830;&#22320;&#35299;&#20915;&#39640;&#38454;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations (PDEs) has been a fundamental problem in computational science and of wide applications for both scientific and engineering research. Due to its universal approximation property, neural network is widely used to approximate the solutions of PDEs. However, existing works are incapable of solving high-order PDEs due to insufficient calculation accuracy of higher-order derivatives, and the final network is a black box without explicit explanation. To address these issues, we propose a deep learning framework to solve high-order PDEs, named SHoP. Specifically, we derive the high-order derivative rule for neural network, to get the derivatives quickly and accurately; moreover, we expand the network into a Taylor series, providing an explicit solution for the PDEs. We conduct experimental validations four high-order PDEs with different dimensions, showing that we can solve high-order PDEs efficiently and accurately.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;ViT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#29992;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#30340;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#65292;&#27604;&#20256;&#32479;CNN&#21644;ViT&#37117;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#20013;&#22270;&#20687;&#26631;&#27880;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10018</link><description>&lt;p&gt;
&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;Transformer&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#31867;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers. (arXiv:2305.10018v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10018
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;ViT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#29992;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#30340;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#65292;&#27604;&#20256;&#32479;CNN&#21644;ViT&#37117;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#20013;&#22270;&#20687;&#26631;&#27880;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#20998;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#35782;&#21035;&#21516;&#19968;&#31867;&#21035;&#20869;&#29289;&#20307;&#20043;&#38388;&#24494;&#23567;&#24046;&#24322;&#12290;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#39033;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#20854;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#35270;&#35273;&#25968;&#25454;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#26368;&#36817;&#24050;&#25104;&#20026;&#22270;&#20687;&#20998;&#31867;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21322;&#30417;&#30563;ViT&#65292;&#19968;&#31181;&#24212;&#29992;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;ViT&#27169;&#22411;&#24494;&#35843;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#36825;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#29305;&#21035;&#24120;&#35265;&#65292;&#20854;&#20013;&#22270;&#20687;&#26131;&#20110;&#33719;&#21462;&#20294;&#26631;&#31614;&#26377;&#22122;&#38899;&#12289;&#19981;&#23384;&#22312;&#25110;&#33719;&#21462;&#26114;&#36149;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#21322;&#30417;&#30563;ViT&#20173;&#28982;&#20248;&#20110;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;ViT&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#21322;&#30417;&#30563;ViT&#22312;&#38656;&#35201;&#31934;&#30830;&#21644;&#32454;&#31890;&#24230;&#20998;&#31867;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained classification is a challenging task that involves identifying subtle differences between objects within the same category. This task is particularly challenging in scenarios where data is scarce. Visual transformers (ViT) have recently emerged as a powerful tool for image classification, due to their ability to learn highly expressive representations of visual data using self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine tuned using semi-supervised learning techniques, suitable for situations where we have lack of annotated data. This is particularly common in e-commerce, where images are readily available but labels are noisy, nonexistent, or expensive to obtain. Our results demonstrate that Semi-ViT outperforms traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned with limited annotated data. These findings indicate that Semi-ViTs hold significant promise for applications that require precise and fine-grained classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#65292;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#25581;&#31034;&#20102;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.10015</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#25928;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Utility Theory of Synthetic Data Generation. (arXiv:2305.10015v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#65292;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#25581;&#31034;&#20102;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#23545;&#20110;&#34913;&#37327;&#21512;&#25104;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#32467;&#26524;&#20391;&#37325;&#20110;&#23545;&#21512;&#25104;&#25968;&#25454;&#25928;&#29992;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#32780;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25928;&#29992;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#12290;&#35813;&#25351;&#26631;&#23450;&#20041;&#20026;&#22312;&#21512;&#25104;&#21644;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#30340;&#32477;&#23545;&#24046;&#24322;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35813;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#26469;&#30740;&#31350;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#32467;&#26524;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#21017;&#35813;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25928;&#29992;&#25351;&#26631;&#22522;&#20110;&#21512;&#25104;&#21644;&#21407;&#22987;&#25968;&#25454;&#20043;&#38388;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#19968;&#33268;&#24615;&#12290;&#35813;&#29702;&#35770;&#20351;&#29992;&#20960;&#31181;&#21512;&#25104;&#31639;&#27861;&#36827;&#34892;&#35828;&#26126;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#25928;&#29992;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the utility of synthetic data is critical for measuring the effectiveness and efficiency of synthetic algorithms. Existing results focus on empirical evaluations of the utility of synthetic data, whereas the theoretical understanding of how utility is affected by synthetic data algorithms remains largely unexplored. This paper establishes utility theory from a statistical perspective, aiming to quantitatively assess the utility of synthetic algorithms based on a general metric. The metric is defined as the absolute difference in generalization between models trained on synthetic and original datasets. We establish analytical bounds for this utility metric to investigate critical conditions for the metric to converge. An intriguing result is that the synthetic feature distribution is not necessarily identical to the original one for the convergence of the utility metric as long as the model specification in downstream learning tasks is correct. Another important utility metri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21333;&#19968;&#30446;&#26631;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.10014</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#30446;&#26631;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multi-Objective based Parameter Optimization for Deep Learning. (arXiv:2305.10014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21333;&#19968;&#30446;&#26631;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#30446;&#21069;&#26368;&#20026;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#23588;&#20854;&#26159;&#22312;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#35745;&#21442;&#25968;&#20173;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#65292;&#22240;&#27492;&#24471;&#21040;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#38750;&#24120;&#36153;&#26102;&#19988;&#26102;&#24120;&#19981;&#21487;&#33021;&#12290;&#20248;&#21270;&#28145;&#24230;&#32593;&#32476;&#21442;&#25968;&#38656;&#35201;&#26356;&#39640;&#25910;&#25947;&#29575;&#30340;&#25913;&#36827;&#20248;&#21270;&#31639;&#27861;&#12290;&#32780;&#22522;&#20110;&#21333;&#19968;&#30446;&#26631;&#30340;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#32791;&#26102;&#19988;&#19981;&#33021;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#20445;&#35777;&#26368;&#20339;&#24615;&#33021;&#12290;&#21253;&#21547;&#22810;&#20010;&#38656;&#35201;&#21516;&#26102;&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#23646;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#33539;&#30068;&#65292;&#26377;&#26102;&#20063;&#34987;&#31216;&#20026;&#24085;&#32047;&#25176;&#20248;&#21270;&#12290;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26159;&#21442;&#25968;&#20248;&#21270;&#30340;&#19968;&#31181;&#22791;&#36873;&#19988;&#23454;&#29992;&#30340;&#26041;&#26696;&#65292;&#20294;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#26412;&#25991;&#23558;&#39318;&#20808;&#32508;&#36848;&#21333;&#19968;&#30446;&#26631;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#65292;&#28982;&#21518;&#37325;&#28857;&#20851;&#27880;&#38754;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#22810;&#30446;&#26631;&#20248;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models form one of the most powerful machine learning models for the extraction of important features. Most of the designs of deep neural models, i.e., the initialization of parameters, are still manually tuned. Hence, obtaining a model with high performance is exceedingly time-consuming and occasionally impossible. Optimizing the parameters of the deep networks, therefore, requires improved optimization algorithms with high convergence rates. The single objective-based optimization methods generally used are mostly time-consuming and do not guarantee optimum performance in all cases. Mathematical optimization problems containing multiple objective functions that must be optimized simultaneously fall under the category of multi-objective optimization sometimes referred to as Pareto optimization. Multi-objective optimization problems form one of the alternatives yet useful options for parameter optimization. However, this domain is a bit less explored. In this survey, we f
&lt;/p&gt;</description></item><item><title>Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09993</link><description>&lt;p&gt;
Reprompting: &#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#33258;&#21160;&#25512;&#26029;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09993
&lt;/p&gt;
&lt;p&gt;
Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Reprompting&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#32473;&#23450;&#20219;&#21153;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#25105;&#20204;&#25512;&#26029;&#36866;&#29992;&#20110;&#19968;&#32452;&#35757;&#32451;&#26679;&#20363;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20808;&#21069;&#37319;&#26679;&#30340;&#35299;&#20316;&#20026;&#29238;&#25552;&#31034;&#65292;&#36845;&#20195;&#22320;&#37319;&#26679;&#26032;&#30340;&#37197;&#26041;&#26469;&#35299;&#20915;&#20854;&#20182;&#35757;&#32451;&#38382;&#39064;&#12290;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#20116;&#20010;Big-Bench Hard&#20219;&#21153;&#20013;&#65292;Reprompting&#30340;&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#22522;&#32447;&#12290;Reprompting&#36824;&#21487;&#20197;&#20419;&#36827;&#30693;&#35782;&#20174;&#19968;&#20010;&#26356;&#24378;&#30340;&#27169;&#22411;&#21040;&#19968;&#20010;&#36739;&#24369;&#30340;&#27169;&#22411;&#30340;&#36716;&#31227;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Reprompting&#30456;&#23545;&#20110;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24102;&#26469;&#20102;&#39640;&#36798;+17&#20010;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22810;&#22495;&#32593;&#32476;&#65292;&#26088;&#22312;&#24674;&#22797;&#30701;&#26102;&#38388;&#20869;&#33719;&#21462;&#30340;&#20302;&#36136;&#37327;&#28096;&#31881;&#26679;PET&#22270;&#20687;&#12290;&#32593;&#32476;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#24341;&#20837;&#20102;&#26144;&#23556;&#26631;&#31614;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#65292;&#20351;&#24471;&#35813;&#32593;&#32476;&#21487;&#22312;&#22810;&#20010;&#35757;&#32451;&#22495;&#21644;&#26410;&#30693;&#22495;&#20013;&#39640;&#25928;&#26657;&#27491;&#28096;&#31881;&#26679;PET&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.09986</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22810;&#22495;&#32593;&#32476;&#29992;&#20110;&#30701;&#26102;&#38388;&#25195;&#25551;&#28096;&#31881;&#26679;PET&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A robust multi-domain network for short-scanning amyloid PET reconstruction. (arXiv:2305.09986v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22810;&#22495;&#32593;&#32476;&#65292;&#26088;&#22312;&#24674;&#22797;&#30701;&#26102;&#38388;&#20869;&#33719;&#21462;&#30340;&#20302;&#36136;&#37327;&#28096;&#31881;&#26679;PET&#22270;&#20687;&#12290;&#32593;&#32476;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#24341;&#20837;&#20102;&#26144;&#23556;&#26631;&#31614;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#65292;&#20351;&#24471;&#35813;&#32593;&#32476;&#21487;&#22312;&#22810;&#20010;&#35757;&#32451;&#22495;&#21644;&#26410;&#30693;&#22495;&#20013;&#39640;&#25928;&#26657;&#27491;&#28096;&#31881;&#26679;PET&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22810;&#22495;&#32593;&#32476;&#65292;&#26088;&#22312;&#24674;&#22797;&#30701;&#26102;&#38388;&#20869;&#33719;&#21462;&#30340;&#20302;&#36136;&#37327;&#28096;&#31881;&#26679;PET&#22270;&#20687;&#12290;&#20351;&#29992;&#22810;&#20010;&#22495;&#20013;&#26469;&#33258;&#30701;&#65288;2&#20998;&#38047;&#65289;&#21644;&#26631;&#20934;&#65288;20&#20998;&#38047;&#65289;&#25195;&#25551;&#26102;&#38388;&#30340;PET&#22270;&#20687;&#23545;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#26144;&#23556;&#26631;&#31614;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#32593;&#32476;&#21487;&#22312;&#22810;&#20010;&#35757;&#32451;&#22495;&#21644;&#26410;&#30693;&#22495;&#20013;&#39640;&#25928;&#26657;&#27491;&#28096;&#31881;&#26679;PET&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a robust multi-domain network designed to restore low-quality amyloid PET images acquired in a short period of time. The proposed method is trained on pairs of PET images from short (2 minutes) and standard (20 minutes) scanning times, sourced from multiple domains. Learning relevant image features between these domains with a single network is challenging. Our key contribution is the introduction of a mapping label, which enables effective learning of specific representations between different domains. The network, trained with various mapping labels, can efficiently correct amyloid PET datasets in multiple training domains and unseen domains, such as those obtained with new radiotracers, acquisition protocols, or PET scanners. Internal, temporal, and external validations demonstrate the effectiveness of the proposed method. Notably, for external validation datasets from unseen domains, the proposed method achieved comparable or superior results relative to methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32463;&#20856;&#30340;SGD&#26694;&#26550;&#19979;&#23454;&#29616;&#33258;&#36866;&#24212;&#27493;&#38271;&#36873;&#25321;&#65292;&#22312;&#36923;&#36753;&#22238;&#24402;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#25163;&#21160;&#35843;&#25972;&#24471;&#21040;&#30340;&#26368;&#20339;&#27493;&#38271;&#30456;&#24403;&#30340;&#27493;&#38271;&#12290;</title><link>http://arxiv.org/abs/2305.09978</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#38543;&#26426;&#27604;&#29575;&#36319;&#36394;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems. (arXiv:2305.09978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32463;&#20856;&#30340;SGD&#26694;&#26550;&#19979;&#23454;&#29616;&#33258;&#36866;&#24212;&#27493;&#38271;&#36873;&#25321;&#65292;&#22312;&#36923;&#36753;&#22238;&#24402;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#25163;&#21160;&#35843;&#25972;&#24471;&#21040;&#30340;&#26368;&#20339;&#27493;&#38271;&#30456;&#24403;&#30340;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21644;&#20219;&#21153;&#37117;&#20381;&#36182;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#12290;&#26377;&#25928;&#30340;&#27493;&#38271;&#36873;&#25321;&#23545;&#31639;&#27861;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20419;&#36827;&#20102;&#35832;&#22914;ADAM&#25110;AdaGrad&#20043;&#31867;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;&#32463;&#20856;&#30340;SGD&#26694;&#26550;&#19979;&#23454;&#29616;&#33258;&#36866;&#24212;&#27493;&#38271;&#36873;&#25321;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#20854;&#20182;&#38543;&#26426;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#28789;&#24863;&#26469;&#33258;&#20256;&#32479;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#25216;&#26415;&#65292;&#24182;&#21463;&#21040;&#20998;&#26512;&#21457;&#29616;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21512;&#29702;&#26465;&#20214;&#19979;&#65292;&#35813;&#31639;&#27861;&#20135;&#29983;&#31526;&#21512;&#33391;&#22909;&#29702;&#35770;&#35201;&#27714;&#30340;&#27493;&#38271;&#65292;&#24182;&#22312;&#26399;&#26395;&#19979;&#29983;&#25104;&#25910;&#25947;&#20110;&#35299;&#30340;&#38745;&#27490;&#37051;&#22495;&#30340;&#36845;&#20195;&#12290;&#25105;&#20204;&#22312;&#36923;&#36753;&#22238;&#24402;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#25163;&#21160;&#35843;&#25972;&#24471;&#21040;&#30340;&#26368;&#20339;&#27493;&#38271;&#30456;&#24403;&#30340;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning applications and tasks rely on the stochastic gradient descent (SGD) algorithm and its variants. Effective step length selection is crucial for the success of these algorithms, which has motivated the development of algorithms such as ADAM or AdaGrad. In this paper, we propose a novel algorithm for adaptive step length selection in the classical SGD framework, which can be readily adapted to other stochastic algorithms. Our proposed algorithm is inspired by traditional nonlinear optimization techniques and is supported by analytical findings. We show that under reasonable conditions, the algorithm produces step lengths in line with well-established theoretical requirements, and generates iterates that converge to a stationary neighborhood of a solution in expectation. We test the proposed algorithm on logistic regressions and deep neural networks and demonstrate that the algorithm can generate step lengths comparable to the best step length obtained from manual tu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv8&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#26816;&#27979;&#30340;&#39134;&#34892;&#29289;&#20307;&#65307;&#36890;&#36807;&#36827;&#19968;&#27493;&#35757;&#32451;&#65292;&#29983;&#25104;&#31934;&#32454;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09972</link><description>&lt;p&gt;
&#22522;&#20110;YOLOv8&#30340;&#23454;&#26102;&#39134;&#34892;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-Time Flying Object Detection with YOLOv8. (arXiv:2305.09972v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv8&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#26816;&#27979;&#30340;&#39134;&#34892;&#29289;&#20307;&#65307;&#36890;&#36807;&#36827;&#19968;&#27493;&#35757;&#32451;&#65292;&#29983;&#25104;&#31934;&#32454;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#39134;&#34892;&#29289;&#20307;&#65292;&#21487;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20379;&#23454;&#26045;&#30340;&#31934;&#32454;&#27169;&#22411;&#12290;&#25105;&#20204;&#20808;&#20351;&#29992;&#21253;&#21547;40&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#23545;&#36890;&#29992;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24378;&#21046;&#27169;&#22411;&#25552;&#21462;&#25277;&#35937;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#21442;&#25968;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#22312;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#30495;&#23454;&#29615;&#22659;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#25105;&#20204;&#30340;&#31934;&#32454;&#27169;&#22411;&#12290;&#30001;&#20110;&#39134;&#34892;&#29289;&#20307;&#30340;&#29289;&#20307;&#31354;&#38388;&#22823;&#23567;/&#32437;&#27178;&#27604;&#12289;&#36895;&#24230;&#12289;&#36974;&#25377;&#21644;&#32972;&#26223;&#30340;&#24046;&#24322;&#24456;&#22823;&#65292;&#22240;&#27492;&#39134;&#34892;&#29289;&#20307;&#30340;&#26816;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#21333;&#27425;&#26816;&#27979;&#22120;YOLOv8&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#24615;&#33021;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a generalized model for real-time detection of flying objects that can be used for transfer learning and further research, as well as a refined model that is ready for implementation. We achieve this by training our first generalized model on a data set containing 40 different classes of flying objects, forcing the model to extract abstract feature representations. We then perform transfer learning with these learned parameters on a data set more representative of real world environments (i.e., higher frequency of occlusion, small spatial sizes, rotations, etc.) to generate our refined model. Object detection of flying objects remains challenging due to large variance object spatial sizes/aspect ratios, rate of speed, occlusion, and clustered backgrounds. To address some of the presented challenges while simultaneously maximizing performance, we utilize the current state of the art single-shot detector, YOLOv8, in an attempt to find the best tradeoff between inferen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#21464;&#38271;&#24230;&#23884;&#20837;&#65288;VLEs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#29983;&#25104;&#30001;&#20219;&#24847;&#25968;&#37327;&#26631;&#35760;&#32452;&#25104;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;VLE&#22312;&#28041;&#21450;&#37325;&#26500;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;VAE&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.09967</link><description>&lt;p&gt;
&#21487;&#21464;&#38271;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Variable Length Embeddings. (arXiv:2305.09967v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#21464;&#38271;&#24230;&#23884;&#20837;&#65288;VLEs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#29983;&#25104;&#30001;&#20219;&#24847;&#25968;&#37327;&#26631;&#35760;&#32452;&#25104;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;VLE&#22312;&#28041;&#21450;&#37325;&#26500;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;VAE&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;&#21487;&#21464;&#38271;&#24230;&#23884;&#20837;&#65288;VLEs&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#30001;&#20219;&#24847;&#25968;&#37327;&#26631;&#35760;&#32452;&#25104;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;VLEs&#22312;&#28041;&#21450;&#37325;&#26500;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iNaturalist&#21644;ImageNet&#25968;&#25454;&#38598;&#28151;&#21512;&#20351;&#29992;&#30340;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;VLE&#19982;&#26368;&#20808;&#36827;&#30340;VAE&#30456;&#27604;&#65292;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#37325;&#26500;&#32467;&#26524;&#65292;&#20165;&#20351;&#29992;&#20102;&#19981;&#21040;&#21313;&#20998;&#20043;&#19968;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a novel deep learning architecture, Variable Length Embeddings (VLEs), an autoregressive model that can produce a latent representation composed of an arbitrary number of tokens. As a proof of concept, we demonstrate the capabilities of VLEs on tasks that involve reconstruction and image decomposition. We evaluate our experiments on a mix of the iNaturalist and ImageNet datasets and find that VLEs achieve comparable reconstruction results to a state of the art VAE, using less than a tenth of the parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SIMGA&#65292;&#23427;&#36890;&#36807;SimRank&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#33410;&#28857;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340;&#20256;&#25773;&#25928;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09958</link><description>&lt;p&gt;
SIMGA&#65306;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#19982;&#39640;&#25928;&#30340;&#20840;&#23616;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SIMGA: A Simple and Effective Heterophilous Graph Neural Network with Efficient Global Aggregation. (arXiv:2305.09958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SIMGA&#65292;&#23427;&#36890;&#36807;SimRank&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#33410;&#28857;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340;&#20256;&#25773;&#25928;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36935;&#21040;&#24322;&#36136;&#24615;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#21363;&#22240;&#20026;&#23616;&#37096;&#21644;&#32479;&#19968;&#32858;&#21512;&#32780;&#23548;&#33268;&#30340;&#30456;&#37051;&#33410;&#28857;&#19981;&#30456;&#20284;&#12290;&#29616;&#26377;&#30340;&#24322;&#36136;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35797;&#22270;&#25972;&#21512;&#20840;&#23616;&#32858;&#21512;&#30340;&#23581;&#35797;&#36890;&#24120;&#38656;&#35201;&#36845;&#20195;&#22320;&#32500;&#25252;&#21644;&#26356;&#26032;&#20840;&#22270;&#20449;&#24687;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377; $n$ &#20010;&#33410;&#28857;&#30340;&#22270;&#65292;&#36825;&#38656;&#35201; $\mathcal{O}(n^2)$ &#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#22823;&#22411;&#22270;&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SIMGA&#65292;&#19968;&#31181;&#23558; SimRank &#32467;&#26500;&#30456;&#20284;&#24230;&#27979;&#37327;&#20316;&#20026;&#20840;&#23616;&#32858;&#21512;&#30340; GNN &#32467;&#26500;&#12290; SIMGA &#30340;&#35774;&#35745;&#31616;&#21333;&#65292;&#19988;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#37117;&#26377;&#30528;&#26377; promising &#30340;&#32467;&#26524;&#12290;SIMGA &#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#25104;&#20026;&#31532;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340; $n$ &#20256;&#25773;&#25928;&#29575;&#30340;&#24322;&#36136;&#24615; GNN &#27169;&#22411;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558; SimRank &#35270;&#20026; GNN &#30340;&#19968;&#31181;&#26032;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#20102;&#27719;&#32858;&#33410;&#28857;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) realize great success in graph learning but suffer from performance loss when meeting heterophily, i.e. neighboring nodes are dissimilar, due to their local and uniform aggregation. Existing attempts in incoorporating global aggregation for heterophilous GNNs usually require iteratively maintaining and updating full-graph information, which entails $\mathcal{O}(n^2)$ computation efficiency for a graph with $n$ nodes, leading to weak scalability to large graphs. In this paper, we propose SIMGA, a GNN structure integrating SimRank structural similarity measurement as global aggregation. The design of SIMGA is simple, yet it leads to promising results in both efficiency and effectiveness. The simplicity of SIMGA makes it the first heterophilous GNN model that can achieve a propagation efficiency near-linear to $n$. We theoretically demonstrate its effectiveness by treating SimRank as a new interpretation of GNN and prove that the aggregated node representation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.09957</link><description>&lt;p&gt;
&#28145;&#24230;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23545;&#24212;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep quantum neural networks form Gaussian processes. (arXiv:2305.09957v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20808;&#39564;&#26465;&#20214;&#24320;&#22987;&#21021;&#22987;&#21270;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#25968;&#30446;&#36275;&#22815;&#22823;&#30340;&#26497;&#38480;&#19979;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#20063;&#23384;&#22312;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;QNNs&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#32500;&#24230;$d$&#36275;&#22815;&#22823;&#26102;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#30001;&#20110;&#36755;&#20837;&#29366;&#24577;&#12289;&#27979;&#37327;&#30340;&#21487;&#35266;&#27979;&#37327;&#20197;&#21450;&#37193;&#30697;&#38453;&#30340;&#20803;&#32032;&#19981;&#29420;&#31435;&#31561;&#22240;&#32032;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#23545;&#36825;&#19968;&#32467;&#26524;&#30340;&#25512;&#23548;&#27604;&#32463;&#20856;&#24773;&#24418;&#26356;&#21152;&#24494;&#22937;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#20010;&#37325;&#35201;&#21518;&#26524;&#26159;&#65292;&#36825;&#20010;&#32467;&#26524;&#24471;&#21040;&#30340;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#22320;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;Haar&#38543;&#26426;QNNs&#20013;&#30340;&#27979;&#37327;&#29616;&#35937;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#35201;&#26356;&#20005;&#37325;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28436;&#21592;&#30340;&#38598;&#20013;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that artificial neural networks initialized from independent and identically distributed priors converge to Gaussian processes in the limit of large number of neurons per hidden layer. In this work we prove an analogous result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of certain models based on Haar random unitary or orthogonal deep QNNs converge to Gaussian processes in the limit of large Hilbert space dimension $d$. The derivation of this result is more nuanced than in the classical case due the role played by the input states, the measurement observable, and the fact that the entries of unitary matrices are not independent. An important consequence of our analysis is that the ensuing Gaussian processes cannot be used to efficiently predict the outputs of the QNN via Bayesian statistics. Furthermore, our theorems imply that the concentration of measure phenomenon in Haar random QNNs is much worse than previously thought, as we prove that ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#30340;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#23567;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.09956</link><description>&lt;p&gt;
&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Adversarial Consistency of Surrogate Risks for Binary Classification. (arXiv:2305.09956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#30340;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29992;&#20110;&#40065;&#26834;&#20108;&#20998;&#31867;&#30340;&#20195;&#29702;&#39118;&#38505;&#30340;&#19968;&#33268;&#24615;&#12290;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23398;&#20064;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#35797;&#22270;&#22312;&#27599;&#20010;&#31034;&#20363;&#21487;&#20197;&#22312;&#23567;&#29699;&#20869;&#34987;&#24694;&#24847;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#26399;&#26395;&#30340;$0$-$1$&#25439;&#22833;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#23436;&#25972;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#36825;&#20123;&#38598;&#26159;&#8220;&#19968;&#33268;&#8221;&#30340;&#65292;&#21363;&#21487;&#20197;&#26367;&#25442;$0$-$1$&#25439;&#22833;&#32780;&#19981;&#24433;&#21709;&#21407;&#22987;&#23545;&#25239;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#24207;&#21015;&#30340;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#29992;&#20110;$\rho$-margin&#25439;&#22833;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#30340;&#37327;&#21270;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#26126;&#26174;&#36739;&#23567;&#65292;&#22312;&#26631;&#20934;&#35774;&#32622;&#20013;&#65292;&#35768;&#22810;&#24120;&#35265;&#30340;&#20195;&#29702;&#37117;&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the consistency of surrogate risks for robust binary classification. It is common to learn robust classifiers by adversarial training, which seeks to minimize the expected $0$-$1$ loss when each example can be maliciously corrupted within a small ball. We give a simple and complete characterization of the set of surrogate loss functions that are \emph{consistent}, i.e., that can replace the $0$-$1$ loss without affecting the minimizing sequences of the original adversarial risk, for any data distribution. We also prove a quantitative version of adversarial consistency for the $\rho$-margin loss. Our results reveal that the class of adversarially consistent surrogates is substantially smaller than in the standard setting, where many common surrogates are known to be consistent.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#23567;&#21021;&#22987;&#21270;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#26041;&#27861;&#19979;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20957;&#32858;&#29616;&#35937;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#35757;&#32451;&#26399;&#38388;&#65292;CNN&#30340;&#21367;&#31215;&#26680;&#23558;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#25110;&#20960;&#20010;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.09947</link><description>&lt;p&gt;
&#29702;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#20957;&#32467;
&lt;/p&gt;
&lt;p&gt;
Understanding the Initial Condensation of Convolutional Neural Networks. (arXiv:2305.09947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#23567;&#21021;&#22987;&#21270;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#26041;&#27861;&#19979;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20957;&#32858;&#29616;&#35937;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#35757;&#32451;&#26399;&#38388;&#65292;CNN&#30340;&#21367;&#31215;&#26680;&#23558;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#25110;&#20960;&#20010;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#23567;&#21021;&#22987;&#21270;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#20840;&#36830;&#25509;&#32593;&#32476;&#22312;&#35757;&#32451;&#26399;&#38388;&#34920;&#29616;&#20986;&#19968;&#31181;&#31216;&#20026;&#20957;&#32467;&#30340;&#29616;&#35937;&#12290;&#36825;&#31181;&#29616;&#35937;&#25351;&#30340;&#26159;&#38544;&#23618;&#31070;&#32463;&#20803;&#30340;&#36755;&#20837;&#26435;&#37325;&#22312;&#35757;&#32451;&#26399;&#38388;&#20957;&#32858;&#25104;&#23396;&#31435;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#21442;&#25968;&#31354;&#38388;&#20013;&#26397;&#21521;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23545;&#20957;&#32858;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#21463;&#21040;&#23567;&#21021;&#22987;&#21270;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#24433;&#21709;&#26102;&#65292;CNN&#23618;&#20869;&#30340;&#21367;&#31215;&#26680;&#26435;&#37325;&#22312;&#35757;&#32451;&#26399;&#38388;&#20063;&#20250;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20957;&#32858;&#24230;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#65292;&#23567;&#21021;&#22987;&#21270;&#30340;&#20004;&#23618;CNN&#30340;&#21367;&#31215;&#26680;&#23558;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#25110;&#20960;&#20010;&#26041;&#21521;&#12290;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#34892;&#20026;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research has shown that fully-connected networks with small initialization and gradient-based training methods exhibit a phenomenon known as condensation during training. This phenomenon refers to the input weights of hidden neurons condensing into isolated orientations during training, revealing an implicit bias towards simple solutions in the parameter space. However, the impact of neural network structure on condensation has not been investigated yet. In this study, we focus on the investigation of convolutional neural networks (CNNs). Our experiments suggest that when subjected to small initialization and gradient-based training methods, kernel weights within the same CNN layer also cluster together during training, demonstrating a significant degree of condensation. Theoretically, we demonstrate that in a finite training period, kernels of a two-layer CNN with small initialization will converge to one or a few directions. This work represents a step towards a better under
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;DeepMSS&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09946</link><description>&lt;p&gt;
DeepMSS&#65306;&#22522;&#20110;PET/CT&#22270;&#20687;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20999;&#29255;&#21040;&#29983;&#23384;&#39044;&#27979;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images. (arXiv:2305.09946v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;DeepMSS&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#39044;&#27979;&#26159;&#30284;&#30151;&#31649;&#29702;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#29992;&#20110;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#29983;&#23384;&#39044;&#27979;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#25191;&#34892;&#32959;&#30244;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25351;&#23548;&#27169;&#22411;&#25552;&#21462;&#19982;&#32959;&#30244;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#25506;&#32034;&#32959;&#30244;&#22806;&#39044;&#21518;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#23616;&#37096;&#28107;&#24052;&#32467;&#36716;&#31227;&#21644;&#37051;&#36817;&#32452;&#32455;&#20405;&#34989;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#26041;&#38754;&#27424;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepMSS&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20999;&#29255;&#21040;&#29983;&#23384;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#20219;&#21153;&#26469;&#36827;&#34892;&#12290;&#23545;&#20110;&#20998;&#21106;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#12290;&#23545;&#20110;&#29983;&#23384;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;MMPAN&#30340;&#29305;&#24449;&#34920;&#31034;&#24182;&#25191;&#34892;&#29983;&#23384;&#39044;&#27979;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DeepMSS&#27169;&#22411;&#22312;&#29983;&#23384;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival prediction is a major concern for cancer management. Deep survival models based on deep learning have been widely adopted to perform end-to-end survival prediction from medical images. Recent deep survival models achieved promising performance by jointly performing tumor segmentation with survival prediction, where the models were guided to extract tumor-related information through Multi-Task Learning (MTL). However, existing deep survival models have difficulties in exploring out-of-tumor prognostic information (e.g., local lymph node metastasis and adjacent tissue invasions). In addition, existing deep survival models are underdeveloped in utilizing multi-modality images. Empirically-designed strategies were commonly adopted to fuse multi-modality information via fixed pre-designed networks. In this study, we propose a Deep Multi-modality Segmentation-to-Survival model (DeepMSS) for survival prediction from PET/CT images. Instead of adopting MTL, we propose a novel Segmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#21305;&#20857;&#22561;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479;&#65292;&#19982;&#32463;&#20856;&#30340;&#23494;&#27463;&#26681;&#31995;&#32479; XCS &#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426; FrozenLake &#29615;&#22659;&#20013;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09945</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21305;&#20857;&#22561;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479;&#65306;&#19982; XCS &#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS. (arXiv:2305.09945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#21305;&#20857;&#22561;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479;&#65292;&#19982;&#32463;&#20856;&#30340;&#23494;&#27463;&#26681;&#31995;&#32479; XCS &#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426; FrozenLake &#29615;&#22659;&#20013;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#23545;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20852;&#36259;&#26377;&#25152;&#22686;&#21152;&#65292;&#20294;&#26159;&#19982;&#35937;&#24449;&#24615;&#31995;&#32479;&#30456;&#27604;&#65292;&#36825;&#20123;&#36830;&#25509;&#20027;&#20041;&#26041;&#27861;&#19981;&#36879;&#26126;&#12290;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479; (LCS) &#26159;&#36827;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#34987;&#24402;&#31867;&#20026;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021; (XAI)&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#35268;&#21017;&#30340;&#26412;&#36136;&#12290;&#23494;&#27463;&#26681; LCS &#36890;&#24120;&#22312; RL &#39046;&#22495;&#20013;&#20351;&#29992;&#65292;&#32780;&#21305;&#20857;&#22561;&#31995;&#32479;&#65288;&#20363;&#22914; SAMUEL&#65289;&#30001;&#20110;&#22797;&#26434;&#30340;&#31639;&#27861;&#35774;&#35745;&#21644;&#39640;&#35745;&#31639;&#35201;&#27714;&#32780;&#24456;&#23569;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#27604;&#23494;&#27463;&#26681;&#31995;&#32479;&#26356;&#31616;&#27905;/&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#20004;&#20010;&#26032;&#30340;&#21305;&#20857;&#22561; LCS&#65292;&#20197;&#35299;&#20915; RL &#39046;&#22495;&#30340;&#38382;&#39064;&#65306;PPL-DL &#21644; PPL-ST&#12290;&#21069;&#32773;&#20805;&#24403;&#8220;&#38646;&#32423;&#8221;&#31995;&#32479;&#65292;&#21518;&#32773;&#37325;&#35775;&#20102; SAMUEL &#30340;&#26680;&#24515;&#33945;&#29305;&#21345;&#32599;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#20272;&#35745;&#35268;&#21017;&#24378;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20004;&#20010;&#21305;&#20857;&#22561;&#31995;&#32479;&#19982;&#23494;&#27463;&#26681;&#31995;&#32479; XCS &#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426; FrozenLake &#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PPL-ST&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in reinforcement learning (RL) has recently surged due to the application of deep learning techniques, but these connectionist approaches are opaque compared with symbolic systems. Learning Classifier Systems (LCSs) are evolutionary machine learning systems that can be categorised as eXplainable AI (XAI) due to their rule-based nature. Michigan LCSs are commonly used in RL domains as the alternative Pittsburgh systems (e.g. SAMUEL) suffer from complex algorithmic design and high computational requirements; however they can produce more compact/interpretable solutions than Michigan systems. We aim to develop two novel Pittsburgh LCSs to address RL domains: PPL-DL and PPL-ST. The former acts as a "zeroth-level" system, and the latter revisits SAMUEL's core Monte Carlo learning mechanism for estimating rule strength. We compare our two Pittsburgh systems to the Michigan system XCS across deterministic and stochastic FrozenLake environments. Results show that PPL-ST performs on-pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IBC&#65289;&#65292;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#31232;&#30095;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09943</link><description>&lt;p&gt;
&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#22686;&#24378;&#23398;&#20064;&#65306;&#38544;&#24335;&#21452;&#21521;&#35838;&#31243;&#27861;
&lt;/p&gt;
&lt;p&gt;
Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum. (arXiv:2305.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IBC&#65289;&#65292;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#31232;&#30095;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#20165;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#33719;&#24471;&#22797;&#26434;&#25216;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20551;&#35774;&#22312;&#27599;&#20010;&#21608;&#26399;&#32467;&#26463;&#26102;&#37117;&#21487;&#20197;&#36731;&#26131;&#22320;&#22238;&#21040;&#21021;&#22987;&#29366;&#24577;&#12290;&#36825;&#31181;&#20551;&#35774;&#22952;&#30861;&#20102;&#20855;&#36523;&#20195;&#29702;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#22240;&#20026;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#37325;&#32622;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#32321;&#29712;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#33021;&#22815;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#65288;ARL&#65289;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ARL&#26041;&#27861;&#21463;&#21040;&#20854;&#23545;&#20808;&#21069;&#25968;&#25454;&#30340;&#20381;&#36182;&#30340;&#38480;&#21046;&#65292;&#26080;&#27861;&#22312;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#21644;&#21452;&#21521;&#35838;&#31243;&#30340;&#26080;&#28436;&#31034;ARL&#31639;&#27861;&#65288;IBC&#65289;&#12290;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#20197;&#21450;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#27604;&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#27861;&#36824;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has achieved great success in acquiring complex skills solely from environmental interactions, it assumes that resets to the initial state are readily available at the end of each episode. Such an assumption hinders the autonomous learning of embodied agents due to the time-consuming and cumbersome workarounds for resetting in the physical world. Hence, there has been a growing interest in autonomous RL (ARL) methods that are capable of learning from non-episodic interactions. However, existing works on ARL are limited by their reliance on prior data and are unable to learn in environments where task-relevant interactions are sparse. In contrast, we propose a demonstration-free ARL algorithm via Implicit and Bi-directional Curriculum (IBC). With an auxiliary agent that is conditionally activated upon learning progress and a bidirectional goal curriculum based on optimal transport, our method outperforms previous methods, even the ones that leverage dem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.09938</link><description>&lt;p&gt;
&#22270;&#20013;&#38271;&#23614;&#31867;&#21035;&#30340;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterizing Long-Tail Categories on Graphs. (arXiv:2305.09938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#21253;&#25324;&#37329;&#34701;&#20132;&#26131;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#21512;&#20316;&#32593;&#32476;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#20316;&#21697;&#20027;&#35201;&#38598;&#20013;&#20110;&#36890;&#36807;&#22270;&#22686;&#24378;&#25110;&#30446;&#26631;&#37325;&#26032;&#21152;&#26435;&#28040;&#38500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#25991;&#29486;&#25552;&#20379;&#29702;&#35770;&#24037;&#20855;&#26469;&#34920;&#24449;&#22270;&#19978;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#65292;&#24182;&#29702;&#35299;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21363;&#27599;&#20010;&#20219;&#21153;&#23545;&#24212;&#20110;&#39044;&#27979;&#19968;&#20010;&#29305;&#23450;&#30340;&#31867;&#21035;&#65292;&#25552;&#20986;&#20102;&#22270;&#19978;&#38271;&#23614;&#20998;&#31867;&#30340;&#31532;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#38271;&#23614;&#20998;&#31867;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#25152;&#26377;&#20219;&#21153;&#20013;&#30340;&#25439;&#22833;&#33539;&#22260;&#21644;&#20219;&#21153;&#24635;&#25968;&#30340;&#25903;&#37197;&#12290;&#22312;&#29702;&#35770;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#34920;&#24449;&#38271;&#23614;&#31867;&#21035;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tail data distributions are prevalent in many real-world networks, including financial transaction networks, e-commerce networks, and collaboration networks. Despite the success of recent developments, the existing works mainly focus on debiasing the machine learning models via graph augmentation or objective reweighting. However, there is limited literature that provides a theoretical tool to characterize the behaviors of long-tail categories on graphs and understand the generalization performance in real scenarios. To bridge this gap, we propose the first generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular category. Our theoretical results show that the generalization performance of long-tail classification is dominated by the range of losses across all tasks and the total number of tasks. Building upon the theoretical findings, we propose a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#20844;&#24179;&#12289;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#32771;&#34385;&#32676;&#20307;&#20844;&#24179;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09931</link><description>&lt;p&gt;
&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32676;&#20307;&#20559;&#35265;:&#36229;&#36234;&#26412;&#22320;&#20844;&#24179;
&lt;/p&gt;
&lt;p&gt;
Mitigating Group Bias in Federated Learning: Beyond Local Fairness. (arXiv:2305.09931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#20844;&#24179;&#12289;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#32771;&#34385;&#32676;&#20307;&#20844;&#24179;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#32676;&#20307;&#20844;&#24179;&#30340;&#38382;&#39064;&#24050;&#32463;&#34987;&#35748;&#35782;&#21040;&#19968;&#27573;&#26102;&#38388;&#20102;&#65292;&#20854;&#20013;&#26576;&#20123;&#23376;&#20154;&#32676;&#25110;&#32676;&#20307;&#34987;&#20248;&#20808;&#32771;&#34385;&#12290;&#34429;&#28982;&#22312;&#38598;&#20013;&#24335;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#32531;&#35299;&#31574;&#30053;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#30452;&#25509;&#36866;&#29992;&#65292;&#22240;&#20026;&#25968;&#25454;&#23384;&#22312;&#20110;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#24182;&#34987;&#31169;&#19979;&#23384;&#20648;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#25552;&#35758;&#35797;&#22270;&#22312;&#32858;&#21512;&#20043;&#21069;&#22312;&#23458;&#25143;&#31471;&#27700;&#24179;&#19978;&#32531;&#35299;&#20559;&#24046;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26412;&#22320;&#20844;&#24179;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20840;&#23616;&#27169;&#22411;&#20844;&#24179;&#24615;&#21644;&#26412;&#22320;&#27169;&#22411;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#20102;&#26412;&#22320;&#20844;&#24179;&#35757;&#32451;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20844;&#24179;&#25351;&#26631;&#31867;&#21035;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27719;&#24635;&#32479;&#35745;&#20449;&#24687;&#26469;&#33719;&#24471;&#20840;&#23616;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20840;&#23616;&#20844;&#24179;&#24615;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30452;&#25509;&#26368;&#23567;&#21270;&#20840;&#23616;&#27169;&#22411;&#21644;&#26412;&#22320;&#27169;&#22411;&#20043;&#38388;&#30340;&#32463;&#39564;&#24809;&#32602;L2&#36317;&#31163;&#65292;&#21516;&#26102;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#32771;&#34385;&#32676;&#20307;&#20844;&#24179;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The issue of group fairness in machine learning models, where certain sub-populations or groups are favored over others, has been recognized for some time. While many mitigation strategies have been proposed in centralized learning, many of these methods are not directly applicable in federated learning, where data is privately stored on multiple clients. To address this, many proposals try to mitigate bias at the level of clients before aggregation, which we call locally fair training. However, the effectiveness of these approaches is not well understood. In this work, we investigate the theoretical foundation of locally fair training by studying the relationship between global model fairness and local model fairness. Additionally, we prove that for a broad class of fairness metrics, the global model's fairness can be obtained using only summary statistics from local clients. Based on that, we propose a globally fair training algorithm that directly minimizes the penalized empirical l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27169;&#22411;&#39564;&#35777;&#20316;&#20026;&#27010;&#29575;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#20272;&#35745;&#25925;&#38556;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#34920;&#31034;&#25925;&#38556;&#36712;&#36857;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#36712;&#36857;&#26799;&#24230;&#12290;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#26041;&#38754;&#21462;&#24471;&#20102;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.09930</link><description>&lt;p&gt;
&#27169;&#22411;&#39564;&#35777;&#20316;&#20026;&#27010;&#29575;&#25512;&#26029;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-based Validation as Probabilistic Inference. (arXiv:2305.09930v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27169;&#22411;&#39564;&#35777;&#20316;&#20026;&#27010;&#29575;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#20272;&#35745;&#25925;&#38556;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#34920;&#31034;&#25925;&#38556;&#36712;&#36857;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#36712;&#36857;&#26799;&#24230;&#12290;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#26041;&#38754;&#21462;&#24471;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22833;&#36133;&#20998;&#24067;&#26159;&#39564;&#35777;&#33258;&#20027;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#23547;&#25214;&#19968;&#23567;&#33539;&#22260;&#21021;&#22987;&#26465;&#20214;&#19979;&#30340;&#25925;&#38556;&#25110;&#23545;&#27979;&#35797;&#31995;&#32479;&#30340;&#23646;&#24615;&#20570;&#20986;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#23558;&#39034;&#24207;&#31995;&#32479;&#30340;&#25925;&#38556;&#36712;&#36857;&#20998;&#24067;&#20272;&#35745;&#35270;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#21033;&#29992;&#31995;&#32479;&#21160;&#24577;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#31034;&#25925;&#38556;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#22312;&#35745;&#31639;&#36712;&#36857;&#26799;&#24230;&#26102;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#12290;&#25105;&#20204;&#22312;&#20498;&#31435;&#25670;&#25511;&#21046;&#31995;&#32479;&#12289;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#22330;&#26223;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#26376;&#29699;&#30528;&#38470;&#22120;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#24320;&#31665;&#21363;&#29992;&#30340;Hamiltonian Monte Carlo&#36827;&#34892;&#37319;&#26679;&#65292;&#21516;&#26102;&#20351;&#29992;&#22810;&#38142;&#20197;&#25429;&#33719;&#22810;&#27169;&#24577;&#65292;&#24182;&#37319;&#29992;&#26799;&#24230;&#24179;&#28369;&#20197;&#23454;&#29616;&#23433;&#20840;&#36712;&#36857;&#12290;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#65292;&#19982;&#40657;&#30418;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the distribution over failures is a key step in validating autonomous systems. Existing approaches focus on finding failures for a small range of initial conditions or make restrictive assumptions about the properties of the system under test. We frame estimating the distribution over failure trajectories for sequential systems as Bayesian inference. Our model-based approach represents the distribution over failure trajectories using rollouts of system dynamics and computes trajectory gradients using automatic differentiation. Our approach is demonstrated in an inverted pendulum control system, an autonomous vehicle driving scenario, and a partially observable lunar lander. Sampling is performed using an off-the-shelf implementation of Hamiltonian Monte Carlo with multiple chains to capture multimodality and gradient smoothing for safe trajectories. In all experiments, we observed improvements in sample efficiency and parameter space coverage compared to black-box baseline a
&lt;/p&gt;</description></item><item><title>Tinto&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#25968;&#23383;&#38706;&#22836;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#21644;&#39564;&#35777;&#22320;&#36136;&#21046;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#30340;3D&#25968;&#25454;&#22914;&#28857;&#20113;&#12290;</title><link>http://arxiv.org/abs/2305.09928</link><description>&lt;p&gt;
Tinto&#65306;&#22320;&#29699;&#31185;&#23398;&#20013;3D&#39640;&#20809;&#35889;&#28857;&#20113;&#20998;&#21106;&#30340;&#22810;&#20256;&#24863;&#22120;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences. (arXiv:2305.09928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09928
&lt;/p&gt;
&lt;p&gt;
Tinto&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#25968;&#23383;&#38706;&#22836;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#21644;&#39564;&#35777;&#22320;&#36136;&#21046;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#30340;3D&#25968;&#25454;&#22914;&#28857;&#20113;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20351;&#29992;&#20943;&#23569;&#20102;&#22320;&#36136;&#22270;&#30340;&#35299;&#37322;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#20174;&#25968;&#23383;&#38706;&#22836;&#27169;&#22411;&#33258;&#21160;&#27966;&#29983;&#22320;&#36136;&#22270;&#26469;&#29702;&#24819;&#22320;&#20943;&#23569;&#20102;&#35299;&#37322;&#32773;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#36136;&#22270;&#35299;&#37322;&#30340;&#20027;&#35266;&#24615;&#20197;&#21450;&#25910;&#38598;&#23450;&#37327;&#39564;&#35777;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#23545;&#36825;&#20123;&#33258;&#21160;&#21270;&#21046;&#22270;&#26041;&#27861;&#30340;&#20934;&#30830;&#39564;&#35777;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20165;&#38480;&#20110;2D&#22270;&#20687;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#35832;&#22914;&#36229;&#32423;&#20113;&#36825;&#26679;&#30340;3D&#25968;&#23383;&#38706;&#22836;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tinto&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#25968;&#23383;&#38706;&#22836;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#21644;&#39564;&#35777;&#22320;&#36136;&#21046;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#30340;3D&#25968;&#25454;&#22914;&#28857;&#20113;&#12290;Tinto&#21253;&#25324;&#20004;&#20010;&#20114;&#34917;&#30340;&#25968;&#25454;&#38598;&#65306;1&#65289;&#26469;&#33258;Corta Atalaya&#65288;&#35199;&#29677;&#29273;&#65289;&#30340;&#30495;&#23454;&#25968;&#23383;&#38706;&#22836;&#27169;&#22411;&#65292;&#20855;&#26377;&#20809;&#35889;&#23646;&#24615;&#21644;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#21450;2&#65289;&#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#27169;&#25311;&#30495;&#23454;&#25968;&#25454;&#30340;&#21508;&#31181;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of deep learning techniques has reduced interpretation time and, ideally, reduced interpreter bias by automatically deriving geological maps from digital outcrop models. However, accurate validation of these automated mapping approaches is a significant challenge due to the subjective nature of geological mapping and the difficulty in collecting quantitative validation data. Additionally, many state-of-the-art deep learning methods are limited to 2D image data, which is insufficient for 3D digital outcrops, such as hyperclouds. To address these challenges, we present Tinto, a multi-sensor benchmark digital outcrop dataset designed to facilitate the development and validation of deep learning approaches for geological mapping, especially for non-structured 3D data like point clouds. Tinto comprises two complementary sets: 1) a real digital outcrop model from Corta Atalaya (Spain), with spectral attributes and ground-truth data, and 2) a synthetic twin that uses latent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#22240;&#27169;&#31946;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#28436;&#21270;&#20986;&#21487;&#35299;&#37322;&#30340;&#31616;&#32422;&#31574;&#30053;&#65292;&#24182;&#33021;&#26377;&#25928;&#24179;&#34913;&#31574;&#30053;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09922</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#31616;&#32422;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#22522;&#22240;&#27169;&#31946;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Genetic Fuzzy System for Interpretable and Parsimonious Reinforcement Learning Policies. (arXiv:2305.09922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#22240;&#27169;&#31946;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#28436;&#21270;&#20986;&#21487;&#35299;&#37322;&#30340;&#31616;&#32422;&#31574;&#30053;&#65292;&#24182;&#33021;&#26377;&#25928;&#24179;&#34913;&#31574;&#30053;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27491;&#32463;&#21382;&#30528;&#30740;&#31350;&#20852;&#36259;&#30340;&#22797;&#33487;&#65292;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479;&#65288;LCS&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#22810;&#24180;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23494;&#27463;&#26681;&#26041;&#27861;&#24448;&#24448;&#20250;&#28436;&#21270;&#25104;&#22823;&#37327;&#30340;&#35268;&#21017;&#24211;&#65292;&#36825;&#20123;&#35268;&#21017;&#24211;&#38590;&#20197;&#35299;&#37322;&#25110;&#25193;&#23637;&#21040;&#36229;&#20986;&#26631;&#20934;&#36855;&#23467;&#20043;&#22806;&#30340;&#39046;&#22495;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#20857;&#22561;&#22522;&#22240;&#27169;&#31946;&#31995;&#32479;&#65292;&#21517;&#20026;Fuzzy MoCoCo&#65292;&#20854;&#21033;&#29992;&#22810;&#30446;&#26631;&#21644;&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#26426;&#21046;&#65292;&#20026;RL&#29615;&#22659;&#28436;&#21270;&#27169;&#31946;&#35268;&#21017;&#31574;&#30053;&#12290;&#31995;&#32479;&#20013;&#30340;&#22810;&#30446;&#26631;&#19982;&#31574;&#30053;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#26377;&#20851;&#12290;&#36830;&#32493;&#29366;&#24577;&#30340;RL&#29615;&#22659;Mountain Car&#34987;&#29992;&#20316;&#27979;&#35797;&#22522;&#30784;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#25506;&#32034;&#31574;&#30053;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#65292;&#39640;&#24615;&#33021;&#30340;&#31574;&#30053;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20351;&#29992;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is experiencing a resurgence in research interest, where Learning Classifier Systems (LCSs) have been applied for many years. However, traditional Michigan approaches tend to evolve large rule bases that are difficult to interpret or scale to domains beyond standard mazes. A Pittsburgh Genetic Fuzzy System (dubbed Fuzzy MoCoCo) is proposed that utilises both multiobjective and cooperative coevolutionary mechanisms to evolve fuzzy rule-based policies for RL environments. Multiobjectivity in the system is concerned with policy performance vs. complexity. The continuous state RL environment Mountain Car is used as a testing bed for the proposed system. Results show the system is able to effectively explore the trade-off between policy performance and complexity, and learn interpretable, high-performing policies that use as few rules as possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#23398;&#20064;&#24178;&#39044;&#36873;&#39033;&#36873;&#25321;&#31574;&#30053;&#65292;&#32467;&#26524;&#34920;&#26126;&#19978;&#19979;&#25991;&#25512;&#26029;&#35823;&#24046;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#23545;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#19981;&#30830;&#23450;&#24615;&#22686;&#21152;&#26102;&#20174;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#25928;&#26524;&#65292;&#32780;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#29366;&#24577;&#20449;&#24687;&#30340;&#38750;&#20961;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09913</link><description>&lt;p&gt;
&#35780;&#20272;&#19978;&#19979;&#25991;&#25512;&#26029;&#35823;&#24046;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#23545;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;RL&#26041;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions. (arXiv:2305.09913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#23398;&#20064;&#24178;&#39044;&#36873;&#39033;&#36873;&#25321;&#31574;&#30053;&#65292;&#32467;&#26524;&#34920;&#26126;&#19978;&#19979;&#25991;&#25512;&#26029;&#35823;&#24046;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#23545;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#19981;&#30830;&#23450;&#24615;&#22686;&#21152;&#26102;&#20174;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#25928;&#26524;&#65292;&#32780;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#29366;&#24577;&#20449;&#24687;&#30340;&#38750;&#20961;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;(JITAIs)&#26159;&#34892;&#20026;&#31185;&#23398;&#30028;&#24320;&#21457;&#30340;&#19968;&#31867;&#20010;&#24615;&#21270;&#20581;&#24247;&#24178;&#39044;&#12290; JITAIs&#26088;&#22312;&#36890;&#36807;&#20174;&#39044;&#23450;&#20041;&#30340;&#32452;&#20214;&#38598;&#20013;&#36845;&#20195;&#36873;&#25321;&#24178;&#39044;&#36873;&#39033;&#24207;&#21015;&#26469;&#21709;&#24212;&#27599;&#20010;&#20010;&#20307;&#30340;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#27491;&#30830;&#31867;&#22411;&#21644;&#25968;&#37327;&#30340;&#25903;&#25345;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#24178;&#39044;&#36873;&#39033;&#36873;&#25321;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#25512;&#26029;&#35823;&#24046;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#23545;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#19978;&#19979;&#25991;&#19981;&#30830;&#23450;&#24615;&#22686;&#21152;&#26102;&#65292;&#20174;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#25552;&#39640;&#24178;&#39044;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#29366;&#24577;&#20449;&#24687;&#30340;&#38750;&#20961;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Just-in-Time Adaptive Interventions (JITAIs) are a class of personalized health interventions developed within the behavioral science community. JITAIs aim to provide the right type and amount of support by iteratively selecting a sequence of intervention options from a pre-defined set of components in response to each individual's time varying state. In this work, we explore the application of reinforcement learning methods to the problem of learning intervention option selection policies. We study the effect of context inference error and partial observability on the ability to learn effective policies. Our results show that the propagation of uncertainty from context inferences is critical to improving intervention efficacy as context uncertainty increases, while policy gradient algorithms can provide remarkable robustness to partially observed behavioral state information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#24314;&#31435;&#20102;&#27969;&#24335;&#29615;&#22659;&#19979;&#30340;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21462;&#24471;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.09907</link><description>&lt;p&gt;
&#21033;&#29992;&#27969;&#24335;&#20998;&#26512;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#36827;&#34892;&#22686;&#37327;&#24322;&#24120;&#26816;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Incremental Outlier Detection Modelling Using Streaming Analytics in Finance &amp; Health Care. (arXiv:2305.09907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#24314;&#31435;&#20102;&#27969;&#24335;&#29615;&#22659;&#19979;&#30340;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21462;&#24471;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#22312;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#27969;&#29615;&#22659;&#19979;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#22686;&#37327;&#26500;&#24314;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#24212;&#24403;&#20351;&#29992;&#27969;&#24335;&#27169;&#22411;&#26469;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#30340;&#39640;&#24230;&#24517;&#35201;&#24615;&#12290;&#26412;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#21644;&#20998;&#26512;&#36866;&#29992;&#20110;&#29616;&#23454;&#29615;&#22659;&#30340;&#27969;&#24335;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#22914;One class&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;OC-SVM&#65289;&#12289;&#23396;&#31435;&#26862;&#26519;&#33258;&#36866;&#24212;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#65288;IForest ASD&#65289;&#12289;Exact Storm&#12289;&#22522;&#20110;&#35282;&#24230;&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;ABOD&#65289;&#12289;&#23616;&#37096;&#24322;&#24120;&#22240;&#23376;&#65288;LOF&#65289;&#12289;KitNet&#12289;KNN ASD&#26041;&#27861;&#12290;&#24182;&#39564;&#35777;&#20102;&#19978;&#36848;&#26500;&#24314;&#27169;&#22411;&#22312;&#21508;&#31181;&#37329;&#34701;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#20363;&#22914;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#12289;&#27969;&#22833;&#39044;&#27979;&#12289;&#20197;&#22826;&#22346;&#27450;&#35784;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;&#20581;&#24247;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#22914;&#24515;&#33039;&#20013;&#39118;&#39044;&#27979;&#12289;&#31958;&#23615;&#30149;&#39044;&#27979;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we had built the online model which are built incrementally by using online outlier detection algorithms under the streaming environment. We identified that there is highly necessity to have the streaming models to tackle the streaming data. The objective of this project is to study and analyze the importance of streaming models which is applicable in the real-world environment. In this work, we built various Outlier Detection (OD) algorithms viz., One class Support Vector Machine (OC-SVM), Isolation Forest Adaptive Sliding window approach (IForest ASD), Exact Storm, Angle based outlier detection (ABOD), Local outlier factor (LOF), KitNet, KNN ASD methods. The effectiveness and validity of the above-built models on various finance problems such as credit card fraud detection, churn prediction, ethereum fraud prediction. Further, we also analyzed the performance of the models on the health care prediction problems such as heart stroke prediction, diabetes prediction and h
&lt;/p&gt;</description></item><item><title>&#36807;&#25311;&#21512;&#22914;&#20309;&#24433;&#21709;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#22312;&#26799;&#24230;&#20272;&#35745;&#19981;&#30830;&#23450;&#26102;&#30340;&#31283;&#20581;&#24615;&#65292;&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#20010;&#20219;&#24847;&#23485;&#24230;&#30340;&#38544;&#34255;&#23618;&#21644;&#20219;&#24847;&#36755;&#20837;&#36755;&#20986;&#25968;&#37327;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2305.09904</link><description>&lt;p&gt;
&#21333;&#38544;&#23618;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#27969;&#30340;ISS&#23646;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the ISS Property of the Gradient Flow for Single Hidden-Layer Neural Networks with Linear Activations. (arXiv:2305.09904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09904
&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#22914;&#20309;&#24433;&#21709;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#22312;&#26799;&#24230;&#20272;&#35745;&#19981;&#30830;&#23450;&#26102;&#30340;&#31283;&#20581;&#24615;&#65292;&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#20010;&#20219;&#24847;&#23485;&#24230;&#30340;&#38544;&#34255;&#23618;&#21644;&#20219;&#24847;&#36755;&#20837;&#36755;&#20986;&#25968;&#37327;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#27604;&#21021;&#22987;&#22238;&#24402;&#38382;&#39064;&#25152;&#38656;&#21442;&#25968;&#26356;&#22810;&#30340;&#21442;&#25968;&#21487;&#20197;&#23548;&#33268;&#26356;&#20934;&#30830;&#25110;&#26356;&#24555;&#25910;&#25947;&#30340;&#27169;&#22411;-&#19982;&#32463;&#20856;&#32479;&#35745;&#23398;&#30340;&#20449;&#24565;&#30456;&#21453;&#12290;&#36825;&#31181;&#29616;&#35937;&#26377;&#26102;&#34987;&#31216;&#20026;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#65292;&#23427;&#24341;&#21457;&#20102;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#21487;&#33021;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#38382;&#39064;&#23646;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#25311;&#21512;&#23545;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#65292;&#24403;&#26799;&#24230;&#20272;&#35745;&#19981;&#30830;&#23450;&#26102;&#65292;&#20250;&#33258;&#28982;&#20135;&#29983;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#20250;&#30001;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#25110;&#30452;&#25509;&#27979;&#37327;&#30340;&#26799;&#24230;&#20272;&#35745;&#32780;&#20135;&#29983;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#20010;&#20219;&#24847;&#23485;&#24230;&#30340;&#38544;&#34255;&#23618;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#20026;&#19968;&#32500;&#30340;&#24773;&#20917;&#65292;&#23548;&#20986;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in neural networks and machine learning suggests that using many more parameters than strictly required by the initial complexity of a regression problem can result in more accurate or faster-converging models -contrary to classical statistical belief. This phenomenon, sometimes known as ``benign overfitting'', raises questions regarding in what other ways might overparameterization affect the properties of a learning problem. In this work, we investigate the effects of overfitting on the robustness of gradient-descent training when subject to uncertainty on the gradient estimation. This uncertainty arises naturally if the gradient is estimated from noisy data or directly measured. Our object of study is a linear neural network with a single, arbitrarily wide, hidden layer and an arbitrary number of inputs and outputs. In this paper we solve the problem for the case where the input and output of our neural-network are one-dimensional, deriving sufficient conditions fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DP-SGD&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#22122;&#22768;&#24615;SGD&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#38544;&#31169;&#25439;&#22833;&#21576;&#25351;&#25968;&#25910;&#25947;&#65292;&#19981;&#38656;&#35201;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#24179;&#28369;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2305.09903</link><description>&lt;p&gt;
&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#31169;&#25439;&#22833;&#21487;&#33021;&#20250;&#25910;&#25947;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#38750;&#20984;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Privacy Loss of Noisy Stochastic Gradient Descent Might Converge Even for Non-Convex Losses. (arXiv:2305.09903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DP-SGD&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#22122;&#22768;&#24615;SGD&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#38544;&#31169;&#25439;&#22833;&#21576;&#25351;&#25968;&#25910;&#25947;&#65292;&#19981;&#38656;&#35201;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#24179;&#28369;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#31169;&#23494;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20294;&#20256;&#32479;&#30340;&#38544;&#31169;&#20998;&#26512;&#20551;&#23450;&#20869;&#37096;&#29366;&#24577;&#20844;&#24320;&#65292;&#23548;&#33268;&#38543;&#30528;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#38544;&#31169;&#25439;&#22833;&#36793;&#30028;&#20250;&#26080;&#38480;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#20869;&#37096;&#29366;&#24577;&#20445;&#25345;&#38544;&#34255;&#65292;&#21017;&#38544;&#31169;&#25439;&#22833;&#21487;&#33021;&#20445;&#25345;&#26377;&#38480;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;DP-SGD&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#21253;&#25324;&#26799;&#24230;&#21098;&#35009;&#30340;&#22122;&#22768;&#24615;SGD&#21464;&#20307;&#65292;&#20197;&#38480;&#21046;&#20010;&#20307;&#26679;&#26412;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#22312;&#19981;&#38656;&#35201;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#24179;&#28369;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#32463;&#36807;&#25237;&#24433;&#30340;DP-SGD&#30340;&#38544;&#31169;&#25439;&#22833;&#21576;&#25351;&#25968;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Noisy-SGD algorithm is widely used for privately training machine learning models. Traditional privacy analyses of this algorithm assume that the internal state is publicly revealed, resulting in privacy loss bounds that increase indefinitely with the number of iterations. However, recent findings have shown that if the internal state remains hidden, then the privacy loss might remain bounded. Nevertheless, this remarkable result heavily relies on the assumption of (strong) convexity of the loss function. It remains an important open problem to further relax this condition while proving similar convergent upper bounds on the privacy loss. In this work, we address this problem for DP-SGD, a popular variant of Noisy-SGD that incorporates gradient clipping to limit the impact of individual samples on the training process. Our findings demonstrate that the privacy loss of projected DP-SGD converges exponentially fast, without requiring convexity or smoothness assumptions on the loss fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09900</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#26159;&#22522;&#30784;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697; \cite{basu2022equi} &#21644; \cite{kaba2022equivariance} &#20998;&#21035;&#25552;&#20986;&#20102;&#20351;&#29992;&#20174;&#32676;&#21464;&#25442;&#36755;&#20837;&#24471;&#21040;&#30340;&#29305;&#24449;&#30340;&#32676;&#24179;&#22343;&#20540;&#65288;\textit{equitune}&#65289;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20174;&#19981;&#31561;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#21462;&#31561;&#21464;&#36755;&#20986;&#12290;&#34429;&#28982; \cite{kaba2022equivariance} &#21482;&#20851;&#27880;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#33391;&#22909;&#30340;&#24494;&#35843;&#32467;&#26524;&#19979;&#65292;\textit{equitune} &#22312;&#31561;&#21464;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#26576;&#20123;&#36716;&#25442;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#65292;&#32780;&#23545;&#20854;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#30340;$\lambda$-\textit{equitune} &#26041;&#27861;&#12290;&#36825;&#20123;&#26435;&#37325;&#26159;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34917;&#20805;&#26631;&#31614;&#35825;&#23548;&#34917;&#20805;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#20998;&#31867;&#22120;&#19982;&#20256;&#32479;PLL&#20998;&#31867;&#22120;&#24418;&#25104;&#23545;&#25239;&#20851;&#31995;&#65292;&#20197;&#28040;&#38500;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#30340;&#35823;&#25253;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#22270;&#21327;&#21161;&#28040;&#38500;&#27495;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;PLL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09897</link><description>&lt;p&gt;
&#34917;&#20805;&#20998;&#31867;&#22120;&#35825;&#23548;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Complementary Classifier Induced Partial Label Learning. (arXiv:2305.09897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34917;&#20805;&#26631;&#31614;&#35825;&#23548;&#34917;&#20805;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#20998;&#31867;&#22120;&#19982;&#20256;&#32479;PLL&#20998;&#31867;&#22120;&#24418;&#25104;&#23545;&#25239;&#20851;&#31995;&#65292;&#20197;&#28040;&#38500;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#30340;&#35823;&#25253;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#22270;&#21327;&#21161;&#28040;&#38500;&#27495;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;PLL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#20013;&#65292;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#37117;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614;&#30456;&#20851;&#32852;&#65292;&#20854;&#20013;&#20165;&#26377;&#19968;&#20010;&#26159;&#26377;&#25928;&#30340;&#12290;PLL&#30340;&#26680;&#24515;&#26159;&#28040;&#38500;&#20505;&#36873;&#26631;&#31614;&#20013;&#30340;&#27495;&#20041;&#65292;&#20197;&#33719;&#24471;&#30495;&#23454;&#30340;&#26631;&#31614;&#12290;&#22312;&#28040;&#38500;&#27495;&#20041;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#30740;&#31350;&#38750;&#20505;&#36873;&#26631;&#31614;&#38598;&#65288;&#20063;&#31216;&#20026;&#34917;&#20805;&#26631;&#31614;&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#20934;&#30830;&#22320;&#25351;&#31034;&#20102;&#19968;&#20010;&#19981;&#23646;&#20110;&#26679;&#26412;&#30340;&#26631;&#31614;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#20505;&#36873;&#26631;&#31614;&#26469;&#35825;&#23548;&#19968;&#20010;&#34917;&#20805;&#20998;&#31867;&#22120;&#65292;&#23427;&#33258;&#28982;&#22320;&#24418;&#25104;&#23545;&#20256;&#32479;PLL&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#20851;&#31995;&#65292;&#20197;&#28040;&#38500;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#30340;&#35823;&#25253;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20551;&#35774;&#29305;&#24449;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#20849;&#20139;&#30001;&#21160;&#24577;&#22270;&#25429;&#33719;&#30340;&#30456;&#21516;&#23616;&#37096;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21327;&#21161;&#28040;&#38500;&#27495;&#20041;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;4&#20010;&#25511;&#21046;UCI&#25968;&#25454;&#38598;&#21644;1&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;PLL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In partial label learning (PLL), each training sample is associated with a set of candidate labels, among which only one is valid. The core of PLL is to disambiguate the candidate labels to get the ground-truth one. In disambiguation, the existing works usually do not fully investigate the effectiveness of the non-candidate label set (a.k.a. complementary labels), which accurately indicates a set of labels that do not belong to a sample. In this paper, we use the non-candidate labels to induce a complementary classifier, which naturally forms an adversarial relationship against the traditional PLL classifier, to eliminate the false-positive labels in the candidate label set. Besides, we assume the feature space and the label space share the same local topological structure captured by a dynamic graph, and use it to assist disambiguation. Extensive experimental results validate the superiority of the proposed approach against state-of-the-art PLL methods on 4 controlled UCI data sets an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35757;&#32451;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#33258;&#30456;&#20284;&#24615;&#27880;&#24847;&#21147;&#65288;SS-Attention&#65289;&#65292;&#21487;&#20197;&#25429;&#25417;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#24182;&#35299;&#20915;&#24615;&#33021;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.09890</link><description>&lt;p&gt;
SS-BSN:&#27880;&#24847;&#21147;&#30450;&#28857;&#32593;&#32476;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#21435;&#22122;&#19982;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
SS-BSN: Attentive Blind-Spot Network for Self-Supervised Denoising with Nonlocal Self-Similarity. (arXiv:2305.09890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35757;&#32451;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#33258;&#30456;&#20284;&#24615;&#27880;&#24847;&#21147;&#65288;SS-Attention&#65289;&#65292;&#21487;&#20197;&#25429;&#25417;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#24182;&#35299;&#20915;&#24615;&#33021;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#20851;&#20110;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#22122;&#22768;-&#24178;&#20928;&#22270;&#20687;&#23545;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#33719;&#24471;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21482;&#20351;&#29992;&#26377;&#22122;&#22768;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#24182;&#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#20256;&#32479;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#24615;&#33021;&#19978;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30456;&#20284;&#24615;&#27880;&#24847;&#21147;&#65288;SS-Attention&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#24182;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20197;&#20687;&#32032;&#20026;&#22522;&#30784;&#35774;&#35745;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#32780;&#32463;&#20856;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21017;&#26080;&#27861;&#23454;&#29616;&#65292;&#22240;&#20026;&#20854;&#38543;&#30528;&#31354;&#38388;&#20998;&#36776;&#29575;&#32780;&#21576;&#20108;&#27425;&#36882;&#22686;&#30340;&#22797;&#26434;&#24230;&#20250;&#23548;&#33268;&#23454;&#29616;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, numerous studies have been conducted on supervised learning-based image denoising methods. However, these methods rely on large-scale noisy-clean image pairs, which are difficult to obtain in practice. Denoising methods with self-supervised training that can be trained with only noisy images have been proposed to address the limitation. These methods are based on the convolutional neural network (CNN) and have shown promising performance. However, CNN-based methods do not consider using nonlocal self-similarities essential in the traditional method, which can cause performance limitations. This paper presents self-similarity attention (SS-Attention), a novel self-attention module that can capture nonlocal self-similarities to solve the problem. We focus on designing a lightweight self-attention module in a pixel-wise manner, which is nearly impossible to implement using the classic self-attention module due to the quadratically increasing complexity with spatial resolution. F
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#20998;&#24067;&#24335; GNN &#35757;&#32451;&#26694;&#26550;&#65292;&#21482;&#36827;&#34892;&#23450;&#26399;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#20998;&#21106;&#26469;&#38544;&#24335;&#20132;&#25442;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09887</link><description>&lt;p&gt;
&#31616;&#21270;&#22823;&#35268;&#27169;&#22270;&#19978;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65306;&#38543;&#26426;&#20998;&#21106;&#25913;&#21892;&#27169;&#22411;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation. (arXiv:2305.09887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#20998;&#24067;&#24335; GNN &#35757;&#32451;&#26694;&#26550;&#65292;&#21482;&#36827;&#34892;&#23450;&#26399;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#20998;&#21106;&#26469;&#38544;&#24335;&#20132;&#25442;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335; GNN &#30340;&#35757;&#32451;&#21487;&#20197;&#20351;&#25105;&#20204;&#23398;&#20064;&#36229;&#20986;&#21333;&#20010;&#26426;&#22120;&#23384;&#20648;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#22823;&#35268;&#27169;&#22270;&#65288;&#22914;&#31038;&#20132;&#21644;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#65289;&#12290;&#20026;&#20102;&#36798;&#21040;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20998;&#24067;&#24335;&#26694;&#26550;&#19987;&#27880;&#20110;&#36890;&#36807;&#23454;&#20363;&#38388;&#36890;&#20449;&#25110;&#23450;&#26399;&#22238;&#36864;&#21040;&#38598;&#20013;&#24335;&#35757;&#32451;&#26469;&#26368;&#22823;&#38480;&#24230;&#22320;&#24674;&#22797;&#36328;&#23454;&#20363;&#33410;&#28857;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#38656;&#35201;&#39069;&#22806;&#30340;&#24320;&#38144;&#24182;&#38480;&#21046;&#20102;&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#20998;&#24067;&#24335; GNN &#35757;&#32451;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#20197;&#19978;&#26114;&#36149;&#30340;&#25805;&#20316;&#65292;&#20855;&#26377;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#32452;&#35013;&#29420;&#31435;&#30340;&#35757;&#32451;&#22120;&#65292;&#27599;&#20010;&#35757;&#32451;&#22120;&#24322;&#27493;&#22320;&#22312;&#35757;&#32451;&#22270;&#30340;&#26412;&#22320;&#37096;&#20998;&#19978;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#65292;&#65288;2&#65289;&#21482;&#36827;&#34892;&#23450;&#26399;&#30340;&#65288;&#22522;&#20110;&#26102;&#38388;&#30340;&#65289;&#27169;&#22411;&#32858;&#21512;&#65292;&#20197;&#21516;&#27493;&#21508;&#20010;&#26412;&#22320;&#27169;&#22411;&#12290;&#32463;&#36807;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#38543;&#26426;&#22270;&#20998;&#21106;&#20351;&#24471;&#29420;&#31435;&#30340;&#35757;&#32451;&#22120;&#33021;&#22815;&#38544;&#24335;&#22320;&#20132;&#25442;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#36890;&#20449;&#65292;&#22240;&#27492;&#23450;&#26399;&#32858;&#21512;&#23601;&#36275;&#20197;&#25910;&#25947;&#21040;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#12289;&#39640;&#25928;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181; GNN &#27169;&#22411;&#65288;&#20363;&#22914; GCN&#12289;GAT &#31561;&#65289;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#29992;&#20110;&#23454;&#38469;&#30340;&#20998;&#24067;&#24335; GNN &#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed training of GNNs enables learning on massive graphs (e.g., social and e-commerce networks) that exceed the storage and computational capacity of a single machine. To reach performance comparable to centralized training, distributed frameworks focus on maximally recovering cross-instance node dependencies with either communication across instances or periodic fallback to centralized training, which create overhead and limit the framework scalability. In this work, we present a simplified framework for distributed GNN training that does not rely on the aforementioned costly operations, and has improved scalability, convergence speed and performance over the state-of-the-art approaches. Specifically, our framework (1) assembles independent trainers, each of which asynchronously learns a local model on locally-available parts of the training graph, and (2) only conducts periodic (time-based) model aggregation to synchronize the local models. Backed by our theoretical analysis, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SELO&#30340;&#38142;&#36335;&#31526;&#21495;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#23376;&#22270;&#32534;&#30721;&#26041;&#27861;&#23398;&#20064;&#26377;&#21521;&#32593;&#32476;&#20013;&#30340;&#36793;&#23884;&#20837;&#12290;&#36890;&#36807;&#24341;&#20837;&#26377;&#31526;&#21495;&#23376;&#22270;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#20248;&#21270;&#26041;&#27861;&#23558;&#27599;&#20010;&#23376;&#22270;&#23884;&#20837;&#21040;&#20284;&#28982;&#30697;&#38453;&#20013;&#32780;&#38750;&#37051;&#25509;&#30697;&#38453;&#20013;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09869</link><description>&lt;p&gt;
&#22522;&#20110;&#32447;&#24615;&#20248;&#21270;&#30340;&#26377;&#31526;&#21495;&#23376;&#22270;&#32534;&#30721;&#26041;&#27861;&#29992;&#20110;&#38142;&#36335;&#31526;&#21495;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Signed Subgraph Encoding Approach via Linear Optimization for Link Sign Prediction. (arXiv:2305.09869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SELO&#30340;&#38142;&#36335;&#31526;&#21495;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#23376;&#22270;&#32534;&#30721;&#26041;&#27861;&#23398;&#20064;&#26377;&#21521;&#32593;&#32476;&#20013;&#30340;&#36793;&#23884;&#20837;&#12290;&#36890;&#36807;&#24341;&#20837;&#26377;&#31526;&#21495;&#23376;&#22270;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#20248;&#21270;&#26041;&#27861;&#23558;&#27599;&#20010;&#23376;&#22270;&#23884;&#20837;&#21040;&#20284;&#28982;&#30697;&#38453;&#20013;&#32780;&#38750;&#37051;&#25509;&#30697;&#38453;&#20013;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26377;&#38480;&#30340;&#26377;&#31526;&#21495;&#25968;&#25454;&#20013;&#26377;&#25928;&#22320;&#25512;&#26029;&#38142;&#36335;&#30340;&#31526;&#21495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SELO&#30340;&#38142;&#36335;&#31526;&#21495;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#23376;&#22270;&#32534;&#30721;&#26041;&#27861;&#26469;&#23398;&#20064;&#26377;&#21521;&#32593;&#32476;&#20013;&#30340;&#36793;&#23884;&#20837;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#26041;&#27861;&#24341;&#20837;&#20102;&#26377;&#31526;&#21495;&#23376;&#22270;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#23376;&#22270;&#23884;&#20837;&#21040;&#20284;&#28982;&#30697;&#38453;&#20013;&#32780;&#19981;&#26159;&#37051;&#25509;&#30697;&#38453;&#20013;&#12290;&#22312;&#20845;&#20010;&#30495;&#23454;&#30340;&#26377;&#31526;&#21495;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;AUC&#12289;F1&#12289;micro-F1&#21644;Macro-F1&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SELO&#27169;&#22411;&#22312;&#25152;&#26377;&#22235;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of inferring the sign of a link based on limited sign data in signed networks. Regarding this link sign prediction problem, SDGNN (Signed Directed Graph Neural Networks) provides the best prediction performance currently to the best of our knowledge. In this paper, we propose a different link sign prediction architecture call SELO (Subgraph Encoding via Linear Optimization), which obtains overall leading prediction performances compared the state-of-the-art algorithm SDGNN. The proposed model utilizes a subgraph encoding approach to learn edge embeddings for signed directed networks. In particular, a signed subgraph encoding approach is introduced to embed each subgraph into a likelihood matrix instead of the adjacency matrix through a linear optimization method. Comprehensive experiments are conducted on six real-world signed networks with AUC, F1, micro-F1, and Macro-F1 as the evaluation metrics. The experiment results show that the proposed SEL
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.09868</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Principle of Uncertain Maximum Entropy. (arXiv:2305.09868v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09868
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#21407;&#29702;&#22312;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#24341;&#20837;&#65292;&#20026;&#32479;&#35745;&#21147;&#23398;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#24577;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#20854;&#24471;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20652;&#21270;&#21058;&#65292;&#20419;&#36827;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#26144;&#23556;&#21040;&#33719;&#21462;&#26080;&#20559;&#27169;&#22411;&#65292;&#21516;&#26102;&#21152;&#28145;&#20102;&#23545;&#22797;&#26434;&#31995;&#32479;&#21644;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#20803;&#32032;&#19981;&#30452;&#25509;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23384;&#22312;&#22122;&#22768;&#25110;&#30524;&#37096;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#26368;&#22823;&#29109;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21305;&#37197;&#29305;&#24449;&#32422;&#26463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#20219;&#24847;&#22122;&#22768;&#35266;&#23519;&#65292;&#23427;&#21516;&#26102;&#23558;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#65292;&#32780;&#19988;&#20248;&#20110;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#30340;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#22312;&#19982;&#26368;&#22823;&#20284;&#28982;&#31639;&#27861;&#30456;&#27604;&#26102;&#24314;&#31435;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of maximum entropy, as introduced by Jaynes in information theory, has contributed to advancements in various domains such as Statistical Mechanics, Machine Learning, and Ecology. Its resultant solutions have served as a catalyst, facilitating researchers in mapping their empirical observations to the acquisition of unbiased models, whilst deepening the understanding of complex systems and phenomena. However, when we consider situations in which the model elements are not directly observable, such as when noise or ocular occlusion is present, possibilities arise for which standard maximum entropy approaches may fail, as they are unable to match feature constraints. Here we show the Principle of Uncertain Maximum Entropy as a method that both encodes all available information in spite of arbitrarily noisy observations while surpassing the accuracy of some ad-hoc methods. Additionally, we utilize the output of a black-box machine learning model as input into an uncertain ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.09863</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#19981;&#36879;&#26126;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#19968;&#20010;&#8220;&#25991;&#26412;&#27169;&#22359;&#8221;&#26159;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#26631;&#37327;&#36830;&#32493;&#20540;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#20363;&#22914;LLM&#20869;&#30340;&#23376;&#27169;&#22359;&#25110;&#22823;&#33041;&#21306;&#22495;&#30340;&#25311;&#21512;&#27169;&#22411;&#12290;&#8220;&#40657;&#30418;&#8221;&#34920;&#31034;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#27169;&#22359;&#30340;&#36755;&#20837;/&#36755;&#20986;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Summarize and Score&#65288;SASC&#65289;&#26041;&#27861;&#65292;&#23427;&#25509;&#21463;&#25991;&#26412;&#27169;&#22359;&#24182;&#36820;&#22238;&#27169;&#22359;&#36873;&#25321;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;SASC&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#27169;&#22359;&#19978;&#35780;&#20272;SASC&#65292;&#24182;&#21457;&#29616;&#23427;&#32463;&#24120;&#24674;&#22797;&#22522;&#26412;&#30495;&#30456;&#35828;&#26126;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;SASC&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#65292;&#20351;&#24471;&#26816;&#26597;BERT&#30340;&#27169;&#22359;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2305.09860</link><description>&lt;p&gt;
Epsilon Sampling Rocks: &#30740;&#31350;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#37319;&#26679;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#24050;&#32463;&#26174;&#31034;&#20986;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29992;&#20989;&#25968;&#30456;&#32467;&#21512;&#26102;&#12290;&#28982;&#32780;&#65292;MBR&#35299;&#30721;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#21644;&#25968;&#37327;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;MBR&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20363;&#22914;&#31062;&#20808;&#37319;&#26679;&#65292;&#26680;&#37319;&#26679;&#21644;top-k&#37319;&#26679;&#12290;&#22522;&#20110;&#25105;&#20204;&#23545;&#23427;&#20204;&#23616;&#38480;&#24615;&#30340;&#35748;&#35782;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;epsilon&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25152;&#26377;&#23567;&#20110;epsilon&#30340;&#26631;&#35760;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#33719;&#24471;&#20844;&#24179;&#30340;&#27010;&#29575;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;epsilon&#37319;&#26679;&#30340;MBR&#35299;&#30721;&#26174;&#33879;&#20248;&#20110;&#19981;&#20165;&#26159;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#32780;&#19988;&#36824;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#27861;&#30340;MBR&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09859</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#30021;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#38750;&#24120;&#30456;&#20284;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35805;&#35821;&#65292;&#22240;&#27492;&#21306;&#20998;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#30340;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#35780;&#35770;&#24182;&#27169;&#20223;&#26576;&#20123;&#20316;&#32773;&#21644;&#20154;&#29289;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340; logits&#65292;&#25110;&#38656;&#35201;&#21487;&#20197;&#20174;&#30446;&#26631;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#31181;&#40657;&#21283;&#23376;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#29983;&#25104;&#25991;&#26412;&#22312;&#29983;&#25104;&#22120;&#30340;&#20284;&#28982;&#20989;&#25968;&#19979;&#26159;&#23616;&#37096;&#26368;&#20248;&#30340;&#65292;&#32780;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36739;&#23567;&#19988;&#37096;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65306;&#23427;&#20204;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#26816;&#27979;&#26469;&#33258;&#23567;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#30456;&#21516;&#30340;&#35821;&#26009;&#24211;&#23545;&#26816;&#27979;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09858</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#65306;&#20197; LLMS &#22312;&#30005;&#21830;&#20013;&#30340;&#20851;&#31995;&#26631;&#27880;&#20026;&#20363;&#30340;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs. (arXiv:2305.09858v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#22686;&#24378;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20363;&#22914;&#20135;&#21697;&#25110;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#30340;&#20114;&#34917;&#25110;&#26367;&#20195;&#20851;&#31995;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#21160;&#24577;&#24615;&#21644;&#20154;&#21147;&#25104;&#26412;&#30456;&#20851;&#30340;&#21407;&#22240;&#65292;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20851;&#31995;&#26631;&#27880;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#20046;&#24847;&#26009;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20851;&#20110; LLM &#22312;&#30005;&#23376;&#21830;&#21153;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181; LLM&#65292;&#21253;&#25324; PaLM &#21644; GPT-3.5&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#20851;&#31995;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20855;&#26377;&#19981;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23481;&#38169;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#30456;&#23545;&#36739;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#22312;&#27492;&#24773;&#22659;&#19979;&#20063;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.09856</link><description>&lt;p&gt;
&#31616;&#21333;&#26131;&#29992;&#65306;&#20855;&#26377;&#19981;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#23481;&#38169;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients. (arXiv:2305.09856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20855;&#26377;&#19981;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23481;&#38169;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#30456;&#23545;&#36739;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#22312;&#27492;&#24773;&#22659;&#19979;&#20063;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#36827;&#34892;&#20998;&#25955;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#19981;&#27844;&#38706;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#25552;&#39640;FL&#23481;&#38169;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#21487;&#38752;&#35774;&#22791;&#65288;&#20363;&#22914;&#25481;&#32447;&#12289;&#38169;&#35823;&#37197;&#32622;&#12289;&#24046;&#25968;&#25454;&#36136;&#37327;&#65289;&#30340;&#30495;&#23454;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#20004;&#20010;&#20855;&#26377;&#26377;&#38480;&#23458;&#25143;&#31471;&#30340;&#20195;&#34920;&#24615;&#23454;&#38469;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#26356;&#22909;&#22320;&#20998;&#26512;FL&#23481;&#38169;&#24615;&#12290;&#19982;&#30452;&#35273;&#30456;&#21453;&#65292;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#22312;&#23384;&#22312;&#19981;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#20986;&#22855;&#22320;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), as an emerging artificial intelligence (AI) approach, enables decentralized model training across multiple devices without exposing their local training data. FL has been increasingly gaining popularity in both academia and industry. While research works have been proposed to improve the fault tolerance of FL, the real impact of unreliable devices (e.g., dropping out, misconfiguration, poor data quality) in real-world applications is not fully investigated. We carefully chose two representative, real-world classification problems with a limited numbers of clients to better analyze FL fault tolerance. Contrary to the intuition, simple FL algorithms can perform surprisingly well in the presence of unreliable clients.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#36890;&#36807;&#20248;&#21270;&#21435;&#22122;&#27493;&#39588;&#26469;&#20943;&#23569;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#24341;&#23548;&#24335;&#25512;&#29702;&#27969;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#21518;&#32493;&#36845;&#20195;&#23545;&#20248;&#21270;&#30340;&#25935;&#24863;&#24230;&#36739;&#20302;&#12290;&#20248;&#21270;&#21435;&#22122;&#22788;&#29702;&#24490;&#29615;&#30340;&#26368;&#21518;20&#65285;&#25110;&#26368;&#21518;50&#65285;&#21487;&#20197;&#20998;&#21035;&#23558;&#25512;&#29702;&#26102;&#38388;&#20943;&#23569;8.2&#65285;&#25110;20.3&#65285;&#65292;&#21516;&#26102;&#23545;&#20154;&#30524;&#20960;&#20046;&#27809;&#26377;&#21487;&#23519;&#35273;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.09847</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#25351;&#23548;&#65306;&#24341;&#23548;&#25193;&#25955;&#30340;&#25152;&#26377;&#21435;&#22122;&#27493;&#39588;&#37117;&#37325;&#35201;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?. (arXiv:2305.09847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09847
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#36890;&#36807;&#20248;&#21270;&#21435;&#22122;&#27493;&#39588;&#26469;&#20943;&#23569;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#24341;&#23548;&#24335;&#25512;&#29702;&#27969;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#21518;&#32493;&#36845;&#20195;&#23545;&#20248;&#21270;&#30340;&#25935;&#24863;&#24230;&#36739;&#20302;&#12290;&#20248;&#21270;&#21435;&#22122;&#22788;&#29702;&#24490;&#29615;&#30340;&#26368;&#21518;20&#65285;&#25110;&#26368;&#21518;50&#65285;&#21487;&#20197;&#20998;&#21035;&#23558;&#25512;&#29702;&#26102;&#38388;&#20943;&#23569;8.2&#65285;&#25110;20.3&#65285;&#65292;&#21516;&#26102;&#23545;&#20154;&#30524;&#20960;&#20046;&#27809;&#26377;&#21487;&#23519;&#35273;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#20248;&#21270;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#24341;&#23548;&#24335;&#25512;&#29702;&#27969;&#31243;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23558;&#22122;&#22768;&#35745;&#31639;&#38480;&#21046;&#20026;&#26465;&#20214;&#22122;&#22768;&#24182;&#28040;&#38500;&#26080;&#26465;&#20214;&#22122;&#22768;&#35745;&#31639;&#26469;&#20248;&#21270;&#26576;&#20123;&#21435;&#22122;&#27493;&#39588;&#65292;&#20174;&#32780;&#23558;&#30446;&#26631;&#36845;&#20195;&#30340;&#22797;&#26434;&#24615;&#38477;&#20302;50%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;SD&#30340;&#21518;&#32493;&#36845;&#20195;&#23545;&#20248;&#21270;&#30340;&#25935;&#24863;&#24230;&#36739;&#20302;&#65292;&#22240;&#27492;&#23427;&#20204;&#26159;&#24212;&#29992;&#24314;&#35758;&#20248;&#21270;&#30340;&#29702;&#24819;&#20505;&#36873;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20248;&#21270;&#21435;&#22122;&#22788;&#29702;&#24490;&#29615;&#30340;&#26368;&#21518;20&#65285;&#20250;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#20943;&#23569;8.2&#65285;&#65292;&#21516;&#26102;&#23545;&#20154;&#30524;&#20960;&#20046;&#27809;&#26377;&#21487;&#23519;&#35273;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#20248;&#21270;&#25193;&#23637;&#21040;&#26368;&#21518;50&#65285;&#30340;&#36845;&#20195;&#21487;&#20197;&#23558;&#25512;&#29702;&#26102;&#38388;&#20943;&#23569;&#32422;20.3&#65285;&#65292;&#21516;&#26102;&#29983;&#25104;&#35270;&#35273;&#19978;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the impact of optimizing the Stable Diffusion (SD) guided inference pipeline. We propose optimizing certain denoising steps by limiting the noise computation to conditional noise and eliminating unconditional noise computation, thereby reducing the complexity of the target iterations by 50%. Additionally, we demonstrate that later iterations of the SD are less sensitive to optimization, making them ideal candidates for applying the suggested optimization. Our experiments show that optimizing the last 20% of the denoising loop iterations results in an 8.2% reduction in inference time with almost no perceivable changes to the human eye. Furthermore, we found that by extending the optimization to 50% of the last iterations, we can reduce inference time by approximately 20.3%, while still generating visually pleasing images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DNN-EIM&#30340;&#31639;&#27861;&#26469;&#22312;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;EIM&#31639;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#32500;&#25968;&#12290;&#21516;&#26102;&#32771;&#34385;&#20102;&#22312;&#20998;&#31867;&#21644;PDEs&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#31867;&#21035;&#25110;EIM&#28857;&#35774;&#35745;&#24182;&#34892;&#30340;DNN&#65292;&#25152;&#38656;&#26435;&#37325;&#27604;&#20256;&#32479;&#26041;&#27861;&#23569;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.09842</link><description>&lt;p&gt;
&#20351;&#29992;&#32463;&#39564;&#25554;&#20540;&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#38477;&#32500;&#30340;&#27880;&#35760;
&lt;/p&gt;
&lt;p&gt;
A Note on Dimensionality Reduction in Deep Neural Networks using Empirical Interpolation Method. (arXiv:2305.09842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DNN-EIM&#30340;&#31639;&#27861;&#26469;&#22312;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;EIM&#31639;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#32500;&#25968;&#12290;&#21516;&#26102;&#32771;&#34385;&#20102;&#22312;&#20998;&#31867;&#21644;PDEs&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#31867;&#21035;&#25110;EIM&#28857;&#35774;&#35745;&#24182;&#34892;&#30340;DNN&#65292;&#25152;&#38656;&#26435;&#37325;&#27604;&#20256;&#32479;&#26041;&#27861;&#23569;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#25554;&#20540;&#26041;&#27861;&#65288;EIM&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#20272;&#35745;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DNN-EIM&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;EIM&#31639;&#27861;&#22312;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#32500;&#25968;&#12290;&#32771;&#34385;&#20102;&#22312;&#25968;&#25454;&#31185;&#23398;&#65288;&#20363;&#22914;MNIST&#65289;&#21644;&#21442;&#25968;&#65288;&#20197;&#21450;&#26102;&#21464;&#65289;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23545;&#20110;&#20998;&#31867;&#65292;&#25152;&#25552;&#20986;&#30340;DNN&#26159;&#20026;&#27599;&#20010;&#31867;&#21035;&#24182;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#39034;&#24207;&#30340;&#65292;&#21363;&#21487;&#20197;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#65292;&#32780;&#19981;&#24517;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#12290;&#22312;PDE&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#27599;&#20010;EIM&#28857;&#35774;&#35745;&#20102;&#19968;&#20010;DNN&#12290;&#21516;&#26679;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;EIM&#28857;&#24182;&#34892;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#19982;&#35757;&#32451;&#26435;&#37325;&#30456;&#27604;&#65292;&#24182;&#34892;&#32593;&#32476;&#25152;&#38656;&#30340;&#26435;&#37325;&#23569;&#20110;10&#20493;&#12290;&#36890;&#36807;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical interpolation method (EIM) is a well-known technique to efficiently approximate parameterized functions. This paper proposes to use EIM algorithm to efficiently reduce the dimension of the training data within supervised machine learning. This is termed as DNN-EIM. Applications in data science (e.g., MNIST) and parameterized (and time-dependent) partial differential equations (PDEs) are considered. The proposed DNNs in case of classification are trained in parallel for each class. This approach is sequential, i.e., new classes can be added without having to retrain the network. In case of PDEs, a DNN is designed corresponding to each EIM point. Again, these networks can be trained in parallel, for each EIM point. In all cases, the parallel networks require fewer than ten times the number of training weights. Significant gains are observed in terms of training times, without sacrificing accuracy.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#21512;&#20316;&#26234;&#33021;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#24322;&#27493;&#35745;&#31639;&#32593;&#32476;&#19981;&#21516;&#37096;&#20998;&#12289;&#21560;&#25910;&#21453;&#21521;&#20256;&#25773;&#19981;&#33021;&#20351;&#29992;&#30340;&#19981;&#21487;&#24494;&#32452;&#20214;&#12289;&#25506;&#32034;&#21644;/&#25110;&#26102;&#24577;&#25277;&#35937;&#30340;&#20998;&#23618;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20998;&#24067;&#24335;&#21644;&#24182;&#34892;&#23398;&#20064;&#12290;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.09838</link><description>&lt;p&gt;
&#21512;&#20316;&#26234;&#33021;&#32593;&#32476;&#65306;&#27867;&#21270;&#21644;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Coagent Networks: Generalized and Scaled. (arXiv:2305.09838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09838
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#21512;&#20316;&#26234;&#33021;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#24322;&#27493;&#35745;&#31639;&#32593;&#32476;&#19981;&#21516;&#37096;&#20998;&#12289;&#21560;&#25910;&#21453;&#21521;&#20256;&#25773;&#19981;&#33021;&#20351;&#29992;&#30340;&#19981;&#21487;&#24494;&#32452;&#20214;&#12289;&#25506;&#32034;&#21644;/&#25110;&#26102;&#24577;&#25277;&#35937;&#30340;&#20998;&#23618;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20998;&#24067;&#24335;&#21644;&#24182;&#34892;&#23398;&#20064;&#12290;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21512;&#20316;&#26234;&#33021;&#32593;&#32476;&#20026;&#20219;&#24847;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#21017;&#24615;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#23427;&#19981;&#20165;&#33021;&#22815;&#24322;&#27493;&#35745;&#31639;&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#65292;&#36824;&#33021;&#22815;&#21560;&#25910;&#19968;&#20123;&#21453;&#21521;&#20256;&#25773;&#19981;&#33021;&#20351;&#29992;&#30340;&#19981;&#21487;&#24494;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#22312;&#21160;&#20316;&#31354;&#38388;&#32423;&#21035;&#20197;&#19978;&#36827;&#34892;&#25506;&#32034;&#65292;&#21363;&#21487;&#20197;&#35774;&#35745;&#20026;&#25506;&#32034;&#21644;/&#25110;&#26102;&#24577;&#25277;&#35937;&#30340;&#20998;&#23618;&#32593;&#32476;&#12290;&#26412;&#25991;&#23558;&#21327;&#20316;&#29702;&#35770;&#21644;&#23398;&#20064;&#35268;&#21017;&#25512;&#24191;&#21040;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#21644;&#24182;&#34892;&#23398;&#20064;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#25193;&#23637;&#23427;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coagent networks for reinforcement learning (RL) [Thomas and Barto, 2011] provide a powerful and flexible framework for deriving principled learning rules for arbitrary stochastic neural networks. The coagent framework offers an alternative to backpropagation-based deep learning (BDL) that overcomes some of backpropagation's main limitations. For example, coagent networks can compute different parts of the network \emph{asynchronously} (at different rates or at different times), can incorporate non-differentiable components that cannot be used with backpropagation, and can explore at levels higher than their action spaces (that is, they can be designed as hierarchical networks for exploration and/or temporal abstraction). However, the coagent framework is not just an alternative to BDL; the two approaches can be blended: BDL can be combined with coagent learning rules to create architectures with the advantages of both approaches. This work generalizes the coagent theory and learning r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#23427;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#35774;&#35745;&#20803;&#32032;&#65292;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19978;&#30340;&#39046;&#20808;&#22320;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.09836</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#31616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Minimalist Approach to Offline Reinforcement Learning. (arXiv:2305.09836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#23427;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#35774;&#35745;&#20803;&#32032;&#65292;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19978;&#30340;&#39046;&#20808;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#34429;&#28982;&#36825;&#20123;&#31639;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#24456;&#22810;&#31639;&#27861;&#21253;&#21547;&#20102;&#30475;&#20284;&#24494;&#19981;&#36275;&#36947;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36825;&#20123;&#36873;&#25321;&#23545;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#36229;&#20986;&#20102;&#26680;&#24515;&#31639;&#27861;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#23545;&#20110;&#24050;&#26377;&#22522;&#32447;&#31639;&#27861;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#36825;&#20123;&#35774;&#35745;&#20803;&#32032;&#12290;&#25105;&#20204;&#20351;&#29992;D4RL&#21644;V-D4RL&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#20102;ReBRAC&#22312;51&#20010;&#20855;&#26377;&#33258;&#25105;&#24863;&#30693;&#21644;&#35270;&#35273;&#29366;&#24577;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#26041;&#27861;&#20013;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#28040;&#34701;&#30740;&#31350;&#21644;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;ReBRAC&#30340;&#25104;&#21151;&#28304;&#20110;&#20854;&#22522;&#20110;&#31574;&#30053;&#25913;&#36827;&#21644;&#35780;&#35770;&#23478;&#27491;&#21017;&#21270;&#30340;&#21407;&#21017;&#24615;&#35774;&#35745;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant advancements in offline reinforcement learning (RL), resulting in the development of numerous algorithms with varying degrees of complexity. While these algorithms have led to noteworthy improvements, many incorporate seemingly minor design choices that impact their effectiveness beyond core algorithmic advances. However, the effect of these design choices on established baselines remains understudied. In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method. We evaluate ReBRAC on 51 datasets with both proprioceptive and visual state spaces using D4RL and V-D4RL benchmarks, demonstrating its state-of-the-art performance among ensemble-free methods. To further illustrate the efficacy of these design choices, we perform a large-scale ablation study and hyperparameter sensitivity anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;&#25311;&#24577;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20165;&#35843;&#25972;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#21363;&#21487;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#22823;&#22823;&#25552;&#39640;Transformer&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.09828</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#25311;&#24577;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Mimetic Initialization of Self-Attention Layers. (arXiv:2305.09828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;&#25311;&#24577;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20165;&#35843;&#25972;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#21363;&#21487;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#22823;&#22823;&#25552;&#39640;Transformer&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Transformer&#21313;&#20998;&#22256;&#38590;&#12290;&#36890;&#24120;&#38656;&#35201;&#20197;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#36215;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#39044;&#35757;&#32451;Transformer&#30340;&#26435;&#37325;&#65288;&#23588;&#20854;&#26159;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#65289;&#65292;&#35797;&#22270;&#25214;&#21040;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#21021;&#22987;&#21270;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#26435;&#37325;&#65292;&#20351;&#20854;&#8220;&#30475;&#36215;&#26469;&#8221;&#26356;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23601;&#33021;&#22815;&#26356;&#24555;&#19988;&#26356;&#39640;&#31934;&#24230;&#22320;&#35757;&#32451;&#26222;&#36890;Transformer&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;CIFAR-10&#21644;ImageNet&#20998;&#31867;&#36825;&#26679;&#30340;&#35270;&#35273;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#31934;&#24230;&#25552;&#39640;&#36229;&#36807;5&#65285;&#21644;4&#65285;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#26159;&#38381;&#24335;&#30340;&#12289;&#26080;&#38656;&#23398;&#20064;&#30340;&#12289;&#38750;&#24120;&#31616;&#21333;&#65306;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26435;&#37325;&#30340;&#20056;&#31215;&#35774;&#32622;&#20026;&#36817;&#20284;&#20110;&#26631;&#35782;&#65292;&#23558;&#20540;&#21644;&#25237;&#24433;&#26435;&#37325;&#30340;&#20056;&#31215;&#36817;&#20284;&#20110;&#36127;&#26631;&#35782;&#12290;&#30001;&#20110;&#36825;&#31867;&#20284;&#20110;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;Transformer&#20013;&#30475;&#21040;&#30340;&#27169;&#24335;&#65292;&#25152;&#20197;&#25105;&#20204;&#31216;&#20026;&#8220;&#25311;&#24577;&#21021;&#22987;&#21270;&#8221;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they "look" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique "mimetic initialization".
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2305.09820</link><description>&lt;p&gt;
&#26426;&#22120;&#21046;&#36896;&#30340;&#23186;&#20307;&#65306;&#30417;&#27979;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#19978;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#30340;&#21160;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26085;&#30410;&#27969;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#38395;&#32593;&#31449;&#24320;&#22987;&#21033;&#29992;&#23427;&#20204;&#29983;&#25104;&#25991;&#31456;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#33021;&#22312;&#22768;&#35465;&#33391;&#22909;&#30340;&#32593;&#31449;&#19978;&#20135;&#29983;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#25991;&#31456;&#65292;&#32780;&#19988;&#19981;&#33391;&#26032;&#38395;&#32593;&#31449;&#20063;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;LLM&#25209;&#37327;&#29983;&#20135;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#24320;&#22987;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#21512;&#25104;&#25991;&#31456;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#20013;&#26222;&#21450;&#29575;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;DeBERTa&#30340;&#21512;&#25104;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#24182;&#23545;3074&#20010;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#36229;&#36807;1291&#19975;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;2022&#24180;1&#26376;1&#26085;&#33267;2023&#24180;4&#26376;1&#26085;&#26399;&#38388;&#65292;&#21512;&#25104;&#26032;&#38395;&#25991;&#31456;&#30340;&#30456;&#23545;&#25968;&#37327;&#22312;&#20027;&#27969;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;79.4&#65285;&#65292;&#32780;&#22312;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;342&#65285;&#12290;&#20998;&#26512;ChatGPT&#21457;&#24067;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20013;&#26029;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#23427;&#30340;&#21457;&#24067;&#23548;&#33268;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#26174;&#33879;&#22686;&#21152;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#30340;&#21512;&#25104;&#25991;&#31456;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#30340;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of generative large language models (LLMs) like ChatGPT, an increasing number of news websites have begun utilizing them to generate articles. However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize these LLMs to mass produce misinformation. To begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. To do this, we train a DeBERTa-based synthetic news detector and classify over 12.91 million articles from 3,074 misinformation and mainstream news websites. We find that between January 1, 2022 and April 1, 2023, the relative number of synthetic news articles increased by 79.4% on mainstream websites while increasing by 342% on misinformation sites. Analyzing the impact of the release of ChatGPT using an interrupted-time-series, we show that while its release resulted in a marked
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35282;&#33394;&#22270;&#20687;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25552;&#20379;&#35282;&#33394;&#30340;&#22270;&#29255;&#65292;&#29983;&#25104;&#21305;&#37197;&#26399;&#26395;&#30340;&#22270;&#20687;&#24182;&#35843;&#25972;&#21508;&#31181;&#32454;&#33410;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#20010;&#20307;/&#21160;&#30011;&#35282;&#33394;&#22270;&#20687;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09817</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#20154;&#20687;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method for Training-free Person Image Picture Generation. (arXiv:2305.09817v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35282;&#33394;&#22270;&#20687;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25552;&#20379;&#35282;&#33394;&#30340;&#22270;&#29255;&#65292;&#29983;&#25104;&#21305;&#37197;&#26399;&#26395;&#30340;&#22270;&#20687;&#24182;&#35843;&#25972;&#21508;&#31181;&#32454;&#33410;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#20010;&#20307;/&#21160;&#30011;&#35282;&#33394;&#22270;&#20687;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#22312;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#29255;&#37117;&#26159;&#21333;&#35843;&#30340;&#65292;&#22823;&#22810;&#25968;&#37117;&#26159;&#35757;&#32451;&#38598;&#20013;&#20154;&#29289;&#22270;&#29255;&#30340;&#20998;&#24067;&#32467;&#26524;&#65292;&#38590;&#20197;&#20026;&#22266;&#23450;&#25968;&#37327;&#30340;&#20010;&#20307;&#29983;&#25104;&#22810;&#24352;&#22270;&#29255;&#12290;&#22914;&#26524;&#35201;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#24517;&#39035;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#22914;&#26524;&#35201;&#32472;&#21046;&#27599;&#20010;&#20010;&#20307;/&#21160;&#30011;&#35282;&#33394;&#22270;&#20687;&#65292;&#24517;&#39035;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#31181;&#35757;&#32451;&#30340;&#30828;&#20214;&#21644;&#25104;&#26412;&#24120;&#24120;&#36229;&#20986;&#20102;&#26222;&#36890;&#29992;&#25143;&#30340;&#33021;&#21147;&#65292;&#32780;&#26222;&#36890;&#29992;&#25143;&#23454;&#38469;&#19978;&#21344;&#20102;&#20154;&#25968;&#26368;&#22810;&#30340;&#19968;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#35282;&#33394;&#22270;&#20687;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22411;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25552;&#20379;&#35282;&#33394;&#30340;&#22270;&#29255;&#65292;&#20351;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#35282;&#33394;&#19982;&#26399;&#26395;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#22312;&#36807;&#31243;&#20013;&#21487;&#20197;&#20351;&#29992;&#25552;&#31034;&#35843;&#25972;&#21508;&#31181;&#32454;&#33410;&#12290;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#27169;&#22411;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#35282;&#33394;&#22270;&#20687;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22411;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#65292;&#22240;&#27492;&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#20154;&#20687;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art Diffusion model has demonstrated excellent results in generating images. However, the images are monotonous and are mostly the result of the distribution of images of people in the training set, making it challenging to generate multiple images for a fixed number of individuals. This problem can often only be solved by fine-tuning the training of the model. This means that each individual/animated character image must be trained if it is to be drawn, and the hardware and cost of this training is often beyond the reach of the average user, who accounts for the largest number of people. To solve this problem, the Character Image Feature Encoder model proposed in this paper enables the user to use the process by simply providing a picture of the character to make the image of the character in the generated image match the expectation. In addition, various details can be adjusted during the process using prompts. Unlike traditional Image-to-Image models, the Ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09807</link><description>&lt;p&gt;
&#20851;&#20110;transformer&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#21487;&#36801;&#31227;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26597;&#35810;&#23545;&#27169;&#22411;&#23398;&#20064;&#26368;&#26377;&#30410;&#30340;&#31034;&#20363;&#26469;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#20110;&#24494;&#35843;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20027;&#21160;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#19981;&#28165;&#26970;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#20027;&#21160;&#23398;&#20064;&#25910;&#30410;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36866;&#29992;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#31215;&#26497;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#22312;&#20351;&#29992;&#19981;&#21516;PLM&#35757;&#32451;&#26102;&#33021;&#21542;&#20445;&#25345;AL&#25910;&#30410;&#12290;&#25105;&#20204;&#23558;AL&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#19982;&#19981;&#21516;PLMs&#26597;&#35810;&#21040;&#30340;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#34920;&#26126;&#20855;&#26377;&#31867;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;AL&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#20855;&#26377;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#33719;&#21462;&#24207;&#21015;&#30340;&#30456;&#20284;&#24615;&#26356;&#21463;&#21040;AL&#26041;&#27861;&#30340;&#36873;&#25321;&#32780;&#38750;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#23631;&#38556;&#20989;&#25968;&#21450;LBAC&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#36798;&#24615;&#26465;&#20214;&#19979;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#22312;&#23454;&#38469;2D&#22235;&#26059;&#32764;&#23548;&#33322;&#20219;&#21153;&#20013;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09793</link><description>&lt;p&gt;
&#24212;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#23631;&#38556;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#23433;&#20840;&#26426;&#22120;&#20154;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions. (arXiv:2305.09793v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#23631;&#38556;&#20989;&#25968;&#21450;LBAC&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#36798;&#24615;&#26465;&#20214;&#19979;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#22312;&#23454;&#38469;2D&#22235;&#26059;&#32764;&#23548;&#33322;&#20219;&#21153;&#20013;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#23545;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23637;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#24378;&#22823;&#30340;&#23433;&#20840;&#20445;&#38556;&#65292;&#20854;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#23631;&#38556;&#20989;&#25968;&#65288;CLBF&#65289;&#65292;&#20165;&#22522;&#20110;&#25968;&#25454;&#20998;&#26512;&#23433;&#20840;&#24615;&#21644;&#21487;&#36798;&#24615;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#20351;&#29992;&#21160;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LyapunovBarrierActor-Critic&#65288;LBAC&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#28385;&#36275;&#22522;&#20110;&#25968;&#25454;&#30340;&#23433;&#20840;&#21644;&#21487;&#36798;&#24615;&#26465;&#20214;&#30340;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#25511;&#21046;&#23454;&#39564;&#65292;&#21363;2D&#22235;&#26059;&#32764;&#23548;&#33322;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#22312;&#21487;&#36798;&#24615;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) exhibits impressive performance when managing complicated control tasks for robots. However, its wide application to physical robots is limited by the absence of strong safety guarantees. To overcome this challenge, this paper explores the control Lyapunov barrier function (CLBF) to analyze the safety and reachability solely based on data without explicitly employing a dynamic model. We also proposed the Lyapunov barrier actor-critic (LBAC), a model-free RL algorithm, to search for a controller that satisfies the data-based approximation of the safety and reachability conditions. The proposed approach is demonstrated through simulation and real-world robot control experiments, i.e., a 2D quadrotor navigation task. The experimental findings reveal this approach's effectiveness in reachability and safety, surpassing other model-free RL methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376;Newton&#26041;&#27861;&#65292;&#21487;&#20197;&#36845;&#20195;&#26500;&#36896;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28385;&#36275;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09792</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376; Newton &#26041;&#27861;&#29992;&#20110;&#27979;&#37327;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
A score-based operator Newton method for measure transport. (arXiv:2305.09792v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376;Newton&#26041;&#27861;&#65292;&#21487;&#20197;&#36845;&#20195;&#26500;&#36896;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28385;&#36275;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#27979;&#24230;&#30340;&#36816;&#36755;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#35768;&#22810;&#26680;&#24515;&#20219;&#21153;&#30340;&#22522;&#30784;&#65292;&#20174;&#21464;&#20998;&#25512;&#29702;&#21040;&#29983;&#25104;&#24314;&#27169;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#30446;&#26631;&#26159;&#23558;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#27010;&#29575;&#27979;&#24230;&#34920;&#31034;&#20026;&#36890;&#36807;&#23398;&#20064;&#30340;&#26144;&#23556;&#23558;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#25512;&#21521;&#21069;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#36816;&#36755;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#32473;&#20986;&#20102;&#35780;&#20272;&#30446;&#26631;&#20998;&#24067;&#20998;&#25968;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35813;&#26144;&#23556;&#29305;&#24449;&#21270;&#20026;&#19968;&#20010;&#26080;&#31351;&#32500;&#30340;&#20998;&#25968;&#27531;&#24046;&#31639;&#23376;&#30340;&#38646;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#36845;&#20195;&#26500;&#36896;&#36825;&#26679;&#19968;&#20010;&#38646;&#30340;&#29275;&#39039;&#31867;&#22411;&#26041;&#27861;&#12290;&#36890;&#36807;&#35843;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32463;&#20856;&#26925;&#22278;&#27491;&#21017;&#24615;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#22312;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#36825;&#31181;&#26500;&#36896;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#23558;&#22522;&#26412;&#30340;&#29275;&#39039;&#26041;&#27861;&#25512;&#24191;&#21040;&#26080;&#31351;&#32500;&#31639;&#23376;&#65292;&#20854;&#20182;&#24418;&#24335;&#30340;&#26080;&#31351;&#32500;&#31639;&#23376;&#24050;&#32463;&#20986;&#29616;&#22312;&#38750;&#32447;&#24615; PDE &#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transportation of probability measures underlies many core tasks in statistics and machine learning, from variational inference to generative modeling. A typical goal is to represent a target probability measure of interest as the push-forward of a tractable source measure through a learned map. We present a new construction of such a transport map, given the ability to evaluate the score of the target distribution. Specifically, we characterize the map as a zero of an infinite-dimensional score-residual operator and derive a Newton-type method for iteratively constructing such a zero. We prove convergence of these iterations by invoking classical elliptic regularity theory for partial differential equations (PDE) and show that this construction enjoys rapid convergence, under smoothness assumptions on the target score. A key element of our approach is a generalization of the elementary Newton method to infinite-dimensional operators, other forms of which have appeared in nonlinear PDE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Molecule-Morphology Contrastive Pretraining (MoCoP)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#22270;&#24418;&#21644;&#32454;&#32990;&#24418;&#24577;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoCoP&#21487;&#20197;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09790</link><description>&lt;p&gt;
&#20998;&#23376;&#24418;&#24577;&#23545;&#27604;&#39044;&#35757;&#32451;&#25552;&#39640;&#20998;&#23376;&#34920;&#31034;&#36801;&#31227;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Molecule-Morphology Contrastive Pretraining for Transferable Molecular Representation. (arXiv:2305.09790v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Molecule-Morphology Contrastive Pretraining (MoCoP)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#22270;&#24418;&#21644;&#32454;&#32990;&#24418;&#24577;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoCoP&#21487;&#20197;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#26512;&#25216;&#26415;&#22240;&#20854;&#22312;&#30446;&#26631;&#37492;&#23450;&#12289;&#20316;&#29992;&#26426;&#21046;&#25512;&#26029;&#21644;&#27979;&#23450;&#21457;&#23637;&#20013;&#30340;&#24212;&#29992;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#25216;&#26415;&#20135;&#29983;&#20102;&#22823;&#37327;&#32454;&#32990;&#24418;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#29992;&#20110;&#30740;&#31350;&#23567;&#20998;&#23376;&#24178;&#25200;&#29289;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;Molecule-Morphology Contrastive Pretraining (MoCoP)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20998;&#23376;&#22270;&#24418;&#21644;&#32454;&#32990;&#24418;&#24577;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#23558;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#25193;&#23637;&#21040;&#20102;&#25913;&#36827;&#37327;&#21270;&#32467;&#26500;-&#27963;&#24615;&#20851;&#31995; (QSAR) &#27169;&#22411;&#12290;&#20351;&#29992;&#26469;&#33258;JUMP-CP&#32852;&#30431;&#30340;&#25968;&#25454;&#65292;&#23558;MoCoP&#25193;&#23637;&#21040;&#20102;&#32422;100K&#30340;&#20998;&#23376;&#21644;&#32422;600K&#30340;&#24418;&#24577;&#25991;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;MoCoP&#22312;ChEMBL20&#19978;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#22987;&#32456;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#30340;&#34920;&#29616;&#65292;&#19988;&#22312;&#20869;&#37096; GSK&#33647;&#20195;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based profiling techniques have become increasingly popular over the past decade for their applications in target identification, mechanism-of-action inference, and assay development. These techniques have generated large datasets of cellular morphologies, which are typically used to investigate the effects of small molecule perturbagens. In this work, we extend the impact of such dataset to improving quantitative structure-activity relationship (QSAR) models by introducing Molecule-Morphology Contrastive Pretraining (MoCoP), a framework for learning multi-modal representation of molecular graphs and cellular morphologies. We scale MoCoP to approximately 100K molecules and 600K morphological profiles using data from the JUMP-CP Consortium and show that MoCoP consistently improves performances of graph neural networks (GNNs) on molecular property prediction tasks in ChEMBL20 across all dataset sizes. The pretrained GNNs are also evaluated on internal GSK pharmacokinetic data and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#24341;&#23548;&#36710;&#36742;&#65288;AGV&#65289;&#25511;&#21046;&#12289;&#36793;&#32536;&#26234;&#33021;&#21644;&#20154;&#31867;&#36755;&#20837;&#30340;&#21644;&#35856;&#35774;&#35745;&#65292;&#20197;&#23454;&#29616;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#36816;&#36755;&#65292;&#20854;&#26680;&#24515;&#25216;&#26415;&#26159;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36830;&#25509;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;AGV&#23454;&#29616;&#20154;&#26426;&#21327;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.09788</link><description>&lt;p&gt;
&#36793;&#32536;&#26234;&#33021;&#19982;&#33258;&#21160;&#24341;&#23548;&#36710;&#36742;&#25511;&#21046;&#30340;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Codesign of Edge Intelligence and Automated Guided Vehicle Control. (arXiv:2305.09788v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#24341;&#23548;&#36710;&#36742;&#65288;AGV&#65289;&#25511;&#21046;&#12289;&#36793;&#32536;&#26234;&#33021;&#21644;&#20154;&#31867;&#36755;&#20837;&#30340;&#21644;&#35856;&#35774;&#35745;&#65292;&#20197;&#23454;&#29616;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#36816;&#36755;&#65292;&#20854;&#26680;&#24515;&#25216;&#26415;&#26159;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36830;&#25509;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;AGV&#23454;&#29616;&#20154;&#26426;&#21327;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#24341;&#23548;&#36710;&#36742;&#65288;AGV&#65289;&#25511;&#21046;&#12289;&#36793;&#32536;&#26234;&#33021;&#21644;&#20154;&#31867;&#36755;&#20837;&#30340;&#21644;&#35856;&#35774;&#35745;&#65292;&#20197;&#23454;&#29616;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#36816;&#36755;&#12290;&#35813;AGV&#20855;&#26377;&#22312;&#28304;&#21644;&#30446;&#26631;&#20043;&#38388;&#23548;&#33322;&#24182;&#25342;&#21462;/&#25918;&#32622;&#29289;&#21697;&#30340;&#33021;&#21147;&#12290;&#20154;&#31867;&#36755;&#20837;&#38544;&#21547;&#22320;&#25552;&#20379;&#20102;&#30446;&#30340;&#22320;&#21644;&#20934;&#30830;&#30340;&#21368;&#36135;&#28857;&#30340;&#20559;&#22909;&#65292;&#36825;&#20123;&#20559;&#22909;&#26469;&#33258;&#20110;&#32593;&#32476;&#36793;&#32536;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#19982;AGV&#20849;&#20139;&#12290;&#28436;&#31034;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#30828;&#20214;&#12289;&#36719;&#20214;&#21644;AI&#35774;&#35745;&#30340;&#32508;&#21512;&#35774;&#35745;&#36798;&#21040;&#20102;&#25216;&#26415;&#25104;&#29087;&#24230;&#27700;&#24179;&#65288;TRL&#65289;4-5&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a harmonic design of autonomous guided vehicle (AGV) control, edge intelligence, and human input to enable autonomous transportation in industrial environments. The AGV has the capability to navigate between a source and destinations and pick/place objects. The human input implicitly provides preferences of the destination and exact drop point, which are derived from an artificial intelligence (AI) module at the network edge and shared with the AGV over a wireless network. The demonstration indicates that the proposed integrated design of hardware, software, and AI design achieve a technology readiness level (TRL) of range 4-5
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21516;&#26102;&#27714;&#35299;&#21644;&#20272;&#35745;&#37329;&#34701;&#32463;&#27982;&#20013;&#32463;&#20856;&#30340;&#36830;&#32493;&#26102;&#38388;&#19968;&#33324;&#22343;&#34913;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20135;&#19994;&#21160;&#24577;&#21644;&#24102;&#26377;&#37329;&#34701;&#25705;&#25830;&#30340;&#23439;&#35266;&#32463;&#27982;&#27169;&#22411;&#20004;&#20010;&#26679;&#20363;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#28857;&#65292;&#20855;&#26377;&#22810;&#31181;&#29992;&#36884;&#12290;</title><link>http://arxiv.org/abs/2305.09783</link><description>&lt;p&gt;
&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#21644;&#20272;&#35745;&#23439;&#35266;&#37329;&#34701;&#21160;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Solving and Estimating Dynamic Macro-Finance Models. (arXiv:2305.09783v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21516;&#26102;&#27714;&#35299;&#21644;&#20272;&#35745;&#37329;&#34701;&#32463;&#27982;&#20013;&#32463;&#20856;&#30340;&#36830;&#32493;&#26102;&#38388;&#19968;&#33324;&#22343;&#34913;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20135;&#19994;&#21160;&#24577;&#21644;&#24102;&#26377;&#37329;&#34701;&#25705;&#25830;&#30340;&#23439;&#35266;&#32463;&#27982;&#27169;&#22411;&#20004;&#20010;&#26679;&#20363;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#28857;&#65292;&#20855;&#26377;&#22810;&#31181;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21516;&#26102;&#35299;&#20915;&#21644;&#20272;&#35745;&#37329;&#34701;&#32463;&#27982;&#20013;&#32463;&#20856;&#30340;&#36830;&#32493;&#26102;&#38388;&#19968;&#33324;&#22343;&#34913;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26679;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#20225;&#19994;&#30340;&#20135;&#19994;&#21160;&#24577;&#21644;&#65288;2&#65289;&#24102;&#26377;&#37329;&#34701;&#25705;&#25830;&#30340;&#23439;&#35266;&#32463;&#27982;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#20123;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#28857;&#65306;&#36890;&#29992;&#24615;&#12289;&#21516;&#26102;&#27714;&#35299;&#21644;&#20272;&#35745;&#12289;&#36816;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22788;&#29702;&#22823;&#35268;&#27169;&#29366;&#24577;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#22810;&#31181;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a methodology that utilizes deep learning to simultaneously solve and estimate canonical continuous-time general equilibrium models in financial economics. We illustrate our method in two examples: (1) industrial dynamics of firms and (2) macroeconomic models with financial frictions. Through these applications, we illustrate the advantages of our method: generality, simultaneous solution and estimation, leveraging the state-of-art machine-learning techniques, and handling large state space. The method is versatile and can be applied to a vast variety of problems.
&lt;/p&gt;</description></item><item><title>SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.09781</link><description>&lt;p&gt;
SpecInfer&#65306;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09781
&lt;/p&gt;
&lt;p&gt;
SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;SpecInfer&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;LLM&#25512;&#26029;&#12290;SpecInfer&#32972;&#21518;&#30340;&#20851;&#38190;&#26159;&#23558;&#21508;&#31181;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#25552;&#21319;&#35843;&#25972;&#65292;&#20849;&#21516;&#39044;&#27979;LLM&#30340;&#36755;&#20986;&#65307; &#39044;&#27979;&#32467;&#26524;&#32452;&#32455;&#25104;&#19968;&#20010;&#20196;&#29260;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#34920;&#31034;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26641;&#30340;&#24182;&#34892;&#35299;&#30721;&#26426;&#21046;&#65292;&#20197;LMM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#20196;&#29260;&#26641;&#25152;&#20195;&#34920;&#30340;&#25152;&#26377;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#30340;&#27491;&#30830;&#24615;&#12290;SpecInfer&#20351;&#29992;LLM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#65292;&#32780;&#19981;&#26159;&#22686;&#37327;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#29983;&#25104;&#24335;LLM&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#21487;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#25193;&#23637;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#36991;&#20813;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#38454;&#39057;&#29575;&#20197;&#21450;&#35823;&#35782;&#21035;&#20302;&#38454;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09779</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#20811;&#26381;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#38454;&#35889;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks. (arXiv:2305.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#25193;&#23637;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#36991;&#20813;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#38454;&#39057;&#29575;&#20197;&#21450;&#35823;&#35782;&#21035;&#20302;&#38454;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23398;&#20064;&#20219;&#24847;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#20294;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#27169;&#22411;&#24120;&#24120;&#34920;&#29616;&#20986;&#23545;&#8220;&#26356;&#31616;&#21333;&#8221;&#20989;&#25968;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#20613;&#37324;&#21494;&#65288;Walsh-Hadamard&#65289;&#21464;&#25442;&#65292;&#20174;&#31163;&#25955;&#65288;&#38646;&#19968;&#65289;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#31616;&#21333;&#24615;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#20613;&#37324;&#21494;&#31995;&#25968;&#30340;&#8220;&#38454;&#8221;&#26469;&#25429;&#25417;&#31616;&#21333;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#26377;&#23398;&#20064;&#36739;&#20302;&#38454;&#39057;&#29575;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35889;&#20559;&#24046;&#21521;&#36739;&#31616;&#21333;&#29305;&#24449;&#30340;&#36235;&#21183;&#23454;&#38469;&#19978;&#20250;&#25439;&#23475;&#31070;&#32463;&#32593;&#32476;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26356;&#39640;&#30340;&#38454;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#36824;&#26377;&#21161;&#20110;&#36991;&#20813;&#23545;&#20302;&#38454;&#39057;&#29575;&#30340;&#38169;&#35823;&#35782;&#21035;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#22312;&#20302;&#25968;&#25454;&#37327;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the capacity of neural nets to learn arbitrary functions, models trained through gradient descent often exhibit a bias towards ``simpler'' functions. Various notions of simplicity have been introduced to characterize this behavior. Here, we focus on the case of neural networks with discrete (zero-one) inputs through the lens of their Fourier (Walsh-Hadamard) transforms, where the notion of simplicity can be captured through the \emph{degree} of the Fourier coefficients. We empirically show that neural networks have a tendency to learn lower-degree frequencies. We show how this spectral bias towards simpler features can in fact \emph{hurt} the neural network's generalization on real-world datasets. To remedy this we propose a new scalable functional regularization scheme that aids the neural network to learn higher degree frequencies. Our regularizer also helps avoid erroneous identification of low-degree frequencies, which further improves generalization. We extensively evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BSGAN&#30340;&#28151;&#21512;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#26377;&#36793;&#30028;SMOTE&#36807;&#37319;&#26679;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#36807;&#37319;&#26679;&#21518;&#21457;&#29983;&#30340;&#36793;&#32536;&#21270;&#38382;&#39064;&#65292;&#24182;&#29983;&#25104;&#26356;&#22810;&#22810;&#26679;&#24615;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09777</link><description>&lt;p&gt;
BSGAN&#65306;&#19968;&#31181;&#26032;&#30340;&#19981;&#24179;&#34913;&#27169;&#24335;&#35782;&#21035;&#36807;&#37319;&#26679;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
BSGAN: A Novel Oversampling Technique for Imbalanced Pattern Recognitions. (arXiv:2305.09777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BSGAN&#30340;&#28151;&#21512;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#26377;&#36793;&#30028;SMOTE&#36807;&#37319;&#26679;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#36807;&#37319;&#26679;&#21518;&#21457;&#29983;&#30340;&#36793;&#32536;&#21270;&#38382;&#39064;&#65292;&#24182;&#29983;&#25104;&#26356;&#22810;&#22810;&#26679;&#24615;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;CIP&#65289;&#26159;&#24320;&#21457;&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#30340;&#28508;&#22312;&#25361;&#25112;&#20043;&#19968;&#12290;CIP&#21457;&#29983;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#31867;&#20043;&#38388;&#30340;&#25968;&#25454;&#26679;&#26412;&#26410;&#31561;&#20998;&#24067;&#26102;&#12290;&#36793;&#30028;&#21512;&#25104;&#23569;&#25968;&#26679;&#26412;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#36807;&#36807;&#37319;&#26679;&#23569;&#25968;&#65288;&#26377;&#38480;&#65289;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#36793;&#30028;SMOTE&#30340;&#28508;&#22312;&#32570;&#28857;&#26159;&#23427;&#20391;&#37325;&#20110;&#20301;&#20110;&#36793;&#32536;&#28857;&#30340;&#25968;&#25454;&#26679;&#26412;&#24182;&#26356;&#21152;&#20851;&#27880;&#26497;&#31471;&#35266;&#27979;&#20540;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#36807;&#37319;&#26679;&#21518;&#26356;&#22810;&#19981;&#21516;&#26679;&#26412;&#30340;&#21019;&#36896;&#65292;&#36825;&#20960;&#20046;&#26159;&#22823;&#22810;&#25968;&#36793;&#30028;SMOTE&#30340;&#22522;&#20110;&#36807;&#37319;&#26679;&#31574;&#30053;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#36807;&#37319;&#26679;&#21518;&#20250;&#21457;&#29983;&#36793;&#32536;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#36793;&#30028;SMOTE&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21147;&#37327;&#26469;&#29983;&#25104;&#26356;&#22810;&#22810;&#26679;&#24615;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalanced problems (CIP) are one of the potential challenges in developing unbiased Machine Learning (ML) models for predictions. CIP occurs when data samples are not equally distributed between the two or multiple classes. Borderline-Synthetic Minority Oversampling Techniques (SMOTE) is one of the approaches that has been used to balance the imbalance data by oversampling the minor (limited) samples. One of the potential drawbacks of existing Borderline-SMOTE is that it focuses on the data samples that lay at the border point and gives more attention to the extreme observations, ultimately limiting the creation of more diverse data after oversampling, and that is the almost scenario for the most of the borderline-SMOTE based oversampling strategies. As an effect, marginalization occurs after oversampling. To address these issues, in this work, we propose a hybrid oversampling technique by combining the power of borderline SMOTE and Generative Adversarial Network to generate mor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#36828;&#31243;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#28436;&#31034;&#25991;&#20214;&#30340;&#36136;&#37327;&#19981;&#36275;&#19982;&#25910;&#38598;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20195;&#30721;&#24320;&#28304;&#12289;&#30828;&#20214;&#26131;&#24471;&#12289;&#26131;&#20110;&#20462;&#25913;&#21644;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.09765</link><description>&lt;p&gt;
OpenVR&#65306;&#25805;&#20316;&#30340;&#36828;&#31243;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
OpenVR: Teleoperation for Manipulation. (arXiv:2305.09765v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#36828;&#31243;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#28436;&#31034;&#25991;&#20214;&#30340;&#36136;&#37327;&#19981;&#36275;&#19982;&#25910;&#38598;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20195;&#30721;&#24320;&#28304;&#12289;&#30828;&#20214;&#26131;&#24471;&#12289;&#26131;&#20110;&#20462;&#25913;&#21644;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#39640;&#36136;&#37327;&#30340;&#28436;&#31034;&#25991;&#20214;&#26159;&#35768;&#22810;&#25511;&#21046;&#27969;&#31243;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#28436;&#31034;&#36712;&#36857;&#20173;&#28982;&#36153;&#26102;&#36153;&#21147;&#65292;&#24448;&#24448;&#23548;&#33268;&#28436;&#31034;&#30340;&#25968;&#37327;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#36828;&#31243;&#25511;&#21046;&#26041;&#27861;&#65292;&#20351;&#29992;Oculus VR&#22836;&#25140;&#24335;&#26174;&#31034;&#35774;&#22791;&#23545;Franka Emika Panda&#26426;&#22120;&#20154;&#36827;&#34892;&#36828;&#31243;&#25805;&#25511;&#12290;&#34429;&#28982;&#36824;&#23384;&#22312;&#20854;&#20182;VR&#36828;&#31243;&#25511;&#21046;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#36866;&#29992;&#20110;&#26131;&#24471;&#30340;&#28040;&#36153;&#32423;&#30828;&#20214;&#65292;&#26131;&#20110;&#20462;&#25913;&#65292;&#23545;&#23454;&#39564;&#35774;&#22791;&#35774;&#32622;&#19981;&#20855;&#26377;&#29305;&#23450;&#30340;&#35201;&#27714;&#65292;&#31616;&#21333;&#26131;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Across the robotics field, quality demonstrations are an integral part of many control pipelines. However, collecting high-quality demonstration trajectories remains time-consuming and difficult, often resulting in the number of demonstrations being the performance bottleneck. To address this issue, we present a method of Virtual Reality (VR) Teleoperation that uses an Oculus VR headset to teleoperate a Franka Emika Panda robot. Although other VR teleoperation methods exist, our code is open source, designed for readily available consumer hardware, easy to modify, agnostic to experimental setup, and simple to use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#28082;&#20307;&#27689;&#25506;&#27979;&#22120;&#20302;&#33021;&#29289;&#29702;&#20013;&#20351;&#29992;Few-Hits&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#22312;&#21333;&#27604;&#29305;&#19982;&#21452;&#27604;&#29305;&#20107;&#20214;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer-Encoder&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;DUNE Phase II&#25506;&#27979;&#22120;&#20248;&#21270;&#20102;&#25506;&#27979;&#22120;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.09744</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#28082;&#20307;&#27689;&#25506;&#27979;&#22120;&#20302;&#33021;&#29289;&#29702;&#30340;Few-Hits&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Assessment of few-hits machine learning classification algorithms for low energy physics in liquid argon detectors. (arXiv:2305.09744v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#28082;&#20307;&#27689;&#25506;&#27979;&#22120;&#20302;&#33021;&#29289;&#29702;&#20013;&#20351;&#29992;Few-Hits&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#22312;&#21333;&#27604;&#29305;&#19982;&#21452;&#27604;&#29305;&#20107;&#20214;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer-Encoder&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;DUNE Phase II&#25506;&#27979;&#22120;&#20248;&#21270;&#20102;&#25506;&#27979;&#22120;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#33021;&#21306;&#22495;&#65292;&#22823;&#22411;&#28082;&#20307;&#27689;TPCs&#30340;&#29289;&#29702;&#28508;&#21147;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#22240;&#20026;Few-Hits&#20107;&#20214;&#25152;&#32534;&#30721;&#30340;&#20449;&#24687;&#24456;&#38590;&#34987;&#20256;&#32479;&#20998;&#31867;&#31639;&#27861;&#21033;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#36825;&#20123;&#31867;&#22411;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20256;&#32479;&#65288;&#30830;&#23450;&#24615;&#65289;&#31639;&#27861;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer-Encoder&#26041;&#27861;&#22312;&#20302;&#33021;&#29289;&#29702;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#38382;&#39064;&#65288;&#21333;&#27604;&#29305;&#19982;&#21452;&#27604;&#29305;&#20107;&#20214;&#65289;&#20013;&#20248;&#20110;&#30830;&#23450;&#24615;&#31639;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Transformer-Encoder&#26041;&#27861;&#30456;&#23545;&#20110;CNN&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#20102;&#25506;&#27979;&#22120;&#21442;&#25968;&#65292;&#37325;&#28857;&#20851;&#27880;DUNE Phase II&#25506;&#27979;&#22120;&#65288;"&#26426;&#20250;&#27169;&#22359;"&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The physics potential of massive liquid argon TPCs in the low-energy regime is still to be fully reaped because few-hits events encode information that can hardly be exploited by conventional classification algorithms. Machine learning (ML) techniques give their best in these types of classification problems. In this paper, we evaluate their performance against conventional (deterministic) algorithms. We demonstrate that both Convolutional Neural Networks (CNN) and Transformer-Encoder methods outperform deterministic algorithms in one of the most challenging classification problems of low-energy physics (single- versus double-beta events). We discuss the advantages and pitfalls of Transformer-Encoder methods versus CNN and employ these methods to optimize the detector parameters, with an emphasis on the DUNE Phase II detectors ("Module of Opportunity").
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;CNN&#30340;&#37327;&#23376;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#37322;&#21738;&#20123;&#29305;&#24449;&#23545;&#20110;&#20998;&#31867;&#26368;&#37325;&#35201;&#26469;&#36991;&#20813;&#36951;&#24536;&#65292;&#24182;&#22768;&#31216;&#22914;&#26524;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21017;&#20250;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09738</link><description>&lt;p&gt;
CQural&#65306;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;CNN&#30340;&#37327;&#23376;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning. (arXiv:2305.09738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;CNN&#30340;&#37327;&#23376;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#37322;&#21738;&#20123;&#29305;&#24449;&#23545;&#20110;&#20998;&#31867;&#26368;&#37325;&#35201;&#26469;&#36991;&#20813;&#36951;&#24536;&#65292;&#24182;&#22768;&#31216;&#22914;&#26524;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21017;&#20250;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#37327;&#24335;&#23398;&#20064;&#20013;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#20165;&#24456;&#37325;&#35201;&#65292;&#32780;&#19988;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#24456;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#26469;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30340;&#36951;&#24536;&#24433;&#21709;&#65292;&#20294;&#26159;&#25152;&#26377;&#30340;&#25216;&#26415;&#37117;&#26159;&#22312;&#32463;&#20856;&#23398;&#20064;&#19978;&#30740;&#31350;&#30340;&#65292;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#30340;&#25913;&#21464;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26032;&#30340;&#28151;&#21512;&#32463;&#20856; - &#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36991;&#20813;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#35299;&#37322;&#20102;&#21738;&#20123;&#21151;&#33021;&#23545;&#20110;&#20998;&#31867;&#26368;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22768;&#31216;&#22914;&#26524;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21017;&#20250;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20025;&#40614;&#25163;&#35821;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;ADDSL&#65289;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv5&#30340;&#25163;&#21183;&#26816;&#27979;&#21644;&#23383;&#27597;&#25968;&#23383;&#35782;&#21035;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#26368;&#39640;&#21487;&#36798;92%&#12290;&#19982;&#21516;&#39046;&#22495;&#29616;&#26377;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.09736</link><description>&lt;p&gt;
ADDSL: &#22522;&#20110;&#26631;&#27880;&#30340;&#20025;&#40614;&#25163;&#35821;&#30340;&#25163;&#21183;&#26816;&#27979;&#19982;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language. (arXiv:2305.09736v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20025;&#40614;&#25163;&#35821;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;ADDSL&#65289;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv5&#30340;&#25163;&#21183;&#26816;&#27979;&#21644;&#23383;&#27597;&#25968;&#23383;&#35782;&#21035;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#26368;&#39640;&#21487;&#36798;92%&#12290;&#19982;&#21516;&#39046;&#22495;&#29616;&#26377;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#23558;&#25163;&#21183;&#26816;&#27979;&#24182;&#23558;&#20854;&#35782;&#21035;&#20026;&#23383;&#27597;&#25110;&#25968;&#23383;&#19968;&#30452;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#32473;&#27531;&#38556;&#20154;&#22763;&#24102;&#26469;&#20102;&#27807;&#36890;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#20025;&#40614;&#25163;&#35821;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;ADDSL&#65289;&#12290;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;LabelImg&#22312;YOLO&#26684;&#24335;&#20013;&#21046;&#20316;&#20102;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#12290;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;CSP-DarkNet53&#39592;&#24178;&#21644;YOLOv3&#22836;&#30340;&#21333;&#38454;&#27573;&#30446;&#26631;&#26816;&#27979;&#22120;&#27169;&#22411;&#65288;YOLOv5&#65289;&#36890;&#36807;&#27599;&#31867;&#20165;&#20351;&#29992;&#19971;&#20010;&#29420;&#29305;&#30340;&#22270;&#20687;&#65288;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65289;&#26469;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#23383;&#27597;&#65288;A-Z&#65289;&#21644;&#25968;&#23383;&#65288;0-9&#65289;&#12290;&#35757;&#32451;&#20116;&#20010;&#27169;&#22411;&#65292;&#20849;350&#20010;&#21608;&#26399;&#65292;&#24471;&#21040;&#27599;&#24352;&#22270;&#20687;&#30340;&#24179;&#22343;&#25512;&#26029;&#26102;&#38388;&#20026;9.02ms&#65292;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#26368;&#20339;&#20934;&#30830;&#29575;&#20026;92%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#25913;&#21518;&#30340;&#27169;&#22411;&#27604;&#21516;&#39046;&#22495;&#30340;&#29616;&#26377;&#24037;&#20316;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#20195;&#30721;&#24211;&#21487;&#22312;GitHub&#23384;&#20648;&#24211;https://github.com/s4nyam/pvt-addsl &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository https://github.com/s4nyam/pvt-addsl.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20219;&#21153;&#35782;&#21035;&#21644;&#20219;&#21153;&#23398;&#20064;&#20004;&#31181;&#26041;&#24335;&#34920;&#24449;&#20102;ICL&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;LLMs&#21033;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#20219;&#21153;&#30340;&#35299;&#20915;&#65292;TR&#20027;&#35201;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;TL&#21017;&#20855;&#22791;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09731</link><description>&lt;p&gt;
&#22312;&#35821;&#22659;&#20013;&#23398;&#20064;&#65306;&#8220;&#23398;&#20064;&#8221;&#35821;&#22659;&#20013;&#30340;&#20219;&#21153;&#35782;&#21035;&#21644;&#20219;&#21153;&#23398;&#20064;&#30340;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning. (arXiv:2305.09731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20219;&#21153;&#35782;&#21035;&#21644;&#20219;&#21153;&#23398;&#20064;&#20004;&#31181;&#26041;&#24335;&#34920;&#24449;&#20102;ICL&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;LLMs&#21033;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#20219;&#21153;&#30340;&#35299;&#20915;&#65292;TR&#20027;&#35201;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;TL&#21017;&#20855;&#22791;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35821;&#22659;&#20013;&#30340;&#23398;&#20064;&#26469;&#35299;&#20915;&#21482;&#26377;&#23569;&#25968;&#28436;&#31034;&#30340;&#20219;&#21153;&#65292;&#20294;&#20854;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;LLMs&#20165;&#22238;&#24518;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#24050;&#23398;&#27010;&#24565;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#21017;&#26263;&#31034;ICL&#25191;&#34892;&#28436;&#31034;&#30340;&#38544;&#21547;&#23398;&#20064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#35782;&#21035;(TR)&#21644;&#20219;&#21153;&#23398;&#20064;(TL)&#20004;&#31181;&#26041;&#24335;&#34920;&#24449;&#20102;ICL&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;LLM&#31995;&#21015;&#65288;GPT-3&#12289;LLaMA&#21644;OPT&#65289;&#36827;&#34892;&#25511;&#21046;&#23454;&#39564;&#65292;&#22312;ICL&#20013;&#21306;&#20998;TR&#21644;TL&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#27169;&#22411;&#21482;&#20351;&#29992;TR&#23601;&#33021;&#21462;&#24471;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#65292;TR&#19981;&#20250;&#38543;&#30528;&#26356;&#22823;&#30340;&#27169;&#22411;&#25110;&#26356;&#22810;&#30340;&#28436;&#31034;&#32780;&#36827;&#19968;&#27493;&#25913;&#21892;&#65307;&#65288;2&#65289;LLMs&#33021;&#22815;&#36890;&#36807;TL&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#65292;&#32780;TR&#21017;&#20027;&#35201;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquir
&lt;/p&gt;</description></item><item><title>FedHGN&#26159;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#27169;&#24335;&#26435;&#37325;&#35299;&#32806;&#21644;&#31995;&#25968;&#23545;&#40784;&#25216;&#26415;&#65292;&#20351;&#24471;&#19981;&#21516;&#23458;&#25143;&#31471;&#21487;&#20197;&#20849;&#20139;&#30693;&#35782;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#21152;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.09729</link><description>&lt;p&gt;
FedHGN&#65306;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks. (arXiv:2305.09729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09729
&lt;/p&gt;
&lt;p&gt;
FedHGN&#26159;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#27169;&#24335;&#26435;&#37325;&#35299;&#32806;&#21644;&#31995;&#25968;&#23545;&#40784;&#25216;&#26415;&#65292;&#20351;&#24471;&#19981;&#21516;&#23458;&#25143;&#31471;&#21487;&#20197;&#20849;&#20139;&#30693;&#35782;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;GNN&#30456;&#27604;&#65292;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20174;&#31867;&#22411;&#21270;&#21644;&#20851;&#31995;&#21270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#30001;&#20110;&#38544;&#31169;&#27861;&#35268;&#65288;&#20363;&#22914;GDPR&#65289;&#65292;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#24448;&#24448;&#24456;&#23569;&#65292;&#32780;&#20351;&#29992;&#26356;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#32852;&#37030;&#22270;&#23398;&#20064;&#65288;FGL&#65289;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;GNN&#32780;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FGL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21516;&#26500;GNN&#25110;&#30693;&#35782;&#22270;&#23884;&#20837;&#19978;&#65307;&#24456;&#23569;&#32771;&#34385;&#24322;&#26500;&#22270;&#21644;HGNN&#12290;&#22312;&#32852;&#37030;&#24322;&#26500;&#22270;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#25317;&#26377;&#31169;&#26377;&#22270;&#27169;&#24335;&#65292;&#23581;&#35797;&#23450;&#20041;&#20840;&#23616;HGNN&#27169;&#22411;&#30340;&#20256;&#32479;FL/FGL&#26041;&#27861;&#20250;&#20405;&#29359;&#27169;&#24335;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHGN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;HGNN FGL&#26694;&#26550;&#12290;FedHGN&#37319;&#29992;&#27169;&#24335;&#26435;&#37325;&#35299;&#32806;&#26469;&#23454;&#29616;&#29420;&#31435;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#24182;&#37319;&#29992;&#31995;&#25968;&#23545;&#40784;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#21644;&#25552;&#39640;HGNN&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedHGN&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks (HGNNs) can learn from typed and relational graph data more effectively than conventional GNNs. With larger parameter spaces, HGNNs may require more training data, which is often scarce in real-world applications due to privacy regulations (e.g., GDPR). Federated graph learning (FGL) enables multiple clients to train a GNN collaboratively without sharing their local data. However, existing FGL methods mainly focus on homogeneous GNNs or knowledge graph embeddings; few have considered heterogeneous graphs and HGNNs. In federated heterogeneous graph learning, clients may have private graph schemas. Conventional FL/FGL methods attempting to define a global HGNN model would violate schema privacy. To address these challenges, we propose FedHGN, a novel and general FGL framework for HGNNs. FedHGN adopts schema-weight decoupling to enable schema-agnostic knowledge sharing and employs coefficients alignment to stabilize the training process and improve HGNN
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#36793;&#32534;&#30721;&#26159;&#19968;&#31181;&#21387;&#32553;&#22823;&#22411;&#26631;&#35760;&#22270;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;bits-back&#32534;&#30721;&#20174;&#36793;&#32536;&#21015;&#34920;&#20013;&#26080;&#26367;&#25442;&#22320;&#23545;&#36793;&#32536;&#21644;&#39030;&#28857;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09705</link><description>&lt;p&gt;
&#38543;&#26426;&#36793;&#32534;&#30721;&#65306;&#22823;&#22411;&#26631;&#35760;&#22270;&#30340;&#19968;&#27425;&#24615;Bits-Back&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Random Edge Coding: One-Shot Bits-Back Coding of Large Labeled Graphs. (arXiv:2305.09705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09705
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36793;&#32534;&#30721;&#26159;&#19968;&#31181;&#21387;&#32553;&#22823;&#22411;&#26631;&#35760;&#22270;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;bits-back&#32534;&#30721;&#20174;&#36793;&#32536;&#21015;&#34920;&#20013;&#26080;&#26367;&#25442;&#22320;&#23545;&#36793;&#32536;&#21644;&#39030;&#28857;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#22823;&#22411;&#26631;&#35760;&#22270;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;&#36793;&#32534;&#30721;&#12290;&#24403;&#19982;&#22522;&#20110;P&#243;lya's Urn&#30340;&#26080;&#21442;&#25968;&#27169;&#22411;&#37197;&#23545;&#20351;&#29992;&#26102;&#65292;&#26368;&#22351;&#24773;&#20917;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#38543;&#35266;&#23519;&#21040;&#30340;&#36793;&#25968;&#20960;&#20046;&#32447;&#24615;&#21644;&#32447;&#24615;&#22320;&#32553;&#25918;&#65292;&#20351;&#20854;&#22312;&#31232;&#30095;&#22270;&#19978;&#39640;&#25928;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#25972;&#25968;&#31639;&#26415;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;bits-back&#32534;&#30721;&#65292;&#23427;&#34987;&#29992;&#20110;&#20174;&#36793;&#32536;&#21015;&#34920;&#20013;&#26080;&#26367;&#25442;&#22320;&#23545;&#36793;&#32536;&#21644;&#39030;&#28857;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#20445;&#30041;&#22270;&#30340;&#32467;&#26500;&#12290;&#22312;&#19968;&#31867;&#38543;&#26426;&#22270;&#27169;&#22411;&#19979;&#35777;&#26126;&#20102;&#26368;&#20248;&#24615;&#65292;&#35813;&#27169;&#22411;&#23545;&#25490;&#21015;&#30340;&#36793;&#21644;&#36793;&#20869;&#39030;&#28857;&#30340;&#25490;&#21015;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#38543;&#26426;&#36793;&#32534;&#30721;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a one-shot method for compressing large labeled graphs called Random Edge Coding. When paired with a parameter-free model based on P\'olya's Urn, the worst-case computational and memory complexities scale quasi-linearly and linearly with the number of observed edges, making it efficient on sparse graphs, and requires only integer arithmetic. Key to our method is bits-back coding, which is used to sample edges and vertices without replacement from the edge-list in a way that preserves the structure of the graph. Optimality is proven under a class of random graph models that are invariant to permutations of the edges and of vertices within an edge. Experiments indicate Random Edge Coding can achieve competitive compression performance on real-world network datasets and scales to graphs with millions of nodes and edges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#25193;&#25955;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;&#65292;&#22312;&#21160;&#24577;&#22270;&#26500;&#24314;&#19978;&#32771;&#34385;&#20102;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#22270;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09703</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#25193;&#25955;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting. (arXiv:2305.09703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#25193;&#25955;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;&#65292;&#22312;&#21160;&#24577;&#22270;&#26500;&#24314;&#19978;&#32771;&#34385;&#20102;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#22270;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#25104;&#20026;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#34429;&#28982;&#35768;&#22810;&#21160;&#24577;&#22270;&#26500;&#24314;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#65292;&#20294;&#20854;&#20013;&#30456;&#23545;&#36739;&#23569;&#30340;&#25506;&#32034;&#20102;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#32570;&#20047;&#23545;&#21160;&#24577;&#29983;&#25104;&#22270;&#30340;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#24378;&#22823;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#24456;&#23481;&#26131;&#23548;&#33268;&#21518;&#32493;&#20915;&#31574;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#20154;&#32771;&#34385;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#21160;&#24577;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#22768;&#65292;&#32780;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#32467;&#26500;&#32593;&#32476;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DVGNN&#65289;&#30340;&#26102;&#31354;&#39044;&#27979;&#26041;&#27861;&#12290;&#23545;&#20110;&#21160;&#24577;&#22270;&#26500;&#24314;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#32534;&#30721;&#22120;&#38454;&#27573;&#65292;&#24212;&#29992;&#20004;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#35745;&#31639;&#28508;&#22312;&#33410;&#28857;&#23884;&#20837;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), especially dynamic GNNs, have become a research hotspot in spatio-temporal forecasting problems. While many dynamic graph construction methods have been developed, relatively few of them explore the causal relationship between neighbour nodes. Thus, the resulting models lack strong explainability for the causal relationship between the neighbour nodes of the dynamically generated graphs, which can easily lead to a risk in subsequent decisions. Moreover, few of them consider the uncertainty and noise of dynamic graphs based on the time series datasets, which are ubiquitous in real-world graph structure networks. In this paper, we propose a novel Dynamic Diffusion-Variational Graph Neural Network (DVGNN) for spatio-temporal forecasting. For dynamic graph construction, an unsupervised generative model is devised. Two layers of graph convolutional network (GCN) are applied to calculate the posterior distribution of the latent node embeddings in the encoder sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#26684;&#39044;&#35757;&#32451;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#34920;&#26684;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#65292;&#24182;&#21487;&#20197;&#19982;&#22810;&#20010;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.09696</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#34920;&#26684;&#39044;&#35757;&#32451;&#22686;&#24378;&#20102;&#34920;&#26684;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#26684;&#39044;&#35757;&#32451;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#34920;&#26684;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#65292;&#24182;&#21487;&#20197;&#19982;&#22810;&#20010;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34920;&#26684;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#30340;&#28909;&#28857;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#34920;&#26684;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#34920;&#26684;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#34920;&#26684;&#39044;&#27979;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;&#22312;&#23545;&#22823;&#37327;&#23454;&#38469;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21518;&#65292;TapTap&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#34920;&#26684;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#24212;&#29992;&#65292;&#21253;&#25324;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#36164;&#28304;&#29615;&#22659;&#12289;&#32570;&#22833;&#20540;&#25554;&#34917;&#21644;&#22833;&#34913;&#20998;&#31867;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#21508;&#31181;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#21253;&#25324;LightGBM&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;Transformer&#12290;&#27492;&#22806;&#65292;&#22312;&#34920;&#26684;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;TapTap&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#29978;&#33267;&#21487;&#20197;&#19982;&#20351;&#29992;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#30340;&#27169;&#22411;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the or
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#35745;&#31639;&#32047;&#35745;&#36719;&#20214;&#25925;&#38556;&#27700;&#24179;&#65292;&#24182;&#25552;&#20986;&#39044;&#27979;&#27531;&#30041;&#32570;&#38519;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09695</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36719;&#20214;&#36136;&#37327;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Applying Machine Learning Analysis for Software Quality Test. (arXiv:2305.09695v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#35745;&#31639;&#32047;&#35745;&#36719;&#20214;&#25925;&#38556;&#27700;&#24179;&#65292;&#24182;&#25552;&#20986;&#39044;&#27979;&#27531;&#30041;&#32570;&#38519;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#32500;&#25252;&#26159;&#36719;&#20214;&#24320;&#21457;&#20013;&#26368;&#22823;&#30340;&#24320;&#38144;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#32500;&#25252;&#30340;&#35302;&#21457;&#22240;&#32032;&#20197;&#21450;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;&#23427;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35780;&#20272;&#31243;&#24207;&#22797;&#26434;&#24230;&#30340;&#29305;&#23450;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#30001;&#20110;&#36719;&#20214;&#25925;&#38556;&#23548;&#33268;&#32500;&#25252;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21487;&#29992;&#25968;&#25454;&#65292;&#20197;&#35745;&#31639;&#32047;&#35745;&#36719;&#20214;&#25925;&#38556;&#27700;&#24179;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#36719;&#20214;&#27531;&#30041;&#32570;&#38519;&#30340;&#25216;&#26415;&#65292;&#20316;&#20026;&#39044;&#27979;&#27531;&#30041;&#32570;&#38519;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the biggest expense in software development is the maintenance. Therefore, it is critical to comprehend what triggers maintenance and if it may be predicted. Numerous research have demonstrated that specific methods of assessing the complexity of created programs may produce useful prediction models to ascertain the possibility of maintenance due to software failures. As a routine it is performed prior to the release, and setting up the models frequently calls for certain, object-oriented software measurements. It is not always the case that software developers have access to these measurements. In this paper, the machine learning is applied on the available data to calculate the cumulative software failure levels. A technique to forecast a software`s residual defectiveness using machine learning can be looked into as a solution to the challenge of predicting residual flaws. Software metrics and defect data were separated out of the static source code repository. Static code is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#34928;&#20943;&#20989;&#25968;&#30340;&#28857;&#35843;&#25972;&#21327;&#35758;&#65288;PAdf&#65289;&#20197;&#35299;&#20915;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#35780;&#20272;&#26041;&#24335;&#39640;&#20272;&#25110;&#20302;&#20272;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26032;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;PAdf&#21327;&#35758;&#19981;&#20165;&#32771;&#34385;&#35201;&#26597;&#25214;&#23613;&#21487;&#33021;&#22810;&#30340;&#27573;&#25968;&#65292;&#36824;&#32771;&#34385;&#24555;&#36895;&#20934;&#30830;&#22320;&#26816;&#27979;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2305.09691</link><description>&lt;p&gt;
&#24102;&#34928;&#20943;&#20989;&#25968;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#35780;&#20272;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evaluation Strategy of Time-series Anomaly Detection with Decay Function. (arXiv:2305.09691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#34928;&#20943;&#20989;&#25968;&#30340;&#28857;&#35843;&#25972;&#21327;&#35758;&#65288;PAdf&#65289;&#20197;&#35299;&#20915;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#35780;&#20272;&#26041;&#24335;&#39640;&#20272;&#25110;&#20302;&#20272;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26032;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;PAdf&#21327;&#35758;&#19981;&#20165;&#32771;&#34385;&#35201;&#26597;&#25214;&#23613;&#21487;&#33021;&#22810;&#30340;&#27573;&#25968;&#65292;&#36824;&#32771;&#34385;&#24555;&#36895;&#20934;&#30830;&#22320;&#26816;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#19968;&#33324;&#37319;&#29992;&#28857;&#35843;&#25972;&#21327;&#35758;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21327;&#35758;&#23481;&#26131;&#39640;&#20272;&#26816;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#21482;&#32771;&#34385;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#27573;&#25968;&#21644;&#22823;&#23567;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#8212;&#8212;&#24102;&#34928;&#20943;&#20989;&#25968;&#30340;&#28857;&#35843;&#25972;&#21327;&#35758;&#65288;PAdf&#65289;&#65292;&#20197;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#35813;&#21327;&#35758;&#32771;&#34385;&#20102;&#24555;&#36895;&#20934;&#30830;&#22320;&#26816;&#27979;&#24322;&#24120;&#21644;&#36991;&#20813;&#35823;&#25253;&#30340;&#29702;&#24819;&#35201;&#27714;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#20004;&#20010;&#26041;&#38754;&#35777;&#26126;&#20102;PAdf&#21327;&#35758;&#35299;&#20915;&#20102;&#29616;&#26377;&#21327;&#35758;&#22914;PA&#21644;PA\%K&#31561;&#30340;&#39640;&#20272;&#21644;&#20302;&#20272;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35780;&#20272;SOTA&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;PA&#21327;&#35758;&#21482;&#32771;&#34385;&#26597;&#25214;&#23613;&#21487;&#33021;&#22810;&#30340;&#24322;&#24120;&#27573;&#65292;&#32780;PAdf&#21327;&#35758;&#21017;&#19981;&#20165;&#32771;&#34385;&#26597;&#25214;&#23613;&#21487;&#33021;&#22810;&#30340;&#27573;&#25968;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#24555;&#36895;&#26816;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent algorithms of time-series anomaly detection have been evaluated by applying a Point Adjustment (PA) protocol. However, the PA protocol has a problem of overestimating the performance of the detection algorithms because it only depends on the number of detected abnormal segments and their size. We propose a novel evaluation protocol called the Point-Adjusted protocol with decay function (PAdf) to evaluate the time-series anomaly detection algorithm by reflecting the following ideal requirements: detect anomalies quickly and accurately without false alarms. This paper theoretically and experimentally shows that the PAdf protocol solves the over- and under-estimation problems of existing protocols such as PA and PA\%K. By conducting re-evaluations of SOTA models in benchmark datasets, we show that the PA protocol only focuses on finding many anomalous segments, whereas the score of the PAdf protocol considers not only finding many segments but also detecting anomalies quickly witho
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38899;&#39057;&#25551;&#36848;&#26041;&#27861;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#20250;&#24433;&#21709;&#38899;&#39057;&#25551;&#36848;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09690</link><description>&lt;p&gt;
&#29992;&#21512;&#25104;&#23383;&#24149;&#21644;&#36801;&#31227;&#23398;&#20064;&#35757;&#32451;&#30340;&#38899;&#39057;&#25551;&#36848;&#27169;&#22411;Whisper Transformer&#30340;&#30740;&#31350;(arXiv:2305.09690v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
A Whisper transformer for audio captioning trained with synthetic captions and transfer learning. (arXiv:2305.09690v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09690
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38899;&#39057;&#25551;&#36848;&#26041;&#27861;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#20250;&#24433;&#21709;&#38899;&#39057;&#25551;&#36848;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#38899;&#39057;&#25551;&#36848;&#39046;&#22495;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#31687;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38899;&#39057;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;Whisper&#27169;&#22411;&#21644;&#29992;&#20110;&#21512;&#25104;&#23383;&#24149;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21576;&#29616;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#21253;&#25324;&#27169;&#22411;&#22823;&#23567;&#21464;&#21270;&#12289;&#25968;&#25454;&#38598;&#28151;&#21512;&#21644;&#20854;&#20182;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#20102;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#23545;&#38899;&#39057;&#25551;&#36848;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#20844;&#24320;&#22312;GitHub&#21644;Hugging Face Hub&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of audio captioning has seen significant advancements in recent years, driven by the availability of large-scale audio datasets and advancements in deep learning techniques. In this technical report, we present our approach to audio captioning, focusing on the use of a pretrained speech-to-text Whisper model and pretraining on synthetic captions. We discuss our training procedures and present our experiments' results, which include model size variations, dataset mixtures, and other hyperparameters. Our findings demonstrate the impact of different training strategies on the performance of the audio captioning model. Our code and trained models are publicly available on GitHub and Hugging Face Hub.
&lt;/p&gt;</description></item><item><title>OOD-Speech &#26159;&#29992;&#20110; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30001;&#20247;&#21253;&#25910;&#38598;&#20102;&#27597;&#35821;&#20026; Bengali &#30340; 22,645 &#21517;&#35828;&#35805;&#32773;&#24405;&#21046;&#30340; 1177.94 &#23567;&#26102;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#32463;&#36807;&#25163;&#21160;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547; 17 &#31181;&#19981;&#21516;&#30340;&#36164;&#28304;&#65292;&#22914; Bengali &#30005;&#35270;&#21095;&#12289;&#26377;&#22768;&#35835;&#29289;&#12289;&#33073;&#21475;&#31168;&#12289;&#22312;&#32447;&#25945;&#23398;&#20197;&#21450;&#20234;&#26031;&#20848;&#35762;&#36947;&#31561;&#65292;&#21487;&#20316;&#20026; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.09688</link><description>&lt;p&gt;
OOD-Speech: &#29992;&#20110; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking. (arXiv:2305.09688v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09688
&lt;/p&gt;
&lt;p&gt;
OOD-Speech &#26159;&#29992;&#20110; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30001;&#20247;&#21253;&#25910;&#38598;&#20102;&#27597;&#35821;&#20026; Bengali &#30340; 22,645 &#21517;&#35828;&#35805;&#32773;&#24405;&#21046;&#30340; 1177.94 &#23567;&#26102;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#32463;&#36807;&#25163;&#21160;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547; 17 &#31181;&#19981;&#21516;&#30340;&#36164;&#28304;&#65292;&#22914; Bengali &#30005;&#35270;&#21095;&#12289;&#26377;&#22768;&#35835;&#29289;&#12289;&#33073;&#21475;&#31168;&#12289;&#22312;&#32447;&#25945;&#23398;&#20197;&#21450;&#20234;&#26031;&#20848;&#35762;&#36947;&#31561;&#65292;&#21487;&#20316;&#20026; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; OOD-Speech&#65292;&#36825;&#26159; Bengali &#30340;&#31532;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#20840;&#29699;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#20043;&#19968;&#65292;Bengali &#23637;&#31034;&#20102;&#22823;&#37327;&#30340;&#26041;&#35328;&#21644;&#38901;&#24459;&#29305;&#24449;&#65292;&#36825;&#35201;&#27714; ASR &#26694;&#26550;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20363;&#22914;&#65292;Bengali &#20013;&#30340;&#20234;&#26031;&#20848;&#23447;&#25945;&#35762;&#36947;&#26159;&#29992;&#26126;&#26174;&#19981;&#21516;&#30340;&#35821;&#35843;&#36827;&#34892;&#30340;&#65292;&#36825;&#20063;&#25104;&#20026;&#20102;&#20998;&#24067;&#21464;&#21270;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#22312;&#32447;&#20247;&#21253;&#27963;&#21160;&#25910;&#38598;&#24182;&#31579;&#36873;&#32780;&#26469;&#65292;&#20849;&#25910;&#38598;&#20102;&#26469;&#33258;&#21335;&#20122;&#30340; 22,645 &#21517;&#27597;&#35821;&#20026; Bengali &#30340;&#35828;&#35805;&#32773;&#25152;&#24405;&#21046;&#30340; 1177.94 &#23567;&#26102;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#21017;&#21253;&#25324;&#26469;&#33258; 17 &#20010;&#19981;&#21516;&#36164;&#28304;&#65288;&#22914; Bengali &#30005;&#35270;&#21095;&#12289;&#26377;&#22768;&#35835;&#29289;&#12289;&#33073;&#21475;&#31168;&#12289;&#22312;&#32447;&#25945;&#23398;&#20197;&#21450;&#20234;&#26031;&#20848;&#35762;&#36947;&#31561;&#65289;&#30340; 23.03 &#23567;&#26102;&#35821;&#38899;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#20063;&#37117;&#32463;&#36807;&#20102;&#25163;&#21160;&#27880;&#37322;&#12290;OOD-Speech &#26082;&#26159;&#24403;&#21069;&#20844;&#24320;&#30340;&#26368;&#22823;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#31532;&#19968;&#20010;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present OOD-Speech, the first out-of-distribution (OOD) benchmarking dataset for Bengali automatic speech recognition (ASR). Being one of the most spoken languages globally, Bengali portrays large diversity in dialects and prosodic features, which demands ASR frameworks to be robust towards distribution shifts. For example, islamic religious sermons in Bengali are delivered with a tonality that is significantly different from regular speech. Our training dataset is collected via massively online crowdsourcing campaigns which resulted in 1177.94 hours collected and curated from $22,645$ native Bengali speakers from South Asia. Our test dataset comprises 23.03 hours of speech collected and manually annotated from 17 different sources, e.g., Bengali TV drama, Audiobook, Talk show, Online class, and Islamic sermons to name a few. OOD-Speech is jointly the largest publicly available speech dataset, as well as the first out-of-distribution ASR benchmarking dataset for Bengali.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35762;&#36848;&#20102;&#25968;&#25454;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>http://arxiv.org/abs/2305.09686</link><description>&lt;p&gt;
&#25968;&#25454;&#20559;&#24046;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Bias Management. (arXiv:2305.09686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35762;&#36848;&#20102;&#25968;&#25454;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20559;&#24046;&#21644;&#20844;&#24179;&#31561;&#27010;&#24565;&#22312;&#31185;&#30740;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#65292;&#26080;&#35770;&#26159;&#22312;&#20135;&#19994;&#30028;&#36824;&#26159;&#23398;&#26415;&#30028;&#20013;&#65292;&#37117;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#28304;&#20110;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25968;&#25454;&#36136;&#37327;&#19981;&#21516;&#12290;&#38543;&#30528;&#36825;&#20123;&#31995;&#32479;&#34987;&#21830;&#19994;&#21270;&#21644;&#37096;&#32626;&#65292;&#26377;&#26102;&#34987;&#22996;&#25176;&#20570;&#20986;&#25913;&#21464;&#29983;&#27963;&#30340;&#20915;&#31574;&#65292;&#20154;&#20204;&#27491;&#22312;&#20570;&#20986;&#37325;&#22823;&#21162;&#21147;&#26469;&#30830;&#23450;&#21644;&#28040;&#38500;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20559;&#24046;&#30340;&#26469;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#30740;&#31350;&#32467;&#26524;&#65292;&#23637;&#31034;&#25968;&#25454;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#29992;&#25143;&#65292;&#20559;&#24046;&#30340;&#36215;&#28304;&#20197;&#21450;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#24517;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#65292;&#32780;&#26159;&#24212;&#23558;&#30740;&#31350;&#37325;&#28857;&#36716;&#21521;&#20559;&#35265;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the widespread use of data-powered systems in our everyday lives, concepts like bias and fairness gained significant attention among researchers and practitioners, in both industry and academia. Such issues typically emerge from the data, which comes with varying levels of quality, used to train supervised machine learning systems. With the commercialization and deployment of such systems that are sometimes delegated to make life-changing decisions, significant efforts are being made towards the identification and removal of possible sources of data bias that may resurface to the final end user or in the decisions being made. In this paper, we present research results that show how bias in data affects end users, where bias is originated, and provide a viewpoint about what we should do about it. We argue that data bias is not something that should necessarily be removed in all cases, and that research attention should instead shift from bias removal towards the identification, m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;'ICS-Flow'&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35780;&#20272;&#25152;&#38656;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#36807;&#31243;&#29366;&#24577;&#21464;&#37327;&#26085;&#24535;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#31639;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#30784;&#24615;&#33021;&#35780;&#20272;&#65292;&#26088;&#22312;&#20419;&#36827;&#24037;&#25511;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#21457;&#23637;&#21644;&#21152;&#24378;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#30340;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09678</link><description>&lt;p&gt;
&#24037;&#25511;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection Dataset for Industrial Control Systems. (arXiv:2305.09678v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;'ICS-Flow'&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35780;&#20272;&#25152;&#38656;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#36807;&#31243;&#29366;&#24577;&#21464;&#37327;&#26085;&#24535;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#31639;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#30784;&#24615;&#33021;&#35780;&#20272;&#65292;&#26088;&#22312;&#20419;&#36827;&#24037;&#25511;&#31995;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#21457;&#23637;&#21644;&#21152;&#24378;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#30340;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#24037;&#25511;&#31995;&#32479;(ICSS)&#24050;&#32463;&#25104;&#20026;&#32593;&#32476;&#25915;&#20987;&#30340;&#30446;&#26631;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;ICS&#19982;&#20114;&#32852;&#32593;&#30456;&#36830;&#65292;&#23433;&#20840;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#33030;&#24369;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#36827;&#34892;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(IDS)&#26159;&#20445;&#25252;ICS&#32593;&#32476;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#36866;&#21512;&#35780;&#20272;ML&#31639;&#27861;&#30340;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;'ICS-Flow'&#25968;&#25454;&#38598;&#65292;&#20026;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;ML&#30340;IDS&#35780;&#20272;&#25552;&#20379;&#20102;&#32593;&#32476;&#25968;&#25454;&#21644;&#36807;&#31243;&#29366;&#24577;&#21464;&#37327;&#26085;&#24535;&#12290;&#32593;&#32476;&#25968;&#25454;&#21253;&#25324;&#20174;&#27169;&#25311;&#30340;ICS&#32452;&#20214;&#21644;&#20223;&#30495;&#32593;&#32476;&#20013;&#25429;&#33719;&#30340;&#27491;&#24120;&#21644;&#24322;&#24120;&#32593;&#32476;&#25968;&#25454;&#21253;&#21644;&#27969;&#12290;&#24322;&#24120;&#26159;&#36890;&#36807;&#40657;&#23458;&#24120;&#29992;&#30340;&#21508;&#31181;&#25915;&#20987;&#25216;&#26415;&#27880;&#20837;&#21040;&#31995;&#32479;&#20013;&#30340;&#65292;&#29992;&#20110;&#20462;&#25913;&#32593;&#32476;&#27969;&#37327;&#21644;&#25915;&#20987;ICS&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;ML&#31639;&#27861;&#22312;ICS-Flow&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#30784;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#19982;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;IDS&#36827;&#34892;&#27604;&#36739;&#12290;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#32467;&#26524;&#23558;&#26377;&#21161;&#20110;&#24320;&#21457;&#20808;&#36827;&#30340;ICS&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#21644;&#25552;&#39640;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#30340;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few decades, Industrial Control Systems (ICSs) have been targeted by cyberattacks and are becoming increasingly vulnerable as more ICSs are connected to the internet. Using Machine Learning (ML) for Intrusion Detection Systems (IDS) is a promising approach for ICS cyber protection, but the lack of suitable datasets for evaluating ML algorithms is a challenge. Although there are a few commonly used datasets, they may not reflect realistic ICS network data, lack necessary features for effective anomaly detection, or be outdated. This paper presents the 'ICS-Flow' dataset, which offers network data and process state variables logs for supervised and unsupervised ML-based IDS assessment. The network data includes normal and anomalous network packets and flows captured from simulated ICS components and emulated networks. The anomalies were injected into the system through various attack techniques commonly used by hackers to modify network traffic and compromise ICSs. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#65292;&#20854;&#21487;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#21508;&#31181;&#31867;&#22411;&#30340;&#28431;&#27934;&#26041;&#38754;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20256;&#32479;SAST&#21644;DAST&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09673</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vulnerability Detection Using Two-Stage Deep Learning Models. (arXiv:2305.09673v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#65292;&#20854;&#21487;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#21508;&#31181;&#31867;&#22411;&#30340;&#28431;&#27934;&#26041;&#38754;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20256;&#32479;SAST&#21644;DAST&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#31243;&#24207;&#23433;&#20840;&#26159;&#29616;&#20195;&#36719;&#20214;&#24320;&#21457;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#35768;&#22810;&#25915;&#20987;&#21462;&#20915;&#20110;&#36719;&#20214;&#20013;&#30340;&#28431;&#27934;&#12290;&#30001;&#20110;&#25216;&#26415;&#36827;&#27493;&#65292;&#25915;&#20987;&#25968;&#37327;&#27491;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#22686;&#21152;&#12290;&#20844;&#21496;&#24517;&#39035;&#22312;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#36719;&#20214;&#30340;&#27599;&#20010;&#38454;&#27573;&#20013;&#37117;&#21253;&#21547;&#23433;&#20840;&#21151;&#33021;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#12290;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#26041;&#27861;&#26377;&#35768;&#22810;&#31181;&#65292;&#22914;&#38750;AI&#26041;&#27861;(&#22914;SAST&#21644;DAST)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22823;&#37327;&#30340;&#35823;&#25253;&#21644;&#28431;&#25253;&#12290;&#19982;&#27492;&#30456;&#23545;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#33268;&#21147;&#20110;&#24320;&#21457;&#22522;&#20110;AI&#30340;&#28431;&#27934;&#26816;&#27979;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;BERT&#12289;BLSTM&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;C/C++&#28304;&#20195;&#30721;&#30340;&#28431;&#27934;&#26816;&#27979;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;CNN&#65292;&#29992;&#20110;&#26816;&#27979;&#28304;&#20195;&#30721;&#26159;&#21542;&#21253;&#21547;&#20219;&#20309;&#28431;&#27934;(&#20108;&#20803;&#20998;&#31867;)&#65292;&#31532;&#20108;&#38454;&#27573;&#26159;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;&#31867;&#22411;&#30340;&#28431;&#27934;(&#22810;&#31867;&#20998;&#31867;)&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#21508;&#31181;&#31867;&#22411;&#30340;&#28431;&#27934;&#26041;&#38754;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#29575;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;SAST&#21644;DAST&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application security is an essential part of developing modern software, as lots of attacks depend on vulnerabilities in software. The number of attacks is increasing globally due to technological advancements. Companies must include security in every stage of developing, testing, and deploying their software in order to prevent data breaches. There are several methods to detect software vulnerability Non-AI-based such as Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). However, these approaches have substantial false-positive and false-negative rates. On the other side, researchers have been interested in developing an AI-based vulnerability detection system employing deep learning models like BERT, BLSTM, etc. In this paper, we proposed a two-stage solution, two deep learning models were proposed for vulnerability detection in C/C++ source codes, the first stage is CNN which detects if the source code contains any vulnerability (binary class
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23384;&#22312;&#26816;&#27979;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#20105;&#65306;&#27745;&#26579;&#22826;&#23569;&#23548;&#33268;&#25915;&#20987;&#22833;&#25928;&#65292;&#27745;&#26579;&#22826;&#22810;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20004;&#31181;&#38450;&#24481;&#25514;&#26045;&#65292;&#23545;&#26377;&#38480;&#30340;&#20449;&#20219;&#22270;&#20687;&#26631;&#31614;&#23545;&#36827;&#34892;&#21518;&#22788;&#29702;&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;&#34987;&#27745;&#26579;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09671</link><description>&lt;p&gt;
&#36873;&#25321;&#20320;&#30340;&#27602;&#33647;&#65306;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#20013;&#30340;&#26816;&#27979;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#20105;
&lt;/p&gt;
&lt;p&gt;
Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification. (arXiv:2305.09671v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09671
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23384;&#22312;&#26816;&#27979;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#20105;&#65306;&#27745;&#26579;&#22826;&#23569;&#23548;&#33268;&#25915;&#20987;&#22833;&#25928;&#65292;&#27745;&#26579;&#22826;&#22810;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20004;&#31181;&#38450;&#24481;&#25514;&#26045;&#65292;&#23545;&#26377;&#38480;&#30340;&#20449;&#20219;&#22270;&#20687;&#26631;&#31614;&#23545;&#36827;&#34892;&#21518;&#22788;&#29702;&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;&#34987;&#27745;&#26579;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#26263;&#34255;&#21518;&#38376;&#30340;&#26426;&#21046;&#12290;&#21363;&#20351;&#22521;&#35757;&#36807;&#31243;&#20013;&#21482;&#26377;&#23569;&#37327;&#27745;&#26579;&#26679;&#26412;&#65292;&#20063;&#36275;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;&#34429;&#28982;&#24050;&#30693;&#27745;&#26579;&#26356;&#22810;&#30340;&#26679;&#26412;&#21487;&#20197;&#22686;&#24378;&#25915;&#20987;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#27745;&#26579;&#22826;&#22810;&#26679;&#26412;&#26159;&#21542;&#20250;&#20351;&#25915;&#20987;&#21464;&#24471;&#26356;&#26131;&#34987;&#26816;&#27979;&#21040;&#20174;&#32780;&#21066;&#24369;&#25915;&#20987;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#20013;&#23384;&#22312;&#19968;&#20010;&#22522;&#26412;&#30340;&#26816;&#27979;&#24615;/&#40065;&#26834;&#24615;&#26435;&#34913;&#65306;&#27745;&#26579;&#22826;&#23569;&#30340;&#26679;&#26412;&#20250;&#23548;&#33268;&#25915;&#20987;&#22833;&#25928;&#21644;&#19981;&#40065;&#26834;&#65292;&#20294;&#27745;&#26579;&#22826;&#22810;&#30340;&#26679;&#26412;&#21017;&#20250;&#20351;&#25915;&#20987;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#36825;&#25552;&#39640;&#20102;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#32773;&#30340;&#38376;&#27099;&#65292;&#20182;&#20204;&#24517;&#39035;&#26435;&#34913;&#36825;&#31181;&#26435;&#34913;&#20197;&#20445;&#25345;&#40065;&#26834;&#21644;&#19981;&#26131;&#34987;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#20449;&#20219;&#22270;&#20687;&#26631;&#31614;&#23545;&#20316;&#20026;&#22521;&#35757;&#21518;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;&#34987;&#27745;&#26579;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38450;&#24481;&#25514;&#26045;&#21487;&#20197;&#20943;&#36731;&#22823;&#37327;&#27745;&#26579;&#25915;&#20987;&#65292;&#21516;&#26102;&#23545;&#36867;&#36991;&#23581;&#35797;&#20445;&#25345;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep image classification models trained on large amounts of web-scraped data are vulnerable to data poisoning, a mechanism for backdooring models. Even a few poisoned samples seen during training can entirely undermine the model's integrity during inference. While it is known that poisoning more samples enhances an attack's effectiveness and robustness, it is unknown whether poisoning too many samples weakens an attack by making it more detectable. We observe a fundamental detectability/robustness trade-off in data poisoning attacks: Poisoning too few samples renders an attack ineffective and not robust, but poisoning too many samples makes it detectable. This raises the bar for data poisoning attackers who have to balance this trade-off to remain robust and undetectable. Our work proposes two defenses designed to (i) detect and (ii) repair poisoned models as a post-processing step after training using a limited amount of trusted image-label pairs. We show that our defenses mitigate a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#38544;&#31169;&#35201;&#27714;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20004;&#32452;&#20855;&#26377;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#30340;&#29992;&#25143;&#26102;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#30340;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39281;&#21644;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.09668</link><description>&lt;p&gt;
&#24322;&#26500;&#38544;&#31169;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;: &#37096;&#20998;&#38544;&#31169;&#26159;&#21487;&#20197;&#20813;&#36153;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mean Estimation Under Heterogeneous Privacy: Some Privacy Can Be Free. (arXiv:2305.09668v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#38544;&#31169;&#35201;&#27714;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20004;&#32452;&#20855;&#26377;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#30340;&#29992;&#25143;&#26102;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#30340;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39281;&#21644;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169; (DP) &#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#36816;&#29992;&#29992;&#20110;&#34913;&#37327;&#31639;&#27861;&#38544;&#31169;&#25439;&#22833;&#30340;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;DP&#24418;&#24335;&#23545;&#25152;&#26377;&#29992;&#25143;&#24378;&#21046;&#26045;&#21152;&#19968;&#33268;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#36825;&#19982;&#29616;&#23454;&#22330;&#26223;&#36890;&#24120;&#19981;&#19968;&#33268;&#65292;&#22240;&#20026;&#29992;&#25143;&#20010;&#20307;&#20915;&#23450;&#20182;&#20204;&#30340;&#38544;&#31169;&#20559;&#22909;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24322;&#26500;DP&#32422;&#26463;&#19979;&#30340;&#24179;&#22343;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#26045;&#21152;&#33258;&#24049;&#29420;&#29305;&#30340;&#38544;&#31169;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20004;&#32452;&#20855;&#26377;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#30340;&#29992;&#25143;&#26102;&#34987;&#35777;&#26126;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39281;&#21644;&#29616;&#35937;&#65292;&#21363;&#22312;&#19968;&#32452;&#29992;&#25143;&#30340;&#38544;&#31169;&#27700;&#24179;&#34987;&#25918;&#23485;&#32780;&#21478;&#19968;&#32452;&#29992;&#25143;&#30340;&#38544;&#31169;&#27700;&#24179;&#20445;&#25345;&#19981;&#21464;&#26102;&#21457;&#29983;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#26576;&#20010;&#29305;&#23450;&#24773;&#24418;&#19979;&#65292;&#36827;&#19968;&#27493;&#25918;&#23485;&#21069;&#19968;&#32452;&#30340;&#38544;&#31169;&#35201;&#27714;&#24182;&#19981;&#20250;&#25913;&#21892;&#26368;&#23567;&#20108;&#20056;&#24179;&#22343;&#25968;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#20013;&#22830;&#26381;&#21153;&#22120;&#21487;&#20197;&#25552;&#20379;&#19968;&#23450;&#31243;&#24230;&#30340;&#38544;&#31169;&#32780;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy (DP) is a well-established framework to quantify privacy loss incurred by any algorithm. Traditional DP formulations impose a uniform privacy requirement for all users, which is often inconsistent with real-world scenarios in which users dictate their privacy preferences individually. This work considers the problem of mean estimation under heterogeneous DP constraints, where each user can impose their own distinct privacy level. The algorithm we propose is shown to be minimax optimal when there are two groups of users with distinct privacy levels. Our results elicit an interesting saturation phenomenon that occurs as one group's privacy level is relaxed, while the other group's privacy level remains constant. Namely, after a certain point, further relaxing the privacy requirement of the former group does not improve the performance of the minimax optimal mean estimator. Thus, the central server can offer a certain degree of privacy without any sacrifice in perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;SHAP&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#27169;&#31946;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#65292;&#23545;&#38544;&#24615;&#20559;&#35265;&#36827;&#34892;&#27979;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#29305;&#24449;&#37325;&#35201;&#24615;&#20316;&#20026;&#32477;&#23545;&#24037;&#20855;&#19981;&#36866;&#24212;&#20110;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;&#65292;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20559;&#35265;&#25968;&#37327;&#21487;&#33021;&#22240;&#29305;&#24449;&#26159;&#25968;&#20540;&#32534;&#30721;&#36824;&#26159;&#20998;&#31867;&#32534;&#30721;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.09399</link><description>&lt;p&gt;
&#20351;&#29992;SHAP&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#27169;&#31946;&#35748;&#30693;&#22320;&#22270;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Measuring Implicit Bias Using SHAP Feature Importance and Fuzzy Cognitive Maps. (arXiv:2305.09399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;SHAP&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#27169;&#31946;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#65292;&#23545;&#38544;&#24615;&#20559;&#35265;&#36827;&#34892;&#27979;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#29305;&#24449;&#37325;&#35201;&#24615;&#20316;&#20026;&#32477;&#23545;&#24037;&#20855;&#19981;&#36866;&#24212;&#20110;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;&#65292;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20559;&#35265;&#25968;&#37327;&#21487;&#33021;&#22240;&#29305;&#24449;&#26159;&#25968;&#20540;&#32534;&#30721;&#36824;&#26159;&#20998;&#31867;&#32534;&#30721;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29305;&#24449;&#37325;&#35201;&#24615;&#27010;&#24565;&#19982;&#27169;&#24335;&#20998;&#31867;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#19977;&#27493;&#26041;&#27861;&#23454;&#29616;&#65306;&#65288;i&#65289;&#26500;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#65292;&#65288;ii&#65289;&#26500;&#24314;&#19968;&#20010;&#33021;&#37327;&#21270;&#38544;&#24615;&#20559;&#35265;&#30340;&#27169;&#31946;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#65292;&#65288;iii&#65289;&#20351;&#29992;SHAP&#29305;&#24449;&#37325;&#35201;&#24615;&#22312;&#27169;&#25311;&#20013;&#28608;&#27963;&#31070;&#32463;&#20803;&#27010;&#24565;&#12290;&#20197;&#20851;&#20110;&#20844;&#24179;&#30740;&#31350;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#21452;&#37325;&#20551;&#35774;&#12290;&#19968;&#26041;&#38754;&#65292;&#38416;&#26126;&#20102;&#20351;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#20316;&#20026;&#32477;&#23545;&#24037;&#20855;&#26469;&#34913;&#37327;&#38544;&#24615;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24471;&#20986;&#32467;&#35770;&#65306;&#23545;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20559;&#35265;&#25968;&#37327;&#21487;&#33021;&#22240;&#29305;&#24449;&#26159;&#25968;&#20540;&#32534;&#30721;&#36824;&#26159;&#20998;&#31867;&#32534;&#30721;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we integrate the concepts of feature importance with implicit bias in the context of pattern classification. This is done by means of a three-step methodology that involves (i) building a classifier and tuning its hyperparameters, (ii) building a Fuzzy Cognitive Map model able to quantify implicit bias, and (iii) using the SHAP feature importance to active the neural concepts when performing simulations. The results using a real case study concerning fairness research support our two-fold hypothesis. On the one hand, it is illustrated the risks of using a feature importance method as an absolute tool to measure implicit bias. On the other hand, it is concluded that the amount of bias towards protected features might differ depending on whether the features are numerically or categorically encoded.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21322;&#24377;&#24615;&#32442;&#32455;&#21697;&#34920;&#38754;&#19978;&#36793;&#32536;&#30340;&#20256;&#24863;&#22120;&#36827;&#34892;&#25509;&#35302;&#24863;&#27979;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#22312;&#24863;&#27979;&#21306;&#22495;&#25918;&#32622;&#39069;&#22806;&#20256;&#24863;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#22312;&#21487;&#31359;&#25140;&#25216;&#26415;&#21644;&#26234;&#33021;&#32442;&#32455;&#21697;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#33021;&#22815;&#20197;82.85%&#30340;&#20934;&#30830;&#24230;&#20998;&#31867;&#35782;&#21035;&#19977;&#20010;&#21387;&#21147;&#27700;&#24179;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.09222</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#20256;&#24863;&#22120;&#30340;&#21322;&#24377;&#24615;&#32442;&#32455;&#21697;&#35302;&#25720;&#24863;&#27979;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Touch Sensing on Semi-Elastic Textiles with Border-Based Sensors. (arXiv:2305.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21322;&#24377;&#24615;&#32442;&#32455;&#21697;&#34920;&#38754;&#19978;&#36793;&#32536;&#30340;&#20256;&#24863;&#22120;&#36827;&#34892;&#25509;&#35302;&#24863;&#27979;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#22312;&#24863;&#27979;&#21306;&#22495;&#25918;&#32622;&#39069;&#22806;&#20256;&#24863;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#22312;&#21487;&#31359;&#25140;&#25216;&#26415;&#21644;&#26234;&#33021;&#32442;&#32455;&#21697;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#33021;&#22815;&#20197;82.85%&#30340;&#20934;&#30830;&#24230;&#20998;&#31867;&#35782;&#21035;&#19977;&#20010;&#21387;&#21147;&#27700;&#24179;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25509;&#35302;&#24863;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#21322;&#24377;&#24615;&#32442;&#32455;&#21697;&#34920;&#38754;&#19978;&#36793;&#32536;&#30340;&#20256;&#24863;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#25509;&#35302;&#21306;&#22495;&#25918;&#32622;&#39069;&#22806;&#30340;&#20256;&#24863;&#22120;&#12290;&#36890;&#36807;&#22312;&#24377;&#24615;&#36816;&#21160;&#32455;&#29289;&#19978;&#36827;&#34892;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#30340;&#20256;&#24863;&#22120;&#35774;&#35745;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#26631;&#35760;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;&#35270;&#35273;&#20256;&#24863;&#22120;&#39044;&#27979;125mm&#215;125mm&#21306;&#22495;&#19978;&#19968;&#20010;&#28857;&#30340;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#20026;1.36mm&#12290;&#25105;&#20204;&#21046;&#20316;&#20102;&#19968;&#31181;&#20165;&#29992;&#32442;&#32455;&#21697;&#23454;&#29616;&#30340;&#21407;&#22411;&#65292;&#33021;&#22815;&#20197;82.85%&#30340;&#20934;&#30830;&#24230;&#20998;&#31867;&#35782;&#21035;&#19977;&#20010;&#21387;&#21147;&#27700;&#24179;&#65288;0&#12289;15&#21644;20mm&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#31359;&#25140;&#25216;&#26415;&#21644;&#26234;&#33021;&#32442;&#32455;&#21697;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#32034;&#36825;&#20123;&#39046;&#22495;&#65292;&#36825;&#23558;&#26159;&#19968;&#20010;&#20805;&#28385;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach for touch sensing using semi-elastic textile surfaces that does not require the placement of additional sensors in the sensing area, instead relying on sensors located on the border of the textile. The proposed approach is demonstrated through experiments involving an elastic Jersey fabric and a variety of machine-learning models. The performance of one particular border-based sensor design is evaluated in depth. By using visual markers, the best-performing visual sensor arrangement predicts a single touch point with a mean squared error of 1.36 mm on an area of 125mm by 125mm. We built a textile only prototype that is able to classify touch at three indent levels (0, 15, and 20 mm) with an accuracy of 82.85%. Our results suggest that this approach has potential applications in wearable technology and smart textiles, making it a promising avenue for further exploration in these fields.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HumanMotionQA&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;NSPose&#26041;&#27861;&#65292;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#24182;&#36229;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08953</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22359;&#21270;&#21160;&#20316;&#31243;&#24207;&#30340;&#21160;&#20316;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Motion Question Answering via Modular Motion Programs. (arXiv:2305.08953v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08953
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HumanMotionQA&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;NSPose&#26041;&#27861;&#65292;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#24182;&#36229;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#24863;&#30693;&#21644;&#29702;&#35299;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#39318;&#20808;&#35774;&#35745;&#27169;&#22411;&#65292;&#23545;&#21160;&#20316;&#24207;&#21015;&#36827;&#34892;&#22797;&#26434;&#30340;&#26102;&#31354;&#25512;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HumanMotionQA&#20219;&#21153;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#38271;&#26102;&#38388;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#22312;&#21160;&#20316;&#24207;&#21015;&#30340;&#23567;&#37096;&#20998;&#20013;&#26816;&#27979;&#36816;&#21160;&#32447;&#32034;&#65292;&#23545;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#26597;&#35810;&#29305;&#23450;&#30340;&#36816;&#21160;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;NSPose&#65292;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35813;&#20219;&#21153;&#65292;&#23427;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27010;&#24565;&#12289;&#23646;&#24615;&#31070;&#32463;&#25805;&#20316;&#31526;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#26469;&#22788;&#29702;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;NSPose&#22312;HumanMotionQA&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#32988;&#36807;&#20102;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.08876</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#20998;&#31867;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI and its Taxonomy: a survey. (arXiv:2305.08876v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#32452;&#21512;&#31526;&#21495;&#22788;&#29702;&#65288;&#22914;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#25104;&#29087;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#19968;&#31181;&#23581;&#35797;&#65292;&#22312;&#25506;&#32034;&#38500;&#20102;&#22686;&#21152;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#23610;&#23544;&#20197;&#22806;&#30340;&#26367;&#20195;&#26041;&#26696;&#20197;&#21450;&#23558;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20316;&#29992;&#12290;&#26412;&#27425;&#35843;&#26597;&#30740;&#31350;&#20102;&#36825;&#19968;&#39046;&#22495;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#33337;&#33334;&#35774;&#35745;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;Ship-D&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#19975;&#33368;&#33337;&#33334;&#22806;&#24418;&#20449;&#24687;&#12289;&#35774;&#35745;&#21442;&#25968;&#12289;&#32593;&#26684;&#34920;&#31034;&#12289;&#28857;&#20113;&#25968;&#25454;&#12289;&#22270;&#20687;&#34920;&#31034;&#20197;&#21450;&#19977;&#21313;&#20108;&#31181;&#27700;&#21160;&#21147;&#38459;&#21147;&#25968;&#25454;&#65292;&#24182;&#21487;&#25903;&#25345;&#20154;&#31867;&#21644;&#35745;&#31639;&#26041;&#27861;&#36827;&#34892;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.08279</link><description>&lt;p&gt;
&#29992;&#20110;&#33337;&#33334;&#35774;&#35745;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#65306;Ship-D
&lt;/p&gt;
&lt;p&gt;
Ship-D: Ship Hull Dataset for Design Optimization using Machine Learning. (arXiv:2305.08279v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#33337;&#33334;&#35774;&#35745;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;Ship-D&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#19975;&#33368;&#33337;&#33334;&#22806;&#24418;&#20449;&#24687;&#12289;&#35774;&#35745;&#21442;&#25968;&#12289;&#32593;&#26684;&#34920;&#31034;&#12289;&#28857;&#20113;&#25968;&#25454;&#12289;&#22270;&#20687;&#34920;&#31034;&#20197;&#21450;&#19977;&#21313;&#20108;&#31181;&#27700;&#21160;&#21147;&#38459;&#21147;&#25968;&#25454;&#65292;&#24182;&#21487;&#25903;&#25345;&#20154;&#31867;&#21644;&#35745;&#31639;&#26041;&#27861;&#36827;&#34892;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#20943;&#23569;&#22797;&#26434;&#20135;&#21697;&#35774;&#35745;&#21608;&#26399;&#26041;&#38754;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;&#30446;&#21069;&#33337;&#33334;&#35774;&#35745;&#38656;&#35201;&#25968;&#24180;&#26102;&#38388;&#65292;&#19988;&#29983;&#20135;&#35268;&#27169;&#24456;&#23567;&#65292;&#22240;&#27492;&#21487;&#20197;&#20174;&#36825;&#20123;&#36827;&#23637;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33337;&#33334;&#35774;&#35745;&#24037;&#20855;&#65292;&#23398;&#20064;&#19981;&#21516;&#31867;&#22411;&#30340;&#33337;&#33334;&#35774;&#35745;&#65292;&#20174;&#32780;&#35782;&#21035;&#21644;&#20248;&#21270;&#33337;&#33334;&#35774;&#35745;&#20013;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#20844;&#24320;&#30340;&#33337;&#33334;&#35774;&#35745;&#25968;&#25454;&#38598;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#36890;&#29992;&#33337;&#33334;&#35774;&#35745;&#20013;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#19975;&#33368;&#33337;&#33334;&#22806;&#24418;&#20449;&#24687;&#12289;&#35774;&#35745;&#21442;&#25968;&#12289;&#32593;&#26684;&#34920;&#31034;&#12289;&#28857;&#20113;&#25968;&#25454;&#12289;&#22270;&#20687;&#34920;&#31034;&#20197;&#21450;&#22312;&#19981;&#21516;&#36816;&#34892;&#26465;&#20214;&#19979;&#30340;&#19977;&#21313;&#20108;&#31181;&#27700;&#21160;&#21147;&#38459;&#21147;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;--Ship-D&#65292;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#20026;&#20154;&#31867;&#21644;&#35745;&#31639;&#26041;&#27861;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has recently made significant strides in reducing design cycle time for complex products. Ship design, which currently involves years long cycles and small batch production, could greatly benefit from these advancements. By developing a machine learning tool for ship design that learns from the design of many different types of ships, tradeoffs in ship design could be identified and optimized. However, the lack of publicly available ship design datasets currently limits the potential for leveraging machine learning in generalized ship design. To address this gap, this paper presents a large dataset of thirty thousand ship hulls, each with design and functional performance information, including parameterization, mesh, point cloud, and image representations, as well as thirty two hydrodynamic drag measures under different operating conditions. The dataset is structured to allow human input and is also designed for computational methods. Additionally, the paper introduce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.07961</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#21551;&#29992;&#23454;&#26102;&#30340;&#22810;&#36718;&#23545;&#35805;&#20351;&#29992;&#25143;&#26356;&#21152;&#36879;&#26126;&#21644;&#25484;&#25511;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#23545;&#35805;&#33258;&#28982;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#19990;&#30028;&#30693;&#35782;&#21644;&#24120;&#35782;&#25512;&#29702;&#34701;&#20837;&#21040;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#21253;&#25324;&#36866;&#24403;&#22320;&#29702;&#35299;&#21644;&#25511;&#21046;&#22797;&#26434;&#30340;&#23545;&#35805;&#21644;&#20174;&#22806;&#37096;&#20449;&#24687;&#28304;&#26816;&#32034;&#12290;&#30001;&#20110;&#22823;&#32780;&#19981;&#26029;&#22686;&#38271;&#30340;&#39033;&#30446;&#35821;&#26009;&#24211;&#21644;&#32570;&#20047;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#38382;&#39064;&#21152;&#21095;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#25143;&#20559;&#22909;&#29702;&#35299;&#12289;&#28789;&#27963;&#30340;&#23545;&#35805;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#20316;&#20026;&#25972;&#20010;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#30340;&#26032;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Nazr&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36830;&#32493;&#30417;&#27979;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.07772</link><description>&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30417;&#27979;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Monitoring and Adapting ML Models on Mobile Devices. (arXiv:2305.07772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Nazr&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36830;&#32493;&#30417;&#27979;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20302;&#24310;&#36831;&#25512;&#29702;&#21644;&#31163;&#32447;&#25805;&#20316;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#21040;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#37096;&#32626;&#20102;&#27169;&#22411;&#65292;&#36816;&#33829;&#32773;&#38590;&#20197;&#36861;&#36394;&#20854;&#31934;&#30830;&#24230;&#65292;&#21487;&#33021;&#20250;&#22240;&#20026;&#25968;&#25454;&#28418;&#31227;&#31561;&#38382;&#39064;&#32780;&#19981;&#21487;&#39044;&#27979;&#22320;&#38477;&#20302;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;Nazr&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36830;&#32493;&#30417;&#27979;&#21644;&#35843;&#25972;&#27169;&#22411;&#65292;&#26080;&#38656;&#29992;&#25143;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#27169;&#22411;&#36864;&#21270;&#36890;&#24120;&#26159;&#30001;&#29305;&#23450;&#30340;&#26681;&#26412;&#21407;&#22240;&#36896;&#25104;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#22823;&#37327;&#35774;&#22791;&#12290;&#22240;&#27492;&#65292;&#19968;&#26086;Nazr&#26816;&#27979;&#21040;&#22823;&#37327;&#35774;&#22791;&#19978;&#30340;&#19968;&#33268;&#24615;&#36864;&#21270;&#65292;&#23427;&#23601;&#20250;&#37319;&#29992;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#26469;&#30830;&#23450;&#38382;&#39064;&#30340;&#36215;&#28304;&#65292;&#24182;&#24212;&#29992;&#29305;&#23450;&#20110;&#21407;&#22240;&#30340;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Nazr&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#20174;&#39550;&#39542;&#27773;&#36710;&#20013;&#25910;&#38598;&#30340;&#29031;&#29255;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;Nazr&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML models are increasingly being pushed to mobile devices, for low-latency inference and offline operation. However, once the models are deployed, it is hard for ML operators to track their accuracy, which can degrade unpredictably (e.g., due to data drift). We design Nazar, the first end-to-end system for continuously monitoring and adapting models on mobile devices without requiring feedback from users. Our key observation is that often model degradation is due to a specific root cause, which may affect a large group of devices. Therefore, once Nazar detects a consistent degradation across a large number of devices, it employs a root cause analysis to determine the origin of the problem and applies a cause-specific adaptation. We evaluate Nazar on two computer vision datasets, and show it consistently boosts accuracy compared to existing approaches. On a dataset containing photos collected from driving cars, Nazar improves the accuracy on average by 15%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2305.07437</link><description>&lt;p&gt;
&#24102;&#26377;&#38750;&#23545;&#35282;&#20449;&#24687;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#36861;&#36394;&#36830;&#32493;&#26356;&#26032;&#30340;CLIP&#27169;&#22411;&#20013;&#34920;&#31034;&#21521;&#37327;&#30340;&#26041;&#21521;&#21464;&#21270;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#24635;&#32467;&#20102;&#36825;&#20123;&#31354;&#38388;&#21464;&#21270;&#65292;&#31216;&#20026;&#31354;&#38388;&#28151;&#20081;&#65288;SD&#65289;&#65292;&#21487;&#20197;&#20998;&#20026;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#22914;&#20309;&#23548;&#33268;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#31354;&#38388;&#28151;&#20081;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X: &#32500;&#25252;&#38750;&#23545;&#35282;&#20449;&#24687;&#30697;&#38453;&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#35268;&#27169;&#21644;&#33539;&#22260;&#30340;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#32452;&#20214;&#34701;&#20837;&#39044;&#27979;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22330;&#26469;&#34920;&#31034;&#36741;&#21161;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22312;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#26102;&#23384;&#22312;&#30340;&#19968;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06827</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#31070;&#32463;&#22330;&#22312;&#26102;&#31354;&#39044;&#27979;&#20013;&#23558;&#26102;&#38388;&#34701;&#20837;&#21040;&#36890;&#29992;&#26041;&#27861;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Generic Approach to Integrating Time into Spatial-Temporal Forecasting via Conditional Neural Fields. (arXiv:2305.06827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#32452;&#20214;&#34701;&#20837;&#39044;&#27979;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22330;&#26469;&#34920;&#31034;&#36741;&#21161;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22312;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#26102;&#23384;&#22312;&#30340;&#19968;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#24847;&#35782;&#26159;&#33258;&#20027;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#20363;&#22914;&#33258;&#20027;&#39550;&#39542;&#32593;&#32476;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#25512;&#26029;&#29615;&#22659;&#30340;&#26410;&#26469;&#29366;&#24577;&#20197;&#21450;&#20854;&#38543;&#26102;&#38388;&#25512;&#31227;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#22823;&#37327;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#31639;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20197;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#32479;&#35745;&#26041;&#27861;&#19978;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#20294;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#34920;&#31034;&#23395;&#33410;&#24615;&#27169;&#24335;&#30340;&#20840;&#23616;&#20449;&#24687;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#32452;&#20214;&#25972;&#21512;&#21040;&#39044;&#27979;&#27169;&#22411;&#20013;&#20197;&#25552;&#39640;&#20854;&#31934;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#32452;&#20214;&#34701;&#20837;&#39044;&#27979;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22330;&#26469;&#34920;&#31034;&#36741;&#21161;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-awareness is the key capability of autonomous systems, e.g., autonomous driving network, which relies on highly efficient time series forecasting algorithm to enable the system to reason about the future state of the environment, as well as its effect on the system behavior as time progresses. Recently, a large number of forecasting algorithms using either convolutional neural networks or graph neural networks have been developed to exploit the complex temporal and spatial dependencies present in the time series. While these solutions have shown significant advantages over statistical approaches, one open question is to effectively incorporate the global information which represents the seasonality patterns via the time component of time series into the forecasting models to improve their accuracy. This paper presents a general approach to integrating the time component into forecasting models. The main idea is to employ conditional neural fields to represent the auxiliary feature
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06137</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#31561;&#25928;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;WIRL&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#19982;&#25237;&#24433;&#27425;&#26799;&#24230;&#27861;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;WIRL&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;&#26368;&#22823;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#23548;&#24341;&#25104;&#26412;&#23398;&#20064;&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;</title><link>http://arxiv.org/abs/2305.05465</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#21160;&#24577;&#20013;&#30340;&#32858;&#31867;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;Transformer&#35270;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24403;&#26435;&#37325;&#19981;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#34920;token&#30340;&#31890;&#23376;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#32780;&#36235;&#21521;&#20110;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#12290;&#20986;&#29616;&#30340;&#26497;&#38480;&#23545;&#35937;&#31867;&#22411;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#30697;&#38453;&#25910;&#25947;&#20110;&#20302;&#31209;&#24067;&#23572;&#30697;&#38453;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#32452;&#21512;&#22312;&#25968;&#23398;&#19978;&#35777;&#23454;&#20102;Vaswani&#31561;&#20154;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#20250;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;Deep Evidence Regression&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#20449;&#29992;&#39118;&#38505;&#20013;&#30340;&#36829;&#32422;&#25439;&#22833;&#65307;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.04967</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#31649;&#29702;&#20013;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#31181;&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UQ for Credit Risk Management: A deep evidence regression approach. (arXiv:2305.04967v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;Deep Evidence Regression&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#20449;&#29992;&#39118;&#38505;&#20013;&#30340;&#36829;&#32422;&#25439;&#22833;&#65307;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20449;&#29992;&#39118;&#38505;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#30001;&#20110;&#20449;&#29992;&#39118;&#38505;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#37327;&#21270;&#39044;&#27979;&#39118;&#38505;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#35201;&#30340;&#65292;&#23558;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35774;&#32622;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;UQ&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;Deep Evidence Regression&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#36829;&#32422;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;Deep Evidence Regression&#26041;&#27861;&#25193;&#23637;&#21040;&#36890;&#36807;Weibull&#36807;&#31243;&#29983;&#25104;&#30340;&#30446;&#26631;&#21464;&#37327;&#30340;&#23398;&#20064;&#26469;&#20026;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning has invariantly found its way into various Credit Risk applications. Due to the intrinsic nature of Credit Risk, quantifying the uncertainty of the predicted risk metrics is essential, and applying uncertainty-aware deep learning models to credit risk settings can be very helpful. In this work, we have explored the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression and applied it to predicting Loss Given Default. We contribute to the literature by extending the Deep Evidence Regression methodology to learning target variables generated by a Weibull process and provide the relevant learning framework. We demonstrate the application of our approach to both simulated and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03355</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#32508;&#21512;&#30740;&#31350;&#65306;&#24615;&#33021;&#12289;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26088;&#22312;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#29305;&#24449;&#32534;&#30721;&#25104;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#26159;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#30456;&#20851;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#21387;&#32553;&#22270;&#20687;&#30340;&#20449;&#24687;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20174;&#23433;&#20840;&#24615;&#35282;&#24230;&#20840;&#38754;&#20998;&#26512;&#36825;&#19968;&#25216;&#26415;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#39118;&#38505;&#32570;&#20047;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#25104;&#21151;&#20351;&#29992;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26469;&#26174;&#31034;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#21387;&#32553;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#25918;&#22823;&#31867;&#21035;&#38388;&#30340;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#25968;&#25454;&#38598;&#21387;&#32553;&#35780;&#20272;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#28216;&#25103;&#29702;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#65292;&#36890;&#36807;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#24471;&#21040;&#21807;&#19968;&#20840;&#38754;&#30340;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.03100</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21327;&#21516;&#21151;&#33021;&#65306;&#32479;&#19968;&#21338;&#24328;&#35770;&#20132;&#20114;&#26041;&#27861;&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability. (arXiv:2305.03100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#28216;&#25103;&#29702;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#65292;&#36890;&#36807;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#24471;&#21040;&#21807;&#19968;&#20840;&#38754;&#30340;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#36825;&#20123;&#39640;&#24615;&#33021;&#27169;&#22411;&#36890;&#24120;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#12290;&#35299;&#37322;&#27492;&#31867;&#27169;&#22411;&#23558;&#25552;&#39640;AI&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#65292;&#24182;&#19988;&#23545;&#20110;&#29702;&#35299;&#20854;&#20182;&#23454;&#38469;&#38656;&#27714;&#65288;&#22914;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#65289;&#26159;&#24517;&#35201;&#30340;&#12290;&#22686;&#24378;&#27169;&#22411;&#36879;&#26126;&#24230;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#37327;&#21270;&#21333;&#20010;&#36755;&#20837;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#36129;&#29486;&#65288;&#31216;&#20026;&#24402;&#22240;&#65289;&#20197;&#21450;&#32676;&#32452;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#36825;&#20123;&#26041;&#27861;&#23548;&#20837;&#21338;&#24328;&#35770;&#30340;&#27010;&#24565;&#21644;&#32467;&#26524;&#26469;&#20135;&#29983;&#24402;&#22240;&#21644;&#20132;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21338;&#24328;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#65292;&#20551;&#35774;&#36866;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#29305;&#24449;&#20043;&#38388;&#20132;&#20114;&#30340;&#21807;&#19968;&#20840;&#38754;&#35828;&#26126;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has revolutionized many areas of machine learning, from computer vision to natural language processing, but these high-performance models are generally "black box." Explaining such models would improve transparency and trust in AI-powered decision making and is necessary for understanding other practical needs such as robustness and fairness. A popular means of enhancing model transparency is to quantify how individual inputs contribute to model outputs (called attributions) and the magnitude of interactions between groups of inputs. A growing number of these methods import concepts and results from game theory to produce attributions and interactions. This work presents a unifying framework for game-theory-inspired attribution and $k^\text{th}$-order interaction methods. We show that, given modest assumptions, a unique full account of interactions between features, called synergies, is possible in the continuous input setting. We identify how various methods are characte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.10517</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#33719;&#21462;&#25104;&#26412;&#65292;&#35757;&#32451;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#27169;&#22411;&#65292;&#32463;&#36807;&#36229;&#36807;10&#20159;&#20010;&#27880;&#37322;&#30340;&#35757;&#32451;&#65292;&#20027;&#35201;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#65292;&#26088;&#22312;&#33021;&#22815;&#20197;&#20132;&#20114;&#26041;&#24335;&#20998;&#21106;&#29992;&#25143;&#23450;&#20041;&#30340;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19981;&#28165;&#26970;&#35813;&#27169;&#22411;&#22312;&#36716;&#25442;&#21040;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#26102;&#20250;&#21463;&#21040;&#22810;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;SAM&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#30340;11&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#29983;&#25104;&#28857;&#25552;&#31034;&#26469;&#27169;&#25311;&#20132;&#20114;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#22522;&#20110;&#21333;&#28857;&#25552;&#31034;&#30340;&#34920;&#29616;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#39640;&#24230;&#21464;&#21270;&#65292;&#21363;&#20174;&#33034;&#26609;MRI&#25968;&#25454;&#38598;&#30340;0.1135&#21040;&#39627;&#20851;&#33410;X&#23556;&#32447;&#25968;&#25454;&#38598;&#30340;0.8650&#12290;
&lt;/p&gt;
&lt;p&gt;
Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
&lt;/p&gt;</description></item><item><title>PED-ANOVA &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#35745;&#31639;&#36229;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.10255</link><description>&lt;p&gt;
PED-ANOVA: &#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#37327;&#21270;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10255
&lt;/p&gt;
&lt;p&gt;
PED-ANOVA &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#35745;&#31639;&#36229;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#27969;&#34892;&#20351;&#24471;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#23545;&#20110;&#35757;&#32451;&#24378;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#21448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20102;&#35299;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#12290;&#36825;&#28608;&#21457;&#20102;&#20851;&#20110;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;&#20351;&#29992;&#21151;&#33021;&#26041;&#24046;&#20998;&#26512; (f-ANOVA) &#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340; f-ANOVA &#20844;&#24335;&#19981;&#36866;&#29992;&#20110;&#31639;&#27861;&#35774;&#35745;&#24072;&#26368;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#65292;&#20363;&#22914;&#30001;&#26368;&#20339;&#24615;&#33021;&#23450;&#20041;&#30340;&#23376;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#20219;&#24847;&#23376;&#31354;&#38388;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20351;&#29992; Pearson &#25955;&#24230; (PED) &#23454;&#29616;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#38381;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026; PED-ANOVA&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#35782;&#21035;&#19981;&#21516;&#23376;&#31354;&#38388;&#20013;&#37325;&#35201;&#30340;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26497;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09513</link><description>&lt;p&gt;
NetGPT&#65306;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NetGPT: Generative Pretrained Transformer for Network Traffic. (arXiv:2304.09513v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#20026;&#36755;&#20837;&#27969;&#37327;&#29983;&#25104;&#21487;&#21306;&#20998;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#19979;&#28216;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#27969;&#37327;&#20998;&#31867;&#12289;&#25915;&#20987;&#26816;&#27979;&#12289;&#36164;&#28304;&#35843;&#24230;&#12289;&#21327;&#35758;&#20998;&#26512;&#21644;&#27969;&#37327;&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NetGPT&#65292;&#26088;&#22312;&#20026;&#32593;&#32476;&#27969;&#37327;&#26500;&#24314;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#35299;&#20915;&#22810;&#26679;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as traffic classification, attack detection, resource scheduling, protocol analysis, and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.  To tackle these challenges, in this paper, we make the first attemp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.08649</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;BERT&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#35789;&#24615;&#65288;POS&#65289;&#26631;&#35760;&#31561;&#65289;&#19978;&#20135;&#29983;&#20102;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#32467;&#26524;&#12290;&#24403;&#20998;&#31867;&#38271;&#25991;&#26723;&#65288;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#25991;&#26723;&#65289;&#26102;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#21487;&#33021;&#27604;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#20915;&#23450;&#25110;&#26368;&#39640;&#27861;&#38498;&#25968;&#25454;&#24211;&#65288;SCDB&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;SOTA&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#38024;&#23545;&#38271;&#25991;&#26723;&#30340;SOTA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#65288;1&#65289;&#24191;&#27867;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;15&#20010;&#31867;&#21035;&#65307;&#65288;2&#65289;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;279&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#32467;&#26524;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#20135;&#29983;80&#65285;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#20135;&#29983;60&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#35774;&#35745;&#20986;&#19968;&#20010;&#22810;&#27169;&#24577;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#30340;&#30701;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#19982;&#22806;&#37096;&#30693;&#35782;&#65292;&#33021;&#26377;&#25928;&#22320;&#21306;&#20998;&#30701;&#35270;&#39057;&#35875;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.08401</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multimodal Short Video Rumor Detection System Based on Contrastive Learning. (arXiv:2304.08401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#35774;&#35745;&#20986;&#19968;&#20010;&#22810;&#27169;&#24577;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#30340;&#30701;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#19982;&#22806;&#37096;&#30693;&#35782;&#65292;&#33021;&#26377;&#25928;&#22320;&#21306;&#20998;&#30701;&#35270;&#39057;&#35875;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30701;&#35270;&#39057;&#24179;&#21488;&#25104;&#20026;&#26032;&#38395;&#20998;&#20139;&#30340;&#37325;&#35201;&#28192;&#36947;&#20043;&#19968;&#65292;&#20013;&#22269;&#20027;&#35201;&#30701;&#35270;&#39057;&#24179;&#21488;&#36880;&#28176;&#25104;&#20026;&#34394;&#20551;&#26032;&#38395;&#30340;&#26032;&#28363;&#29983;&#22320;&#12290;&#30001;&#20110;&#30701;&#35270;&#39057;&#21253;&#21547;&#20102;&#22823;&#37327;&#20449;&#24687;&#21644;&#29305;&#24449;&#65292;&#20197;&#21450;&#35270;&#39057;&#20043;&#38388;&#30340;&#20005;&#37325;&#21516;&#36136;&#21270;&#21644;&#30456;&#20284;&#24615;&#65292;&#22240;&#27492;&#24456;&#38590;&#21306;&#20998;&#30701;&#35270;&#39057;&#35875;&#35328;&#12290;&#20026;&#20102;&#20943;&#36731;&#30701;&#35270;&#39057;&#35875;&#35328;&#30340;&#20256;&#25773;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#32771;&#34385;&#21040;&#27599;&#31181;&#31639;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#26500;&#24314;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#24182;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#12290;&#26816;&#27979;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#65306;&#65288;1&#65289;&#21019;&#24314;&#25968;&#25454;&#38598;&#65306;&#26500;&#24314;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#30340;&#30701;&#35270;&#39057;&#25968;&#25454;&#38598;&#65307;&#65288;2&#65289;&#22810;&#27169;&#24577;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#65306;&#39318;&#20808;&#20351;&#29992;TSN&#35270;&#39057;&#32534;&#30721;&#27169;&#22411;&#25552;&#21462;&#35270;&#39057;&#29305;&#24449;&#65307;&#28982;&#21518;&#20351;&#29992;OCR&#21644;ASR&#25552;&#21462;&#35270;&#39057;&#30340;&#25991;&#26412;&#29305;&#24449;&#65307;&#26368;&#21518;&#65292;&#23558;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#26469;&#36827;&#34892;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
With short video platforms becoming one of the important channels for news sharing, major short video platforms in China have gradually become new breeding grounds for fake news. However, it is not easy to distinguish short video rumors due to the great amount of information and features contained in short videos, as well as the serious homogenization and similarity of features among videos. In order to mitigate the spread of short video rumors, our group decides to detect short video rumors by constructing multimodal feature fusion and introducing external knowledge after considering the advantages and disadvantages of each algorithm. The ideas of detection are as follows: (1) dataset creation: to build a short video dataset with multiple features; (2) multimodal rumor detection model: firstly, we use TSN (Temporal Segment Networks) video coding model to extract video features; then, we use OCR (Optical Character Recognition) and ASR (Automatic Character Recognition) to extract video 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q-&#30697;&#38453;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19981;&#23384;&#22312;&#20107;&#20808;&#30830;&#23450;&#30340;&#25216;&#33021;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2304.08168</link><description>&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#30340;&#27880;&#24847;&#21147;Q-&#30697;&#38453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attentive Q-Matrix Learning for Knowledge Tracing. (arXiv:2304.08168v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q-&#30697;&#38453;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19981;&#23384;&#22312;&#20107;&#20808;&#30830;&#23450;&#30340;&#25216;&#33021;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#30340;&#36805;&#29467;&#21457;&#23637;&#20013;&#65292;&#36861;&#36394;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#20415;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#25351;&#23548;&#12290;&#36825;&#26159;&#30693;&#35782;&#36861;&#36394; (KT) &#30340;&#20027;&#35201;&#24605;&#24819;&#65292;&#23427;&#26681;&#25454;&#23398;&#29983;&#22312;&#24179;&#21488;&#19978;&#30340;&#36807;&#21435;&#20132;&#20114;&#26469;&#24314;&#31435;&#27169;&#22411;&#65292;&#20197;&#27169;&#25311;&#23398;&#29983;&#25484;&#25569;&#30693;&#35782;&#27010;&#24565; (KCs&#65292;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;&#25216;&#33021;)&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;KT&#27169;&#22411;&#65292;&#24182;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#27169;&#22411;&#20351;&#29992;&#27010;&#24565;&#26469;&#32034;&#24341;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#39044;&#20808;&#30830;&#23450;&#27599;&#20010;&#38382;&#39064;&#25152;&#38656;&#30340;&#25216;&#33021;&#26631;&#31614;&#65292;&#20197;&#25351;&#31034;&#27491;&#30830;&#22238;&#31572;&#35813;&#38382;&#39064;&#25152;&#38656;&#30340;KC&#12290;&#36825;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#24456;&#38590;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#65292;&#22240;&#20026;&#38382;&#39064;&#36890;&#24120;&#27809;&#26377;&#25353;&#29031;&#25216;&#33021;&#26631;&#31614;&#36827;&#34892;&#24456;&#22909;&#30340;&#32452;&#32455;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Q-&#30697;&#38453;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#36861;&#36394; (QAKT)&#65292;&#36825;&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#27880;&#24847;&#21147;&#26041;&#27861;&#24212;&#29992;&#20110;&#22330;&#26223;&#20013;&#65292;&#20854;&#20013;&#19981;&#23384;&#22312;&#20107;&#20808;&#30830;&#23450;&#30340;&#25216;&#33021;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the rapid development of Intelligent Tutoring Systems (ITS) in the past decade, tracing the students' knowledge state has become more and more important in order to provide individualized learning guidance. This is the main idea of Knowledge Tracing (KT), which models students' mastery of knowledge concepts (KCs, skills needed to solve a question) based on their past interactions on platforms. Plenty of KT models have been proposed and have shown remarkable performance recently. However, the majority of these models use concepts to index questions, which means the predefined skill tags for each question are required in advance to indicate the KCs needed to answer that question correctly. This makes it pretty hard to apply on large-scale online education platforms where questions are often not well-organized by skill tags. In this paper, we propose Q-matrix-based Attentive Knowledge Tracing (QAKT), an end-to-end style model that is able to apply the attentive method to scenes where n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.06803</link><description>&lt;p&gt;
&#29992;&#20110;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample Average Approximation for Black-Box VI. (arXiv:2304.06803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#30340;&#22256;&#38590;&#65292;&#21253;&#25324;&#36873;&#25321;&#27493;&#38271;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#19968;&#31995;&#21015;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#38382;&#39064;&#65288;SAA&#65289;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;SAA&#36924;&#36817;&#20102;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#25311;&#29275;&#39039;&#26041;&#27861;&#21644;&#32447;&#24615;&#25628;&#32034;&#26469;&#35299;&#20915;&#27599;&#20010;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#33258;&#21160;&#36873;&#25321;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#21464;&#20998;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for black-box VI that bypasses the difficulties of stochastic gradient ascent, including the task of selecting step-sizes. Our approach involves using a sequence of sample average approximation (SAA) problems. SAA approximates the solution of stochastic optimization problems by transforming them into deterministic ones. We use quasi-Newton methods and line search to solve each deterministic optimization problem and present a heuristic policy to automate hyperparameter selection. Our experiments show that our method simplifies the VI problem and achieves faster performance than existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02396</link><description>&lt;p&gt;
AutoRL&#36229;&#21442;&#25968;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21462;&#24471;&#20196;&#20154;&#30633;&#30446;&#25104;&#26524;&#30340;&#21516;&#26102;&#65292;&#20854;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;&#36825;&#32463;&#24120;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#33258;&#21160;&#21270;RL&#65288;AutoRL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#20294;&#26377;&#20851;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26041;&#27861;&#22312;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#26102;&#25152;&#36941;&#21382;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#21160;&#24577;&#21464;&#21270;&#30340;&#20449;&#24687;&#24456;&#23569;&#12290;&#37492;&#20110;&#29616;&#26377;AutoRL&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20165;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#36229;&#21442;&#25968;&#26223;&#35266;&#12290;&#38024;&#23545;&#20851;&#20110;&#36825;&#31181;&#21160;&#24577;AutoRL&#26041;&#27861;&#21512;&#27861;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#29615;&#22659;&#65288;Cartpole&#21644;Pendulum&#65289;&#20013;&#65292;&#26469;&#33258;RL&#25991;&#29486;&#30340;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#24378;&#28872;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#21644;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01347</link><description>&lt;p&gt;
&#26102;&#38388;&#21160;&#24577;&#21516;&#27493;&#21151;&#33021;&#33041;&#32593;&#32476;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis. (arXiv:2304.01347v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#21644;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#21487;&#20197;&#25429;&#25417;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#20013;&#30340;&#33041;&#27963;&#21160;&#26102;&#21464;&#24322;&#24120;&#65292;&#24182;&#22312;&#25581;&#31034;&#31934;&#31070;&#20998;&#35010;&#30151;&#65288;SZ&#65289;&#24739;&#32773;&#24322;&#24120;&#33041;&#27963;&#21160;&#26426;&#21046;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#21160;&#24577;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#8212;&#8212;&#26102;&#24577;&#33041;&#31867;&#21035;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;temporal-BCGCN&#65289;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#29420;&#29305;&#30340;&#21160;&#24577;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22359;DSF-BrainNet&#65292;&#29992;&#20110;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;TemporalConv&#65292;&#22522;&#20110;&#29305;&#24449;&#30340;&#21516;&#27493;&#26102;&#38388;&#23646;&#24615;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#21270;&#24322;&#24120;&#21322;&#29699;&#20391;&#21270;&#26816;&#27979;&#24037;&#20855;&#65292;&#31216;&#20026;CategoryPool&#12290;&#35813;&#30740;&#31350;&#22312;COBRE&#21644;UCLA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#20998;&#21035;&#36798;&#21040;83.62&#65285;&#21644;89.71&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Available evidence suggests that dynamic functional connectivity (dFC) can capture time-varying abnormalities in brain activity in rs-fMRI data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed, based on the synchronous temporal properties of feature. Finally, the first modular abnormal hemispherical lateralization test tool in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracy, respectively, outperforming the baseline model and other State-of-the-Art
&lt;/p&gt;</description></item><item><title>Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.13937</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#31890;&#23376;&#29289;&#29702;&#36807;&#31243;&#30340;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13937
&lt;/p&gt;
&lt;p&gt;
Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Topograph&#65292;&#23427;&#21033;&#29992;&#31890;&#23376;&#29289;&#29702;&#34928;&#21464;&#30340;&#26412;&#36136;&#21644;&#20449;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#65292;&#37325;&#24314;&#20102;&#21253;&#25324;&#20013;&#20171;&#31890;&#23376;&#22312;&#20869;&#30340;&#24213;&#23618;&#29289;&#29702;&#36807;&#31243;&#12290;Topograph&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#30340;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#23558;&#23427;&#20204;&#19982;&#23427;&#20204;&#21407;&#26469;&#30340;&#27597;&#31890;&#23376;&#20851;&#32852;&#36215;&#26469;&#65292;&#32780;&#19988;&#30452;&#25509;&#39044;&#27979;&#20102;&#30828;&#25955;&#23556;&#36807;&#31243;&#20013;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#12290;&#19982;&#26631;&#20934;&#30340;&#32452;&#21512;&#26041;&#27861;&#25110;&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#30340;&#22797;&#26434;&#24230;&#19982;&#37325;&#26500;&#23545;&#35937;&#30340;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#24212;&#29992;Topograph&#20110;&#20840;&#24378;&#23376;&#34928;&#21464;&#27169;&#24335;&#19979;&#30340;&#39030;&#22840;&#20811;&#23545;&#20135;&#29983;&#38382;&#39064;&#65292;&#30456;&#23545;&#26631;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;MFVB&#26041;&#27861;&#65292;&#26377;&#25928;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#33539;&#22260;&#65292;&#21487;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#27874;&#21160;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.13930</link><description>&lt;p&gt;
&#31890;&#23376;&#24179;&#22343;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Particle Mean Field Variational Bayes. (arXiv:2303.13930v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;MFVB&#26041;&#27861;&#65292;&#26377;&#25928;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#33539;&#22260;&#65292;&#21487;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#27874;&#21160;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#26041;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26368;&#39640;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#25216;&#26415;&#20043;&#19968;&#65292;&#28982;&#32780;&#20854;&#20351;&#29992;&#20165;&#38480;&#20110;&#20855;&#26377;&#20849;&#36717;&#20808;&#39564;&#25110;&#38656;&#35201;&#35299;&#26512;&#35745;&#31639;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;MFVB&#26041;&#27861;&#65292;&#22823;&#22823;&#25193;&#23637;&#20102;MFVB&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;Wasserstein&#26799;&#24230;&#27969;&#19982;Langevin&#25193;&#25955;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#26500;&#24314;&#20102;&#26032;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#27874;&#21160;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mean Field Variational Bayes (MFVB) method is one of the most computationally efficient techniques for Bayesian inference. However, its use has been restricted to models with conjugate priors or those that require analytical calculations. This paper proposes a novel particle-based MFVB approach that greatly expands the applicability of the MFVB method. We establish the theoretical basis of the new method by leveraging the connection between Wasserstein gradient flows and Langevin diffusion dynamics, and demonstrate the effectiveness of this approach using Bayesian logistic regression, stochastic volatility, and deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#25439;&#22833;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#38656;&#21442;&#32771;&#25968;&#25454;&#30340;&#36229;&#22768;&#30456;&#20301;&#20687;&#24046;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2303.05747</link><description>&lt;p&gt;
&#26080;&#38656;&#21442;&#32771;&#25968;&#25454;&#30340;&#30456;&#20301;&#20687;&#24046;&#20462;&#22797;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#25439;&#22833;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Phase Aberration Correction without Reference Data: An Adaptive Mixed Loss Deep Learning Approach. (arXiv:2303.05747v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#25439;&#22833;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#38656;&#21442;&#32771;&#25968;&#25454;&#30340;&#36229;&#22768;&#30456;&#20301;&#20687;&#24046;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#20687;&#24046;&#26159;&#36229;&#22768;&#25104;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#30001;&#20171;&#36136;&#20013;&#22768;&#36895;&#30340;&#31354;&#38388;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#31181;&#25928;&#24212;&#30772;&#22351;&#20102;&#20256;&#36755;&#27874;&#24182;&#38459;&#27490;&#22238;&#27874;&#20449;&#21495;&#30340;&#30456;&#24178;&#21472;&#21152;&#65292;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#20302;&#19979;&#12290;&#22312;&#23454;&#38469;&#23454;&#39564;&#20013;&#65292;&#33719;&#24471;&#38750;&#20687;&#24046;&#30340;&#22522;&#30784;&#30495;&#20540;&#21487;&#33021;&#20250;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#22914;&#26524;&#19981;&#26159;&#19981;&#21487;&#33021;&#30340;&#35805;&#12290;&#36825;&#31181;&#25928;&#24212;&#38459;&#30861;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30456;&#20301;&#20687;&#24046;&#20462;&#22797;&#25216;&#26415;&#30340;&#34920;&#29616;&#65292;&#22240;&#20026;&#20165;&#20381;&#36182;&#20110;&#27169;&#25311;&#25968;&#25454;&#20197;&#21450;&#27169;&#25311;&#21644;&#23454;&#39564;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#21442;&#32771;&#25968;&#25454;&#26469;&#34917;&#20607;&#30456;&#20301;&#35823;&#24046;&#25928;&#24212;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#32593;&#32476;&#65292;&#20854;&#20013;&#36755;&#20837;&#21644;&#30446;&#26631;&#36755;&#20986;&#37117;&#26159;&#38543;&#26426;&#30072;&#21464;&#30340;&#23556;&#39057;&#65288;RF&#65289;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65292;&#23545;&#20110;&#35757;&#32451;&#32593;&#32476;&#20197;&#33719;&#21462;&#21487;&#23454;&#29616;&#30340;&#30456;&#20301;&#20462;&#22797;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase aberration is one of the primary sources of image quality degradation in ultrasound, which is induced by spatial variations in sound speed across the heterogeneous medium. This effect disrupts transmitted waves and prevents coherent summation of echo signals, resulting in suboptimal image quality. In real experiments, obtaining non-aberrated ground truths can be extremely challenging, if not infeasible. It hinders the performance of deep learning-based phase aberration correction techniques due to sole reliance on simulated data and the presence of domain shift between simulated and experimental data. Here, for the first time, we propose a deep learning-based method that does not require reference data to compensate for the phase aberration effect. We train a network wherein both input and target output are randomly aberrated radio frequency (RF) data. Moreover, we demonstrate that a conventional loss function such as mean square error is inadequate for training the network to ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#32422;&#26463;&#24182;&#19988;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.01141</link><description>&lt;p&gt;
DeepSaDe: &#23398;&#20064;&#30830;&#20445;&#28385;&#36275;&#39046;&#22495;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction. (arXiv:2303.01141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#32422;&#26463;&#24182;&#19988;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#34892;&#20026;&#24517;&#39035;&#26159;&#23433;&#20840;&#30340;&#12290;&#24403;&#21069;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32422;&#26463;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#65288;&#21363;&#20351;&#22312;&#26410;&#30475;&#36807;&#30340;&#25968;&#25454;&#19978;&#65289;&#65292;&#25110;&#32773;&#23427;&#20204;&#23545;&#21487;&#24378;&#21046;&#25191;&#34892;&#30340;&#32422;&#26463;&#31867;&#22411;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#24191;&#27867;&#32422;&#26463;&#24182;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20197;&#24448;&#23558;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;&#35270;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#23558;&#36825;&#20010;&#24819;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#25991;&#22686;&#21152;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#26032;&#20803;&#32032;&#65306;&#32593;&#32476;&#23618;&#19978;&#30340;&#32422;&#26463;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight upda
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#28145;&#24230;&#39640;&#26031;&#27169;&#22411;&#30340;&#29305;&#24449;&#21508;&#21521;&#24322;&#24615;&#23637;&#24320;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31532;&#19968;&#23618;&#29305;&#24449;&#34892;&#20043;&#38388;&#20801;&#35768;&#23384;&#22312;&#30456;&#20851;&#24615;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#65292;&#32780;&#21518;&#32493;&#23618;&#30340;&#32467;&#26500;&#36890;&#24120;&#26159;&#19981;&#21033;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.00564</link><description>&lt;p&gt;
&#28145;&#24230;&#32467;&#26500;&#39640;&#26031;&#29305;&#24449;&#27169;&#22411;&#30340;&#23398;&#20064;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Learning curves for deep structured Gaussian feature models. (arXiv:2303.00564v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00564
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#28145;&#24230;&#39640;&#26031;&#27169;&#22411;&#30340;&#29305;&#24449;&#21508;&#21521;&#24322;&#24615;&#23637;&#24320;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31532;&#19968;&#23618;&#29305;&#24449;&#34892;&#20043;&#38388;&#20801;&#35768;&#23384;&#22312;&#30456;&#20851;&#24615;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#65292;&#32780;&#21518;&#32493;&#23618;&#30340;&#32467;&#26500;&#36890;&#24120;&#26159;&#19981;&#21033;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#23545;&#20110;&#22810;&#23618;&#39640;&#26031;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20998;&#26512;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#29305;&#24449;&#21508;&#21521;&#24322;&#24615;&#30340;&#24433;&#21709;&#65307;&#22823;&#22810;&#25968;&#27169;&#22411;&#37117;&#20551;&#35774;&#29305;&#24449;&#26159;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#26435;&#37325;&#29983;&#25104;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#20855;&#26377;&#35768;&#22810;&#23618;&#32467;&#26500;&#39640;&#26031;&#29305;&#24449;&#30340;&#27169;&#22411;&#23548;&#20986;&#20102;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20801;&#35768;&#31532;&#19968;&#23618;&#29305;&#24449;&#30340;&#34892;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#21487;&#20419;&#36827;&#27867;&#21270;&#65292;&#32780;&#21518;&#32493;&#23618;&#30340;&#32467;&#26500;&#36890;&#24120;&#26159;&#19981;&#21033;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26435;&#37325;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#21487;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant attention in deep learning theory has been devoted to analyzing the generalization performance of models with multiple layers of Gaussian random features. However, few works have considered the effect of feature anisotropy; most assume that features are generated using independent and identically distributed Gaussian weights. Here, we derive learning curves for models with many layers of structured Gaussian features. We show that allowing correlations between the rows of the first layer of features can aid generalization, while structure in later layers is generally detrimental. Our results shed light on how weight structure affects generalization in a simple class of solvable models.
&lt;/p&gt;</description></item><item><title>MCoCo&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#19968;&#33268;&#24615;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#40784;&#35821;&#20041;&#31354;&#38388;&#21644;&#22810;&#32423;&#19968;&#33268;&#24615;&#21327;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22810;&#35270;&#35282;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2302.13339</link><description>&lt;p&gt;
MCoCo&#65306;&#22810;&#32423;&#19968;&#33268;&#24615;&#21327;&#20316;&#22810;&#35270;&#35282;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
MCoCo: Multi-level Consistency Collaborative Multi-view Clustering. (arXiv:2302.13339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13339
&lt;/p&gt;
&lt;p&gt;
MCoCo&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#19968;&#33268;&#24615;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#40784;&#35821;&#20041;&#31354;&#38388;&#21644;&#22810;&#32423;&#19968;&#33268;&#24615;&#21327;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22810;&#35270;&#35282;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#21487;&#20197;&#21033;&#29992;&#19981;&#21516;&#35270;&#35282;&#30340;&#19968;&#33268;&#20449;&#24687;&#25351;&#23548;&#32858;&#31867;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#37117;&#19987;&#27880;&#20110;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36861;&#27714;&#27973;&#23618;&#27425;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#22810;&#20010;&#35270;&#35282;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#20013;&#36827;&#34892;&#32858;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21644;&#25506;&#32034;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32423;&#19968;&#33268;&#24615;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;(MCoCo)&#26469;&#36827;&#34892;&#22810;&#35270;&#35282;&#32858;&#31867;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MCoCo&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#32852;&#21512;&#23398;&#20064;&#22810;&#20010;&#35270;&#35282;&#30340;&#38598;&#32676;&#20998;&#37197;&#65292;&#24182;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#19981;&#21516;&#35270;&#35282;&#30340;&#35821;&#20041;&#26631;&#31614;&#36827;&#34892;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#32423;&#19968;&#33268;&#24615;&#21327;&#20316;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#20041;&#31354;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#20316;&#20026;&#33258;&#30417;&#30563;&#20449;&#21495;&#19982;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#38598;&#32676;&#20998;&#37197;&#36827;&#34892;&#21327;&#20316;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#32423;&#21035;&#30340;&#31354;&#38388;&#30456;&#20114;&#21327;&#20316;&#65292;&#21516;&#26102;&#23454;&#29616;&#33258;&#24049;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering can explore consistent information from different views to guide clustering. Most existing works focus on pursuing shallow consistency in the feature space and integrating the information of multiple views into a unified representation for clustering. These methods did not fully consider and explore the consistency in the semantic space. To address this issue, we proposed a novel Multi-level Consistency Collaborative learning framework (MCoCo) for multi-view clustering. Specifically, MCoCo jointly learns cluster assignments of multiple views in feature space and aligns semantic labels of different views in semantic space by contrastive learning. Further, we designed a multi-level consistency collaboration strategy, which utilizes the consistent information of semantic space as a self-supervised signal to collaborate with the cluster assignments in feature space. Thus, different levels of spaces collaborate with each other while achieving their own consistency goal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31526;&#21512;&#24050;&#30693;&#30005;&#27969;&#29289;&#29702;&#29305;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#36890;&#36807;Helmholtz&#20998;&#35299;&#33719;&#24471;&#30340;&#21521;&#37327;&#22330;&#30340;&#21457;&#25955;&#21644;&#26080;&#26059;&#20998;&#37327;&#19978;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#39044;&#27979;&#28023;&#27969;&#12290;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#28014;&#26631;&#25968;&#25454;&#26041;&#38754;&#37117;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.10364</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#36203;&#36203;&#23572;&#22982;&#38669;&#20857;&#20998;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#26356;&#27969;&#20307;&#30340;&#28023;&#27915;&#27668;&#27969;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes at the Helm(holtz): A more fluid model for ocean currents. (arXiv:2302.10364v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31526;&#21512;&#24050;&#30693;&#30005;&#27969;&#29289;&#29702;&#29305;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#36890;&#36807;Helmholtz&#20998;&#35299;&#33719;&#24471;&#30340;&#21521;&#37327;&#22330;&#30340;&#21457;&#25955;&#21644;&#26080;&#26059;&#20998;&#37327;&#19978;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#39044;&#27979;&#28023;&#27969;&#12290;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#28014;&#26631;&#25968;&#25454;&#26041;&#38754;&#37117;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#23398;&#23478;&#26377;&#20852;&#36259;&#39044;&#27979;&#28023;&#27969;&#21644;&#22522;&#20110;&#28014;&#26631;&#36895;&#24230;&#30340;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#26469;&#35782;&#21035;&#24403;&#21069;&#30690;&#37327;&#22330;&#20013;&#30340;&#21457;&#25955;&#24615;&#12290;&#39640;&#26031;&#36807;&#31243;(GPs)&#22312;&#31354;&#38388;&#20301;&#32622;&#19978;&#20805;&#24403;&#36830;&#32493;&#20294;&#39640;&#24230;&#38750;&#32447;&#24615;&#21151;&#33021;&#30340;&#36895;&#24230;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#27169;&#22411;&#12290;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#20855;&#26377;&#26631;&#20934;&#24179;&#31283;&#26680;&#30340;GP&#30452;&#25509;&#24212;&#29992;&#20110;&#28014;&#26631;&#25968;&#25454;&#21487;&#33021;&#22312;&#24403;&#21069;&#39044;&#27979;&#21644;&#21457;&#25955;&#24615;&#35782;&#21035;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#8212;&#30001;&#20110;&#19968;&#20123;&#29289;&#29702;&#19978;&#19981;&#20999;&#23454;&#38469;&#30340;&#20808;&#39564;&#20551;&#35774;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21453;&#26144;&#24050;&#30693;&#30340;&#30005;&#27969;&#29289;&#29702;&#29305;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26631;&#20934;&#24179;&#31283;&#26680;&#25918;&#22312;&#36890;&#36807;Helmholtz&#20998;&#35299;&#33719;&#24471;&#30340;&#21521;&#37327;&#22330;&#30340;&#21457;&#25955;&#21644;&#26080;&#26059;&#20998;&#37327;&#19978;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30001;&#20110;&#35813;&#20998;&#35299;&#20165;&#36890;&#36807;&#28151;&#21512;&#20559;&#23548;&#25968;&#19982;&#21407;&#22987;&#21521;&#37327;&#22330;&#30456;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#39069;&#22806;&#36827;&#34892;&#23569;&#25968;&#35745;&#31639;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#28014;&#26631;&#25968;&#25454;&#35777;&#26126;&#20102;&#36825;&#31181;&#34746;&#26059;GP&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#22343;&#26041;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oceanographers are interested in predicting ocean currents and identifying divergences in a current vector field based on sparse observations of buoy velocities. Since we expect current velocity to be a continuous but highly non-linear function of spatial location, Gaussian processes (GPs) offer an attractive model. But we show that applying a GP with a standard stationary kernel directly to buoy data can struggle at both current prediction and divergence identification -- due to some physically unrealistic prior assumptions. To better reflect known physical properties of currents, we propose to instead put a standard stationary kernel on the divergence and curl-free components of a vector field obtained through a Helmholtz decomposition. We show that, because this decomposition relates to the original vector field just via mixed partial derivatives, we can still perform inference given the original data with only a small constant multiple of additional computational expense. We illust
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;"Growing Steerable Neural Cellular Automata"&#65292;&#36890;&#36807;&#20351;&#27599;&#20010;&#32454;&#32990;&#36127;&#36131;&#20854;&#33258;&#24049;&#30340;&#26041;&#21521;&#65292;&#20135;&#29983;&#20102;&#20855;&#26377;&#21464;&#21270;&#26041;&#21521;&#30340;&#32454;&#32990;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10197</link><description>&lt;p&gt;
&#29983;&#38271;&#21487;&#25805;&#32437;&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Growing Steerable Neural Cellular Automata. (arXiv:2302.10197v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;"Growing Steerable Neural Cellular Automata"&#65292;&#36890;&#36807;&#20351;&#27599;&#20010;&#32454;&#32990;&#36127;&#36131;&#20854;&#33258;&#24049;&#30340;&#26041;&#21521;&#65292;&#20135;&#29983;&#20102;&#20855;&#26377;&#21464;&#21270;&#26041;&#21521;&#30340;&#32454;&#32990;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#65288;NCA&#65289;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#27169;&#24335;&#24418;&#25104;&#33021;&#21147;&#21644;&#28304;&#20110;&#23616;&#37096;&#21327;&#35843;&#30340;&#22797;&#26434;&#20840;&#23616;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#21407;&#22987;&#30340;NCA&#23454;&#29616;&#20013;&#65292;&#32454;&#32990;&#26080;&#27861;&#35843;&#25972;&#33258;&#24049;&#30340;&#26041;&#21521;&#65292;&#23450;&#21521;&#26159;&#30001;&#27169;&#22411;&#35774;&#35745;&#24072;&#22312;&#22806;&#37096;&#23450;&#21521;&#12290;&#26368;&#36817;&#30340;&#21508;&#21521;&#21516;&#24615;NCA&#21464;&#20307;&#65288;Growing Isotropic Neural Cellular Automata&#65289;&#36890;&#36807;&#28040;&#38500;&#23545;&#20854;&#37051;&#22495;&#20013;&#31354;&#38388;&#29366;&#24577;&#26799;&#24230;&#24863;&#30693;&#30340;&#20381;&#36182;&#24615;&#65292;&#20351;&#27169;&#22411;&#19982;&#23450;&#21521;&#26080;&#20851; - &#32454;&#32990;&#26080;&#27861;&#20174;&#19978;&#21040;&#19979;&#25110;&#20174;&#24038;&#21040;&#21491;&#65292;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;NCA&#65306;&#25105;&#20204;&#20351;&#27599;&#20010;&#32454;&#32990;&#36127;&#36131;&#20854;&#33258;&#24049;&#30340;&#23450;&#21521;&#65292;&#20801;&#35768;&#20854;&#26681;&#25454;&#21487;&#35843;&#25972;&#30340;&#20869;&#37096;&#29366;&#24577;&#8220;&#36716;&#21521;&#8221;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21487;&#25805;&#32437;NCA&#21253;&#21547;&#23884;&#20837;&#21516;&#19968;&#27169;&#24335;&#20013;&#20855;&#26377;&#19981;&#21516;&#26041;&#21521;&#30340;&#32454;&#32990;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;&#21508;&#21521;&#21516;&#24615;NCA&#27809;&#26377;&#23450;&#21521;&#65292;&#21487;&#25805;&#32437;&#30340;NCA&#20855;&#26377;&#25163;&#24615;&#65306;&#23427;&#20204;&#26377;&#39044;&#23450;
&lt;/p&gt;
&lt;p&gt;
Neural Cellular Automata (NCA) models have shown remarkable capacity for pattern formation and complex global behaviors stemming from local coordination. However, in the original implementation of NCA, cells are incapable of adjusting their own orientation, and it is the responsibility of the model designer to orient them externally. A recent isotropic variant of NCA (Growing Isotropic Neural Cellular Automata) makes the model orientation-independent - cells can no longer tell up from down, nor left from right - by removing its dependency on perceiving the gradient of spatial states in its neighborhood. In this work, we revisit NCA with a different approach: we make each cell responsible for its own orientation by allowing it to "turn" as determined by an adjustable internal state. The resulting Steerable NCA contains cells of varying orientation embedded in the same pattern. We observe how, while Isotropic NCA are orientation-agnostic, Steerable NCA have chirality: they have a predete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;FPGA&#25216;&#26415;&#23454;&#29616;&#30340;ATLAS&#23454;&#39564;&#29992;&#20110;&#35745;&#31639;&#28082;&#24577;&#27689;&#38378;&#28865;&#35745;&#25968;&#22120;&#20013;&#27785;&#31215;&#33021;&#37327;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#65292;&#20248;&#20110;&#30446;&#21069;&#22522;&#20110;&#28388;&#27874;&#31639;&#27861;&#30340;&#35745;&#31639;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.07555</link><description>&lt;p&gt;
ATLAS&#23454;&#39564;&#20013;&#28082;&#24577;&#27689;&#38378;&#28865;&#35745;&#25968;&#22120;&#20013;&#27785;&#31215;&#33021;&#37327;&#35745;&#31639;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22266;&#20214;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Firmware implementation of a recurrent neural network for the computation of the energy deposited in the liquid argon calorimeter of the ATLAS experiment. (arXiv:2302.07555v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;FPGA&#25216;&#26415;&#23454;&#29616;&#30340;ATLAS&#23454;&#39564;&#29992;&#20110;&#35745;&#31639;&#28082;&#24577;&#27689;&#38378;&#28865;&#35745;&#25968;&#22120;&#20013;&#27785;&#31215;&#33021;&#37327;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#65292;&#20248;&#20110;&#30446;&#21069;&#22522;&#20110;&#28388;&#27874;&#31639;&#27861;&#30340;&#35745;&#31639;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ATLAS&#23454;&#39564;&#27979;&#37327;LHC&#36136;&#23376;-&#36136;&#23376;&#30896;&#25758;&#20135;&#29289;&#30340;&#23646;&#24615;&#12290;&#22312;LHC&#39640;&#20142;&#24230;&#38454;&#27573;&#20043;&#21069;&#65292;ATLAS&#25506;&#27979;&#22120;&#23558;&#36827;&#34892;&#37325;&#22823;&#21319;&#32423;&#12290;ATLAS&#28082;&#24577;&#27689;&#38378;&#28865;&#35745;&#25968;&#22120;&#27979;&#37327;&#25506;&#27979;&#22120;&#20013;&#30005;&#30913;&#30456;&#20114;&#20316;&#29992;&#20135;&#29983;&#30340;&#31890;&#23376;&#30340;&#33021;&#37327;&#12290;&#35813;&#38378;&#28865;&#35745;&#25968;&#22120;&#30340;&#35835;&#20986;&#30005;&#23376;&#23558;&#22312;ATLAS&#21319;&#32423;&#26399;&#38388;&#26356;&#25442;&#12290;&#26032;&#30340;&#30005;&#23376;&#26495;&#23558;&#22522;&#20110;&#26469;&#33258;&#33521;&#29305;&#23572;&#30340;&#26368;&#20808;&#36827;&#30340;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;(FPGA)&#65292;&#20801;&#35768;&#23454;&#29616;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#22266;&#20214;&#12290;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#26174;&#31034;&#20986;&#20248;&#20110;&#24403;&#21069;&#29992;&#20110;&#35745;&#31639;&#35745;&#25968;&#22120;&#20013;&#27785;&#31215;&#33021;&#37327;&#30340;&#26368;&#20339;&#28388;&#27874;&#31639;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Stratix 10 FPGA&#19978;&#23454;&#29616;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#20197;&#37325;&#24314;&#27785;&#31215;&#22312;&#35745;&#25968;&#22120;&#20013;&#33021;&#37327;&#30340;&#23454;&#29616;&#12290;&#39640;&#32423;&#32508;&#21512;&#35821;&#35328;(HLS)&#23454;&#29616;&#20351;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ATLAS experiment measures the properties of particles that are products of proton-proton collisions at the LHC. The ATLAS detector will undergo a major upgrade before the high luminosity phase of the LHC. The ATLAS liquid argon calorimeter measures the energy of particles interacting electromagnetically in the detector. The readout electronics of this calorimeter will be replaced during the aforementioned ATLAS upgrade. The new electronic boards will be based on state-of-the-art field-programmable gate arrays (FPGA) from Intel allowing the implementation of neural networks embedded in firmware. Neural networks have been shown to outperform the current optimal filtering algorithms used to compute the energy deposited in the calorimeter. This article presents the implementation of a recurrent neural network (RNN) allowing the reconstruction of the energy deposited in the calorimeter on Stratix 10 FPGAs. The implementation in high level synthesis (HLS) language allowed fast prototypin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20989;&#25968;&#22238;&#24402;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65292;&#26500;&#24314;&#20102;&#32447;&#24615;&#31639;&#23376;&#23558;&#36755;&#20837;&#36793;&#32536;&#20998;&#24067;&#19982;&#36755;&#20986;&#26465;&#20214;&#20998;&#24067;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20998;&#24067;&#20381;&#36182;&#24615;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#39044;&#27979;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04724</link><description>&lt;p&gt;
&#20989;&#25968;&#22238;&#24402;&#30340;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization by Functional Regression. (arXiv:2302.04724v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04724
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20989;&#25968;&#22238;&#24402;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65292;&#26500;&#24314;&#20102;&#32447;&#24615;&#31639;&#23376;&#23558;&#36755;&#20837;&#36793;&#32536;&#20998;&#24067;&#19982;&#36755;&#20986;&#26465;&#20214;&#20998;&#24067;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20998;&#24067;&#20381;&#36182;&#24615;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#39044;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#26159;&#23398;&#20064;&#22914;&#20309;&#22312;&#32473;&#23450;&#22810;&#31181;&#19981;&#21516;&#28304;&#20998;&#24067;&#30340;&#25968;&#25454;&#26102;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#36866;&#29992;&#20110;&#21482;&#36890;&#36807;&#26410;&#26631;&#35760;&#26679;&#26412;&#35266;&#23519;&#21040;&#30340;&#26032;&#30446;&#26631;&#20998;&#24067;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#20197;&#20989;&#25968;&#22238;&#24402;&#30340;&#24418;&#24335;&#30740;&#31350;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;&#20174;&#36755;&#20837;&#30340;&#36793;&#32536;&#20998;&#24067;&#21040;&#32473;&#23450;&#36755;&#20837;&#30340;&#26465;&#20214;&#19979;&#36755;&#20986;&#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#32447;&#24615;&#31639;&#23376;&#12290;&#35813;&#31639;&#27861;&#20801;&#35768;&#22522;&#20110;&#28304;&#20998;&#24067;&#20381;&#36182;&#24615;&#26500;&#24314;&#39044;&#27979;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#28385;&#36275;&#29702;&#24819;&#39118;&#38505;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#12290;&#25968;&#20540;&#23454;&#29616;&#21644;&#28304;&#20195;&#30721;&#24050;&#32463;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of domain generalization is to learn, given data from different source distributions, a model that can be expected to generalize well on new target distributions which are only seen through unlabeled samples. In this paper, we study domain generalization as a problem of functional regression. Our concept leads to a new algorithm for learning a linear operator from marginal distributions of inputs to the corresponding conditional distributions of outputs given inputs. Our algorithm allows a source distribution-dependent construction of reproducing kernel Hilbert spaces for prediction, and, satisfies finite sample error bounds for the idealized risk. Numerical implementations and source code are available.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#22914;&#20309;&#25913;&#36827;&#22312;&#32447;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#30340;TS&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20381;&#36182;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#22312;&#32447;&#24615;&#33021;&#65292;&#25913;&#36827;&#31243;&#24230;&#38543;&#19987;&#23478;&#33021;&#21147;&#27700;&#24179;&#30340;&#25552;&#39640;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2302.03319</link><description>&lt;p&gt;
&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#25913;&#36827;&#22312;&#32447;&#23398;&#20064;:&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Leveraging Demonstrations to Improve Online Learning: Quality Matters. (arXiv:2302.03319v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#22914;&#20309;&#25913;&#36827;&#22312;&#32447;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#30340;TS&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20381;&#36182;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#22312;&#32447;&#24615;&#33021;&#65292;&#25913;&#36827;&#31243;&#24230;&#38543;&#19987;&#23478;&#33021;&#21147;&#27700;&#24179;&#30340;&#25552;&#39640;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#21487;&#20197;&#22914;&#20309;&#25913;&#36827;&#22312;&#32447;&#23398;&#20064;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#26399;&#26395;&#20250;&#26377;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#25913;&#36827;&#20197;&#21450;&#21487;&#20197;&#25913;&#36827;&#22810;&#23569;&#65311;&#25105;&#20204;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#31243;&#24230;&#24517;&#39035;&#21462;&#20915;&#20110;&#28436;&#31034;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#29983;&#25104;&#21487;&#31227;&#26893;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#20316;&#20026;&#20856;&#22411;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21644;&#27169;&#22411;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#24212;&#29992;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#12290;&#28436;&#31034;&#25968;&#25454;&#26159;&#30001;&#20855;&#26377;&#32473;&#23450;&#33021;&#21147;&#27700;&#24179;&#30340;&#19987;&#23478;&#29983;&#25104;&#30340;&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#20010;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#24773;TS&#31639;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#23450;&#29702;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#24182;&#23548;&#20986;&#20381;&#36182;&#20110;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;&#36825;&#25552;&#20379;&#20102;&#27934;&#35265;&#65292;&#21363;&#39044;&#35757;&#32451;&#22914;&#20309;&#26497;&#22823;&#22320;&#25552;&#39640;&#22312;&#32447;&#24615;&#33021;&#65292;&#20197;&#21450;&#25913;&#36827;&#31243;&#24230;&#38543;&#19987;&#23478;&#33021;&#21147;&#27700;&#24179;&#30340;&#25552;&#39640;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36125;&#21494;&#26031;&#24341;&#23548;&#23454;&#29616;&#20102;&#23454;&#29992;&#30340;&#12289;&#36817;&#20284;&#30340;&#30693;&#24773;TS&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23454;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#36951;&#25022;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which offline demonstration data can improve online learning. It is natural to expect some improvement, but the question is how, and by how much? We show that the degree of improvement must depend on the quality of the demonstration data. To generate portable insights, we focus on Thompson sampling (TS) applied to a multi-armed bandit as a prototypical online learning algorithm and model. The demonstration data is generated by an expert with a given competence level, a notion we introduce. We propose an informed TS algorithm that utilizes the demonstration data in a coherent way through Bayes' rule and derive a prior-dependent Bayesian regret bound. This offers insight into how pretraining can greatly improve online performance and how the degree of improvement increases with the expert's competence level. We also develop a practical, approximate informed TS algorithm through Bayesian bootstrapping and show substantial empirical regret reduction through exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;&#65292;&#36890;&#36807;&#25193;&#23637;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#22312;&#35745;&#31639;&#24050;&#30693;&#26597;&#35810;&#22270;&#20687;&#30340;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.02865</link><description>&lt;p&gt;
&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#24674;&#22797;&#20102;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;&#65292;&#36890;&#36807;&#25193;&#23637;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#22312;&#35745;&#31639;&#24050;&#30693;&#26597;&#35810;&#22270;&#20687;&#30340;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#32534;&#30721;&#22120;&#34987;&#35777;&#26126;&#21487;&#20197;&#32763;&#36716;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65306;&#23427;&#20204;&#21487;&#20197;&#23558;&#27599;&#20010;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#32534;&#30721;&#25104;&#29983;&#25104;&#35813;&#22270;&#20687;&#30340;&#30495;&#23454;&#28508;&#21464;&#37327;&#65288;Zimmermann&#31561;&#20154;&#65292;2021&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#35266;&#23519;&#32467;&#26524;&#36890;&#24120;&#23384;&#22312;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#12290;&#20363;&#22914;&#65292;&#22270;&#20687;&#21487;&#33021;&#27169;&#31946;&#25110;&#21482;&#26174;&#31034;3D&#29289;&#20307;&#30340;2D&#35270;&#22270;&#65292;&#22240;&#27492;&#21487;&#33021;&#26377;&#22810;&#20010;&#28508;&#21464;&#37327;&#29983;&#25104;&#23427;&#20204;&#12290;&#36825;&#20351;&#24471;&#28508;&#21464;&#37327;&#30340;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#20855;&#26377;&#24322;&#26041;&#24046;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24120;&#35265;&#30340;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#65292;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#32780;&#19981;&#26159;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#20998;&#24067;&#24674;&#22797;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#27491;&#30830;&#21518;&#39564;&#20998;&#24067;&#65292;&#21253;&#25324;&#20854;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#20272;&#35745;&#65292;&#35813;&#20272;&#35745;&#23384;&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#26059;&#36716;&#12290;&#38500;&#20102;&#25552;&#20379;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20043;&#22806;&#65292;&#36825;&#20123;&#21518;&#39564;&#20998;&#24067;&#36824;&#20801;&#35768;&#22312;&#22270;&#20687;&#26816;&#32034;&#20013;&#35745;&#31639;&#21487;&#20449;&#21306;&#38388;&#12290;&#23427;&#20204;&#21253;&#25324;&#20855;&#26377;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#21516;&#30340;&#28508;&#21464;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#65288;DirectUQ&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30452;&#25509;&#36755;&#20986;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#39118;&#38505;&#36793;&#30028;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2302.02420</link><description>&lt;p&gt;
&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Uncertainty Quantification. (arXiv:2302.02420v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#65288;DirectUQ&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30452;&#25509;&#36755;&#20986;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#39118;&#38505;&#36793;&#30028;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26131;&#20110;&#35757;&#32451;&#65292;&#20294;&#20250;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65307;&#32780;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#37327;&#21270;&#65292;&#20294;&#20248;&#21270;&#23427;&#20204;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#8220;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#8221;&#65288;DirectUQ&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#36755;&#20986;&#26368;&#21518;&#19968;&#23618;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#12290;DirectUQ&#21487;&#20197;&#23548;&#20986;&#20026;&#19968;&#20010;&#26367;&#20195;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#22240;&#27492;&#20174;&#33853;&#21333;&#21464;&#20998;&#25512;&#29702;&#20013;&#33719;&#30410;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;&#38750;&#27010;&#29575;&#27169;&#22411;&#19968;&#26679;&#65292;DirectUQ&#20855;&#26377;&#31616;&#21333;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;Rademacher&#22797;&#26434;&#24615;&#20026;&#27169;&#22411;&#25552;&#20379;&#39118;&#38505;&#36793;&#30028;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DirectUQ&#21644;DirectUQ&#38598;&#25104;&#25552;&#20379;&#20102;&#26102;&#38388;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#30340;&#33391;&#22909;&#24179;&#34913;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional neural networks are simple to train but they produce overconfident predictions, while Bayesian neural networks provide good uncertainty quantification but optimizing them is time consuming. This paper introduces a new approach, direct uncertainty quantification (DirectUQ), that combines their advantages where the neural network directly outputs the mean and variance of the last layer. DirectUQ can be derived as an alternative variational lower bound, and hence benefits from collapsed variational inference that provides improved regularizers. On the other hand, like non-probabilistic models, DirectUQ enjoys simple training and one can use Rademacher complexity to provide risk bounds for the model. Experiments show that DirectUQ and ensembles of DirectUQ provide a good tradeoff in terms of run time and uncertainty quantification, especially for out of distribution data.
&lt;/p&gt;</description></item><item><title>GAFM&#26159;&#19968;&#31181;&#29992;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#26631;&#31614;&#20445;&#25252;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#38388;&#25509;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#26469;&#20943;&#36731;&#26799;&#24230;&#26631;&#31614;&#27844;&#28431;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02245</link><description>&lt;p&gt;
&#38754;&#21521;&#20108;&#20803;&#20998;&#31867;&#20013;&#26631;&#31614;&#20445;&#25252;&#30340;&#22522;&#20110;GAN&#30340;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GAN-based Vertical Federated Learning for Label Protection in Binary Classification. (arXiv:2302.02245v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02245
&lt;/p&gt;
&lt;p&gt;
GAFM&#26159;&#19968;&#31181;&#29992;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#26631;&#31614;&#20445;&#25252;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#38388;&#25509;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#26469;&#20943;&#36731;&#26799;&#24230;&#26631;&#31614;&#27844;&#28431;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#23398;&#20064;&#65288;SplitNN&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#20013;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20302;&#24314;&#27169;&#25928;&#29575;&#38382;&#39064;&#30340;&#24120;&#29992;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SplitNN&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#32570;&#20047;&#21152;&#23494;&#20445;&#25252;&#65292;&#22240;&#27492;&#23481;&#26131;&#20986;&#29616;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26799;&#24230;&#26631;&#31614;&#27844;&#28431;&#65288;LLG&#65289;&#12290;&#20986;&#20110;&#23545;&#20351;&#29992;&#26631;&#31614;&#35757;&#32451;&#24341;&#36215;&#30340;LLG&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#23545;&#25239;&#31446;&#30452;&#32852;&#37030;&#27169;&#22411;&#65288;GAFM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36890;&#36807;&#23558;SplitNN&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#38598;&#25104;&#26469;&#22686;&#24378;&#26631;&#31614;&#38544;&#31169;&#20445;&#25252;&#12290;GAFM&#21033;&#29992;GAN&#38388;&#25509;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#65292;&#23398;&#20064;&#26631;&#31614;&#20998;&#24067;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#26174;&#24335;&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#36731;LLG&#38382;&#39064;&#12290; GAFM&#36824;&#37319;&#29992;&#22522;&#20110;&#22122;&#22768;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#65292;GAN&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#32452;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning (splitNN) has emerged as a popular strategy for addressing the high computational costs and low modeling efficiency in Vertical Federated Learning (VFL). However, despite its popularity, vanilla splitNN lacks encryption protection, leaving it vulnerable to privacy leakage issues, especially Label Leakage from Gradients (LLG). Motivated by the LLG issue resulting from the use of labels during training, we propose the Generative Adversarial Federated Model (GAFM), a novel method designed specifically to enhance label privacy protection by integrating splitNN with Generative Adversarial Networks (GANs). GAFM leverages GANs to indirectly utilize label information by learning the label distribution rather than relying on explicit labels, thereby mitigating LLG. GAFM also employs an additional cross-entropy loss based on the noisy labels to further improve the prediction accuracy. Our ablation experiment demonstrates that the combination of GAN and the cross-entropy loss compo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#20215;&#20540;&#20998;&#35299;&#26694;&#26550;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23436;&#20840;&#25682;&#24323;&#20102;&#20010;&#20307;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21069;&#25552;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.02180</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#20215;&#20540;&#20998;&#35299;&#26694;&#26550;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2302.02180v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#20215;&#20540;&#20998;&#35299;&#26694;&#26550;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23436;&#20840;&#25682;&#24323;&#20102;&#20010;&#20307;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21069;&#25552;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#37117;&#36981;&#24490;&#20010;&#20307;&#20840;&#23616;&#26368;&#22823;&#20540;&#65288;IGM&#65289;&#25110;&#20854;&#21464;&#20307;&#30340;&#21407;&#21017;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#20013;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#27010;&#24565;&#30340;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#20215;&#20540;&#20998;&#35299;&#26694;&#26550;&#65292;&#23436;&#20840;&#25682;&#24323;&#20102;IGM&#21069;&#25552;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#21253;&#25324;&#33258;&#25105;&#31574;&#30053;&#20197;&#36827;&#34892;&#21160;&#20316;&#36873;&#25321;&#21644;&#26367;&#36523;&#20215;&#20540;&#20989;&#25968;&#20197;&#35299;&#20915;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#21487;&#20197;&#21033;&#29992;&#26174;&#24335;&#25628;&#32034;&#36807;&#31243;&#24573;&#30053;IGM&#20551;&#35774;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#33258;&#25105;&#25506;&#32034;&#26426;&#21046;&#65292;&#20197;&#36991;&#20813;&#31639;&#27861;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#23436;&#20840;&#19981;&#29992;IGM&#30340;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#21327;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value decomposition methods have gained popularity in the field of cooperative multi-agent reinforcement learning. However, almost all existing methods follow the principle of Individual Global Max (IGM) or its variants, which limits their problem-solving capabilities. To address this, we propose a dual self-awareness value decomposition framework, inspired by the notion of dual self-awareness in psychology, that entirely rejects the IGM premise. Each agent consists of an ego policy for action selection and an alter ego value function to solve the credit assignment problem. The value function factorization can ignore the IGM assumption by utilizing an explicit search procedure. On the basis of the above, we also suggest a novel anti-ego exploration mechanism to avoid the algorithm becoming stuck in a local optimum. As the first fully IGM-free value decomposition method, our proposed framework achieves desirable performance in various cooperative tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00808</link><description>&lt;p&gt;
&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#21046;&#26465;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#24179;&#22343;&#26631;&#20934;&#27604;&#25240;&#25187;&#26631;&#20934;&#26356;&#21512;&#36866;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#24179;&#22343;&#38480;&#21046; CMDP &#30340;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#25240;&#25187;&#38480;&#21046; RL &#38382;&#39064;&#35774;&#35745;&#30340;&#31639;&#27861;&#36890;&#24120;&#22312;&#24179;&#22343; CMDP &#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#65288;ACPO&#65289;&#31639;&#27861;&#30340;&#28789;&#24863;&#26469;&#33258;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#30340;&#33879;&#21517; PPO &#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#22522;&#26412;&#30340;&#24179;&#22343; MDP &#25935;&#24863;&#24615;&#29702;&#35770;&#65292;&#28982;&#21518;&#22312;&#31639;&#27861;&#35774;&#35745;&#20013;&#20351;&#29992;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; MuJoCo &#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#24037;&#20316;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#19982;&#20854;&#20182;&#24120;&#35268;&#31639;&#27861;&#30456;&#27604;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00533</link><description>&lt;p&gt;
&#33976;&#39311;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20511;&#37492;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35270;&#35282;&#21644;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#25968;&#25454;&#30340;&#20132;&#21449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#26041;&#24046;&#20943;&#23569;&#26426;&#21046;&#65292;&#20363;&#22914;&#32479;&#19968;&#20248;&#21183;&#20272;&#35745;&#22120; (UAE) &#21644;&#19968;&#20010;&#23398;&#20064;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#26159;&#36830;&#25509;&#21040;&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#26725;&#26753;&#65292;&#36824;&#33021;&#25552;&#28860;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#29305;&#24449;&#32447;&#24615;&#35843;&#21046;&#30340;&#38543;&#26426;&#32593;&#32476;&#30910;&#21387;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#21306;&#20998;&#24230;&#30340;&#38382;&#39064;&#65292;&#22312; D4RL &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#21487;&#19982;&#38598;&#25104;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13616</link><description>&lt;p&gt;
&#38543;&#26426;&#32593;&#32476;&#30910;&#21387;&#23545;&#38450;&#27490;&#25506;&#32034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Anti-Exploration by Random Network Distillation. (arXiv:2301.13616v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#29305;&#24449;&#32447;&#24615;&#35843;&#21046;&#30340;&#38543;&#26426;&#32593;&#32476;&#30910;&#21387;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#21306;&#20998;&#24230;&#30340;&#38382;&#39064;&#65292;&#22312; D4RL &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#21487;&#19982;&#38598;&#25104;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38543;&#26426;&#32593;&#32476;&#30910;&#21387; (RND) &#22312;&#21508;&#31181;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#29992;&#20316;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24809;&#32602;&#36234;&#30028;&#25805;&#20316;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#26102;&#65292;&#23427;&#34987;&#35777;&#26126;&#19981;&#20855;&#26377;&#36275;&#22815;&#30340;&#21306;&#20998;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545; RND &#20808;&#39564;&#36827;&#34892;&#26420;&#32032;&#30340;&#35843;&#33410;&#36873;&#25321;&#65292;&#28436;&#21592;&#26377;&#25928;&#22320;&#26368;&#23567;&#21270;&#21453;&#25506;&#32034;&#22870;&#21169;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#24182;&#19988;&#21306;&#20998;&#24230;&#19981;&#20877;&#26159;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#32447;&#24615;&#35843;&#21046; (FiLM) &#30340;&#35843;&#33410;&#26469;&#36991;&#20813;&#36825;&#31181;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22522;&#20110;&#36719;&#34892;&#20026;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#26080;&#38598;&#25104;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312; D4RL &#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#19982;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20248;&#20110;&#26080;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning. In this paper, we revisit these results and show that, with a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL benchmark, showing that it is capable of achieving performance comparable to ensemble-based methods and outperforming ensemble-free approaches by a wide margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRC&#30340;&#26032;&#22411;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12379</link><description>&lt;p&gt;
FedRC&#65306;&#36890;&#36807;&#40065;&#26834;&#32858;&#31867;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering. (arXiv:2301.12379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRC&#30340;&#26032;&#22411;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#26679;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20445;&#30041;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#65292;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#20250;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24403;&#23458;&#25143;&#31471;&#20043;&#38388;&#20986;&#29616;&#20998;&#24067;&#36716;&#31227;&#26102;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#65292;&#20294;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#21516;&#26102;&#21457;&#29983;&#22810;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#65292;&#20363;&#22914;&#29305;&#24449;&#20998;&#24067;&#36716;&#31227;&#12289;&#26631;&#31614;&#20998;&#24067;&#36716;&#31227;&#21644;&#27010;&#24565;&#36716;&#31227;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#20840;&#23616;&#24615;&#33021;&#20173;&#28982;&#26159;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#26679;&#20998;&#24067;&#36716;&#31227;&#21516;&#26102;&#21457;&#29983;&#26102;&#25152;&#24102;&#26469;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#21407;&#21017;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#35299;&#20915;&#32858;&#31867;&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861;&#26694;&#26550;&#8212;&#8212;FedRC&#65292;&#23427;&#36981;&#24490;&#25105;&#20204;&#25552;&#20986;&#30340;&#32858;&#31867;&#21407;&#21017;&#65292;&#36890;&#36807;&#21253;&#21547;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#29616;&#26377;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm that safeguards privacy by retaining client data on edge devices. However, optimizing FL in practice can be challenging due to the diverse and heterogeneous nature of the learning system. Though recent research has focused on improving the optimization of FL when distribution shifts occur among clients, ensuring global performance when multiple types of distribution shifts occur simultaneously among clients -- such as feature distribution shift, label distribution shift, and concept shift -- remain under-explored.  In this paper, we identify the learning challenges posed by the simultaneous occurrence of diverse distribution shifts and propose a clustering principle to overcome these challenges. Through our research, we find that existing methods failed to address the clustering principle. Therefore, we propose a novel clustering algorithm framework, dubbed as FedRC, which adheres to our proposed clustering principle by incorporati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26469;&#35748;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36870;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#36807;&#20110;&#19981;&#21464;&#24615;&#30340;&#29616;&#35937;&#22312;&#20004;&#20010;&#24773;&#22659;&#19979;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#22914;&#20309;&#29992;&#36825;&#20123;&#21457;&#29616;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21487;&#36870;&#24615;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.11783</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Certified Invertibility in Neural Networks via Mixed-Integer Programming. (arXiv:2301.11783v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26469;&#35748;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36870;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#36807;&#20110;&#19981;&#21464;&#24615;&#30340;&#29616;&#35937;&#22312;&#20004;&#20010;&#24773;&#22659;&#19979;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#22914;&#20309;&#29992;&#36825;&#20123;&#21457;&#29616;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21487;&#36870;&#24615;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#24494;&#23567;&#20294;&#24433;&#21709;&#26174;&#33879;&#30340;&#25200;&#21160;&#21487;&#20197;&#25913;&#21464;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#30456;&#21453;&#65292;&#21487;&#33021;&#23384;&#22312;&#22823;&#30340;&#12289;&#26377;&#24847;&#20041;&#30340;&#25200;&#21160;&#65292;&#20294;&#19981;&#24433;&#21709;&#32593;&#32476;&#30340;&#21028;&#26029;&#65288;&#36807;&#20110;&#19981;&#21464;&#24615;&#65289;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#36825;&#21518;&#19968;&#31181;&#29616;&#35937;&#22312;&#20004;&#20010;&#24773;&#22659;&#19979;&#30340;&#34920;&#29616;&#65306;&#65288;a&#65289;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#31995;&#32479;&#35782;&#21035;&#65292;&#20197;&#21450;&#65288;b&#65289;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#26657;&#20934;&#21040;&#21478;&#19968;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#20248;&#21270;&#30340;&#26041;&#24335;&#26469;&#30740;&#31350;&#38750;&#21487;&#36870;&#24615;&#65292;&#20854;&#20013;&#20840;&#23616;&#35299;&#36890;&#36807;&#19982;&#38750;&#21487;&#36870;&#24615;&#36793;&#30028;&#30340;&#36317;&#31163;&#26469;&#24230;&#37327;&#32593;&#32476;&#39044;&#27979;&#30340;&#8220;&#23433;&#20840;&#24615;&#8221;&#12290;&#25105;&#20204;&#38024;&#23545;ReLU&#32593;&#32476;&#21644;$L_p$&#33539;&#25968;&#65288;$p = 1,2,\infty$&#65289;&#26500;&#24314;&#20102;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#65292;&#36825;&#20123;&#35268;&#21010;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#21160;&#24577;&#31995;&#32479;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21487;&#36870;&#24615;&#35748;&#35777;&#65292;&#20363;&#22914;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are known to be vulnerable to adversarial attacks, which are small, imperceptible perturbations that can significantly alter the network's output. Conversely, there may exist large, meaningful perturbations that do not affect the network's decision (excessive invariance). In our research, we investigate this latter phenomenon in two contexts: (a) discrete-time dynamical system identification, and (b) the calibration of a neural network's output to that of another network. We examine noninvertibility through the lens of mathematical optimization, where the global solution measures the ``safety" of the network predictions by their distance from the non-invertibility boundary. We formulate mixed-integer programs (MIPs) for ReLU networks and $L_p$ norms ($p=1,2,\infty$) that apply to neural network approximators of dynamical systems. We also discuss how our findings can be useful for invertibility certification in transformations between neural networks, e.g. between differ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11719</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#32435;&#20837;&#25991;&#26723;&#25688;&#35201;&#29983;&#25104;&#20013;&#65306;&#22522;&#20110;GPT-2&#30340;&#21069;&#32512;&#35843;&#25972;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#22312;&#25991;&#26723;&#25688;&#35201;&#25216;&#26415;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#20294;&#26159;&#29983;&#25104;&#30340;&#25688;&#35201;&#21644;&#21407;&#22987;&#25991;&#26412;&#20043;&#38388;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#20173;&#28982;&#26102;&#26377;&#21457;&#29983;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37319;&#29992;&#25552;&#31034;&#26469;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#21069;&#32512;&#35843;&#25972;&#65292;&#23427;&#20351;&#29992;&#19968;&#32452;&#21487;&#35757;&#32451;&#30340;&#36830;&#32493;&#21069;&#32512;&#25552;&#31034;&#21644;&#31163;&#25955;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#24110;&#21161;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35757;&#32451;&#30340;&#21069;&#32512;&#21487;&#20197;&#24110;&#21161;&#25688;&#35201;&#27169;&#22411;&#20934;&#30830;&#22320;&#20174;&#31163;&#25955;&#25552;&#31034;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#36825;&#20123;&#25688;&#35201;&#22312;&#20107;&#23454;&#19978;&#19982;&#31163;&#25955;&#25552;&#31034;&#19968;&#33268;&#12290;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;ROUGE&#25913;&#36827;&#34920;&#26126;&#65292;&#23558;&#20107;&#23454;&#30693;&#35782;&#26126;&#30830;&#22320;&#28155;&#21152;&#21040;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#33021;&#22815;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#25913;&#21892;&#25972;&#20010;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11375</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Neural networks learn to magnify areas near decision boundaries. (arXiv:2301.11375v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11375
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#33021;&#22815;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#25913;&#21892;&#25972;&#20010;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#22914;&#20309;&#22609;&#36896;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#22270;&#35825;&#23548;&#30340;&#40654;&#26364;&#20960;&#20309;&#12290;&#22312;&#23485;&#24230;&#20026;&#26080;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#36755;&#20837;&#31354;&#38388;&#19978;&#24341;&#23548;&#39640;&#24230;&#23545;&#31216;&#30340;&#24230;&#37327;&#12290;&#35757;&#32451;&#20998;&#31867;&#20219;&#21153;&#30340;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#25918;&#22823;&#20102;&#27839;&#20915;&#31574;&#36793;&#30028;&#30340;&#23616;&#37096;&#21306;&#22495;&#12290;&#36825;&#20123;&#21464;&#21270;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#25163;&#21160;&#35843;&#25972;&#26680;&#26041;&#27861;&#20197;&#25913;&#21892;&#27867;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how training molds the Riemannian geometry induced by neural network feature maps. At infinite width, neural networks with random parameters induce highly symmetric metrics on input space. Feature learning in networks trained to perform classification tasks magnifies local areas along decision boundaries. These changes are consistent with previously proposed geometric approaches for hand-tuning of kernel methods to improve generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#35774;&#35745;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2301.10774</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#19977;&#32423;&#32467;&#26500;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design. (arXiv:2301.10774v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#35774;&#35745;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#24050;&#22312;&#25581;&#31034;&#29983;&#29289;&#22823;&#20998;&#23376;&#30340;&#19968;&#32423;&#24207;&#21015;&#19982;&#19977;&#32423;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#29305;&#23450;&#19977;&#32423;&#32467;&#26500;&#35774;&#35745;RNA&#24207;&#21015;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#30340;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#24443;&#24213;&#25506;&#32034;&#20102;&#34507;&#30333;&#36136;&#20013;&#32467;&#26500;&#21040;&#24207;&#21015;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;RNA&#35774;&#35745;&#20173;&#38754;&#20020;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#34429;&#28982;RNA&#19982;&#34507;&#30333;&#36136;&#20849;&#20139;&#31867;&#20284;&#30340;&#32467;&#26500;&#32452;&#20998;&#65292;&#20294;&#30452;&#25509;&#23558;&#34507;&#30333;&#36136;&#35774;&#35745;&#26041;&#27861;&#31227;&#26893;&#21040;RNA&#35774;&#35745;&#20013;&#21364;&#26080;&#27861;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;RNA&#19977;&#32423;&#32467;&#26500;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#32467;&#26500;&#34920;&#31034;&#30340;&#22810;&#20010;&#23618;&#27425;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural repre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20132;&#27969;&#30005;&#26368;&#20248;&#28526;&#27969;&#35745;&#31639;&#30340;&#32500;&#24230;&#65292;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#36755;&#20986;&#21487;&#20197;&#29992;&#20110;&#28909;&#21551;&#21160;&#31934;&#30830;&#30340;AC&#27714;&#35299;&#22120;&#20197;&#24674;&#22797;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.08840</link><description>&lt;p&gt;
&#21387;&#32553;&#20248;&#21270;&#23398;&#20064;&#29992;&#20110;&#20132;&#27969;&#30005;&#26368;&#20248;&#28526;&#27969;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Compact Optimization Learning for AC Optimal Power Flow. (arXiv:2301.08840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20132;&#27969;&#30005;&#26368;&#20248;&#28526;&#27969;&#35745;&#31639;&#30340;&#32500;&#24230;&#65292;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#36755;&#20986;&#21487;&#20197;&#29992;&#20110;&#28909;&#21551;&#21160;&#31934;&#30830;&#30340;AC&#27714;&#35299;&#22120;&#20197;&#24674;&#22797;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#26368;&#20248;&#28526;&#27969;&#35745;&#31639;&#65288;OPF&#65289;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#23398;&#20064;&#36755;&#20837;/&#36755;&#20986;&#26144;&#23556;&#30340;&#26041;&#27861;&#30001;&#20110;&#36755;&#20986;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#32780;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26368;&#20248;&#35299;&#30340;&#31354;&#38388;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#21387;&#32553;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20027;&#35201;&#25104;&#20998;&#30340;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#28982;&#21518;&#23558;&#21521;&#37327;&#36716;&#25442;&#20026;&#21407;&#22987;&#36755;&#20986;&#31354;&#38388;&#12290;&#36825;&#31181;&#21387;&#32553;&#22823;&#22823;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#21387;&#32553;&#23398;&#20064;&#22312;PGLib&#30340;&#21508;&#31181;&#27979;&#35797;&#29992;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26368;&#39640;&#21487;&#36798;30,000&#20010;&#24635;&#32447;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#21387;&#32553;&#23398;&#20064;&#30340;&#36755;&#20986;&#21487;&#20197;&#29992;&#20110;&#28909;&#21551;&#21160;&#31934;&#30830;&#30340;AC&#27714;&#35299;&#22120;&#20197;&#24674;&#22797;&#21487;&#34892;&#24615;&#65292;&#24182;&#24102;&#26469;&#26174;&#30528;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reconsiders end-to-end learning approaches to the Optimal Power Flow (OPF). Existing methods, which learn the input/output mapping of the OPF, suffer from scalability issues due to the high dimensionality of the output space. This paper first shows that the space of optimal solutions can be significantly compressed using principal component analysis (PCA). It then proposes Compact Learning, a new method that learns in a subspace of the principal components before translating the vectors into the original output space. This compression reduces the number of trainable parameters substantially, improving scalability and effectiveness. Compact Learning is evaluated on a variety of test cases from the PGLib with up to 30,000 buses. The paper also shows that the output of Compact Learning can be used to warm-start an exact AC solver to restore feasibility, while bringing significant speed-ups.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;A-NeSI&#30340;&#26032;&#39062;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#65292;&#33021;&#22815;&#20445;&#35777;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;PNL&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;</title><link>http://arxiv.org/abs/2212.12393</link><description>&lt;p&gt;
A-NeSI: &#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;A-NeSI&#30340;&#26032;&#39062;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#65292;&#33021;&#22815;&#20445;&#35777;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;PNL&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#65288;PNL&#65289;&#26694;&#26550;&#65292;&#22914;DeepProbLog&#65292;&#25191;&#34892;&#25351;&#25968;&#26102;&#38388;&#30340;&#31934;&#30830;&#25512;&#29702;&#65292;&#38480;&#21046;&#20102;PNL&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36817;&#20284;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#65288;A-NeSI&#65289;&#65306;&#19968;&#31181;&#26032;&#30340;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#25512;&#29702;&#12290;A-NeSI 1) &#22312;&#19981;&#25913;&#21464;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#25191;&#34892;&#36817;&#20284;&#25512;&#29702;&#65307;2) &#20351;&#29992;&#30001;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;3) &#21487;&#20197;&#29983;&#25104;&#26377;&#20851;&#39044;&#27979;&#30340;&#31526;&#21495;&#35299;&#37322;&#65307;4) &#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#38388;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#65292;&#36825;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;A-NeSI&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#25351;&#25968;&#32452;&#21512;&#25193;&#23637;&#30340;&#19977;&#31181;&#31070;&#32463;&#31526;&#21495;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;A-NeSI&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#32780;&#27809;&#26377;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#35265;&#20449;&#29992;&#20998;&#37197;&#36825;&#31181;&#20998;&#37197;&#20449;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22240;&#26524;&#32467;&#26500;&#30340;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.11636</link><description>&lt;p&gt;
&#26397;&#21521;&#22240;&#26524;&#36131;&#20219;&#21010;&#20998;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Credit Assignment. (arXiv:2212.11636v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#35265;&#20449;&#29992;&#20998;&#37197;&#36825;&#31181;&#20998;&#37197;&#20449;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22240;&#26524;&#32467;&#26500;&#30340;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22914;&#20309;&#26681;&#25454;&#34892;&#20026;&#30340;&#36129;&#29486;&#36866;&#24403;&#22320;&#23558;&#20449;&#29992;&#20998;&#37197;&#32473;&#26410;&#26469;&#30340;&#32467;&#26524;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#20449;&#29992;&#20998;&#37197;&#26041;&#27861;&#30340;&#20551;&#35774;&#22312;&#20915;&#31574;&#25928;&#26524;&#19981;&#31435;&#21363;&#26174;&#29616;&#30340;&#20219;&#21153;&#20013;&#20855;&#26377;&#21155;&#21183;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21482;&#33021;&#35780;&#20272;&#20195;&#29702;&#24050;&#36873;&#25321;&#30340;&#21160;&#20316;&#65292;&#22240;&#27492;&#25928;&#29575;&#26497;&#20302;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21518;&#35265;&#20449;&#29992;&#20998;&#37197;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29615;&#22659;&#22240;&#26524;&#32467;&#26500;&#30340;&#20998;&#35299;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#21518;&#35265;&#20449;&#29992;&#20998;&#37197;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adequately assigning credit to actions for future outcomes based on their contributions is a long-standing open challenge in Reinforcement Learning. The assumptions of the most commonly used credit assignment method are disadvantageous in tasks where the effects of decisions are not immediately evident. Furthermore, this method can only evaluate actions that have been selected by the agent, making it highly inefficient. Still, no alternative methods have been widely adopted in the field. Hindsight Credit Assignment is a promising, but still unexplored candidate, which aims to solve the problems of both long-term and counterfactual credit assignment. In this thesis, we empirically investigate Hindsight Credit Assignment to identify its main benefits, and key points to improve. Then, we apply it to factored state representations, and in particular to state representations based on the causal structure of the environment. In this setting, we propose a variant of Hindsight Credit Assignmen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#36319;&#36394;&#21160;&#24577;&#30446;&#26631;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31574;&#30053;&#21644;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#30446;&#26631;&#29109;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#26799;&#24230;&#36816;&#31639;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.01498</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#30340; SE(3) &#36712;&#36857;&#19978;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#30446;&#26631;&#36319;&#36394;&#30340;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Learning for Active Target Tracking over Continuous SE(3) Trajectories. (arXiv:2212.01498v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#36319;&#36394;&#21160;&#24577;&#30446;&#26631;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31574;&#30053;&#21644;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#30446;&#26631;&#29109;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#26799;&#24230;&#36816;&#31639;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#25645;&#36733;&#26377;&#38480;&#35270;&#37326;&#20256;&#24863;&#22120;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36319;&#36394;&#21160;&#24577;&#30446;&#26631;&#12290;&#20219;&#21153;&#26159;&#33719;&#21462;&#36830;&#32493;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#25910;&#38598;&#20256;&#24863;&#22120;&#27979;&#37327;&#32467;&#26524;&#65292;&#20174;&#32780;&#20943;&#23569;&#30446;&#26631;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#30001;&#30446;&#26631;&#20998;&#24067;&#29109;&#24230;&#37327;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#26426;&#22120;&#20154;&#30340; SE(3) &#23039;&#24577;&#12289;&#32852;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#22343;&#20540;&#21521;&#37327;&#21644;&#20449;&#24687;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#23618;&#26469;&#22788;&#29702;&#21487;&#21464;&#25968;&#37327;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#36824;&#26126;&#30830;&#23548;&#20986;&#20102;&#30446;&#26631;&#29109;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#20801;&#35768;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel model-based policy gradient algorithm for tracking dynamic targets using a mobile robot, equipped with an onboard sensor with limited field of view. The task is to obtain a continuous control policy for the mobile robot to collect sensor measurements that reduce uncertainty in the target states, measured by the target distribution entropy. We design a neural network control policy with the robot $SE(3)$ pose and the mean vector and information matrix of the joint target distribution as inputs and attention layers to handle variable numbers of targets. We also derive the gradient of the target entropy with respect to the network parameters explicitly, allowing efficient model-based policy gradient optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#23398;&#20064;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#29366;&#24577;&#35266;&#27979;&#65292;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#37096;&#20998;&#24050;&#30693;&#21644;&#23436;&#20840;&#26410;&#30693;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#40065;&#26834;&#35266;&#27979;&#22120;&#65292;&#25552;&#39640;&#35266;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#20351;&#20854;&#26356;&#36866;&#21512;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00866</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#23398;&#20064;&#40065;&#26834;&#29366;&#24577;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Robust State Observers using Neural ODEs (longer version). (arXiv:2212.00866v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#23398;&#20064;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#29366;&#24577;&#35266;&#27979;&#65292;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#37096;&#20998;&#24050;&#30693;&#21644;&#23436;&#20840;&#26410;&#30693;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#40065;&#26834;&#35266;&#27979;&#22120;&#65292;&#25552;&#39640;&#35266;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#20351;&#20854;&#26356;&#36866;&#21512;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#29366;&#24577;&#35266;&#27979;&#22120;&#35774;&#35745;&#26041;&#27861;&#65292;&#23398;&#20064;&#37096;&#20998;&#24050;&#30693;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;Luenberger&#35266;&#27979;&#22120;&#21450;&#20854;&#38750;&#32447;&#24615;&#25193;&#23637;&#65292;&#21516;&#26102;&#23398;&#20064;&#23436;&#20840;&#26410;&#30693;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;Kazantzis-Kravaris-Luenberger (KKL)&#35266;&#27979;&#22120;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#21487;&#35843;&#33410;&#30340;KKL&#35266;&#27979;&#22120;&#30340;&#35774;&#35745;&#19982;&#20854;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#23558;&#20854;&#29992;&#20316;&#25913;&#21892;&#35757;&#32451;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#35266;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relying on recent research results on Neural ODEs, this paper presents a methodology for the design of state observers for nonlinear systems based on Neural ODEs, learning Luenberger-like observers and their nonlinear extension (Kazantzis-Kravaris-Luenberger (KKL) observers) for systems with partially-known nonlinear dynamics and fully unknown nonlinear dynamics, respectively. In particular, for tuneable KKL observers, the relationship between the design of the observer and its trade-off between convergence speed and robustness is analysed and used as a basis for improving the robustness of the learning-based observer in training. We illustrate the advantages of this approach in numerical simulations.
&lt;/p&gt;</description></item><item><title>NEVIS'22&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#20250;&#35758;&#19977;&#21313;&#24180;&#26469;&#22343;&#21248;&#21462;&#26679;&#30340;100&#20010;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#21453;&#26144;&#30740;&#31350;&#31038;&#21306;&#30340;&#36827;&#23637;&#21644;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.11747</link><description>&lt;p&gt;
NEVIS'22: &#20174;30&#24180;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#20013;&#21462;&#26679;&#30340;100&#20010;&#20219;&#21153;&#27969;
&lt;/p&gt;
&lt;p&gt;
NEVIS'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research. (arXiv:2211.11747v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11747
&lt;/p&gt;
&lt;p&gt;
NEVIS'22&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#20250;&#35758;&#19977;&#21313;&#24180;&#26469;&#22343;&#21248;&#21462;&#26679;&#30340;100&#20010;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#21453;&#26144;&#30740;&#31350;&#31038;&#21306;&#30340;&#36827;&#23637;&#21644;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#20849;&#21516;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30340;&#31639;&#27861;&#21644;&#27169;&#22411;&#12290;&#26356;&#36827;&#19968;&#27493;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#37027;&#20123;&#19981;&#26029;&#36866;&#24212;&#24182;&#36890;&#36807;&#24688;&#24403;&#22320;&#20256;&#36882;&#24050;&#33719;&#24471;&#30693;&#35782;&#36880;&#28176;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#38500;&#20102;&#30740;&#31350;&#23398;&#20064;&#31639;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#36825;&#31867;&#27169;&#22411;&#30340;&#36824;&#38754;&#20020;&#35768;&#22810;&#22256;&#38590;&#65292;&#20363;&#22914;&#36873;&#25321;&#23398;&#20064;&#21327;&#35758;&#12289;&#25104;&#21151;&#24230;&#37327;&#21644;&#39564;&#35777;&#30740;&#31350;&#20551;&#35774;&#25152;&#38656;&#30340;&#25968;&#25454;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NEVIS'22&#65288;Never-Ending VIsual-classification Stream&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#30001;100&#20010;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#32452;&#25104;&#65292;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#20250;&#35758;&#20013;&#22343;&#21248;&#21462;&#26679;&#30340;&#35770;&#25991;&#20013;&#25552;&#21462;&#32780;&#26469;&#65292;&#36328;&#36234;&#20102;&#36807;&#21435;&#19977;&#21313;&#24180;&#12290;&#24471;&#21040;&#30340;&#25968;&#25454;&#27969;&#21453;&#26144;&#20102;&#30740;&#31350;&#31038;&#21306;&#35748;&#20026;&#26377;&#24847;&#20041;&#30340;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A shared goal of several machine learning communities like continual learning, meta-learning and transfer learning, is to design algorithms and models that efficiently and robustly adapt to unseen tasks. An even more ambitious goal is to build models that never stop adapting, and that become increasingly more efficient through time by suitably transferring the accrued knowledge. Beyond the study of the actual learning algorithm and model architecture, there are several hurdles towards our quest to build such models, such as the choice of learning protocol, metric of success and data needed to validate research hypotheses. In this work, we introduce the Never-Ending VIsual-classification Stream (NEVIS'22), a benchmark consisting of a stream of over 100 visual classification tasks, sorted chronologically and extracted from papers sampled uniformly from computer vision proceedings spanning the last three decades. The resulting stream reflects what the research community thought was meanin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#20294;&#24778;&#20154;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24191;&#31639;&#27861;&#65292;&#22522;&#20110;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#23450;&#20301;&#21487;&#20197;&#20943;&#23569;&#27979;&#37327;&#27425;&#25968;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2211.10790</link><description>&lt;p&gt;
&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23450;&#20301;&#30340;&#31616;&#21333;&#26377;&#25928;&#22686;&#24191;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple and Effective Augmentation Methods for CSI Based Indoor Localization. (arXiv:2211.10790v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#20294;&#24778;&#20154;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24191;&#31639;&#27861;&#65292;&#22522;&#20110;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#23450;&#20301;&#21487;&#20197;&#20943;&#23569;&#27979;&#37327;&#27425;&#25968;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19982;GPS&#22312;&#23460;&#22806;&#29615;&#22659;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#19981;&#21516;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#31283;&#20581;&#19988;&#20960;&#20046;&#26222;&#36941;&#36866;&#29992;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#25104;&#20026;&#23454;&#29616;&#20934;&#30830;&#23460;&#20869;&#23450;&#20301;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#25361;&#25112;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25968;&#25454;&#25910;&#38598;&#31243;&#24207;&#25104;&#26412;&#39640;&#26114;&#65292;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#30340;&#23460;&#20869;&#29615;&#22659;&#36827;&#34892;&#24191;&#27867;&#30340;&#27979;&#37327;&#21644;&#26631;&#35760;&#36807;&#31243;&#12290;&#25968;&#25454;&#22686;&#24191;&#65288;DA&#65289;&#21487;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#65292;&#23427;&#26159;&#25193;&#22823;ML&#25968;&#25454;&#38598;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#20351;ML&#31995;&#32479;&#26356;&#21152;&#31283;&#20581;&#65292;&#24182;&#22686;&#21152;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#20294;&#24778;&#20154;&#26377;&#25928;&#30340;DA&#31639;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#23450;&#20301;&#65292;&#24182;&#24471;&#21040;&#20102;&#29289;&#29702;&#32771;&#34385;&#30340;&#28608;&#21169;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#31934;&#24230;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#27979;&#37327;&#27425;&#25968;&#21487;&#33021;&#20943;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#21464;&#25442;&#30340;&#31639;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor localization is a challenging task. Compared to outdoor environments where GPS is dominant, there is no robust and almost-universal approach. Recently, machine learning (ML) has emerged as the most promising approach for achieving accurate indoor localization. Nevertheless, its main challenge is requiring large datasets to train the neural networks. The data collection procedure is costly and laborious, requiring extensive measurements and labeling processes for different indoor environments. The situation can be improved by Data Augmentation (DA), a general framework to enlarge the datasets for ML, making ML systems more robust and increasing their generalization capabilities. This paper proposes two simple yet surprisingly effective DA algorithms for channel state information (CSI) based indoor localization motivated by physical considerations. We show that the number of measurements for a given accuracy requirement may be decreased by an order of magnitude. Specifically, we d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#37325;&#35201;&#24212;&#29992;&#30340;&#40654;&#26364;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#38750;&#20984;&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20998;&#26512;&#20102;&#31639;&#27861;&#24471;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.02163</link><description>&lt;p&gt;
&#19968;&#31181;&#40654;&#26364;ADMM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Riemannian ADMM. (arXiv:2211.02163v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#37325;&#35201;&#24212;&#29992;&#30340;&#40654;&#26364;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#38750;&#20984;&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20998;&#26512;&#20102;&#31639;&#27861;&#24471;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#40654;&#26364;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#26159;&#22312;&#29615;&#31354;&#38388;&#20013;&#30340;&#20809;&#28369;&#20989;&#25968;&#21644;&#38750;&#20809;&#28369;&#20989;&#25968;&#20043;&#21644;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#31232;&#30095;&#35889;&#32858;&#31867;&#21644;&#27491;&#20132;&#23383;&#20856;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40654;&#26364;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#31867;&#38382;&#39064;&#12290;&#27599;&#27425;&#36845;&#20195;&#25105;&#20204;&#37319;&#29992;&#26131;&#20110;&#35745;&#31639;&#30340;&#27493;&#39588;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#31639;&#27861;&#24471;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#38382;&#39064;&#30340;ADMM&#26041;&#27861;&#35201;&#20040;&#19981;&#20801;&#35768;&#38750;&#20984;&#32422;&#26463;&#38598;&#21512;&#65292;&#35201;&#20040;&#19981;&#20801;&#35768;&#38750;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#21453;&#20043;&#65292;&#25105;&#20204;&#30340;&#22797;&#26434;&#24230;&#32467;&#26524;&#36866;&#29992;&#20110;&#21516;&#26102;&#20855;&#26377;&#38750;&#20809;&#28369;&#30446;&#26631;&#21644;&#27969;&#24418;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a class of Riemannian optimization problems where the objective is the sum of a smooth function and a nonsmooth function, considered in the ambient space. This class of problems finds important applications in machine learning and statistics such as the sparse principal component analysis, sparse spectral clustering, and orthogonal dictionary learning. We propose a Riemannian alternating direction method of multipliers (ADMM) to solve this class of problems. Our algorithm adopts easily computable steps in each iteration. The iteration complexity of the proposed algorithm for obtaining an $\epsilon$-stationary point is analyzed under mild assumptions. Existing ADMM for solving nonconvex problems either does not allow nonconvex constraint set, or does not allow nonsmooth objective function. In contrast, our complexity result is established for problems with simultaneous nonsmooth objective and manifold constraint. Numerical experiments are conducted to demonstrate the advanta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65292;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#38382;&#39064;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2211.00980</link><description>&lt;p&gt;
&#22312;&#23376;&#27169;&#26368;&#22823;&#21270;&#20013;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#65288;&#25216;&#26415;&#25253;&#21578;&#65289;
&lt;/p&gt;
&lt;p&gt;
Balancing Utility and Fairness in Submodular Maximization (Technical Report). (arXiv:2211.00980v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65292;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#38382;&#39064;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#20989;&#25968;&#26368;&#22823;&#21270;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#27719;&#24635;&#12289;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#21644;&#25512;&#33616;&#31561;&#12290;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#35299;&#65292;&#20351;&#24471;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#65292;&#25928;&#29992;&#20989;&#25968;&#26159;&#21333;&#35843;&#23376;&#27169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24179;&#22343;&#25928;&#29992;&#26368;&#22823;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#32676;&#20307;&#30001;&#20960;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#32452;&#32452;&#25104;&#26102;&#65292;&#21478;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#25928;&#29992;&#26159;&#21542;&#20844;&#24179;&#22320;&#20998;&#37197;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#20013;&#12290;&#34429;&#28982;&#25928;&#29992;&#21644;&#20844;&#24179;&#30446;&#26631;&#37117;&#26159;&#21487;&#21462;&#30340;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20114;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#22914;&#20309;&#19968;&#36215;&#20248;&#21270;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65288;BSM&#65289;&#65292;&#20197;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular function maximization is a fundamental combinatorial optimization problem with plenty of applications -- including data summarization, influence maximization, and recommendation. In many of these problems, the goal is to find a solution that maximizes the average utility over all users, for each of whom the utility is defined by a monotone submodular function. However, when the population of users is composed of several demographic groups, another critical problem is whether the utility is fairly distributed across different groups. Although the \emph{utility} and \emph{fairness} objectives are both desirable, they might contradict each other, and, to the best of our knowledge, little attention has been paid to optimizing them jointly.  In this paper, we propose a new problem called \emph{Bicriteria Submodular Maximization} (BSM) to strike a balance between utility and fairness. Specifically, it requires finding a fixed-size solution to maximize the utility function, subject
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SMP&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.06210</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SMP&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20811;&#26381;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36807;&#20110;&#21442;&#25968;&#21270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24191;&#27867;&#22320;&#20351;&#29992;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#21644;&#30452;&#25509;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#30452;&#25509;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#26435;&#37325;&#12290;&#20808;&#21069;&#30340;&#19968;&#38454;&#26041;&#27861;&#25104;&#21151;&#22320;&#23558;PLMs&#21387;&#32553;&#21040;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#34920;&#29616;&#20960;&#20046;&#19981;&#19979;&#38477;&#65292;&#22914;&#36816;&#21160;&#21098;&#26525;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#19968;&#38454;&#20449;&#24687;&#26469;&#21098;&#26525;PLMs&#65292;&#21516;&#26102;&#24494;&#35843;&#20854;&#20313;&#30340;&#26435;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#19968;&#38454;&#21098;&#26525;&#65292;&#24494;&#35843;&#26159;&#22810;&#20313;&#30340;&#65292;&#22240;&#20026;&#19968;&#38454;&#21098;&#26525;&#36275;&#20197;&#23558;PLMs&#25910;&#25947;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#22312;&#36825;&#20010;&#21021;&#34935;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#20351;PLMs&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#33945;&#29256;&#20989;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;SMP&#12290;&#22823;&#37327;&#21508;&#31181;&#31232;&#30095;&#24230;&#27700;&#24179;&#19979;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SMP&#27604;&#19968;&#38454;&#21644;&#38646;&#38454;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24046;&#23481;&#24525;&#22240;&#23376;&#65288;VTF&#65289;&#29702;&#35770;&#65292;&#36890;&#36807;&#25490;&#21517;&#29305;&#24449;&#30340;&#26041;&#24335;&#25506;&#32034;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#26500;&#24314;&#19968;&#20010;&#22522;&#26412;&#27169;&#22411;&#21644;&#29305;&#24449;&#27169;&#22411;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#26469;&#25506;&#32034;&#25152;&#26377;&#34920;&#29616;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#24212;&#29992;&#20110;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.13858</link><description>&lt;p&gt;
&#35299;&#37322;&#25152;&#26377;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24046;&#23481;&#24525;&#22240;&#23376;
&lt;/p&gt;
&lt;p&gt;
Variance Tolerance Factors For Interpreting ALL Neural Networks. (arXiv:2209.13858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24046;&#23481;&#24525;&#22240;&#23376;&#65288;VTF&#65289;&#29702;&#35770;&#65292;&#36890;&#36807;&#25490;&#21517;&#29305;&#24449;&#30340;&#26041;&#24335;&#25506;&#32034;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#26500;&#24314;&#19968;&#20010;&#22522;&#26412;&#27169;&#22411;&#21644;&#29305;&#24449;&#27169;&#22411;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#26469;&#25506;&#32034;&#25152;&#26377;&#34920;&#29616;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#24212;&#29992;&#20110;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#21283;&#23376;&#27169;&#22411;&#21482;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#32467;&#26524;&#65292;&#32570;&#20047;&#26377;&#20851;&#22914;&#20309;&#33719;&#24471;&#36825;&#20123;&#32467;&#26524;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#30693;&#36947;&#36755;&#20837;&#21464;&#37327;&#19982;&#36755;&#20986;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#23427;&#20204;&#30456;&#20851;&#65292;&#21487;&#20197;&#22312;&#23558;&#39044;&#27979;&#36716;&#21270;&#20026;&#23454;&#39564;&#25110;&#22312;&#21463;&#21040;&#23457;&#26597;&#26102;&#32500;&#25252;&#27169;&#22411;&#39044;&#27979;&#30340;&#20851;&#38190;&#26102;&#21051;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33324;&#24615;&#29702;&#35770;&#65292;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#21463;&#24433;&#21709;&#20989;&#25968;&#21551;&#21457;&#30340;&#26041;&#24046;&#23481;&#24525;&#22240;&#23376;&#65288;VTF&#65289;&#65292;&#20174;&#25490;&#21517;&#29305;&#24449;&#30340;&#35282;&#24230;&#35299;&#37322;&#40657;&#21283;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#22522;&#26412;&#27169;&#22411;&#21644;&#29305;&#24449;&#27169;&#22411;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#20197;&#25506;&#32034;&#21253;&#21547;&#25152;&#26377;&#34920;&#29616;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#29790;&#22763;&#20891;&#20992;&#38598;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#21019;&#24314;&#24182;&#25506;&#32034;&#20102;&#20004;&#31181;Rashomon&#38598;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#25490;&#21517;&#26041;&#27861;&#21644;&#22522;&#20110;VTF&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#22240;&#32452;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black box models only provide results for deep learning tasks, and lack informative details about how these results were obtained. Knowing how input variables are related to outputs, in addition to why they are related, can be critical to translating predictions into laboratory experiments, or defending a model prediction under scrutiny. In this paper, we propose a general theory that defines a variance tolerance factor (VTF) inspired by influence function, to interpret features in the context of black box neural networks by ranking the importance of features, and construct a novel architecture consisting of a base model and feature model to explore the feature importance in a Rashomon set that contains all well-performing neural networks. Two feature importance ranking methods in the Rashomon set and a feature selection method based on the VTF are created and explored. A thorough evaluation on synthetic and benchmark datasets is provided, and the method is applied to two real world ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#37325;&#24314;&#24863;&#20852;&#36259;&#21306;&#65288;ROI&#65289;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;U-RDBFB&#12290;&#23427;&#20855;&#26377;&#24555;&#36895;&#30340;&#37325;&#24314;&#36895;&#24230;&#21644;&#39640;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#35270;&#35282;&#36739;&#23569;&#30340;&#25130;&#26029;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2209.13264</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#35282;&#24230;&#23494;&#24230;&#26377;&#38480;&#30340;&#20852;&#36259;&#21306;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25104;&#20687;&#30340;&#28145;&#24230;&#23637;&#24320;DBFB&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Unfolding of the DBFB Algorithm with Application to ROI CT Imaging with Limited Angular Density. (arXiv:2209.13264v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#37325;&#24314;&#24863;&#20852;&#36259;&#21306;&#65288;ROI&#65289;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;U-RDBFB&#12290;&#23427;&#20855;&#26377;&#24555;&#36895;&#30340;&#37325;&#24314;&#36895;&#24230;&#21644;&#39640;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#35270;&#35282;&#36739;&#23569;&#30340;&#25130;&#26029;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#27979;&#37327;&#20013;&#37325;&#24314;&#24863;&#20852;&#36259;&#21306;&#65288;ROI&#65289;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;&#37325;&#24314;&#26041;&#27861;&#23548;&#33268;&#20855;&#26377;&#21487;&#39044;&#27979;&#29305;&#24449;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#23427;&#20204;&#32463;&#24120;&#21463;&#21040;&#32321;&#29712;&#30340;&#21442;&#25968;&#21270;&#21644;&#32531;&#24930;&#30340;&#25910;&#25947;&#30340;&#22256;&#25200;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24555;&#36895;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#36798;&#21040;&#39640;&#37325;&#24314;&#36136;&#37327;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20132;&#21449;&#28857;&#19978;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#12290;&#23427;&#20204;&#30340;&#35774;&#35745;&#21253;&#25324;&#25104;&#20687;&#31995;&#32479;&#30340;&#29289;&#29702;&#23398;&#21644;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#30340;&#27493;&#39588;&#12290;&#21463;&#36825;&#20123;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;U-RDBFB&#30340;&#23637;&#24320;&#31070;&#32463;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#37325;&#24314;ROI CT&#12290;&#30001;&#20110;&#24378;&#22823;&#30340;&#38750;&#20984;&#25968;&#25454;&#20445;&#30495;&#24230;&#39033;&#19982;&#31232;&#30095;&#35825;&#23548;&#27491;&#21017;&#21270;&#39033;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#23569;&#35270;&#35282;&#25130;&#26029;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new method for reconstructing regions of interest (ROI) from a limited number of computed tomography (CT) measurements. Classical model-based iterative reconstruction methods lead to images with predictable features. Still, they often suffer from tedious parameterization and slow convergence. On the contrary, deep learning methods are fast, and they can reach high reconstruction quality by leveraging information from large datasets, but they lack interpretability. At the crossroads of both methods, deep unfolding networks have been recently proposed. Their design includes the physics of the imaging system and the steps of an iterative optimization algorithm. Motivated by the success of these networks for various applications, we introduce an unfolding neural network called U-RDBFB designed for ROI CT reconstruction from limited data. Few-view truncated data are effectively handled thanks to a robust non-convex data fidelity term combined with a sparsity-inducing r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#20195;&#20215;&#26469;&#23398;&#20064;&#20027;&#21160;&#22320;&#26631;&#23450;&#20301;&#21644;&#25506;&#32034;&#30340;&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36716;&#25442;&#37096;&#20998;&#21487;&#35266;&#23519;&#38382;&#39064;&#20026;MDP&#65292;&#20351;&#29992;&#35270;&#37326;&#26469;&#22609;&#36896;&#22870;&#21169;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#19982;&#20027;&#21160;&#20307;&#31215;&#24314;&#22270;&#32467;&#21512;&#20197;&#20419;&#36827;&#22320;&#26631;&#23450;&#20301;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2209.12427</link><description>&lt;p&gt;
&#23398;&#20064;&#20449;&#24687;&#29702;&#35770;&#20027;&#21160;&#24863;&#30693;&#30340;&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Continuous Control Policies for Information-Theoretic Active Perception. (arXiv:2209.12427v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#20195;&#20215;&#26469;&#23398;&#20064;&#20027;&#21160;&#22320;&#26631;&#23450;&#20301;&#21644;&#25506;&#32034;&#30340;&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36716;&#25442;&#37096;&#20998;&#21487;&#35266;&#23519;&#38382;&#39064;&#20026;MDP&#65292;&#20351;&#29992;&#35270;&#37326;&#26469;&#22609;&#36896;&#22870;&#21169;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#19982;&#20027;&#21160;&#20307;&#31215;&#24314;&#22270;&#32467;&#21512;&#20197;&#20419;&#36827;&#22320;&#26631;&#23450;&#20301;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#20195;&#20215;&#26469;&#23398;&#20064;&#20027;&#21160;&#22320;&#26631;&#23450;&#20301;&#21644;&#25506;&#32034;&#30340;&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#26377;&#38480;&#24863;&#30693;&#33539;&#22260;&#20869;&#26816;&#27979;&#22320;&#26631;&#65292;&#24182;&#35299;&#20915;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#35813;&#31574;&#30053;&#26368;&#22823;&#21270;&#22320;&#26631;&#29366;&#24577;&#19982;&#20256;&#24863;&#22120;&#35266;&#27979;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#37319;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#25226;&#22320;&#26631;&#29366;&#24577;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#38382;&#39064;&#36716;&#25442;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#35270;&#37326;&#26469;&#22609;&#36896;&#22870;&#21169;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#36827;&#19968;&#27493;&#19982;&#20027;&#21160;&#20307;&#31215;&#24314;&#22270;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20419;&#36827;&#22320;&#26631;&#23450;&#20301;&#21644;&#25506;&#32034;&#12290;&#22312;&#19982;&#22522;&#20934;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#20960;&#20010;&#27169;&#25311;&#30340;&#22320;&#26631;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for learning continuous control policies for active landmark localization and exploration using an information-theoretic cost. We consider a mobile robot detecting landmarks within a limited sensing range, and tackle the problem of learning a control policy that maximizes the mutual information between the landmark states and the sensor observations. We employ a Kalman filter to convert the partially observable problem in the landmark state to Markov decision process (MDP), a differentiable field of view to shape the reward, and an attention-based neural network to represent the control policy. The approach is further unified with active volumetric mapping to promote exploration in addition to landmark localization. The performance is demonstrated in several simulated landmark localization tasks in comparison with benchmark methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#22240;&#26524;&#22270;&#26469;&#21457;&#29616;&#21644;&#34920;&#31034;&#22240;&#26524;&#20851;&#31995;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.09081</link><description>&lt;p&gt;
&#24341;&#20837;&#21464;&#20998;&#22240;&#26524;&#25512;&#29702;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#22240;&#26524;&#22270;&#26469;&#21457;&#29616;&#21644;&#34920;&#31034;&#22240;&#26524;&#20851;&#31995;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#20154;&#31867;&#26234;&#33021;&#20013;&#36798;&#25104;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#36890;&#36807;&#24635;&#32467;&#37096;&#20998;&#21040;&#25972;&#20307;&#30340;&#21442;&#25968;&#21644;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#20154;&#23454;&#29616;&#21521;&#21508;&#31181;&#30446;&#26631;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21457;&#29616;&#21644;&#34920;&#31034;&#22240;&#26524;&#20851;&#31995;&#20173;&#28982;&#26159;&#38459;&#30861;&#22240;&#26524;RL&#21457;&#23637;&#30340;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22240;&#26524;&#22270;&#32467;&#26500;&#26469;&#22686;&#24378;&#30446;&#26631;&#26465;&#20214;RL&#65288;GCRL&#65289;&#65292;&#35813;&#32467;&#26500;&#24314;&#31435;&#22312;&#23545;&#35937;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#19978;&#12290;&#25105;&#20204;&#23558;GCRL&#38382;&#39064;&#26032;&#39062;&#22320;&#21046;&#23450;&#20026;&#20855;&#26377;CG&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#30340;&#21464;&#20998;&#20284;&#28982;&#26368;&#22823;&#21270;&#12290;&#20026;&#20102;&#20248;&#21270;&#27966;&#29983;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#20132;&#26367;&#20351;&#29992;&#24178;&#39044;&#25968;&#25454;&#26469;&#20272;&#35745;CG&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#20351;&#29992;CG&#26469;&#23398;&#20064;&#36890;&#29992;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#30001;&#20110;&#32570;&#20047;&#39564;&#35777;&#25512;&#29702;&#19979;&#27867;&#21270;&#33021;&#21147;&#30340;&#20844;&#20849;&#22522;&#20934;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;...
&lt;/p&gt;
&lt;p&gt;
As a pivotal component to attaining generalizable solutions in human intelligence, reasoning provides great potential for reinforcement learning (RL) agents' generalization towards varied goals by summarizing part-to-whole arguments and discovering cause-and-effect relations. However, how to discover and represent causalities remains a huge gap that hinders the development of causal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal Graph (CG), a structure built upon the relation between objects and events. We novelly formulate the GCRL problem into variational likelihood maximization with CG as latent variables. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventional data to estimate the posterior of CG; using CG to learn generalizable models and interpretable policies. Due to the lack of public benchmarks that verify generalization capability under reasoning, we design 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#23545;&#20110;&#32477;&#23545;&#36830;&#32493;&#20110;&#21202;&#36125;&#26684;&#24230;&#37327;&#30340;&#20998;&#24067;&#65292;&#21738;&#20123;&#22522;&#20110;&#19978;&#30830;&#30028;&#30340;&#20195;&#29702;&#39118;&#38505;&#26159;&#19968;&#33268;&#30340;&#65307;&#23450;&#37327;&#35745;&#31639;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#19982;&#23545;&#25239;&#24615;&#20998;&#31867;&#39118;&#38505;&#30340;&#20851;&#31995;&#65307;&#25506;&#35752;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340; $\cH$- &#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.09099</link><description>&lt;p&gt;
&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#19968;&#33268;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Consistency of Adversarial Training for Binary Classification. (arXiv:2206.09099v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09099
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#23545;&#20110;&#32477;&#23545;&#36830;&#32493;&#20110;&#21202;&#36125;&#26684;&#24230;&#37327;&#30340;&#20998;&#24067;&#65292;&#21738;&#20123;&#22522;&#20110;&#19978;&#30830;&#30028;&#30340;&#20195;&#29702;&#39118;&#38505;&#26159;&#19968;&#33268;&#30340;&#65307;&#23450;&#37327;&#35745;&#31639;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#19982;&#23545;&#25239;&#24615;&#20998;&#31867;&#39118;&#38505;&#30340;&#20851;&#31995;&#65307;&#25506;&#35752;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340; $\cH$- &#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#35757;&#32451;&#24378;&#20581;&#20998;&#31867;&#22120;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20043;&#19968;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20854;&#20013;&#28041;&#21450;&#26368;&#23567;&#21270;&#22522;&#20110;&#19978;&#30830;&#30028;&#30340;&#20195;&#29702;&#39118;&#38505;&#12290;&#20195;&#29702;&#39118;&#38505;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#22312;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#24050;&#32463;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#65292;&#20294;&#22312;&#23545;&#25239;&#24615;&#32972;&#26223;&#19979;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21051;&#30011;&#20102;&#32477;&#23545;&#36830;&#32493;&#20110;&#21202;&#36125;&#26684;&#24230;&#37327;&#30340;&#20998;&#24067;&#20013;&#21738;&#20123;&#22522;&#20110;&#19978;&#30830;&#30028;&#30340;&#20195;&#29702;&#39118;&#38505;&#26159;&#19968;&#33268;&#30340;&#65292;&#21516;&#26102;&#25105;&#20204;&#33719;&#24471;&#20102;&#26377;&#20851;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#19982;&#23545;&#25239;&#24615;&#20998;&#31867;&#39118;&#38505;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340; $\cH$- &#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness to adversarial perturbations is of paramount concern in modern machine learning. One of the state-of-the-art methods for training robust classifiers is adversarial training, which involves minimizing a supremum-based surrogate risk. The statistical consistency of surrogate risks is well understood in the context of standard machine learning, but not in the adversarial setting. In this paper, we characterize which supremum-based surrogates are consistent for distributions absolutely continuous with respect to Lebesgue measure in binary classification. Furthermore, we obtain quantitative bounds relating adversarial surrogate risks to the adversarial classification risk. Lastly, we discuss implications for the $\cH$-consistency of adversarial training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;Transformer&#32467;&#26500;&#21644;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#26550;&#26500;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24314;&#27169;&#38899;&#39057;&#20449;&#21495;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.08297</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;Transformer&#32467;&#26500;&#24182;&#21033;&#29992;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Language Model With Million Sample Context For Raw Audio Using Transformer Architectures. (arXiv:2206.08297v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;Transformer&#32467;&#26500;&#21644;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#26550;&#26500;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24314;&#27169;&#38899;&#39057;&#20449;&#21495;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#38271;&#26399;&#20381;&#36182;&#24615;&#24314;&#27169;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#21363;&#20351;&#22312;&#23567;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#65292;&#20063;&#20250;&#20135;&#29983;&#25968;&#21313;&#19975;&#20010;&#26679;&#26412;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;Transformer&#30340;&#20986;&#29616;&#65292;&#31070;&#32463;&#32467;&#26500;&#21464;&#24471;&#25797;&#38271;&#20110;&#23545;&#38271;&#26399;&#20381;&#36182;&#24615;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20108;&#27425;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#33258;&#22238;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#27169;&#25311;&#30456;&#24403;&#22823;&#30340;&#19978;&#19979;&#25991;&#36229;&#36807;500,000&#20010;&#26679;&#26412;&#30340;&#38899;&#39057;&#27874;&#24418;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;CNN&#21069;&#31471;&#26469;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#22312;&#36825;&#20123;&#34920;&#31034;&#20043;&#19978;&#23398;&#20064;&#20381;&#36182;&#39033;&#65292;&#23436;&#20840;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#20102;&#35757;&#32451;&#65306;&#20174;&#32780;&#20801;&#35768;&#23427;&#26681;&#25454;&#19979;&#19968;&#20010;&#26679;&#26412;&#33258;&#34892;&#23398;&#20064;&#34920;&#31034;&#12290;&#19982;&#20197;&#21069;&#29992;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#27604;&#36739;&#20197;&#23637;&#31034;&#25913;&#36827;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#30456;&#21516;&#25968;&#30446;&#30340;&#21442;&#25968;/&#19978;&#19979;&#25991;&#26174;&#31034;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-term dependencies for audio signals is a particularly challenging problem, as even small-time scales yield on the order of a hundred thousand samples. With the recent advent of Transformers, neural architectures became good at modeling dependencies over longer time scales, but they suffered from quadratic constraints to scale them. We propose a generative auto-regressive architecture that can model audio waveforms over quite a large context, greater than 500,000 samples. Our work is adapted to learn time dependencies by learning a latent representation by a CNN front-end, and then learning dependencies over these representations using Transformer encoders, fully trained end-to-end: thereby allowing to learn representations as it deems fit for the next sample. Unlike previous works that compared different time scales to show improvement, we use a standard dataset, with the same number of parameters/context to show improvements. We achieve a state-of-the-art performance as 
&lt;/p&gt;</description></item><item><title>LICRA&#26159;&#23398;&#20064;&#22312;&#20195;&#20215;&#39640;&#26114;&#30340;&#34892;&#21160;&#21644;&#39044;&#31639;&#38480;&#21046;&#19979;&#36827;&#34892;&#36873;&#25321;&#24615;&#34892;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2205.15953</link><description>&lt;p&gt;
&#26102;&#26426;&#33267;&#20851;&#37325;&#35201;&#65306;&#23398;&#20064;&#22312;&#20195;&#20215;&#39640;&#26114;&#30340;&#34892;&#21160;&#21644;&#39044;&#31639;&#38480;&#21046;&#19979;&#36827;&#34892;&#36873;&#25321;&#24615;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints. (arXiv:2205.15953v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15953
&lt;/p&gt;
&lt;p&gt;
LICRA&#26159;&#23398;&#20064;&#22312;&#20195;&#20215;&#39640;&#26114;&#30340;&#34892;&#21160;&#21644;&#39044;&#31639;&#38480;&#21046;&#19979;&#36827;&#34892;&#36873;&#25321;&#24615;&#34892;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#25191;&#34892;&#34892;&#21160;&#37117;&#20250;&#20135;&#29983;&#25104;&#26412;&#65307;&#37329;&#34701;&#31995;&#32479;&#20013;&#30340;&#20132;&#26131;&#25104;&#26412;&#21644;&#29123;&#27833;&#25104;&#26412;&#26159;&#24120;&#35265;&#30340;&#20363;&#23376;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#25191;&#34892;&#34892;&#21160;&#36805;&#36895;&#31215;&#32047;&#25104;&#26412;&#65292;&#23548;&#33268;&#26497;&#20854;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21453;&#22797;&#34892;&#21160;&#20250;&#20135;&#29983;&#30952;&#25439;&#21644;&#26368;&#32456;&#25439;&#22351;&#12290;&#30830;&#23450;&#8220;&#20309;&#26102;&#34892;&#21160;&#8221;&#23545;&#20110;&#23454;&#29616;&#25104;&#21151;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#22312;&#34892;&#21160;&#20135;&#29983;&#26368;&#23567;&#38480;&#21046;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#8220;&#23398;&#20064;&#8221;&#34892;&#20026;&#26368;&#20248;&#31574;&#30053;&#30340;&#25361;&#25112;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#21517;&#20026;Learnable Impulse Control Reinforcement Algorithm&#65288;LICRA&#65289;&#65292;&#29992;&#20110;&#22312;&#34892;&#21160;&#20135;&#29983;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#36873;&#25321;&#20309;&#26102;&#34892;&#21160;&#21644;&#37319;&#21462;&#21738;&#20123;&#34892;&#21160;&#20197;&#23454;&#29616;&#26368;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining \textit{when to act} is crucial for achieving successful outcomes and yet, the challenge of efficiently \textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a reinforcement learning (RL) framework named \textbf{L}earnable \textbf{I}mpulse \textbf{C}ontrol \textbf{R}einforcement \textbf{A}lgorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \textit{impulse control} which learns to maximise objectives when actions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38597;&#21487;&#27604;&#25511;&#21046;&#30340;&#24102;&#23485;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#38381;&#24335;&#12289;&#35745;&#31639;&#38750;&#24120;&#36731;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#20851;&#27880;&#24102;&#23485;&#30340;&#21516;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.11956</link><description>&lt;p&gt;
&#36890;&#36807;&#38597;&#21487;&#27604;&#25511;&#21046;&#36873;&#25321;&#39640;&#26031;&#26680;&#23725;&#22238;&#24402;&#24102;&#23485;
&lt;/p&gt;
&lt;p&gt;
Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control. (arXiv:2205.11956v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38597;&#21487;&#27604;&#25511;&#21046;&#30340;&#24102;&#23485;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#38381;&#24335;&#12289;&#35745;&#31639;&#38750;&#24120;&#36731;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#20851;&#27880;&#24102;&#23485;&#30340;&#21516;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#23545;&#20110;&#39640;&#26031;&#26680;&#23725;&#22238;&#24402;&#65292;&#36229;&#21442;&#25968;&#26159;&#24102;&#23485;&#12290;&#24102;&#23485;&#25351;&#23450;&#26680;&#20989;&#25968;&#30340;&#38271;&#24230;&#23610;&#24230;&#65292;&#24517;&#39035;&#23567;&#24515;&#36873;&#25321;&#25165;&#33021;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#27169;&#22411;&#12290;&#24102;&#23485;&#36873;&#25321;&#30340;&#40664;&#35748;&#26041;&#27861;&#26159;&#20132;&#21449;&#39564;&#35777;&#21644;&#36793;&#32536;&#20284;&#28982;&#26368;&#22823;&#21270;&#65292;&#36825;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#30340;&#20272;&#35745;&#24448;&#24448;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#26041;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#26102;&#12290;&#21463;&#38597;&#21487;&#27604;&#27491;&#21017;&#21270;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#36817;&#20284;&#34920;&#36798;&#24335;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#26031;&#26680;&#23725;&#22238;&#24402;&#25512;&#26029;&#20989;&#25968;&#30340;&#23548;&#25968;&#22914;&#20309;&#21462;&#20915;&#20110;&#26680;&#24102;&#23485;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#34920;&#36798;&#24335;&#26469;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38597;&#21487;&#27604;&#25511;&#21046;&#30340;&#38381;&#24335;&#12289;&#35745;&#31639;&#38750;&#24120;&#36731;&#30340;&#24102;&#23485;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#38597;&#21487;&#27604;&#34920;&#36798;&#24335;&#34920;&#26126;&#20102;&#22312;&#26816;&#26597;&#24102;&#23485;&#36873;&#25321;&#30340;&#36136;&#37327;&#26102;&#24212;&#20851;&#27880;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning methods require tuning of hyper-parameters. For kernel ridge regression with the Gaussian kernel, the hyper-parameter is the bandwidth. The bandwidth specifies the length-scale of the kernel and has to be carefully selected in order to obtain a model with good generalization. The default methods for bandwidth selection is cross-validation and marginal likelihood maximization, which often yields good results, albeit at high computational costs. Furthermore, the estimates provided by these methods tend to have very high variance, especially when training data are scarce. Inspired by Jacobian regularization, we formulate an approximate expression for how the derivatives of the functions inferred by kernel ridge regression with the Gaussian kernel depend on the kernel bandwidth. We then use this expression to propose a closed-form, computationally feather-light, bandwidth selection heuristic based on controlling the Jacobian. In addition, the Jacobian expression illum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26410;&#32463;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#19982;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#35299;&#32544;&#25216;&#26415;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#26368;&#23567;&#30340;&#24213;&#23618;&#21160;&#24577;&#20449;&#24687;&#65292;&#26377;&#25928;&#38548;&#31163;&#35270;&#39057;&#25968;&#25454;&#20013;&#19981;&#21516;&#38750;&#32447;&#24615;&#36816;&#21160;&#31867;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.10367</link><description>&lt;p&gt;
&#26410;&#35757;&#32451;&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#35299;&#32544;&#26041;&#27861;&#29992;&#20110;&#38548;&#31163;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#19981;&#21516;&#36816;&#21160;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent-space disentanglement with untrained generator networks for the isolation of different motion types in video data. (arXiv:2205.10367v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26410;&#32463;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#19982;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#35299;&#32544;&#25216;&#26415;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#26368;&#23567;&#30340;&#24213;&#23618;&#21160;&#24577;&#20449;&#24687;&#65292;&#26377;&#25928;&#38548;&#31163;&#35270;&#39057;&#25968;&#25454;&#20013;&#19981;&#21516;&#38750;&#32447;&#24615;&#36816;&#21160;&#31867;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38548;&#31163;&#35270;&#39057;&#25968;&#25454;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#21160;&#20316;&#26159;&#35270;&#39057;&#20998;&#26512;&#20013;&#19968;&#20010;&#38750;&#24120;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#21160;&#24577;&#21307;&#23398;&#25110;&#29983;&#29289;&#25104;&#20687;&#20013;&#65292;&#24863;&#20852;&#36259;&#21160;&#24577;&#30340;&#20998;&#26512;&#21644;&#36827;&#19968;&#27493;&#22788;&#29702;&#36890;&#24120;&#20250;&#22240;&#20026;&#38468;&#21152;&#30340;&#38750;&#20851;&#38190;&#21160;&#24577;&#65288;&#20363;&#22914;&#34987;&#27979;&#20027;&#20307;&#30340;&#36816;&#21160;&#65289;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#21033;&#29992;&#26410;&#32463;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#23545;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#65292;&#20877;&#32467;&#21512;&#19968;&#31181;&#29305;&#27530;&#30340;&#28508;&#31354;&#38388;&#35299;&#32544;&#25216;&#26415;&#65292;&#21487;&#20165;&#21033;&#29992;&#23545;&#19968;&#20123;&#24213;&#23618;&#21160;&#24577;&#30340;&#26368;&#23567;&#19968;&#32500;&#20449;&#24687;&#65292;&#26377;&#25928;&#38548;&#31163;&#19981;&#21516;&#30340;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#36816;&#21160;&#31867;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#31181;&#34920;&#31034;&#20801;&#35768;&#20923;&#32467;&#20219;&#20309;&#21160;&#20316;&#31867;&#22411;&#30340;&#36873;&#25321;&#65292;&#24182;&#33719;&#24471;&#25152;&#38656;&#30340;&#20854;&#20182;&#20851;&#38190;&#21160;&#24577;&#30340;&#20934;&#30830;&#29420;&#31435;&#34920;&#31034;&#12290;&#33719;&#24471;&#36825;&#31181;&#34920;&#31034;&#19981;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#20219;&#20309;&#39044;&#35757;&#32451;&#65292;&#21363;&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#25152;&#26377;&#21442;&#25968;&#37117;&#26159;&#30452;&#25509;&#23398;&#20064;&#24471;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Isolating different types of motion in video data is a highly relevant problem in video analysis. Applications can be found, for example, in dynamic medical or biological imaging, where the analysis and further processing of the dynamics of interest is often complicated by additional, unwanted dynamics, such as motion of the measurement subject. In this work, it is empirically shown that a representation of video data via untrained generator networks, together with a specific technique for latent space disentanglement that uses minimal, one-dimensional information on some of the underlying dynamics, allows to efficiently isolate different, highly non-linear motion types. In particular, such a representation allows to freeze any selection of motion types, and to obtain accurate independent representations of other dynamics of interest. Obtaining such a representation does not require any pre-training on a training data set, i.e., all parameters of the generator network are learned direc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#35270;&#35273;&#20219;&#21153;&#30340;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#26426;&#21046;&#8212;&#8212;&#37051;&#22495;&#20851;&#27880;&#65288;NA&#65289;&#12290;&#22522;&#20110;NA&#65292;&#24320;&#21457;&#20102;NAT&#65292;NAT-Tiny&#22312;ImageNet&#19978;&#36798;&#21040;&#20102;83.2&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.07143</link><description>&lt;p&gt;
&#12298;&#37051;&#22495;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#12299;
&lt;/p&gt;
&lt;p&gt;
Neighborhood Attention Transformer. (arXiv:2204.07143v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07143
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#35270;&#35273;&#20219;&#21153;&#30340;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#26426;&#21046;&#8212;&#8212;&#37051;&#22495;&#20851;&#27880;&#65288;NA&#65289;&#12290;&#22522;&#20110;NA&#65292;&#24320;&#21457;&#20102;NAT&#65292;NAT-Tiny&#22312;ImageNet&#19978;&#36798;&#21040;&#20102;83.2&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#37051;&#22495;&#20851;&#27880;&#8221;&#65288;NA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;&#20219;&#21153;&#30340;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;NA&#26159;&#19968;&#31181;&#20687;&#32032;&#32423;&#36816;&#31639;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#23616;&#38480;&#20110;&#26368;&#36817;&#30340;&#30456;&#37051;&#20687;&#32032;&#65292;&#22240;&#27492;&#19982;SA&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#30456;&#27604;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#28369;&#21160;&#31383;&#21475;&#27169;&#24335;&#20351;NA&#30340;&#24863;&#21463;&#37326;&#33021;&#22815;&#22686;&#38271;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20687;&#32032;&#31227;&#20301;&#65292;&#24182;&#19988;&#20445;&#30041;&#20102;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#36825;&#19982;Swin Transformer&#30340;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65288;WSA&#65289;&#19981;&#21516;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;NATTEN&#65288;&#37051;&#22495;&#20851;&#27880;&#25193;&#23637;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#30340;C++&#21644;CUDA&#20869;&#26680;&#30340;Python&#21253;&#65292;&#20351;NA&#30340;&#36816;&#34892;&#36895;&#24230;&#27604;Swin&#30340;WSA&#24555;&#39640;&#36798;40&#65285;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#20869;&#23384;&#23569;&#20102;25&#65285;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;NA&#30340;&#26032;&#23618;&#27425;&#32467;&#26500;&#21464;&#25442;&#22120;&#35774;&#35745;&#8212;&#8212;&#37051;&#22495;&#20851;&#27880;&#21464;&#25442;&#22120;&#65288;NAT&#65289;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#12290;&#22312;NAT&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;NAT-Tiny&#22312;ImageNet&#19978;&#36798;&#21040;&#20102;83.2&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#21644;51.4&#65285;&#30340;mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#38544;&#34109;&#36890;&#20449;&#23186;&#20171;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedComm&#30340;&#38544;&#34109;&#36890;&#20449;&#25216;&#26415;&#65292;&#20854;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#22312;FL&#26694;&#26550;&#20869;&#20849;&#20139;&#21644;&#20256;&#36755;&#26377;&#38024;&#23545;&#24615;&#30340;&#36127;&#36733;&#65292;&#20351;&#24471;&#36890;&#20449;&#26356;&#20026;&#38544;&#34109;&#65292;&#19988;&#25216;&#26415;&#26816;&#27979;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2201.08786</link><description>&lt;p&gt;
FedComm: &#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#34109;&#36890;&#20449;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FedComm: Federated Learning as a Medium for Covert Communication. (arXiv:2201.08786v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#38544;&#34109;&#36890;&#20449;&#23186;&#20171;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedComm&#30340;&#38544;&#34109;&#36890;&#20449;&#25216;&#26415;&#65292;&#20854;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#22312;FL&#26694;&#26550;&#20869;&#20849;&#20139;&#21644;&#20256;&#36755;&#26377;&#38024;&#23545;&#24615;&#30340;&#36127;&#36733;&#65292;&#20351;&#24471;&#36890;&#20449;&#26356;&#20026;&#38544;&#34109;&#65292;&#19988;&#25216;&#26415;&#26816;&#27979;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#36731;&#28145;&#24230;&#23398;&#20064;&#24102;&#26469;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#22823;&#37327;&#21442;&#19982;&#32773;&#33021;&#22815;&#25104;&#21151;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26080;&#38656;&#21521;&#22806;&#30028;&#36879;&#38706;&#31169;&#23494;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#31350;&#20102;FL&#26041;&#26696;&#30340;&#36890;&#20449;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;FedComm&#65292;&#19968;&#31181;&#22810;&#31995;&#32479;&#38544;&#34109;&#36890;&#20449;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;FL&#26694;&#26550;&#20869;&#31283;&#20581;&#22320;&#20849;&#20139;&#21644;&#20256;&#36755;&#26377;&#38024;&#23545;&#24615;&#30340;&#36127;&#36733;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FedComm&#25552;&#20379;&#20102;&#19968;&#20010;&#38544;&#34109;&#30340;&#36890;&#20449;&#36890;&#36947;&#65292;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24178;&#25200;&#26368;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedComm&#23454;&#29616;&#20102;&#39640;&#20256;&#36755;&#36895;&#29575;&#21644;&#20302;&#26816;&#27979;&#29575;&#65292;&#26159;&#23454;&#38469;&#22330;&#26223;&#20013;&#38544;&#34109;&#36890;&#20449;&#30340;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proposed as a solution to mitigate the privacy implications related to the adoption of deep learning, Federated Learning (FL) enables large numbers of participants to successfully train deep neural networks without having to reveal the actual private training data. To date, a substantial amount of research has investigated the security and privacy properties of FL, resulting in a plethora of innovative attack and defense strategies. This paper thoroughly investigates the communication capabilities of an FL scheme. In particular, we show that a party involved in the FL learning process can use FL as a covert communication medium to send an arbitrary message. We introduce FedComm, a novel multi-system covert-communication technique that enables robust sharing and transfer of targeted payloads within the FL framework. Our extensive theoretical and empirical evaluations show that FedComm provides a stealthy communication channel, with minimal disruptions to the training process. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#39044;&#38450;&#24615;&#23398;&#20064;&#8221;&#26694;&#26550;&#65292;&#20197;&#22312;&#28385;&#36275;&#23545;&#32422;&#26463;&#26631;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#20445;&#35777;&#20855;&#26377;&#20984;&#32422;&#26463;&#21644;&#19968;&#33324;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#30340;DNN&#35299;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#26080;&#38656;&#21518;&#22788;&#29702;&#12290;&#36890;&#36807;&#31995;&#32479;&#26631;&#23450;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#25105;&#20204;&#39044;&#31034;&#39044;&#27979;&#35823;&#24046;&#24182;&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#35299;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#26679;&#26412;&#24863;&#30693;&#30340;&#35757;&#32451;&#31639;&#27861;&#20197;&#25552;&#39640;DNN&#30340;&#26368;&#20248;&#24615;&#33021;&#32780;&#19981;&#29306;&#29298;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2112.08091</link><description>&lt;p&gt;
&#22522;&#20110;&#20984;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;DNN&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#20445;&#35777;&#21450;&#20854;&#22312;&#30452;&#27969;&#26368;&#20248;&#28526;&#27969;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems. (arXiv:2112.08091v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#39044;&#38450;&#24615;&#23398;&#20064;&#8221;&#26694;&#26550;&#65292;&#20197;&#22312;&#28385;&#36275;&#23545;&#32422;&#26463;&#26631;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#20445;&#35777;&#20855;&#26377;&#20984;&#32422;&#26463;&#21644;&#19968;&#33324;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#30340;DNN&#35299;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#26080;&#38656;&#21518;&#22788;&#29702;&#12290;&#36890;&#36807;&#31995;&#32479;&#26631;&#23450;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#25105;&#20204;&#39044;&#31034;&#39044;&#27979;&#35823;&#24046;&#24182;&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#35299;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#26679;&#26412;&#24863;&#30693;&#30340;&#35757;&#32451;&#31639;&#27861;&#20197;&#25552;&#39640;DNN&#30340;&#26368;&#20248;&#24615;&#33021;&#32780;&#19981;&#29306;&#29298;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#29992;&#20110;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26041;&#26696;&#26102;&#65292;&#30830;&#20445;&#35299;&#30340;&#21487;&#34892;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#30001;&#20110;DNN&#22266;&#26377;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#39044;&#38450;&#24615;&#23398;&#20064;&#8221;&#26694;&#26550;&#65292;&#20197;&#22312;&#28385;&#36275;&#23545;&#32422;&#26463;&#26631;&#23450;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#20445;&#35777;&#20855;&#26377;&#20984;&#32422;&#26463;&#21644;&#19968;&#33324;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#30340;DNN&#35299;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#26080;&#38656;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#26080;&#22833;&#19968;&#33324;&#24615;&#22320;&#20851;&#27880;&#21482;&#26377;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#26631;&#23450;DNN&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#20174;&#32780;&#39044;&#31034;&#39044;&#27979;&#35823;&#24046;&#24182;&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#35299;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#26631;&#23450;&#37327;&#21644;DNN&#22823;&#23567;&#36275;&#20197;&#30830;&#20445;&#36890;&#29992;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#26679;&#26412;&#24863;&#30693;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;DNN&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#20010;DNN&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring solution feasibility is a key challenge in developing Deep Neural Network (DNN) schemes for solving constrained optimization problems, due to inherent DNN prediction errors. In this paper, we propose a ``preventive learning'' framework to guarantee DNN solution feasibility for problems with convex constraints and general objective functions without post-processing, upon satisfying a mild condition on constraint calibration. Without loss of generality, we focus on problems with only inequality constraints. We systematically calibrate inequality constraints used in DNN training, thereby anticipating prediction errors and ensuring the resulting solutions remain feasible. We characterize the calibration magnitudes and the DNN size sufficient for ensuring universal feasibility. We propose a new Adversarial-Sample Aware training algorithm to improve DNN's optimality performance without sacrificing feasibility guarantee. Overall, the framework provides two DNNs. The first one from ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#20266;&#36870;&#65292;&#21253;&#25324;&#20854;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#26465;&#20214;&#20197;&#21450;&#24615;&#36136;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#30340;&#19981;&#21487;&#36870;&#38750;&#32447;&#24615;&#31639;&#23376;PI&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#23567;&#27874;&#38408;&#20540;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2111.10755</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#20266;&#36870;&#30340;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Theoretical Foundations for Pseudo-Inversion of Nonlinear Operators. (arXiv:2111.10755v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#20266;&#36870;&#65292;&#21253;&#25324;&#20854;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#26465;&#20214;&#20197;&#21450;&#24615;&#36136;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#30340;&#19981;&#21487;&#36870;&#38750;&#32447;&#24615;&#31639;&#23376;PI&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#23567;&#27874;&#38408;&#20540;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Moore-Penrose&#20266;&#36870;&#22312;&#29289;&#29702;&#23398;&#12289;&#32479;&#35745;&#23398;&#21644;&#21508;&#20010;&#24037;&#31243;&#39046;&#22495;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#38750;&#32447;&#24615;&#31639;&#23376;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#20266;&#36870;&#65292;&#24191;&#20041;&#22320;&#23450;&#20041;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#39318;&#20808;&#23545;&#20110;&#19968;&#33324;&#38598;&#21512;&#65292;&#28982;&#21518;&#23545;&#20110;&#36171;&#33539;&#31354;&#38388;&#36827;&#34892;&#20102;&#32454;&#21270;&#12290;&#24403;&#31639;&#23376;&#26159;&#30697;&#38453;&#26102;&#65292;&#36171;&#33539;&#31354;&#38388;&#30340;PI&#20135;&#29983;&#20102;Moore-Penrose&#20266;&#36870;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;PI&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#30340;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#20854;&#24615;&#36136;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#22914;&#36830;&#32493;&#24615;&#12289;&#31639;&#23376;&#32452;&#21512;&#21644;&#25237;&#24433;&#31639;&#23376;&#30340;&#20215;&#20540;&#31561;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#30340;&#19981;&#21487;&#36870;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;PI&#32473;&#20986;&#20102;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#20363;&#22914;&#30828;/&#36719;&#38408;&#20540;&#21644;ReLU&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#31070;&#32463;&#23618;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#23567;&#27874;&#38408;&#20540;&#26377;&#20851;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Moore-Penrose inverse is widely used in physics, statistics, and various fields of engineering. It captures well the notion of inversion of linear operators in the case of overcomplete data. In data science, nonlinear operators are extensively used. In this paper we characterize the fundamental properties of a pseudo-inverse (PI) for nonlinear operators.  The concept is defined broadly. First for general sets, and then a refinement for normed spaces. The PI for normed spaces yields the Moore-Penrose inverse when the operator is a matrix. We present conditions for existence and uniqueness of a PI and establish theoretical results investigating its properties, such as continuity, its value for operator compositions and projection operators, and others. Analytic expressions are given for the PI of some well-known, non-invertible, nonlinear operators, such as hard- or soft-thresholding and ReLU. Finally, we analyze a neural layer and discuss relations to wavelet thresholding.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#23553;&#35013;&#20026;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20351;&#29992;&#23376;&#37319;&#26679;&#23398;&#20064;&#30340;&#32593;&#32476;&#23884;&#20837;&#30340;&#28176;&#36817;&#20998;&#24067;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#28508;&#22312;&#21442;&#25968;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#31639;&#27861;&#36873;&#25321;&#19982;&#32479;&#35745;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2107.02363</link><description>&lt;p&gt;
&#23376;&#37319;&#26679;&#23398;&#20064;&#30340;&#32593;&#32476;&#23884;&#20837;&#30340;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of Network Embeddings Learned via Subsampling. (arXiv:2107.02363v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.02363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#23553;&#35013;&#20026;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20351;&#29992;&#23376;&#37319;&#26679;&#23398;&#20064;&#30340;&#32593;&#32476;&#23884;&#20837;&#30340;&#28176;&#36817;&#20998;&#24067;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#28508;&#22312;&#21442;&#25968;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#31639;&#27861;&#36873;&#25321;&#19982;&#32479;&#35745;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#25968;&#25454;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#30456;&#20851;&#20219;&#21153;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#33410;&#28857;&#32858;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#23398;&#20064;&#32593;&#32476;&#30340;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21521;&#37327;&#20540;&#25968;&#25454;&#24320;&#21457;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#22823;&#22411;&#32593;&#32476;&#65292;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#23884;&#20837;&#65292;&#20854;&#20013;&#23376;&#37319;&#26679;&#26041;&#26696;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#35832;&#22914;node2vec&#20043;&#31867;&#30340;&#34920;&#31034;&#26041;&#27861;&#23553;&#35013;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;&#22312;&#20551;&#35774;&#22270;&#26159;&#21487;&#20132;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#21521;&#37327;&#30340;&#20998;&#24067;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#20998;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#28508;&#22312;&#21442;&#25968;&#65292;&#21253;&#25324;&#25439;&#22833;&#20989;&#25968;&#21644;&#23884;&#20837;&#32500;&#25968;&#30340;&#36873;&#25321;&#65292;&#34920;&#24449;&#20102;&#28176;&#36817;&#20998;&#24067;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#20026;&#20351;&#29992;&#23376;&#37319;&#26679;&#23398;&#20064;&#30340;&#32593;&#32476;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#26412;&#35265;&#35299;&#65292;&#24182;&#38416;&#26126;&#20102;&#31639;&#27861;&#36873;&#25321;&#21644;&#32479;&#35745;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network data are ubiquitous in modern machine learning, with tasks of interest including node classification, node clustering and link prediction. A frequent approach begins by learning an Euclidean embedding of the network, to which algorithms developed for vector-valued data are applied. For large networks, embeddings are learned using stochastic gradient methods where the sub-sampling scheme can be freely chosen. Despite the strong empirical performance of such methods, they are not well understood theoretically. Our work encapsulates representation methods using a subsampling approach, such as node2vec, into a single unifying framework. We prove, under the assumption that the graph is exchangeable, that the distribution of the learned embedding vectors asymptotically decouples. Moreover, we characterize the asymptotic distribution and provided rates of convergence, in terms of the latent parameters, which includes the choice of loss function and the embedding dimension. This provid
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#35757;&#32451;&#38598;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#21327;&#21464;&#37327;&#28418;&#31227;&#24773;&#20917;&#19979;&#25913;&#36827;&#30417;&#30563;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#23431;&#23449;&#23398;&#39046;&#22495;&#30340;&#20004;&#20010;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#30446;&#26631;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2106.11211</link><description>&lt;p&gt;
&#20998;&#23618;&#23398;&#20064;&#65306;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stratified Learning: A General-Purpose Statistical Method for Improved Learning under Covariate Shift. (arXiv:2106.11211v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#35757;&#32451;&#38598;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#21327;&#21464;&#37327;&#28418;&#31227;&#24773;&#20917;&#19979;&#25913;&#36827;&#30417;&#30563;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#23431;&#23449;&#23398;&#39046;&#22495;&#30340;&#20004;&#20010;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#30446;&#26631;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#20855;&#26377;&#32479;&#35745;&#23398;&#21407;&#29702;&#21644;&#29702;&#35770;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#38598;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#30417;&#30563;&#23398;&#20064;&#65292;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#21327;&#21464;&#37327;&#28418;&#31227;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#19968;&#31181;&#25104;&#29087;&#30340;&#26041;&#27861;&#22522;&#30784;&#20043;&#19978;&#65292;&#34920;&#26126;&#21327;&#21464;&#37327;&#28418;&#31227;&#30340;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#22312;&#22240;&#27425;&#20998;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#26469;&#20943;&#23569;&#25110;&#28040;&#38500;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#22312;&#20272;&#35745;&#30340;&#22240;&#27425;&#20998;&#25968;&#30340;&#22522;&#30784;&#19978;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#23618;&#26500;&#36896;&#65292;&#20174;&#32780;&#23454;&#29616;&#24179;&#34913;&#21327;&#21464;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#30446;&#26631;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#29616;&#20195;&#23431;&#23449;&#23398;&#30740;&#31350;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#36825;&#31181;&#19968;&#33324;&#24615;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26356;&#26032;&#30340;&#8220;&#36229;&#26032;&#26143;&#20809;&#24230;&#20998;&#31867;&#25361;&#25112;&#8221;&#20013;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;AUC&#20540;&#65288;0.958&#65289;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;SDSS&#25968;&#25454;&#20013;&#30340;&#26143;&#31995;&#32418;&#31227;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple, statistically principled, and theoretically justified method to improve supervised learning when the training set is not representative, a situation known as covariate shift. We build upon a well-established methodology in causal inference, and show that the effects of covariate shift can be reduced or eliminated by conditioning on propensity scores. In practice, this is achieved by fitting learners within strata constructed by partitioning the data based on the estimated propensity scores, leading to approximately balanced covariates and much-improved target prediction. We demonstrate the effectiveness of our general-purpose method on two contemporary research questions in cosmology, outperforming state-of-the-art importance weighting methods. We obtain the best reported AUC (0.958) on the updated "Supernovae photometric classification challenge", and we improve upon existing conditional density estimation of galaxy redshift from Sloan Data Sky Survey (SDSS) data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#24230;&#23548;&#21521;&#24418;&#29366;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#36755;&#20837;&#27861;&#21521;&#37327;&#65292;&#20197;&#36719;&#32422;&#26463;&#20542;&#21521;&#20110;&#24179;&#28369;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#38752;&#22320;&#23450;&#20301;&#27599;&#20010;&#28857;&#30340;&#26410;&#30693;&#27861;&#21521;&#37327;&#30340;&#26799;&#24230;&#65292;&#29978;&#33267;&#27604;&#20351;&#29992;&#30495;&#23454;&#27861;&#21521;&#37327;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;INR&#20960;&#20309;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.10811</link><description>&lt;p&gt;
DiGS&#65306;&#22522;&#20110;&#25955;&#24230;&#23548;&#21521;&#30340;&#26410;&#23450;&#21521;&#28857;&#20113;&#24418;&#29366;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#24230;&#23548;&#21521;&#24418;&#29366;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#36755;&#20837;&#27861;&#21521;&#37327;&#65292;&#20197;&#36719;&#32422;&#26463;&#20542;&#21521;&#20110;&#24179;&#28369;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#38752;&#22320;&#23450;&#20301;&#27599;&#20010;&#28857;&#30340;&#26410;&#30693;&#27861;&#21521;&#37327;&#30340;&#26799;&#24230;&#65292;&#29978;&#33267;&#27604;&#20351;&#29992;&#30495;&#23454;&#27861;&#21521;&#37327;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;INR&#20960;&#20309;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#26368;&#36817;&#24050;&#32463;&#22312;&#24418;&#29366;&#20998;&#26512;&#21644;&#37325;&#24314;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30340;INR&#38656;&#35201;&#20960;&#20309;&#22352;&#26631;&#26469;&#23398;&#20064;&#24418;&#29366;&#30340;&#38544;&#24335;&#27700;&#24179;&#38598;&#12290;&#24403;&#27599;&#20010;&#28857;&#37117;&#26377;&#27861;&#21521;&#37327;&#26102;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#36890;&#24120;&#21407;&#22987;&#25968;&#25454;&#20013;&#24456;&#23569;&#25552;&#20379;&#27861;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#34920;&#26126;&#26041;&#27861;&#30340;&#21021;&#22987;&#21270;&#23545;&#20110;&#34920;&#38754;&#37325;&#26500;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#27861;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#30340;&#25955;&#24230;&#23548;&#21521;&#24418;&#29366;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36317;&#31163;&#20989;&#25968;&#30340;&#25955;&#24230;&#19978;&#28155;&#21152;&#36719;&#32422;&#26463;&#20250;&#20542;&#21521;&#20110;&#24179;&#28369;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#21487;&#38752;&#22320;&#23450;&#20301;&#27599;&#20010;&#28857;&#30340;&#26410;&#30693;&#27861;&#21521;&#37327;&#30340;&#26799;&#24230;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#27604;&#30452;&#25509;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#27861;&#21521;&#37327;&#30340;&#26041;&#27861;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;INR&#20960;&#20309;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape implicit neural representations (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves conve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#39564;&#35777;&#26631;&#20934;&#65292;&#22522;&#20110;&#32858;&#31867;&#31283;&#23450;&#24615;&#30340;&#20869;&#37096;&#39564;&#35777;&#21407;&#21017;&#65292;&#22312;&#32858;&#31867;&#31283;&#23450;&#24615;&#21644;&#32858;&#31867;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2006.08530</link><description>&lt;p&gt;
&#24102;&#26377;&#31283;&#23450;&#24615;&#25240;&#34935;&#30340;&#36873;&#25321;&#32858;&#31867;&#25968;&#30446; $K$&#65306;&#19968;&#31181;&#20869;&#37096;&#39564;&#35777;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Selecting the Number of Clusters $K$ with a Stability Trade-off: an Internal Validation Criterion. (arXiv:2006.08530v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.08530
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#39564;&#35777;&#26631;&#20934;&#65292;&#22522;&#20110;&#32858;&#31867;&#31283;&#23450;&#24615;&#30340;&#20869;&#37096;&#39564;&#35777;&#21407;&#21017;&#65292;&#22312;&#32858;&#31867;&#31283;&#23450;&#24615;&#21644;&#32858;&#31867;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26159;&#38750;&#21442;&#25968;&#32858;&#31867;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#27809;&#26377;&#21487;&#20197;&#20316;&#20026;&#26631;&#20934;&#31572;&#26696;&#30340;&#30495;&#23454;&#25968;&#25454;&#23384;&#22312;&#65292;&#22240;&#27492;&#35780;&#20215;&#32858;&#31867;&#32467;&#26524;&#30340;&#36890;&#29992;&#26041;&#27861;&#23578;&#26410;&#20986;&#29616;&#12290;&#32858;&#31867;&#30446;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#20102;&#26222;&#36941;&#25509;&#21463;&#30340;&#35780;&#20215;&#26631;&#20934;&#38590;&#20197;&#30830;&#23450;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#32858;&#31867;&#31283;&#23450;&#24615;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#19988;&#26080;&#38656;&#27169;&#22411;&#30340;&#21407;&#21017;&#32780;&#20986;&#29616;&#65306;&#32858;&#31867;&#31639;&#27861;&#24212;&#21457;&#29616;&#25968;&#25454;&#20013;&#31283;&#23450;&#30340;&#32467;&#26500;&#12290;&#22914;&#26524;&#25968;&#25454;&#38598;&#20174;&#30456;&#21516;&#30340;&#22522;&#30784;&#20998;&#24067;&#20013;&#37325;&#22797;&#37319;&#26679;&#65292;&#21017;&#31639;&#27861;&#24212;&#25214;&#21040;&#30456;&#20284;&#30340;&#20998;&#21306;&#12290;&#28982;&#32780;&#65292;&#21333;&#32431;&#30340;&#31283;&#23450;&#24615;&#24182;&#19981;&#36866;&#21512;&#30830;&#23450;&#32858;&#31867;&#25968;&#30446;&#12290;&#20363;&#22914;&#65292;&#23427;&#26080;&#27861;&#26816;&#27979;&#32858;&#31867;&#25968;&#30446;&#26159;&#21542;&#22826;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21407;&#21017;&#65306;&#19968;&#31181;&#22909;&#30340;&#32858;&#31867;&#24212;&#35813;&#26159;&#31283;&#23450;&#30340;&#65292;&#19988;&#22312;&#27599;&#20010;&#32858;&#31867;&#20869;&#37096;&#65292;&#19981;&#23384;&#22312;&#31283;&#23450;&#30340;&#23376;&#20998;&#21306;&#12290;&#36825;&#20010;&#21407;&#21017;&#24102;&#26469;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#31283;&#23450;&#24615;&#30340;&#26032;&#22411;&#32858;&#31867;&#39564;&#35777;&#26631;&#20934;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#22522;&#20110;&#31283;&#23450;&#24615;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26631;&#20934;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#24674;&#22797;&#30495;&#23454;&#30340;&#32858;&#31867;&#25968;&#30446;&#65292;&#24182;&#19988;&#22312;&#32858;&#31867;&#31283;&#23450;&#24615;&#21644;&#32858;&#31867;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection is a major challenge in non-parametric clustering. There is no universally admitted way to evaluate clustering results for the obvious reason that no ground truth is available. The difficulty to find a universal evaluation criterion is a consequence of the ill-defined objective of clustering. In this perspective, clustering stability has emerged as a natural and model-agnostic principle: an algorithm should find stable structures in the data. If data sets are repeatedly sampled from the same underlying distribution, an algorithm should find similar partitions. However, stability alone is not well-suited to determine the number of clusters. For instance, it is unable to detect if the number of clusters is too small. We propose a new principle: a good clustering should be stable, and within each cluster, there should exist no stable partition. This principle leads to a novel clustering validation criterion based on between-cluster and within-cluster stability, overcoming 
&lt;/p&gt;</description></item></channel></rss>