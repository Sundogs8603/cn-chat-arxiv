<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#24182;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;&#36827;&#19968;&#27493;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#12290;</title><link>http://arxiv.org/abs/2311.01797</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#24182;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;&#36827;&#19968;&#27493;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#31435;&#19968;&#20010;&#38543;&#26426;&#20256;&#36755;&#26144;&#23556;&#65292;&#23558;&#32463;&#39564;&#35266;&#27979;&#21040;&#30340;&#20294;&#26410;&#30693;&#30340;&#30446;&#26631;&#20998;&#24067;&#19982;&#24050;&#30693;&#30340;&#20808;&#39564;&#20998;&#24067;&#32852;&#31995;&#36215;&#26469;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#26410;&#20805;&#20998;&#21457;&#23637;&#12290;&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#34920;&#26126;&#22312;&#26679;&#26412;&#22823;&#23567;$n$&#21644;&#27169;&#22411;&#23481;&#37327;$m$&#19978;&#37117;&#23384;&#22312;&#22810;&#39033;&#24335;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;($O(n^{-2/5}+m^{-4/5})$)&#65292;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#65288;&#21363;&#25968;&#25454;&#32500;&#24230;&#19981;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#20998;&#24067;&#34987;&#25551;&#32472;&#20026;&#19968;&#31995;&#21015;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#31561;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.18784</link><description>&lt;p&gt;
&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#19979;&#30340;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#37325;&#23614;&#22122;&#22768;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. (arXiv:2310.18784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#31561;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#30740;&#31350;&#24037;&#20316;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21450;&#20854;&#21098;&#20999;&#21464;&#20307;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#12290;&#19982;&#26222;&#36890;&#30340;SGD&#30456;&#27604;&#65292;&#21098;&#20999;SGD&#22312;&#23454;&#38469;&#20013;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#23545;&#25968;&#20381;&#36182;&#20110;&#22833;&#36133;&#27010;&#29575;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#23454;&#38469;&#38750;&#32447;&#24615;SGD&#21464;&#20307;&#65288;&#22914;&#31526;&#21495;SGD&#12289;&#37327;&#21270;SGD&#21644;&#24402;&#19968;&#21270;SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#29702;&#35299;&#35201;&#23569;&#24471;&#22810;&#65292;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#36890;&#20449;&#25928;&#29575;&#25110;&#21152;&#36895;&#25910;&#25947;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#20041;&#38750;&#32447;&#24615;SGD&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#19982;&#21098;&#20999;SGD&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26356;&#20026;&#19968;&#33324;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#20219;&#20309;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#22914;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent works have studied the convergence \textit{in high probability} of stochastic gradient descent (SGD) and its clipped variant. Compared to vanilla SGD, clipped SGD is practically more stable and has the additional theoretical benefit of logarithmic dependence on the failure probability. However, the convergence of other practical nonlinear variants of SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved communication efficiency or accelerated convergence is much less understood. In this work, we study the convergence bounds \textit{in high probability} of a broad class of nonlinear SGD methods. For strongly convex loss functions with Lipschitz continuous gradients, we prove a logarithmic dependence on the failure probability, even when the noise is heavy-tailed. Strictly more general than the results for clipped SGD, our results hold for any nonlinearity with bounded (component-wise or joint) outputs, such as clipping, normalization, and quantizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17784</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#22914;&#37329;&#34701;&#26102;&#21364;&#36935;&#21040;&#22256;&#38590;&#12290;LLMs&#38590;&#20197;&#25512;&#29702;&#21644;&#25972;&#21512;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#37329;&#34701;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19981;&#26159;&#19968;&#27425;&#24615;&#32473;LLM&#36127;&#36733;&#36807;&#22810;&#20449;&#24687;&#65292;&#32780;&#26159;&#26356;&#26377;&#25928;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20219;&#21153;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26469;&#21019;&#24314;&#37329;&#34701;LLM&#65288;FLLM&#65289;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#25163;&#21160;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#24378;&#25512;&#29702;&#65288;AAR&#65289;&#26469;&#20462;&#25913;FLLM&#33258;&#36523;&#36755;&#20986;&#30340;&#20266;&#26631;&#31614;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;FLLM&#19982;AAR&#30456;&#27604;&#65292;&#26174;&#33879;&#20248;&#20110;&#20026;&#21407;&#22987;&#25991;&#26412;&#35774;&#35745;&#30340;&#22522;&#32447;&#37329;&#34701;LLMs&#65292;&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#24179;&#34913;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.16600</link><description>&lt;p&gt;
&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;
&lt;/p&gt;
&lt;p&gt;
Balancing central and marginal rejection when combining independent significance tests. (arXiv:2310.16600v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#24179;&#34913;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21407;&#22987;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#35780;&#20272;&#19968;&#32452;p&#20540;&#30340;&#26174;&#33879;&#24615;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#23427;&#20204;&#19982;&#27719;&#38598;&#20989;&#25968;&#36827;&#34892;&#32452;&#21512;&#12290;&#36825;&#20123;&#27719;&#38598;&#30340;p&#20540;&#23558;p&#20540;&#26679;&#26412;&#36716;&#21270;&#20026;&#19968;&#20010;&#34920;&#29616;&#31867;&#20284;&#20110;&#21333;&#21464;&#37327;p&#20540;&#30340;&#21333;&#19968;&#25968;&#20540;&#12290;&#20026;&#20102;&#26126;&#30830;&#35752;&#35770;&#36825;&#20123;&#20989;&#25968;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20132;&#21449;&#20551;&#35774;&#65292;&#20197;&#20256;&#36798;p&#20540;&#20013;&#38750;&#38646;&#35777;&#25454;&#30340;&#24378;&#24230;&#21644;&#26222;&#36941;&#24615;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#24120;&#35268;&#27719;&#38598;&#20844;&#24335;&#12290;&#22312;&#29305;&#23450;&#20132;&#21449;&#20551;&#35774;&#30340;UMP&#27719;&#38598;p&#20540;&#20013;&#35266;&#23519;&#21040;&#30340;&#27169;&#24335;&#25512;&#21160;&#20102;&#23545;&#20110;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#27700;&#24179;&#22312;&#945;&#22788;&#30340;&#23450;&#20041;&#21644;&#35752;&#35770;&#12290;&#35777;&#26126;&#20102;&#20013;&#24515;&#25298;&#32477;&#24635;&#26159;&#22823;&#20110;&#31561;&#20110;&#36793;&#32536;&#25298;&#32477;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#22312;&#27719;&#38598;&#30340;p&#20540;&#20013;&#24179;&#34913;&#30340;&#21830;&#12290;&#22522;&#20110;&#967;&#178;_&#954;&#20998;&#20301;&#25968;&#21464;&#25442;&#30340;&#32452;&#21512;&#20989;&#25968;&#34987;&#25552;&#20986;&#20197;&#25511;&#21046;&#36825;&#20010;&#21830;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to evaluating the significance of a collection of $p$-values combines them with a pooling function, in particular when the original data are not available. These pooled $p$-values convert a sample of $p$-values into a single number which behaves like a univariate $p$-value. To clarify discussion of these functions, a telescoping series of alternative hypotheses are introduced that communicate the strength and prevalence of non-null evidence in the $p$-values before general pooling formulae are discussed. A pattern noticed in the UMP pooled $p$-value for a particular alternative motivates the definition and discussion of central and marginal rejection levels at $\alpha$. It is proven that central rejection is always greater than or equal to marginal rejection, motivating a quotient to measure the balance between the two for pooled $p$-values. A combining function based on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this quotient and shown to be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;PulseDiff&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#20808;&#39564;&#26469;&#25913;&#36827;&#24515;&#30005;&#22270;(ECG)&#30340;&#25554;&#20540;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#27169;&#22411;&#32771;&#34385;&#20102;&#20027;&#20307;&#38388;&#21464;&#21270;&#21644;&#24515;&#36339;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15742</link><description>&lt;p&gt;
&#29992;&#22686;&#24378;&#30340;&#27169;&#26495;&#20808;&#39564;&#25913;&#36827;&#24515;&#30005;&#22270;(ECG)&#25554;&#20540;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion Models for ECG Imputation with an Augmented Template Prior. (arXiv:2310.15742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;PulseDiff&#65292;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#20808;&#39564;&#26469;&#25913;&#36827;&#24515;&#30005;&#22270;(ECG)&#30340;&#25554;&#20540;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#27169;&#22411;&#32771;&#34385;&#20102;&#20027;&#20307;&#38388;&#21464;&#21270;&#21644;&#24515;&#36339;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#24120;&#35268;&#20020;&#24202;&#25252;&#29702;&#30340;&#19968;&#37096;&#20998;&#65292;&#22914;&#24515;&#30005;&#22270;(ECG)&#31561;&#33033;&#20914;&#20449;&#21495;&#34987;&#24191;&#27867;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31227;&#21160;&#20581;&#24247;&#31995;&#32479;&#25910;&#38598;&#30340;&#20449;&#21495;&#23384;&#22312;&#22122;&#38899;&#21644;&#36136;&#37327;&#24046;&#30340;&#24405;&#38899;&#65292;&#23548;&#33268;&#32570;&#22833;&#20540;&#30340;&#23384;&#22312;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#38477;&#20302;&#20102;&#20449;&#21495;&#30340;&#36136;&#37327;&#65292;&#24182;&#24433;&#21709;&#20102;&#33258;&#21160;&#21270;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#23545;ECG&#36827;&#34892;&#32570;&#22833;&#20540;&#25554;&#34917;&#12290;&#28982;&#32780;&#65292;&#19982;&#30830;&#23450;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#65292;&#22240;&#20026;&#35757;&#32451;&#30446;&#26631;&#20013;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#20027;&#20307;&#38388;&#30340;&#21464;&#21270;&#21644;&#24515;&#36339;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#25913;&#21892;&#27010;&#29575;&#27169;&#22411;&#30340;ECG&#25554;&#20540;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#20449;&#24687;&#20808;&#39564;&#30340;&#27169;&#26495;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;PulseDiff&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1)&#25105;&#20204;&#39318;&#20808;&#20174;&#35266;&#27979;&#20013;&#25552;&#21462;&#19968;&#20010;&#20027;&#20307;&#32423;&#21035;&#30340;&#33033;&#20914;&#27169;&#26495;&#20316;&#20026;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Pulsative signals such as the electrocardiogram (ECG) are extensively collected as part of routine clinical care. However, noisy and poor-quality recordings, leading to missing values, are a major issue for signals collected using mobile health systems, decreasing the signal quality and affecting the automated downstream tasks. Recent studies have explored imputation of missing values for ECG with probabilistic time-series models. Nevertheless, in comparison with the deterministic models, their performance is still limited, as the variations across subjects and heart-beat relationships are not explicitly considered in the training objective. In this work, to improve the ECG imputation and forecasting accuracy with probabilistic models, we present an template-guided denoising diffusion probabilistic model, PulseDiff, which is conditioned an informative prior for a range of health conditions. Specifically, 1) we first extract a subject-level pulsative template from the observation as an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.14421</link><description>&lt;p&gt;
&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#65292;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#38024;&#23545;&#65288;&#23616;&#37096;&#65289;&#21807;&#19968;&#21487;&#36870;&#20998;&#31867;&#22120;&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#21644;&#29109;AI&#65288;EAI&#65289;&#20855;&#26377;&#26368;&#23567;&#23545;&#25239;&#36335;&#24452;&#65288;MAP&#65289;&#21644;&#26368;&#23567;&#23545;&#25239;&#36317;&#31163;&#65288;MAD&#65289;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#26126;&#30830;&#30340;&#20998;&#26512;&#35745;&#31639;&#30340;&#31616;&#21333;&#21487;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#12290;&#22312;&#24120;&#35265;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#12289;&#25552;&#21319;&#38543;&#26426;&#26862;&#26519;&#12289;GLM&#21644;EAI&#31561;&#21508;&#31867;AI&#24037;&#20855;&#36827;&#34892;MAP&#21644;MAD&#30340;&#23454;&#38469;&#35745;&#31639;&#12289;&#27604;&#36739;&#21644;&#35299;&#37322;&#65292;&#21253;&#25324;&#21452;&#21367;&#29366;&#34746;&#26059;&#32447;&#21450;&#20854;&#25193;&#23637;&#20197;&#21450;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38382;&#39064;&#65288;&#29992;&#20110;&#20581;&#24247;&#20445;&#38505;&#29702;&#36180;&#39044;&#27979;&#21644;&#24515;&#33039;&#30149;&#21457;&#20316;&#33268;&#27515;&#29575;&#20998;&#31867;&#65289;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#23637;&#31034;&#20102;MAP&#22914;&#20309;&#22312;&#39044;&#23450;&#20041;&#30340;&#21487;&#35775;&#38382;&#25511;&#21046;&#21464;&#37327;&#23376;&#38598;&#20013;&#25552;&#20379;&#21807;&#19968;&#30340;&#26368;&#23567;&#24739;&#32773;&#29305;&#23450;&#39118;&#38505;&#32531;&#35299;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
&lt;/p&gt;</description></item><item><title>DA-TransUNet &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#22359;&#12290;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12570</link><description>&lt;p&gt;
DA-TransUNet: &#23558;Spatial&#21644;Channel Dual Attention&#19982;Transformer U-Net&#38598;&#25104;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12570
&lt;/p&gt;
&lt;p&gt;
DA-TransUNet &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#22359;&#12290;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24378;&#22823;&#30340;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#65292;&#33258;&#21160;&#21270;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;Transformer&#30340;&#24433;&#21709;&#23548;&#33268;&#20102;&#23545;&#20854;&#21464;&#20307;&#30340;&#30740;&#31350;&#65292;&#24182;&#22823;&#35268;&#27169;&#26367;&#20195;&#20256;&#32479;&#30340;CNN&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#32463;&#24120;&#24573;&#35270;&#20102;Transformer&#30340;&#22266;&#26377;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#24494;&#23567;&#35843;&#25972;&#23545;&#27169;&#22411;&#21644;Transformer&#27169;&#22359;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#31216;&#20026;DA-TransUNet&#65292;&#26088;&#22312;&#23558;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#22359;&#24341;&#20837;&#20256;&#32479;U&#24418;&#26550;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#12290;&#19982;&#20043;&#21069;&#22522;&#20110;Transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;DA-TransUNet&#21033;&#29992;&#20102;Transformer&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;DA-Block&#30340;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20043;&#21069;&#30340;Transformer U-Net&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#19968;&#20010;&#21452;&#37325;&#27880;&#24847;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Great progress has been made in automatic medical image segmentation due to powerful deep representation learning. The influence of transformer has led to research into its variants, and large-scale replacement of traditional CNN modules. However, such trend often overlooks the intrinsic feature extraction capabilities of the transformer and potential refinements to both the model and the transformer module through minor adjustments. This study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to introduce the Transformer and dual attention block into the encoder and decoder of the traditional U-shaped architecture. Unlike prior transformer-based solutions, our DA-TransUNet utilizes attention mechanism of transformer and multifaceted feature extraction of DA-Block, which can efficiently combine global, local, and multi-scale features to enhance medical image segmentation. Meanwhile, experimental results show that a dual attention block is added bef
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20869;&#22312;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#23398;&#20064;&#21644;&#20869;&#22312;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24335;&#32467;&#26524;&#30340;&#32508;&#21512;&#22788;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#32771;&#34385;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08100</link><description>&lt;p&gt;
&#29983;&#25104;&#20869;&#22312;&#20248;&#21270;&#65306;&#20855;&#26377;&#27169;&#22411;&#23398;&#20064;&#30340;&#20869;&#22312;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Generative Intrinsic Optimization: Intrisic Control with Model Learning. (arXiv:2310.08100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08100
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20869;&#22312;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#23398;&#20064;&#21644;&#20869;&#22312;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24335;&#32467;&#26524;&#30340;&#32508;&#21512;&#22788;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#32771;&#34385;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#24207;&#21015;&#20195;&#34920;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#21160;&#20316;&#21518;&#30340;&#32467;&#26524;&#12290;&#24403;&#22522;&#20110;&#20449;&#24687;&#35770;&#27010;&#24565;&#30340;&#30456;&#20114;&#20449;&#24687;&#39537;&#21160;&#26102;&#65292;&#23427;&#23547;&#27714;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#32467;&#26524;&#12290;&#26174;&#24335;&#32467;&#26524;&#21487;&#33021;&#22240;&#29366;&#24577;&#12289;&#22238;&#25253;&#25110;&#36712;&#36857;&#32780;&#24322;&#65292;&#29992;&#20110;&#19981;&#21516;&#30446;&#30340;&#65292;&#22914;&#23398;&#20998;&#20998;&#37197;&#25110;&#27169;&#20223;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23558;&#20869;&#22312;&#21160;&#26426;&#19982;&#22870;&#21169;&#26368;&#22823;&#21270;&#32467;&#21512;&#30340;&#22266;&#26377;&#24615;&#36136;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#65292;&#20849;&#21516;&#23398;&#20064;&#20272;&#35745;&#30456;&#20114;&#20449;&#24687;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#24517;&#35201;&#25968;&#37327;&#65292;&#20026;&#21512;&#24182;&#19981;&#21516;&#24418;&#24335;&#30340;&#24863;&#20852;&#36259;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;&#32467;&#21512;&#21040;&#31574;&#30053;&#36845;&#20195;&#26041;&#26696;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#34429;&#28982;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#29702;&#35770;&#20998;&#26512;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#25171;&#24320;&#20102;&#21033;&#29992;&#24102;&#26377;&#27169;&#22411;&#23398;&#20064;&#30340;&#20869;&#22312;&#25511;&#21046;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#32435;&#20837;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future sequence represents the outcome after executing the action into the environment. When driven by the information-theoretic concept of mutual information, it seeks maximally informative consequences. Explicit outcomes may vary across state, return, or trajectory serving different purposes such as credit assignment or imitation learning. However, the inherent nature of incorporating intrinsic motivation with reward maximization is often neglected. In this work, we propose a variational approach to jointly learn the necessary quantity for estimating the mutual information and the dynamics model, providing a general framework for incorporating different forms of outcomes of interest. Integrated into a policy iteration scheme, our approach guarantees convergence to the optimal policy. While we mainly focus on theoretical analysis, our approach opens the possibilities of leveraging intrinsic control with model learning to enhance sample efficiency and incorporate uncertainty of the env
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07838</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#20174;&#25945;&#24072;&#21040;&#27010;&#29575;&#21270;&#23398;&#29983;&#20998;&#31867;&#22120;&#30340;n&#20010;&#26679;&#26412;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#20013;&#36755;&#20837;&#31354;&#38388;S&#21644;&#26631;&#31614;A&#20026;&#26377;&#38480;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#28176;&#36827;&#32423;&#21035;&#19978;&#30340;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#21152;&#24555;&#20256;&#36882;&#30340;&#36895;&#24230;&#12290;&#22312;&#31532;&#19968;&#32423;&#21035;&#19978;&#65292;&#21482;&#26377;&#20855;&#26377;&#22256;&#38590;&#26631;&#31614;&#30340;&#26679;&#26412;&#26159;&#24050;&#30693;&#30340;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#33021;&#22815;&#36798;&#21040;&#26368;&#23567;&#21270;&#36895;&#29575;sqrt(|S||A|/n)&#12290;&#31532;&#20108;&#32423;&#21035;&#19978;&#65292;&#38500;&#20102;&#24050;&#30693;&#30340;&#22256;&#38590;&#26631;&#31614;&#26679;&#26412;&#22806;&#65292;&#36824;&#26377;&#37319;&#26679;&#26631;&#31614;&#30340;&#25945;&#24072;&#27010;&#29575;&#21487;&#29992;&#65292;&#36825;&#23558;&#25910;&#25947;&#36895;&#24230;&#30340;&#19979;&#30028;&#25552;&#39640;&#21040;|S||A|/n&#12290;&#28982;&#32780;&#65292;&#22312;&#31532;&#20108;&#20010;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#19979;&#65292;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26420;&#32032;&#36866;&#24212;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#24046;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#21464;&#20307;&#30340;&#24179;&#26041;&#35823;&#24046;&#36923;&#36753;&#25439;&#22833;&#26469;&#23454;&#29616;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#31532;&#19977;&#32423;&#21035;&#36827;&#19968;&#27493;&#36171;&#20104;&#23398;&#29983;&#36719;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#35299;&#20915;&#20102;&#23545;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#24573;&#30053;&#20505;&#36873;&#35299;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05166</link><description>&lt;p&gt;
&#19968;&#20010;&#22312;&#26377;&#22122;&#22768;&#35266;&#27979;&#19979;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Corrected Expected Improvement Acquisition Function Under Noisy Observations. (arXiv:2310.05166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#35299;&#20915;&#20102;&#23545;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#24573;&#30053;&#20505;&#36873;&#35299;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26368;&#22823;&#21270;&#26399;&#26395;&#25913;&#21892;(EI)&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26368;&#24120;&#29992;&#30340;&#31574;&#30053;&#20043;&#19968;&#65292;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#22788;&#29702;&#22122;&#22768;&#35266;&#27979;&#30340;&#33021;&#21147;&#32780;&#24191;&#27867;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#25913;&#21892;&#20989;&#25968;&#36890;&#24120;&#20351;&#29992;&#26368;&#20339;&#21518;&#39564;&#22343;&#20540;&#20316;&#20026;&#26368;&#20339;&#20505;&#36873;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#35299;&#26512;&#30340;EI&#31867;&#22411;&#26041;&#27861;&#20013;&#65292;&#24120;&#24120;&#24573;&#30053;&#19982;&#20505;&#36873;&#35299;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#22312;&#26080;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#24418;&#24335;&#30340;&#37319;&#38598;&#20989;&#25968;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;EI&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;(GP)&#27169;&#22411;&#25552;&#20379;&#30340;&#21327;&#26041;&#24046;&#20449;&#24687;&#32435;&#20837;&#20854;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#20013;&#12290;&#36825;&#20010;&#37319;&#38598;&#20989;&#25968;&#19982;&#32463;&#20856;&#30340;&#26080;&#22122;&#22768;&#32467;&#26524;&#30456;&#21563;&#21512;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#24212;&#35813;&#21462;&#20195;&#36125;&#21494;&#26031;&#20248;&#21270;&#36719;&#20214;&#21253;&#12289;&#25945;&#31243;&#21644;&#25945;&#26448;&#20013;&#30340;&#37027;&#20010;&#20844;&#24335;&#12290;&#36825;&#20010;&#25913;&#36827;&#30340;&#37319;&#38598;&#20989;&#25968;&#20026;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#30340;&#35299;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;</title><link>http://arxiv.org/abs/2309.14610</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#28304;&#20110;&#19982;&#27946;&#27700;&#21361;&#38505;&#12289;&#27946;&#27700;&#26292;&#38706;&#20197;&#21450;&#31038;&#20250;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#27946;&#27700;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#34920;&#24449;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#27946;&#27700;&#24179;&#21407;&#22320;&#22270;&#65292;&#20391;&#37325;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#35201;&#32032;&#65292;&#20027;&#35201;&#26159;&#21361;&#38505;&#21644;&#26292;&#38706;&#35201;&#32032;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#31354;&#38388;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;FloodRisk-Net&#65289;&#30340;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#12290;FloodRisk-Net&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#27946;&#27700;&#21361;&#38505;&#21644;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#30830;&#23450;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#12290;&#21033;&#29992;&#32654;&#22269;&#22810;&#20010;&#37117;&#24066;&#32479;&#35745;&#21306;&#65288;MSAs&#65289;&#30340;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#27946;&#27700;&#39118;&#38505;&#29305;&#24449;&#21270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#37329;&#34701;&#30456;&#20851;&#30697;&#38453;&#37327;&#21270;&#20449;&#36151;&#32452;&#21512;&#23545;&#36164;&#20135;&#30456;&#20851;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#24433;&#21709;&#25237;&#36164;&#32452;&#21512;&#22810;&#20803;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.08652</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#20449;&#36151;&#32452;&#21512;&#23545;&#36164;&#20135;&#30456;&#20851;&#24615;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Credit Portfolio sensitivity to asset correlations with interpretable generative neural networks. (arXiv:2309.08652v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#37329;&#34701;&#30456;&#20851;&#30697;&#38453;&#37327;&#21270;&#20449;&#36151;&#32452;&#21512;&#23545;&#36164;&#20135;&#30456;&#20851;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#24433;&#21709;&#25237;&#36164;&#32452;&#21512;&#22810;&#20803;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#37329;&#34701;&#30456;&#20851;&#30697;&#38453;&#65292;&#37327;&#21270;&#20449;&#36151;&#32452;&#21512;&#20215;&#20540;&#39118;&#38505;&#65288;VaR&#65289;&#23545;&#36164;&#20135;&#30456;&#20851;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#28436;&#31034;&#20102;&#29983;&#25104;&#33021;&#25429;&#25417;&#21040;&#36164;&#20135;&#25910;&#30410;&#30340;&#32463;&#39564;&#30456;&#20851;&#30697;&#38453;&#20013;&#35266;&#23519;&#21040;&#30340;&#22522;&#26412;&#29305;&#24449;&#30340;&#21487;&#20449;&#30456;&#20851;&#30697;&#38453;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#32780;&#19981;&#26159;GANs&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;VAE&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25104;&#20026;&#25429;&#25417;&#24433;&#21709;&#25237;&#36164;&#32452;&#21512;&#22810;&#20803;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#19982;&#20449;&#36151;&#32452;&#21512;&#25935;&#24863;&#24615;&#21644;&#36164;&#20135;&#30456;&#20851;&#24615;&#21464;&#21270;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we propose a novel approach for the quantification of credit portfolio Value-at-Risk (VaR) sensitivity to asset correlations with the use of synthetic financial correlation matrices generated with deep learning models. In previous work Generative Adversarial Networks (GANs) were employed to demonstrate the generation of plausible correlation matrices, that capture the essential characteristics observed in empirical correlation matrices estimated on asset returns. Instead of GANs, we employ Variational Autoencoders (VAE) to achieve a more interpretable latent space representation. Through our analysis, we reveal that the VAE latent space can be a useful tool to capture the crucial factors impacting portfolio diversification, particularly in relation to credit portfolio sensitivity to asset correlations changes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39640;&#25928;&#39044;&#27979;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24494;&#32467;&#26500;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00305</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#26448;&#26009;&#31185;&#23398;&#27169;&#25311;&#20195;&#29702;&#27169;&#22411;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24494;&#32467;&#26500;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties. (arXiv:2309.00305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39640;&#25928;&#39044;&#27979;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24494;&#32467;&#26500;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#27668;&#35937;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#24037;&#31243;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#30830;&#23450;&#12289;&#29702;&#35299;&#21644;&#39044;&#27979;&#25152;&#35859;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#32467;&#26500;&#25351;&#30340;&#26159;&#29289;&#36136;&#12289;&#26448;&#26009;&#25110;&#29289;&#36136;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#32780;&#24615;&#36136;&#26159;&#19968;&#31181;&#32467;&#26524;&#29305;&#24615;&#65292;&#36890;&#24120;&#20197;&#38750;&#24179;&#20961;&#30340;&#26041;&#24335;&#21462;&#20915;&#20110;&#32467;&#26500;&#30340;&#31354;&#38388;&#32454;&#33410;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#37319;&#29992;&#27491;&#21521;&#27169;&#25311;&#27169;&#22411;&#36827;&#34892;&#12290;&#26368;&#36817;&#65292;&#22312;&#36825;&#20123;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#24212;&#29992;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#22686;&#24378;&#21644;&#21152;&#36895;&#27169;&#25311;&#27169;&#22411;&#65292;&#25110;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#30740;&#31350;&#20102;&#22522;&#20110;&#20004;&#20010;&#19981;&#21516;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#30340;&#20845;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65306;&#29992;&#20110;&#39044;&#27979;&#30913;&#24615;&#22495;&#24418;&#25104;&#30340;&#20108;&#32500;&#28784;&#24230;&#27169;&#22411;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#34920;&#31034;&#21452;&#30456;&#24494;&#32467;&#26500;&#28436;&#21464;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining, understanding, and predicting the so-called structure-property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures fro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13816</link><description>&lt;p&gt;
&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Homological Convolutional Neural Networks. (arXiv:2308.13816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#65292;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#27604;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#19988;&#21516;&#26679;&#26377;&#25928;&#12290;&#36825;&#26159;&#22240;&#20026;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#38388;&#30340;&#30456;&#20851;&#24615;&#27604;&#22270;&#20687;&#25110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31354;&#38388;&#25110;&#35821;&#20041;&#20851;&#31995;&#35201;&#24369;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23545;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#25152;&#24471;&#27169;&#22411;&#21033;&#29992;&#20102;&#21367;&#31215;&#30340;&#33021;&#21147;&#65292;&#24182;&#22260;&#32469;&#32593;&#32476;&#25299;&#25169;&#20013;&#30340;&#26377;&#38480;&#27010;&#24565;&#26469;&#20445;&#35777;&#65288;i&#65289;&#25968;&#25454;&#19968;&#33268;&#24615;&#12289;&#65288;ii&#65289;&#34920;&#31034;&#26377;&#25928;&#24615;&#12289;&#21644;&#65288;iii&#65289;&#23481;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have demonstrated outstanding performances on classification and regression tasks on homogeneous data types (e.g., image, audio, and text data). However, tabular data still poses a challenge with classic machine learning approaches being often computationally cheaper and equally effective than increasingly complex deep learning architectures. The challenge arises from the fact that, in tabular data, the correlation among features is weaker than the one from spatial or semantic relationships in images or natural languages, and the dependency structures need to be modeled without any prior information. In this work, we propose a novel deep learning architecture that exploits the data structural organization through topologically constrained network representations to gain spatial information from sparse tabular data. The resulting model leverages the power of convolutions and is centered on a limited number of concepts from network topology to guarantee (i) a data-c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#24037;&#19994;&#26426;&#26800;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#24037;&#19994;&#26426;&#26800;&#24322;&#24120;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15807</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;&#26426;&#26800;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#31181;&#31995;&#32479;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping. (arXiv:2307.15807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#24037;&#19994;&#26426;&#26800;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#24037;&#19994;&#26426;&#26800;&#24322;&#24120;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#24037;&#19994;&#20013;&#65292;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#39044;&#38450;&#35774;&#22791;&#25925;&#38556;&#12289;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#21644;&#25552;&#39640;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#20351;&#24471;&#20174;&#24037;&#19994;&#26426;&#26800;&#20013;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#65292;&#20026;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#29983;&#25104;&#30340;&#25968;&#25454;&#37327;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#20154;&#24037;&#25163;&#21160;&#26816;&#27979;&#24322;&#24120;&#21464;&#24471;&#22256;&#38590;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#33258;&#21160;&#21270;&#24037;&#19994;&#26426;&#26800;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#27599;&#31181;&#25216;&#26415;&#22522;&#20110;&#25968;&#25454;&#30340;&#24615;&#36136;&#21644;&#30456;&#24212;&#30340;&#31995;&#32479;&#37117;&#26377;&#29305;&#23450;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#32593;&#32476;&#21644;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#38382;&#39064;&#65292;&#23545;&#20110;&#24037;&#19994;&#37096;&#38376;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30740;&#31350;&#36824;&#26410;&#28085;&#30422;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;&#26426;&#26800;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is critical in the smart industry for preventing equipment failure, reducing downtime, and improving safety. Internet of Things (IoT) has enabled the collection of large volumes of data from industrial machinery, providing a rich source of information for Anomaly Detection. However, the volume and complexity of data generated by the Internet of Things ecosystems make it difficult for humans to detect anomalies manually. Machine learning (ML) algorithms can automate anomaly detection in industrial machinery by analyzing generated data. Besides, each technique has specific strengths and weaknesses based on the data nature and its corresponding systems. However, the current systematic mapping studies on Anomaly Detection primarily focus on addressing network and cybersecurity-related problems, with limited attention given to the industrial sector. Additionally, these studies do not cover the challenges involved in using ML for Anomaly Detection in industrial machinery wi
&lt;/p&gt;</description></item><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10485</link><description>&lt;p&gt;
FinGPT: &#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10485
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#37329;&#34701;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLM&#22312;&#37329;&#34701;&#39046;&#22495;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#19968;&#33324;&#25991;&#26412;&#25968;&#25454;&#19982;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#65288;&#22823;&#23567;&#36739;&#23567;&#65289;&#65292;&#32780;&#31532;&#19968;&#20010;&#37329;&#34701;LLM&#65288;FinLLM&#65289;BloombergGPT&#26159;&#23553;&#38381;&#30340;&#65288;&#21482;&#21457;&#24067;&#20102;&#35757;&#32451;&#26085;&#24535;&#65289;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;Internet&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#23558;LLM&#27665;&#20027;&#21270;&#65292;&#30001;&#20110;&#25968;&#25454;&#26469;&#28304;&#22810;&#26679;&#12289;&#20449;&#22122;&#27604;&#20302;&#21644;&#26102;&#38388;&#26377;&#25928;&#24615;&#39640;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#21644;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#8220;&#37329;&#34701;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;FinGPT&#65289;&#8221;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#26469;&#33258;&#20114;&#32852;&#32593;&#19978;&#36229;&#36807;34&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from &gt;34 diverse sources on the Internet, p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>HyenaDNA&#26159;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21367;&#31215;&#30340;&#22522;&#22240;&#32452;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#26680;&#33527;&#37240;&#20998;&#36776;&#29575;&#19979;&#23545;&#38271;&#33539;&#22260;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.15794</link><description>&lt;p&gt;
HyenaDNA&#65306;&#21333;&#26680;&#33527;&#37240;&#20998;&#36776;&#29575;&#19979;&#30340;&#38271;&#33539;&#22260;&#22522;&#22240;&#32452;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution. (arXiv:2306.15794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15794
&lt;/p&gt;
&lt;p&gt;
HyenaDNA&#26159;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21367;&#31215;&#30340;&#22522;&#22240;&#32452;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#26680;&#33527;&#37240;&#20998;&#36776;&#29575;&#19979;&#23545;&#38271;&#33539;&#22260;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32452;&#65288;DNA&#65289;&#24207;&#21015;&#32534;&#30721;&#20102;&#22823;&#37327;&#20851;&#20110;&#22522;&#22240;&#35843;&#25511;&#21644;&#34507;&#30333;&#36136;&#21512;&#25104;&#30340;&#20449;&#24687;&#12290;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22522;&#22240;&#32452;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#38750;&#26631;&#35760;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#22914;&#35782;&#21035;&#35843;&#25511;&#20803;&#20214;&#12290;&#30001;&#20110;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#25193;&#23637;&#65292;&#20808;&#21069;&#22522;&#20110;Transformer&#30340;&#22522;&#22240;&#32452;&#27169;&#22411;&#20165;&#20351;&#29992;512&#21040;4k&#20010;&#26631;&#35760;&#20316;&#20026;&#19978;&#19979;&#25991;&#65288;&lt;0.001%&#30340;&#20154;&#31867;&#22522;&#22240;&#32452;&#65289;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;DNA&#30340;&#38271;&#33539;&#22260;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#26631;&#35760;&#22120;&#26469;&#32858;&#21512;&#26377;&#24847;&#20041;&#30340;DNA&#21333;&#20803;&#65292;&#20002;&#22833;&#20102;&#21333;&#26680;&#33527;&#37240;&#20998;&#36776;&#29575;&#65292;&#20854;&#20013;&#24494;&#23567;&#30340;&#36951;&#20256;&#21464;&#24322;&#21487;&#20197;&#36890;&#36807;&#21333;&#26680;&#33527;&#37240;&#22810;&#24577;&#24615;&#65288;SNP&#65289;&#23436;&#20840;&#25913;&#21464;&#34507;&#30333;&#36136;&#21151;&#33021;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#38544;&#24335;&#21367;&#31215;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Hyena&#26174;&#31034;&#20986;&#33021;&#22815;&#19982;&#27880;&#24847;&#21147;&#36136;&#37327;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#20801;&#35768;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#26356;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;&#21033;&#29992;Hyenas n
&lt;/p&gt;
&lt;p&gt;
Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (&lt;0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas n
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32508;&#36848;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#22312;&#21387;&#32553;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13724</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#21387;&#32553;&#23884;&#20837;&#23618;&#21450;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of compressed embedding layers and their applications for recommender systems. (arXiv:2306.13724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13724
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32508;&#36848;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#22312;&#21387;&#32553;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#39038;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#30340;&#25991;&#29486;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#21387;&#32553;&#24040;&#22411;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#21387;&#32553;&#23884;&#20837;&#23618;&#25152;&#27979;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We review the literature on trainable, compressed embedding layers and discuss their applicability for compressing gigantic neural recommender systems. We also report the results we measured with our compressed embedding layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#33021;&#22312;&#20445;&#25345;&#25968;&#25454;&#32431;&#24230;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25628;&#32034;&#36895;&#24230;&#12289;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#21644;&#25628;&#32034;&#28789;&#25935;&#24230;&#65292;&#32780;&#35813;&#27169;&#22411;&#22312;LIGO&#25968;&#25454;&#19978;&#27979;&#35797;&#30340;&#34920;&#29616;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11797</link><description>&lt;p&gt;
&#38024;&#23545;&#24341;&#21147;&#27874;&#25968;&#25454;&#20013;&#30340;&#32039;&#20945;&#20108;&#36827;&#21046;&#21512;&#24182;&#20107;&#20214;&#26816;&#27979;&#65292;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#31283;&#20581;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Towards a robust and reliable deep learning approach for detection of compact binary mergers in gravitational wave data. (arXiv:2306.11797v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#33021;&#22312;&#20445;&#25345;&#25968;&#25454;&#32431;&#24230;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25628;&#32034;&#36895;&#24230;&#12289;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#21644;&#25628;&#32034;&#28789;&#25935;&#24230;&#65292;&#32780;&#35813;&#27169;&#22411;&#22312;LIGO&#25968;&#25454;&#19978;&#27979;&#35797;&#30340;&#34920;&#29616;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#27867;&#21270;&#30340;&#20449;&#21495;&#21644;&#22122;&#22768;&#27169;&#22411;&#20197;&#21450;&#22312;GPU&#19978;&#24555;&#36895;&#25512;&#26029;&#65292;&#20026;&#22686;&#24378;&#24341;&#21147;&#27874;&#65288;GW&#65289;&#25628;&#32034;&#30340;&#36895;&#24230;&#12289;&#21442;&#25968;&#31354;&#38388;&#35206;&#30422;&#21644;&#25628;&#32034;&#28789;&#25935;&#24230;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#20294;&#26159;DL&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#20005;&#37325;&#25439;&#23475;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36880;&#27493;&#24320;&#21457;&#19968;&#20010;DL&#27169;&#22411;&#65292;&#24182;&#21162;&#21147;&#25913;&#36827;&#20854;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#32431;&#24230;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23548;&#20986;&#19968;&#31181;&#26356;&#22909;&#22320;&#21453;&#26144;&#25968;&#25454;&#20013;&#8220;&#21825;&#21886;&#8221;&#20449;&#21495;&#29305;&#24449;&#35270;&#35273;&#24378;&#24230;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#12290;&#21033;&#29992;&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#33719;&#24471;&#30340;&#20943;&#23569;&#12289;&#24179;&#28369;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#25628;&#32034;&#32039;&#20945;&#20108;&#36827;&#21046;&#21512;&#24182;&#65288;CBC&#65289;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;LIGO&#25968;&#25454;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#26102;&#65292;&#20854;&#31616;&#21333;&#25925;&#38556;&#27169;&#24335;&#21017;&#25104;&#20026;&#20102;&#19968;&#20010;&#30171;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of deep learning (DL) approaches to learn generalised signal and noise models, coupled with their fast inference on GPUs, holds great promise for enhancing gravitational-wave (GW) searches in terms of speed, parameter space coverage, and search sensitivity. However, the opaque nature of DL models severely harms their reliability. In this work, we meticulously develop a DL model stage-wise and work towards improving its robustness and reliability. First, we address the problems in maintaining the purity of training data by deriving a new metric that better reflects the visual strength of the "chirp" signal features in the data. Using a reduced, smooth representation obtained through a variational auto-encoder (VAE), we build a classifier to search for compact binary coalescence (CBC) signals. Our tests on real LIGO data show an impressive performance of the model. However, upon probing the robustness of the model through adversarial attacks, its simple failure modes were ide
&lt;/p&gt;</description></item><item><title>&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;&#30740;&#31350;&#25506;&#32034;&#20102;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#26159;&#21542;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#24490;&#29615;&#30456;&#21464;&#65292;&#24182;&#35777;&#26126;&#20102;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26222;&#36941;&#26426;&#21046;&#21487;&#33021;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.08698</link><description>&lt;p&gt;
&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase Transitions of Civil Unrest across Countries and Time. (arXiv:2306.08698v2 [physics.soc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08698
&lt;/p&gt;
&lt;p&gt;
&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;&#30740;&#31350;&#25506;&#32034;&#20102;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#26159;&#21542;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#24490;&#29615;&#30456;&#21464;&#65292;&#24182;&#35777;&#26126;&#20102;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26222;&#36941;&#26426;&#21046;&#21487;&#33021;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#21464;&#26159;&#22797;&#26434;&#31995;&#32479;&#20013;&#31361;&#21457;&#36716;&#21464;&#30340;&#29305;&#24449;&#65292;&#23613;&#31649;&#22312;&#29289;&#29702;&#21644;&#33258;&#28982;&#31185;&#23398;&#20013;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#22312;&#31038;&#20250;&#31995;&#32479;&#20013;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#30340;&#21160;&#21147;&#23398;&#26159;&#21542;&#21487;&#20197;&#34987;&#21512;&#29702;&#22320;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#24490;&#29615;&#30456;&#21464;&#65292;&#20854;&#20013;&#27599;&#20010;&#38454;&#27573;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23439;&#35266;&#27700;&#24179;&#30340;&#31038;&#20250;&#21160;&#33633;&#32479;&#35745;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21253;&#25324;1946&#24180;&#33267;2017&#24180;&#22312;&#20869;&#30340;170&#20010;&#22269;&#23478;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20854;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#26222;&#36941;&#30340;&#26426;&#21046;&#21487;&#33021;&#28508;&#22312;&#22320;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#34920;&#26469;&#34913;&#37327;&#19968;&#20010;&#22269;&#23478;&#30340;&#31038;&#20250;&#21160;&#33633;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase transitions, characterized by abrupt shifts between macroscopic patterns of organization, are ubiquitous in complex systems. Despite considerable research in the physical and natural sciences, the empirical study of this phenomenon in societal systems is relatively underdeveloped. The goal of this study is to explore whether the dynamics of collective civil unrest can be plausibly characterized as a sequence of recurrent phase shifts, with each phase having measurable and identifiable latent characteristics. We introduce a macro-level statistical model of civil unrest and evaluate its plausibility using a comprehensive dataset of civil unrest events in 170 countries from 1946 to 2017. Our findings demonstrate that the macro-level phase model effectively captures the characteristics of civil unrest data from diverse countries globally and that universal mechanisms may underlie certain aspects of the dynamics of civil unrest. We also introduce a new scale to quantify a country's lo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#22270;&#20687;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#28789;&#27963;&#20284;&#28982;&#65292;&#32467;&#21512;&#20004;&#31181;&#26694;&#26550;&#30340;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;SVAE&#30340;&#26032;&#31639;&#27861;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#20248;&#21270;&#21019;&#26032;&#20351;&#24471;SVAE&#39318;&#27425;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.08230</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#31163;&#25955;&#34920;&#31034;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26080;&#20559;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unbiased Learning of Deep Generative Models with Structured Discrete Representations. (arXiv:2306.08230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#22270;&#20687;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#28789;&#27963;&#20284;&#28982;&#65292;&#32467;&#21512;&#20004;&#31181;&#26694;&#26550;&#30340;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;SVAE&#30340;&#26032;&#31639;&#27861;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#20248;&#21270;&#21019;&#26032;&#20351;&#24471;SVAE&#39318;&#27425;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22270;&#24418;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#32452;&#21512;&#65292;&#25105;&#20204;&#23398;&#20064;&#20855;&#26377;&#20004;&#31181;&#26694;&#26550;&#20248;&#21183;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290; &#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;SVAE&#65289;&#20174;&#22270;&#24418;&#27169;&#22411;&#32487;&#25215;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#28145;&#24230;&#23398;&#20064;&#20013;&#32487;&#25215;&#20102;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#28789;&#27963;&#20284;&#28982;&#65292;&#20294;&#26159;&#20250;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#20248;&#21270;&#25361;&#25112;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;SVAE&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#35777;&#26126;&#20102;SVAE&#22312;&#21547;&#26377;&#32570;&#22833;&#25968;&#25454;&#19988;&#21253;&#21547;&#31163;&#25955;&#28508;&#21464;&#37327;&#26102;&#22788;&#29702;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20869;&#23384;&#39640;&#25928;&#38544;&#24335;&#24494;&#20998;&#26041;&#26696;&#20351;&#24471;SVAE&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26469;&#23398;&#20064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#26356;&#24555;&#22320;&#23398;&#20064;&#20934;&#30830;&#30340;&#22270;&#24418;&#27169;&#22411;&#21442;&#25968;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#23548;&#20986;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#21457;&#29616;&#30340;&#20559;&#24046;&#12290;&#36825;&#20123;&#20248;&#21270;&#21019;&#26032;&#20351;&#24471;&#39318;&#27425;&#33021;&#22815;&#23558;SVAE&#19982;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges. We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables. Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work. These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06210</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#32456;&#23618;&#21453;&#28436;&#36827;&#34892;&#21333;&#27169;&#22411;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#24320;&#21019;&#24615;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#23454;&#29992;&#21333;&#27169;&#22411;&#24402;&#22240;&#30340;&#20852;&#36259;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#19968;&#20010;&#26679;&#26412;&#26159;&#30001;&#29305;&#23450;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#19981;&#26159;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#35777;&#26126;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#65292;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FLIPAD&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#21033;&#29992;&#30340;&#26368;&#32456;&#23618;&#21453;&#28436;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#20984;&#30340; Lasso &#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#29702;&#35770;&#32467;&#26524;&#36824;&#24471;&#21040;&#20102;&#23454;&#39564;&#30740;&#31350;&#30340;&#25903;&#25345;&#65292;&#35777;&#26126;&#26412;&#25991;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#38024;&#23545;&#36229;&#21442;&#25968;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;SGLD&#20449;&#24687;&#20934;&#21017;&#65292;&#36890;&#36807;KL&#20449;&#24687;&#21644;KL&#25955;&#24230;&#32602;&#39033;&#26469;&#36861;&#36394;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2306.05583</link><description>&lt;p&gt;
&#22522;&#20110;SGLD&#30340;&#20449;&#24687;&#20934;&#21017;&#19982;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SGLD-Based Information Criteria and the Over-Parameterized Regime. (arXiv:2306.05583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#38024;&#23545;&#36229;&#21442;&#25968;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;SGLD&#20449;&#24687;&#20934;&#21017;&#65292;&#36890;&#36807;KL&#20449;&#24687;&#21644;KL&#25955;&#24230;&#32602;&#39033;&#26469;&#36861;&#36394;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#21452;&#19992;&#38517;&#8221;&#26159;&#25351;&#36807;&#24230;&#21442;&#25968;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25554;&#20540;&#38408;&#20540;&#20043;&#22806;&#30340;&#24847;&#22806;&#27979;&#35797;&#25439;&#22833;&#19979;&#38477;&#65292;&#36825;&#19981;&#26159;&#30001;&#20110;&#26631;&#20934;&#28176;&#36827;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#32463;&#20856;&#24418;&#24335;&#30340;&#20449;&#24687;&#20934;&#21017;&#26080;&#27861;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#26356;&#26032;&#36825;&#20123;&#20998;&#26512;&#65292;&#24182;&#20026;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;Akaike&#20449;&#24687;&#20934;&#21017;&#65288;AIC&#65289;&#21644;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65288;BIC&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SGLD&#30340;AIC&#21644;BIC&#32602;&#39033;&#23545;&#24212;&#29305;&#23450;&#30340;&#20449;&#24687;&#24230;&#37327;&#65292;&#21363;&#23545;&#31216;&#30340;KL&#20449;&#24687;&#21644;KL&#25955;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#34920;&#24449;&#22823;&#37327;&#21442;&#25968;&#27169;&#22411;&#30340;SGLD-BIC&#25193;&#23637;&#20102;&#27492;&#20449;&#24687;&#29702;&#35770;&#20998;&#26512;&#65292;&#20854;&#20013;&#21442;&#25968;&#25968;$p$&#21644;&#26679;&#26412;&#25968;$n$&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;$p/n$&#22266;&#23450;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;SGLD-BIC&#21487;&#20197;&#36319;&#36394;&#21452;&#19979;&#38477;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Double-descent refers to the unexpected drop in test loss of a learning algorithm beyond an interpolating threshold with over-parameterization, which is not predicted by information criteria in their classical forms due to the limitations in the standard asymptotic approach. We update these analyses using the information risk minimization framework and provide Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for models learned by stochastic gradient Langevin dynamics (SGLD). Notably, the AIC and BIC penalty terms for SGLD correspond to specific information measures, i.e., symmetrized KL information and KL divergence. We extend this information-theoretic analysis to over-parameterized models by characterizing the SGLD-based BIC for the random feature model in the regime where the number of parameters $p$ and the number of samples $n$ tend to infinity, with $p/n$ fixed. Our experiments demonstrate that the refined SGLD-based BIC can track the double-descent cur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.00183</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#25193;&#25955;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#34920;&#31034;&#24050;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#21152;&#28145;&#20837;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#26159;&#22914;&#20309;&#34987;&#32534;&#30721;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#23618;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#23637;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#23545;&#20110;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#22823;&#23567;&#30340;&#20219;&#20309;&#38543;&#26426;&#23376;&#38598;&#31070;&#32463;&#20803;&#65292;&#37117;&#19982;&#23436;&#25972;&#23618;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#25972;&#20010;&#23618;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#65288;&#21253;&#25324;CNN&#21644;Transformer&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;ImageNet1k&#21644;ImageNet21k&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#26469;&#38477;&#20302;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00041</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#21487;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#30740;&#31350;&#19982;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Research And Implementation Of Drug Target Interaction Confidence Measurement Method Based On Causal Intervention. (arXiv:2306.00041v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#19982;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#65288;DTI&#65289;&#30340;&#35782;&#21035;&#21644;&#21457;&#29616;&#26159;&#33647;&#29289;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#21487;&#20197;&#24110;&#21161;&#31185;&#23398;&#23478;&#21457;&#29616;&#26032;&#33647;&#24182;&#21152;&#36895;&#24320;&#21457;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#30693;&#35782;&#22270;&#35889;&#21644;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#36805;&#36895;&#21457;&#23637;&#24182;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;&#22312;&#33647;&#29289;&#38774;&#28857;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#21644;&#20934;&#30830;&#24615;&#19981;&#36275;&#20250;&#23548;&#33268;&#35823;&#21028;&#29575;&#22686;&#21152;&#21644;&#33647;&#29289;&#24320;&#21457;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#20197;&#30693;&#35782;&#26144;&#23556;&#20026;&#26680;&#24515;&#25216;&#26415;&#30340;&#33647;&#29289;&#38774;&#28857;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#26469;&#27979;&#37327;&#19977;&#20803;&#32452;&#24471;&#20998;&#65292;&#20174;&#32780;&#25552;&#39640;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;KGE&#27169;&#22411;&#19978;&#19982;&#20256;&#32479;&#30340;Softmax&#21644;Sigmod&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
The identification and discovery of drug-target Interaction (DTI) is an important step in the field of Drug research and development, which can help scientists discover new drugs and accelerate the development process. KnowledgeGraph and the related knowledge graph Embedding (KGE) model develop rapidly and show good performance in the field of drug discovery in recent years. In the task of drug target identification, the lack of authenticity and accuracy of the model will lead to the increase of misjudgment rate and the low efficiency of drug development. To solve the above problems, this study focused on the problem of drug target link prediction with knowledge mapping as the core technology, and adopted the confidence measurement method based on causal intervention to measure the triplet score, so as to improve the accuracy of drug target interaction prediction model. By comparing with the traditional Softmax and Sigmod confidence measurement methods on different KGE models, the resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#35266;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#33021;&#22815;&#33258;&#21160;&#28385;&#36275;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#29992;&#25143;&#21487;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#26465;&#20214;&#65292;&#22312;&#20004;&#20010;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26377;&#24378;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06193</link><description>&lt;p&gt;
&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#25910;&#25947;&#21644;Lipschitz&#38381;&#29615;&#23398;&#20064;&#31574;&#30053;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems. (arXiv:2304.06193v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#35266;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#33021;&#22815;&#33258;&#21160;&#28385;&#36275;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#29992;&#25143;&#21487;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#26465;&#20214;&#65292;&#22312;&#20004;&#20010;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26377;&#24378;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#35266;&#27979;&#21160;&#24577;&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Youla &#21442;&#25968;&#21270;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340; REN &#27169;&#22411;&#30340;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35813;&#31574;&#30053;&#21442;&#25968;&#21270;&#33258;&#21160;&#28385;&#36275;&#20102;&#38381;&#29615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65288;&#25910;&#25947;&#65289;&#21644;&#29992;&#25143;&#21487;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#65288;Lipschitz&#65289;&#26465;&#20214;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#21487;&#20197;&#29992;&#20110;&#23433;&#20840;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32422;&#26463;&#25110;&#25237;&#24433;&#26469;&#24378;&#21046;&#31283;&#23450;&#24615;&#25110;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#25311;&#20013;&#27979;&#35797;&#20102;&#26032;&#30340;&#31574;&#30053;&#31867;&#65306;1&#65289;&#30913;&#24748;&#28014;&#65292;2&#65289;&#20498;&#32622;&#26059;&#36716;&#33218;&#25670;&#12290;&#25105;&#20204;&#21457;&#29616; Youla-REN &#22312;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#23637;&#31034;&#23545;&#25932;&#23545;&#25200;&#21160;&#25552;&#39640;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#29616;&#26377;&#30340;&#23398;&#20064;&#25511;&#21046;&#21644;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a policy parameterization for learning-based control on nonlinear, partially-observed dynamical systems. The parameterization is based on a nonlinear version of the Youla parameterization and the recently proposed Recurrent Equilibrium Network (REN) class of models. We prove that the resulting Youla-REN parameterization automatically satisfies stability (contraction) and user-tunable robustness (Lipschitz) conditions on the closed-loop system. This means it can be used for safe learning-based control with no additional constraints or projections required to enforce stability or robustness. We test the new policy class in simulation on two reinforcement learning tasks: 1) magnetic suspension, and 2) inverting a rotary-arm pendulum. We find that the Youla-REN performs similarly to existing learning-based and optimal control methods while also ensuring stability and exhibiting improved robustness to adversarial disturbances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;DWA&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#22312;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#38598;&#25104;DWA&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01994</link><description>&lt;p&gt;
DWA&#65306;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2304.01994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;DWA&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#22312;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#38598;&#25104;DWA&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;(DWA)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;(SR)&#27169;&#22359;&#12290;DWA&#20026;&#26368;&#36817;&#25910;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#28151;&#21512;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;(DWT)&#26041;&#27861;&#27880;&#20837;&#27963;&#21147;&#12290;DWT&#33021;&#22815;&#26377;&#25928;&#22320;&#20026;SR&#25552;&#20379;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#31354;&#38388;&#38754;&#31215;&#20943;&#23569;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#27169;&#22411;&#24635;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#25104;&#20026;&#21487;&#25345;&#32493;ML&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DWA&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#65292;&#24378;&#35843;&#23616;&#37096;&#23545;&#27604;&#24230;&#24182;&#25233;&#21046;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#24120;&#35265;&#22122;&#22768;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;SR&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;DWA&#20351;DWSR&#21644;MWCNN&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#36755;&#20837;&#22270;&#20687;&#31354;&#38388;&#65292;&#22240;&#20026;&#23427;&#30465;&#30053;&#20102;DWT&#34920;&#31034;&#30340;&#36890;&#36947;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.10167</link><description>&lt;p&gt;
&#24191;&#20041;&#21010;&#20998;&#23616;&#37096;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#36817;&#30001;Berenhaut&#12289;Moore&#21644;Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]&#25552;&#20986;&#30340;&#20957;&#32858;&#27010;&#24565;&#30340;&#27010;&#25324;&#12290;&#25152;&#25552;&#20986;&#30340;&#34920;&#36848;&#22522;&#20110;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#24182;&#25552;&#28860;&#20102;&#20004;&#20010;&#20851;&#38190;&#27010;&#29575;&#27010;&#24565;&#65306;&#23616;&#37096;&#30456;&#20851;&#24615;&#21644;&#25903;&#25345;&#20998;&#21106;&#12290;&#26089;&#26399;&#32467;&#26524;&#22312;&#26032;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#25193;&#23637;&#65292;&#24182;&#21253;&#25324;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#20013;&#25581;&#31034;&#31038;&#21306;&#30340;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
&lt;/p&gt;</description></item><item><title>&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;</title><link>http://arxiv.org/abs/2302.08942</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08942
&lt;/p&gt;
&lt;p&gt;
&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#24320;&#21457;&#20102;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#31532;&#19968;&#20010;&#20851;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#20363;&#31354;&#38388;&#26159;&#26377;&#30028;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#32467;&#26524;&#21033;&#29992;&#20102;&#38477;&#32500;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#33258;&#28982;&#36866;&#29992;&#20110;Wasserstein GAN&#21644;Energy-Based GAN&#65292;&#32780;&#25105;&#20204;&#30340;&#30028;&#38480;&#20026;&#36825;&#20004;&#31181;GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#29702;&#35770;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;Wasserstein GAN&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#23545;&#20110;&#27973;&#23618;ViT&#36827;&#34892;&#35757;&#32451;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;SGD&#35757;&#32451;&#20250;&#20135;&#29983;&#31232;&#30095;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#30446;&#21069;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26631;&#35760;&#30456;&#20851;&#20196;&#29260;&#30340;&#20998;&#25968;&#20498;&#25968;&#12289;&#26631;&#35760;&#32423;&#21035;&#30340;&#20196;&#29260;&#22122;&#22768;&#27700;&#24179;&#21644;&#21021;&#22987;&#27169;&#22411;&#38169;&#35823;&#21576;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.06015</link><description>&lt;p&gt;
&#27973;&#23618;&#35270;&#35273;Transformer&#30340;&#29702;&#35770;&#29702;&#35299;&#65306;&#23398;&#20064;&#12289;&#27867;&#21270;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity. (arXiv:2302.06015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#23545;&#20110;&#27973;&#23618;ViT&#36827;&#34892;&#35757;&#32451;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;SGD&#35757;&#32451;&#20250;&#20135;&#29983;&#31232;&#30095;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#30446;&#21069;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26631;&#35760;&#30456;&#20851;&#20196;&#29260;&#30340;&#20998;&#25968;&#20498;&#25968;&#12289;&#26631;&#35760;&#32423;&#21035;&#30340;&#20196;&#29260;&#22122;&#22768;&#27700;&#24179;&#21644;&#21021;&#22987;&#27169;&#22411;&#38169;&#35823;&#21576;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20855;&#26377;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23618;&#38388;&#30340;&#38750;&#20984;&#20132;&#20114;&#65292;&#29702;&#35770;&#19978;&#30340;&#23398;&#20064;&#21644;&#27867;&#21270;&#20998;&#26512;&#22823;&#22810;&#26159;&#38590;&#20197;&#29702;&#35299;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#19968;&#39033;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#19968;&#20010;&#33258;&#25105;&#27880;&#24847;&#23618;&#21644;&#20004;&#23618;&#24863;&#30693;&#26426;&#30340;&#27973;&#23618;ViT&#36827;&#34892;&#35757;&#32451;&#30340;&#31532;&#19968;&#31687;&#29702;&#35770;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#23545;&#20110;&#25968;&#25454;&#27169;&#22411;&#30340;&#25551;&#36848;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#34920;&#24449;&#26631;&#35760;&#30456;&#20851;&#21644;&#26631;&#35760;&#19981;&#30456;&#20851;&#30340;&#20196;&#29260;&#12290;&#25105;&#20204;&#30028;&#23450;&#20102;&#36798;&#21040;&#38646;&#27867;&#21270;&#35823;&#24046;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38480;&#21046;&#19982;&#26631;&#35760;&#30456;&#20851;&#20196;&#29260;&#30340;&#37096;&#20998;&#20498;&#25968;&#12289;&#26631;&#35760;&#32423;&#21035;&#30340;&#20196;&#29260;&#22122;&#22768;&#27700;&#24179;&#21644;&#21021;&#22987;&#27169;&#22411;&#35823;&#24046;&#21576;&#27491;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;SGD&#65288;stochastic gradient descent&#65289;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#20250;&#23548;&#33268;&#31232;&#30095;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#36825;&#26159;&#23545;&#20110;&#27880;&#24847;&#21147;&#25104;&#21151;&#30340;&#19968;&#31181;&#24418;&#24335;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25351;&#20986;&#65292;&#36866;&#24403;&#30340;&#20196;&#29260;&#30830;&#23450;&#26159;&#30830;&#20445;&#23454;&#29616;&#26368;&#20248;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a shallow ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token spa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32593;&#26684;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#23432;&#24658;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21253;&#25324;&#26102;&#38388;&#30456;&#20851;&#30340;Fokker-Planck&#26041;&#31243;&#21644;Wasserstein&#26799;&#24230;&#27969;&#12290;&#36890;&#36807;&#33258;&#27965;&#30340;&#36895;&#24230;&#21305;&#37197;&#26041;&#27861;&#21644;&#36845;&#20195;&#24418;&#24335;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32469;&#36807;&#20102;&#35745;&#31639;&#38556;&#30861;&#65292;&#24182;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13737</link><description>&lt;p&gt;
&#33258;&#27965;&#30340;&#27010;&#29575;&#27969;&#36895;&#24230;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Velocity Matching of Probability Flows. (arXiv:2301.13737v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13737
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32593;&#26684;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#23432;&#24658;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21253;&#25324;&#26102;&#38388;&#30456;&#20851;&#30340;Fokker-Planck&#26041;&#31243;&#21644;Wasserstein&#26799;&#24230;&#27969;&#12290;&#36890;&#36807;&#33258;&#27965;&#30340;&#36895;&#24230;&#21305;&#37197;&#26041;&#27861;&#21644;&#36845;&#20195;&#24418;&#24335;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32469;&#36807;&#20102;&#35745;&#31639;&#38556;&#30861;&#65292;&#24182;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32593;&#26684;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#19968;&#31867;&#21253;&#25324;&#26102;&#38388;&#30456;&#20851;&#30340;Fokker-Planck&#26041;&#31243;&#21644;Wasserstein&#26799;&#24230;&#27969;&#22312;&#20869;&#30340;&#23432;&#24658;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#20027;&#35201;&#35266;&#23519;&#26159;PDE&#35299;&#30340;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#22330;&#38656;&#35201;&#26159;&#33258;&#27965;&#30340;&#65306;&#23427;&#24517;&#39035;&#28385;&#36275;&#19968;&#20010;&#21253;&#21547;&#30456;&#21516;&#36895;&#24230;&#22330;&#30340;&#27010;&#29575;&#27969;&#30340;&#19981;&#21160;&#28857;&#26041;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#24418;&#24335;&#21644;&#26377;&#20559;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#32469;&#36807;&#20102;&#20855;&#26377;&#24378;&#22823;&#23454;&#35777;&#24615;&#33021;&#30340;&#37325;&#22823;&#35745;&#31639;&#38556;&#30861;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#26368;&#23567;&#21270;&#19981;&#21160;&#28857;&#26041;&#31243;&#30340;&#27531;&#24046;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21463;&#26102;&#38388;&#25110;&#31354;&#38388;&#31163;&#25955;&#21270;&#30340;&#38480;&#21046;&#65292;&#28085;&#30422;&#20102;&#26356;&#24191;&#27867;&#30340;PDEs&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#36827;&#34892;&#25193;&#23637;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#35299;&#26512;&#35299;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#35299;&#26512;&#35299;&#65292;&#24182;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#21462;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a discretization-free scalable framework for solving a large class of mass-conserving partial differential equations (PDEs), including the time-dependent Fokker-Planck equation and the Wasserstein gradient flow. The main observation is that the time-varying velocity field of the PDE solution needs to be self-consistent: it must satisfy a fixed-point equation involving the probability flow characterized by the same velocity field. Instead of directly minimizing the residual of the fixed-point equation with neural parameterization, we use an iterative formulation with a biased gradient estimator that bypasses significant computational obstacles with strong empirical performance. Compared to existing approaches, our method does not suffer from temporal or spatial discretization, covers a wider range of PDEs, and scales to high dimensions. Experimentally, our method recovers analytical solutions accurately when they are available and achieves superior performance in high dimensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#36235;&#21183;&#19982;&#27979;&#35797;&#35823;&#24046;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#24314;&#31435;&#21442;&#25968;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#26799;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#30830;&#23450;&#20102;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#21644;&#36317;&#31163;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#36317;&#31163;&#23545;&#20110;&#28145;&#24230;&#32593;&#32476;&#30340;&#20248;&#21270;&#21644;&#27169;&#22411;&#20989;&#25968;&#22797;&#26434;&#24230;&#38480;&#21046;&#26159;&#20851;&#38190;&#22240;&#32032;&#65292;&#35813;&#30740;&#31350;&#23545;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#32593;&#32476;&#30340;&#26377;&#25928;&#27169;&#22411;&#22797;&#26434;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.12309</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#32593;&#32476;&#21644;&#21452;&#37325;&#19979;&#38477;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the Lipschitz Constant of Deep Networks and Double Descent. (arXiv:2301.12309v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#36235;&#21183;&#19982;&#27979;&#35797;&#35823;&#24046;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#24314;&#31435;&#21442;&#25968;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#26799;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#30830;&#23450;&#20102;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#21644;&#36317;&#31163;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#36317;&#31163;&#23545;&#20110;&#28145;&#24230;&#32593;&#32476;&#30340;&#20248;&#21270;&#21644;&#27169;&#22411;&#20989;&#25968;&#22797;&#26434;&#24230;&#38480;&#21046;&#26159;&#20851;&#38190;&#22240;&#32032;&#65292;&#35813;&#30740;&#31350;&#23545;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#32593;&#32476;&#30340;&#26377;&#25928;&#27169;&#22411;&#22797;&#26434;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#28145;&#24230;&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;&#30028;&#38480;&#37117;&#26159;&#22522;&#20110;&#36755;&#20837;&#21464;&#37327;&#30340;&#24179;&#28369;&#25110;&#26377;&#30028;&#20381;&#36182;&#24615;&#65292;&#27809;&#26377;&#30740;&#31350;&#25506;&#31350;&#23454;&#36341;&#20013;&#25511;&#21046;&#36825;&#20123;&#22240;&#32032;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#23545;&#32463;&#21382;&#21452;&#37325;&#34928;&#20943;&#30340;&#28145;&#24230;&#32593;&#32476;&#30340;&#23454;&#39564;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#38750;&#21333;&#35843;&#30340;&#36235;&#21183;&#65292;&#19982;&#27979;&#35797;&#35823;&#24046;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#24314;&#31435;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21442;&#25968;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#26799;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#20998;&#31163;&#20986;&#20004;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#21644;&#36317;&#31163;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#36317;&#31163;&#65292;&#20998;&#21035;&#25511;&#21046;&#20851;&#38190;&#28857;&#21608;&#22260;&#30340;&#20248;&#21270;&#21160;&#24577;&#65292;&#24182;&#38480;&#21046;&#27169;&#22411;&#20989;&#25968;&#30340;&#22797;&#26434;&#24230;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#23454;&#36341;&#20013;&#32593;&#32476;&#30340;&#26377;&#25928;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing bounds on the generalization error of deep networks assume some form of smooth or bounded dependence on the input variable, falling short of investigating the mechanisms controlling such factors in practice. In this work, we present an extensive experimental study of the empirical Lipschitz constant of deep networks undergoing double descent, and highlight non-monotonic trends strongly correlating with the test error. Building a connection between parameter-space and input-space gradients for SGD around a critical point, we isolate two important factors -- namely loss landscape curvature and distance of parameters from initialization -- respectively controlling optimization dynamics around a critical point and bounding model function complexity, even beyond the training data. Our study presents novels insights on implicit regularization via overparameterization, and effective model complexity for networks trained in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Synthesis Model Bank&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;SMB&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#20809;&#29031;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09702</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;&#21512;&#25104;&#36827;&#34892;&#20809;&#29031;&#21464;&#21270;&#26657;&#27491;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Synthesis Model Bank&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;SMB&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#20809;&#29031;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#20154;&#29289;&#20877;&#35782;&#21035;&#26088;&#22312;&#20174;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#22270;&#20687;&#20013;&#23398;&#20064;&#36523;&#20221;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#22270;&#20687;&#12290;&#35768;&#22810;&#26080;&#30417;&#30563;&#20877;&#35782;&#21035;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#23427;&#20204;&#22312;&#22823;&#30340;&#39046;&#22495;&#21464;&#21270;&#65288;&#22914;&#20809;&#29031;&#12289;&#35270;&#35282;&#21644;&#36974;&#25377;&#65289;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#27169;&#22411;&#24211;&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;SMB&#21253;&#25324;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#30340;&#22810;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#29992;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#12290;&#23427;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20351;&#24471;SMB&#23545;&#20809;&#29031;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#37327;&#21270;&#20809;&#29031;&#24378;&#24230;&#24182;&#25552;&#39640;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;GAN&#30340;&#26032;&#22411;&#19977;&#32500;&#34394;&#25311;&#20154;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#22270;&#20687;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#26032;&#31639;&#27861;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#26680;&#30697;&#38453;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.12989</link><description>&lt;p&gt;
&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#25913;&#36827;&#26680;&#23545;&#40784;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Improved Kernel Alignment Regret Bound for Online Kernel Learning. (arXiv:2212.12989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#26032;&#31639;&#27861;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#26680;&#30697;&#38453;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Hinge&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#22312;&#32447;&#26680;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#26680;&#23545;&#40784;&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#22914;&#26524;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#21576;&#25351;&#25968;&#34928;&#20943;&#65292;&#21017;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$O(\ln^2{T})$&#65292;&#36951;&#25022;&#30028;&#20026;$O(\sqrt{\mathcal{A}_T})$&#12290;&#21542;&#21017;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$O(\sqrt{\mathcal{A}_TT})$&#65292;&#36951;&#25022;&#30028;&#20026;$O((\mathcal{A}_TT)^{\frac{1}{4}})$&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#25209;&#37327;&#23398;&#20064;&#65292;&#24182;&#33719;&#24471;&#20102;$O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$&#30340;&#20313;&#37327;&#39118;&#38505;&#30028;&#65292;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we improve the kernel alignment regret bound for online kernel learning in the regime of the Hinge loss function. Previous algorithm achieves a regret of $O((\mathcal{A}_TT\ln{T})^{\frac{1}{4}})$ at a computational complexity (space and per-round time) of $O(\sqrt{\mathcal{A}_TT\ln{T}})$, where $\mathcal{A}_T$ is called \textit{kernel alignment}. We propose an algorithm whose regret bound and computational complexity are better than previous results. Our results depend on the decay rate of eigenvalues of the kernel matrix. If the eigenvalues of the kernel matrix decay exponentially, then our algorithm enjoys a regret of $O(\sqrt{\mathcal{A}_T})$ at a computational complexity of $O(\ln^2{T})$. Otherwise, our algorithm enjoys a regret of $O((\mathcal{A}_TT)^{\frac{1}{4}})$ at a computational complexity of $O(\sqrt{\mathcal{A}_TT})$. We extend our algorithm to batch learning and obtain a $O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$ excess risk bound which improves the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.08253</link><description>&lt;p&gt;
HMOE: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel domain generalization method called HMOE, which does not rely on domain labels and is more interpretable. HMOE uses hypernetworks to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. Experimental results show that HMOE can divide mixed data and achieve better performance.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#36825;&#23601;&#26159;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#30340;&#30446;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;DG&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#29992;&#30340;&#39046;&#22495;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DG&#26041;&#27861;&#65292;&#31216;&#20026;HMOE&#65306;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#26631;&#31614;&#65292;&#24182;&#19988;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#12290;MoE&#22312;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#27169;&#24335;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;DG&#38382;&#39064;&#65292;&#24322;&#36136;&#24615;&#27491;&#26159;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#32780;&#20135;&#29983;&#30340;&#12290;HMOE&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#36825;&#20351;&#24471;&#19987;&#23478;&#21487;&#20197;&#20849;&#20139;&#26377;&#29992;&#30340;&#20803;&#30693;&#35782;&#65292;&#24182;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#20844;&#24179;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;-DomainBed&#19979;&#23558;HMOE&#19982;&#20854;&#20182;DG&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is what domain generalization (DG) aims to address. Although various DG methods have been developed, most of them lack interpretability and require domain labels that are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shift. HMOE uses hypernetworks taking vectors as input to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Our extensive experiments show that HMOE can divide mixe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23558;&#35813;&#20272;&#35745;&#22120;&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2206.10397</link><description>&lt;p&gt;
&#40065;&#26834;&#39134;&#34892;&#25511;&#21046;&#30340;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v10 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23558;&#35813;&#20272;&#35745;&#22120;&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21644;&#24212;&#23545;&#24178;&#25200;&#23545;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#36890;&#24120;&#38656;&#35201;&#23545;&#29305;&#23450;&#39134;&#34892;&#22330;&#26223;&#36827;&#34892;&#22823;&#37327;&#35843;&#25972;&#65292;&#25110;&#32773;&#32463;&#36807;&#24191;&#27867;&#30340;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#35757;&#32451;&#65292;&#25165;&#33021;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#30001;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;MHE&#20272;&#35745;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#23558;MHE&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#26080;&#32541;&#34701;&#21512;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#36882;&#24402;&#24418;&#24335;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#26799;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating and reacting to disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive ground-truth disturbance data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the key parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradients of the MHE estimates with respect to the weighting matrices, which enables a seamless embedding of the MHE as a learnable layer into neural networks for highly effective learning. Interestingly, we show that the gradients can be computed efficiently using a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the quadrotor trajectory tracking error without needing the ground-truth disturbance data. The effec
&lt;/p&gt;</description></item></channel></rss>