<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12942</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#12290;Siegelmann&#21644;Sontag(1992)&#26366;&#32463;&#23637;&#31034;&#20102;&#20855;&#26377;&#26377;&#29702;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#30340;RNNs&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#28982;&#32780;&#65292;LMs&#19981;&#20165;&#23450;&#20041;&#20102;&#23383;&#31526;&#20018;&#19978;&#30340;&#21152;&#26435;&#65292;&#36824;&#23450;&#20041;&#20102;(&#38750;&#21152;&#26435;)&#35821;&#35328;&#25104;&#21592;&#20851;&#31995;&#65292;&#23545;RNN LMs&#65288;RLMs&#65289;&#30340;&#35745;&#31639;&#33021;&#21147;&#20998;&#26512;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23558;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26377;&#29702;&#26435;&#37325;&#30340;RLM&#21644;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#26469;&#27169;&#25311;&#20219;&#20309;&#27010;&#29575;&#22270;&#28789;&#26426;(PTM)&#12290;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#65292;RLMs&#23454;&#26102;&#24037;&#20316;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#19968;&#20010;&#31526;&#21495;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#19978;&#36848;&#32467;&#26524;&#20316;&#20026;RLMs&#34920;&#36798;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#22312;&#23454;&#26102;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#30830;&#23450;&#24615;&#23454;&#26102;&#26377;&#29702;PTMs&#26469;&#25552;&#20379;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#23398;&#20064;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29983;&#25104;&#27969;&#32593;&#32476;&#35757;&#32451;&#20013;&#20855;&#26377;&#23454;&#38469;&#25928;&#29575;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12934</link><description>&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#20316;&#20026;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks as Entropy-Regularized RL. (arXiv:2310.12934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#23398;&#20064;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29983;&#25104;&#27969;&#32593;&#32476;&#35757;&#32451;&#20013;&#20855;&#26377;&#23454;&#38469;&#25928;&#29575;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#26159;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#20197;&#20415;&#26679;&#26412;&#20855;&#26377;&#19982;&#32473;&#23450;&#22870;&#21169;&#25104;&#27604;&#20363;&#30340;&#32452;&#21512;&#31163;&#25955;&#23545;&#35937;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#21160;&#20316;&#12290; GFlowNets&#21033;&#29992;&#38382;&#39064;&#30340;&#24207;&#21015;&#24615;&#36136;&#65292;&#19982;&#24378;&#21270;&#23398;&#20064;(RL)&#36827;&#34892;&#31867;&#27604;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;RL&#21644;GFlowNets&#20043;&#38388;&#30340;&#32852;&#31995;&#25193;&#23637;&#21040;&#20102;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#23398;&#20064;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#20219;&#21153;&#39640;&#25928;&#22320;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;RL&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26631;&#20934;&#30340;&#36719;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#27010;&#29575;&#24314;&#27169;&#20219;&#21153;&#30340;GFlowNet&#35757;&#32451;&#65292;&#26469;&#35828;&#26126;&#36825;&#31181;&#37325;&#23450;&#20041;&#30340;&#23454;&#38469;&#25928;&#29575;&#12290;&#19982;&#20808;&#21069;&#25253;&#36947;&#30340;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#19982;&#24050;&#26377;&#30340;GFlowNet&#35757;&#32451;&#26041;&#27861;&#31454;&#20105;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20010;&#35266;&#28857;&#20026;&#23558;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#34701;&#20837;&#23454;&#38469;&#38382;&#39064;&#25552;&#20379;&#20102;&#30452;&#25509;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating reinforcement learning principles into the real
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12823</link><description>&lt;p&gt;
AgentTuning: &#20026;LLMs&#23454;&#29616;&#36890;&#29992;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;LLMs&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#21830;&#19994;&#27169;&#22411;&#12290;&#36825;&#20123;&#20195;&#29702;&#20219;&#21153;&#23558;LLMs&#20316;&#20026;&#36127;&#36131;&#35268;&#21010;&#12289;&#35760;&#24518;&#21644;&#24037;&#20855;&#21033;&#29992;&#30340;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#38656;&#35201;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;LLMs&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25552;&#31034;&#26041;&#27861;&#26469;&#23436;&#25104;&#29305;&#23450;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#39640;LLMs&#33258;&#36523;&#30340;&#20195;&#29702;&#33021;&#21147;&#32780;&#19981;&#25439;&#23475;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#30340;LLM&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;AgentInstruct&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.12487</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#27880;&#24847;&#21147;&#25552;&#21319;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improved Operator Learning by Orthogonal Attention. (arXiv:2310.12487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#21463;&#21040;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#24050;&#25104;&#20026;&#30456;&#20851;&#30740;&#31350;&#30340;&#20027;&#27969;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#26426;&#21046;&#20013;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#27491;&#20132;&#27880;&#24847;&#21147;&#12290;&#27491;&#20132;&#21270;&#33258;&#28982;&#22320;&#23545;&#32467;&#26524;&#31070;&#32463;&#36816;&#31639;&#31526;&#26045;&#21152;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#25269;&#25239;&#36807;&#25311;&#21512;&#21644;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#25324;&#27491;&#24120;&#21644;&#38750;&#27491;&#24120;&#20960;&#20309;&#24418;&#29366;&#30340;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#31454;&#20105;&#23545;&#25163;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators, as an efficient surrogate model for learning the solutions of PDEs, have received extensive attention in the field of scientific machine learning. Among them, attention-based neural operators have become one of the mainstreams in related research. However, existing approaches overfit the limited training data due to the considerable number of parameters in the attention mechanism. To address this, we develop an orthogonal attention based on the eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions. The orthogonalization naturally poses a proper regularization effect on the resulting neural operator, which aids in resisting overfitting and boosting generalization. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#19982;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20851;&#31995;&#26469;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12403</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21327;&#20316;&#23567;&#25209;&#27425;
&lt;/p&gt;
&lt;p&gt;
Cooperative Minibatching in Graph Neural Networks. (arXiv:2310.12403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#19982;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20851;&#31995;&#26469;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26102;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20010;&#36807;&#31243;&#38750;&#24120;&#23494;&#38598;&#12290;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#26159;&#23558;&#23567;&#25209;&#37327;&#35757;&#32451;&#19982;&#22270;&#37319;&#26679;&#30456;&#32467;&#21512;&#12290;GNN&#20855;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21363;&#23567;&#25209;&#37327;&#20013;&#30340;&#39033;&#20855;&#26377;&#37325;&#21472;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#29420;&#31435;&#23567;&#25209;&#37327;&#26041;&#27861;&#23558;&#27599;&#20010;&#22788;&#29702;&#21333;&#20803;&#65288;PE&#65289;&#20998;&#37197;&#32473;&#33258;&#24049;&#30340;&#23567;&#25209;&#37327;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#37325;&#22797;&#35745;&#31639;&#21644;&#36328;PE&#30340;&#36755;&#20837;&#25968;&#25454;&#35775;&#38382;&#12290;&#36825;&#25918;&#22823;&#20102;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#36825;&#26159;&#38480;&#21046;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#23569;&#22810;PE&#29615;&#22659;&#20013;NEP&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#26159;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20985;&#20989;&#25968;&#36825;&#19968;&#29305;&#24615;&#65292;&#21487;&#20197;&#26126;&#26174;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21033;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#36827;&#34892;&#29305;&#24449;&#30740;&#31350;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12184</link><description>&lt;p&gt;
GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#30340;&#26550;&#26500;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Architectural Implications of GNN Aggregation Programming Abstractions. (arXiv:2310.12184v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#36827;&#34892;&#29305;&#24449;&#30740;&#31350;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38543;&#30528;&#23545;&#39640;&#25928;GNN&#35745;&#31639;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20026;&#20248;&#21270;GNN&#32858;&#21512;&#32780;&#35774;&#35745;&#30340;&#21508;&#31181;&#32534;&#31243;&#25277;&#35937;&#24212;&#36816;&#32780;&#29983;&#65292;&#20197;&#20419;&#36827;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#23545;&#29616;&#26377;&#25277;&#35937;&#27809;&#26377;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#22240;&#27492;&#23545;&#21738;&#31181;&#26041;&#27861;&#26356;&#22909;&#27809;&#26377;&#26126;&#30830;&#30340;&#20849;&#35782;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#32452;&#32455;&#21644;&#20256;&#25773;&#26041;&#27861;&#30340;&#32500;&#24230;&#23545;&#29616;&#26377;&#30340;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#26500;&#24314;&#36825;&#20123;&#25277;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#21644;&#35814;&#32454;&#30340;&#29305;&#24449;&#30740;&#31350;&#65292;&#20197;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#24182;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained significant popularity due to the powerful capability to extract useful representations from graph data. As the need for efficient GNN computation intensifies, a variety of programming abstractions designed for optimizing GNN Aggregation have emerged to facilitate acceleration. However, there is no comprehensive evaluation and analysis upon existing abstractions, thus no clear consensus on which approach is better. In this letter, we classify existing programming abstractions for GNN Aggregation by the dimension of data organization and propagation method. By constructing these abstractions on a state-of-the-art GNN library, we perform a thorough and detailed characterization study to compare their performance and efficiency, and provide several insights on future GNN acceleration based on our analysis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.11960</link><description>&lt;p&gt;
&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;&#38271;&#24207;&#21015;&#30340;&#20998;&#27835;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#33258;&#27880;&#24847;&#21147;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;Transformer&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#38271;&#24230;&#20026;n&#30340;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;&#36825;&#31181;&#20998;&#23618;&#26041;&#27861;&#23558;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20998;&#20026;O(log n)&#32423;&#30340;&#20998;&#36776;&#29575;&#65292;&#36739;&#36828;&#36317;&#31163;&#30340;&#32452;&#32676;&#36234;&#26469;&#36234;&#22823;&#65292;&#24182;&#23398;&#20064;&#35745;&#31639;&#32452;&#32676;&#25968;&#37327;&#30340;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#20197;&#39640;&#25928;&#20998;&#23618;&#30340;&#26041;&#24335;&#22312;&#36739;&#20302;&#30340;&#20998;&#36776;&#29575;&#20013;&#32771;&#34385;&#36828;&#31163;&#24444;&#27492;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#24635;&#20307;&#22797;&#26434;&#24230;&#20026;O(n)&#25110;O(n log n)&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.11594</link><description>&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#39537;&#21160;&#29615;&#22659;&#20013;&#65292;&#32500;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#37322;&#25918;&#25968;&#25454;&#28508;&#21147;&#20043;&#38388;&#24494;&#22937;&#30340;&#24179;&#34913;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20197;&#38544;&#31169;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#23454;&#29616;&#20102;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#36825;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#24102;&#26469;&#20102;&#23433;&#20840;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24694;&#24847;&#23454;&#20307;&#27880;&#20837;&#25439;&#22351;&#25968;&#25454;&#30340;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#21021;&#21463;&#21040;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#12290;ARU&#34987;&#19968;&#37096;&#20998;&#23545;&#25163;&#20351;&#29992;&#65292;&#20197;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;ARU&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#29616;&#26377;&#30340;&#40065;&#26834;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#23545;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.11122</link><description>&lt;p&gt;
&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#29616;&#20195;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#36873;&#25321;&#28041;&#21450;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#20998;&#24067;&#30340;&#35268;&#33539;&#12289;&#21518;&#39564;&#36924;&#36817;&#22120;&#21644;&#25968;&#25454;&#12290;&#27599;&#20010;&#36873;&#25321;&#37117;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#22522;&#20110;&#27169;&#22411;&#30340;&#25512;&#26029;&#21644;&#21518;&#32493;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;&#25935;&#24863;&#24615;&#20998;&#26512;&#25972;&#21512;&#21040;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#65288;ABI&#65292;&#21363;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#25512;&#26029;&#65289;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#26435;&#37325;&#20849;&#20139;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32534;&#30721;&#26367;&#20195;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#25512;&#26029;&#26469;&#35780;&#20272;&#23545;&#21508;&#31181;&#25968;&#25454;&#25200;&#21160;&#25110;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#36125;&#21494;&#26031;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#21644;&#20135;&#21697;-&#38190;&#23384;&#20648;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#21442;&#25968;&#30456;&#31561;&#30340;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.10837</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Approximating Two-Layer Feedforward Networks for Efficient Transformers. (arXiv:2310.10837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#21644;&#20135;&#21697;-&#38190;&#23384;&#20648;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#21442;&#25968;&#30456;&#31561;&#30340;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;(NNs)&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65311;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MoEs)&#26500;&#24314;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;MoEs&#30340;&#20960;&#20010;&#26032;&#39062;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#21508;&#31181;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#20197;&#36817;&#20284;&#20004;&#23618;NNs(&#20363;&#22914;Transformer&#30340;&#21069;&#39304;&#22359;)&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20135;&#21697;-&#38190;&#23384;&#20648;(PKMs)&#12290;&#20511;&#21161;&#36825;&#20010;&#26694;&#26550;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;MoEs&#21644;PKMs&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#22312;&#35745;&#31639;&#30456;&#31561;&#26465;&#20214;&#19979;&#27604;&#36739;MoEs&#19982;&#23494;&#38598;&#22522;&#20934;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26465;&#20214;&#26159;&#21442;&#25968;&#30456;&#31561;&#65292;&#36825;&#23545;&#20110;&#27491;&#30830;&#35780;&#20272;LMs&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;MoEs&#22312;WikiText-103&#21644;enwiki8&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#19981;&#21516;&#35268;&#27169;&#19978;&#19982;&#23494;&#38598;&#30340;Transformer-XL&#30456;&#31454;&#20105;&#65292;&#21516;&#26102;&#36164;&#28304;&#25928;&#29575;&#26356;&#39640;&#12290;&#36825;&#35777;&#26126;MoEs&#19981;&#20165;&#36866;&#29992;&#20110;&#36229;&#22823;&#22411;LMs&#65292;&#20063;&#36866;&#29992;&#20110;&#20219;&#20309;&#35268;&#27169;&#30340;&#36164;&#28304;-
&lt;/p&gt;
&lt;p&gt;
How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>AdaLomo&#26159;&#19968;&#31181;&#20302;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.10195</link><description>&lt;p&gt;
AdaLomo: &#20302;&#20869;&#23384;&#20248;&#21270;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
AdaLomo: Low-memory Optimization with Adaptive Learning Rate. (arXiv:2310.10195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10195
&lt;/p&gt;
&lt;p&gt;
AdaLomo&#26159;&#19968;&#31181;&#20302;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#35268;&#27169;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#20869;&#23384;&#65292;&#20174;&#32780;&#35774;&#32622;&#20102;&#24456;&#39640;&#30340;&#38376;&#27099;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#30340;&#20302;&#20869;&#23384;&#20248;&#21270;&#65288;LOMO&#65289;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#20854;&#20248;&#21270;&#25216;&#26415;&#31867;&#20284;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#23545;&#36229;&#21442;&#25968;&#25935;&#24863;&#24182;&#23637;&#29616;&#20986;&#27425;&#20248;&#30340;&#25910;&#25947;&#24615;&#65292;&#26080;&#27861;&#19982;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;AdamW&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36890;&#36807;&#23545;Adam&#20248;&#21270;&#22120;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#21160;&#37327;&#26469;&#35828;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#23545;&#20110;&#24357;&#21512;&#24046;&#36317;&#26356;&#20026;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;&#20302;&#20869;&#23384;&#20248;&#21270;&#65288;AdaLomo&#65289;&#65292;&#20026;&#27599;&#20010;&#21442;&#25968;&#25552;&#20379;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#20026;&#20102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#29366;&#24577;&#20013;&#37319;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26469;&#20272;&#35745;&#20108;&#38454;&#30697;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20998;&#32452;&#26356;&#26032;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update norma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#23618;&#25351;&#25968;&#26063;&#33021;&#37327;&#27169;&#22411;(HEE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#26679;&#26799;&#24230;&#20174;&#32780;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#21644;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#36127;&#30456;&#20301;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#23398;&#20064;&#36807;&#31243;&#23616;&#37096;&#21270;&#19988;&#23481;&#26131;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2310.08431</link><description>&lt;p&gt;
&#20998;&#23618;&#25351;&#25968;&#26063;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Neural Sampling in Hierarchical Exponential-family Energy-based Models. (arXiv:2310.08431v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#23618;&#25351;&#25968;&#26063;&#33021;&#37327;&#27169;&#22411;(HEE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#26679;&#26799;&#24230;&#20174;&#32780;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#21644;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#36127;&#30456;&#20301;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#23398;&#20064;&#36807;&#31243;&#23616;&#37096;&#21270;&#19988;&#23481;&#26131;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#33041;&#29702;&#35770;&#35748;&#20026;&#22823;&#33041;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#29702;&#35299;&#22806;&#37096;&#19990;&#30028;&#12290;&#22522;&#20110;&#37319;&#26679;&#30340;&#35266;&#28857;&#35748;&#20026;&#65292;&#22823;&#33041;&#36890;&#36807;&#38543;&#26426;&#31070;&#32463;&#21709;&#24212;&#26679;&#26412;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#22823;&#33041;&#36824;&#19981;&#26029;&#26356;&#26032;&#20854;&#29983;&#25104;&#27169;&#22411;&#20197;&#36924;&#36817;&#22806;&#37096;&#19990;&#30028;&#30340;&#30495;&#23454;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#25351;&#25968;&#26063;&#33021;&#37327;&#27169;&#22411;(HEE)&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#25512;&#26029;&#21644;&#23398;&#20064;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#22312;HEE&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23558;&#20998;&#21306;&#20989;&#25968;&#20998;&#35299;&#20026;&#22810;&#20010;&#23618;&#27425;&#65292;&#24182;&#21033;&#29992;&#19968;&#32452;&#20855;&#26377;&#36739;&#30701;&#26102;&#38388;&#24120;&#25968;&#30340;&#31070;&#32463;&#20803;&#26469;&#37319;&#26679;&#20998;&#35299;&#30340;&#24402;&#19968;&#21270;&#39033;&#30340;&#26799;&#24230;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#24182;&#36827;&#34892;&#25512;&#26029;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20256;&#32479;&#33021;&#37327;&#27169;&#22411;(EBMs)&#20013;&#36935;&#21040;&#30340;&#36127;&#30456;&#20301;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#36807;&#31243;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#37117;&#26159;&#23616;&#37096;&#21270;&#30340;&#65292;&#27169;&#22411;&#25910;&#25947;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To
&lt;/p&gt;</description></item><item><title>GRASP&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35782;&#21035;&#23376;&#22270;&#26469;&#21152;&#36895;&#26368;&#30701;&#36335;&#24452;&#25915;&#20987;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#36816;&#34892;&#36895;&#24230;&#24555;&#36798;&#21040;10&#20493;&#32780;&#20173;&#20445;&#25345;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.07980</link><description>&lt;p&gt;
GRASP&#65306;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#21152;&#36895;&#26368;&#30701;&#36335;&#24452;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
GRASP: Accelerating Shortest Path Attacks via Graph Attention. (arXiv:2310.07980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07980
&lt;/p&gt;
&lt;p&gt;
GRASP&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35782;&#21035;&#23376;&#22270;&#26469;&#21152;&#36895;&#26368;&#30701;&#36335;&#24452;&#25915;&#20987;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#36816;&#34892;&#36895;&#24230;&#24555;&#36798;&#21040;10&#20493;&#32780;&#20173;&#20445;&#25345;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#22312;&#36741;&#21161;&#21644;&#21152;&#36895;&#20256;&#32479;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#20197;&#30452;&#25509;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#20026;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#24448;&#24448;&#20250;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#35299;&#20915;&#36136;&#37327;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#24615;&#33021;&#20445;&#35777;&#30340;&#21516;&#26102;&#21152;&#36895;&#29616;&#26377;&#27714;&#35299;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#22791;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;APX&#22256;&#38590;&#38382;&#39064;&#65292;&#20854;&#20013;&#23545;&#25163;&#36890;&#36807;&#21024;&#38500;&#26368;&#23567;&#25968;&#37327;&#30340;&#36793;&#26469;&#25915;&#20987;&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GRASP&#31639;&#27861;&#65306;&#22270;&#27880;&#24847;&#21147;&#21152;&#36895;&#26368;&#30701;&#36335;&#24452;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;ML&#36741;&#21161;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#39640;10&#20493;&#30340;&#36816;&#34892;&#26102;&#38388;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;GRASP&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35782;&#21035;&#21253;&#21547;&#32452;&#21512;&#35299;&#20915;&#26041;&#26696;&#30340;&#26356;&#23567;&#23376;&#22270;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23567;&#20102;&#36755;&#20837;&#38382;&#39064;&#30340;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31934;&#30830;&#34920;&#31034;&#36755;&#20837;&#22270;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning (ML) have shown promise in aiding and accelerating classical combinatorial optimization algorithms. ML-based speed ups that aim to learn in an end to end manner (i.e., directly output the solution) tend to trade off run time with solution quality. Therefore, solutions that are able to accelerate existing solvers while maintaining their performance guarantees, are of great interest. We consider an APX-hard problem, where an adversary aims to attack shortest paths in a graph by removing the minimum number of edges. We propose the GRASP algorithm: Graph Attention Accelerated Shortest Path Attack, an ML aided optimization algorithm that achieves run times up to 10x faster, while maintaining the quality of solution generated. GRASP uses a graph attention network to identify a smaller subgraph containing the combinatorial solution, thus effectively reducing the input problem size. Additionally, we demonstrate how careful representation of the input graph, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07587</link><description>&lt;p&gt;
Fed-GraB&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#30340;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#21644;&#38271;&#23614;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20013;&#26159;&#24120;&#24577;&#32780;&#38750;&#20363;&#22806;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#19968;&#20010;&#26412;&#22320;&#24322;&#26500;&#25968;&#25454;&#38598;&#65307;&#22914;&#26524;&#21487;&#20197;&#20840;&#23616;&#32858;&#21512;&#25968;&#25454;&#38598;&#65292;&#21017;&#23427;&#20204;&#20849;&#21516;&#23637;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#20248;&#21270;&#21644;/&#25110;&#38598;&#20013;&#24335;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#24212;&#29992;&#65292;&#22240;&#20026;&#23384;&#22312;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;a&#65289;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;b&#65289;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#20197;&#24212;&#23545;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;$\texttt{Fed-GraB}$&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#65288;SGB&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20197;&#38381;&#29615;&#26041;&#24335;&#26681;&#25454;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#30340;&#21453;&#39304;&#23545;&#23458;&#25143;&#31471;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#65292;&#35780;&#20272;&#26041;&#27861;&#20026;&#30452;&#25509;&#20808;&#39564;&#20998;&#26512;&#22120;&#65288;DPA&#65289;&#27169;&#22359;&#12290;&#20351;&#29992;$\texttt{Fed-GraB}$&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07338</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Foundation Models for Learning on Tabular Data. (arXiv:2310.07338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#25903;&#25745;&#30528;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#24320;&#21457;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#26377;&#25928;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#30446;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#30452;&#25509;&#25351;&#20196;&#36319;&#38543;&#26032;&#20219;&#21153;&#30340;&#25903;&#25345;&#65292;&#35201;&#20040;&#24573;&#35270;&#20174;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#22522;&#30784;&#30693;&#35782;&#21644;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;TabFMs&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32463;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#30446;&#26631;&#22312;&#22823;&#33539;&#22260;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#26041;&#27861;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;TabFM&#30340;&#26377;&#25928;&#24615;&#65306;&#23427;&#19981;&#20165;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#25512;&#29702;&#31561;&#36981;&#24490;&#25351;&#20196;&#30340;&#20219;&#21153;&#20013;&#26126;&#26174;&#20986;&#33394;&#65292;
&lt;/p&gt;
&lt;p&gt;
Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#65292;&#21033;&#29992;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.06918</link><description>&lt;p&gt;
&#29992;&#32858;&#28966;-&#20449;&#24687;&#29109;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#65292;&#21033;&#29992;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SimCSE&#30340;&#26368;&#26032;&#25104;&#21151;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#21477;&#23376;&#34920;&#31034;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;SimCSE&#30340;&#21407;&#22987;&#34920;&#36798;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20013;&#30828;&#36127;&#26679;&#26412;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;SimCSE&#19982;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#22312;&#23545;&#27604;&#30446;&#26631;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#35843;&#33410;&#39033;&#65292;&#38477;&#20302;&#19982;&#26131;&#36127;&#26679;&#26412;&#30456;&#20851;&#30340;&#25439;&#22833;&#65292;&#24182;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#20110;&#22256;&#38590;&#36127;&#26679;&#26412;&#12290;&#22312;&#21508;&#31181;STS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#12289;&#34920;&#31034;&#23545;&#40784;&#24615;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#25913;&#36827;&#20102;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#35752;&#35770;&#21644;&#20915;&#31574;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#35299;&#37322;&#20869;&#23481;&#31649;&#29702;&#20915;&#31574;&#26041;&#38754;&#22686;&#21152;&#20102;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05779</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#24212;&#35813;&#21024;&#38500;&#36825;&#31687;&#25991;&#31456;&#65311;&#22810;&#35821;&#35328;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#35752;&#35770;&#20013;&#30340;&#36879;&#26126;&#31435;&#22330;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions. (arXiv:2310.05779v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#35752;&#35770;&#21644;&#20915;&#31574;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#35299;&#37322;&#20869;&#23481;&#31649;&#29702;&#20915;&#31574;&#26041;&#38754;&#22686;&#21152;&#20102;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#31649;&#29702;&#36890;&#24120;&#26159;&#38750;&#36879;&#26126;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#65292;&#36825;&#31181;&#35752;&#35770;&#26159;&#20844;&#24320;&#36827;&#34892;&#30340;&#65292;&#32534;&#36753;&#34987;&#40723;&#21169;&#20351;&#29992;&#20869;&#23481;&#31649;&#29702;&#25919;&#31574;&#26469;&#35299;&#37322;&#20182;&#20204;&#30340;&#20915;&#31574;&#12290;&#30446;&#21069;&#65292;&#21482;&#26377;&#23569;&#25968;&#35780;&#35770;&#26126;&#30830;&#25552;&#21040;&#36825;&#20123;&#25919;&#31574;-&#33521;&#25991;&#35780;&#35770;20%&#65292;&#20294;&#24503;&#25991;&#21644;&#22303;&#32819;&#20854;&#35780;&#35770;&#21482;&#26377;2%&#12290;&#20026;&#20102;&#24110;&#21161;&#29702;&#35299;&#20869;&#23481;&#31649;&#29702;&#36807;&#31243;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#35752;&#35770;&#20197;&#21450;&#20182;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#32534;&#36753;&#30340;&#31435;&#22330;&#65288;&#20445;&#30041;&#12289;&#21024;&#38500;&#12289;&#21512;&#24182;&#12289;&#35780;&#35770;&#65289;&#65292;&#20197;&#21450;&#27599;&#20010;&#32534;&#36753;&#20915;&#31574;&#25152;&#38472;&#36848;&#30340;&#21407;&#22240;&#21644;&#20869;&#23481;&#31649;&#29702;&#25919;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31435;&#22330;&#21644;&#30456;&#24212;&#30340;&#21407;&#22240;&#65288;&#25919;&#31574;&#65289;&#21487;&#20197;&#34987;&#39640;&#24230;&#20934;&#30830;&#22320;&#32852;&#21512;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20915;&#31574;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#32852;&#21512;&#39044;&#27979;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The moderation of content on online platforms is usually non-transparent. On Wikipedia, however, this discussion is carried out publicly and the editors are encouraged to use the content moderation policies as explanations for making moderation decisions. Currently, only a few comments explicitly mention those policies -- 20% of the English ones, but as few as 2% of the German and Turkish comments. To aid in this process of understanding how content is moderated, we construct a novel multilingual dataset of Wikipedia editor discussions along with their reasoning in three languages. The dataset contains the stances of the editors (keep, delete, merge, comment), along with the stated reason, and a content moderation policy, for each edit decision. We demonstrate that stance and corresponding reason (policy) can be predicted jointly with a high degree of accuracy, adding transparency to the decision-making process. We release both our joint prediction models and the multilingual content m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.05703</link><description>&lt;p&gt;
Siamese&#32534;&#30721;&#22120;&#30340;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Attribution Method for Siamese Encoders. (arXiv:2310.05703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21477;&#23376;&#36716;&#25442;&#22120;&#31561;Siamese&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#20851;&#27880;&#30340;&#36755;&#20837;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#20010;&#38556;&#30861;&#26159;&#23427;&#20204;&#30340;&#39044;&#27979;&#19981;&#33021;&#24402;&#22240;&#20110;&#20010;&#21035;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#20204;&#27604;&#36739;&#30340;&#26159;&#20004;&#20010;&#36755;&#20837;&#32780;&#19981;&#26159;&#19968;&#20010;&#36755;&#20837;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#29305;&#24449;&#23545;&#24402;&#22240;&#30340;&#24418;&#24335;&#65292;&#24182;&#21487;&#23558;&#20854;&#31616;&#21270;&#20026;&#21477;&#23376;&#36716;&#25442;&#22120;&#30340;&#20196;&#29260;-&#20196;&#29260;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24341;&#20837;&#38598;&#25104;&#38597;&#21487;&#27604;&#30697;&#38453;&#65292;&#24182;&#32487;&#25215;&#20102;&#38598;&#25104;&#26799;&#24230;&#30340;&#20248;&#21183;&#24418;&#24335;&#29305;&#24615;&#65306;&#23427;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23436;&#25972;&#35745;&#31639;&#22270;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#23454;&#38469;&#39044;&#27979;&#32467;&#26524;&#12290;&#19968;&#39033;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21477;&#23376;&#36716;&#25442;&#22120;&#20013;&#65292;&#24456;&#23569;&#30340;&#20196;&#29260;&#23545;&#24448;&#24448;&#21487;&#20197;&#35299;&#37322;&#22823;&#37096;&#20998;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23427;&#38656;&#35201;&#20851;&#27880;&#22823;&#22810;&#25968;&#30340;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SAMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38544;&#24335;&#24494;&#20998;&#31639;&#27861;&#21644;&#31995;&#32479;&#36827;&#23637;&#65292;&#20351;&#24471;&#20803;&#23398;&#20064;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;SAMA&#22312;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#31243;&#24207;&#20013;&#28789;&#27963;&#25903;&#25345;&#21508;&#31181;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65292;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#26174;&#24335;&#35745;&#31639;&#20108;&#38454;&#26799;&#24230;&#20449;&#24687;&#21644;&#21033;&#29992;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAMA&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#20803;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#21644;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.05674</link><description>&lt;p&gt;
&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20803;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Scalable Meta Learning Practical. (arXiv:2310.05674v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SAMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38544;&#24335;&#24494;&#20998;&#31639;&#27861;&#21644;&#31995;&#32479;&#36827;&#23637;&#65292;&#20351;&#24471;&#20803;&#23398;&#20064;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;SAMA&#22312;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#31243;&#24207;&#20013;&#28789;&#27963;&#25903;&#25345;&#21508;&#31181;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65292;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#26174;&#24335;&#35745;&#31639;&#20108;&#38454;&#26799;&#24230;&#20449;&#24687;&#21644;&#21033;&#29992;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAMA&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#20803;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#21644;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20803;&#23398;&#20064;&#65288;&#21363;&#23398;&#20250;&#23398;&#20064;&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#23398;&#20064;&#22810;&#26679;&#30340;&#24402;&#32435;&#20559;&#32622;&#26041;&#38754;&#38750;&#24120;&#28789;&#27963;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;/&#20869;&#23384;&#24320;&#38144;&#24040;&#22823;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#32570;&#20047;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#25903;&#25345;&#65292;&#23427;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#35748;&#20026;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#24341;&#20837;SAMA&#65292;&#23558;&#38544;&#24335;&#24494;&#20998;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#20351;&#21487;&#25193;&#23637;&#30340;&#20803;&#23398;&#20064;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SAMA&#26088;&#22312;&#28789;&#27963;&#25903;&#25345;&#20803;&#23398;&#20064;&#31243;&#24207;&#30340;&#22522;&#26412;&#32423;&#21035;&#20013;&#36866;&#24212;&#24615;&#20248;&#21270;&#22120;&#30340;&#24191;&#27867;&#33539;&#22260;&#65292;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#26174;&#24335;&#35745;&#31639;&#20108;&#38454;&#26799;&#24230;&#20449;&#24687;&#21644;&#21033;&#29992;&#20026;&#19968;&#38454;&#26799;&#24230;&#23454;&#29616;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#26469;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#20803;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;SAMA&#22312;&#21333;&#20010;/&#22810;&#20010;GPU&#19978;&#20998;&#21035;&#23637;&#31034;&#20102;&#39640;&#36798;1.7 / 4.8&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#21152;&#21644;2.0 / 3.8&#20493;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#22359;&#30340;&#37327;&#21270;&#22312;&#20302;&#20110;8&#20301;LLM&#25512;&#29702;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#30340;6&#20301;LLMs&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;8&#20301;&#37327;&#21270;&#65292;&#22312;&#31639;&#26415;&#23494;&#24230;&#21644;&#20869;&#23384;&#23494;&#24230;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#26657;&#20934;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.05079</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#22359;&#30340;&#37327;&#21270;: &#23545;&#20110;&#20302;&#20110;8&#20301;LLM&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?. (arXiv:2310.05079v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#22359;&#30340;&#37327;&#21270;&#22312;&#20302;&#20110;8&#20301;LLM&#25512;&#29702;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#30340;6&#20301;LLMs&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;8&#20301;&#37327;&#21270;&#65292;&#22312;&#31639;&#26415;&#23494;&#24230;&#21644;&#20869;&#23384;&#23494;&#24230;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#26657;&#20934;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#20026;&#20102;&#38477;&#20302;&#36825;&#20123;&#25104;&#26412;&#65292;&#37327;&#21270;&#25104;&#20026;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#37327;&#21270;&#20027;&#35201;&#38598;&#20013;&#22312;8&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;LLM&#23618;&#30340;&#32479;&#35745;&#21644;&#23398;&#20064;&#29305;&#24615;&#65292;&#24182;&#23558;LLM&#37327;&#21270;&#30340;&#29942;&#39048;&#24402;&#22240;&#20110;&#25968;&#20540;&#32553;&#25918;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;LLMs&#25913;&#36827;&#20102;&#22359;&#37327;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31867;&#22312;&#25171;&#21253;&#30340;&#25968;&#23383;&#20043;&#38388;&#20849;&#20139;&#32553;&#25918;&#22240;&#23376;&#30340;&#26041;&#27861;&#12290;&#22359;&#37327;&#21270;&#20165;&#20174;&#31639;&#26415;&#35282;&#24230;&#26377;&#25928;&#22320;&#20943;&#23567;&#20102;&#25968;&#20540;&#32553;&#25918;&#20559;&#31227;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#35745;&#31639;&#36335;&#24452;&#20013;&#36827;&#34892;&#20854;&#20182;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#30340;6&#20301;LLMs&#30340;&#31639;&#26415;&#23494;&#24230;&#27604;float32&#22522;&#32447;&#39640;&#20986;19&#20493;&#65292;&#20869;&#23384;&#23494;&#24230;&#39640;&#20986;5&#20493;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;8&#20301;&#37327;&#21270;&#30340;&#31639;&#26415;&#23494;&#24230;&#39640;&#20986;2.5&#20493;&#65292;&#20869;&#23384;&#23494;&#24230;&#39640;&#20986;1.2&#20493;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#25968;&#25454;&#26657;&#20934;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has merged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a $19\times$ higher arithmetic density and $5\times$ memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by $2.5\times$ in arithmetic density and $1.2\times$ in memory density, without requiring any data calibration or re-training. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#24605;&#24615;&#22320;&#23398;&#20064;&#21644;&#36981;&#24490;&#25351;&#21335;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20219;&#21153;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05066</link><description>&lt;p&gt;
&#25351;&#21335;&#23398;&#20064;&#29992;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Guideline Learning for In-context Information Extraction. (arXiv:2310.05066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#24605;&#24615;&#22320;&#23398;&#20064;&#21644;&#36981;&#24490;&#25351;&#21335;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20219;&#21153;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#36890;&#36807;&#20165;&#20381;&#36182;&#20219;&#21153;&#25351;&#20196;&#21644;&#23569;&#37327;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#26469;&#25191;&#34892;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20248;&#21270;&#20219;&#20309;&#21442;&#25968;&#12290;&#36825;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#12290;&#26368;&#36817;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#25277;&#21462;(IE)&#22312;&#30740;&#31350;&#30028;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;IE&#30340;&#24615;&#33021;&#36890;&#24120;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#19987;&#23478;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;: &#20219;&#21153;&#25551;&#36848;&#19981;&#26126;&#30830;&#12290;&#26377;&#38480;&#38271;&#24230;&#30340;&#19978;&#19979;&#25991;&#38590;&#20197;&#20805;&#20998;&#34920;&#36798;&#22797;&#26434;&#30340;IE&#20219;&#21153;&#25351;&#20196;&#21644;&#21508;&#31181;&#36793;&#30028;&#24773;&#20917;&#65292;&#23548;&#33268;&#20219;&#21153;&#29702;&#35299;&#19982;&#20154;&#31867;&#20986;&#29616;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;IE&#30340;&#25351;&#21335;&#23398;&#20064;(GL)&#26694;&#26550;&#65292;&#20854;&#21453;&#24605;&#24615;&#22320;&#23398;&#20064;&#24182;&#36981;&#24490;&#25351;&#21335;&#12290;&#22312;&#23398;&#20064;&#38454;&#27573;&#65292;GL&#22522;&#20110;&#23569;&#37327;&#38169;&#35823;&#26696;&#20363;&#33258;&#21160;&#21512;&#25104;&#19968;&#32452;&#25351;&#21335;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;GL&#26816;&#32034;&#26377;&#29992;&#30340;&#25351;&#21335;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;ICL&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;s
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04445</link><description>&lt;p&gt;
LoFT: &#29992;&#20110;&#25913;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#30340;&#26412;&#22320;&#20195;&#29702;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#29305;&#21046;&#30340;&#25915;&#20987;&#21518;&#32512;&#21644;&#26377;&#23475;&#26597;&#35810;&#26469;&#35268;&#36991;&#65292;&#20197;&#24341;&#21457;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#23545;&#26410;&#30693;&#29305;&#24449;&#30340;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#65292;&#21487;&#20197;&#20351;&#29992;&#20844;&#20849;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#26500;&#24314;&#25915;&#20987;&#65292;&#24182;&#23558;&#25104;&#21151;&#30340;&#25915;&#20987;&#20174;&#20844;&#20849;&#20195;&#29702;&#20256;&#36882;&#21040;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#12290;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21462;&#20915;&#20110;&#20195;&#29702;&#27169;&#22411;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#36924;&#36817;&#31169;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#23545;&#20110;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#26469;&#35828;&#65292;&#21482;&#35201;&#20195;&#29702;&#33021;&#22815;&#22312;&#26377;&#23475;&#26597;&#35810;&#30340;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#20869;&#36924;&#36817;&#30446;&#26631;&#27169;&#22411;&#21363;&#21487;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26412;&#22320;&#24494;&#35843;&#65288;LoFT&#65289;&#8221;&#65292;&#21363;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#20943;&#23567;&#20195;&#29702;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19977;&#31181;&#20419;&#20351;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#21464;&#24471;&#26131;&#21463;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01875</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#28418;&#31227;&#35843;&#25972;&#23454;&#29616;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Backdoor Purification through Feature Shift Tuning. (arXiv:2310.01875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#19968;&#23567;&#32452;&#35757;&#32451;&#26679;&#26412;&#26469;&#24694;&#24847;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#23041;&#32961;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#22797;&#26434;&#20462;&#25913;&#65292;&#35201;&#20040;&#20005;&#37325;&#20381;&#36182;&#29305;&#23450;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24494;&#35843;&#24320;&#22987;&#65292;&#36890;&#36807;&#23545;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#30340;&#20840;&#38754;&#35780;&#20272;&#26469;&#25506;&#32034;&#26368;&#24120;&#35265;&#21644;&#26131;&#20110;&#37096;&#32626;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#36807;&#21021;&#27493;&#23454;&#39564;&#35266;&#23519;&#21457;&#29616;&#65292;&#19982;&#39640;&#27745;&#26579;&#29575;&#30340;&#26377;&#24076;&#26395;&#30340;&#38450;&#24481;&#32467;&#26524;&#30456;&#27604;&#65292;&#26222;&#36890;&#30340;&#35843;&#25972;&#26041;&#27861;&#22312;&#20302;&#27745;&#26579;&#29575;&#22330;&#26223;&#19979;&#23436;&#20840;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#30772;&#22351;&#20102;&#22522;&#20110;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01210</link><description>&lt;p&gt;
&#23454;&#29616;&#31283;&#20581;&#30340;&#24515;&#33039;&#20998;&#21106;&#65306;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Cardiac Segmentation using Graph Convolutional Networks. (arXiv:2310.01210v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#33258;&#21160;&#21270;&#30340;&#24515;&#33039;&#20998;&#21106;&#21487;&#20197;&#24555;&#36895;&#12289;&#21487;&#37325;&#22797;&#22320;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#26816;&#26597;&#20013;&#25552;&#21462;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;U-Net&#32467;&#26500;&#26159;&#30446;&#21069;&#21307;&#23398;&#20998;&#21106;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#26102;&#20998;&#21106;&#24515;&#33039;&#32467;&#26500;&#65292;&#24182;&#19988;&#24179;&#22343;&#35823;&#24046;&#21487;&#19982;&#35266;&#27979;&#32773;&#38388;&#21464;&#24322;&#24615;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#35813;&#26550;&#26500;&#20173;&#28982;&#20250;&#29983;&#25104;&#35768;&#22810;&#35299;&#31163;&#24322;&#24120;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#39044;&#27979;&#20986;&#24863;&#20852;&#36259;&#32467;&#26500;&#30340;&#36718;&#24275;&#28857;&#65292;&#32780;&#19981;&#26159;&#23545;&#27599;&#20010;&#20687;&#32032;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#33039;&#35299;&#21078;&#23398;&#30340;&#22270;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#36825;&#28040;&#38500;&#20102;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;CAMUS&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#32467;&#26500;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#22312;&#20020;&#24202;HUNT4&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully automatic cardiac segmentation can be a fast and reproducible method to extract clinical measurements from an echocardiography examination. The U-Net architecture is the current state-of-the-art deep learning architecture for medical segmentation and can segment cardiac structures in real-time with average errors comparable to inter-observer variability. However, this architecture still generates large outliers that are often anatomically incorrect. This work uses the concept of graph convolutional neural networks that predict the contour points of the structures of interest instead of labeling each pixel. We propose a graph architecture that uses two convolutional rings based on cardiac anatomy and show that this eliminates anatomical incorrect multi-structure segmentations on the publicly available CAMUS dataset. Additionally, this work contributes with an ablation study on the graph convolutional architecture and an evaluation of clinical measurements on the clinical HUNT4 dat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#26469;&#36991;&#20813;&#20551;&#38452;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.00797</link><description>&lt;p&gt;
&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Familiar Features for Deep Anomaly Detection. (arXiv:2310.00797v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#26469;&#36991;&#20813;&#20551;&#38452;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#28041;&#21450;&#35782;&#21035;&#19981;&#31526;&#21512;&#24050;&#23398;&#20064;&#30340;&#27491;&#24120;&#27169;&#22411;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#20043;&#21069;&#30340;&#28145;&#24230;AD&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#29087;&#24713;&#24615;&#20551;&#35774;&#65292;&#22312;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#29087;&#24713;&#29305;&#24449;&#20316;&#20026;&#21442;&#32771;&#12290;&#34429;&#28982;&#36825;&#31181;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#22312;&#24322;&#24120;&#21253;&#21547;&#26410;&#34987;&#39044;&#35757;&#32451;&#32534;&#30721;&#24456;&#22909;&#25429;&#25417;&#21040;&#30340;&#20840;&#26032;&#29305;&#24449;&#26102;&#65292;&#23427;&#20250;&#23548;&#33268;&#25345;&#32493;&#30340;&#20551;&#38452;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;AD&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#20316;&#20026;&#26410;&#35299;&#37322;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#36890;&#36807;&#22312;&#28151;&#21512;&#26041;&#27861;&#20013;&#32467;&#21512;&#30456;&#20284;&#24615;&#21644;&#26032;&#39062;&#24615;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#24322;&#24120;&#31867;&#22411;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#32771;&#34385;&#26032;&#39062;&#29305;&#24449;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#30340;&#20551;&#38452;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection (AD) is a critical task that involves identifying observations that do not conform to a learned model of normality. Prior work in deep AD is predominantly based on a familiarity hypothesis, where familiar features serve as the reference in a pre-trained embedding space. While this strategy has proven highly successful, it turns out that it causes consistent false negatives when anomalies consist of truly novel features that are not well captured by the pre-trained encoding. We propose a novel approach to AD using explainability to capture novel features as unexplained observations in the input space. We achieve strong performance across a wide range of anomaly benchmarks by combining similarity and novelty in a hybrid approach. Our approach establishes a new state-of-the-art across multiple benchmarks, handling diverse anomaly types while eliminating the need for expensive background models and dense matching. In particular, we show that by taking account of novel fea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#38544;&#34255;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#23545;AR&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23436;&#32654;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.00290</link><description>&lt;p&gt;
&#23436;&#32654;&#39044;&#27979;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data. (arXiv:2310.00290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#38544;&#34255;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#23545;AR&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23436;&#32654;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#22791;&#35745;&#31639;&#65288;RC&#65289;&#26159;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#27627;&#26080;&#30097;&#38382;&#65292;RC&#23558;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26410;&#26469;&#39044;&#27979;&#27169;&#22411;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#12289;&#39640;&#36895;&#24230;&#21644;&#39640;&#35745;&#31639;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;RC&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#30452;&#21040;&#26368;&#36817;&#25165;&#24320;&#22987;&#12290;Bollt&#65288;2021&#65289;&#38416;&#26126;&#20102;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;RC&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#32467;&#26500;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;Wold&#20998;&#35299;&#23450;&#29702;&#26159;&#29702;&#35299;&#36825;&#20123;&#32467;&#26500;&#30340;&#37324;&#31243;&#30865;&#12290;&#22312;&#38125;&#35760;&#36825;&#19968;&#33879;&#21517;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;RC&#31070;&#32463;&#32593;&#32476;&#20013;&#36755;&#20837;&#21644;&#24490;&#29615;&#26435;&#37325;&#30697;&#38453;&#30340;&#38544;&#34255;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#32467;&#26500;&#23545;&#20110;AR&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23454;&#29616;&#20102;&#23436;&#32654;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing (RC) is a type of recursive neural network (RNN), and there can be no doubt that the RC will be more and more widely used for building future prediction models for time-series data, with low training cost, high speed and high computational power. However, research into the mathematical structure of RC neural networks has only recently begun. Bollt (2021) clarified the necessity of the autoregressive (AR) model for gaining the insight into the mathematical structure of RC neural networks, and indicated that the Wold decomposition theorem is the milestone for understanding of these. Keeping this celebrated result in mind, in this paper, we clarify hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures attain perfect prediction for the AR type of time series data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36335;&#30001;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#26469;&#26368;&#22823;&#21270;&#28304;&#33410;&#28857;&#30340;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#25152;&#38656;&#20449;&#24687;&#36335;&#30001;&#21040;&#30446;&#26631;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.00248</link><description>&lt;p&gt;
&#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#23398;&#20064;&#22686;&#24378;&#29366;&#24577;&#31574;&#30053;&#36827;&#34892;&#20449;&#24687;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Learning State-Augmented Policies for Information Routing in Communication Networks. (arXiv:2310.00248v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36335;&#30001;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#26469;&#26368;&#22823;&#21270;&#28304;&#33410;&#28857;&#30340;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#25152;&#38656;&#20449;&#24687;&#36335;&#30001;&#21040;&#30446;&#26631;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36335;&#30001;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21482;&#33021;&#35775;&#38382;&#23616;&#37096;&#20449;&#24687;&#30340;&#32422;&#26463;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#22686;&#24378;&#65288;SA&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#36890;&#20449;&#32593;&#32476;&#30340;&#25299;&#25169;&#38142;&#36335;&#19978;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#26469;&#26368;&#22823;&#21270;&#28304;&#33410;&#28857;&#30340;&#32858;&#21512;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#20165;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#19978;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#25152;&#38656;&#30340;&#20449;&#24687;&#36335;&#30001;&#21040;&#30446;&#26631;&#33410;&#28857;&#12290;&#25105;&#20204;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#23558;GNN&#26550;&#26500;&#30340;&#36755;&#20986;&#36716;&#25442;&#20026;&#26368;&#20248;&#30340;&#20449;&#24687;&#36335;&#30001;&#31574;&#30053;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#23454;&#26102;&#32593;&#32476;&#25299;&#25169;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#25968;&#20540;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#20986;&#19982;&#22522;&#32447;&#31639;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;GNN&#21442;&#25968;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the problem of information routing in a large-scale communication network, which can be formulated as a constrained statistical learning problem having access to only local information. We delineate a novel State Augmentation (SA) strategy to maximize the aggregate information at source nodes using graph neural network (GNN) architectures, by deploying graph convolutions over the topological links of the communication network. The proposed technique leverages only the local information available at each node and efficiently routes desired information to the destination nodes. We leverage an unsupervised learning procedure to convert the output of the GNN architecture to optimal information routing strategies. In the experiments, we perform the evaluation on real-time network topologies to validate our algorithms. Numerical simulations depict the improved performance of the proposed method in training a GNN parameterization as compared to baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.15325</link><description>&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#31185;&#23398;&#21457;&#29616;&#21644;&#24037;&#31243;&#35774;&#35745;&#21463;&#38480;&#20110;&#29289;&#29702;&#23454;&#39564;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#23454;&#39564;&#36890;&#24120;&#26159;&#36890;&#36807;&#35797;&#39564;&#21644;&#30452;&#35273;&#36873;&#25321;&#30340;&#65292;&#24182;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25968;&#20540;&#27169;&#25311;&#26159;&#29289;&#29702;&#23454;&#39564;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#39046;&#22495;&#26469;&#35828;&#65292;&#30001;&#20110;&#29616;&#26377;&#25968;&#20540;&#26041;&#27861;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#29305;&#21035;&#26159;&#65292;&#19968;&#20010;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;AI&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#26144;&#23556;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#20363;&#22914;&#26102;&#31354;&#36807;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#20301;&#32622;&#36827;&#34892;&#22806;&#25512;&#21644;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36827;&#34892;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#12290;&#31070;&#32463;&#36816;&#31639;&#31526;&#21487;&#20197;&#22686;&#24378;&#29978;&#33267;&#26367;&#20195;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#29616;&#26377;&#27169;&#25311;&#22120;&#65292;&#20363;&#22914;&#35745;&#31639;&#21147;&#23398;&#27969;&#20307;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#40723;&#21169;&#24335;&#36890;&#20449;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#33258;&#21033;&#30340;&#32852;&#37030;&#36172;&#33218;&#26426;&#20013;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#26469;&#20419;&#20351;&#23458;&#25143;&#26426;&#20998;&#20139;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#23454;&#38469;&#25805;&#20316;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11702</link><description>&lt;p&gt;
&#40723;&#21169;&#24335;&#27807;&#36890;&#30340;&#32852;&#37030;&#36172;&#33218;&#26426;
&lt;/p&gt;
&lt;p&gt;
Incentivized Communication for Federated Bandits. (arXiv:2309.11702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#40723;&#21169;&#24335;&#36890;&#20449;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#33258;&#21033;&#30340;&#32852;&#37030;&#36172;&#33218;&#26426;&#20013;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#26469;&#20419;&#20351;&#23458;&#25143;&#26426;&#20998;&#20139;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#23454;&#38469;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20851;&#20110;&#32852;&#37030;&#36172;&#33218;&#26426;&#30340;&#24037;&#20316;&#37117;&#40664;&#35748;&#25152;&#26377;&#23458;&#25143;&#26426;&#37117;&#24895;&#24847;&#22312;&#38656;&#35201;&#30340;&#26102;&#20505;&#23558;&#20854;&#25968;&#25454;&#26080;&#31169;&#22320;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20197;&#33719;&#21462;&#38598;&#20307;&#21033;&#30410;&#12290;&#23613;&#31649;&#36825;&#31181;&#20551;&#35774;&#22312;&#24615;&#33021;&#21644;&#36890;&#20449;&#25928;&#29575;&#19978;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#36807;&#20110;&#29702;&#24819;&#21270;&#65292;&#24182;&#19988;&#32463;&#24120;&#34987;&#36829;&#32972;&#65292;&#29305;&#21035;&#26159;&#24403;&#31639;&#27861;&#22312;&#33258;&#21033;&#30340;&#23458;&#25143;&#26426;&#19978;&#36816;&#34892;&#26102;&#65292;&#36825;&#20123;&#23458;&#25143;&#26426;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#22909;&#22788;&#30340;&#24773;&#20917;&#19979;&#20998;&#20139;&#25968;&#25454;&#12290;&#24573;&#35270;&#36825;&#31181;&#33258;&#21033;&#34892;&#20026;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#32852;&#37030;&#36172;&#33218;&#26426;&#23398;&#20064;&#30340;&#25928;&#29575;&#29978;&#33267;&#23454;&#38469;&#21487;&#25805;&#20316;&#24615;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#27491;&#24335;&#24341;&#20837;&#19968;&#31181;&#40723;&#21169;&#24335;&#36890;&#20449;&#38382;&#39064;&#26469;&#20026;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#30740;&#31350;&#39046;&#22495;&#24102;&#26469;&#26032;&#30340;&#35265;&#35299;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#25552;&#20379;&#28608;&#21169;&#26469;&#28608;&#21169;&#23458;&#25143;&#26426;&#20998;&#20139;&#25968;&#25454;&#12290;&#22312;&#19981;&#22833;&#19968;&#33324;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36172;&#33218;&#26426;&#38382;&#39064;&#23454;&#20363;&#21270;&#20026;&#19978;&#19979;&#25991;&#32447;&#24615;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works on federated bandits take it for granted that all clients are altruistic about sharing their data with the server for the collective good whenever needed. Despite their compelling theoretical guarantee on performance and communication efficiency, this assumption is overly idealistic and oftentimes violated in practice, especially when the algorithm is operated over self-interested clients, who are reluctant to share data without explicit benefits. Negligence of such self-interested behaviors can significantly affect the learning efficiency and even the practical operability of federated bandit learning. In light of this, we aim to spark new insights into this under-explored research area by formally introducing an incentivized communication problem for federated bandits, where the server shall motivate clients to share data by providing incentives. Without loss of generality, we instantiate this bandit problem with the contextual linear setting and propose the first
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.05855</link><description>&lt;p&gt;
&#38543;&#26426;&#28388;&#27874;&#22120;&#32452;&#30340;&#33021;&#37327;&#20445;&#25345;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Energy Preservation and Stability of Random Filterbanks. (arXiv:2309.05855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#24418;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#20026;&#20160;&#20040;&#22914;&#27492;&#22256;&#38590;&#65311;&#23613;&#31649;&#26377;&#22810;&#27425;&#23581;&#35797;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(convnets)&#36827;&#34892;&#28388;&#27874;&#22120;&#35774;&#35745;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#36229;&#36234;&#25163;&#24037;&#21019;&#24314;&#30340;&#22522;&#32447;&#12290;&#36825;&#26356;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#32447;&#26159;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65306;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#20256;&#36882;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#22823;&#24863;&#21463;&#37326;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#36825;&#22312;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#20013;&#26159;&#20856;&#22411;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#26426;&#28388;&#27874;&#22120;&#32452;&#30340;&#26399;&#26395;&#33021;&#37327;&#20445;&#25345;&#23545;&#20110;&#25968;&#20540;&#31283;&#23450;&#24615;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
&lt;/p&gt;</description></item><item><title>&#36807;&#28388;&#34920;&#38754;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#22788;&#29702;&#20381;&#36182;&#20110;&#36793;&#26435;&#37325;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#26102;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#24635;&#20307;&#26631;&#20934;&#24046;&#65292;&#24182;&#19988;&#35201;&#20040;&#23436;&#20840;&#26080;&#21442;&#25968;&#65292;&#35201;&#20040;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03616</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#20998;&#31867;&#30340;&#36807;&#28388;&#34920;&#38754;
&lt;/p&gt;
&lt;p&gt;
Filtration Surfaces for Dynamic Graph Classification. (arXiv:2309.03616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03616
&lt;/p&gt;
&lt;p&gt;
&#36807;&#28388;&#34920;&#38754;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#22788;&#29702;&#20381;&#36182;&#20110;&#36793;&#26435;&#37325;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#26102;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#24635;&#20307;&#26631;&#20934;&#24046;&#65292;&#24182;&#19988;&#35201;&#20040;&#23436;&#20840;&#26080;&#21442;&#25968;&#65292;&#35201;&#20040;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#24577;&#22270;&#20998;&#31867;&#26041;&#27861;&#35201;&#20040;&#23558;&#22270;&#20869;&#26680;&#25193;&#23637;&#21040;&#26102;&#38388;&#22495;&#65292;&#35201;&#20040;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#32447;&#26041;&#27861;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#26080;&#27861;&#22788;&#29702;&#19981;&#26029;&#21464;&#21270;&#30340;&#33410;&#28857;&#38598;&#65292;&#25110;&#32773;&#19981;&#33021;&#32771;&#34385;&#36793;&#26435;&#37325;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36807;&#28388;&#34920;&#38754;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#20943;&#36731;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#36807;&#28388;&#34920;&#38754;&#22312;&#20381;&#36182;&#36793;&#26435;&#37325;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#26080;&#21442;&#25968;&#25110;&#26368;&#22810;&#19968;&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#26368;&#20302;&#30340;&#24635;&#20307;&#26631;&#20934;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for classifying dynamic graphs either lift graph kernels to the temporal domain, or use graph neural networks (GNNs). However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account. We propose filtration surfaces, a novel method that is scalable and flexible, to alleviate said restrictions. We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information. Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#36830;&#32493;&#21464;&#37327;&#12289;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#21644;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#21487;&#20998;&#31163;&#30340;&#36739;&#20302;&#19979;&#30028;&#65292;&#23454;&#29616;&#20102;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#19968;&#36339;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#20108;&#36827;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.02606</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#22312;&#32447;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Variational Inference for Online Supervised Learning. (arXiv:2309.02606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#36830;&#32493;&#21464;&#37327;&#12289;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#21644;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#21487;&#20998;&#31163;&#30340;&#36739;&#20302;&#19979;&#30028;&#65292;&#23454;&#29616;&#20102;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#19968;&#36339;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#20108;&#36827;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#24320;&#21457;&#39640;&#25928;&#30340;&#25512;&#26029;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#19979;&#19968;&#20195;&#23450;&#20301;&#12289;&#36319;&#36394;&#21644;&#22320;&#22270;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#12289;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#27010;&#29575;&#21644;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#12290;&#22312;&#38598;&#20013;&#24335;&#35774;&#32622;&#20013;&#65292;&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#25191;&#34892;&#36817;&#20284;&#36125;&#21494;&#26031;&#20272;&#35745;&#30340;&#22522;&#26412;&#25216;&#26415;&#65292;&#20854;&#20013;&#23558;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#23494;&#24230;&#29992;&#21442;&#25968;&#21270;&#23494;&#24230;&#26469;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#20998;&#31163;&#30340;&#36739;&#20302;&#19979;&#30028;&#65292;&#29992;&#20110;&#38598;&#20013;&#24335;&#20272;&#35745;&#30446;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#19968;&#36339;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35777;&#25454;&#36739;&#20302;&#19979;&#30028; (DELBO) &#21253;&#25324;&#35266;&#27979;&#20284;&#28982;&#21644;&#36317;&#31163;&#20808;&#39564;&#23494;&#24230;&#30340;&#24046;&#20540;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#19982;&#27979;&#37327;&#35777;&#25454;&#30340;&#24046;&#36317;&#26159;&#30001;&#20110;&#20849;&#35782;&#21644;&#24314;&#27169;&#35823;&#24046;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#20108;&#36827;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developing efficient solutions for inference problems in intelligent sensor networks is crucial for the next generation of location, tracking, and mapping services. This paper develops a scalable distributed probabilistic inference algorithm that applies to continuous variables, intractable posteriors and large-scale real-time data in sensor networks. In a centralized setting, variational inference is a fundamental technique for performing approximate Bayesian estimation, in which an intractable posterior density is approximated with a parametric density. Our key contribution lies in the derivation of a separable lower bound on the centralized estimation objective, which enables distributed variational inference with one-hop communication in a sensor network. Our distributed evidence lower bound (DELBO) consists of a weighted sum of observation likelihood and divergence to prior densities, and its gap to the measurement evidence is due to consensus and modeling errors. To solve binary 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.13150</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#30340;&#36801;&#31227;ResNet&#22686;&#24378;&#20083;&#33146;&#30284;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#22270;&#20687;&#20998;&#31867;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;ResNet&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#65292;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#65292;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#12290;&#25105;&#20204;&#22312;Breakhis&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#35768;&#22810;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20256;&#32479;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#22312;&#24403;&#20195;&#35270;&#35273;&#21464;&#25442;&#22120;&#31561;&#26368;&#26032;&#26041;&#27861;&#19978;&#20063;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;&#22312;&#35832;&#22914;&#31934;&#24230;&#12289;&#20934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;G-means&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#25910;&#25947;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#20123;&#32467;&#26524;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24041;&#22266;&#20102;&#20854;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
&lt;/p&gt;</description></item><item><title>OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13137</link><description>&lt;p&gt;
OmniQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13137
&lt;/p&gt;
&lt;p&gt;
OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#20102;&#20854;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25552;&#39640;LLM&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#25163;&#24037;&#21046;&#23450;&#37327;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#26497;&#20302;&#20301;&#37327;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#65288;OmniQuant&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;LLMs&#65292;&#23427;&#22312;&#22810;&#31181;&#37327;&#21270;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#26469;&#20445;&#25345;PTQ&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;OmniQuant&#21253;&#21547;&#20004;&#20010;&#21019;&#26032;&#32452;&#20214;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#21098;&#35009;&#65288;LWC&#65289;&#21644;&#21487;&#23398;&#20064;&#30340;&#31561;&#25928;&#21464;&#25442;&#65288;LET&#65289;&#12290;LWC&#36890;&#36807;&#20248;&#21270;&#21098;&#35009;&#38408;&#20540;&#26469;&#35843;&#33410;&#26435;&#37325;&#30340;&#26497;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LET&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;ICU&#24739;&#32773;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#38544;&#34255;&#30340;&#29305;&#24449;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.12575</link><description>&lt;p&gt;
&#29992;&#20110;&#32454;&#31890;&#24230;ICU&#24739;&#32773;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction. (arXiv:2308.12575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;ICU&#24739;&#32773;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#38544;&#34255;&#30340;&#29305;&#24449;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#26159;&#21307;&#38498;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#65292;&#29992;&#20110;&#25910;&#27835;&#37325;&#30151;&#24739;&#32773;&#24182;&#25552;&#20379;&#36830;&#32493;&#30417;&#27979;&#21644;&#27835;&#30103;&#12290;&#24050;&#32463;&#23581;&#35797;&#20102;&#21508;&#31181;&#24739;&#32773;&#39044;&#27979;&#26041;&#27861;&#26469;&#36741;&#21161;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#12290;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#34913;&#37327;&#24739;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#25429;&#25417;&#38544;&#34255;&#30340;&#29305;&#24449;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#34987;&#24573;&#35270;&#20102;&#65292;&#20363;&#22914;&#24739;&#32773;&#29305;&#24449;&#65288;&#22914;&#35786;&#26029;&#20195;&#30721;&#65289;&#20197;&#21450;&#23427;&#20204;&#23545;&#19979;&#28216;&#20020;&#24202;&#39044;&#27979;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#20801;&#35768;&#22312;&#36229;&#22270;&#20013;&#34920;&#31034;&#35786;&#26029;&#20195;&#30721;&#20043;&#38388;&#30340;&#38750;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25429;&#25417;&#38544;&#34255;&#30340;&#29305;&#24449;&#32467;&#26500;&#65292;&#20174;&#32780;&#21487;&#20197;&#35745;&#31639;&#32454;&#31890;&#24230;&#24739;&#32773;&#30456;&#20284;&#24615;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;eICU&#21327;&#20316;&#30740;&#31350;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Intensive Care Unit (ICU) is one of the most important parts of a hospital, which admits critically ill patients and provides continuous monitoring and treatment. Various patient outcome prediction methods have been attempted to assist healthcare professionals in clinical decision-making. Existing methods focus on measuring the similarity between patients using deep neural networks to capture the hidden feature structures. However, the higher-order relationships are ignored, such as patient characteristics (e.g., diagnosis codes) and their causal effects on downstream clinical predictions.  In this paper, we propose a novel Hypergraph Convolutional Network that allows the representation of non-pairwise relationships among diagnosis codes in a hypergraph to capture the hidden feature structures so that fine-grained patient similarity can be calculated for personalized mortality risk prediction. Evaluation using a publicly available eICU Collaborative Research Database indicates that
&lt;/p&gt;</description></item><item><title>HoSNN&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#20445;&#25252;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10373</link><description>&lt;p&gt;
HoSNN: &#20855;&#26377;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds. (arXiv:2308.10373v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10373
&lt;/p&gt;
&lt;p&gt;
HoSNN&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#20445;&#25252;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#39640;&#25928;&#21644;&#24378;&#22823;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#65292;SNNs&#38754;&#20020;&#30528;&#23545;&#25239;&#25915;&#20987;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20174;&#31070;&#32463;&#24658;&#31283;&#24615;&#20013;&#27762;&#21462;&#28789;&#24863;&#30340;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#19968;&#31181;&#20223;&#29983;&#35299;&#20915;&#26041;&#26696;&#65292;&#26469;&#24212;&#23545;SNNs&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#23427;&#26469;&#26500;&#24314;&#25152;&#25552;&#20986;&#30340;&#23545;&#25239;&#24615;&#31283;&#24577;SNN&#65288;HoSNN&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;LIF&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;TA-LIF&#27169;&#22411;&#34701;&#20837;&#20102;&#33258;&#31283;&#23450;&#21160;&#24577;&#38408;&#20540;&#26426;&#21046;&#65292;&#38480;&#21046;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#20256;&#25773;&#65292;&#24182;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20445;&#25252;HoSNN&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#38416;&#26126;TA-LIF&#31070;&#32463;&#20803;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#36755;&#20837;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#21331;&#36234;&#21160;&#24577;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer promise for efficient and powerful neurally inspired computation. Common to other types of neural networks, however, SNNs face the severe issue of vulnerability to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to develop a bio-inspired solution that counters the susceptibilities of SNNs to adversarial onslaughts. At the heart of our approach is a novel threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model, which we adopt to construct the proposed adversarially robust homeostatic SNN (HoSNN). Distinct from traditional LIF models, our TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, curtailing adversarial noise propagation and safeguarding the robustness of HoSNNs in an unsupervised manner. Theoretical analysis is presented to shed light on the stability and convergence properties of the TA-LIF neurons, underscoring their superior dynamic robustness under input di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07037</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;BFNs&#20013;&#65292;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#20250;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#24433;&#21709;&#19979;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#20462;&#25913;&#65292;&#28982;&#21518;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#20174;&#31616;&#21333;&#30340;&#20808;&#39564;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#36825;&#20004;&#20010;&#20998;&#24067;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#30340;&#29983;&#25104;&#36807;&#31243;&#65307;&#19981;&#36807;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#26080;&#38656;&#21069;&#21521;&#36807;&#31243;&#12290;&#23545;&#20110;&#36830;&#32493;&#12289;&#31163;&#25955;&#21270;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31163;&#25955;&#25968;&#25454;&#65292;&#32593;&#32476;&#30340;&#36755;&#20837;&#20301;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26679;&#26412;&#24341;&#23548;&#21644;&#22312;&#35821;&#35328;&#24314;&#27169;&#31561;&#31163;&#25955;&#39046;&#22495;&#36827;&#34892;&#23569;&#37327;&#27493;&#39588;&#29983;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#20102;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#19988;&#19981;&#25918;&#32622;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
&lt;/p&gt;</description></item><item><title>PDE-Refiner &#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05732</link><description>&lt;p&gt;
PDE-Refiner: &#21033;&#29992;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#23454;&#29616;&#20934;&#30830;&#30340;&#38271;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers. (arXiv:2308.05732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05732
&lt;/p&gt;
&lt;p&gt;
PDE-Refiner &#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20256;&#32479;&#35299;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#36825;&#20123;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#23454;&#29992;&#20215;&#20540;&#20381;&#36182;&#20110;&#23427;&#20204;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#24403;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#24120;&#35265;&#30340;&#26102;&#38388;&#23637;&#24320;&#31574;&#30053;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#21457;&#29616;&#24573;&#30053;&#38750;&#20027;&#23548;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#65288;&#36890;&#24120;&#19982;PDE&#35299;&#20013;&#30340;&#39640;&#39057;&#29575;&#30456;&#20851;&#65289;&#26159;&#38480;&#21046;&#31283;&#23450;&#12289;&#20934;&#30830;&#23637;&#24320;&#24615;&#33021;&#30340;&#20027;&#35201;&#38519;&#38449;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#23519;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24341;&#20837;&#20102;PDE-Refiner&#65307;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#36890;&#36807;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#23454;&#29616;&#23545;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#26356;&#20934;&#30830;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;PDE-Refiner&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;GNN&#27169;&#22411;&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#20851;&#27880;&#21644;&#24314;&#31435;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22270;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04220</link><description>&lt;p&gt;
&#23545;GNN&#27169;&#22411;&#22522;&#20110;&#22270;Attention&#30340;&#35299;&#37322;&#30340;&#35821;&#20041;&#35299;&#37322;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models. (arXiv:2308.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;GNN&#27169;&#22411;&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#20851;&#27880;&#21644;&#24314;&#31435;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22270;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#20041;&#20851;&#27880;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#35821;&#20041;&#20449;&#24687;&#30340;&#25200;&#21160;&#65292;&#24182;&#24314;&#31435;&#39044;&#27979;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22270;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24212;&#29992;&#20110;&#22330;&#26223;&#35299;&#37322;&#31561;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#65292;&#21033;&#29992;&#28789;&#27963;&#30340;&#22270;&#32467;&#26500;&#26469;&#31616;&#27905;&#22320;&#25551;&#36848;&#22797;&#26434;&#30340;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#20013;&#20351;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#32467;&#26500;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#22270;&#29305;&#23450;&#30340;&#26041;&#27861;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22240;&#27492;&#20808;&#21069;&#24050;&#32463;&#20351;&#29992;&#23427;&#20204;&#20026;GNN&#39044;&#27979;&#25552;&#20379;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#37322;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#22270;Attention&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27491;&#35268;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#36801;&#31227;&#23398;&#20064;&#24182;&#23545;&#22810;&#20010;EDFA&#22120;&#20214;&#30340;&#27874;&#38271;&#30456;&#20851;&#22686;&#30410;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2308.02233</link><description>&lt;p&gt;
&#33258;&#27491;&#35268;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;EDFA&#27874;&#38271;&#30456;&#20851;&#22686;&#30410;&#24314;&#27169;&#30340;&#19968;&#27425;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain. (arXiv:2308.02233v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27491;&#35268;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#36801;&#31227;&#23398;&#20064;&#24182;&#23545;&#22810;&#20010;EDFA&#22120;&#20214;&#30340;&#27874;&#38271;&#30456;&#20851;&#22686;&#30410;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#30417;&#30563;&#12289;&#33258;&#27491;&#35268;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20010;EDFA&#22120;&#20214;&#30340;&#27874;&#38271;&#30456;&#20851;&#22686;&#30410;&#24314;&#27169;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#27425;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;Open Ireland&#21644;COSMOS&#27979;&#35797;&#24179;&#21488;&#19978;&#23545;22&#20010;EDFA&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#25918;&#22823;&#22120;&#31867;&#22411;&#19979;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel ML framework for modeling the wavelength-dependent gain of multiple EDFAs, based on semi-supervised, self-normalizing neural networks, enabling one-shot transfer learning. Our experiments on 22 EDFAs in Open Ireland and COSMOS testbeds show high-accuracy transfer-learning even when operated across different amplifier types.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.01362</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE. (arXiv:2308.01362v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25903;&#25345;&#32959;&#30244;&#33647;&#29289;&#30340;&#24320;&#21457;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#22686;&#21152;&#39044;&#27979;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#27835;&#30103;&#24182;&#25913;&#21892;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32959;&#30244;&#21160;&#21147;&#31070;&#32463;-ODE&#65288;TDNODE&#65289;&#20316;&#20026;&#19968;&#31181;&#33647;&#29702;&#23398;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#32437;&#21521;&#32959;&#30244;&#22823;&#23567;&#25968;&#25454;&#20013;&#23454;&#29616;&#27169;&#22411;&#21457;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TDNODE&#22312;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#19978;&#30340;&#33021;&#21147;&#65292;&#21363;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#12290;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#35774;&#35745;&#29992;&#20110;&#34920;&#36798;&#20855;&#26377;&#26102;&#38388;&#30340;&#24191;&#20041;&#40784;&#27425;&#24615;&#36825;&#19968;&#22522;&#26412;&#29305;&#24615;&#30340;&#22522;&#30784;&#21160;&#21147;&#23398;&#23450;&#24459;&#12290;&#22240;&#27492;&#65292;&#24314;&#27169;&#24418;&#24335;&#20351;&#24471;&#32534;&#30721;&#22120;&#36755;&#20986;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21160;&#21147;&#23398;&#36895;&#29575;&#25351;&#26631;&#65292;&#20854;&#20013;&#20498;&#25968;&#26102;&#38388;&#20316;&#20026;&#29289;&#29702;&#21333;&#20301;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#25351;&#26631;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#29992;&#20110;&#39044;&#27979;&#24739;&#32773;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;&#25152;&#25552;&#20986;&#30340;&#24314;&#27169;&#24418;&#24335;&#20026;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
While tumor dynamic modeling has been widely applied to support the development of oncology drugs, there remains a need to increase predictivity, enable personalized therapy, and improve decision-making. We propose the use of Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to enable model discovery from longitudinal tumor size data. We show that TDNODE overcomes a key limitation of existing models in its ability to make unbiased predictions from truncated data. The encoder-decoder architecture is designed to express an underlying dynamical law which possesses the fundamental property of generalized homogeneity with respect to time. Thus, the modeling formalism enables the encoder output to be interpreted as kinetic rate metrics, with inverse time as the physical unit. We show that the generated metrics can be used to predict patients' overall survival (OS) with high accuracy. The proposed modeling formalism provides a principled way to integrate multimodal d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#24615;&#22320;&#20998;&#37197;&#36164;&#28304;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#36827;&#31243;&#30340;&#30417;&#25511;&#21644;&#21160;&#24577;&#25506;&#32034;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.14208</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#26465;&#20214;&#19979;&#22312;&#32447;&#24314;&#27169;&#21644;&#30417;&#25511;&#30456;&#20851;&#36827;&#31243;
&lt;/p&gt;
&lt;p&gt;
Online Modeling and Monitoring of Dependent Processes under Resource Constraints. (arXiv:2307.14208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#24615;&#22320;&#20998;&#37197;&#36164;&#28304;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#36827;&#31243;&#30340;&#30417;&#25511;&#21644;&#21160;&#24577;&#25506;&#32034;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#30417;&#25511;&#30456;&#20851;&#36827;&#31243;&#30340;&#32676;&#20307;&#23545;&#20110;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#24615;&#22320;&#20998;&#37197;&#36164;&#28304;&#65292;&#23454;&#29616;&#39640;&#39118;&#38505;&#36827;&#31243;&#30340;&#24320;&#21457;&#21033;&#29992;&#21644;&#30456;&#20851;&#21160;&#24577;&#30340;&#25506;&#32034;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring a population of dependent processes under limited resources is critical for abnormal events detection. A novel online collaborative learning method is proposed to adaptively allocate the resources for exploitation of high-risk processes and exploration of dependent dynamics. Efficiency of the proposed method is proved through theoretical analysis and experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13899</link><description>&lt;p&gt;
&#29992;&#20803;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Regularizing Neural Networks with Meta-Learning Generative Models. (arXiv:2307.13899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#26679;&#26412;&#20316;&#20026;&#23567;&#25968;&#25454;&#38598;&#20998;&#31867;&#30340;&#39069;&#22806;&#25968;&#25454;&#38598;&#12290;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21512;&#25104;&#25968;&#25454;&#20013;&#21253;&#21547;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26080;&#20449;&#24687;&#26679;&#26412;&#12290;&#36825;&#26159;&#22240;&#20026;&#21512;&#25104;&#26679;&#26412;&#19981;&#33021;&#23436;&#32654;&#22320;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#65292;&#22343;&#21248;&#25277;&#26679;&#20063;&#19981;&#19968;&#23450;&#20026;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#36991;&#20813;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#38477;&#32423;&#65292;MGR&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#20132;&#21449;&#29109;&#12290;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#36890;&#36807;&#20803;&#23398;&#20064;&#21160;&#24577;&#30830;&#23450;&#65292;&#20197;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.11949</link><description>&lt;p&gt;
HIQL: &#20197;&#28508;&#22312;&#29366;&#24577;&#20316;&#20026;&#21160;&#20316;&#30340;&#31163;&#32447;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26368;&#36817;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#65288;&#26080;&#22870;&#21169;&#65289;&#25968;&#25454;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#30452;&#25509;&#20174;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20934;&#30830;&#20272;&#35745;&#36828;&#26399;&#30446;&#26631;&#30340;&#20215;&#20540;&#20989;&#25968;&#24456;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#21363;&#36798;&#21040;&#36828;&#26399;&#30446;&#26631;&#38656;&#35201;&#39318;&#20808;&#36890;&#36807;&#36739;&#36817;&#23376;&#30446;&#26631;&#12290;&#36825;&#31181;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35780;&#20272;&#37051;&#36817;&#30446;&#26631;&#30340;&#21160;&#20316;&#36136;&#37327;&#36890;&#24120;&#27604;&#26356;&#36828;&#30446;&#26631;&#23481;&#26131;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;&#21033;&#29992;&#19968;&#20010;&#27809;&#26377;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65306;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11373</link><description>&lt;p&gt;
&#36890;&#36807;Fenchel&#23545;&#20598;&#23454;&#29616;&#22810;&#26679;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21508;&#31181;&#24037;&#20316;&#25552;&#20986;&#20102;&#20197;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#65292;&#20316;&#20026;&#20869;&#22312;&#39537;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#38656;&#35201;&#22312;&#32447;&#29615;&#22659;&#35775;&#38382;&#30340;&#31639;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;\textit{&#31163;&#32447;}&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#32771;&#34385;&#20102;&#22312;KL-&#25955;&#24230;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#32422;&#26463;&#30830;&#20445;&#27599;&#20010;&#25216;&#33021;&#30340;&#29366;&#24577;&#21344;&#29992;&#20445;&#25345;&#22312;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#29366;&#24577;&#25805;&#20316;&#35206;&#30422;&#29575;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#33539;&#22260;&#20869;&#19982;&#19987;&#23478;&#30340;&#29366;&#24577;&#21344;&#29992;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36830;&#25509;Fenchel&#23545;&#20598;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#20316;&#20026;&#36817;&#20284;&#21518;&#39564;&#25512;&#26029;&#30340;&#19968;&#31181;&#36890;&#29992;&#26367;&#20195;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20309;&#26102;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#30340;&#22240;&#23376;&#21270;&#21464;&#20998;&#25512;&#26029;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.11018</link><description>&lt;p&gt;
&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#65306;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#20351;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Amortized Variational Inference: When and Why?. (arXiv:2307.11018v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#20316;&#20026;&#36817;&#20284;&#21518;&#39564;&#25512;&#26029;&#30340;&#19968;&#31181;&#36890;&#29992;&#26367;&#20195;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20309;&#26102;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#30340;&#22240;&#23376;&#21270;&#21464;&#20998;&#25512;&#26029;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#65288;A-VI&#65289;&#26159;&#19968;&#31181;&#36817;&#20284;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#38590;&#20197;&#35745;&#31639;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;A-VI&#30340;&#23450;&#20041;&#29305;&#28857;&#26159;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#25512;&#26029;&#20989;&#25968;&#65292;&#23558;&#27599;&#20010;&#35266;&#23519;&#26144;&#23556;&#21040;&#20854;&#23616;&#37096;&#28508;&#21464;&#37327;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#19982;&#26356;&#20256;&#32479;&#30340;&#20998;&#35299;&#65288;&#25110;&#22343;&#22330;&#65289;&#21464;&#20998;&#25512;&#26029;&#65288;F-VI&#65289;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#30452;&#25509;&#23398;&#20064;&#27599;&#20010;&#28508;&#21464;&#37327;&#30340;&#36817;&#20284;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;A-VI&#29992;&#20316;&#21152;&#36895;&#23616;&#37096;&#28508;&#21464;&#37327;&#25512;&#26029;&#30340;&#35745;&#31639;&#25216;&#24039;&#12290;&#26412;&#25991;&#30740;&#31350;A-VI&#20316;&#20026;&#36817;&#20284;&#21518;&#39564;&#25512;&#26029;&#30340;&#19968;&#31181;&#36890;&#29992;&#26367;&#20195;&#26041;&#27861;&#12290;&#30001;&#20110;&#20998;&#26399;&#23478;&#26063;&#26159;&#20998;&#35299;&#23478;&#26063;&#30340;&#23376;&#38598;&#65292;A-VI&#26080;&#27861;&#20135;&#29983;&#27604;F-VI&#26368;&#20248;&#35299;&#26356;&#20302;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#36817;&#20284;&#20540;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#29702;&#35770;&#38382;&#39064;&#26159;&#21051;&#30011;A-VI&#20309;&#26102;&#20173;&#28982;&#36798;&#21040;F-VI&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amortized variational inference (A-VI) is a method for approximating the intractable posterior distributions that arise in probabilistic models. The defining feature of A-VI is that it learns a global inference function that maps each observation to its local latent variable's approximate posterior. This stands in contrast to the more classical factorized (or mean-field) variational inference (F-VI), which directly learns the parameters of the approximating distribution for each latent variable. In deep generative models, A-VI is used as a computational trick to speed up inference for local latent variables. In this paper, we study A-VI as a general alternative to F-VI for approximate posterior inference. A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family. Thus a central theoretical problem is to characterize when A-VI still attains F-VI's optimal solution. We deri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;ID-LMID&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ID2Graph&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10318</link><description>&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#28040;&#38500;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Eliminating Label Leakage in Tree-Based Vertical Federated Learning. (arXiv:2307.10318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;ID-LMID&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ID2Graph&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20351;&#24471;&#20855;&#26377;&#20849;&#21516;&#29992;&#25143;&#38598;&#21512;&#30340;&#22810;&#20010;&#21442;&#19982;&#26041;&#33021;&#22815;&#22312;&#19981;&#20998;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#65292;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;&#27169;&#22411;&#22312;VFL&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#26641;&#22411;VFL&#30340;&#33030;&#24369;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#65288;&#21363;&#23454;&#20363;&#31354;&#38388;&#65289;&#20998;&#37197;&#30340;&#35760;&#24405;&#26631;&#35782;&#38598;&#21512;&#26469;&#25512;&#23548;&#31169;&#26377;&#35757;&#32451;&#26631;&#31614;&#12290;ID2Graph&#25915;&#20987;&#29983;&#25104;&#35757;&#32451;&#26679;&#26412;&#30340;&#22270;&#32467;&#26500;&#65292;&#20174;&#22270;&#20013;&#25552;&#21462;&#31038;&#21306;&#65292;&#24182;&#20351;&#29992;&#31038;&#21306;&#20449;&#24687;&#23545;&#23616;&#37096;&#25968;&#25454;&#38598;&#36827;&#34892;&#32858;&#31867;&#12290;&#20026;&#20102;&#25269;&#24481;&#23454;&#20363;&#31354;&#38388;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;ID-LMID&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;ID2Graph&#25915;&#20987;&#21576;&#29616;&#20986;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#35270;&#39057;&#24103;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#21644;&#37096;&#20998;&#35299;&#30721;&#24103;&#26469;&#25552;&#39640;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07847</link><description>&lt;p&gt;
&#20113;&#28216;&#25103;&#20013;&#30340;&#31070;&#32463;&#35270;&#39057;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#35270;&#39057;&#24103;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#21644;&#37096;&#20998;&#35299;&#30721;&#24103;&#26469;&#25552;&#39640;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#28216;&#25103;&#26159;&#19968;&#20010;&#20215;&#20540;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#34892;&#19994;&#12290;&#22312;&#20113;&#28216;&#25103;&#20013;&#65292;&#23458;&#25143;&#31471;&#23558;&#33258;&#24049;&#30340;&#31227;&#21160;&#21457;&#36865;&#21040;&#20114;&#32852;&#32593;&#19978;&#30340;&#28216;&#25103;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23558;&#28210;&#26579;&#24182;&#20256;&#36755;&#32467;&#26524;&#35270;&#39057;&#22238;&#26469;&#12290;&#20026;&#20102;&#25552;&#20379;&#33391;&#22909;&#30340;&#28216;&#25103;&#20307;&#39564;&#65292;&#38656;&#35201;&#20302;&#20110;80&#27627;&#31186;&#30340;&#24310;&#36831;&#12290;&#36825;&#24847;&#21619;&#30528;&#35270;&#39057;&#30340;&#28210;&#26579;&#12289;&#32534;&#30721;&#12289;&#20256;&#36755;&#12289;&#35299;&#30721;&#21644;&#26174;&#31034;&#24517;&#39035;&#22312;&#36825;&#20010;&#26102;&#38388;&#33539;&#22260;&#20869;&#23436;&#25104;&#65292;&#30001;&#20110;&#26381;&#21153;&#22120;&#36807;&#36733;&#12289;&#32593;&#32476;&#25317;&#22622;&#21644;&#20002;&#21253;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#28857;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#35270;&#39057;&#24103;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#35270;&#39057;&#24103;&#24674;&#22797;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#26174;&#33879;&#25552;&#21319;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#37096;&#20998;&#35299;&#30721;&#30340;&#24103;&#26469;&#24674;&#22797;&#20002;&#22833;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;(i)&#39640;&#25928;&#25552;&#21462;&#28216;&#25103;&#29366;&#24577;&#65292;(ii)&#20462;&#25913; H.264 &#35270;&#39057;&#35299;&#30721;&#22120;&#29983;&#25104;&#19968;&#20010;&#25351;&#31034;&#38656;&#35201;&#24674;&#22797;&#35270;&#39057;&#24103;&#21738;&#20123;&#37096;&#20998;&#30340;&#25513;&#30721;&#65292;&#21644; (iii)&#35774;&#35745;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35270;&#39057;&#24103;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural networ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#35782;&#21035;&#22810;&#26426;&#22120;&#20154;&#32593;&#32476;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22270;&#24418;&#25299;&#25169;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#29366;&#24577;&#36712;&#36857;&#23548;&#33268;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04374</link><description>&lt;p&gt;
&#20174;&#22810;&#26426;&#22120;&#20154;&#32593;&#32476;&#33410;&#28857;&#36712;&#36857;&#20013;&#23398;&#20064;&#35782;&#21035;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
Learning to Identify Graphs from Node Trajectories in Multi-Robot Networks. (arXiv:2307.04374v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#35782;&#21035;&#22810;&#26426;&#22120;&#20154;&#32593;&#32476;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22270;&#24418;&#25299;&#25169;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#29366;&#24577;&#36712;&#36857;&#23548;&#33268;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#35782;&#21035;&#38382;&#39064;&#26159;&#25351;&#22312;&#32473;&#23450;&#33410;&#28857;&#30340;&#29366;&#24577;/&#29305;&#24449;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#32593;&#32476;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#33410;&#28857;&#30340;&#34892;&#20026;&#21463;&#21040;&#26410;&#30693;&#20132;&#20114;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#29366;&#24577;&#36712;&#36857;&#20351;&#24471;&#38590;&#20197;&#30830;&#23450;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#30456;&#36830;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#21160;&#24577;&#34892;&#20026;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22240;&#27492;&#22312;&#20854;&#20182;&#32593;&#32476;&#37197;&#32622;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#65288;i&#65289;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#30340;&#39640;&#24230;&#20984;&#20248;&#21270;&#31243;&#24207;&#65292;&#26377;&#25928;&#22320;&#25581;&#31034;&#22270;&#24418;&#25299;&#25169;&#32467;&#26500;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#33258;&#25105;&#27880;&#24847;&#32534;&#30721;&#22120;&#65292;&#23427;&#23398;&#20064;&#23558;&#21407;&#22987;&#29366;&#24577;&#36712;&#36857;&#23884;&#20837;&#21040;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#24182;&#39044;&#27979;&#36866;&#24403;&#30340;&#27491;&#35268;&#21270;&#22120;&#29992;&#20110;&#20248;&#21270;&#31243;&#24207;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#26410;&#30693;&#22270;&#24418;&#25299;&#25169;&#30340;&#32593;&#32476;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The graph identification problem consists of discovering the interactions among nodes in a network given their state/feature trajectories. This problem is challenging because the behavior of a node is coupled to all the other nodes by the unknown interaction model. Besides, high-dimensional and nonlinear state trajectories make it difficult to identify if two nodes are connected. Current solutions rely on prior knowledge of the graph topology and the dynamic behavior of the nodes, and hence, have poor generalization to other network configurations. To address these issues, we propose a novel learning-based approach that combines (i) a strongly convex program that efficiently uncovers graph topologies with global convergence guarantees and (ii) a self-attention encoder that learns to embed the original state trajectories into a feature space and predicts appropriate regularizers for the optimization program. In contrast to other works, our approach can identify the graph topology of uns
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04033</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#22312;&#26410;&#30693;&#30340;&#30446;&#26631;&#39046;&#22495;&#20013;&#21482;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#28304;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30446;&#26631;&#22495;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#26412;&#36523;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#26679;&#26412;&#30340;&#27010;&#29575;&#20266;&#26631;&#31614;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#23558;&#28304;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#27979;&#35797;&#26102;&#30340;&#25512;&#24191;&#24314;&#27169;&#20026;&#21464;&#20998;&#25512;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#24314;&#27169;&#20026;&#20998;&#24067;&#65292;&#32771;&#34385;&#27867;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20943;&#36731;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#24615;&#24102;&#26469;&#30340;&#35823;&#23548;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#65292;&#23558;&#37051;&#36817;&#30446;&#26631;&#26679;&#26412;&#30340;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#24378;&#40065;&#26834;&#20266;&#26631;&#31614;&#30340;&#36807;&#31243;&#20013;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#23398;&#20064;&#23558;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#30446;&#26631;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#20934;&#30830;&#12289;&#26356;&#24378;&#40065;&#26834;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#30340;&#33021;&#21147;&#20013;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we 
&lt;/p&gt;</description></item><item><title>S-TLLR&#26159;&#19968;&#20010;&#21463;&#21040;STDP&#26426;&#21046;&#21551;&#21457;&#30340;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.15220</link><description>&lt;p&gt;
S-TLLR: &#21463;&#21040;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#30340;STDP&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks. (arXiv:2306.15220v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15220
&lt;/p&gt;
&lt;p&gt;
S-TLLR&#26159;&#19968;&#20010;&#21463;&#21040;STDP&#26426;&#21046;&#21551;&#21457;&#30340;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#21487;&#29992;&#20110;&#36793;&#32536;&#26234;&#33021;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;SNN&#30340;&#35757;&#32451;&#38754;&#20020;&#30528;&#31934;&#30830;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#29992;&#20998;&#37197;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;BPTT&#31639;&#27861;&#26159;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23427;&#20135;&#29983;&#20102;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;BPTT&#21450;&#20854;&#36817;&#20284;&#20165;&#21033;&#29992;&#20174;&#33033;&#20914;&#27963;&#21160;&#20013;&#23548;&#20986;&#30340;&#22240;&#26524;&#20449;&#24687;&#26469;&#35745;&#31639;&#31361;&#35302;&#26356;&#26032;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S-TLLR&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;Spike-Timing Dependent Plasticity&#65288;STDP&#65289;&#26426;&#21046;&#21551;&#21457;&#30340;&#26032;&#22411;&#19977;&#22240;&#32032;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#26088;&#22312;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;SNN&#35757;&#32451;&#12290;S-TLLR&#21516;&#26102;&#32771;&#34385;&#20102;&#21069;&#21518;&#31361;&#35302;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are biologically plausible models that have been identified as potentially apt for the deployment for energy-efficient intelligence at the edge, particularly for sequential learning tasks. However, training of SNNs poses a significant challenge due to the necessity for precise temporal and spatial credit assignment. Back-propagation through time (BPTT) algorithm, whilst being the most widely used method for addressing these issues, incurs a high computational cost due to its temporal dependency. Moreover, BPTT and its approximations solely utilize causal information derived from the spiking activity to compute the synaptic updates, thus neglecting non-causal relationships. In this work, we propose S-TLLR, a novel three-factor temporal local learning rule inspired by the Spike-Timing Dependent Plasticity (STDP) mechanism, aimed at training SNNs on event-based learning tasks. S-TLLR considers both causal and non-causal relationships between pre and post-syn
&lt;/p&gt;</description></item><item><title>GloptiNets&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#21644;&#24182;&#34892;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14932</link><description>&lt;p&gt;
GloptiNets&#65306;&#20855;&#26377;&#35777;&#26126;&#30340;&#21487;&#25193;&#23637;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GloptiNets: Scalable Non-Convex Optimization with Certificates. (arXiv:2306.14932v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14932
&lt;/p&gt;
&lt;p&gt;
GloptiNets&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#21644;&#24182;&#34892;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36229;&#31435;&#26041;&#20307;&#25110;&#29615;&#38754;&#19978;&#30340;&#20809;&#28369;&#20989;&#25968;&#30340;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20381;&#36182;&#20195;&#25968;&#24615;&#36136;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#65292;&#35813;&#27491;&#21017;&#24615;&#22312;&#20854;&#20613;&#37324;&#21494;&#35889;&#30340;&#34928;&#20943;&#20013;&#20307;&#29616;&#20986;&#26469;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#27169;&#22411;&#26063;&#65292;&#25105;&#20204;&#26082;&#33021;&#22815;&#33719;&#24471;&#31934;&#30830;&#30340;&#35777;&#26126;&#65292;&#21448;&#33021;&#22815;&#21033;&#29992;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#36827;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#25216;&#26415;&#12290;&#36890;&#36807;&#19982;GPU&#30340;&#24182;&#34892;&#35745;&#31639;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#22686;&#24378;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#25968;&#21315;&#20010;&#31995;&#25968;&#20294;&#32500;&#24230;&#36866;&#20013;&#30340;&#22810;&#39033;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#22522;&#20110;Lasserre&#23618;&#27425;&#30340;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31454;&#20105;&#32773;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to non-convex optimization with certificates, which handles smooth functions on the hypercube or on the torus. Unlike traditional methods that rely on algebraic properties, our algorithm exploits the regularity of the target function intrinsic in the decay of its Fourier spectrum. By defining a tractable family of models, we allow at the same time to obtain precise certificates and to leverage the advanced and powerful computational techniques developed to optimize neural networks. In this way the scalability of our approach is naturally enhanced by parallel computing with GPUs. Our approach, when applied to the case of polynomials of moderate dimensions but with thousands of coefficients, outperforms the state-of-the-art optimization methods with certificates, as the ones based on Lasserre's hierarchy, addressing problems intractable for the competitors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#22810;&#26631;&#31614;&#20998;&#31867;&#26694;&#26550;FUTEX&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;&#65307;&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.14003</link><description>&lt;p&gt;
&#20840;&#25991;&#31185;&#25216;&#35770;&#25991;&#30340;&#24369;&#30417;&#30563;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. (arXiv:2306.14003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#22810;&#26631;&#31614;&#20998;&#31867;&#26694;&#26550;FUTEX&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;&#65307;&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#20998;&#31867;&#20381;&#36182;&#20110;&#20998;&#31867;&#25551;&#36848;&#32780;&#38750;&#20154;&#24037;&#26631;&#27880;&#26679;&#26412;&#24314;&#31435;&#20998;&#31867;&#22120;&#12290;&#24050;&#26377;&#30340;&#24369;&#30417;&#30563;&#20998;&#31867;&#30740;&#31350;&#36739;&#23569;&#32771;&#34385;&#21040;&#20004;&#20010;&#25361;&#25112;&#65306;(1)&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;; (2)&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#24212;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#35770;&#25991;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#31561;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FUTEX&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and (2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FUTEX, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;D3FG&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;&#22522;&#20110;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.13769</link><description>&lt;p&gt;
&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration. (arXiv:2306.13769v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;D3FG&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;&#22522;&#20110;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;AI&#36741;&#21161;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#32473;&#23450;&#30446;&#26631;&#34507;&#30333;&#36136;&#30340;&#21475;&#34955;&#32467;&#26500;&#30340;&#20998;&#23376;&#12290;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#21407;&#23376;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#23376;&#35270;&#20026;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#29983;&#25104;&#21407;&#23376;&#20301;&#32622;&#21644;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#30340;&#29616;&#23454;&#29255;&#27573;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D3FG&#65292;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;D3FG&#23558;&#20998;&#23376;&#20998;&#35299;&#20026;&#20004;&#31867;&#32452;&#20214;&#65306;&#23450;&#20041;&#20026;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;&#32452;&#20214;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;D3FG&#23558;&#32452;&#20214;&#30340;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#31867;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#25193;&#25955;&#21040;&#19968;&#20010;&#20808;&#39564;&#20998;&#24067;&#65307;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#21435;&#22122;&#22120;&#36880;&#28176;&#21435;&#38500;&#19977;&#20010;&#21464;&#37327;&#30340;&#22122;&#22768;&#65292;&#20197;&#33719;&#24471;&#29616;&#23454;&#20998;&#23376;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#38024;&#23545;&#20843;&#20010;&#30446;&#26631;&#36827;&#34892;&#24211;&#29983;&#25104;&#65292;&#38024;&#23545;&#20116;&#31181;&#29616;&#26377;&#33647;&#29289;&#36827;&#34892;&#29255;&#27573;&#23436;&#21892;&#65292;&#20197;&#21450;&#38024;&#23545;&#20004;&#20010;&#26410;&#30693;&#21442;&#32771;&#20998;&#23376;&#30340;&#30446;&#26631;&#36827;&#34892;&#21435;&#26032;&#35774;&#35745;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#21697;&#36136;&#30340;&#20998;&#23376;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, AI-assisted drug design methods have been proposed to generate molecules given the pockets' structures of target proteins. Most of them are atom-level-based methods, which consider atoms as basic components and generate atom positions and types. In this way, however, it is hard to generate realistic fragments with complicated structures. To solve this, we propose D3FG, a functional-group-based diffusion model for pocket-specific molecule generation and elaboration. D3FG decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. And the two kinds of components can together form complicated fragments that enhance ligand-protein interactions.  To be specific, in the diffusion process, D3FG diffuses the data distribution of the positions, orientations, and types of the components into a prior distribution; In the generative process, the noise is gradually removed from the three variables by denoisers parame
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13089</link><description>&lt;p&gt;
GIMLET&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25351;&#20196;&#20998;&#23376;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#23454;&#39564;&#36896;&#25104;&#30340;&#26631;&#31614;&#19981;&#36275;&#38382;&#39064;&#23558;&#26159;&#20854;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#30693;&#35782;&#36827;&#34892;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#20998;&#23376;-&#25991;&#26412;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22788;&#29702;&#25351;&#20196;&#19981;&#36275;&#20197;&#21450;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIMLET&#65292;&#23427;&#32479;&#19968;&#20102;&#22270;&#24418;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#24191;&#20041;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#25193;&#23637;&#20197;&#32534;&#30721;&#22270;&#24418;&#32467;&#26500;&#21644;&#25351;&#20196;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22270;&#24418;&#32534;&#30721;&#27169;&#22359;&#12290;GIMLET&#36824;&#22312;&#27880;&#24847;&#26426;&#21046;&#20013;&#35299;&#32806;&#20102;&#22270;&#24418;&#30340;&#32534;&#30721;&#21644;&#20219;&#21153;&#25351;&#20196;&#65292;&#22686;&#24378;&#20102;&#36328;&#26032;&#20219;&#21153;&#30340;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.12045</link><description>&lt;p&gt;
&#22788;&#29702;&#33258;&#28982;&#35270;&#35273;&#22330;&#26223;&#31070;&#32463;&#21709;&#24212;&#30340;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31070;&#32463;&#21709;&#24212;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#24863;&#30693;&#22788;&#29702;&#21644;&#31070;&#32463;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#36807;&#28388;&#22120;&#26469;&#22788;&#29702;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#22788;&#29702;&#27969;&#31243;&#19981;&#29616;&#23454;&#19988;&#19981;&#28789;&#27963;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38024;&#23545;&#35797;&#39564;&#24179;&#22343;&#21457;&#25918;&#29575;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#33033;&#20914;&#21015;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;TeCoS-LVM&#65289;&#26469;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20135;&#29983;&#30452;&#25509;&#21305;&#37197;&#35760;&#24405;&#33033;&#20914;&#21015;&#30340;&#33033;&#20914;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#36991;&#20813;&#20002;&#22833;&#23884;&#20837;&#22312;&#21407;&#22987;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#25490;&#38500;&#26102;&#38388;&#32500;&#24230;&#65292;&#24182;&#24341;&#20837;&#26102;&#38388;&#26465;&#20214;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#33258;&#28982;&#33539;&#24335;&#20013;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TeCoS-LVM &#27169;&#22411;&#33021;&#22815;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;UUKG&#65292;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26500;&#24314;UrbanKG&#24182;&#20998;&#26512;&#20854;&#39640;&#38454;&#32467;&#26500;&#27169;&#24335;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#20379;&#20851;&#38190;&#30693;&#35782;&#65292;&#22686;&#24378;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11443</link><description>&lt;p&gt;
UUKG&#65306;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction. (arXiv:2306.11443v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11443
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;UUKG&#65292;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26500;&#24314;UrbanKG&#24182;&#20998;&#26512;&#20854;&#39640;&#38454;&#32467;&#26500;&#27169;&#24335;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#20379;&#20851;&#38190;&#30693;&#35782;&#65292;&#22686;&#24378;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#23545;&#26234;&#24935;&#22478;&#24066;&#30340;&#21457;&#23637;&#21644;&#36816;&#33829;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#26032;&#20852;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#36890;&#24120;&#34987;&#25972;&#21512;&#20026;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#65288;UrbanKG&#65289;&#65292;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20851;&#38190;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UrbanKG&#36890;&#24120;&#20026;&#29305;&#23450;&#30340;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#65292;&#19988;&#19981;&#20844;&#24320;&#21487;&#29992;&#65292;&#38480;&#21046;&#20102;&#28508;&#22312;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UUKG&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22686;&#24378;&#30693;&#35782;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19977;&#20803;&#32452;&#30340;UrbanKG&#65292;&#36830;&#25509;&#20102;&#22478;&#24066;&#20013;&#30340;&#24322;&#26500;&#23454;&#20307;&#65292;&#22914;&#34892;&#25919;&#21306;&#12289;&#20852;&#36259;&#28857;&#21644;&#36947;&#36335;&#27573;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26500;&#24314;&#30340;UrbanKG&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#21508;&#31181;&#39640;&#38454;&#32467;&#26500;&#27169;&#24335;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#24490;&#29615;&#65292;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the development and operation of the smart city. As an emerging building block, multi-sourced urban data are usually integrated as urban knowledge graphs (UrbanKGs) to provide critical knowledge for urban spatiotemporal prediction models. However, existing UrbanKGs are often tailored for specific downstream prediction tasks and are not publicly available, which limits the potential advancement. This paper presents UUKG, the unified urban knowledge graph dataset for knowledge-enhanced urban spatiotemporal predictions. Specifically, we first construct UrbanKGs consisting of millions of triplets for two metropolises by connecting heterogeneous urban entities such as administrative boroughs, POIs, and road segments. Moreover, we conduct qualitative and quantitative analysis on constructed UrbanKGs and uncover diverse high-order structural patterns, such as hierarchies and cycles, that can be leveraged to benefit down
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#30340;&#23545;&#25239;&#25628;&#32034;&#21644;&#36861;&#36394;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36807;&#28388;&#27169;&#22411;&#26469;&#20272;&#35745;&#23545;&#25163;&#20301;&#32622;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26816;&#27979;&#29575;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.11301</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#23545;&#25239;&#25628;&#32034;&#21644;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Adversarial Search and Tracking with Multiagent Reinforcement Learning in Sparsely Observable Environment. (arXiv:2306.11301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#30340;&#23545;&#25239;&#25628;&#32034;&#21644;&#36861;&#36394;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36807;&#28388;&#27169;&#22411;&#26469;&#20272;&#35745;&#23545;&#25163;&#20301;&#32622;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26816;&#27979;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#25628;&#32034;&#21644;&#36861;&#36394;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#21160;&#24577;&#25628;&#32034;&#22242;&#38431;&#24517;&#39035;&#21512;&#20316;&#36861;&#36394;&#19968;&#20010;&#23545;&#25239;&#24615;&#30340;&#12289;&#38590;&#20197;&#25429;&#25417;&#30340;&#20195;&#29702;&#12290;&#24322;&#26500;&#30340;&#25628;&#32034;&#22242;&#38431;&#21487;&#33021;&#21482;&#33021;&#22312;&#19968;&#20010;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20869;&#35775;&#38382;&#26377;&#38480;&#25968;&#37327;&#30340;&#36807;&#21435;&#23545;&#25163;&#36712;&#36857;&#12290;&#30001;&#20110;&#23545;&#25163;&#22312;&#22823;&#31354;&#38388;&#20869;&#34920;&#29616;&#20986;&#21453;&#24212;&#24615;&#21644;&#27450;&#39575;&#24615;&#30340;&#36867;&#36991;&#34892;&#20026;&#65292;&#23548;&#33268;&#25628;&#32034;&#20195;&#29702;&#30340;&#26816;&#27979;&#21464;&#24471;&#31232;&#30095;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#25105;&#20204;&#21487;&#23398;&#20064;&#30340;&#36807;&#28388;&#27169;&#22411;&#23545;&#23545;&#25163;&#20301;&#32622;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;MARL&#26550;&#26500;&#21487;&#20197;&#36229;&#36234;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;46%&#30340;&#26816;&#27979;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a search and tracking (S&amp;T) problem where a team of dynamic search agents must collaborate to track an adversarial, evasive agent. The heterogeneous search team may only have access to a limited number of past adversary trajectories within a large search space. This problem is challenging for both model-based searching and reinforcement learning (RL) methods since the adversary exhibits reactionary and deceptive evasive behaviors in a large space leading to sparse detections for the search agents. To address this challenge, we propose a novel Multi-Agent RL (MARL) framework that leverages the estimated adversary location from our learnable filtering model. We show that our MARL architecture can outperform all baselines and achieves a 46% increase in detection rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.10050</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interpolating Item and User Fairness in Recommendation Systems. (arXiv:2306.10050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#36793;&#24179;&#21488;&#20013;&#65292;&#24179;&#21488;&#19982;&#21334;&#23478;&#65288;&#39033;&#30446;&#65289;&#21644;&#23458;&#25143;&#65288;&#29992;&#25143;&#65289;&#31561;&#21508;&#31181;&#21508;&#26679;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#20114;&#21160;&#65292;&#27599;&#20010;&#30456;&#20851;&#32773;&#37117;&#26377;&#33258;&#24049;&#30340;&#26399;&#26395;&#32467;&#26524;&#65292;&#23547;&#25214;&#21512;&#36866;&#30340;&#24179;&#34913;&#28857;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#8220;&#20844;&#24179;&#25104;&#26412;&#8221;&#65292;&#23427;&#25429;&#25417;&#20102;&#24179;&#21488;&#22312;&#24179;&#34913;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21033;&#30410;&#26102;&#21487;&#33021;&#20570;&#20986;&#30340;&#22949;&#21327;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#25512;&#33616;&#26694;&#26550;&#65292;&#20854;&#20013;&#24179;&#21488;&#22312;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#32422;&#26463;&#26102;&#26368;&#22823;&#21270;&#20854;&#25910;&#30410;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26356;&#29616;&#23454;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22312;&#32447;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20844;&#24179;&#25512;&#33616;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24179;&#21488;&#32570;&#20047;&#20102;&#35299;&#29992;&#25143;&#20559;&#22909;&#30340;&#30693;&#35782;&#65292;&#21482;&#33021;&#35266;&#23519;&#20108;&#36827;&#21046;&#36141;&#20080;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#22312;&#32500;&#25252;&#24179;&#21488;&#25910;&#30410;&#30340;&#21516;&#26102;&#31649;&#29702;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#21516;&#26102;&#20445;&#25345;&#39640;&#25910;&#30410;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online platforms employ recommendation systems to enhance customer engagement and drive revenue. However, in a multi-sided platform where the platform interacts with diverse stakeholders such as sellers (items) and customers (users), each with their own desired outcomes, finding an appropriate middle ground becomes a complex operational challenge. In this work, we investigate the ``price of fairness'', which captures the platform's potential compromises when balancing the interests of different stakeholders. Motivated by this, we propose a fair recommendation framework where the platform maximizes its revenue while interpolating between item and user fairness constraints. We further examine the fair recommendation problem in a more realistic yet challenging online setting, where the platform lacks knowledge of user preferences and can only observe binary purchase decisions. To address this, we design a low-regret online optimization algorithm that preserves the platform's revenue while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#19987;&#20026;Landsat&#31995;&#21015;&#21355;&#26143;&#35774;&#35745;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#38598;SSL4EO-L&#65292;&#36825;&#20063;&#26159;&#21382;&#21490;&#19978;&#26368;&#22823;&#30340;Landsat&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09424</link><description>&lt;p&gt;
Landsat&#24433;&#20687;&#25968;&#25454;&#38598;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;SSL4EO-L&#65306;&#28145;&#24230;&#33258;&#23398;&#20064;&#30340;&#39318;&#20010;&#21355;&#26143;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SSL4EO-L: Datasets and Foundation Models for Landsat Imagery. (arXiv:2306.09424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#19987;&#20026;Landsat&#31995;&#21015;&#21355;&#26143;&#35774;&#35745;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#38598;SSL4EO-L&#65292;&#36825;&#20063;&#26159;&#21382;&#21490;&#19978;&#26368;&#22823;&#30340;Landsat&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Landsat&#35745;&#21010;&#26159;&#26377;&#21490;&#20197;&#26469;&#36816;&#34892;&#26102;&#38388;&#26368;&#38271;&#30340;&#22320;&#29699;&#35266;&#27979;&#35745;&#21010;&#65292;&#36890;&#36807;8&#20010;&#21355;&#26143;&#30340;&#25968;&#25454;&#33719;&#21462;&#65292;&#24050;&#32463;&#26377;50&#22810;&#24180;&#30340;&#21382;&#21490;&#12290;&#36825;&#20123;&#21355;&#26143;&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;&#22810;&#20809;&#35889;&#22270;&#20687;&#23545;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21644;&#36965;&#24863;&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#30001;&#20110;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#27969;&#34892;&#21644;&#32570;&#20047;&#22522;&#30784;&#27169;&#22411;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20173;&#20351;&#29992;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;Landsat&#22270;&#20687;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SSL4EO-L&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;Landsat&#31995;&#21015;&#21355;&#26143;&#65288;&#21253;&#25324;3&#20010;&#20256;&#24863;&#22120;&#21644;2&#20010;&#20135;&#21697;&#32423;&#21035;&#65289;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#21382;&#21490;&#19978;&#26368;&#22823;&#30340;Landsat&#25968;&#25454;&#38598;&#65288;500&#19975;&#22270;&#20687;&#22359;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;L7 Irish&#21644;L8 Biome&#20113;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20171;&#32461;&#20102;Landsats 4-5 TM&#21644;Landsat 7 ETM + SR&#30340;&#31532;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;SSL4EO-L&#39044;&#35757;&#32451;&#20102;&#31532;&#19968;&#20010;Landsat&#25968;&#25454;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery captured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Landsat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat family of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4-5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#26222;&#36866;&#30340;&#26657;&#20934;&#35823;&#24046;&#23450;&#20041;&#8212;&#8212;&#20998;&#21306;&#26657;&#20934;&#35823;&#24046;&#65288;PCE&#65289;&#65292;&#25351;&#20986;&#20102;&#20998;&#21306;&#21010;&#20998;&#26159;&#21508;&#31181;&#26657;&#20934;&#35823;&#24046;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21629;&#39064;&#65306;&#20934;&#30830;&#30340;&#27169;&#22411;&#24212;&#35813;&#22312;&#20219;&#20309;&#20998;&#21306;&#19978;&#37117;&#20855;&#26377;&#26657;&#20934;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#27010;&#29575;&#20998;&#21306;&#12290;&#36890;&#36807;&#35821;&#20041;&#30456;&#20851;&#30340;&#20998;&#21306;&#20989;&#25968;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#20998;&#21306;&#20989;&#25968;&#30340;&#31890;&#24230;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04985</link><description>&lt;p&gt;
&#36229;&#36234;&#27010;&#29575;&#21010;&#20998;&#65306;&#35821;&#20041;&#24863;&#30693;&#20998;&#32452;&#26657;&#20934;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Beyond Probability Partitions: Calibrating Neural Networks with Semantic Aware Grouping. (arXiv:2306.04985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#26222;&#36866;&#30340;&#26657;&#20934;&#35823;&#24046;&#23450;&#20041;&#8212;&#8212;&#20998;&#21306;&#26657;&#20934;&#35823;&#24046;&#65288;PCE&#65289;&#65292;&#25351;&#20986;&#20102;&#20998;&#21306;&#21010;&#20998;&#26159;&#21508;&#31181;&#26657;&#20934;&#35823;&#24046;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21629;&#39064;&#65306;&#20934;&#30830;&#30340;&#27169;&#22411;&#24212;&#35813;&#22312;&#20219;&#20309;&#20998;&#21306;&#19978;&#37117;&#20855;&#26377;&#26657;&#20934;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#27010;&#29575;&#20998;&#21306;&#12290;&#36890;&#36807;&#35821;&#20041;&#30456;&#20851;&#30340;&#20998;&#21306;&#20989;&#25968;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#20998;&#21306;&#20989;&#25968;&#30340;&#31890;&#24230;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#32593;&#32476;&#24448;&#24448;&#23545;&#20854;&#39044;&#27979;&#36807;&#20110;&#20048;&#35266;&#65292;&#23548;&#33268;&#39044;&#27979;&#35823;&#24046;&#34987;&#20302;&#20272;&#12290;&#30001;&#20110;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#65292;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#27010;&#29575;&#30340;&#26041;&#27861;&#26469;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#32452;&#24182;&#35780;&#20272;&#26657;&#20934;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36890;&#29992;&#30340;&#26657;&#20934;&#35823;&#24046;&#23450;&#20041;&#65292;&#31216;&#20026;&#20998;&#21306;&#26657;&#20934;&#35823;&#24046;&#65288;PCE&#65289;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#26657;&#20934;&#35823;&#24046;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#22914;&#20309;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#21629;&#39064;&#65292;&#21363;&#20934;&#30830;&#30340;&#27169;&#22411;&#24212;&#35813;&#22312;&#20219;&#20309;&#20998;&#21306;&#19978;&#37117;&#20855;&#26377;&#26657;&#20934;&#24615;&#65292;&#36825;&#34920;&#26126;&#36755;&#20837;&#31354;&#38388;&#20998;&#21306;&#21487;&#20197;&#25193;&#23637;&#21040;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#27010;&#29575;&#20998;&#21306;&#65292;&#36824;&#21487;&#20197;&#21253;&#25324;&#19982;&#36755;&#20837;&#30452;&#25509;&#30456;&#20851;&#30340;&#20998;&#21306;&#12290;&#36890;&#36807;&#35821;&#20041;&#30456;&#20851;&#30340;&#20998;&#21306;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#20110;&#20998;&#21306;&#20989;&#25968;&#30340;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research has shown that deep networks tend to be overly optimistic about their predictions, leading to an underestimation of prediction errors. Due to the limited nature of data, existing studies have proposed various methods based on model prediction probabilities to bin the data and evaluate calibration error. We propose a more generalized definition of calibration error called Partitioned Calibration Error (PCE), revealing that the key difference among these calibration error metrics lies in how the data space is partitioned. We put forth an intuitive proposition that an accurate model should be calibrated across any partition, suggesting that the input space partitioning can extend beyond just the partitioning of prediction probabilities, and include partitions directly related to the input. Through semantic-related partitioning functions, we demonstrate that the relationship between model accuracy and calibration lies in the granularity of the partitioning function. This highlight
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;L2&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#27979;&#35797;&#26102;&#20165;&#38656;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2306.04072</link><description>&lt;p&gt;
L2&#24402;&#19968;&#21270;&#25216;&#26415;&#22312;&#31616;&#21333;&#39640;&#36136;&#37327;OoD&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple High Quality OoD Detection with L2 Normalization. (arXiv:2306.04072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;L2&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#27979;&#35797;&#26102;&#20165;&#38656;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#26041;&#27861;--&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;L2&#24402;&#19968;&#21270;--&#33021;&#22815;&#20135;&#29983;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;&#24403;&#22312;&#27979;&#35797;&#26102;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#26102;&#65292;&#29305;&#24449;&#21521;&#37327;&#30340;L2&#33539;&#25968;&#25104;&#20026;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#20010;&#24778;&#20154;&#30340;&#26367;&#20195;&#32773;&#65292;&#32780;&#24403;&#27809;&#26377;L2&#24402;&#19968;&#21270;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#34892;&#20026;&#21364;&#27809;&#26377;&#37027;&#20040;&#26377;&#25928;&#12290;&#30452;&#35266;&#19978;&#65292;&#29087;&#24713;&#30340;&#22270;&#20687;&#20250;&#20135;&#29983;&#22823;&#30340;&#21521;&#37327;&#65292;&#32780;&#38476;&#29983;&#30340;&#22270;&#20687;&#21017;&#20250;&#20135;&#29983;&#23567;&#30340;&#21521;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#26102;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#22312;&#27979;&#35797;&#26102;&#20063;&#27809;&#26377;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple modification to standard ResNet architectures during training--L2 normalization over feature space--that produces results competitive with state-of-the-art Out-of-Distribution (OoD) detection performance. When L2 normalization is removed at test time, the L2 norm of feature vectors becomes a surprisingly good proxy for network uncertainty, whereas this behaviour is not nearly as effective when training without L2 normalization. Intuitively, familiar images result in large magnitude vectors, while unfamiliar images result in small magnitudes. Notably, this is achievable with almost no additional cost during training, and no cost at test time.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#22914;&#20309;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#21644;&#39044;&#31639;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23548;&#25968;&#27861;&#23454;&#29992;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#22320;&#35782;&#21035;&#27599;&#20010;&#39044;&#31639;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.03543</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#21644;&#39044;&#31639;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
How to Select Which Active Learning Strategy is Best Suited for Your Specific Problem and Budget. (arXiv:2306.03543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03543
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#22914;&#20309;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#21644;&#39044;&#31639;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23548;&#25968;&#27861;&#23454;&#29992;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#22320;&#35782;&#21035;&#27599;&#20010;&#39044;&#31639;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#22312;&#19968;&#23450;&#30340;&#39044;&#31639;&#32422;&#26463;&#19979;&#20027;&#21160;&#36873;&#25321;&#26410;&#26631;&#35760;&#31034;&#20363;&#20197;&#21521;&#31070;&#35861;&#35831;&#27714;&#20854;&#26631;&#35760;&#12290;&#19981;&#21516;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#31574;&#30053;&#26356;&#36866;&#21512;&#19981;&#21516;&#30340;&#38382;&#39064;&#21644;&#39044;&#31639;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20107;&#20808;&#30693;&#36947;&#21738;&#20010;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26368;&#36866;&#21512;&#25163;&#22836;&#30340;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23548;&#25968;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#21160;&#24577;&#22320;&#35782;&#21035;&#27599;&#20010;&#39044;&#31639;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#24773;&#20917;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#28608;&#21457;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#24314;&#31435;&#30452;&#35273;&#12290;&#28982;&#21518;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#20307;&#38382;&#39064;&#21644;&#39044;&#31639;&#26469;&#21160;&#24577;&#36873;&#25321;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#39044;&#31639;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Active Learning (AL), a learner actively chooses which unlabeled examples to query for labels from an oracle, under some budget constraints. Different AL query strategies are more suited to different problems and budgets. Therefore, in practice, knowing in advance which AL strategy is most suited for the problem at hand remains an open problem. To tackle this challenge, we propose a practical derivative-based method that dynamically identifies the best strategy for each budget. We provide theoretical analysis of a simplified case to motivate our approach and build intuition. We then introduce a method to dynamically select an AL strategy based on the specific problem and budget. Empirical results showcase the effectiveness of our approach across diverse budgets and computer vision tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#32467;&#26500;&#22270;&#21387;&#32553;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25299;&#25169;&#32467;&#26500;&#20449;&#24687;&#38544;&#24335;&#32534;&#30721;&#21040;&#21512;&#25104;&#30340;&#26080;&#32467;&#26500;&#22270;&#25968;&#25454;&#30340;&#33410;&#28857;&#23646;&#24615;&#20013;&#65292;&#23558;&#22823;&#35268;&#27169;&#22270;&#21387;&#32553;&#20026;&#19968;&#20010;&#23567;&#35268;&#27169;&#22270;&#33410;&#28857;&#38598;&#65292;&#19981;&#21253;&#21547;&#26174;&#24335;&#30340;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.02664</link><description>&lt;p&gt;
&#26080;&#32467;&#26500;&#22270;&#21387;&#32553;&#65306;&#20174;&#22823;&#35268;&#27169;&#22270;&#21040;&#21387;&#32553;&#30340;&#26080;&#32467;&#26500;&#22270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data. (arXiv:2306.02664v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#32467;&#26500;&#22270;&#21387;&#32553;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25299;&#25169;&#32467;&#26500;&#20449;&#24687;&#38544;&#24335;&#32534;&#30721;&#21040;&#21512;&#25104;&#30340;&#26080;&#32467;&#26500;&#22270;&#25968;&#25454;&#30340;&#33410;&#28857;&#23646;&#24615;&#20013;&#65292;&#23558;&#22823;&#35268;&#27169;&#22270;&#21387;&#32553;&#20026;&#19968;&#20010;&#23567;&#35268;&#27169;&#22270;&#33410;&#28857;&#38598;&#65292;&#19981;&#21253;&#21547;&#26174;&#24335;&#30340;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21387;&#32553;&#36890;&#36807;&#21512;&#25104;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#21387;&#32553;&#22270;&#26469;&#20943;&#23567;&#22823;&#35268;&#27169;&#22270;&#30340;&#22823;&#23567;&#65292;&#23545;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#26377;&#30452;&#25509;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#21387;&#32553;&#22270;&#20013;&#33410;&#28857;&#21644;&#32467;&#26500;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#24573;&#35270;&#20102;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#32467;&#26500;&#22270;&#21387;&#32553;&#33539;&#24335;&#65292;&#31216;&#20026;SFGC&#65292;&#23558;&#22823;&#35268;&#27169;&#22270;&#31934;&#28860;&#20026;&#19968;&#20010;&#23567;&#35268;&#27169;&#22270;&#33410;&#28857;&#38598;&#65292;&#19981;&#21253;&#21547;&#26174;&#24335;&#30340;&#22270;&#32467;&#26500;&#65292;&#21363;&#22270;&#26080;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#23558;&#25299;&#25169;&#32467;&#26500;&#20449;&#24687;&#38544;&#24335;&#32534;&#30721;&#21040;&#21512;&#25104;&#30340;&#26080;&#32467;&#26500;&#22270;&#25968;&#25454;&#30340;&#33410;&#28857;&#23646;&#24615;&#20013;&#65292;&#20854;&#25299;&#25169;&#32467;&#26500;&#34987;&#31616;&#21270;&#20026;&#19968;&#20010;&#21333;&#20301;&#30697;&#38453;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SFGC&#21253;&#21547;&#20004;&#20010;&#21327;&#21516;&#32452;&#20214;&#65306;&#65288;1&#65289;&#29992;&#20110;&#26377;&#25928;&#21512;&#25104;&#23567;&#35268;&#27169;&#26080;&#32467;&#26500;&#22270;&#25968;&#25454;&#30340;&#35757;&#32451;&#36712;&#36857;&#20803;&#21305;&#37197;&#26041;&#26696;&#65307;&#65288;2&#65289;&#29992;&#20110;&#21160;&#24577;&#35780;&#20272;&#22270;&#31070;&#32463;&#29305;&#24449;&#20998;&#25968;&#30340;&#22270;&#23646;&#24615;&#35780;&#20215;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediate benefits for various graph learning tasks. However, existing graph condensation methods rely on the joint optimization of nodes and structures in the condensed graph, and overlook critical issues in effectiveness and generalization ability. In this paper, we advocate a new Structure-Free Graph Condensation paradigm, named SFGC, to distill a large-scale graph into a small-scale graph node set without explicit graph structures, i.e., graph-free data. Our idea is to implicitly encode topology structure information into the node attributes in the synthesized graph-free data, whose topology is reduced to an identity matrix. Specifically, SFGC contains two collaborative components: (1) a training trajectory meta-matching scheme for effectively synthesizing small-scale graph-free data; (2) a graph neural feature score metric for dynamically evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21355;&#26143;&#25968;&#25454;&#25429;&#25417;&#26102;&#31354;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#22826;&#38451;&#36752;&#23556;&#26102;&#24207;&#30340;&#39640;&#31934;&#24230;&#26085;&#21069;&#39044;&#27979;&#65292;&#34920;&#29616;&#20248;&#20110;&#19981;&#37319;&#29992;&#21355;&#26143;&#25968;&#25454;&#30340;&#26102;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#24110;&#21161;&#26356;&#26377;&#25928;&#22320;&#23558;&#22826;&#38451;&#33021;&#34701;&#20837;&#30005;&#32593;&#12290;</title><link>http://arxiv.org/abs/2306.01112</link><description>&lt;p&gt;
&#22914;&#20309;&#29992;&#26102;&#31354;&#19978;&#19979;&#25991;&#20016;&#23500;&#26085;&#21069;&#22826;&#38451;&#36752;&#23556;&#26102;&#24207;&#39044;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
What if We Enrich day-ahead Solar Irradiance Time Series Forecasting with Spatio-Temporal Context?. (arXiv:2306.01112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21355;&#26143;&#25968;&#25454;&#25429;&#25417;&#26102;&#31354;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#22826;&#38451;&#36752;&#23556;&#26102;&#24207;&#30340;&#39640;&#31934;&#24230;&#26085;&#21069;&#39044;&#27979;&#65292;&#34920;&#29616;&#20248;&#20110;&#19981;&#37319;&#29992;&#21355;&#26143;&#25968;&#25454;&#30340;&#26102;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#24110;&#21161;&#26356;&#26377;&#25928;&#22320;&#23558;&#22826;&#38451;&#33021;&#34701;&#20837;&#30005;&#32593;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#33021;&#28508;&#21147;&#24040;&#22823;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;CO2&#25490;&#25918;&#20197;&#32531;&#35299;&#27668;&#20505;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22826;&#38451;&#36752;&#23556;&#30340;&#22266;&#26377;&#21464;&#24322;&#24615;&#32473;&#26080;&#32541;&#34701;&#20837;&#30005;&#32593;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21355;&#26143;&#25968;&#25454;&#26469;&#25429;&#25417;&#26102;&#31354;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#23545;&#24403;&#22320;&#20219;&#20309;&#32473;&#23450;&#31449;&#28857;&#39640;&#31934;&#24230;&#30340;&#26085;&#21069;&#26102;&#24207;&#39044;&#27979;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#23545;&#20840;&#29699;&#27700;&#24179;&#36752;&#23556;&#65288;GHI&#65289;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21462;&#27599;&#20010;&#26102;&#38388;&#27493;&#39044;&#27979;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#39044;&#27979;&#19981;&#30830;&#23450;&#24230;&#30340;&#26377;&#20215;&#20540;&#24230;&#37327;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#30340;&#19977;&#20010;&#31449;&#28857;&#19978;&#20351;&#29992;&#25968;&#25454;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20837;&#26102;&#31354;&#19978;&#19979;&#25991;&#25152;&#24102;&#26469;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#32988;&#36807;&#19981;&#37319;&#29992;&#21355;&#26143;&#25968;&#25454;&#30340;&#26102;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solar power harbors immense potential in mitigating climate change by substantially reducing CO$_{2}$ emissions. Nonetheless, the inherent variability of solar irradiance poses a significant challenge for seamlessly integrating solar power into the electrical grid. While the majority of prior research has centered on employing purely time series-based methodologies for solar forecasting, only a limited number of studies have taken into account factors such as cloud cover or the surrounding physical context. In this paper, we put forth a deep learning architecture designed to harness spatio-temporal context using satellite data, to attain highly accurate \textit{day-ahead} time-series forecasting for any given station, with a particular emphasis on forecasting Global Horizontal Irradiance (GHI). We also suggest a methodology to extract a distribution for each time step prediction, which can serve as a very valuable measure of uncertainty attached to the forecast. When evaluating models,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.18396</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#29702;&#35299;&#21152;&#23494;&#25552;&#31034;&#65306;&#38754;&#21521;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;Transformers
&lt;/p&gt;
&lt;p&gt;
LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#26381;&#21153;&#22120;&#23458;&#25143;&#31471;&#29615;&#22659;&#20013;&#20026;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26500;&#24314;&#31169;&#26377;&#25512;&#26029;&#26694;&#26550;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#25345;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#23458;&#25143;&#31471;&#36755;&#20837;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#31169;&#26377;&#36755;&#20837;&#36890;&#36807;&#21407;&#22987;LLMs&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#26102;&#65292;&#36825;&#20123;&#26694;&#26550;&#20250;&#20135;&#29983;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#19982;&#26368;&#26032;&#30340;Iron&#65288;NeurIPS 2022&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#27169;&#22411;&#25512;&#26029;&#31649;&#36947;&#22312;&#35745;&#31639;&#19978;&#23454;&#29616;&#20102;$5 \times$&#30340;&#21152;&#36895;&#65292;&#22312;&#36890;&#20449;&#24320;&#38144;&#19978;&#23454;&#29616;&#20102;80\%&#30340;&#38477;&#20302;&#65292;&#21516;&#26102;&#20960;&#20046;&#20445;&#25345;&#20102;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#25104;&#21151;&#23558;&#25968;&#25454;&#36827;&#34892;&#20102;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#26368;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18378</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#36827;&#34892;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#25104;&#21151;&#23558;&#25968;&#25454;&#36827;&#34892;&#20102;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#26368;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#23558;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#24182;&#29420;&#31435;&#22320;&#34920;&#31034;&#20986;&#26469;&#65292;&#32780;&#27169;&#22411;&#24182;&#27809;&#26377;&#25552;&#20379;&#26377;&#20851;&#36825;&#20123;&#22240;&#32032;&#30340;&#30495;&#23454;&#20449;&#24687;&#65292;&#24402;&#32435;&#20559;&#35265;&#22312;&#23454;&#29616;&#35299;&#32544;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26045;&#21152;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#26397;&#30528;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#25968;&#25454;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#28508;&#22312;&#32500;&#24230;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#31163;&#25955;&#32534;&#30721;&#65292;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#24212;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#26631;&#37327;&#30721;&#20070;&#12290;&#28508;&#22312;&#37327;&#21270;&#36843;&#20351;&#32534;&#30721;&#22120;&#22312;&#35768;&#22810;&#25968;&#25454;&#28857;&#19978;&#20351;&#29992;&#23569;&#37327;&#28508;&#22312;&#20540;&#65292;&#20174;&#32780;&#20351;&#35299;&#30721;&#22120;&#33021;&#22815;&#20026;&#27599;&#20010;&#20540;&#20998;&#37197;&#19968;&#33268;&#30340;&#21547;&#20041;&#12290;&#35268;&#33539;&#21270;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#24341;&#21521;&#36825;&#31181;&#31616;&#26126;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24191;&#27867;&#24212;&#29992;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
&lt;/p&gt;</description></item><item><title>&#35748;&#35777;&#25216;&#26415;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#39564;&#35777;&#19981;&#36866;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#65292;&#20197;&#39564;&#35777;&#20854;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16822</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Certification of Machine Learning-Based Distributed Systems. (arXiv:2305.16822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16822
&lt;/p&gt;
&lt;p&gt;
&#35748;&#35777;&#25216;&#26415;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#39564;&#35777;&#19981;&#36866;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#65292;&#20197;&#39564;&#35777;&#20854;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26085;&#30410;&#34987;&#29992;&#20110;&#39537;&#21160;&#37096;&#32626;&#22312;5G&#20113;&#36793;&#32536;&#36830;&#32493;&#20307;&#19978;&#30340;&#22797;&#26434;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#36816;&#34892;&#12290;&#30456;&#24212;&#22320;&#65292;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#26356;&#20855;&#38750;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#28436;&#21270;&#38656;&#35201;&#23450;&#20041;&#26032;&#30340;&#20445;&#35777;&#26041;&#27861;&#26469;&#39564;&#35777;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;&#35748;&#35777;&#20316;&#20026;&#31995;&#32479;&#21644;&#36719;&#20214;&#39564;&#35777;&#30340;&#26368;&#27969;&#34892;&#30340;&#20445;&#35777;&#25216;&#26415;&#65292;&#19981;&#33021;&#31435;&#21363;&#36866;&#29992;&#20110;&#20854;&#34892;&#20026;&#30001;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25512;&#29702;&#20915;&#23450;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#25919;&#31574;&#21046;&#23450;&#32773;&#12289;&#30417;&#31649;&#26426;&#26500;&#21644;&#20135;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#36234;&#26469;&#36234;&#25512;&#23815;&#23450;&#20041;ML&#30340;&#38750;&#21151;&#33021;&#23646;&#24615;&#65288;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#65289;&#30340;&#35748;&#35777;&#25216;&#26415;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#21069;&#35748;&#35777;&#26041;&#26696;&#30340;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#35752;&#35770;&#20102;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is increasingly used to drive the operation of complex distributed systems deployed on the cloud-edge continuum enabled by 5G. Correspondingly, distributed systems' behavior is becoming more non-deterministic in nature. This evolution of distributed systems requires the definition of new assurance approaches for the verification of non-functional properties. Certification, the most popular assurance technique for system and software verification, is not immediately applicable to systems whose behavior is determined by Machine Learning-based inference. However, there is an increasing push from policy makers, regulators, and industrial stakeholders towards the definition of techniques for the certification of non-functional properties (e.g., fairness, robustness, privacy) of ML. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues and proposes a first certification scheme for ML-based distributed syst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#30340;Sharpness-Aware Minimization&#31639;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#20854;&#20013;&#35268;&#33539;&#21270;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#31283;&#23450;&#31639;&#27861;&#21644;&#20351;&#20854;&#28418;&#31227;&#27839;&#30528;&#19968;&#31995;&#21015;&#26497;&#23567;&#20540;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#20351;&#31639;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15287</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#22312;Sharpness-Aware Minimization&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Crucial Role of Normalization in Sharpness-Aware Minimization. (arXiv:2305.15287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15287
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#30340;Sharpness-Aware Minimization&#31639;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#20854;&#20013;&#35268;&#33539;&#21270;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#31283;&#23450;&#31639;&#27861;&#21644;&#20351;&#20854;&#28418;&#31227;&#27839;&#30528;&#19968;&#31995;&#21015;&#26497;&#23567;&#20540;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#20351;&#31639;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;SAM&#26356;&#26032;&#20013;&#35268;&#33539;&#21270;&#36825;&#19968;&#20851;&#38190;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#20004;&#26041;&#38754;&#20998;&#26512;&#20102;&#35268;&#33539;&#21270;&#22312;SAM&#20013;&#23545;&#20984;&#20989;&#25968;&#21644;&#38750;&#20984;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#35268;&#33539;&#21270;&#21457;&#25381;&#30340;&#20004;&#20010;&#20851;&#38190;&#20316;&#29992;&#65306;i&#65289;&#23427;&#26377;&#21161;&#20110;&#31283;&#23450;&#31639;&#27861;&#65307;ii&#65289;&#23427;&#20351;&#31639;&#27861;&#33021;&#22815;&#27839;&#30528;&#19968;&#31995;&#21015;&#26497;&#23567;&#20540;&#65288;&#27969;&#24418;&#65289;&#28418;&#31227;&#65292;&#36825;&#26159;&#26368;&#36817;&#19968;&#20123;&#29702;&#35770;&#24037;&#20316;&#30830;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#20851;&#38190;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35748;&#20026;&#65292;&#36825;&#20004;&#20010;&#27491;&#24120;&#21270;&#30340;&#23646;&#24615;&#20351;SAM&#23545;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#35777;&#23454;&#20102;SAM&#30340;&#23454;&#29992;&#24615;&#12290;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization (SAM) is a recently proposed gradient-based optimizer (Foret et al., ICLR 2021) that greatly improves the prediction performance of deep neural networks. Consequently, there has been a surge of interest in explaining its empirical success. We focus, in particular, on understanding the role played by normalization, a key component of the SAM updates. We theoretically and empirically study the effect of normalization in SAM for both convex and non-convex functions, revealing two key roles played by normalization: i) it helps in stabilizing the algorithm; and ii) it enables the algorithm to drift along a continuum (manifold) of minima -- a property identified by recent theoretical works that is the key to better performance. We further argue that these two properties of normalization make SAM robust against the choice of hyper-parameters, supporting the practicality of SAM. Our conclusions are backed by various experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#38598;&#25104;&#21644;&#65288;&#21464;&#20998;&#65289;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32479;&#19968;&#29702;&#35770;&#65292;&#36890;&#36807;&#23558;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#26063;&#20132;&#20114;&#24335;&#28145;&#24230;&#38598;&#25104;&#26041;&#26696;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15027</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#19982;&#65288;&#21464;&#20998;&#65289;&#36125;&#21494;&#26031;&#26041;&#27861;&#20043;&#38388;&#30340;&#20005;&#26684;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods. (arXiv:2305.15027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#38598;&#25104;&#21644;&#65288;&#21464;&#20998;&#65289;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32479;&#19968;&#29702;&#35770;&#65292;&#36890;&#36807;&#23558;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#26063;&#20132;&#20114;&#24335;&#28145;&#24230;&#38598;&#25104;&#26041;&#26696;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#22312;&#25968;&#23398;&#19978;&#24314;&#31435;&#20102;&#36125;&#21494;&#26031;&#12289;&#21464;&#20998;&#36125;&#21494;&#26031;&#21644;&#38598;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#20005;&#26684;&#32852;&#31995;&#12290;&#20854;&#20851;&#38190;&#27493;&#39588;&#26159;&#23558;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#24120;&#36935;&#21040;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#25216;&#26415;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#36890;&#36807;Wasserstein&#26799;&#24230;&#27969;&#30340;&#36879;&#38236;&#30740;&#31350;&#24191;&#20041;&#21464;&#20998;&#25512;&#26029;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#28085;&#30422;&#22810;&#31181;&#30475;&#20284;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#21253;&#25324;&#28145;&#24230;&#38598;&#25104;&#21644;&#65288;&#21464;&#20998;&#65289;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#36825;&#20026;&#28145;&#24230;&#38598;&#25104;&#32988;&#36807;&#22522;&#20110;&#21442;&#25968;&#21270;&#21464;&#20998;&#25512;&#26029;&#30340;&#31243;&#24207;&#32972;&#21518;&#30340;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20801;&#35768;&#25512;&#23548;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#26032;&#38598;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#26063;&#20855;&#26377;&#30452;&#25509;&#31867;&#27604;&#20110;&#29289;&#29702;&#23398;&#20013;&#31890;&#23376;&#31995;&#32479;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#38598;&#25104;&#26469;&#23637;&#31034;&#36825;&#19968;&#28857;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. A key step towards this it to reformulate the non-convex optimisation problem typically encountered in deep learning as a convex optimisation in the space of probability measures. On a technical level, our contribution amounts to studying generalised variational inference through the lense of Wasserstein gradient flows. The result is a unified theory of various seemingly disconnected approaches that are commonly used for uncertainty quantification in deep learning -- including deep ensembles and (variational) Bayesian methods. This offers a fresh perspective on the reasons behind the success of deep ensembles over procedures based on parameterised variational inference, and allows the derivation of new ensembling schemes with convergence guarantees. We showcase this by proposing a family of interacting deep ensembles with direct parallels to the interactions of particle sys
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Calc-X&#21644;Calcformers&#65292;&#23427;&#20204;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20934;&#30830;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15017</link><description>&lt;p&gt;
Calc-X&#21644;Calcformers&#65306;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#22686;&#24378;&#31639;&#26415;&#25512;&#29702;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems. (arXiv:2305.15017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Calc-X&#21644;Calcformers&#65292;&#23427;&#20204;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20934;&#30830;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#20250;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Calc-X&#65292;&#36825;&#26159;&#19968;&#20010;&#28436;&#31034;&#22914;&#20309;&#22312;&#25512;&#29702;&#38142;&#20013;&#27491;&#30830;&#20351;&#29992;&#35745;&#31639;&#22120;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;Calc-X&#36866;&#29992;&#20110;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#23558;&#35745;&#31639;&#20219;&#21153;&#36716;&#31227;&#21040;&#31526;&#21495;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#35843;&#26597;&#24182;&#32479;&#19968;&#20102;&#20960;&#20010;&#24050;&#26377;&#30340;&#25512;&#29702;&#38142;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#26684;&#24335;&#65292;&#32467;&#26524;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;30&#19975;&#20010;&#38656;&#35201;&#36827;&#34892;&#31639;&#26415;&#25512;&#29702;&#30340;&#26679;&#26412;&#30340;&#26631;&#20934;&#25968;&#25454;&#38598;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26032;&#30340;Calc-X&#38598;&#21512;&#26469;&#35757;&#32451;&#25105;&#20204;&#31216;&#20043;&#20026;Calcformers&#30340;&#24320;&#28304;&#35745;&#31639;&#22120;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#36825;&#20123;&#27169;&#22411;&#30456;&#23545;&#20110;&#26222;&#36890;&#35821;&#35328;&#27169;&#22411;&#22522;&#32447;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#36817;&#20046;&#32763;&#20493;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#25152;&#26377;&#30340;Calc-X&#25968;&#25454;&#38598;&#12289;&#28304;&#20195;&#30721;&#21644;Calcformers&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains. Calc-X is suitable for teaching language models to offload computations to a symbolic system. We survey and unify several existing chain-of-thought datasets into a proposed format, resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X collection to train open-source calculator-using models we call Calcformers and show that these models approximately double the accuracy of generating correct results compared to vanilla language model baselines. We make all Calc-X datasets, source code and Calcformers models publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;RAP&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#37096;&#30340;&#19990;&#30028;&#27169;&#22411;&#24182;&#27169;&#25311;&#38271;&#26399;&#34892;&#21160;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#20687;&#20154;&#31867;&#22823;&#33041;&#19968;&#26679;&#30340;&#26377;&#24847;&#35782;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2305.14992</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#23601;&#26159;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model is Planning with World Model. (arXiv:2305.14992v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;RAP&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#37096;&#30340;&#19990;&#30028;&#27169;&#22411;&#24182;&#27169;&#25311;&#38271;&#26399;&#34892;&#21160;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#20687;&#20154;&#31867;&#22823;&#33041;&#19968;&#26679;&#30340;&#26377;&#24847;&#35782;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#31034;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26102;&#65288;&#20363;&#22914;&#24605;&#32500;&#38142;&#65289;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#19968;&#20123;&#23545;&#20154;&#31867;&#26469;&#35828;&#23481;&#26131;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20363;&#22914;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#29983;&#25104;&#25191;&#34892;&#20219;&#21153;&#30340;&#34892;&#21160;&#35745;&#21010;&#65292;&#25110;&#36827;&#34892;&#22797;&#26434;&#30340;&#25968;&#23398;&#12289;&#36923;&#36753;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290;&#36825;&#31181;&#19981;&#36275;&#28304;&#20110;LLMs&#32570;&#20047;&#19968;&#20010;&#20869;&#37096;&#30340;&#8220;&#19990;&#30028;&#27169;&#22411;&#8221;&#65292;&#29992;&#20110;&#39044;&#27979;&#19990;&#30028;&#30340;&#29366;&#24577;&#65288;&#20363;&#22914;&#29615;&#22659;&#29366;&#20917;&#12289;&#20013;&#38388;&#21464;&#37327;&#20540;&#65289;&#24182;&#27169;&#25311;&#34892;&#21160;&#30340;&#38271;&#26399;&#32467;&#26524;&#12290;&#36825;&#20351;&#24471;LLMs&#26080;&#27861;&#20687;&#20154;&#31867;&#22823;&#33041;&#37027;&#26679;&#36827;&#34892;&#26377;&#24847;&#35782;&#30340;&#35268;&#21010;&#65292;&#20854;&#20013;&#21253;&#25324;&#25506;&#32034;&#26367;&#20195;&#30340;&#25512;&#29702;&#36335;&#24452;&#12289;&#39044;&#27979;&#26410;&#26469;&#30340;&#29366;&#24577;&#21644;&#22238;&#25253;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;&#25512;&#29702;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#25512;&#29702;&#26694;&#26550;&#65292;&#21363;RAP&#65288;&#36890;&#36807;&#35268;&#21010;&#36827;&#34892;&#25512;&#29702;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $\textbf{(RAP)}$. RAP repurpo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23613;&#31649;&#22312;&#33521;&#25991;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38463;&#25289;&#20271;&#35821;&#19978;&#30340;&#24615;&#33021;&#19981;&#22914;&#32463;&#36807;&#38463;&#25289;&#20271;&#35821;&#24494;&#35843;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14976</link><description>&lt;p&gt;
GPTAraEval: &#23545;Arabic NLP&#19978;&#30340;ChatGPT&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP. (arXiv:2305.14976v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23613;&#31649;&#22312;&#33521;&#25991;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38463;&#25289;&#20271;&#35821;&#19978;&#30340;&#24615;&#33021;&#19981;&#22914;&#32463;&#36807;&#38463;&#25289;&#20271;&#35821;&#24494;&#35843;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;NLP&#39046;&#22495;&#30340;&#19968;&#27425;&#21464;&#38761;&#65292;&#23588;&#20854;&#22312;&#35768;&#22810;&#33521;&#25991;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#26159;&#26410;&#30693;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#37325;&#28857;&#35780;&#20272;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#26041;&#35328;&#19978;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;44&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28041;&#21450;60&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;ChatGPT&#22312;Arabic NLP&#20013;&#36827;&#34892;&#30340;&#39318;&#27425;&#20840;&#38754;&#24615;&#24615;&#33021;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#19978;&#30340;&#24615;&#33021;&#22987;&#32456;&#19981;&#22914;&#32463;&#36807;&#38463;&#25289;&#20271;&#35821;&#24494;&#35843;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#27604;&#20102;ChatGPT&#21644;GPT-4&#22312;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#21644;&#26041;&#35328;&#38463;&#25289;&#20271;&#35821;&#65288;DA&#65289;&#19978;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
ChatGPT's emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model's efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT's capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a large-scale automated and human evaluation of ChatGPT, encompassing 44 distinct language understanding and generation tasks on over 60 different datasets. To our knowledge, this marks the first extensive performance analysis of ChatGPT's deployment in Arabic NLP. Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#19987;&#38376;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#24615;&#33021;&#36739;&#24369;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14926</link><description>&lt;p&gt;
&#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Universal Self-Adaptive Prompting. (arXiv:2305.14926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#19987;&#38376;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#24615;&#33021;&#36739;&#24369;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26631;&#24535;&#26159;&#23427;&#20204;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#36890;&#24120;&#36890;&#36807;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#39640;&#24230;&#20196;&#20154;&#22402;&#28046;&#24182;&#19988;&#26368;&#20026;&#36890;&#29992;&#65292;LLMs&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#36890;&#24120;&#36739;&#24369;&#65292;&#22240;&#20026;&#32570;&#20047;&#24341;&#23548;&#24182;&#19988;&#38590;&#20197;&#22312;&#22522;&#20110;&#26222;&#36890;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#29616;&#26377;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#24403;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#19981;&#21487;&#29992;&#26102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#36890;&#29992;&#33258;&#36866;&#24212;&#25552;&#31034;(USP)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;(&#21516;&#26102;&#20860;&#23481;&#23569;&#26679;&#26412;&#23398;&#20064;)&#12290;USP&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#20165;&#36827;&#34892;&#25512;&#29702;&#30340;LLM&#65292;&#38750;&#24120;&#28789;&#27963;&#65306;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#25552;&#31034;&#65292;USP&#23558;&#21487;&#33021;&#30340;NLP&#20219;&#21153;&#24402;&#31867;&#20026;&#19977;&#31181;&#21487;&#33021;&#30340;&#20219;&#21153;&#31867;&#22411;&#20043;&#19968;&#65292;&#28982;&#21518;&#20351;&#29992;&#30456;&#24212;&#30340;&#36873;&#25321;&#22120;&#26469;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26597;&#35810;&#21644;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#30340;&#21709;&#24212;&#20316;&#20026;&#20266;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstration
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.14794</link><description>&lt;p&gt;
&#30465;&#24515;&#23398;&#20064;&#21464;&#24471;&#39046;&#20808;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#31616;&#21333;&#31181;&#23376;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#23618;&#27425;&#30340;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#31181;&#23376;&#21305;&#37197;&#30340;&#26377;&#38480;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#31181;&#23376;&#21305;&#37197;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#24046;&#65292;&#36825;&#20250;&#38459;&#27490;&#20998;&#31867;&#22120;&#23398;&#20064;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#31616;&#21333;&#22320;&#21024;&#38500;&#21305;&#37197;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#31181;&#23376;&#35789;&#21487;&#20197;&#32531;&#35299;&#26631;&#31614;&#20559;&#24046;&#24182;&#24110;&#21161;&#23398;&#20064;&#26356;&#22909;&#30340;&#32622;&#20449;&#24230;&#12290;&#38543;&#21518;&#65292;&#31181;&#23376;&#21305;&#37197;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#65292;&#20351;&#23427;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#31181;&#23376;&#35789;&#19981;&#20026;&#20154;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#35758;&#31616;&#21333;&#22320;&#21024;&#38500;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.14735</link><description>&lt;p&gt;
&#36793;&#32536;&#32858;&#28966;&#65306;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#25439;&#20154;&#32676;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#23545;&#36793;&#32536;&#31038;&#21306;&#24433;&#21709;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#30830;&#23450;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20260;&#23475;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20250;&#25513;&#30422;&#30001;&#20132;&#21449;&#23376;&#32676;&#25110;&#36328;&#20154;&#21475;&#32676;&#20307;&#20849;&#20139;&#30340;&#20260;&#23475;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#8220;&#36793;&#32536;&#8221;&#23450;&#20041;&#20026;&#20855;&#26377;&#36828;&#31163;&#8220;&#24120;&#24577;&#8221; &#30340;&#20154;&#21475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#24230;&#37327;&#38024;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#25351;&#25968;&#65288;GPDI&#65289;&#65292;&#20197;&#34913;&#37327;&#25968;&#25454;&#38598;&#32454;&#20998;&#20026;&#23376;&#32452;&#23545;&#38754;&#20020;&#22686;&#21152;&#30340;&#20260;&#23475;&#30340;&#35782;&#21035;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#24322;&#24120;&#20540;&#30340;&#25991;&#26412;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#27602;&#24615;&#26816;&#39564;&#20013;&#27602;&#24615;&#26356;&#39640;&#65292;&#39640;&#36798;28&#65285;&#33267;86&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#21475;&#23398;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#22987;&#32456;&#36739;&#24046;&#65292;&#24322;&#24120;&#20540;&#21644;&#38750;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#38169;&#35823;&#24046;&#36317;&#39640;&#36798;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14257</link><description>&lt;p&gt;
&#20998;&#23618;&#25552;&#31034;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#35266;&#23519;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#24635;&#26159;&#25226;\emph{&#23436;&#25972;}&#35266;&#23519;&#65288;&#20363;&#22914;&#32593;&#39029;&#65289;&#25918;&#21040;&#25552;&#31034;&#20013;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#30340;\emph{&#21387;&#32553;}&#21644;\emph{&#30456;&#20851;}&#30340;&#35266;&#23519;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;\summ&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;\actor&#25552;&#31034;&#26681;&#25454;&#24635;&#32467;&#30340;&#35266;&#23519;&#39044;&#27979;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#23588;&#20854;&#23637;&#31034;&#20102;&#23427;&#22312;&#22797;&#26434;&#30340;&#32593;&#32476;&#23548;&#33322;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#23436;&#25972;&#30340;&#35266;&#23519;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#19978;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;6.2\%&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SciMult&#30340;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20849;&#20139;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2305.14232</link><description>&lt;p&gt;
&#20026;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#39044;&#35757;&#32451;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding. (arXiv:2305.14232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SciMult&#30340;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20849;&#20139;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#22240;&#20854;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36328;&#22810;&#20010;&#24322;&#26500;&#20219;&#21153;&#20849;&#21516;&#21033;&#29992;&#39044;&#35757;&#32451;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26497;&#38480;&#22810;&#26631;&#31614;&#35770;&#25991;&#20998;&#31867;&#12289;&#24341;&#25991;&#39044;&#27979;&#21644;&#25991;&#29486;&#25628;&#32034;&#65289;&#20173;&#28982;&#22522;&#26412;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;SciMult&#65292;&#37325;&#28857;&#26159;&#20419;&#36827;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#25216;&#26415;-&#20219;&#21153;&#24863;&#30693;&#30340;&#29305;&#21270;&#21644;&#25351;&#20196;&#35843;&#25972;&#12290;&#21069;&#32773;&#37319;&#29992;&#20102;&#20855;&#26377;&#20219;&#21153;&#24863;&#30693;&#23376;&#23618;&#30340;&#22810;&#19987;&#23478;&#21464;&#21387;&#22120;&#26550;&#26500;&#65307;&#21518;&#32773;&#22312;&#36755;&#20837;&#25991;&#26412;&#20043;&#21069;&#28155;&#21152;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#20197;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored. To bridge this gap, we propose a multi-task contrastive learning framework, SciMult, with a focus on facilitating common knowledge sharing across different scientific literature understanding tasks while preventing task-specific skills from interfering with each other. To be specific, we explore two techniques -task-aware specialization and instruction tuning. The former adopts a Mixture-of-Experts Transformer architecture with task-aware sub-layers; the latter prepends task-specific instructions to the input text so as to produce t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14032</link><description>&lt;p&gt;
&#24102;&#26377;&#38899;&#39057;&#20809;&#35889;&#21464;&#25442;&#22120;&#30340; Patch-Mix &#23545;&#27604;&#23398;&#20064;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification. (arXiv:2305.14032v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#22768;&#21253;&#21547;&#26089;&#26399;&#35786;&#26029;&#33268;&#21629;&#32954;&#37096;&#30142;&#30149;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#33258; COVID-19 &#30123;&#24773;&#20197;&#26469;&#65292;&#22522;&#20110;&#30005;&#23376;&#21548;&#35786;&#22120;&#30340;&#26080;&#25509;&#35302;&#21307;&#30103;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#31232;&#32570;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340; Patch-Mix &#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#28151;&#21512;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#34917;&#19969;&#65292;&#19982; Audio Spectrogram Transformer (AST) &#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340; Patch-Mix &#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28151;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; ICBHI &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#39640;&#24471;&#20998; 4.08%&#12290;
&lt;/p&gt;
&lt;p&gt;
Respiratory sound contains crucial information for the early diagnosis of fatal lung diseases. Since the COVID-19 pandemic, there has been a growing interest in contact-free medical care based on electronic stethoscopes. To this end, cutting-edge deep learning models have been developed to diagnose lung diseases; however, it is still challenging due to the scarcity of medical data. In this study, we demonstrate that the pretrained model on large-scale visual and audio datasets can be generalized to the respiratory sound classification task. In addition, we introduce a straightforward Patch-Mix augmentation, which randomly mixes patches between different samples, with Audio Spectrogram Transformer (AST). We further propose a novel and effective Patch-Mix Contrastive Learning to distinguish the mixed representations in the latent space. Our method achieves state-of-the-art performance on the ICBHI dataset, outperforming the prior leading score by an improvement of 4.08%.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#32858;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;MoE&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13999</link><description>&lt;p&gt;
&#36808;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13999
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#32858;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;MoE&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#19988;&#31232;&#30095;&#30340;&#21069;&#39304;&#23618;&#65288;S-FFN&#65289;&#65292;&#22914;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#25193;&#22823;Transformer&#27169;&#22411;&#35268;&#27169;&#20197;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#20165;&#28608;&#27963;&#37096;&#20998;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;FFN&#21442;&#25968;&#65292;S-FFN&#22312;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#65288;&#20197;FLOPs&#35745;&#31639;&#65289;&#19981;&#21464;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#31232;&#30095;&#31070;&#32463;&#35760;&#24518;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#19979;&#65292;&#20998;&#26512;&#20102;S-FFN&#30340;&#20004;&#20010;&#20027;&#35201;&#35774;&#35745;&#36873;&#25321;&#65306;&#20869;&#23384;&#22359;&#65288;&#21363;&#19987;&#23478;&#65289;&#22823;&#23567;&#21644;&#20869;&#23384;&#22359;&#36873;&#25321;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;S-FFN&#26550;&#26500;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#36873;&#25321;&#26041;&#27861; - Avg-K&#65292;&#36890;&#36807;&#22343;&#20540;&#32858;&#21512;&#30340;&#38544;&#34255;&#29366;&#24577;&#26469;&#36873;&#25321;&#22359;&#65292;&#30456;&#27604;&#21253;&#25324;Switch Transformer&#65288;Fedus&#31561;&#65292;2021&#65289;&#21644;HashLaye&#22312;&#20869;&#30340;&#29616;&#26377;MoE&#26550;&#26500;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for \textit{pretraining} large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method -\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLaye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13735</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#30340;LLMs&#25511;&#21046;&#65292;&#20363;&#22914;&#20351;&#23427;&#20204;&#25353;&#29031;&#29305;&#23450;&#30340;&#25351;&#20196;&#25805;&#20316;&#32780;&#19981;&#20250;&#20135;&#29983;&#26377;&#23475;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#31034;&#33539;&#21644;&#21453;&#39304;&#12290;&#26368;&#36817;&#65292;&#24320;&#28304;&#27169;&#22411;&#35797;&#22270;&#36890;&#36807;&#25552;&#28860;&#26469;&#33258;&#24050;&#23545;&#40784;&#30340;LLMs&#65288;&#22914;InstructGPT&#25110;ChatGPT&#65289;&#30340;&#25968;&#25454;&#26469;&#22797;&#21046;&#23545;&#40784;&#23398;&#20064;&#36807;&#31243;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#20943;&#23569;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#20294;&#26159;&#26500;&#24314;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#25945;&#24072;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#23398;&#20064;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#31867;&#21171;&#21160;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#23567;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890;LLMs&#30340;&#21709;&#24212;&#36827;&#34892;&#21512;&#25104;&#21453;&#39304;&#30340;&#22870;&#21169;&#24314;&#27169;(RM)&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;RM&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs, e.g., making them follow given instructions while keeping them less toxic. However, it requires a significant amount of human demonstrations and feedback. Recently, open-sourced models have attempted to replicate the alignment learning process by distilling data from already aligned LLMs like InstructGPT or ChatGPT. While this process reduces human efforts, constructing these datasets has a heavy dependency on the teacher models. In this work, we propose a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM for simulating high-quality demonstrations to train a supervised policy and for further optimizing the model with reinforcement learning. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#21464;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#38750;&#21442;&#25968;&#21270;&#20960;&#20309;&#21464;&#24322;&#19979;&#30340;&#29289;&#29702;&#38382;&#39064;&#22238;&#24402;&#12290;</title><link>http://arxiv.org/abs/2305.12871</link><description>&lt;p&gt;
MMGP&#65306;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#21464;&#24418;&#30340;&#39640;&#26031;&#36807;&#31243;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#38750;&#21442;&#25968;&#21270;&#20960;&#20309;&#21464;&#24322;&#19979;&#30340;&#29289;&#29702;&#38382;&#39064;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
MMGP: a Mesh Morphing Gaussian Process-based machine learning method for regression of physical problems under non-parameterized geometrical variability. (arXiv:2305.12871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#21464;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#38750;&#21442;&#25968;&#21270;&#20960;&#20309;&#21464;&#24322;&#19979;&#30340;&#29289;&#29702;&#38382;&#39064;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#35774;&#35745;&#20013;&#23398;&#20064;&#24314;&#27169;&#29289;&#29702;&#29616;&#35937;&#26102;&#65292;&#20960;&#20309;&#21464;&#24322;&#26159;&#37325;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#22238;&#24402;&#25216;&#26415;&#23545;&#21442;&#25968;&#21270;&#20960;&#20309;&#24418;&#29366;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#25512;&#29702;&#38454;&#27573;&#24120;&#24120;&#32570;&#20047;&#24418;&#29366;&#21442;&#25968;&#21270;&#65292;&#21482;&#33021;&#20351;&#29992;&#32593;&#26684;&#31163;&#25955;&#21270;&#20316;&#20026;&#21487;&#29992;&#25968;&#25454;&#12290;&#20174;&#36825;&#31181;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#20013;&#23398;&#20064;&#20223;&#30495;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#36817;&#26399;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20811;&#26381;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#23545;&#22823;&#37327;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#21450;&#26080;&#27861;&#25552;&#20379;&#20869;&#32622;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25110;&#22788;&#29702;&#22823;&#22411;&#32593;&#26684;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#22266;&#23450;&#25299;&#25169;&#32467;&#26500;&#22788;&#29702;&#22797;&#26434;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
When learning simulations for modeling physical phenomena in industrial designs, geometrical variabilities are of prime interest. While classical regression techniques prove effective for parameterized geometries, practical scenarios often involve the absence of shape parametrization during the inference stage, leaving us with only mesh discretizations as available data. Learning simulations from such mesh-based representations poses significant challenges, with recent advances relying heavily on deep graph neural networks to overcome the limitations of conventional machine learning approaches. Despite their promising results, graph neural networks exhibit certain drawbacks, including their dependency on extensive datasets and limitations in providing built-in predictive uncertainties or handling large meshes. In this work, we propose a machine learning method that do not rely on graph neural networks. Complex geometrical shapes and variations with fixed topology are dealt with using w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11141</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;Clifford&#32676;&#65292;&#23427;&#26159;Clifford&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#23376;&#32676;&#65292;&#20854;&#23450;&#20041;&#32463;&#36807;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;&#20027;&#35201;&#22320;&#65292;&#35813;&#32676;&#30340;&#20316;&#29992;&#24418;&#25104;&#20102;&#19968;&#20010;&#27491;&#20132;&#33258;&#21516;&#26500;&#65292;&#25193;&#23637;&#21040;&#25972;&#20010;Clifford&#20195;&#25968;&#65292;&#21516;&#26102;&#23562;&#37325;&#22810;&#30690;&#20998;&#32423;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#24212;&#20110;&#22810;&#30690;&#20998;&#35299;&#30340;&#22810;&#20010;&#38750;&#31561;&#20215;&#23376;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20316;&#29992;&#19981;&#20165;&#23562;&#37325;Clifford&#20195;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#36824;&#23562;&#37325;&#20854;&#20056;&#27861;&#32467;&#26500;&#65292;&#21363;&#20960;&#20309;&#20056;&#31215;&#12290;&#36825;&#20123;&#21457;&#29616;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#22312;&#20219;&#24847;&#32500;&#30340;&#20869;&#31215;&#31354;&#38388;&#20013;&#20248;&#38597;&#22320;&#25512;&#24191;&#30340;&#34920;&#36798;&#23618;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#20174;&#19968;&#20010;sin
&lt;/p&gt;
&lt;p&gt;
We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;--Prompt&#24179;&#22374;&#24230;&#65292;&#21487;&#20197;&#20248;&#21270;&#35821;&#35328;&#25552;&#31034;&#36873;&#25321;&#65292;&#25552;&#39640;&#27169;&#22411;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#32467;&#21512;&#29616;&#26377;&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10713</link><description>&lt;p&gt;
&#24179;&#22374;&#24230;&#24863;&#30693;&#30340;Prompt&#36873;&#25321;&#33021;&#25552;&#39640;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;--Prompt&#24179;&#22374;&#24230;&#65292;&#21487;&#20197;&#20248;&#21270;&#35821;&#35328;&#25552;&#31034;&#36873;&#25321;&#65292;&#25552;&#39640;&#27169;&#22411;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#32467;&#21512;&#29616;&#26377;&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#25552;&#31034;&#24050;&#25104;&#20026;&#35775;&#38382;&#23427;&#20204;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#36825;&#28608;&#21457;&#20102;&#33258;&#21160;&#36873;&#25321;&#26377;&#25928;&#35821;&#35328;&#25552;&#31034;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;Prompt&#24179;&#22374;&#24230;&#65292;&#19968;&#31181;&#37327;&#21270;&#35821;&#35328;&#25552;&#31034;&#39044;&#26399;&#25928;&#29992;&#30340;&#26032;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#21463;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#24179;&#22374;&#24230;&#27491;&#21017;&#21270;&#21551;&#21457;&#65292;&#37327;&#21270;&#27169;&#22411;&#23545;&#20854;&#21442;&#25968;&#25200;&#21160;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35813;&#24230;&#37327;&#30340;&#29702;&#35770;&#22522;&#30784;&#21450;&#20854;&#19982;&#20854;&#20182;Prompt&#36873;&#25321;&#24230;&#37327;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#26041;&#27861;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;Prompt&#24179;&#22374;&#24230;&#19982;&#29616;&#26377;&#24230;&#37327;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;6&#20010;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#20248;&#20110;&#20197;&#21069;&#30340;Prompt&#36873;&#25321;&#24230;&#37327;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;5&#65285;&#65292;Pearson&#30456;&#20851;&#24615;&#25552;&#39640;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce prompt flatness, a new metric to quantify the expected utility of a language prompt. This metric is inspired by flatness regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining prompt flatness with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 5% in accuracy and 10% in Pearson correlation across 6 classification benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10434</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26412;&#20250;&#22312;&#20154;&#20204;&#30340;&#33041;&#28023;&#20013;&#21576;&#29616;&#22270;&#20687;&#65292;&#32780;&#38750;&#35270;&#35273;&#25991;&#26412;&#21017;&#26080;&#27861;&#36798;&#21040;&#27492;&#25928;&#26524;&#12290;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#23558;&#26377;&#21161;&#20110;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;3620&#20010;&#33521;&#35821;&#21477;&#23376;&#21450;&#20854;&#22810;&#20010;&#20154;&#31867;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#35270;&#35273;&#24615;&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;&#25991;&#26412;&#21644;&#35270;&#35273;&#36164;&#20135;&#30340;&#25991;&#26723;&#26469;&#21019;&#24314;&#36828;&#31243;&#30417;&#30563;&#35821;&#26009;&#24211;&#65292;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#19968;&#31181;&#21516;&#26102;&#23558;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#21644;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#32467;&#21512;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07100</link><description>&lt;p&gt;
$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
$\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks. (arXiv:2305.07100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#19968;&#31181;&#21516;&#26102;&#23558;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#21644;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#32467;&#21512;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#22312;&#20960;&#20309;&#22270;&#24418;&#21644;&#28857;&#20113;&#19978;&#30340;&#26041;&#27861;&#65292;&#20854;&#31561;&#21464;&#20110;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#21453;&#23556;&#12290;EMPSNs&#21487;&#20197;&#23398;&#20064;&#22312;&#22270;&#24418;&#20013;&#30340;&#39640;&#32500;&#21333;&#32431;&#38754;&#65288;&#22914;&#19977;&#35282;&#24418;&#65289;&#65292;&#24182;&#20197;$\mathrm{E}(n)$&#31561;&#21464;&#26041;&#24335;&#21033;&#29992;&#26356;&#39640;&#32500;&#21333;&#32431;&#20307;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;EMPSNs&#21516;&#26102;&#23558;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#26356;&#21152;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#39046;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#20013;&#21253;&#21547;&#20960;&#20309;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EMPSNs&#21487;&#20197;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#21333;&#29420;&#20351;&#29992;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#65292;&#24615;&#33021;&#26377;&#20102;&#26222;&#36941;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#32500;&#25805;&#20316;&#20013;&#65292;&#21253;&#21547;&#20960;&#20309;&#20449;&#24687;&#26159;&#38450;&#27490;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#36807;&#24230;&#24179;&#28369;&#30340;&#26377;&#25928;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents $\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks (EMPSNs), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, and reflections. EMPSNs can learn high-dimensional simplex features in graphs (e.g. triangles), and use the increase of geometric information of higher-dimensional simplices in an $\mathrm{E}(n)$ equivariant fashion. EMPSNs simultaneously generalize $\mathrm{E}(n)$ Equivariant Graph Neural Networks to a topologically more elaborate counterpart and provide an approach for including geometric information in Message Passing Simplicial Networks. The results indicate that EMPSNs can leverage the benefits of both approaches, leading to a general increase in performance when compared to either method. Furthermore, the results suggest that incorporating geometric information serves as an effective measure against over-smoothing in message passing networks, especially when operating on high
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20102;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06983</link><description>&lt;p&gt;
&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20102;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20855;&#26377;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#34394;&#20551;&#30340;&#21644;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#20174;&#22806;&#37096;&#30693;&#35782;&#36164;&#28304;&#20013;&#26816;&#32034;&#20449;&#24687;&#26469;&#22686;&#24378;LM&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;LM&#37319;&#29992;&#19968;&#31181;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#35774;&#32622;&#65292;&#20165;&#22522;&#20110;&#36755;&#20837;&#19968;&#27425;&#26816;&#32034;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#29983;&#25104;&#38271;&#25991;&#26412;&#30340;&#26356;&#26222;&#36941;&#30340;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#26029;&#22320;&#25910;&#38598;&#20449;&#24687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36807;&#21435;&#26377;&#19968;&#20123;&#26816;&#32034;&#20449;&#24687;&#65292;&#21516;&#26102;&#29983;&#25104;&#36755;&#20986;&#30340;&#21162;&#21147;&#65292;&#22823;&#22810;&#25968;&#37117;&#26159;&#20351;&#29992;&#21069;&#19968;&#20010;&#19978;&#19979;&#25991;&#20316;&#20026;&#26597;&#35810;&#65292;&#22312;&#22266;&#23450;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#26816;&#32034;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#24191;&#20041;&#35270;&#22270;&#65292;&#21363;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#20174;&#21738;&#37324;&#26816;&#32034;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21069;&#30651;&#24615;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;FLARE&#65289;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#29983;&#25104;&#22120;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#20027;&#21160;&#26597;&#35810;&#26816;&#32034;&#32452;&#20214;&#26469;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#12290;FLARE&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20027;&#21160;&#26816;&#32034;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#26032;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#65292;&#21363;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#25351;&#20986;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25506;&#32034;&#30340;&#23454;&#36136;&#26159;&#35745;&#31639;&#24403;&#21069;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.06851</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30340;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Algorithms Implicitly Optimize by Continuation. (arXiv:2305.06851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#26032;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#65292;&#21363;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#25351;&#20986;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25506;&#32034;&#30340;&#23454;&#36136;&#26159;&#35745;&#31639;&#24403;&#21069;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#36890;&#24120;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#20248;&#21270;&#31574;&#30053;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#24314;&#31435;&#22312;&#20248;&#21270;&#36830;&#32493;&#26694;&#26550;&#19979;&#12290;&#21518;&#32773;&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#38750;&#20984;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20197;&#36830;&#32493;&#30340;&#26367;&#20195;&#30446;&#26631;&#20989;&#25968;&#24207;&#21015;&#20026;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#21270;&#20223;&#23556;&#39640;&#26031;&#31574;&#30053;&#24182;&#25191;&#34892;&#29109;&#27491;&#21017;&#21270;&#21487;&#20197;&#35299;&#37322;&#20026;&#36890;&#36807;&#36830;&#32493;&#38544;&#24335;&#22320;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#30340;&#25506;&#32034;&#21253;&#25324;&#35745;&#31639;&#24403;&#21069;&#30340;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#65292;&#20197;&#36991;&#20813;&#23616;&#37096;&#26368;&#20540;&#32780;&#19981;&#26159;&#20165;&#20165;&#26368;&#22823;&#21270;&#25919;&#31574;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>CAMEL&#25552;&#20986;&#20102;&#20351;&#29992;&#23884;&#20837;&#24335;DRAM&#20316;&#20026;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35774;&#22791;&#31471;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#36807;&#31243;&#20013;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;AI&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.03148</link><description>&lt;p&gt;
CAMEL&#65306;&#38754;&#21521;&#39640;&#25928;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;AI&#27169;&#22411;&#21644;&#23884;&#20837;&#24335;DRAM&#30340;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning. (arXiv:2305.03148v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03148
&lt;/p&gt;
&lt;p&gt;
CAMEL&#25552;&#20986;&#20102;&#20351;&#29992;&#23884;&#20837;&#24335;DRAM&#20316;&#20026;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35774;&#22791;&#31471;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#36807;&#31243;&#20013;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;AI&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#20852;&#36215;&#23548;&#33268;&#36793;&#32536;&#35774;&#22791;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#35774;&#22791;&#31471;&#23398;&#20064;&#20351;&#36793;&#32536;&#24179;&#21488;&#33021;&#22815;&#19981;&#26029;&#22320;&#26681;&#25454;&#29992;&#25143;&#20010;&#20154;&#25968;&#25454;&#35843;&#25972;AI&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;AI&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20250;&#24102;&#26469;&#23494;&#38598;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;&#21344;&#29992;&#22823;&#37327;&#33455;&#29255;&#20869;&#23384;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#23884;&#20837;&#24335;&#21160;&#24577;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;&#65288;eDRAM&#65289;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#12290;&#19982;&#38745;&#24577;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;&#65288;SRAM&#65289;&#30456;&#27604;&#65292;eDRAM&#22312;&#23384;&#20648;&#23494;&#24230;&#19978;&#24341;&#20837;&#20102;&#36229;&#36807;2&#20493;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#30340;&#27969;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25345;&#23384;&#20648;&#30340;&#25968;&#25454;&#23436;&#25972;&#65292;eDRAM&#38656;&#35201;&#25191;&#34892;&#32791;&#30005;&#30340;&#25968;&#25454;&#21047;&#26032;&#25805;&#20316;&#12290;&#22914;&#26524;&#25968;&#25454;&#23384;&#20648;&#19968;&#27573;&#26102;&#38388;&#65292;&#23601;&#21487;&#20197;&#36991;&#20813;eDRAM&#21047;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of the Internet of Things (IoT) has resulted in a remarkable amount of data generated on edge devices, which are often processed using AI algorithms. On-device learning enables edge platforms to continually adapt the AI models to user personal data and further allows for a better service quality. However, AI training on resource-limited devices is extremely difficult because of the intensive computing workload and the significant amount of on-chip memory consumption exacted by deep neural networks (DNNs). To mitigate this, we propose to use embedded dynamic random-access memory (eDRAM) as the main storage medium of training data. Compared with static random-access memory (SRAM), eDRAM introduces more than $2\times$ improvement on storage density, enabling reduced off-chip memory traffic. However, to keep the stored data intact, eDRAM is required to perform the power-hungry data refresh operations.  eDRAM refresh can be eliminated if the data is stored for a period of time
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02996</link><description>&lt;p&gt;
&#24102;&#26377;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;CUR k-NN&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#38170;&#23450;&#39033;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders. (arXiv:2305.02996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;ANNCUR&#27169;&#22411;&#20013;&#39640;&#21069;k&#39033;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#21484;&#22238;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-encoder models, which jointly encode and score a query-item pair, are typically prohibitively expensive for k-nearest neighbor search. Consequently, k-NN search is performed not with a cross-encoder, but with a heuristic retrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work proposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization to produce an embedding space for efficient vector-based search that directly approximates the cross-encoder without the need for dual-encoders. ANNCUR defines this shared query-item embedding space by scoring the test query against anchor items which are sampled uniformly at random. While this minimizes average approximation error over all items, unsuitably high approximation error on top-k items remains and leads to poor recall of top-k (and especially top-1) items. Increasing the number of anchor items is a straightforward way of improving the approximation error and hence k-NN recall of ANNCUR but at the cost o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#25968;&#25454;&#29420;&#31435;&#30340;&#25209;&#22788;&#29702;&#26041;&#26696;&#65292;&#20960;&#20046;&#25152;&#26377;&#23567;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#37117;&#33021;&#22815;&#20248;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#25152;&#26377;&#30340;&#30830;&#23450;&#24615;&#26041;&#26696;&#21644;&#38543;&#26426;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#36825;&#26679;&#30340;&#25209;&#37327;&#35843;&#24230;&#37117;&#33021;&#36798;&#21040;&#26368;&#20248;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.02247</link><description>&lt;p&gt;
&#27627;&#19981;&#30031;&#24807;&#22320;&#36873;&#25321;&#65306;&#20960;&#20046;&#25152;&#26377;&#30340;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#26696;&#37117;&#33021;&#22815;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally. (arXiv:2305.02247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#25968;&#25454;&#29420;&#31435;&#30340;&#25209;&#22788;&#29702;&#26041;&#26696;&#65292;&#20960;&#20046;&#25152;&#26377;&#23567;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#37117;&#33021;&#22815;&#20248;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#25152;&#26377;&#30340;&#30830;&#23450;&#24615;&#26041;&#26696;&#21644;&#38543;&#26426;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#36825;&#26679;&#30340;&#25209;&#37327;&#35843;&#24230;&#37117;&#33021;&#36798;&#21040;&#26368;&#20248;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#12289;&#25968;&#25454;&#29420;&#31435;&#30340;&#23567;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#21305;&#37197;&#19978;&#19979;&#19968;&#33324;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#20294;&#25209;&#37327;&#36873;&#25321;&#35268;&#21017;&#26159;&#20219;&#24847;&#30340;&#12290;&#25105;&#20204;&#32771;&#34385;&#20809;&#28369;&#30340;Lipschitz-&#20984;&#24615;/&#38750;&#20984;&#24615;/&#24378;&#20984;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#32463;&#20856;&#19978;&#38480;&#30028;&#38480;&#20063;&#36866;&#29992;&#20110;&#36825;&#26679;&#20219;&#24847;&#30340;&#38750;&#33258;&#36866;&#24212;&#25209;&#37327;&#35843;&#24230;&#65292;&#21253;&#25324;&#25152;&#26377;&#30830;&#23450;&#24615;&#30340;&#35843;&#24230;&#26041;&#26696;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#23545;&#20110;&#20984;&#21644;&#24378;&#20984;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30452;&#25509;&#35777;&#26126;&#20102;&#22312;&#19978;&#36848;&#25209;&#37327;&#35843;&#24230;&#31867;&#19978;&#19968;&#33268;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#19979;&#30340;&#21305;&#37197;&#19979;&#38480;&#30028;&#38480;&#65292;&#34920;&#26126;&#25152;&#26377;&#36825;&#26679;&#30340;&#25209;&#37327;&#35843;&#24230;&#37117;&#33021;&#36798;&#21040;&#26368;&#20248;&#30340;&#19968;&#33324;&#21270;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#20809;&#28369;&#30340;&#65288;&#38750;Lipschitz&#65289;&#38750;&#20984;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25152;&#32771;&#34385;&#30340;&#31867;&#21035;&#20869;&#65292;&#21253;&#25324;&#25152;&#26377;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#26696;&#65292;&#20840;&#25209;&#37327;&#65288;&#30830;&#23450;&#24615;&#65289;&#26799;&#24230;&#19979;&#38477;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/strongly-convex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones.
&lt;/p&gt;</description></item><item><title>Zenseact Open Dataset&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#35206;&#30422;&#33539;&#22260;&#24191;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#26368;&#39640;&#33539;&#22260;&#21644;&#20998;&#36776;&#29575;&#30340;&#20256;&#24863;&#22120;&#20197;&#21450;&#35814;&#32454;&#30340;&#20851;&#38190;&#24103;&#27880;&#37322;&#65292;&#19987;&#27880;&#20110;&#38271;&#31243;&#24863;&#30693;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.02008</link><description>&lt;p&gt;
Zenseact&#24320;&#25918;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving. (arXiv:2305.02008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02008
&lt;/p&gt;
&lt;p&gt;
Zenseact Open Dataset&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#35206;&#30422;&#33539;&#22260;&#24191;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#26368;&#39640;&#33539;&#22260;&#21644;&#20998;&#36776;&#29575;&#30340;&#20256;&#24863;&#22120;&#20197;&#21450;&#35814;&#32454;&#30340;&#20851;&#38190;&#24103;&#27880;&#37322;&#65292;&#19987;&#27880;&#20110;&#38271;&#31243;&#24863;&#30693;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#25968;&#25454;&#38598;&#36890;&#24120;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#38271;&#31243;&#33021;&#21147;&#65292;&#32780;&#26356;&#20851;&#27880;&#20110; 360&#24230;&#24863;&#30693;&#21644;&#26102;&#38388;&#25512;&#29702;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Zenseact&#24320;&#25918;&#25968;&#25454;&#38598;&#65288;ZOD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#27431;&#27954;&#21508;&#22269;&#25910;&#38598;&#20004;&#24180;&#30340;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#38754;&#31215;&#26159;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;9&#20493;&#12290;&#19982;&#21487;&#27604;&#36739;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;ZOD&#25317;&#26377;&#26368;&#39640;&#33539;&#22260;&#21644;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#65292;&#21516;&#26102;&#37197;&#22791;&#20102;2D&#21644;3D&#23545;&#35937;&#65288;&#38271;&#36798;245m&#65289;&#12289;&#36947;&#36335;&#23454;&#20363;/&#35821;&#20041;&#20998;&#21106;&#12289;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#21644;&#36947;&#36335;&#20998;&#31867;&#30340;&#35814;&#32454;&#20851;&#38190;&#24103;&#27880;&#37322;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#29420;&#29305;&#32452;&#21512;&#23558;&#26377;&#21161;&#20110;&#31361;&#30772;&#38271;&#31243;&#24863;&#30693;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#38590;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;Frames&#12289; Sequences&#21644; Drives&#19977;&#37096;&#20998;&#32452;&#25104;&#65292;&#26088;&#22312;&#21253;&#21547;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#25903;&#25345;&#26102;&#31354;&#23398;&#20064;&#12289;&#20256;&#24863;&#22120;&#34701;&#21512;&#12289;&#23450;&#20301;&#21644;&#26144;&#23556;&#12290;Frames&#30001;10&#19975;&#20010;&#31579;&#36873;&#21518;&#30340;&#30456;&#26426;&#22270;&#20687;&#21644;&#20004;&#31186;&#38047;&#30340;&#20854;&#20182;&#25903;&#25345;&#25968;&#25454;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing datasets for autonomous driving (AD) often lack diversity and long-range capabilities, focusing instead on 360{\deg} perception and temporal reasoning. To address this gap, we introduce Zenseact Open Dataset (ZOD), a large-scale and diverse multimodal dataset collected over two years in various European countries, covering an area 9x that of existing datasets. ZOD boasts the highest range and resolution sensors among comparable datasets, coupled with detailed keyframe annotations for 2D and 3D objects (up to 245m), road instance/semantic segmentation, traffic sign recognition, and road classification. We believe that this unique combination will facilitate breakthroughs in long-range perception and multi-task learning. The dataset is composed of Frames, Sequences, and Drives, designed to encompass both data diversity and support for spatio-temporal learning, sensor fusion, localization, and mapping. Frames consist of 100k curated camera images with two seconds of other support
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#19981;&#21464;&#27491;&#21017;&#21270;&#26041;&#27861;&#65288;AIR&#65289;&#26469;&#24378;&#21046;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#30340;&#23398;&#20064;&#34920;&#31034;&#21576;&#29616;&#26679;&#24335;&#29420;&#31435;&#24615;&#65292;&#24182;&#29992;&#21152;&#26435;SIR&#21644;AIR&#23454;&#29616;ACL&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#21644;&#24120;&#35265;&#27745;&#26579;&#19979;&#22343;&#26174;&#33879;&#25552;&#39640;ACL&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00374</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#19981;&#21464;&#27491;&#21017;&#21270;&#22686;&#24378;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization. (arXiv:2305.00374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#19981;&#21464;&#27491;&#21017;&#21270;&#26041;&#27861;&#65288;AIR&#65289;&#26469;&#24378;&#21046;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#30340;&#23398;&#20064;&#34920;&#31034;&#21576;&#29616;&#26679;&#24335;&#29420;&#31435;&#24615;&#65292;&#24182;&#29992;&#21152;&#26435;SIR&#21644;AIR&#23454;&#29616;ACL&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#21644;&#24120;&#35265;&#27745;&#26579;&#19979;&#22343;&#26174;&#33879;&#25552;&#39640;ACL&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;(ACL)&#26080;&#38656;&#26631;&#31614;&#65292;&#23558;&#23545;&#25239;&#24615;&#25968;&#25454;&#19982;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(SCL)&#30456;&#32467;&#21512;&#65292;&#36755;&#20986;&#19968;&#20010;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#34920;&#31034;&#65292;&#21487;&#27867;&#21270;&#19988;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#24120;&#35265;&#27745;&#26579;&#12290;&#34920;&#31034;&#30340;&#26679;&#24335;&#29420;&#31435;&#23646;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#36716;&#31227;&#12290;&#26631;&#20934;&#19981;&#21464;&#27491;&#21017;&#21270;(SIR)&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20351;SCL&#36890;&#36807;&#23398;&#20064;&#30340;&#34920;&#31034;&#19981;&#21463;&#26679;&#24335;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36890;&#36807;ACL&#33719;&#24471;&#20855;&#26377;&#26679;&#24335;&#29420;&#31435;&#24615;&#36136;&#30340;&#40065;&#26834;&#34920;&#31034;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#25512;&#29702;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#19981;&#21464;&#27491;&#21017;&#21270;(AIR)&#65292;&#24378;&#21046;&#36890;&#36807;ACL&#23398;&#20064;&#21040;&#30340;&#40065;&#26834;&#34920;&#31034;&#20855;&#26377;&#26679;&#24335;&#29420;&#31435;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21464;&#27491;&#21017;&#21270;(IR)&#22686;&#24378;ACL&#65292;&#23427;&#26159;SIR&#21644;AIR&#30340;&#21152;&#26435;&#24635;&#21644;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;AIR&#36890;&#36807;&#38450;&#27490;&#27169;&#22411;&#20381;&#36182;&#26679;&#24335;&#22240;&#32032;&#26469;&#33719;&#24471;&#39640;&#23545;&#27604;&#20998;&#25968;&#65292;&#38544;&#24335;&#22320;&#20419;&#36827;&#20102;ACL&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#21644;&#24120;&#35265;&#27745;&#26579;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;ACL&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial contrastive learning (ACL), without requiring labels, incorporates adversarial data with standard contrastive learning (SCL) and outputs a robust representation which is generalizable and resistant to adversarial attacks and common corruptions. The style-independence property of representations has been validated to be beneficial in improving robustness transferability. Standard invariant regularization (SIR) has been proposed to make the learned representations via SCL to be independent of the style factors. However, how to equip robust representations learned via ACL with the style-independence property is still unclear so far. To this end, we leverage the technique of causal reasoning to propose an adversarial invariant regularization (AIR) that enforces robust representations learned via ACL to be style-independent. Then, we enhance ACL using invariant regularization (IR), which is a weighted sum of SIR and AIR. Theoretically, we show that AIR implicitly encourages the 
&lt;/p&gt;</description></item><item><title>ProGAP&#26159;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;GNN&#27169;&#22411;&#65292;&#37319;&#29992;&#36880;&#27493;&#35757;&#32451;&#26041;&#26696;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#36890;&#36807;&#23558;GNN&#20998;&#25104;&#19968;&#31995;&#21015;&#37325;&#21472;&#30340;&#23376;&#27169;&#22411;&#26469;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#20801;&#35768;&#26377;&#25928;&#23398;&#20064;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.08928</link><description>&lt;p&gt;
ProGAP: &#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#28176;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees. (arXiv:2304.08928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08928
&lt;/p&gt;
&lt;p&gt;
ProGAP&#26159;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;GNN&#27169;&#22411;&#65292;&#37319;&#29992;&#36880;&#27493;&#35757;&#32451;&#26041;&#26696;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#36890;&#36807;&#23558;GNN&#20998;&#25104;&#19968;&#31995;&#21015;&#37325;&#21472;&#30340;&#23376;&#27169;&#22411;&#26469;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#20801;&#35768;&#26377;&#25928;&#23398;&#20064;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#23398;&#20064;&#22270;&#24418;&#25968;&#25454;&#30340;&#24120;&#29992;&#24037;&#20855;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#22270;&#24418;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#20010;&#20154;&#25110;&#25935;&#24863;&#20449;&#24687;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#24182;&#20801;&#35768;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#22788;&#29702;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;GNN&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#22266;&#26377;&#32467;&#26500;&#36830;&#25509;&#24615;&#65292;GNN&#22312;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProGAP&#30340;&#26032;&#22411;&#24046;&#20998;&#38544;&#31169;GNN&#65292;&#37319;&#29992;&#36880;&#27493;&#35757;&#32451;&#26041;&#26696;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#32467;&#21512;&#32858;&#21512;&#25200;&#21160;&#25216;&#26415;&#20197;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#65292;ProGAP&#23558;GNN&#20998;&#25104;&#19968;&#31995;&#21015;&#37325;&#21472;&#30340;&#23376;&#27169;&#22411;&#65292;&#36880;&#27493;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#31532;&#19968;&#20010;&#23376;&#27169;&#22411;&#25193;&#23637;&#21040;&#23436;&#25972;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#23376;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#36890;&#36807;&#31169;&#26377;&#32858;&#21512;&#23398;&#20064;&#21644;&#32531;&#23384;&#30340;&#33410;&#28857;&#23884;&#20837;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become a popular tool for learning on graphs, but their widespread use raises privacy concerns as graph data can contain personal or sensitive information. Differentially private GNN models have been recently proposed to preserve privacy while still allowing for effective learning over graph-structured datasets. However, achieving an ideal balance between accuracy and privacy in GNNs remains challenging due to the intrinsic structural connectivity of graphs. In this paper, we propose a new differentially private GNN called ProGAP that uses a progressive training scheme to improve such accuracy-privacy trade-offs. Combined with the aggregation perturbation technique to ensure differential privacy, ProGAP splits a GNN into a sequence of overlapping submodels that are trained progressively, expanding from the first submodel to the complete model. Specifically, each submodel is trained over the privately aggregated node embeddings learned and cached by the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65288;TKS&#65289;&#65292;&#22312;&#19981;&#21516;&#26102;&#21051;&#20043;&#38388;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#36741;&#21161;&#30495;&#23454;&#26631;&#31614;&#24341;&#23548;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;SNN&#23545;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;TKS&#21487;&#20197;&#33719;&#24471;&#24403;&#21069;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06540</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#30693;&#35782;&#20849;&#20139;&#23454;&#29616;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23545;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Sharing enable Spiking Neural Network Learning from Past and Future. (arXiv:2304.06540v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65288;TKS&#65289;&#65292;&#22312;&#19981;&#21516;&#26102;&#21051;&#20043;&#38388;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#36741;&#21161;&#30495;&#23454;&#26631;&#31614;&#24341;&#23548;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;SNN&#23545;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;TKS&#21487;&#20197;&#33719;&#24471;&#24403;&#21069;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22240;&#20854;&#31867;&#20284;&#20110;&#22823;&#33041;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#32780;&#21560;&#24341;&#20102;&#35768;&#22810;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26367;&#20195;&#26799;&#24230;&#30340;&#25552;&#20986;&#20351;&#24471;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36801;&#31227;&#21040;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#24182;&#36880;&#27493;&#32553;&#23567;&#19982;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#30446;&#21069;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#25152;&#26377;&#26102;&#21051;&#30340;&#36755;&#20986;&#26469;&#20135;&#29983;&#26368;&#32456;&#30340;&#39044;&#27979;&#65292;&#36825;&#29306;&#29298;&#20102;&#23427;&#20204;&#30340;&#26102;&#38388;&#29305;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#25928;&#29575;&#38477;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65288;TKS&#65289;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#26102;&#21051;&#30340;&#36755;&#20986;&#26469;&#26500;&#25104;&#25945;&#24072;&#20449;&#21495;&#65292;&#20351;&#24471;&#20449;&#24687;&#21487;&#20197;&#22312;&#19981;&#21516;&#26102;&#21051;&#20043;&#38388;&#20132;&#20114;&#65292;&#36741;&#21161;&#30495;&#23454;&#26631;&#31614;&#24341;&#23548;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;CIFAR10&#12289;CIFAR100&#12289;ImageNet-1k&#20197;&#21450;&#31070;&#32463;&#24418;&#24577;&#23398;&#25968;&#25454;&#38598;DVS-CIFAR10&#12289;NCALTECH101&#19978;&#39564;&#35777;&#20102;TKS&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;TKS&#65292;&#25105;&#20204;&#24050;&#32463;&#22312;CIFAR10&#21644;CIFAR100&#19978;&#21462;&#24471;&#20102;&#24403;&#21069;&#26368;&#20248;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;TKS&#30340;NCALTECH101&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks have attracted extensive attention from researchers in many fields due to their brain-like information processing mechanism. The proposal of surrogate gradient enables the spiking neural networks to migrate to more complex tasks, and gradually close the gap with the conventional artificial neural networks. Current spiking neural networks utilize the output of all moments to produce the final prediction, which compromises their temporal characteristics and causes a reduction in performance and efficiency. We propose a temporal knowledge sharing approach (TKS) that enables the interaction of information between different moments, by selecting the output of specific moments to compose teacher signals to guide the training of the network along with the real labels. We have validated TKS on both static datasets CIFAR10, CIFAR100, ImageNet-1k and neuromorphic datasets DVS-CIFAR10, NCALTECH101. Our experimental results indicate that we have achieved the current optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#32508;&#21512;&#24515;&#30005;&#22270;&#35299;&#35835;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24515;&#30005;&#22270;&#35299;&#35835;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#23567;&#25968;&#25454;&#38598;&#12289;&#19981;&#19968;&#33268;&#30340;&#25968;&#25454;&#26631;&#27880;&#12289;&#20302;&#25928;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#24687;&#12289;&#22810;&#27169;&#22411;&#37096;&#32626;&#30340;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#28040;&#32791;&#65292;&#20197;&#21450;&#20219;&#21153;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#36882;&#32570;&#20047;&#12290;</title><link>http://arxiv.org/abs/2304.04646</link><description>&lt;p&gt;
ECG-CL&#65306;&#22522;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#32508;&#21512;&#24515;&#30005;&#22270;&#35299;&#35835;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ECG-CL: A Comprehensive Electrocardiogram Interpretation Method Based on Continual Learning. (arXiv:2304.04646v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#32508;&#21512;&#24515;&#30005;&#22270;&#35299;&#35835;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24515;&#30005;&#22270;&#35299;&#35835;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#23567;&#25968;&#25454;&#38598;&#12289;&#19981;&#19968;&#33268;&#30340;&#25968;&#25454;&#26631;&#27880;&#12289;&#20302;&#25928;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#24687;&#12289;&#22810;&#27169;&#22411;&#37096;&#32626;&#30340;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#28040;&#32791;&#65292;&#20197;&#21450;&#20219;&#21153;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#36882;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#30417;&#27979;&#26159;&#24515;&#34880;&#31649;&#30142;&#30149;&#65288;CVD&#65289;&#26089;&#26399;&#35782;&#21035;&#26368;&#24378;&#22823;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#26234;&#33021;&#21487;&#31359;&#25140;&#24515;&#30005;&#22270;&#35774;&#22791;&#30340;&#24341;&#20837;&#20351;&#24471;&#26085;&#24120;&#30417;&#27979;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24515;&#30005;&#22270;&#35299;&#35835;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#19968;&#33324;&#20844;&#20247;&#25509;&#35302;&#21463;&#38480;&#65292;&#20419;&#20351;&#20102;&#23545;&#20808;&#36827;&#35786;&#26029;&#31639;&#27861;&#30340;&#24320;&#21457;&#38656;&#27714;&#12290;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#29616;&#22312;&#24050;&#34987;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23436;&#20840;&#36229;&#36234;&#12290;&#20294;&#26159;&#65292;&#26234;&#33021;&#35786;&#26029;&#31639;&#27861;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#30340;&#21046;&#32422;&#65292;&#22914;&#23567;&#25968;&#25454;&#38598;&#12289;&#19981;&#19968;&#33268;&#30340;&#25968;&#25454;&#26631;&#27880;&#12289;&#23545;&#23616;&#37096;&#21644;&#20840;&#23616;&#24515;&#30005;&#22270;&#20449;&#24687;&#30340;&#20302;&#25928;&#21033;&#29992;&#12289;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#28040;&#32791;&#30340;&#22810;&#27169;&#22411;&#37096;&#32626;&#65292;&#20197;&#21450;&#20219;&#21153;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#36882;&#32570;&#20047;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#24320;&#21457;&#20302;&#20998;&#36776;&#29575;&#39640;&#32423;&#35821;&#20041;&#20449;&#24687;&#26469;&#20445;&#25345;&#39640;&#20998;&#36776;&#29575;&#20302;&#32423;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) monitoring is one of the most powerful technique of cardiovascular disease (CVD) early identification, and the introduction of intelligent wearable ECG devices has enabled daily monitoring. However, due to the need for professional expertise in the ECGs interpretation, general public access has once again been restricted, prompting the need for the development of advanced diagnostic algorithms. Classic rule-based algorithms are now completely outperformed by deep learning based methods. But the advancement of smart diagnostic algorithms is hampered by issues like small dataset, inconsistent data labeling, inefficient use of local and global ECG information, memory and inference time consuming deployment of multiple models, and lack of information transfer between tasks. We propose a multi-resolution model that can sustain high-resolution low-level semantic information throughout, with the help of the development of low-resolution high-level semantic information,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2304.03398</link><description>&lt;p&gt;
&#37327;&#23376;&#30456;&#23481;&#39044;&#27979;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning. (arXiv:2304.03398v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#22312;&#24403;&#21069;&#30340;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;(NISQ)&#35745;&#31639;&#26426;&#26102;&#20195;&#20013;&#20248;&#21270;&#37327;&#23376;&#31639;&#27861;&#30340;&#26377;&#21069;&#36884;&#30340;&#32534;&#31243;&#33539;&#24335;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#27867;&#21270;&#24615;&#33021;&#65292;&#22240;&#20026;&#35774;&#35745;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#27867;&#21270;&#20998;&#26512;&#34429;&#28982;&#33021;&#22815;&#35782;&#21035;&#37325;&#35201;&#30340;&#19968;&#33324;&#36235;&#21183;&#21644;&#35268;&#27169;&#23450;&#24459;&#65292;&#20294;&#19981;&#33021;&#29992;&#20110;&#20026;&#37327;&#23376;&#27169;&#22411;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#20998;&#37197;&#21487;&#38752;&#21644;&#26377;&#20449;&#24687;&#37327;&#30340;&#8220;&#35823;&#24046;&#26465;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#65292;&#22312;&#27010;&#29575;&#24615;&#30456;&#23481;&#39044;&#27979;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#37327;&#23376;&#27169;&#22411;&#30340;&#20219;&#24847;&#21487;&#33021;&#23567;&#30340;&#25293;&#25668;&#27425;&#25968;&#36716;&#25442;&#20026;&#19968;&#32452;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is a promising programming paradigm for the optimization of quantum algorithms in the current era of noisy intermediate scale quantum (NISQ) computers. A fundamental challenge in quantum machine learning is generalization, as the designer targets performance under testing conditions, while having access only to limited training data. Existing generalization analyses, while identifying important general trends and scaling laws, cannot be used to assign reliable and informative "error bars" to the decisions made by quantum models. In this article, we propose a general methodology that can reliably quantify the uncertainty of quantum models, irrespective of the amount of training data, of the number of shots, of the ansatz, of the training algorithm, and of the presence of quantum hardware noise. The approach, which builds on probabilistic conformal prediction, turns an arbitrary, possibly small, number of shots from a pre-trained quantum model into a set predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#12289;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#21644;&#26657;&#20934;&#39044;&#27979;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#32463;&#23454;&#39564;&#34920;&#26126;&#22312;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#23545;&#20110;&#22122;&#22768;&#26679;&#26412;&#20855;&#26377;&#33391;&#22909;&#25269;&#25239;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#30340;&#26816;&#27979;&#26159;&#21487;&#38752;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01518</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neural Processes for Uncertainty Estimation. (arXiv:2304.01518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#12289;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#21644;&#26657;&#20934;&#39044;&#27979;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#32463;&#23454;&#39564;&#34920;&#26126;&#22312;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#23545;&#20110;&#22122;&#22768;&#26679;&#26412;&#20855;&#26377;&#33391;&#22909;&#25269;&#25239;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#30340;&#26816;&#27979;&#26159;&#21487;&#38752;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;( Neural Processes, NPs)&#23558;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#38750;&#21442;&#25968;&#39640;&#26031;&#36807;&#31243;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#32467;&#21512;&#22312;&#20102;&#19968;&#36215;&#12290;&#34429;&#28982;&#26368;&#36817;NPs&#30340;&#21457;&#23637;&#24050;&#32463;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22914;&#20309;&#23558;NPs&#36866;&#24212;&#22810;&#27169;&#24577;&#25968;&#25454;&#23578;&#26410;&#21463;&#21040;&#20180;&#32454;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NP&#23478;&#26063;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#25972;&#20307;&#30340;&#12289;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#20998;&#31867;&#35823;&#24046;&#26356;&#26032;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#65292;&#19968;&#20010;&#32858;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#26426;&#21046;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#30340;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#21560;&#24341;&#21147;&#65292;&#21363;&#33021;&#22815;&#25269;&#25239;&#22122;&#22768;&#26679;&#26412;&#30340;&#24178;&#25200;&#65292;&#24182;&#21487;&#38752;&#22320;&#22312;&#39046;&#22495;&#20043;&#22806;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural processes (NPs) have brought the representation power of parametric deep neural networks and the reliable uncertainty estimation of non-parametric Gaussian processes together. Although recent development of NPs has shown success in both regression and classification, how to adapt NPs to multimodal data has not be carefully studied. For the first time, we propose a new model of NP family for multimodal uncertainty estimation, namely Multimodal Neural Processes. In a holistic and principled way, we develop a dynamic context memory updated by the classification error, a multimodal Bayesian aggregation mechanism to aggregate multimodal representations, and a new attention mechanism for calibrated predictions. In extensive empirical evaluation, our method achieves the state-of-the-art multimodal uncertainty estimation performance, showing its appealing ability of being robust against noisy samples and reliable in out-of-domain detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#25299;&#25169;&#23545;&#20854;&#36817;&#20284;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#39640;&#38590;&#24230;&#24773;&#20917;&#19979;&#65292;&#22797;&#26434;&#25299;&#25169;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#20294;&#20195;&#20215;&#26159;&#21069;&#21521;&#20256;&#25773;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#21644;&#22270;&#24418;&#25439;&#20260;&#30340;&#40065;&#26834;&#24615;&#30340;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2303.17925</link><description>&lt;p&gt;
&#36229;&#36234;&#22810;&#23618;&#24863;&#30693;&#22120;&#65306;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Beyond Multilayer Perceptrons: Investigating Complex Topologies in Neural Networks. (arXiv:2303.17925v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#25299;&#25169;&#23545;&#20854;&#36817;&#20284;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#39640;&#38590;&#24230;&#24773;&#20917;&#19979;&#65292;&#22797;&#26434;&#25299;&#25169;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#20294;&#20195;&#20215;&#26159;&#21069;&#21521;&#20256;&#25773;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#21644;&#22270;&#24418;&#25439;&#20260;&#30340;&#40065;&#26834;&#24615;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#25299;&#25169;&#23545;&#20854;&#36817;&#20284;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22797;&#26434;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#26500;&#24314;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;Barabasi-Albert&#65292;Erdos-Renyi&#65292;Watts-Strogatz&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#27969;&#24418;&#23398;&#20064;&#29983;&#25104;&#22120;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#26500;&#24314;&#30340;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;&#38590;&#24230;&#21644;&#22122;&#22768;&#27700;&#24179;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;MLP&#30456;&#27604;&#65292;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#22312;&#39640;&#38590;&#24230;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;&#36825;&#31181;&#24615;&#33021;&#20248;&#21183;&#24402;&#22240;&#20110;&#22797;&#26434;&#32593;&#32476;&#21033;&#29992;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#22797;&#21512;&#24615;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22909;&#22788;&#26159;&#20197;&#21069;&#21521;&#20256;&#25773;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#21644;&#22270;&#24418;&#25439;&#20260;&#30340;&#40065;&#26834;&#24615;&#30340;&#38477;&#20302;&#20026;&#20195;&#20215;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21508;&#31181;&#25299;&#25169;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the impact of network topology on the approximation capabilities of artificial neural networks (ANNs), with a particular focus on complex topologies. We propose a novel methodology for constructing complex ANNs based on various topologies, including Barab\'asi-Albert, Erd\H{o}s-R\'enyi, Watts-Strogatz, and multilayer perceptrons (MLPs). The constructed networks are evaluated on synthetic datasets generated from manifold learning generators, with varying levels of task difficulty and noise. Our findings reveal that complex topologies lead to superior performance in high-difficulty regimes compared to traditional MLPs. This performance advantage is attributed to the ability of complex networks to exploit the compositionality of the underlying target function. However, this benefit comes at the cost of increased forward-pass computation time and reduced robustness to graph damage. Additionally, we investigate the relationship between various topological attribute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#23545;&#21333;&#20010;&#35266;&#27979;&#32467;&#26524;&#26377;&#20581;&#22766;&#24615;</title><link>http://arxiv.org/abs/2303.15845</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21487;&#35777;&#26126;&#20855;&#26377;&#20581;&#22766;&#24615;:&#38134;&#28246;&#21453;&#38382;&#39064;&#30340;&#36880;&#28857;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems. (arXiv:2303.15845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#23545;&#21333;&#20010;&#35266;&#27979;&#32467;&#26524;&#26377;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#25104;&#20026;&#37319;&#26679;&#38134;&#28246;&#21453;&#38382;&#39064;&#21518;&#39564;&#27010;&#29575;&#30340;&#24378;&#22823;&#24037;&#20855;. &#32463;&#20856;&#30340;&#36125;&#21494;&#26031;&#25991;&#29486;&#24050;&#32463;&#30693;&#36947;&#21518;&#39564;&#27979;&#24230;&#23545;&#20808;&#21069;&#27979;&#24230;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;(&#21253;&#25324;&#35266;&#23519;&#30340;&#25200;&#21160;)&#38750;&#24120; robust. &#20294;&#26159;, &#23601;&#25105;&#20204;&#25152;&#30693;, &#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#36824;&#27809;&#34987;&#30740;&#31350;&#36807;. &#22312;&#26412;&#25991;&#20013;, &#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36866;&#24403;&#23398;&#20064;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20010;&#35266;&#27979;&#20540;&#26041;&#38754;&#25552;&#20379;&#20102;&#20581;&#22766;&#30340;&#32467;&#26524;.
&lt;/p&gt;
&lt;p&gt;
Conditional generative models became a very powerful tool to sample from Bayesian inverse problem posteriors. It is well-known in classical Bayesian literature that posterior measures are quite robust with respect to perturbations of both the prior measure and the negative log-likelihood, which includes perturbations of the observations. However, to the best of our knowledge, the robustness of conditional generative models with respect to perturbations of the observations has not been investigated yet. In this paper, we prove for the first time that appropriately learned conditional generative models provide robust results for single observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29305;&#24449;&#21487;&#20998;&#24615;&#23545;&#20110;&#27169;&#22411;&#22312;&#20998;&#24067;&#31227;&#20301;&#19979;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31163;&#25955;&#24230;&#30340;&#20998;&#25968;&#29992;&#20110;&#20272;&#35745;&#27979;&#35797;&#20934;&#30830;&#24230;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15488</link><description>&lt;p&gt;
&#35770;&#29305;&#24449;&#21487;&#20998;&#24615;&#22312;&#39044;&#27979;&#20998;&#24067;&#22806;&#35823;&#24046;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Feature Separability in Predicting Out-Of-Distribution Error. (arXiv:2303.15488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29305;&#24449;&#21487;&#20998;&#24615;&#23545;&#20110;&#27169;&#22411;&#22312;&#20998;&#24067;&#31227;&#20301;&#19979;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31163;&#25955;&#24230;&#30340;&#20998;&#25968;&#29992;&#20110;&#20272;&#35745;&#27979;&#35797;&#20934;&#30830;&#24230;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#26631;&#31614;&#30340;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#20272;&#35745;&#23454;&#38469;&#19978;&#24456;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#26041;&#27861;&#24378;&#35843;&#20998;&#24067;&#24046;&#24322;&#19982;&#20998;&#24067;&#22806;&#31934;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22823;&#30340;&#22495;&#38388;&#24046;&#24322;&#24182;&#19981;&#19968;&#23450;&#23548;&#33268;&#20302;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#26412;&#25991;&#20174;&#29305;&#24449;&#21487;&#20998;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31163;&#25955;&#24230;&#30340;&#25968;&#25454;&#38598;&#32423;&#21035;&#30340;&#20998;&#25968;&#65292;&#20197;&#20272;&#35745;&#22312;&#20998;&#24067;&#31227;&#20301;&#19979;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21463;&#34920;&#24449;&#23398;&#20064;&#20013;&#29305;&#24449;&#33391;&#22909;&#23646;&#24615;&#30340;&#21551;&#31034;&#65306;&#39640;&#20869;&#31867;&#31163;&#25955;&#24230;&#21644;&#39640;&#20869;&#31867;&#32039;&#33268;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20869;&#31867;&#31163;&#25955;&#24230;&#19982;&#27169;&#22411;&#20934;&#30830;&#24230;&#24378;&#30456;&#20851;&#65292;&#32780;&#20869;&#31867;&#32039;&#33268;&#24230;&#19981;&#21453;&#26144;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability, and propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12314</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12314
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#36719;&#25552;&#31034;&#24182;&#20351;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#19968;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#33391;&#22909;&#30340;&#36719;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#30417;&#30563;&#20803;&#23398;&#20064;&#26469;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#23545;&#26410;&#35265;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65288;SUPMER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#32452;&#33258;&#30417;&#30563;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20219;&#21153;&#26684;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#21518;&#23558;&#19968;&#31181;&#26032;&#30340;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#38598;&#25104;&#21040;&#20803;&#25552;&#31034;&#23398;&#20064;&#20013;&#12290;&#23427;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#22914;&#20309;&#36716;&#25442;&#21407;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#26469;&#36817;&#20284;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;&#20223;&#30495;&#65292;&#25552;&#39640;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.10256</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#22312;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#20013;&#27714;&#35299;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving Differential-Algebraic Equations in Power Systems Dynamics with Neural Networks and Spatial Decomposition. (arXiv:2303.10256v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#26469;&#36817;&#20284;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;&#20223;&#30495;&#65292;&#25552;&#39640;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30001;&#19968;&#32452;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#25551;&#36848;&#12290;&#26102;&#38388;&#22495;&#20223;&#30495;&#29992;&#20110;&#29702;&#35299;&#31995;&#32479;&#21160;&#24577;&#30340;&#28436;&#21464;&#12290;&#30001;&#20110;&#31995;&#32479;&#30340;&#21018;&#24230;&#38656;&#35201;&#20351;&#29992;&#31934;&#32454;&#31163;&#25955;&#21270;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#22240;&#27492;&#36825;&#20123;&#20223;&#30495;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#20195;&#20215;&#36739;&#39640;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#22686;&#21152;&#20801;&#35768;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#26088;&#22312;&#21152;&#24555;&#36825;&#26679;&#30340;&#20223;&#30495;&#12290;&#26412;&#25991;&#20351;&#29992;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#23613;&#31649;&#21508;&#20010;&#32452;&#20214;&#20351;&#29992;&#20195;&#25968;&#21644;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#65292;&#20294;&#23427;&#20204;&#30340;&#32806;&#21512;&#20165;&#28041;&#21450;&#20195;&#25968;&#26041;&#31243;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#36817;&#20284;&#32452;&#20214;&#29366;&#24577;&#28436;&#21464;&#65292;&#20174;&#32780;&#20135;&#29983;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#25968;&#20540;&#31283;&#23450;&#30340;&#36817;&#20284;&#22120;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#20026;&#20102;&#35299;&#37322;&#32593;&#32476;&#23545;&#32452;&#20214;&#20197;&#21450;&#32452;&#20214;&#23545;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;NN&#23558;&#32806;&#21512;&#20195;&#25968;&#21464;&#37327;&#30340;&#26102;&#38388;&#28436;&#21270;&#20316;&#20026;&#20854;&#39044;&#27979;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#26368;&#21021;&#20351;&#29992;&#31354;&#38388;&#20998;&#35299;&#26041;&#27861;&#26469;&#20272;&#35745;NN&#65292;&#20854;&#20013;&#31995;&#32479;&#34987;&#20998;&#25104;&#31354;&#38388;&#21306;&#22495;&#65292;&#27599;&#20010;&#21306;&#22495;&#26377;&#21333;&#29420;&#30340;NN&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;NN&#30340;&#20223;&#30495;&#19982;&#20256;&#32479;&#30340;&#25968;&#20540;&#31215;&#20998;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of the power system are described by a system of differential-algebraic equations. Time-domain simulations are used to understand the evolution of the system dynamics. These simulations can be computationally expensive due to the stiffness of the system which requires the use of finely discretized time-steps. By increasing the allowable time-step size, we aim to accelerate such simulations. In this paper, we use the observation that even though the individual components are described using both algebraic and differential equations, their coupling only involves algebraic equations. Following this observation, we use Neural Networks (NNs) to approximate the components' state evolution, leading to fast, accurate, and numerically stable approximators, which enable larger time-steps. To account for effects of the network on the components and vice-versa, the NNs take the temporal evolution of the coupling algebraic variables as an input for their prediction. We initially estima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06470</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20998;&#23376;&#36136;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prefix-tree Decoding for Predicting Mass Spectra from Molecules. (arXiv:2303.06470v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms, and decoding the formula set using a prefix tree structure, atom-type by atom-type, overcoming the combinatorial possibilities for chemical subformulae.
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#24050;&#32463;&#23454;&#29616;&#20102;&#20020;&#24202;&#30456;&#20851;&#20195;&#35874;&#29289;&#30340;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#39044;&#27979;&#24037;&#20855;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21344;&#25454;&#20102;&#20004;&#20010;&#26497;&#31471;&#65292;&#35201;&#20040;&#36890;&#36807;&#36807;&#24230;&#21018;&#24615;&#30340;&#32422;&#26463;&#21644;&#36739;&#24046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#32452;&#21512;&#20998;&#23376;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#35201;&#20040;&#36890;&#36807;&#35299;&#30721;&#26377;&#25439;&#21644;&#38750;&#29289;&#29702;&#31163;&#25955;&#21270;&#30340;&#20809;&#35889;&#21521;&#37327;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#36825;&#20123;&#21270;&#23398;&#20844;&#24335;&#26412;&#36523;&#26159;&#21407;&#23376;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;&#22312;&#39318;&#20808;&#23545;&#36755;&#20837;&#20998;&#23376;&#22270;&#36827;&#34892;&#32534;&#30721;&#21518;&#65292;&#25105;&#20204;&#35299;&#30721;&#19968;&#32452;&#21270;&#23398;&#23376;&#20844;&#24335;&#65292;&#27599;&#20010;&#21270;&#23398;&#23376;&#20844;&#24335;&#25351;&#23450;&#36136;&#35889;&#20013;&#30340;&#19968;&#20010;&#39044;&#27979;&#23792;&#65292;&#20854;&#24378;&#24230;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#20351;&#29992;&#21069;&#32512;&#26641;&#32467;&#26500;&#65292;&#36880;&#20010;&#21407;&#23376;&#31867;&#22411;&#22320;&#35299;&#30721;&#20844;&#24335;&#38598;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we introduce a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of chemical subformulae, each of which specify a predicted peak in the mass spectra, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for chemical subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Self-Supervised&#23545;&#27604;&#32858;&#31867;&#31639;&#27861;&#30340;&#33258;&#21160;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;StyleGAN&#29983;&#25104;&#22120;&#20013;&#30340;&#22810;&#23610;&#24230;&#38544;&#34255;&#29305;&#24449;&#23545;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20998;&#21106;&#65292;&#20248;&#20110;&#21322;&#30417;&#30563;&#22522;&#20934;&#26041;&#27861;&#30340;&#24179;&#22343;wIoU&#20540;1.02%&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#36798;4.5&#20493;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#19968;&#27425;&#23398;&#20064;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20854;&#20182;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.05639</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;StyleGAN&#22270;&#20687;&#33258;&#21160;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised One-Shot Learning for Automatic Segmentation of StyleGAN Images. (arXiv:2303.05639v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Self-Supervised&#23545;&#27604;&#32858;&#31867;&#31639;&#27861;&#30340;&#33258;&#21160;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;StyleGAN&#29983;&#25104;&#22120;&#20013;&#30340;&#22810;&#23610;&#24230;&#38544;&#34255;&#29305;&#24449;&#23545;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20998;&#21106;&#65292;&#20248;&#20110;&#21322;&#30417;&#30563;&#22522;&#20934;&#26041;&#27861;&#30340;&#24179;&#22343;wIoU&#20540;1.02%&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#36798;4.5&#20493;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#19968;&#27425;&#23398;&#20064;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20854;&#20182;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Self-Supervised&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#21106;&#29983;&#25104;&#30340;StyleGAN&#22270;&#20687;&#12290;&#22312;GAN&#29983;&#25104;&#22120;&#30340;&#22810;&#23610;&#24230;&#38544;&#34255;&#29305;&#24449;&#20013;&#65292;&#21547;&#26377;&#26377;&#29992;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#23454;&#29616;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#33258;&#21160;&#20998;&#21106;&#12290;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#32858;&#31867;&#31639;&#27861;&#26469;&#23398;&#20064;&#20998;&#21106;&#21512;&#25104;&#22270;&#20687;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23558;&#38544;&#34255;&#29305;&#24449;&#25237;&#24433;&#21040;&#32039;&#20945;&#31354;&#38388;&#36827;&#34892;&#20687;&#32032;&#20998;&#31867;&#12290;&#36825;&#31181;&#26032;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#22120;&#26159;&#22522;&#20110;&#20351;&#29992;&#20687;&#32032;&#20132;&#25442;&#39044;&#27979;&#25439;&#22833;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#24555;&#22320;&#23398;&#20064;&#29992;&#20110;&#19968;&#27425;&#20998;&#21106;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#65292;&#24471;&#20986;&#30340;&#20998;&#21106;&#24615;&#33021;&#19981;&#20165;&#27604;&#21322;&#30417;&#30563;&#22522;&#20934;&#26041;&#27861;&#24179;&#22343;wIoU&#39640;&#20986;1.02&#65285;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;4.5&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#19968;&#27425;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25512;&#24191;&#21040;&#24191;&#27867;&#30340;&#20854;&#20182;StyleGAN&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for the automatic one-shot segmentation of synthetic images generated by a StyleGAN. Our framework is based on the observation that the multi-scale hidden features in the GAN generator hold useful semantic information that can be utilized for automatic on-the-fly segmentation of the generated images. Using these features, our framework learns to segment synthetic images using a self-supervised contrastive clustering algorithm that projects the hidden features into a compact space for per-pixel classification. This novel contrastive learner is based on using a pixel-wise swapped prediction loss for image segmentation that leads to faster learning of the feature vectors for one-shot segmentation. We have tested our implementation on a number of standard benchmarks to yield a segmentation performance that not only outperforms the semi-supervised baseline methods by an average wIoU margin of 1.02% but also improves the inference speeds by a factor of 4.5. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#20844;&#24179;&#27491;&#21017;&#21270;&#31639;&#27861;&#65288;RFR&#65289;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#25200;&#21160;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RFR&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03300</link><description>&lt;p&gt;
&#36861;&#36880;&#20844;&#24179;&#24615;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#65306;&#19968;&#31181;&#27169;&#22411;&#26435;&#37325;&#25200;&#21160;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chasing Fairness Under Distribution Shift: A Model Weight Perturbation Approach. (arXiv:2303.03300v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#20844;&#24179;&#27491;&#21017;&#21270;&#31639;&#27861;&#65288;RFR&#65289;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#25200;&#21160;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RFR&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#38024;&#23545;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#20844;&#24179;&#26041;&#27861;&#21487;&#33021;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#25991;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20998;&#24067;&#36716;&#31227;&#12289;&#25968;&#25454;&#25200;&#21160;&#21644;&#27169;&#22411;&#26435;&#37325;&#25200;&#21160;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30830;&#20445;&#30446;&#26631;&#25968;&#25454;&#38598;&#20844;&#24179;&#24615;&#65288;&#21363;&#20302;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#65289;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#21253;&#25324;&#28304;&#25968;&#25454;&#38598;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#21450;&#27599;&#20010;&#25935;&#24863;&#23646;&#24615;&#32452;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20302;&#39044;&#27979;&#24046;&#24322;&#12290;&#21463;&#21040;&#36825;&#20123;&#20805;&#20998;&#26465;&#20214;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#20844;&#24179;&#27491;&#21017;&#21270;&#65288;RFR&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#25935;&#24863;&#23646;&#24615;&#32452;&#22312;&#27169;&#22411;&#26435;&#37325;&#25200;&#21160;&#29699;&#20869;&#30340;&#26368;&#22351;&#24773;&#20917;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#20998;&#24067;&#36716;&#31227;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;RFR&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RFR&#31639;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#24067;&#36716;&#31227;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in machine learning has attracted increasing attention in recent years. The fairness methods improving algorithmic fairness for in-distribution data may not perform well under distribution shifts. In this paper, we first theoretically demonstrate the inherent connection between distribution shift, data perturbation, and model weight perturbation. Subsequently, we analyze the sufficient conditions to guarantee fairness (i.e., low demographic parity) for the target dataset, including fairness for the source dataset, and low prediction difference between the source and target datasets for each sensitive attribute group. Motivated by these sufficient conditions, we propose robust fairness regularization (RFR) by considering the worst case within the model weight perturbation ball for each sensitive attribute group. We evaluate the effectiveness of our proposed RFR algorithm on synthetic and real distribution shifts across various datasets. Experimental results demonstrate that RFR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#28145;&#24230;&#39640;&#26031;&#27169;&#22411;&#30340;&#29305;&#24449;&#21508;&#21521;&#24322;&#24615;&#23637;&#24320;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31532;&#19968;&#23618;&#29305;&#24449;&#34892;&#20043;&#38388;&#20801;&#35768;&#23384;&#22312;&#30456;&#20851;&#24615;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#65292;&#32780;&#21518;&#32493;&#23618;&#30340;&#32467;&#26500;&#36890;&#24120;&#26159;&#19981;&#21033;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.00564</link><description>&lt;p&gt;
&#28145;&#24230;&#32467;&#26500;&#39640;&#26031;&#29305;&#24449;&#27169;&#22411;&#30340;&#23398;&#20064;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Learning curves for deep structured Gaussian feature models. (arXiv:2303.00564v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00564
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#28145;&#24230;&#39640;&#26031;&#27169;&#22411;&#30340;&#29305;&#24449;&#21508;&#21521;&#24322;&#24615;&#23637;&#24320;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31532;&#19968;&#23618;&#29305;&#24449;&#34892;&#20043;&#38388;&#20801;&#35768;&#23384;&#22312;&#30456;&#20851;&#24615;&#21487;&#20197;&#20419;&#36827;&#27867;&#21270;&#65292;&#32780;&#21518;&#32493;&#23618;&#30340;&#32467;&#26500;&#36890;&#24120;&#26159;&#19981;&#21033;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#23545;&#20110;&#22810;&#23618;&#39640;&#26031;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20998;&#26512;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#29305;&#24449;&#21508;&#21521;&#24322;&#24615;&#30340;&#24433;&#21709;&#65307;&#22823;&#22810;&#25968;&#27169;&#22411;&#37117;&#20551;&#35774;&#29305;&#24449;&#26159;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#26435;&#37325;&#29983;&#25104;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#20855;&#26377;&#35768;&#22810;&#23618;&#32467;&#26500;&#39640;&#26031;&#29305;&#24449;&#30340;&#27169;&#22411;&#23548;&#20986;&#20102;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20801;&#35768;&#31532;&#19968;&#23618;&#29305;&#24449;&#30340;&#34892;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#21487;&#20419;&#36827;&#27867;&#21270;&#65292;&#32780;&#21518;&#32493;&#23618;&#30340;&#32467;&#26500;&#36890;&#24120;&#26159;&#19981;&#21033;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26435;&#37325;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#21487;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant attention in deep learning theory has been devoted to analyzing the generalization performance of models with multiple layers of Gaussian random features. However, few works have considered the effect of feature anisotropy; most assume that features are generated using independent and identically distributed Gaussian weights. Here, we derive learning curves for models with many layers of structured Gaussian features. We show that allowing correlations between the rows of the first layer of features can aid generalization, while structure in later layers is generally detrimental. Our results shed light on how weight structure affects generalization in a simple class of solvable models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#30340;&#22270;&#26500;&#36896;&#21644;&#21021;&#22987;&#21270;&#26041;&#24335;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12001</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Random Projection Forest Initialization for Graph Convolutional Networks. (arXiv:2302.12001v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#30340;&#22270;&#26500;&#36896;&#21644;&#21021;&#22987;&#21270;&#26041;&#24335;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#23558;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#26080;&#32467;&#26500;&#25968;&#25454;&#65288;&#22914;&#22270;&#65289;&#30340;&#19968;&#22823;&#27493;&#12290;&#20294;GCNs&#20173;&#38656;&#35201;&#26500;&#36896;&#22270;&#26469;&#36827;&#34892;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#32463;&#20856;&#22270;&#65288;&#22914;k&#36817;&#37051;&#22270;&#65289;&#26469;&#21021;&#22987;&#21270;GCN&#12290;&#23613;&#31649;&#26500;&#36896;k&#36817;&#37051;&#22270;&#30340;&#35745;&#31639;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#26500;&#36896;&#30340;&#22270;&#23545;&#20110;&#23398;&#20064;&#21487;&#33021;&#27809;&#26377;&#22826;&#22823;&#30340;&#29992;&#22788;&#12290;&#22312;k&#36817;&#37051;&#22270;&#20013;&#65292;&#28857;&#34987;&#38480;&#21046;&#20026;&#20855;&#26377;&#22266;&#23450;&#25968;&#37327;&#30340;&#36793;&#65292;&#22270;&#20013;&#30340;&#25152;&#26377;&#36793;&#37117;&#20855;&#26377;&#30456;&#31561;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#26500;&#24314;&#22270;&#24182;&#21021;&#22987;&#21270;GCN&#12290;&#23427;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#12290;rpForest&#20351;&#25105;&#20204;&#33021;&#22815;&#36171;&#20104;&#36793;&#19981;&#21516;&#30340;&#26435;&#37325;&#65292;&#34920;&#31034;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#12290;&#26641;&#30340;&#25968;&#37327;&#26159;rpForest&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35889;&#20998;&#26512;&#26469;&#24110;&#21161;&#25105;&#20204;&#35774;&#32622;&#27491;&#30830;&#33539;&#22260;&#30340;&#21442;&#25968;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;GCN&#30456;&#27604;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) were a great step towards extending deep learning to unstructured data such as graphs. But GCNs still need a constructed graph to work with. To solve this problem, classical graphs such as $k$-nearest neighbor are usually used to initialize the GCN. Although it is computationally efficient to construct $k$-nn graphs, the constructed graph might not be very useful for learning. In a $k$-nn graph, points are restricted to have a fixed number of edges, and all edges in the graph have equal weights. We present a new way to construct the graph and initialize the GCN. It is based on random projection forest (rpForest). rpForest enables us to assign varying weights on edges indicating varying importance, which enhanced the learning. The number of trees is a hyperparameter in rpForest. We performed spectral analysis to help us setting this parameter in the right range. In the experiments, initializing the GCN using rpForest provides better results compared t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20943;&#23569;&#38454;&#25968;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#22495;&#19978;&#39640;&#25928;&#39044;&#27979;&#34880;&#27969;&#27169;&#25311;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32452;&#34920;&#38754;&#37197;&#20934;&#23545;&#36825;&#20123;&#24418;&#29366;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#30340;&#24494;&#20998;&#21516;&#32986;&#23558;&#34880;&#28082;&#21160;&#21147;&#23398;&#20449;&#24687;&#24418;&#25104;&#24102;&#26377;&#20960;&#20309;&#20449;&#24687;&#30340;&#24555;&#29031;&#12290;</title><link>http://arxiv.org/abs/2302.11006</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20943;&#23569;&#38454;&#25968;&#24314;&#27169;&#26041;&#27861;&#29992;&#20110;&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#24555;&#29031;&#30340;&#34880;&#27969;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Data-driven reduced-order modelling for blood flow simulations with geometry-informed snapshots. (arXiv:2302.11006v3 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20943;&#23569;&#38454;&#25968;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#22495;&#19978;&#39640;&#25928;&#39044;&#27979;&#34880;&#27969;&#27169;&#25311;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32452;&#34920;&#38754;&#37197;&#20934;&#23545;&#36825;&#20123;&#24418;&#29366;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#30340;&#24494;&#20998;&#21516;&#32986;&#23558;&#34880;&#28082;&#21160;&#21147;&#23398;&#20449;&#24687;&#24418;&#25104;&#24102;&#26377;&#20960;&#20309;&#20449;&#24687;&#30340;&#24555;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#20943;&#23569;&#38454;&#25968;&#24314;&#27169;&#36890;&#24120;&#20316;&#20026;&#34880;&#28082;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#35768;&#22810;&#26597;&#35810;&#22330;&#26223;&#25110;&#23454;&#26102;&#27169;&#25311;&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#24555;&#29031;&#38656;&#35201;&#20174;&#30456;&#21516;&#30340;&#31163;&#25955;&#21270;&#20013;&#25910;&#38598;&#65292;&#36825;&#23545;&#20110;&#29289;&#29702;&#21442;&#25968;&#26469;&#35828;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#65292;&#20294;&#23545;&#20110;&#20960;&#20309;&#38382;&#39064;&#26469;&#35828;&#21364;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#37027;&#20123;&#20855;&#26377;&#38750;&#21442;&#25968;&#21270;&#21644;&#29420;&#29305;&#24418;&#29366;&#65288;&#20363;&#22914;&#24739;&#32773;&#29305;&#23450;&#24418;&#29366;&#65289;&#30340;&#22495;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#22495;&#19978;&#39640;&#25928;&#39044;&#27979;&#34880;&#27969;&#27169;&#25311;&#12290;&#25152;&#25552;&#20986;&#30340;&#26367;&#20195;&#27169;&#22411;&#21033;&#29992;&#32452;&#34920;&#38754;&#37197;&#20934;&#23545;&#36825;&#20123;&#24418;&#29366;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#21442;&#32771;&#22495;&#21644;&#21407;&#22987;&#22495;&#20043;&#38388;&#26500;&#24314;&#30340;&#24494;&#20998;&#21516;&#32986;&#26469;&#23558;&#30456;&#24212;&#30340;&#34880;&#28082;&#21160;&#21147;&#23398;&#20449;&#24687;&#24418;&#25104;&#24102;&#26377;&#20960;&#20309;&#20449;&#24687;&#30340;&#24555;&#29031;&#12290;&#38543;&#21518;&#65292;&#23545;&#20960;&#20309;&#21442;&#25968;&#36827;&#34892;&#38750;&#20405;&#20837;&#24615;&#20943;&#23569;&#38454;&#25968;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parametric reduced-order modelling often serves as a surrogate method for hemodynamics simulations to improve the computational efficiency in many-query scenarios or to perform real-time simulations. However, the snapshots of the method require to be collected from the same discretisation, which is a straightforward process for physical parameters, but becomes challenging for geometrical problems, especially for those domains featuring unparameterised and unique shapes, e.g. patient-specific geometries. In this work, a data-driven surrogate model is proposed for the efficient prediction of blood flow simulations on similar but distinct domains. The proposed surrogate model leverages group surface registration to parameterise those shapes and formulates corresponding hemodynamics information into geometry-informed snapshots by the diffeomorphisms constructed between a reference domain and original domains. A non-intrusive reduced-order model for geometrical parameters is subsequently co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;</title><link>http://arxiv.org/abs/2302.09738</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#22312;&#35745;&#31639;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30830;&#20445;&#36845;&#20195;&#20445;&#25345;&#22312;&#23376;&#27969;&#24418;&#19978;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#22256;&#38590;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24230;&#37327;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#21160;&#24577;&#22320;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#31616;&#21270;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21327;&#26041;&#24046;&#26041;&#27861;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#37325;&#21442;&#25968;&#21270;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#22914;&#26524;&#26174;&#24335;&#22320;&#34920;&#31034;&#24230;&#37327;&#24182;&#20351;&#29992;&#27491;&#30830;&#30340;&#30456;&#20851;&#21464;&#25442;&#35268;&#21017;&#65292;&#21017;&#19981;&#21464;&#24615;&#26159;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07384</link><description>&lt;p&gt;
&#37325;&#21442;&#25968;&#21270;&#19979;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#30340;&#20960;&#20309;&#23398;
&lt;/p&gt;
&lt;p&gt;
The Geometry of Neural Nets' Parameter Spaces Under Reparametrization. (arXiv:2302.07384v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#37325;&#21442;&#25968;&#21270;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#22914;&#26524;&#26174;&#24335;&#22320;&#34920;&#31034;&#24230;&#37327;&#24182;&#20351;&#29992;&#27491;&#30830;&#30340;&#30456;&#20851;&#21464;&#25442;&#35268;&#21017;&#65292;&#21017;&#19981;&#21464;&#24615;&#26159;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#21442;&#25968;&#21270;&#26159;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#20294;&#20063;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22914;&#22312;Hessian&#24179;&#22374;&#24230;&#27979;&#37327;&#12289;&#20248;&#21270;&#36712;&#36857;&#21644;&#27010;&#29575;&#23494;&#24230;&#27169;&#24335;&#31561;&#26041;&#38754;&#24341;&#20837;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20351;&#24471;&#19979;&#28216;&#20998;&#26512;&#21464;&#24471;&#26356;&#20026;&#22797;&#26434;&#65306;&#20363;&#22914;&#65292;&#30001;&#20110;&#20219;&#24847;&#30340;&#37325;&#21442;&#25968;&#21270;&#37117;&#21487;&#20197;&#25913;&#21464;&#20108;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#27492;&#26080;&#27861;&#26126;&#30830;&#22320;&#23558;&#24179;&#22374;&#24230;&#19982;&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#40654;&#26364;&#20960;&#20309;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#37325;&#21442;&#25968;&#21270;&#19979;&#30340;&#19981;&#21464;&#24615;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#22914;&#26524;&#25105;&#20204;&#26174;&#24335;&#22320;&#34920;&#31034;&#24230;&#37327;&#24182;&#20351;&#29992;&#27491;&#30830;&#30340;&#30456;&#20851;&#21464;&#25442;&#35268;&#21017;&#65292;&#37027;&#20040;&#19981;&#21464;&#24615;&#26159;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36825;&#19968;&#28857;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23613;&#31649;&#24230;&#37327;&#22987;&#32456;&#23384;&#22312;&#65292;&#20294;&#36890;&#24120;&#34987;&#38544;&#24335;&#22320;&#20551;&#23450;&#20026;&#21333;&#20301;&#30697;&#38453;&#65292;&#24182;&#22240;&#27492;&#20174;&#31526;&#21495;&#20013;&#30465;&#30053;&#65292;&#28982;&#21518;&#22312;&#37325;&#21442;&#25968;&#21270;&#19979;&#20002;&#22833;&#20102;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#34913;&#37327;&#24179;&#22374;&#24230;&#25152;&#24102;&#26469;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way to improve the training of neural nets. But it can also be problematic since it can induce inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes of probability densities. This complicates downstream analyses: e.g. one cannot definitively relate flatness with generalization since arbitrary reparametrization changes their relationship. In this work, we study the invariance of neural nets under reparametrization from the perspective of Riemannian geometry. From this point of view, invariance is an inherent property of any neural net if one explicitly represents the metric and uses the correct associated transformation rules. This is important since although the metric is always present, it is often implicitly assumed as identity, and thus dropped from the notation, then lost under reparametrization. We discuss implications for measuring the flatness of
&lt;/p&gt;</description></item><item><title>Re-ViLM&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04858</link><description>&lt;p&gt;
Re-ViLM: &#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#30340;&#26816;&#32034;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. (arXiv:2302.04858v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04858
&lt;/p&gt;
&lt;p&gt;
Re-ViLM&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#65288;&#22914;Flamingo&#65289;&#30456;&#32467;&#21512;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30693;&#35782;&#23384;&#20648;&#22312;&#20854;&#21442;&#25968;&#20013;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#24040;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#24314;&#27169;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#21644;&#20016;&#23500;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#34701;&#21512;&#26032;&#25968;&#25454;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#38656;&#35201;&#32791;&#26102;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;Re-ViLM&#65292;&#22522;&#20110;Flamingo&#26500;&#24314;&#65292;&#25903;&#25345;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#20869;&#23569;&#26679;&#26412;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#12290;&#36890;&#36807;&#23558;&#26576;&#20123;&#30693;&#35782;&#26126;&#30830;&#23384;&#20648;&#22312;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#26356;&#26032;&#25968;&#25454;&#24211;&#26469;&#36731;&#26494;&#36866;&#24212;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#26032;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#31181;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#19978;&#19979;&#25991;&#20869;&#23569;&#26679;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30740;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2302.04012</link><description>&lt;p&gt;
CodeLMSec&#22522;&#20934;&#65306;&#31995;&#32479;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. (arXiv:2302.04012v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30740;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29992;&#20110;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#22312;&#20960;&#20010;&#32534;&#31243;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#23427;&#20204;&#22312;&#31454;&#36187;&#32423;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#36827;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;AI&#36741;&#21161;&#23545;&#32534;&#31243;&#30340;&#37325;&#35201;&#25903;&#26609;&#65292;&#24037;&#20855;&#22914;GitHub Copilot&#24050;&#32463;&#25104;&#20026;&#25968;&#30334;&#19975;&#24320;&#21457;&#20154;&#21592;&#26085;&#24120;&#32534;&#31243;&#24037;&#20316;&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26469;&#33258;&#20110;&#20114;&#32852;&#32593;&#65288;&#20363;&#22914;&#24320;&#28304;&#23384;&#20648;&#24211;&#65289;&#24182;&#19988;&#21487;&#33021;&#21547;&#26377;&#32570;&#38519;&#21644;&#23433;&#20840;&#28431;&#27934;&#12290;&#36825;&#20123;&#26410;&#32463;&#28040;&#27602;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#28431;&#27934;&#24182;&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#20256;&#25773;&#23427;&#20204;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#24191;&#27867;&#35780;&#20272;&#20102;&#23427;&#20204;&#29983;&#25104;&#21151;&#33021;&#19978;&#27491;&#30830;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) for automatic code generation have achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively assessed for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.  In this work, we propose a method to systematically study the security issues of code l
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#24182;&#32500;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03857</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65306;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection. (arXiv:2302.03857v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#24182;&#32500;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#20294;&#21487;&#20197;&#36755;&#20986;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#19988;&#36866;&#29992;&#20110;&#24191;&#27867;&#19979;&#28216;&#20219;&#21153;&#30340;&#24378;&#40065;&#26834;&#24615;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;ACL&#38656;&#35201;&#24040;&#22823;&#30340;&#36816;&#34892;&#26102;&#38388;&#25165;&#33021;&#29983;&#25104;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#21464;&#20307;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#21152;&#36895;ACL&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#24615;&#24863;&#30693;&#30340;&#25968;&#25454;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;RCS&#65289;&#26041;&#27861;&#12290;RCS&#19981;&#38656;&#35201;&#26631;&#31614;&#20449;&#24687;&#65292;&#25628;&#32034;&#26368;&#23567;&#21270;&#34920;&#31034;&#20998;&#27495;&#30340;&#20449;&#24687;&#23376;&#38598;&#65292;&#21363;&#33258;&#28982;&#25968;&#25454;&#21644;&#20854;&#34394;&#25311;&#23545;&#25239;&#21464;&#20307;&#20043;&#38388;&#34920;&#31034;&#30340;&#36317;&#31163;&#12290;RCS&#30340;&#22522;&#26412;&#35299;&#27861;&#26159;&#36941;&#21382;&#25152;&#26377;&#21487;&#33021;&#30340;&#23376;&#38598;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23558;RCS&#36716;&#21270;&#20026;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#26367;&#20195;&#38382;&#39064;&#65292;&#21033;&#29992;&#36138;&#24515;&#25628;&#32034;&#26159;&#21407;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20855;&#26377;&#21407;&#38382;&#39064;&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RCS&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#26377;&#25928;&#22320;&#21152;&#36895;ACL&#65292;&#24182;&#19988;&#20173;&#28982;&#20445;&#25345;&#20854;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our compr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.03693</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#30340;&#27010;&#24565;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#26576;&#31181;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#65288;&#25110;&#26041;&#21521;&#65289;&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#36825;&#20010;&#24605;&#24819;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21033;&#29992;&#36825;&#20010;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#19968;&#20010;&#33258;&#28982;&#30340;&#34920;&#31034;&#36873;&#25321;&#20855;&#26377;&#36825;&#31181;&#24615;&#36136;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23545;&#34920;&#31034;&#30340;&#20195;&#25968;&#25805;&#20316;&#26469;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#31034;&#20363;&#20013;&#28436;&#31034;&#20102;&#36825;&#20010;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20803;&#25968;&#25454;&#23545;&#31185;&#23398;&#25991;&#29486;&#26631;&#31614;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;19&#20010;&#39046;&#22495;&#20013;&#36873;&#25321;&#20102;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2302.03341</link><description>&lt;p&gt;
&#20803;&#25968;&#25454;&#23545;&#31185;&#23398;&#25991;&#29486;&#26631;&#31614;&#21270;&#30340;&#24433;&#21709;&#65306;&#36328;&#39046;&#22495;&#36328;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study. (arXiv:2302.03341v1 [cs.DL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20803;&#25968;&#25454;&#23545;&#31185;&#23398;&#25991;&#29486;&#26631;&#31614;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;19&#20010;&#39046;&#22495;&#20013;&#36873;&#25321;&#20102;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#20986;&#29256;&#29289;&#22312;&#32593;&#32476;&#19978;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#36843;&#20999;&#38656;&#35201;&#20026;&#27599;&#31687;&#35770;&#25991;&#25171;&#19978;&#32454;&#31890;&#24230;&#20027;&#39064;&#26631;&#31614;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36319;&#36394;&#33258;&#24049;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#32780;&#19981;&#26159;&#28153;&#27809;&#22312;&#25972;&#20010;&#25991;&#29486;&#20013;&#12290;&#31185;&#23398;&#25991;&#29486;&#30340;&#26631;&#31614;&#21270;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#32431;&#31929;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#22240;&#20026;&#32593;&#32476;&#19978;&#30340;&#35770;&#25991;&#36890;&#24120;&#38468;&#24102;&#20803;&#25968;&#25454;&#20449;&#24687;&#65292;&#22914;&#20250;&#35758;&#12289;&#20316;&#32773;&#21644;&#21442;&#32771;&#25991;&#29486;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#20316;&#20026;&#39069;&#22806;&#30340;&#20449;&#21495;&#26469;&#25512;&#26029;&#30456;&#20851;&#30340;&#26631;&#31614;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#30740;&#31350;&#21033;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#23398;&#26415;&#35770;&#25991;&#20998;&#31867;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38480;&#20110;&#19968;&#20010;&#25110;&#20004;&#20010;&#31185;&#23398;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#65289;&#21644;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20803;&#25968;&#25454;&#23545;19&#20010;&#39046;&#22495;&#31185;&#23398;&#25991;&#29486;&#26631;&#31614;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65288;&#35789;&#34955;&#27169;&#22411;&#12289;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65289;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the exponential growth of scientific publications on the Web, there is a pressing need to tag each paper with fine-grained topics so that researchers can track their interested fields of study rather than drowning in the whole literature. Scientific literature tagging is beyond a pure multi-label text classification task because papers on the Web are prevalently accompanied by metadata information such as venues, authors, and references, which may serve as additional signals to infer relevant tags. Although there have been studies making use of metadata in academic paper classification, their focus is often restricted to one or two scientific fields (e.g., computer science and biomedicine) and to one specific model. In this work, we systematically study the effect of metadata on scientific literature tagging across 19 fields. We select three representative multi-label classifiers (i.e., a bag-of-words model, a sequence-based model, and a pre-trained language model) and explore t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2302.02601</link><description>&lt;p&gt;
&#23398;&#20064;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#36229;&#36234;&#38142;&#25509;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#24050;&#30693;&#20107;&#23454;&#12290;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#20165;&#32771;&#34385;&#23454;&#20307;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#32771;&#34385;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26356;&#39640;&#32423;&#30340;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;$\langle T_1$, PrerequisiteFor, $T_2\rangle$&#65292;&#20854;&#20013;PrerequisiteFor&#26159;&#26356;&#39640;&#32423;&#21035;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23450;&#20041;&#19968;&#20010;&#30001;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#30340;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;BiVE&#36890;&#36807;&#32771;&#34385;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#19977;&#20803;&#32452;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Sparling&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#26497;&#24230;&#31232;&#30095;&#28608;&#27963;&#65292;&#22312;&#27809;&#26377;&#20013;&#38388;&#29366;&#24577;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#31471;&#21040;&#31471;&#26631;&#35760;&#31034;&#20363;&#20013;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#25216;&#26415;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20449;&#24687;&#29942;&#39048;&#26469;&#23454;&#29616;&#26497;&#24230;&#31232;&#30095;&#28608;&#27963;&#65292;&#36798;&#21040;&#20102;&#33391;&#22909;&#30340;&#20013;&#38388;&#29366;&#24577;&#24314;&#27169;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01976</link><description>&lt;p&gt;
SPARLING&#65306;&#20351;&#29992;&#26497;&#24230;&#31232;&#30095;&#28608;&#27963;&#36827;&#34892;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SPARLING: Learning Latent Representations with Extremely Sparse Activations. (arXiv:2302.01976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Sparling&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#26497;&#24230;&#31232;&#30095;&#28608;&#27963;&#65292;&#22312;&#27809;&#26377;&#20013;&#38388;&#29366;&#24577;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#31471;&#21040;&#31471;&#26631;&#35760;&#31034;&#20363;&#20013;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#25216;&#26415;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20449;&#24687;&#29942;&#39048;&#26469;&#23454;&#29616;&#26497;&#24230;&#31232;&#30095;&#28608;&#27963;&#65292;&#36798;&#21040;&#20102;&#33391;&#22909;&#30340;&#20013;&#38388;&#29366;&#24577;&#24314;&#27169;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#30340;&#36807;&#31243;&#24120;&#24120;&#21253;&#21547;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#26497;&#24230;&#31232;&#30095;&#24352;&#37327;&#30340;&#20013;&#38388;&#29366;&#24577;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Sparling&#65292;&#19968;&#31181;&#20801;&#35768;&#24744;&#20174;&#20165;&#20855;&#26377;&#31471;&#21040;&#31471;&#26631;&#35760;&#31034;&#20363;&#65288;&#21363;&#26080;&#20013;&#38388;&#29366;&#24577;&#30340;&#30417;&#30563;&#65289;&#20013;&#23398;&#20064;&#19982;&#35813;&#29366;&#24577;&#30456;&#21305;&#37197;&#30340;&#27169;&#22411;&#30340;&#25216;&#26415;&#12290;Sparling&#20351;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#31243;&#24230;&#26469;&#23454;&#29616;&#20854;&#20182;&#25216;&#26415;&#26080;&#27861;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26497;&#24230;&#31232;&#30095;&#24615;&#26159;&#23454;&#29616;&#33391;&#22909;&#30340;&#20013;&#38388;&#29366;&#24577;&#24314;&#27169;&#25152;&#24517;&#38656;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#21512;&#25104;DigitCircle&#39046;&#22495;&#20197;&#21450;LaTeX-OCR&#21644;Audio-MNIST-Sequence&#39046;&#22495;&#20013;&#65292;&#21363;&#20351;&#25105;&#20204;&#20165;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#25105;&#20204;&#20063;&#33021;&#20197;&#36229;&#36807;90&#65285;&#30340;&#20934;&#30830;&#24615;&#31934;&#30830;&#23450;&#20301;&#20013;&#38388;&#29366;&#24577;&#65292;&#21363;&#20351;&#23384;&#22312;&#29305;&#24449;&#32622;&#25442;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world processes often contain intermediate state that can be modeled as an extremely sparse tensor. We introduce Sparling, a technique that allows you to learn models with intermediate layers that match this state from only end-to-end labeled examples (i.e., no supervision on the intermediate state). Sparling uses a new kind of informational bottleneck that enforces levels of activation sparsity unachievable using other techniques. We find that extreme sparsity is necessary to achieve good intermediate state modeling. On our synthetic DigitCircle domain as well as the LaTeX-OCR and Audio-MNIST-Sequence domains, we are able to precisely localize the intermediate states up to feature permutation with &gt; 90% accuracy, even though we only train end-to-end.
&lt;/p&gt;</description></item><item><title>ResMem&#26159;&#19968;&#31181;&#36890;&#36807;&#26174;&#24335;&#35760;&#24518;&#26469;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25311;&#21512;&#27169;&#22411;&#30340;&#27531;&#24046;&#26469;&#23454;&#29616;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ResMem&#19968;&#33268;&#22320;&#25913;&#21892;&#20102;&#21407;&#22987;&#39044;&#27979;&#27169;&#22411;&#30340;&#27979;&#35797;&#38598;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.01576</link><description>&lt;p&gt;
ResMem&#65306;&#23398;&#20064;&#21487;&#20197;&#30340;&#65292;&#35760;&#20303;&#21097;&#19979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
ResMem: Learn what you can and memorize the rest. (arXiv:2302.01576v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01576
&lt;/p&gt;
&lt;p&gt;
ResMem&#26159;&#19968;&#31181;&#36890;&#36807;&#26174;&#24335;&#35760;&#24518;&#26469;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25311;&#21512;&#27169;&#22411;&#30340;&#27531;&#24046;&#26469;&#23454;&#29616;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ResMem&#19968;&#33268;&#22320;&#25913;&#21892;&#20102;&#21407;&#22987;&#39044;&#27979;&#27169;&#22411;&#30340;&#27979;&#35797;&#38598;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#25152;&#23637;&#29616;&#20986;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#27867;&#21270;&#24615;&#33021;&#37096;&#20998;&#24402;&#21151;&#20110;&#20854;&#38544;&#24335;&#35760;&#24518;&#22797;&#26434;&#30340;&#35757;&#32451;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#25913;&#36827;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26426;&#21046;&#65292;&#36890;&#36807;&#26174;&#24335;&#35760;&#24518;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27531;&#24046;&#35760;&#24518;&#65288;ResMem&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#29992;&#22522;&#20110;$k$&#26368;&#36817;&#37051;&#30340;&#22238;&#24402;&#22120;&#25311;&#21512;&#27169;&#22411;&#30340;&#27531;&#24046;&#26469;&#22686;&#21152;&#29616;&#26377;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#39044;&#27979;&#26159;&#21407;&#22987;&#27169;&#22411;&#21644;&#25311;&#21512;&#30340;&#27531;&#24046;&#22238;&#24402;&#22120;&#30340;&#21644;&#12290;&#36890;&#36807;&#26500;&#36896;&#65292;ResMem&#21487;&#20197;&#26174;&#24335;&#22320;&#35760;&#20303;&#35757;&#32451;&#26631;&#31614;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ResMem&#22312;&#21508;&#31181;&#26631;&#20934;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#19968;&#33268;&#22320;&#25913;&#21892;&#20102;&#21407;&#22987;&#39044;&#27979;&#27169;&#22411;&#30340;&#27979;&#35797;&#38598;&#27867;&#21270;&#33021;&#21147;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#20005;&#26684;&#35777;&#26126;&#20102;ResMem&#30456;&#23545;&#20110;&#22522;&#26412;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#27979;&#35797;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive generalization performance of modern neural networks is attributed in part to their ability to implicitly memorize complex training patterns. Inspired by this, we explore a novel mechanism to improve model generalization via explicit memorization. Specifically, we propose the residual-memorization (ResMem) algorithm, a new method that augments an existing prediction model (e.g. a neural network) by fitting the model's residuals with a $k$-nearest neighbor based regressor. The final prediction is then the sum of the original model and the fitted residual regressor. By construction, ResMem can explicitly memorize the training labels. Empirically, we show that ResMem consistently improves the test set generalization of the original prediction model across various standard vision and natural language processing benchmarks. Theoretically, we formulate a stylized linear regression problem and rigorously show that ResMem results in a more favorable test risk over the base predi
&lt;/p&gt;</description></item><item><title>SimMTM&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#24207;&#21015;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#26435;&#32858;&#21512;&#22810;&#20010;&#27969;&#24418;&#20043;&#22806;&#30340;&#37051;&#23621;&#26469;&#24674;&#22797;&#25513;&#30721;&#26102;&#38388;&#28857;&#65292;&#20174;&#32780;&#31616;&#21270;&#37325;&#26500;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.00861</link><description>&lt;p&gt;
SimMTM: &#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#27169;&#22411;&#30340;&#31616;&#21333;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling. (arXiv:2302.00861v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00861
&lt;/p&gt;
&lt;p&gt;
SimMTM&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#24207;&#21015;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#26435;&#32858;&#21512;&#22810;&#20010;&#27969;&#24418;&#20043;&#22806;&#30340;&#37051;&#23621;&#26469;&#24674;&#22797;&#25513;&#30721;&#26102;&#38388;&#28857;&#65292;&#20174;&#32780;&#31616;&#21270;&#37325;&#26500;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#20026;&#20102;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#24182;&#21463;&#30410;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#20854;&#20013;&#19968;&#31181;&#20027;&#27969;&#33539;&#20363;&#26159;&#25513;&#30721;&#24314;&#27169;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#22522;&#20110;&#26410;&#25513;&#30721;&#37096;&#20998;&#30340;&#25513;&#30721;&#20869;&#23481;&#30340;&#37325;&#26500;&#65292;&#25104;&#21151;&#22320;&#39044;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#20041;&#20449;&#24687;&#20027;&#35201;&#21253;&#21547;&#22312;&#26102;&#38388;&#21464;&#21270;&#20013;&#65292;&#26631;&#20934;&#30340;&#38543;&#26426;&#23631;&#34109;&#19968;&#37096;&#20998;&#26102;&#38388;&#28857;&#30340;&#26041;&#24335;&#20250;&#20005;&#37325;&#30772;&#22351;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#35201;&#26102;&#38388;&#21464;&#21270;&#65292;&#20351;&#37325;&#26500;&#20219;&#21153;&#36807;&#20110;&#22256;&#38590;&#65292;&#26080;&#27861;&#24341;&#23548;&#34920;&#31034;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24207;&#21015;&#27169;&#22411;&#30340;&#31616;&#21333;&#39044;&#35757;&#32451;&#26694;&#26550;SimMTM&#12290;&#36890;&#36807;&#23558;&#25513;&#30721;&#24314;&#27169;&#19982;&#27969;&#24418;&#23398;&#20064;&#30456;&#20851;&#32852;&#65292;SimMTM&#25552;&#20986;&#36890;&#36807;&#22810;&#20010;&#27969;&#24418;&#20043;&#22806;&#30340;&#37051;&#23621;&#30340;&#21152;&#26435;&#32858;&#21512;&#26469;&#24674;&#22797;&#25513;&#30721;&#26102;&#38388;&#28857;&#65292;&#20174;&#32780;&#36890;&#36807;&#32452;&#35013;&#34987;&#30772;&#22351;&#20294;&#20114;&#34917;&#30340;&#26102;&#38388;&#21464;&#21270;&#26469;&#31616;&#21270;&#37325;&#26500;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#33976;&#39311;&#21644;&#26631;&#31614;&#24179;&#28369;&#34987;&#35748;&#20026;&#26159;&#31561;&#20215;&#30340;&#26041;&#27861;&#65292;&#20294;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#26041;&#21521;&#23436;&#20840;&#30456;&#21453;&#12290;&#30693;&#35782;&#33976;&#39311;&#19981;&#20165;&#20256;&#36882;&#30693;&#35782;&#65292;&#36824;&#20256;&#36882;&#20102;&#33258;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2301.12609</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#8776;&#26631;&#31614;&#24179;&#28369;&#65306;&#20107;&#23454;&#36824;&#26159;&#35884;&#35823;&#65311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12609
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#21644;&#26631;&#31614;&#24179;&#28369;&#34987;&#35748;&#20026;&#26159;&#31561;&#20215;&#30340;&#26041;&#27861;&#65292;&#20294;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#26041;&#21521;&#23436;&#20840;&#30456;&#21453;&#12290;&#30693;&#35782;&#33976;&#39311;&#19981;&#20165;&#20256;&#36882;&#30693;&#35782;&#65292;&#36824;&#20256;&#36882;&#20102;&#33258;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21021;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20174;&#19968;&#20010;&#27169;&#22411;&#21521;&#21478;&#19968;&#20010;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#30693;&#35782;&#33976;&#39311;(KD)&#23454;&#38469;&#19978;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#24418;&#24335;&#12290;&#26368;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#26469;&#33258;&#20110;&#23427;&#19982;&#26631;&#31614;&#24179;&#28369;(LS)&#26041;&#27861;&#30340;&#26126;&#26174;&#30456;&#20284;&#20043;&#22788;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#37325;&#26032;&#32771;&#23519;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#12290;&#22312;&#28041;&#21450;&#19981;&#21516;&#35268;&#27169;&#27169;&#22411;&#30340;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#26174;&#31034;&#20986;&#65306;(a)&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#65292;KD&#21644;LS&#20250;&#23436;&#20840;&#30456;&#21453;&#22320;&#24433;&#21709;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;(b) &#22312;KD&#20013;&#65292;&#23398;&#29983;&#19981;&#20165;&#32487;&#25215;&#30693;&#35782;&#65292;&#32780;&#19988;&#36824;&#20174;&#32769;&#24072;&#37027;&#37324;&#32487;&#25215;&#33258;&#20449;&#24515;&#65292;&#21152;&#24378;&#20102;&#20256;&#32479;&#30340;&#30693;&#35782;&#20256;&#36882;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest support of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks involving models of different sizes show that: (a) In most settings, KD and LS drive model confidence in completely opposite directions, and (b) In KD, the student inherits not only its knowledge but also its confidence from the teacher, reinforcing the classical knowledge transfer view.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairFor&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23545;&#25239;&#23398;&#20064;&#29983;&#25104;&#20102;&#29420;&#31435;&#20110;&#32676;&#32452;&#21644;&#19982;&#32676;&#32452;&#30456;&#20851;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#35889;&#26494;&#24347;&#21644;&#36807;&#28388;&#34701;&#21512;&#32452;&#20214;&#26469;&#25512;&#26029;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#36827;&#34892;&#21464;&#37327;&#20998;&#32452;&#12290;</title><link>http://arxiv.org/abs/2301.11535</link><description>&lt;p&gt;
&#23398;&#20064;&#20449;&#24687;&#21270;&#34920;&#31034;&#20197;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#22522;&#20110;&#32676;&#32452;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Learning Informative Representation for Fairness-aware Multivariate Time-series Forecasting: A Group-based Perspective. (arXiv:2301.11535v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairFor&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23545;&#25239;&#23398;&#20064;&#29983;&#25104;&#20102;&#29420;&#31435;&#20110;&#32676;&#32452;&#21644;&#19982;&#32676;&#32452;&#30456;&#20851;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#35889;&#26494;&#24347;&#21644;&#36807;&#28388;&#34701;&#21512;&#32452;&#20214;&#26469;&#25512;&#26029;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#36827;&#34892;&#21464;&#37327;&#20998;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#21464;&#37327;&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#26222;&#36941;&#23384;&#22312;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#20851;&#27880;/&#20559;&#21521;&#26576;&#20123;&#65288;&#26377;&#21033;&#30340;&#65289;&#21464;&#37327;&#12290;&#35299;&#20915;&#36825;&#31181;&#19981;&#20844;&#24179;&#38382;&#39064;&#23545;&#20110;&#24179;&#31561;&#20851;&#27880;&#25152;&#26377;&#21464;&#37327;&#24182;&#36991;&#20813;&#27169;&#22411;&#30340;&#20559;&#35265;&#21644;&#39118;&#38505;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;MTS&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20844;&#24179;&#24314;&#27169;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#20449;&#24687;&#21270;&#34920;&#31034;&#65292;&#26082;&#20851;&#27880;&#26377;&#21033;&#30340;&#21464;&#37327;&#20063;&#20851;&#27880;&#19981;&#21033;&#30340;&#21464;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;FairFor&#65292;&#29992;&#20110;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;MTS&#39044;&#27979;&#12290;FairFor&#22522;&#20110;&#23545;&#25239;&#23398;&#20064;&#65292;&#29983;&#25104;&#29420;&#31435;&#20110;&#32676;&#32452;&#21644;&#19982;&#32676;&#32452;&#30456;&#20851;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#21518;&#32493;&#30340;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;K-means&#30446;&#26631;&#30340;&#35889;&#26494;&#24347;&#26469;&#25512;&#26029;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#21464;&#37327;&#20998;&#32452;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#36807;&#28388;&#21644;&#34701;&#21512;&#32452;&#20214;&#26469;&#36807;&#28388;&#21464;&#37327;&#24182;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Performance unfairness among variables widely exists in multivariate time series (MTS) forecasting models since such models may attend/bias to certain (advantaged) variables. Addressing this unfairness problem is important for equally attending to all variables and avoiding vulnerable model biases/risks. However, fair MTS forecasting is challenging and has been less studied in the literature. To bridge such significant gap, we formulate the fairness modeling problem as learning informative representations attending to both advantaged and disadvantaged variables. Accordingly, we propose a novel framework, named FairFor, for fairness-aware MTS forecasting. FairFor is based on adversarial learning to generate both group-independent and group-relevant representations for the downstream forecasting. The framework first leverages a spectral relaxation of the K-means objective to infer variable correlations and thus to group variables. Then, it utilizes a filtering&amp;fusion component to filter 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#38543;&#26426;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#21452;&#37325;&#23545;&#25239;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20219;&#20309;&#32852;&#37030;&#36172;&#21338;&#31639;&#27861;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25509;&#36817;&#26368;&#20248;&#30340;&#31639;&#27861;FEDEXP3&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.09223</link><description>&lt;p&gt;
&#21452;&#37325;&#23545;&#25239;&#24615;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Doubly Adversarial Federated Bandits. (arXiv:2301.09223v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09223
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#38543;&#26426;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#21452;&#37325;&#23545;&#25239;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20219;&#20309;&#32852;&#37030;&#36172;&#21338;&#31639;&#27861;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25509;&#36817;&#26368;&#20248;&#30340;&#31639;&#27861;FEDEXP3&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#38543;&#26426;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#22810;&#20010;&#20195;&#29702;&#36890;&#36807;&#36890;&#20449;&#32593;&#32476;&#36827;&#34892;&#21327;&#20316;&#12290;&#33218;&#30340;&#25439;&#22833;&#30001;&#19968;&#20010;&#26080;&#24847;&#35782;&#30340;&#23545;&#25163;&#20998;&#37197;&#65292;&#35813;&#23545;&#25163;&#19981;&#20165;&#25351;&#23450;&#27599;&#20010;&#26102;&#38388;&#27493;&#21644;&#27599;&#20010;&#20195;&#29702;&#30340;&#27599;&#20010;&#33218;&#30340;&#25439;&#22833;&#65292;&#36824;&#20855;&#26377;&#8220;&#21452;&#37325;&#23545;&#25239;&#24615;&#8221;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#19981;&#21516;&#30340;&#20195;&#29702;&#21487;&#33021;&#22312;&#21516;&#19968;&#26102;&#38388;&#27493;&#36873;&#25321;&#30456;&#21516;&#30340;&#33218;&#65292;&#20294;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#21453;&#39304;&#12290;&#27599;&#20010;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#20840;&#23616;&#26368;&#22909;&#30340;&#33218;&#65292;&#20351;&#24471;&#22312;&#25152;&#26377;&#20195;&#29702;&#19978;&#24179;&#22343;&#32047;&#31215;&#25439;&#22833;&#26368;&#20302;&#65292;&#36825;&#38656;&#35201;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#35774;&#32622;&#25552;&#20379;&#20102;&#20219;&#20309;&#32852;&#37030;&#36172;&#21338;&#31639;&#27861;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#24403;&#20195;&#29702;&#26377;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#25110;&#36172;&#21338;&#21453;&#39304;&#26102;&#12290;&#23545;&#20110;&#36172;&#21338;&#21453;&#39304;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25509;&#36817;&#26368;&#20248;&#30340;&#32852;&#37030;&#36172;&#21338;&#31639;&#27861;&#31216;&#20026;FEDEXP3&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;Cesa-Bianchi&#31561;&#20154;&#65288;2016&#65289;&#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#32473;&#20986;&#20102;&#27491;&#38754;&#31572;&#26696;&#65306;FEDEXP3&#21487;&#20197;&#20445;&#35777;...
&lt;/p&gt;
&lt;p&gt;
We study a new non-stochastic federated multi-armed bandit problem with multiple agents collaborating via a communication network. The losses of the arms are assigned by an oblivious adversary that specifies the loss of each arm not only for each time step but also for each agent, which we call ``doubly adversarial". In this setting, different agents may choose the same arm in the same time step but observe different feedback. The goal of each agent is to find a globally best arm in hindsight that has the lowest cumulative loss averaged over all agents, which necessities the communication among agents. We provide regret lower bounds for any federated bandit algorithm under different settings, when agents have access to full-information feedback, or the bandit feedback. For the bandit feedback setting, we propose a near-optimal federated bandit algorithm called FEDEXP3. Our algorithm gives a positive answer to an open question proposed in Cesa-Bianchi et al. (2016): FEDEXP3 can guarante
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#26368;&#36817;&#22312;NLP&#39046;&#22495;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#21516;&#20043;&#22788;&#21644;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.09112</link><description>&lt;p&gt;
&#19981;&#21516;ially&#31169;&#23494;&#30340;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;: &#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Natural Language Models: Recent Advances and Future Directions. (arXiv:2301.09112v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09112
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#26368;&#36817;&#22312;NLP&#39046;&#22495;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#21516;&#20043;&#22788;&#21644;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24212;&#29992;&#21487;&#33021;&#28041;&#21450;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#22914;&#20309;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#26159;NLP&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25104;&#20026;&#20102;&#38544;&#31169;&#25968;&#25454;&#20998;&#26512;&#20013;&#38450;&#27490;&#37325;&#24314;&#25915;&#20987;&#21644;&#38450;&#25252;&#28508;&#22312;&#36793;&#32536;&#30693;&#35782;&#30340;&#20107;&#23454;&#26631;&#20934;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;DP&#22312;NLP&#27169;&#22411;&#65288;DP-NLP&#65289;&#26041;&#38754;&#24050;&#32463;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20540;&#24471;&#36827;&#34892;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;DP&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;NLP&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;DP&#28145;&#24230;&#23398;&#20064;&#30456;&#27604;&#65292;DP-NLP&#30340;&#19968;&#20123;&#24046;&#24322;&#21644;&#39069;&#22806;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;DP-NLP&#24037;&#20316;&#65292;&#24182;&#20174;&#19977;&#20010;&#26041;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#21457;&#23637;&#65306;&#26799;&#24230;pe
&lt;/p&gt;
&lt;p&gt;
Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP and present its recent developments from three aspects: gradient pe
&lt;/p&gt;</description></item><item><title>AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.08110</link><description>&lt;p&gt;
AtMan:&#36890;&#36807;&#33410;&#32422;&#20869;&#23384;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29702;&#35299;Transformer&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08110
&lt;/p&gt;
&lt;p&gt;
AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;Transformer&#27169;&#22411;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#21442;&#25968;&#25968;&#37327;&#22823;&#19988;&#20855;&#22791;&#22788;&#29702;&#22810;&#36755;&#20837;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#30340;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#21453;&#21521;&#20256;&#25773;&#20250;&#20998;&#37197;&#30340;GPU&#20869;&#23384;&#20960;&#20046;&#26159;&#21069;&#21521;&#20256;&#25773;&#30340;&#20004;&#20493;&#12290;&#36825;&#20351;&#24471;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#38750;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AtMan&#65292;&#23427;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#29992;&#20110;&#35299;&#37322;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AtMan&#26159;&#19968;&#31181;&#27169;&#24577;&#26080;&#20851;&#30340;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#19982;&#36755;&#20986;&#39044;&#27979;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#22270;&#12290;AtMan&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#26159;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#37051;&#36817;&#24615;&#30340;&#21487;&#24182;&#34892;&#21270;&#22522;&#20110;&#35760;&#21495;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;-&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#23398;&#20064;&#30340;&#29305;&#24449;&#21270;&#21028;&#21035;&#26631;&#20934;&#65292;&#21363;&#27599;&#20010;&#21333;&#36755;&#20986;&#23376;&#31867;&#21487;&#23398;&#20064;&#26102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#25165;&#21487;&#23398;&#20064;&#65292;&#22312;&#22810;&#26631;&#35760;&#20998;&#31867;&#21644;&#22810;&#36755;&#20986;&#22238;&#24402;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.02729</link><description>&lt;p&gt;
&#22810;&#36755;&#20986;&#21487;&#23398;&#20064;&#24615;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Characterization of Multioutput Learnability. (arXiv:2301.02729v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#23398;&#20064;&#30340;&#29305;&#24449;&#21270;&#21028;&#21035;&#26631;&#20934;&#65292;&#21363;&#27599;&#20010;&#21333;&#36755;&#20986;&#23376;&#31867;&#21487;&#23398;&#20064;&#26102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#25165;&#21487;&#23398;&#20064;&#65292;&#22312;&#22810;&#26631;&#35760;&#20998;&#31867;&#21644;&#22810;&#36755;&#20986;&#22238;&#24402;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#24403;&#19988;&#20165;&#24403;&#20989;&#25968;&#31867;&#30340;&#27599;&#20010;&#21333;&#36755;&#20986;&#23376;&#31867;&#37117;&#21487;&#23398;&#20064;&#26102;&#65292;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#25165;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;&#36825;&#25552;&#20379;&#20102;&#22810;&#26631;&#35760;&#20998;&#31867;&#21644;&#22810;&#36755;&#20986;&#22238;&#24402;&#22312;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#23398;&#20064;&#20013;&#21487;&#23398;&#20064;&#24615;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#12290;&#20316;&#20026;&#25193;&#23637;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#22312;&#36172;&#21338;&#21453;&#39304;&#29615;&#22659;&#19979;&#30340;&#22810;&#26631;&#35760;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#23436;&#20840;&#21453;&#39304;&#29615;&#22659;&#19979;&#31867;&#20284;&#30340;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning multioutput function classes in batch and online settings. In both settings, we show that a multioutput function class is learnable if and only if each single-output restriction of the function class is learnable. This provides a complete characterization of the learnability of multilabel classification and multioutput regression in both batch and online settings. As an extension, we also consider multilabel learnability in the bandit feedback setting and show a similar characterization as in the full-feedback setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#38382;&#39064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#65292;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20027;&#23548;&#30528;&#20854;&#24615;&#33021;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21452;&#23792;&#39118;&#38505;&#26354;&#32447;&#30456;&#21453;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#26368;&#20248;&#37327;&#23376;&#31070;&#32463;&#20998;&#31867;&#22120;&#19982;Helstrom&#36793;&#30028;&#21644;&#31561;&#35282;&#32039;&#26694;&#20043;&#38388;&#23384;&#22312;&#20869;&#22312;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#35282;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.01597</link><description>&lt;p&gt;
&#25581;&#31034;&#22522;&#20110;&#38382;&#39064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31867;&#20998;&#31867;&#19978;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Demystify Problem-Dependent Power of Quantum Neural Networks on Multi-Class Classification. (arXiv:2301.01597v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#38382;&#39064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#65292;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20027;&#23548;&#30528;&#20854;&#24615;&#33021;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21452;&#23792;&#39118;&#38505;&#26354;&#32447;&#30456;&#21453;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#26368;&#20248;&#37327;&#23376;&#31070;&#32463;&#20998;&#31867;&#22120;&#19982;Helstrom&#36793;&#30028;&#21644;&#31561;&#35282;&#32039;&#26694;&#20043;&#38388;&#23384;&#22312;&#20869;&#22312;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#35282;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#24050;&#25104;&#20026;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#19968;&#20123;&#20351;&#29992;&#29305;&#23450;&#32534;&#30721;&#26041;&#27861;&#30340;QNNs&#21487;&#20197;&#36890;&#36807;&#32463;&#20856;&#20195;&#29702;&#26377;&#25928;&#22320;&#27169;&#25311;&#65292;&#32780;&#20855;&#26377;&#37327;&#23376;&#35760;&#24518;&#30340;&#20854;&#20182;QNNs&#21487;&#33021;&#27604;&#32463;&#20856;&#20998;&#31867;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#37327;&#23376;&#31070;&#32463;&#20998;&#31867;&#22120;&#65288;QCs&#65289;&#22312;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#38382;&#39064;&#30456;&#20851;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#26399;&#26395;&#39118;&#38505;&#30340;&#20998;&#26512;&#65292;&#35813;&#25351;&#26631;&#32508;&#21512;&#32771;&#34385;&#20102;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#25439;&#22833;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#39318;&#20808;&#65292;&#35757;&#32451;&#25439;&#22833;&#20027;&#23548;&#30528;&#21151;&#25928;&#65292;&#32780;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#65307;&#31532;&#20108;&#65292;QCs&#32463;&#21382;U&#24418;&#39118;&#38505;&#26354;&#32447;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21452;&#23792;&#39118;&#38505;&#26354;&#32447;&#30456;&#21453;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#26368;&#20248;QCs&#19982;Helstrom&#36793;&#30028;&#21644;&#31561;&#35282;&#32039;&#26694;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#36890;&#36807;&#37327;&#23376;&#24453;&#27979;&#35797;&#26679;&#26412;&#19982;&#26368;&#20248;QCs&#20043;&#38388;&#30340;&#26368;&#23567;&#35282;&#24230;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#38382;&#39064;&#30340;&#37327;&#23376;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs) have become an important tool for understanding the physical world, but their advantages and limitations are not fully understood. Some QNNs with specific encoding methods can be efficiently simulated by classical surrogates, while others with quantum memory may perform better than classical classifiers. Here we systematically investigate the problem-dependent power of quantum neural classifiers (QCs) on multi-class classification tasks. Through the analysis of expected risk, a measure that weighs the training loss and the generalization error of a classifier jointly, we identify two key findings: first, the training loss dominates the power rather than the generalization ability; second, QCs undergo a U-shaped risk curve, in contrast to the double-descent risk curve of deep neural classifiers. We also reveal the intrinsic connection between optimal QCs and the Helstrom bound and the equiangular tight frame. Using these findings, we propose a method that 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25253;&#21578;&#22238;&#39038;&#20102;&#20851;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#29702;&#35770;&#21644;&#23454;&#36341;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#25910;&#38598;&#26500;&#24314;&#22797;&#26434;&#12289;&#36229;&#22797;&#26434;&#21644;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#29616;&#26377;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2301.00007</link><description>&lt;p&gt;
&#22797;&#26434;&#12289;&#36229;&#22797;&#26434;&#21644;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#23450;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Selected aspects of complex, hypercomplex and fuzzy neural networks. (arXiv:2301.00007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25253;&#21578;&#22238;&#39038;&#20102;&#20851;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#29702;&#35770;&#21644;&#23454;&#36341;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#25910;&#38598;&#26500;&#24314;&#22797;&#26434;&#12289;&#36229;&#22797;&#26434;&#21644;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#29616;&#26377;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30701;&#31687;&#25253;&#21578;&#22238;&#39038;&#20102;&#20851;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#29702;&#35770;&#21644;&#23454;&#36341;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#21644;&#26041;&#27861;&#12290;&#23427;&#26088;&#22312;&#25910;&#38598;&#26500;&#24314;&#22797;&#26434;&#12289;&#36229;&#22797;&#26434;&#21644;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#29616;&#26377;&#30693;&#35782;&#12290;&#35813;&#25253;&#21578;&#21453;&#26144;&#20102;&#20316;&#32773;&#20204;&#20010;&#20154;&#30340;&#20852;&#36259;&#65292;&#32477;&#19981;&#21487;&#35270;&#20026;ANN&#23398;&#31185;&#30340;&#20840;&#38754;&#35780;&#35770;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30446;&#21069;&#19981;&#21487;&#33021;&#23545;&#22823;&#37327;&#39029;&#38754;&#36827;&#34892;&#35814;&#32454;&#35780;&#35770;&#12290;&#35813;&#25253;&#21578;&#26159;2022&#24180;9&#26376;&#22312;&#27874;&#20848;&#22885;&#23572;&#20160;&#19969;&#29926;&#23572;&#31859;&#20122;&#21644;&#39532;&#31062;&#29790;&#22823;&#23398;&#32452;&#32455;&#30340;&#8220;&#22797;&#26434;&#12289;&#36229;&#22797;&#26434;&#21644;&#27169;&#31946;&#31070;&#32463;&#32593;&#32476;&#25968;&#23398;&#26041;&#38754;&#30340;&#25112;&#30053;&#30740;&#31350;&#20249;&#20276;&#20851;&#31995;&#39033;&#30446;&#8221;&#20250;&#35758;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This short report reviews the current state of the research and methodology on theoretical and practical aspects of Artificial Neural Networks (ANN). It was prepared to gather state-of-the-art knowledge needed to construct complex, hypercomplex and fuzzy neural networks.  The report reflects the individual interests of the authors and, by now means, cannot be treated as a comprehensive review of the ANN discipline. Considering the fast development of this field, it is currently impossible to do a detailed review of a considerable number of pages.  The report is an outcome of the Project 'The Strategic Research Partnership for the mathematical aspects of complex, hypercomplex and fuzzy neural networks' meeting at the University of Warmia and Mazury in Olsztyn, Poland, organized in September 2022.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39564;&#35777;&#38598;&#30340;&#26089;&#20572;&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#20219;&#21153;&#36866;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#20855;&#26377;&#25913;&#36827;&#20316;&#29992;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#23545;&#20110;&#25552;&#39640;&#20027;&#21160;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.11680</link><description>&lt;p&gt;
&#24179;&#31283;&#33322;&#34892;&#65306;&#29992;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis. (arXiv:2212.11680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39564;&#35777;&#38598;&#30340;&#26089;&#20572;&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#20219;&#21153;&#36866;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#20855;&#26377;&#25913;&#36827;&#20316;&#29992;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#23545;&#20110;&#25552;&#39640;&#20027;&#21160;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26631;&#27880;&#22797;&#26434;&#24615;&#65292;&#20197;&#32531;&#35299;&#26114;&#36149;&#30340;&#26631;&#27880;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;AL&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#32467;&#21512;&#20351;&#29992;&#20855;&#26377;&#30410;&#22788;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#24433;&#21709;AL&#25928;&#26524;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;AL&#26082;&#26377;&#25928;&#21448;&#23454;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#39564;&#35777;&#38598;&#30340;&#26089;&#20572;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;AL&#26041;&#27861;&#20013;&#23545;&#27604;&#38543;&#26426;&#25277;&#26679;&#65292;&#35266;&#23519;&#21040;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20219;&#21153;&#36866;&#24212;&#25913;&#36827;&#20102;AL&#65292;&#32780;&#26631;&#20934;&#30340;&#30701;&#26399;&#24494;&#35843;&#22312;AL&#20013;&#24182;&#27809;&#26377;&#27604;&#38543;&#26426;&#25277;&#26679;&#25552;&#20379;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#22312;AL&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20943;&#23569;&#20027;&#21160;&#23398;&#20064;&#20572;&#27490;&#26631;&#20934;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developed to alleviate prohibitive labeling costs, active learning (AL) methods aim to reduce label complexity in supervised learning. While recent work has demonstrated the benefit of using AL in combination with large pre-trained language models (PLMs), it has often overlooked the practical challenges that hinder the effectiveness of AL. We address these challenges by leveraging representation smoothness analysis to ensure AL is feasible, that is, both effective and practicable. Firstly, we propose an early stopping technique that does not require a validation set -- often unavailable in realistic AL conditions -- and observe significant improvements over random sampling across multiple datasets and AL methods. Further, we find that task adaptation improves AL, whereas standard short fine-tuning in AL does not provide improvements over random sampling. Our work demonstrates the usefulness of representation smoothness analysis for AL and introduces an AL stopping criterion that reduce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2212.09702</link><description>&lt;p&gt;
&#35770;&#25991;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09702
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#22312;&#22788;&#29702;&#25972;&#20010;&#25991;&#20214;&#26041;&#38754;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#20256;&#32479;&#30340;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#20316;&#20026;&#25991;&#20214;&#32423;&#20449;&#24687;&#25552;&#21462;&#30340;&#22522;&#20934;&#20219;&#21153;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#22312;&#36825;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#35813;&#20219;&#21153;&#35201;&#27714;&#23545;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;&#25552;&#20379;&#26126;&#30830;&#30340;&#31572;&#26696;&#8212;&#8212;&#21363;&#21306;&#20998;&#19981;&#21516;&#30340;&#20107;&#20214;&#8212;&#8212;&#32780;&#21363;&#20351;&#26159;&#20154;&#31867;&#19987;&#23478;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#20063;&#23384;&#22312;&#20998;&#27495;&#12290;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of template filling has seen renewed interest as benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of event individuation -- the problem of distinguishing distinct events -- about which even human experts disagree. Through an annotation study and error analysis, we show that this raises concerns about the usefulness of template filling metrics, the quality of datasets for the task, and the ability of models to learn it. Finally, we consider possible solutions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#20041;&#26799;&#24230;&#27969;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#22266;&#23450;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#38750;&#20984;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#36867;&#36920;&#38750;&#36864;&#21270;&#38797;&#28857;&#12290;</title><link>http://arxiv.org/abs/2212.03765</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#22266;&#23450;&#26102;&#38388;&#25910;&#25947;&#21644;&#24555;&#36895;&#36867;&#36920;&#38750;&#36864;&#21270;&#38797;&#28857;&#30340;&#24191;&#20041;&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Generalized Gradient Flows with Provable Fixed-Time Convergence and Fast Evasion of Non-Degenerate Saddle Points. (arXiv:2212.03765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#20041;&#26799;&#24230;&#27969;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#22266;&#23450;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#38750;&#20984;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#36867;&#36920;&#38750;&#36864;&#21270;&#38797;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#19968;&#38454;&#20984;&#20248;&#21270;&#31639;&#27861;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#21463;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#22266;&#23450;&#26102;&#38388;&#31283;&#23450;&#24615;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#26368;&#24378;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#21152;&#36895;&#20248;&#21270;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#36827;&#19968;&#27493;&#36866;&#29992;&#20110;&#38750;&#20984;&#20989;&#25968;&#30340;&#23376;&#31867;&#12290;&#25105;&#20204;&#29305;&#21035;&#20171;&#32461;&#20102;GenFlow&#31639;&#27861;&#21450;&#20854;&#21160;&#37327;&#21464;&#20307;&#65292;&#23427;&#20204;&#21487;&#35777;&#26126;&#22312;&#22266;&#23450;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#28385;&#36275;Polyak-Lojasiewicz (PL)&#19981;&#31561;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20855;&#26377;&#38750;&#36864;&#21270;&#38797;&#28857;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25552;&#20986;&#30340;GenFlow&#31639;&#27861;&#65292;&#36530;&#36991;&#36825;&#20123;&#38797;&#28857;&#25152;&#38656;&#30340;&#26102;&#38388;&#22312;&#25152;&#26377;&#21021;&#22987;&#26465;&#20214;&#19979;&#37117;&#26377;&#19968;&#33268;&#30340;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#26368;&#20248;&#35299;&#20026;&#38797;&#28857;&#30340;&#24378;&#20984;-&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#31867;&#20284;&#30340;&#26041;&#26696;&#34987;&#35777;&#26126;&#21487;&#20197;&#36798;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based first-order convex optimization algorithms find widespread applicability in a variety of domains, including machine learning tasks. Motivated by the recent advances in fixed-time stability theory of continuous-time dynamical systems, we introduce a generalized framework for designing accelerated optimization algorithms with strongest convergence guarantees that further extend to a subclass of non-convex functions. In particular, we introduce the GenFlow algorithm and its momentum variant that provably converge to the optimal solution of objective functions satisfying the Polyak-{\L}ojasiewicz (PL) inequality in a fixed time. Moreover, for functions that admit non-degenerate saddle-points, we show that for the proposed GenFlow algorithm, the time required to evade these saddle-points is uniformly bounded for all initial conditions. Finally, for strongly convex-strongly concave minimax problems whose optimal solution is a saddle point, a similar scheme is shown to arrive a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#24310;&#36831;&#36890;&#20449;&#32593;&#32476;&#30340;&#21512;&#20316;&#38750;&#38543;&#26426;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#22312;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#36890;&#20449;&#21327;&#35758;&#19979;&#65292;&#37319;&#29992;"FTRL"&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#30340;&#26368;&#23567;&#21270;&#65292;&#19988;&#20855;&#26377;&#21518;&#24724;&#26368;&#20248;&#24615;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#24182;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.17154</link><description>&lt;p&gt;
&#20851;&#20110;&#21518;&#24724;&#26368;&#23567;&#30340;&#21512;&#20316;&#38750;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Regret-optimal Cooperative Nonstochastic Multi-armed Bandits. (arXiv:2211.17154v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#24310;&#36831;&#36890;&#20449;&#32593;&#32476;&#30340;&#21512;&#20316;&#38750;&#38543;&#26426;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#22312;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#36890;&#20449;&#21327;&#35758;&#19979;&#65292;&#37319;&#29992;"FTRL"&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#30340;&#26368;&#23567;&#21270;&#65292;&#19988;&#20855;&#26377;&#21518;&#24724;&#26368;&#20248;&#24615;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#24182;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#24310;&#36831;&#36890;&#20449;&#32593;&#32476;&#30340;&#21512;&#20316;&#38750;&#38543;&#26426;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#20010;&#20307;&#21518;&#24724;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#24403;&#33218;&#30340;&#25968;&#37327;&#30456;&#23545;&#20110;&#36890;&#20449;&#22270;&#20013;&#26234;&#33021;&#20307;&#30340;&#24230;&#25968;&#36275;&#22815;&#22823;&#26102;&#65292;&#37319;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#36890;&#20449;&#21327;&#35758;&#65292;&#21512;&#20316;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;"FTRL"&#31639;&#27861;&#30340;&#20010;&#20307;&#21518;&#24724;&#19978;&#30028;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#26368;&#22810;&#20165;&#30456;&#24046;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20855;&#26377;&#36866;&#24403;&#27491;&#21017;&#21270;&#22120;&#30340;"FTRL"&#31639;&#27861;&#30456;&#23545;&#20110;&#36793;&#24310;&#36831;&#21442;&#25968;&#30340;&#32553;&#25918;&#20855;&#26377;&#21518;&#24724;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#20808;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the nonstochastic multi-agent multi-armed bandit problem with agents collaborating via a communication network with delays. We show a lower bound for individual regret of all agents. We show that with suitable regularizers and communication protocols, a collaborative multi-agent \emph{follow-the-regularized-leader} (FTRL) algorithm has an individual regret upper bound that matches the lower bound up to a constant factor when the number of arms is large enough relative to degrees of agents in the communication graph. We also show that an FTRL algorithm with a suitable regularizer is regret optimal with respect to the scaling with the edge-delay parameter. We present numerical experiments validating our theoretical results and demonstrate cases when our algorithms outperform previously proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26412;&#22320;GAN&#24182;&#19978;&#20256;&#21512;&#25104;&#25968;&#25454;&#21040;&#26381;&#21153;&#22120;&#26469;&#35299;&#20915;&#20256;&#32479;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16965</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#28145;&#24230;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Federated Deep Clustering based on GAN. (arXiv:2211.16965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26412;&#22320;GAN&#24182;&#19978;&#20256;&#21512;&#25104;&#25968;&#25454;&#21040;&#26381;&#21153;&#22120;&#26469;&#35299;&#20915;&#20256;&#32479;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#32858;&#31867;&#26159;&#38024;&#23545;&#32852;&#37030;&#29615;&#22659;&#35774;&#35745;&#30340;&#20013;&#24515;&#21270;&#32858;&#31867;&#30340;&#37325;&#35201;&#25193;&#23637;&#65292;&#20854;&#25361;&#25112;&#22312;&#20110;&#26500;&#24314;&#19968;&#20010;&#20840;&#23616;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20013;&#24515;&#21270;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#22914;K&#22343;&#20540;&#21644;&#27169;&#31946;C&#22343;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#23458;&#25143;&#31471;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#28145;&#24230;&#32858;&#31867;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#35757;&#32451;&#19968;&#20010;&#26412;&#22320;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#24182;&#23558;&#21512;&#25104;&#25968;&#25454;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#12290;&#26381;&#21153;&#22120;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#32593;&#32476;&#20197;&#24314;&#31435;k&#20010;&#32858;&#31867;&#20013;&#24515;&#65292;&#28982;&#21518;&#23558;&#20854;&#19979;&#36733;&#21040;&#23458;&#25143;&#31471;&#36827;&#34892;&#32858;&#31867;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated clustering (FC) is an essential extension of centralized clustering designed for the federated setting, wherein the challenge lies in constructing a global similarity measure without the need to share private data. Conventional approaches to FC typically adopt extensions of centralized methods, like K-means and fuzzy c-means. However, these methods are susceptible to non-independent-and-identically-distributed (non-IID) data among clients, leading to suboptimal performance, particularly with high-dimensional data. In this paper, we present a novel approach to address these limitations by proposing a Privacy-Preserving Federated Deep Clustering based on Generative Adversarial Networks (GANs). Each client trains a local generative adversarial network (GAN) locally and uploads the synthetic data to the server. The server applies a deep clustering network on the synthetic data to establish $k$ cluster centroids, which are then downloaded to the clients for cluster assignment. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;&#20998;&#31163;&#31209;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;&#65292;&#21363;&#20174;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;WISA&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#20197;&#25552;&#39640;GNNs&#30340;&#22788;&#29702;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16494</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Ability of Graph Neural Networks to Model Interactions Between Vertices. (arXiv:2211.16494v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;&#20998;&#31163;&#31209;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;&#65292;&#21363;&#20174;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;WISA&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#20197;&#25552;&#39640;GNNs&#30340;&#22788;&#29702;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#30001;&#22270;&#20013;&#39030;&#28857;&#34920;&#31034;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#29702;&#35770;&#20998;&#26512;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#21162;&#21147;&#65292;&#20294;&#23545;&#20854;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#32570;&#20047;&#19968;&#20010;&#27491;&#24335;&#30340;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#19968;&#20010;&#24050;&#30693;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#20998;&#31163;&#31209;(separation rank)&#26469;&#35268;&#33539;&#21270;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#26576;&#20123;GNNs&#27169;&#25311;&#32473;&#23450;&#39030;&#28857;&#23376;&#38598;&#21450;&#20854;&#34917;&#38598;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21363;&#36755;&#20837;&#39030;&#28857;&#32452;&#25104;&#30340;&#32473;&#23450;&#20998;&#21306;&#30340;&#20004;&#20391;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;(walk index)&#8212;&#8212;&#19968;&#20010;&#30001;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#23450;&#20041;&#30340;&#22270;&#24418;&#29305;&#24449;&#12290;&#24120;&#35265;GNN&#26550;&#26500;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#20316;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Walk Indexed Sparsification Algorithm (WISA)&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#39640;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;GNNs&#25928;&#29575;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Inde
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21487;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23398;&#20064;&#20219;&#24847;&#31283;&#23450;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#20026;&#20854;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#28436;&#31034;&#20102;&#24490;&#29615;&#32467;&#26500;&#22914;&#20309;&#24110;&#21161;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2211.10582</link><description>&lt;p&gt;
&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#35777;&#26126;&#24615;&#22320;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Linear RNNs Provably Learn Linear Dynamic Systems. (arXiv:2211.10582v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21487;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23398;&#20064;&#20219;&#24847;&#31283;&#23450;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#20026;&#20854;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#28436;&#31034;&#20102;&#24490;&#29615;&#32467;&#26500;&#22914;&#20309;&#24110;&#21161;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23545;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#29702;&#35770;&#20445;&#35777;&#65292;&#35777;&#26126;&#20102;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#22823;&#33539;&#22260;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#20219;&#20309;&#31283;&#23450;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#12290;&#23545;&#20110;&#20219;&#24847;&#31283;&#23450;&#30340;&#32447;&#24615;&#31995;&#32479;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#65292;&#23613;&#31649;&#21442;&#25968;&#20248;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#21482;&#35201;RNN&#30340;&#23485;&#24230;&#36275;&#22815;&#22823;&#65288;&#24182;&#19988;&#38544;&#34255;&#23618;&#25152;&#38656;&#30340;&#23485;&#24230;&#19981;&#20381;&#36182;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#38271;&#24230;&#65289;&#65292;&#21017;&#32447;&#24615;RNN&#21487;&#20197;&#35777;&#26126;&#22320;&#23398;&#20064;&#20219;&#20309;&#31283;&#23450;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#20854;&#26679;&#26412;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#22320;&#19982;$\frac{1}{1-\rho_C}$&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#23398;&#20064;&#32447;&#24615;RNN&#30340;&#39318;&#20010;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#28436;&#31034;&#20102;&#24490;&#29615;&#32467;&#26500;&#22914;&#20309;&#24110;&#21161;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the learning ability of linear recurrent neural networks with Gradient Descent. We prove the first theoretical guarantee on linear RNNs to learn any stable linear dynamic system using any a large type of loss functions. For an arbitrary stable linear system with a parameter $\rho_C$ related to the transition matrix $C$, we show that despite the non-convexity of the parameter optimization loss if the width of the RNN is large enough (and the required width in hidden layers does not rely on the length of the input sequence), a linear RNN can provably learn any stable linear dynamic system with the sample and time complexity polynomial in $\frac{1}{1-\rho_C}$. Our results provide the first theoretical guarantee to learn a linear RNN and demonstrate how can the recurrent structure help to learn a dynamic system.
&lt;/p&gt;</description></item><item><title>CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09935</link><description>&lt;p&gt;
CAPE: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09935
&lt;/p&gt;
&lt;p&gt;
CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24120;&#35782;&#30693;&#35782;&#20026;&#35774;&#35745;&#26234;&#33021;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#22312;&#34892;&#21160;&#22833;&#36133;&#26102;&#26080;&#27861;&#24674;&#22797;&#65292;&#24182;&#19988;&#36890;&#24120;&#21482;&#33021;&#23581;&#35797;&#37325;&#26032;&#25191;&#34892;&#22833;&#36133;&#30340;&#34892;&#21160;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65288;CAPE&#65289;&#65292;&#35797;&#22270;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#25552;&#20986;&#32416;&#27491;&#21069;&#32622;&#26465;&#20214;&#38169;&#35823;&#30340;&#34892;&#21160;&#12290;CAPE&#36890;&#36807;&#21033;&#29992;&#23569;&#26679;&#26412;&#25512;&#29702;&#20174;&#34892;&#21160;&#21069;&#32622;&#26465;&#20214;&#20013;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#30830;&#20445;&#35821;&#20041;&#27491;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#37325;&#26032;&#25552;&#31034;&#12290;&#22312;VirtualHome&#20013;&#65292;CAPE&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#24182;&#19988;&#30456;&#27604;SayCan&#65292;&#23558;&#20154;&#24037;&#26631;&#27880;&#30340;&#35745;&#21010;&#27491;&#30830;&#24230;&#25351;&#26631;&#20174;28.89%&#25552;&#39640;&#21040;49.63%&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#20063;&#36866;&#29992;&#20110;&#19968;&#21488;&#37197;&#32622;&#20102;&#19968;&#32452;&#20197;&#35821;&#35328;&#20026;&#25351;&#23450;&#30340;&#25216;&#33021;&#21644;&#30456;&#20851;&#21069;&#32622;&#26465;&#20214;&#30340;&#27874;&#22763;&#39039;&#21160;&#21147;&#20844;&#21496;&#30340;Spot&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;CAPE&#25552;&#39640;&#20102;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.  We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.07482</link><description>&lt;p&gt;
&#29992;&#24352;&#37327;&#32593;&#32476;&#24418;&#24335;&#32479;&#19968;O(3)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism. (arXiv:2211.07482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20013;&#23398;&#20064;&#21183;&#33021;&#38754;&#65292;&#28041;&#21450;&#21040;&#20840;&#23616;&#31354;&#38388;&#23545;&#31216;&#24615;&#21644;&#21407;&#23376;&#25110;&#19968;&#33324;&#31890;&#23376;&#20043;&#38388;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#12290;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#20043;&#19968;&#65292;&#20854;&#20013;&#26368;&#25104;&#21151;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;&#22312;&#31354;&#38388;&#32676;&#19979;&#21464;&#25442;&#30340;&#21508;&#31181;&#24352;&#37327;&#20043;&#38388;&#30340;&#24352;&#37327;&#31215;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#19981;&#21516;&#24352;&#37327;&#30340;&#25968;&#37327;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20445;&#25345;&#31616;&#27905;&#21644;&#31561;&#21464;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;SU(2)&#23545;&#31216;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#30340;&#25216;&#26415;&#65292;&#26469;&#20026;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#26032;&#30340;&#31561;&#21464;&#32452;&#20214;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#24403;&#24212;&#29992;&#20110;&#32473;&#23450;&#23616;&#37096;&#37051;&#22495;&#20013;&#30340;&#31890;&#23376;&#26102;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#34701;&#21512;&#22359;&#8221;&#30340;&#32467;&#26524;&#32452;&#20214;&#36215;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
Many learning tasks, including learning potential energy surfaces from ab initio calculations, involve global spatial symmetries and permutational symmetry between atoms or general particles. Equivariant graph neural networks are a standard approach to such problems, with one of the most successful methods employing tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increase, maintaining parsimony and equivariance becomes increasingly challenging. In this paper, we propose using fusion diagrams, a technique widely employed in simulating SU($2$)-symmetric quantum many-body problems, to design new equivariant components for equivariant neural networks. This results in a diagrammatic approach to constructing novel neural network architectures. When applied to particles within a given local neighborhood, the resulting components, which we term "fusion blocks," serve as 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#32858;&#31867;&#26694;&#26550;SDA-FC&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#23616;&#37096;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#24182;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#32858;&#31867;&#20013;&#38750;I-I-D&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.16524</link><description>&lt;p&gt;
&#22522;&#20110;GAN&#25968;&#25454;&#21512;&#25104;&#30340;&#32852;&#37030;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Federated clustering with GAN-based data synthesis. (arXiv:2210.16524v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16524
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#32858;&#31867;&#26694;&#26550;SDA-FC&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#23616;&#37096;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#24182;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#32858;&#31867;&#20013;&#38750;I-I-D&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#32858;&#31867;&#65288;FC&#65289;&#26159;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#23545;&#38598;&#20013;&#24335;&#32858;&#31867;&#30340;&#25193;&#23637;&#12290;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#26500;&#24314;&#20840;&#23616;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#32780;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#65292;&#22240;&#20026;&#23616;&#37096;&#30456;&#20284;&#24615;&#21487;&#33021;&#19981;&#36275;&#20197;&#27491;&#30830;&#20998;&#32452;&#23616;&#37096;&#25968;&#25454;&#65292;&#32780;&#19988;&#30001;&#20110;&#38544;&#31169;&#32422;&#26463;&#65292;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26679;&#26412;&#30456;&#20284;&#24615;&#12290;&#26174;&#28982;&#65292;&#20998;&#26512;FC&#26368;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#37319;&#29992;&#20174;&#38598;&#20013;&#24335;&#26041;&#27861;&#25193;&#23637;&#32780;&#26469;&#30340;&#26041;&#27861;&#65292;&#22914;K&#22343;&#20540;&#65288;KM&#65289;&#21644;&#27169;&#31946;C&#22343;&#20540;&#65288;FCM&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#26159;&#33030;&#24369;&#30340;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#32858;&#31867;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#36741;&#21161;&#30340;&#32852;&#37030;&#32858;&#31867;&#65288;SDA-FC&#65289;&#12290;&#23427;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#23616;&#37096;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#24182;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;KM&#25110;FCM&#12290;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20351;&#27169;&#22411;&#20813;&#30123;&#20110;&#38750;-IID&#38382;&#39064;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#32852;&#37030;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated clustering (FC) is an extension of centralized clustering in federated settings. The key here is how to construct a global similarity measure without sharing private data, since the local similarity may be insufficient to group local data correctly and the similarity of samples across clients cannot be directly measured due to privacy constraints. Obviously, the most straightforward way to analyze FC is to employ the methods extended from centralized ones, such as K-means (KM) and fuzzy c-means (FCM). However, they are vulnerable to non independent-and-identically-distributed (non-IID) data among clients. To handle this, we propose a new federated clustering framework, named synthetic data aided federated clustering (SDA-FC). It trains generative adversarial network locally in each client and uploads the generated synthetic data to the server, where KM or FCM is performed on the synthetic data. The synthetic data can make the model immune to the non-IID problem and enable us 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20197;&#23454;&#29616;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.15500</link><description>&lt;p&gt;
COFFEE: &#21487;&#35299;&#37322;&#24615;&#25512;&#33616;&#20013;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20197;&#23454;&#29616;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#25968;&#23383;&#29983;&#27963;&#20013;&#65292;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65288;PTG&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;PTG&#27169;&#22411;&#35757;&#32451;&#30340;&#29992;&#25143;&#32534;&#20889;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19981;&#21516;&#27700;&#24179;&#30340;&#35821;&#35328;&#36136;&#37327;&#19982;&#29992;&#25143;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#20851;&#32852;&#36215;&#26469;&#12290;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#22312;&#29983;&#25104;&#19982;&#29992;&#25143;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#26102;&#24310;&#32493;&#19981;&#24179;&#31561;&#65292;&#23548;&#33268;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#20986;&#29616;&#19981;&#20844;&#24179;&#30340;&#23545;&#24453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;PTG&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#29983;&#25104;&#30340;&#35299;&#37322;&#20013;&#30340;&#20559;&#35265;&#21450;&#20854;&#20844;&#24179;&#24615;&#24433;&#21709;&#12290;&#20026;&#20102;&#20419;&#36827;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users' protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users' protected attributes, leading to unfair treatment when serving users. In this work, we investigate fairness of PTG in the context of personalized explanation generation for recommendations. We first discuss the biases in generated explanations and their fairness implications. To promote fairness, we introduce a general framework to achieve measure-specific counterfactual fairness in explanation generation. Extensive experiments and human evaluations demonstrate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12770</link><description>&lt;p&gt;
&#20851;&#20110;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#25968;&#25454;&#21463;&#38480;&#24494;&#35843;&#20013;&#23427;&#20204;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20351;&#29992;&#20174;&#19968;&#33324;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23558;&#20854;&#24494;&#35843;&#21040;&#29305;&#23450;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#65292;&#24182;&#20351;&#29992;&#26032;&#20219;&#21153;&#20013;&#21487;&#29992;&#30340;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#24494;&#35843;&#65292;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#23454;&#36341;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20855;&#20307;&#26159;&#22312;&#33647;&#29289;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;Transformer&#27169;&#22411;&#21644;&#36890;&#36807;&#24494;&#35843;BERT-based LLMs&#65288;&#21253;&#25324;BERT-base&#12289;BioBERT&#21644;ClinicalBERT&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#25193;&#23637;&#27169;&#22411;&#19982;&#24102;&#26377;CRF&#23618;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;CRF&#23618;&#23545;&#25152;&#26377;&#31070;&#32463;&#27169;&#22411;&#37117;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#24433;&#21709;&#65307;2&#65289;&#22312;&#20351;&#29992;&#23439;&#24179;&#22343;F1&#23545;BIO-strict&#36328;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#24494;&#35843;&#30340;LLMs&#33719;&#24471;&#20102;0.83+&#30340;&#24471;&#20998;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;TransformerCRF&#27169;&#22411;&#24471;&#20998;&#20026;0.78+&#65292;&#35777;&#26126;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#20004;&#23618;&#20379;&#24212;&#38142;&#20013;&#26410;&#30693;&#38656;&#27714;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#38598;&#20013;&#24335;&#21644;&#20998;&#25955;&#24335;&#35774;&#32622;&#19979;&#37117;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#24211;&#23384;&#20915;&#31574;&#30340;&#36951;&#25022;&#21644;&#25910;&#25947;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.12663</link><description>&lt;p&gt;
&#26410;&#30693;&#38656;&#27714;&#20998;&#24067;&#30340;&#20004;&#23618;&#20379;&#24212;&#38142;&#20013;&#30340;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
No-Regret Learning in Two-Echelon Supply Chain with Unknown Demand Distribution. (arXiv:2210.12663v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#20004;&#23618;&#20379;&#24212;&#38142;&#20013;&#26410;&#30693;&#38656;&#27714;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#38598;&#20013;&#24335;&#21644;&#20998;&#25955;&#24335;&#35774;&#32622;&#19979;&#37117;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#24211;&#23384;&#20915;&#31574;&#30340;&#36951;&#25022;&#21644;&#25910;&#25947;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20379;&#24212;&#38142;&#31649;&#29702;(SCM)&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19982;&#35768;&#22810;&#34892;&#19994;&#26377;&#20851;&#30340;&#37325;&#35201;&#23398;&#31185;&#65292;&#22312;&#36825;&#20010;&#23398;&#31185;&#20013;&#65292;&#28041;&#21450;&#19979;&#28216;&#38646;&#21806;&#21830;&#21644;&#19978;&#28216;&#20379;&#24212;&#21830;&#30340;&#20004;&#23618;&#38543;&#26426;&#24211;&#23384;&#27169;&#22411;&#23545;&#20110;&#21046;&#23450;&#20844;&#21496;&#30340;&#20379;&#24212;&#38142;&#31649;&#29702;&#31574;&#30053;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#38024;&#23545;&#20855;&#26377;&#26410;&#30693;&#38656;&#27714;&#20998;&#24067;&#30340;&#36825;&#19968;&#38382;&#39064;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#19982;&#32463;&#20856;&#30340;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#30456;&#27604;&#65292;&#36825;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#29305;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;[Cachon&#21644;Zipkin,1999]&#24341;&#20837;&#30340;&#20004;&#23618;&#20379;&#24212;&#38142;&#27169;&#22411;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#65306;&#38598;&#20013;&#24335;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#20010;&#35745;&#21010;&#32773;&#21516;&#26102;&#20915;&#23450;&#20004;&#20010;&#20195;&#29702;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#20998;&#25955;&#24335;&#35774;&#32622;&#65292;&#20854;&#20013;&#20004;&#20010;&#20195;&#29702;&#29420;&#31435;&#33258;&#31169;&#22320;&#20915;&#23450;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#31639;&#27861;&#65292;&#22312;&#20004;&#31181;&#35774;&#32622;&#19979;&#37117;&#23454;&#29616;&#20102;&#23545;&#36951;&#25022;&#21644;&#25910;&#25947;&#21040;&#26368;&#20248;&#24211;&#23384;&#20915;&#31574;&#30340;&#26377;&#21033;&#20445;&#35777;&#65292;&#21516;&#26102;&#36824;&#38024;&#23545;&#20998;&#25955;&#24335;&#35774;&#32622;&#19979;&#30340;&#20010;&#20154;&#36951;&#25022;&#25552;&#20379;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain management (SCM) has been recognized as an important discipline with applications to many industries, where the two-echelon stochastic inventory model, involving one downstream retailer and one upstream supplier, plays a fundamental role for developing firms' SCM strategies. In this work, we aim at designing online learning algorithms for this problem with an unknown demand distribution, which brings distinct features as compared to classic online optimization problems. Specifically, we consider the two-echelon supply chain model introduced in [Cachon and Zipkin, 1999] under two different settings: the centralized setting, where a planner decides both agents' strategy simultaneously, and the decentralized setting, where two agents decide their strategy independently and selfishly. We design algorithms that achieve favorable guarantees for both regret and convergence to the optimal inventory decision in both settings, and additionally for individual regret in the decentrali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#30340;&#32467;&#26500;&#21270;&#33258;&#36866;&#24212;&#28145;&#24230;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#20540;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#24314;&#27169;&#12290;&#24212;&#29992;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#21487;&#20197;&#24471;&#21040;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20855;&#26377;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#29305;&#24449;&#25353;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#24207;&#12290;&#22312;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25130;&#26029;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#34920;&#31034;&#38271;&#24230;&#27604;&#39046;&#20808;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30701;16&#20493;&#65292;&#21516;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12637</link><description>&lt;p&gt;
&#31070;&#32463;&#29305;&#24449;&#21521;&#37327;&#26159;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Eigenfunctions Are Structured Representation Learners. (arXiv:2210.12637v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#30340;&#32467;&#26500;&#21270;&#33258;&#36866;&#24212;&#28145;&#24230;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#20540;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#24314;&#27169;&#12290;&#24212;&#29992;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#21487;&#20197;&#24471;&#21040;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20855;&#26377;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#29305;&#24449;&#25353;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#24207;&#12290;&#22312;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25130;&#26029;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#34920;&#31034;&#38271;&#24230;&#27604;&#39046;&#20808;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30701;16&#20493;&#65292;&#21516;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#30340;&#32467;&#26500;&#21270;&#33258;&#36866;&#24212;&#28145;&#24230;&#34920;&#31034;&#12290;&#19982;&#20808;&#21069;&#30340;&#35889;&#26041;&#27861;&#65288;&#22914;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#65289;&#20197;&#38750;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25805;&#20316;&#19981;&#21516;&#65292;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#20540;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#29305;&#24449;&#20540;&#20989;&#25968;&#26469;&#33258;&#20110;&#25968;&#25454;&#25193;&#22686;&#35774;&#32622;&#20013;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#26102;&#65292;&#24212;&#29992;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#20250;&#20135;&#29983;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#29305;&#24449;&#25353;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#20013;&#28436;&#31034;&#20102;&#20351;&#29992;&#36825;&#26679;&#30340;&#33258;&#36866;&#24212;&#38271;&#24230;&#32534;&#30721;&#26469;&#34920;&#31034;&#12290;&#36890;&#36807;&#26681;&#25454;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25130;&#26029;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#34920;&#31034;&#38271;&#24230;&#27604;&#39046;&#20808;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30701;16&#20493;&#65292;&#21516;&#26102;&#36798;&#21040;&#30456;&#20284;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#24418;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a structured, adaptive-length deep representation called Neural Eigenmap. Unlike prior spectral methods such as Laplacian Eigenmap that operate in a nonparametric manner, Neural Eigenmap leverages NeuralEF to parametrically model eigenfunctions using a neural network. We show that, when the eigenfunction is derived from positive relations in a data augmentation setup, applying NeuralEF results in an objective function that resembles those of popular self-supervised learning methods, with an additional symmetry-breaking property that leads to structured representations where features are ordered by importance. We demonstrate using such representations as adaptive-length codes in image retrieval systems. By truncation according to feature importance, our method requires up to $16\times$ shorter representation length than leading self-supervised learning ones to achieve similar retrieval performance. We further apply our method to graph data and report strong results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32858;&#31867;&#32452;&#21512;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#22823;&#22411;&#23884;&#20837;&#34920;&#22312;&#20869;&#23384;&#20013;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#21387;&#32553;&#26041;&#27861;&#21644;&#21160;&#24577;&#26041;&#27861;&#65292;&#26082;&#20855;&#26377;&#39640;&#21387;&#32553;&#29575;&#21448;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#30721;&#26412;&#65292;&#24182;&#32473;&#20986;&#20102;&#36845;&#20195;&#27425;&#25968;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2210.05974</link><description>&lt;p&gt;
&#32858;&#31867;&#33609;&#22270;&#65306;&#23884;&#20837;&#34920;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Clustering the Sketch: A Novel Approach to Embedding Table Compression. (arXiv:2210.05974v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32858;&#31867;&#32452;&#21512;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#22823;&#22411;&#23884;&#20837;&#34920;&#22312;&#20869;&#23384;&#20013;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#21387;&#32553;&#26041;&#27861;&#21644;&#21160;&#24577;&#26041;&#27861;&#65292;&#26082;&#20855;&#26377;&#39640;&#21387;&#32553;&#29575;&#21448;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#30721;&#26412;&#65292;&#24182;&#32473;&#20986;&#20102;&#36845;&#20195;&#27425;&#25968;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#34920;&#26159;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#22788;&#29702;&#20998;&#31867;&#29305;&#24449;&#30340;&#24037;&#20855;&#12290;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#34920;&#21487;&#33021;&#38750;&#24120;&#24222;&#22823;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#23558;&#23427;&#20204;&#35013;&#20837;&#20869;&#23384;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#32858;&#31867;&#32452;&#21512;&#23884;&#20837;&#65288;Clustered Compositional Embeddings&#65292;CCE&#65289;&#26041;&#27861;&#65292;&#23427;&#23558;&#22522;&#20110;&#32858;&#31867;&#30340;&#21387;&#32553;&#26041;&#27861;&#65288;&#22914;&#37327;&#21270;&#21040;&#30721;&#26412;&#65289;&#19982;&#21160;&#24577;&#26041;&#27861;&#65288;&#22914;&#25955;&#21015;&#25216;&#24039;&#21644;&#32452;&#21512;&#23884;&#20837;&#65289;&#32467;&#21512;&#36215;&#26469;&#65288;Shi&#31561;&#20154;&#65292;2020&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CCE&#22312;&#20004;&#20010;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20339;&#25928;&#26524;&#65306;&#21363;&#22522;&#20110;&#30721;&#26412;&#30340;&#37327;&#21270;&#20855;&#26377;&#39640;&#21387;&#32553;&#29575;&#65292;&#20294;&#20687;&#22522;&#20110;&#25955;&#21015;&#30340;&#26041;&#27861;&#19968;&#26679;&#21160;&#24577;&#65292;&#22240;&#27492;&#21487;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;CCE&#19968;&#23450;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#30721;&#26412;&#65292;&#24182;&#32473;&#20986;&#20102;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#30340;&#32039;&#23494;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding tables are used by machine learning systems to work with categorical features. In modern Recommendation Systems, these tables can be very large, necessitating the development of new methods for fitting them in memory, even during training. We suggest Clustered Compositional Embeddings (CCE) which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and Compositional Embeddings (Shi et al., 2020). Experimentally CCE achieves the best of both worlds: The high compression rate of codebook-based quantization, but *dynamically* like hashing-based methods, so it can be used during training. Theoretically, we prove that CCE is guaranteed to converge to the optimal codebook and give a tight bound for the number of iterations required.
&lt;/p&gt;</description></item><item><title>ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.05845</link><description>&lt;p&gt;
ConSpec: &#31361;&#20986;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05845
&lt;/p&gt;
&lt;p&gt;
ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#25104;&#21151;&#24448;&#24448;&#21462;&#20915;&#20110;&#22810;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#22312;&#26102;&#38388;&#19978;&#30456;&#36317;&#36739;&#36828;&#65292;&#19982;&#26368;&#32456;&#22870;&#21169;&#20063;&#30456;&#36317;&#29978;&#36828;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20381;&#36182;Bellman&#26041;&#31243;&#65292;&#24456;&#38590;&#35782;&#21035;&#36825;&#20123;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20851;&#38190;&#27493;&#39588;&#12290;&#36825;&#20010;&#31639;&#27861;&#34987;&#31216;&#20026;&#23545;&#27604;&#20869;&#30465;&#65288;ConSpec&#65289;&#65292;&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;ConSpec&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#19982;&#36825;&#20123;&#21407;&#22411;&#20043;&#19968;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#12290;ConSpec&#20013;&#30340;&#21407;&#22411;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;&#23427;&#20204;&#20351;&#24471;&#33021;&#22815;&#36805;&#36895;&#35782;&#21035;&#25152;&#26377;&#20851;&#38190;&#27493;&#39588;&#65307;&#65288;2&#65289;&#23427;&#20204;&#20197;&#23481;&#26131;&#35299;&#37322;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20351;&#24471;&#22312;&#24863;&#35273;&#29305;&#24449;&#25913;&#21464;&#26102;&#21487;&#20197;&#36827;&#34892;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#12290;&#19982;&#20854;&#20182;&#24403;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#26368;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#65288;&#32763;&#35793;&#20026;&#20013;&#25991;&#65289;</title><link>http://arxiv.org/abs/2210.03116</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Content-Based Search for Deep Generative Models. (arXiv:2210.03116v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03116
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#26368;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#65288;&#32763;&#35793;&#20026;&#20013;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23450;&#20041;&#21644;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#29992;&#25143;&#19981;&#21487;&#33021;&#23436;&#20840;&#20102;&#35299;&#27599;&#20010;&#23384;&#22312;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#27169;&#22411;&#25628;&#32034;&#20219;&#21153;&#65306;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#21644;&#19968;&#32452;&#22823;&#35268;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25214;&#21040;&#19982;&#26597;&#35810;&#26368;&#21305;&#37197;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#19968;&#31995;&#21015;&#22270;&#20687;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#23558;&#25628;&#32034;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#36873;&#25321;&#29983;&#25104;&#19982;&#26597;&#35810;&#30456;&#20284;&#20869;&#23481;&#27010;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#27010;&#29575;&#30340;&#20844;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#26597;&#35810;&#27169;&#24577;&#65288;&#20363;&#22914;&#22270;&#20687;&#12289;&#33609;&#22270;&#21644;&#25991;&#26412;&#65289;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27169;&#22411;&#26816;&#32034;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#26597;&#35810;&#27169;&#24577;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#27169;&#22411;&#21160;&#29289;&#22253;&#65288;Generative Model Zoo&#65289;&#19978;&#20248;&#20110;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing proliferation of customized and pretrained generative models has made it infeasible for a user to be fully cognizant of every model in existence. To address this need, we introduce the task of content-based model search: given a query and a large set of generative models, finding the models that best match the query. As each generative model produces a distribution of images, we formulate the search task as an optimization problem to select the model with the highest probability of generating similar content as the query. We introduce a formulation to approximate this probability given the query from different modalities, e.g., image, sketch, and text. Furthermore, we propose a contrastive learning framework for model retrieval, which learns to adapt features for various query modalities. We demonstrate that our method outperforms several baselines on Generative Model Zoo, a new benchmark we create for the model retrieval task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31561;&#21464;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#36229;&#20307;&#31215;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30446;&#26631;&#21644;&#25968;&#25454;&#28857;&#22686;&#21152;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.02177</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#36890;&#36807;&#31561;&#21464;&#28145;&#24230;&#36229;&#20307;&#31215;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Multi-objective optimization via equivariant deep hypervolume approximation. (arXiv:2210.02177v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31561;&#21464;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#36229;&#20307;&#31215;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30446;&#26631;&#21644;&#25968;&#25454;&#28857;&#22686;&#21152;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#39046;&#22495;&#65292;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#31454;&#20105;&#30340;&#30446;&#26631;&#26159;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#12290;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#20351;&#24471;&#25105;&#20204;&#38656;&#35201;&#25506;&#32034;&#23427;&#20204;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#23545;&#20110;&#21518;&#32773;&#32780;&#35328;&#65292;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#37327;&#26159;&#36229;&#20307;&#31215;&#25351;&#26631;&#65292;&#23427;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270; (BO) &#21644;&#36827;&#21270;&#31639;&#27861; (EA) &#20013;&#34987;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#30446;&#26631;&#25968;&#37327;&#21644;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#36229;&#20307;&#31215;&#30340;&#22797;&#26434;&#24230;&#19981;&#21033;&#22320;&#25193;&#23637;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#24120;&#35265;&#22810;&#30446;&#26631;&#20248;&#21270;&#26694;&#26550;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#36229;&#20307;&#31215;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;DeepHV&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27599;&#20010;&#30446;&#26631;&#19978;&#36229;&#20307;&#31215;&#30340;&#23610;&#24230;&#31561;&#21464;&#24615;&#20197;&#21450;&#23545;&#30446;&#26631;&#21644;&#26679;&#26412;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#31561;&#21464;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing multiple competing objectives is a common problem across science and industry. The inherent inextricable trade-off between those objectives leads one to the task of exploring their Pareto front. A meaningful quantity for the purpose of the latter is the hypervolume indicator, which is used in Bayesian Optimization (BO) and Evolutionary Algorithms (EAs). However, the computational complexity for the calculation of the hypervolume scales unfavorably with increasing number of objectives and data points, which restricts its use in those common multi-objective optimization frameworks. To overcome these restrictions we propose to approximate the hypervolume function with a deep neural network, which we call DeepHV. For better sample efficiency and generalization, we exploit the fact that the hypervolume is scale-equivariant in each of the objectives as well as permutation invariant w.r.t. both the objectives and the samples, by using a deep neural network that is equivariant w.r.t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#36229;&#32423;&#24378;&#21270;&#23398;&#20064;&#65292;&#23427;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#36807;&#21435;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#26410;&#25259;&#38706;&#20449;&#24687;&#30340;&#27934;&#35265;&#12290;&#36890;&#36807;&#20197;&#21512;&#27861;&#30340;&#26041;&#24335;&#23558;&#36825;&#20123;&#20449;&#24687;&#32435;&#20837;&#31574;&#30053;&#25628;&#32034;&#20013;&#65292;&#36229;&#32423;&#24378;&#21270;&#23398;&#20064;&#23558;&#24471;&#21040;&#19968;&#20010;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26631;&#20934;&#26368;&#20248;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#30340;&#36229;&#32423;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26356;&#24378;&#22823;&#30340;&#31070;&#35861;&#31216;&#20026;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#31119;&#38899;&#12290;</title><link>http://arxiv.org/abs/2209.15448</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#31119;&#38899;&#65306;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#30340;&#36229;&#32423;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Blessing from Human-AI Interaction: Super Reinforcement Learning in Confounded Environments. (arXiv:2209.15448v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#36229;&#32423;&#24378;&#21270;&#23398;&#20064;&#65292;&#23427;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#36807;&#21435;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#26410;&#25259;&#38706;&#20449;&#24687;&#30340;&#27934;&#35265;&#12290;&#36890;&#36807;&#20197;&#21512;&#27861;&#30340;&#26041;&#24335;&#23558;&#36825;&#20123;&#20449;&#24687;&#32435;&#20837;&#31574;&#30053;&#25628;&#32034;&#20013;&#65292;&#36229;&#32423;&#24378;&#21270;&#23398;&#20064;&#23558;&#24471;&#21040;&#19968;&#20010;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26631;&#20934;&#26368;&#20248;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#30340;&#36229;&#32423;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26356;&#24378;&#22823;&#30340;&#31070;&#35861;&#31216;&#20026;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#31119;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#20013;&#30340;&#26222;&#21450;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21457;&#25381;&#21508;&#33258;&#30340;&#20248;&#21183;&#24182;&#20943;&#23569;&#39118;&#38505;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#36229;&#32423;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#65288;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#65289;&#20316;&#20026;&#20915;&#31574;&#32773;&#65288;&#20154;&#31867;&#25110;&#20154;&#24037;&#26234;&#33021;&#65289;&#31574;&#30053;&#23398;&#20064;&#30340;&#26356;&#24378;&#22823;&#30340;&#31070;&#35861;&#36755;&#20837;&#12290;&#22312;&#23384;&#22312;&#26410;&#27979;&#37327;&#28151;&#26434;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#36807;&#21435;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#26410;&#25259;&#38706;&#20449;&#24687;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#36890;&#36807;&#20197;&#19968;&#31181;&#26032;&#39062;&#21644;&#21512;&#27861;&#30340;&#26041;&#24335;&#23558;&#36825;&#20123;&#20449;&#24687;&#21253;&#25324;&#22312;&#31574;&#30053;&#25628;&#32034;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#36229;&#32423;&#24378;&#21270;&#23398;&#20064;&#23558;&#20135;&#29983;&#19968;&#20010;&#31649;&#20445;&#33021;&#22312;&#26631;&#20934;&#26368;&#20248;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#65288;&#20363;&#22914;&#36807;&#21435;&#20195;&#29702;&#30340;&#34892;&#20026;&#65289;&#20043;&#19978;&#34920;&#29616;&#26356;&#22909;&#30340;&#36229;&#32423;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26356;&#24378;&#22823;&#30340;&#31070;&#35861;&#31216;&#20026;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#31119;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI becomes more prevalent throughout society, effective methods of integrating humans and AI systems that leverage their respective strengths and mitigate risk have become an important priority. In this paper, we introduce the paradigm of super reinforcement learning that takes advantage of Human-AI interaction for data driven sequential decision making. This approach utilizes the observed action, either from AI or humans, as input for achieving a stronger oracle in policy learning for the decision maker (humans or AI). In the decision process with unmeasured confounding, the actions taken by past agents can offer valuable insights into undisclosed information. By including this information for the policy search in a novel and legitimate manner, the proposed super reinforcement learning will yield a super-policy that is guaranteed to outperform both the standard optimal policy and the behavior one (e.g., past agents' actions). We call this stronger oracle a blessing from human-AI in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#32858;&#31867;&#23398;&#20064;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#26063;&#65292;&#36890;&#36807;&#23616;&#37096;&#35745;&#31639;&#21644;&#32858;&#31867;&#32858;&#21512;&#27493;&#39588;&#65292;&#22312;&#27599;&#20010;&#29992;&#25143;&#22788;&#23398;&#20064;&#20986;&#30495;&#23454;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.10866</link><description>&lt;p&gt;
&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#32858;&#31867;&#23398;&#20064;&#30340;&#19968;&#27425;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A One-shot Framework for Distributed Clustered Learning in Heterogeneous Environments. (arXiv:2209.10866v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#32858;&#31867;&#23398;&#20064;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#26063;&#65292;&#36890;&#36807;&#23616;&#37096;&#35745;&#31639;&#21644;&#32858;&#31867;&#32858;&#21512;&#27493;&#39588;&#65292;&#22312;&#27599;&#20010;&#29992;&#25143;&#22788;&#23398;&#20064;&#20986;&#30495;&#23454;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#26063;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#29992;&#25143;&#20174;$K$&#20010;&#19981;&#21516;&#20998;&#24067;&#20013;&#33719;&#21462;&#25968;&#25454;&#30340;&#24322;&#26500;&#29615;&#22659;&#20013;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#35774;&#32622;&#20013;&#65292;&#29992;&#25143;&#30340;&#20998;&#32452;&#65288;&#22522;&#20110;&#20182;&#20204;&#37319;&#26679;&#30340;&#25968;&#25454;&#20998;&#24067;&#65289;&#20197;&#21450;&#20998;&#24067;&#30340;&#32479;&#35745;&#23646;&#24615;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#12290;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#38598;&#21512;&#21487;&#25509;&#21463;&#30340;&#32858;&#31867;&#31639;&#27861;$\mathcal{C}$&#21442;&#25968;&#21270;&#30340;&#19968;&#27425;&#24615;&#20998;&#24067;&#24335;&#32858;&#31867;&#23398;&#20064;&#26041;&#27861;&#65288;ODCL-$\mathcal{C}$&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#27599;&#20010;&#29992;&#25143;&#22788;&#23398;&#20064;&#30495;&#23454;&#27169;&#22411;&#12290;&#21487;&#25509;&#21463;&#30340;&#32858;&#31867;&#26041;&#27861;&#21253;&#25324;$K$&#22343;&#20540;&#65288;KM&#65289;&#21644;&#20984;&#32858;&#31867;&#65288;CC&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#25152;&#25552;&#20986;&#30340;&#21508;&#31181;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#22914;ODCL-KM&#21644;ODCL-CC&#12290;&#25152;&#25552;&#20986;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#30340;&#26412;&#22320;&#35745;&#31639;&#21644;&#26381;&#21153;&#22120;&#19978;&#22522;&#20110;&#32858;&#31867;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#34987;&#35777;&#26126;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#24378;&#20984;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a family of communication efficient methods for distributed learning in heterogeneous environments in which users obtain data from one of $K$ different distributions. In the proposed setup, the grouping of users (based on the data distributions they sample), as well as the underlying statistical properties of the distributions, are apriori unknown. A family of One-shot Distributed Clustered Learning methods (ODCL-$\mathcal{C}$) is proposed, parametrized by the set of admissible clustering algorithms $\mathcal{C}$, with the objective of learning the true model at each user. The admissible clustering methods include $K$-means (KM) and convex clustering (CC), giving rise to various one-shot methods within the proposed family, such as ODCL-KM and ODCL-CC. The proposed one-shot approach, based on local computations at the users and a clustering based aggregation step at the server is shown to provide strong learning guarantees. In particular, for strongly convex problems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29305;&#24449;&#21270;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#19979;&#20869;&#37096;&#35268;&#36991;&#25915;&#20987;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#27169;&#22411;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#23458;&#25143;&#21046;&#36896;&#35268;&#36991;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.08412</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#29305;&#24449;&#21270;&#20869;&#37096;&#35268;&#36991;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Characterizing Internal Evasion Attacks in Federated Learning. (arXiv:2209.08412v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29305;&#24449;&#21270;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#19979;&#20869;&#37096;&#35268;&#36991;&#25915;&#20987;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#27169;&#22411;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#23458;&#25143;&#21046;&#36896;&#35268;&#36991;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#30340;&#23458;&#25143;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#65292;&#23458;&#25143;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23545;&#25239;&#24615;&#23458;&#25143;&#25191;&#34892;"&#20869;&#37096;&#35268;&#36991;&#25915;&#20987;"&#30340;&#38382;&#39064;&#65306;&#22312;&#27979;&#35797;&#26102;&#21046;&#36896;&#35268;&#36991;&#25915;&#20987;&#20197;&#27450;&#39575;&#20854;&#20182;&#23458;&#25143;&#12290;&#20363;&#22914;&#65292;&#23545;&#25163;&#21487;&#33021;&#26088;&#22312;&#36890;&#36807;&#20266;&#36896;&#30340;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#21644;&#25512;&#33616;&#31995;&#32479;&#26469;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#20197;&#33719;&#21462;&#21033;&#30410;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#23545;&#25239;&#24615;&#23458;&#25143;&#23545;&#21463;&#23475;&#27169;&#22411;&#25317;&#26377;&#24191;&#27867;&#30340;&#20449;&#24687;&#65292;&#22240;&#20026;&#26435;&#37325;&#20449;&#24687;&#22312;&#23458;&#25143;&#20043;&#38388;&#20849;&#20139;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23545;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#19979;&#27492;&#31867;&#20869;&#37096;&#35268;&#36991;&#25915;&#20987;&#30340;&#21487;&#36801;&#31227;&#24615;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#20998;&#26512;&#20102;&#27169;&#22411;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#23458;&#25143;&#25968;&#25454;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#38450;&#24481;&#20165;&#26174;&#31034;&#26377;&#38480;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning allows for clients in a distributed system to jointly train a machine learning model. However, clients' models are vulnerable to attacks during the training and testing phases. In this paper, we address the issue of adversarial clients performing "internal evasion attacks": crafting evasion attacks at test time to deceive other clients. For example, adversaries may aim to deceive spam filters and recommendation systems trained with federated learning for monetary gain. The adversarial clients have extensive information about the victim model in a federated learning setting, as weight information is shared amongst clients. We are the first to characterize the transferability of such internal evasion attacks for different learning methods and analyze the trade-off between model accuracy and robustness depending on the degree of similarities in client data. We show that adversarial training defenses in the federated learning setting only display limited improvements aga
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#31354;&#38388;&#26469;&#26500;&#24314;&#20581;&#24247;&#25351;&#26631;&#65292;&#21487;&#24212;&#29992;&#20110;&#24037;&#19994;&#36164;&#20135;&#30340;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2208.13288</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#26377;&#20449;&#24687;&#20581;&#24247;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Informative Health Indicators Through Unsupervised Contrastive Learning. (arXiv:2208.13288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#31354;&#38388;&#26469;&#26500;&#24314;&#20581;&#24247;&#25351;&#26631;&#65292;&#21487;&#24212;&#29992;&#20110;&#24037;&#19994;&#36164;&#20135;&#30340;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#30417;&#27979;&#23545;&#20110;&#23433;&#20840;&#39640;&#25928;&#22320;&#36816;&#33829;&#24037;&#19994;&#36164;&#20135;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36817;&#24180;&#26469;&#23545;&#31283;&#20581;&#20581;&#24247;&#25351;&#26631;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#25351;&#26631;&#33021;&#22815;&#25552;&#20379;&#23454;&#26102;&#23450;&#37327;&#30340;&#24037;&#19994;&#36164;&#20135;&#20581;&#24247;&#29366;&#24577;&#20449;&#24687;&#65292;&#26159;&#25925;&#38556;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#23398;&#20064;&#20581;&#24247;&#25351;&#26631;&#30340;&#26032;&#26041;&#27861;&#12290;&#36816;&#34892;&#26102;&#38388;&#20316;&#20026;&#36164;&#20135;&#36864;&#21270;&#29366;&#24577;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#31354;&#38388;&#65292;&#27979;&#37327;&#19982;&#20581;&#24247;&#29366;&#24577;&#30340;&#36317;&#31163;&#26469;&#26500;&#24314;&#20581;&#24247;&#25351;&#26631;&#12290;&#20026;&#20102;&#31361;&#26174;&#25152;&#25552;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; - &#39118;&#36710;&#30952;&#24202;&#26696;&#20363;&#30740;&#31350;&#21644;&#30495;&#23454;&#26465;&#20214;&#30417;&#27979;&#20219;&#21153;&#20013;&#30340;&#30952;&#24202;&#30952;&#24202;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Condition monitoring is essential to operate industrial assets safely and efficiently. To achieve this goal, the development of robust health indicators has recently attracted significant attention. These indicators, which provide quantitative real-time insights into the health status of industrial assets over time, serve as valuable tools for fault detection and prognostics. In this study, we propose a novel and universal approach to learn health indicators based on unsupervised contrastive learning. Operational time acts as a proxy for the asset's degradation state, enabling the learning of a contrastive feature space that facilitates the construction of a health indicator by measuring the distance to the healthy condition. To highlight the universality of the proposed approach, we assess the proposed contrastive learning framework in two distinct tasks - wear assessment and fault detection - across two different case studies: a milling machines case study and a real condition monito
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#21453;&#39304;&#65292;&#23398;&#20064;&#20102;&#30001;&#19981;&#21516;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36328;&#32676;&#20307;&#30456;&#20284;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2208.12731</link><description>&lt;p&gt;
&#27604;&#36739;&#33529;&#26524;&#21644;&#27225;&#23376;&#65306;&#23398;&#20064;&#19981;&#21516;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions. (arXiv:2208.12731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#21453;&#39304;&#65292;&#23398;&#20064;&#20102;&#30001;&#19981;&#21516;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36328;&#32676;&#20307;&#30456;&#20284;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#24615;&#20989;&#25968;&#34913;&#37327;&#20102;&#21487;&#27604;&#36739;&#30340;&#20803;&#32032;&#23545;&#30340;&#30456;&#20284;&#31243;&#24230;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#36981;&#24490;Dwork&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#20197;&#21450;&#32858;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#33021;&#24635;&#26159;&#20445;&#35777;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#29978;&#33267;Dwork&#31561;&#20154;&#20063;&#25552;&#20986;&#20102;&#36825;&#19968;&#28857;&#12290;&#20363;&#22914;&#65292;&#21512;&#29702;&#22320;&#20551;&#35774;&#65292;&#24403;&#35201;&#27604;&#36739;&#30340;&#20803;&#32032;&#30001;&#19981;&#21516;&#30340;&#20998;&#24067;&#29983;&#25104;&#65292;&#25110;&#32773;&#25442;&#21477;&#35805;&#35828;&#23646;&#20110;&#19981;&#21516;&#30340;&#8220;&#20154;&#21475;&#8221;&#32676;&#20307;&#26102;&#65292;&#33719;&#24471;&#23427;&#20204;&#30340;&#30495;&#23454;&#30456;&#20284;&#24615;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#21453;&#39304;&#26469;&#23398;&#20064;&#36825;&#20123;&#36328;&#32676;&#20307;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#30028;&#38480;&#23637;&#31034;&#20102;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similarity functions measure how comparable pairs of elements are, and play a key role in a wide variety of applications, e.g., notions of Individual Fairness abiding by the seminal paradigm of Dwork et al., as well as Clustering problems. However, access to an accurate similarity function should not always be considered guaranteed, and this point was even raised by Dwork et al. For instance, it is reasonable to assume that when the elements to be compared are produced by different distributions, or in other words belong to different ``demographic'' groups, knowledge of their true similarity might be very difficult to obtain. In this work, we present an efficient sampling framework that learns these across-groups similarity functions, using only a limited amount of experts' feedback. We show analytical results with rigorous theoretical bounds, and empirically validate our algorithms via a large suite of experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#32763;&#35793;&#20026;&#21487;&#25191;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;</title><link>http://arxiv.org/abs/2208.08374</link><description>&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#20013;&#32763;&#35793;&#25112;&#30053;&#24847;&#22270;&#30340;&#35745;&#31639;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting. (arXiv:2208.08374v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#32763;&#35793;&#20026;&#21487;&#25191;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#28041;&#21450;&#21040;&#20154;&#31867;&#21644;AI&#31995;&#32479;&#21327;&#21516;&#23436;&#25104;&#20219;&#21153;&#30340;&#28151;&#21512;&#21457;&#36215;&#35774;&#23450;&#12290;&#23613;&#31649;&#22312;&#36890;&#36807;&#35821;&#35328;&#31934;&#30830;&#25351;&#23450;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#30340;&#20302;&#32423;&#35268;&#33539;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20294;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#35299;&#37322;&#20154;&#31867;&#25351;&#25381;&#23448;&#30340;&#39640;&#32423;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#32570;&#20047;&#12290;&#20174;&#35821;&#35328;&#20013;&#35299;&#26512;&#25112;&#30053;&#24847;&#22270;&#23558;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#35745;&#21010;&#29420;&#31435;&#36816;&#34892;&#65292;&#32780;&#26080;&#38656;&#39057;&#32321;&#30340;&#25351;&#23548;&#25110;&#25351;&#20196;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#24418;&#24335;&#30340;&#35745;&#31639;&#25509;&#21475;&#12290;&#21033;&#29992;&#19968;&#20010;&#28216;&#25103;&#29615;&#22659;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;1000&#22810;&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#35821;&#35328;&#31574;&#30053;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world tasks involve a mixed-initiative setup, wherein humans and AI systems collaboratively perform a task. While significant work has been conducted towards enabling humans to specify, through language, exactly how an agent should complete a task (i.e., low-level specification), prior work lacks on interpreting the high-level strategic intent of the human commanders. Parsing strategic intent from language will allow autonomous systems to independently operate according to the user's plan without frequent guidance or instruction. In this paper, we build a computational interface capable of translating unstructured language strategies into actionable intent in the form of goals and constraints. Leveraging a game environment, we collect a dataset of over 1000 examples, mapping language strategies to the corresponding goals and constraints, and show that our model, trained on this dataset, significantly outperforms human interpreters in inferring strategic intent (i.e., goals an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#33258;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#35268;&#21017;/&#24322;&#27493;&#35266;&#23519;&#21644;&#34892;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;episode&#20043;&#38388;&#20135;&#29983;&#24040;&#22823;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#21464;&#21270;&#65292;&#36825;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2207.12062</link><description>&lt;p&gt;
&#24212;&#29992;&#20803;&#23398;&#20064;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#33258;&#36866;&#24212;&#24322;&#27493;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Adaptive Asynchronous Control Using Meta-learned Neural Ordinary Differential Equations. (arXiv:2207.12062v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12062
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#33258;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#35268;&#21017;/&#24322;&#27493;&#35266;&#23519;&#21644;&#34892;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;episode&#20043;&#38388;&#20135;&#29983;&#24040;&#22823;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#21464;&#21270;&#65292;&#36825;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#22312;&#21253;&#25324;&#26426;&#22120;&#20154;&#29615;&#22659;&#22312;&#20869;&#30340;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#24120;&#24120;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#19981;&#35268;&#21017;/&#24322;&#27493;&#35266;&#23519;&#21644;&#34892;&#21160;&#20197;&#21450;&#22312;&#19981;&#21516;episode&#20043;&#38388;&#20135;&#29983;&#24040;&#22823;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#19981;&#21516;&#30340;&#36733;&#33655;&#24815;&#24615;&#29305;&#24615;&#65289;&#30340;&#20803;&#23398;&#20064;&#33258;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#26159;&#20219;&#21153;&#26080;&#20851;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#31616;&#21333;&#22320;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#21644;&#19968;&#20010;&#30495;&#23454;&#30340;&#24037;&#19994;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based Reinforcement Learning and Control have demonstrated great potential in various sequential decision making problem domains, including in robotics settings. However, real-world robotics systems often present challenges that limit the applicability of those methods. In particular, we note two problems that jointly happen in many industrial systems: 1) Irregular/asynchronous observations and actions and 2) Dramatic changes in environment dynamics from an episode to another (e.g. varying payload inertial properties). We propose a general framework that overcomes those difficulties by meta-learning adaptive dynamics models for continuous-time prediction and control. The proposed approach is task-agnostic and can be adapted to new tasks in a straight-forward manner. We present evaluations in two different robot simulations and on a real industrial robot.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2207.11749</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26410;&#30693;&#25968;&#37327;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#28304;&#20998;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#26410;&#30693;&#25968;&#37327;&#20449;&#21495;&#30340;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#36755;&#20986;&#19982;&#30446;&#26631;&#23545;&#40784;&#24341;&#36215;&#30340;&#25490;&#21015;&#38382;&#39064;&#23548;&#33268;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20004;&#27493;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;&#26377;&#38745;&#38899;&#36890;&#36947;&#30340;&#24773;&#20917;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#28151;&#21512;&#30340;&#36752;&#23556;&#33337;&#22122;&#22768;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36798;&#21040;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few existing studies focus on the source separation problem with unknown numbers of signals, and how to evaluate the performances of the systems is not yet clear. We propose a solution with a fixed number of output channels to address these two problems, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we propose a two-step algorithm based on autoencoders and a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The proposed algorithm achieved competitive performance as two algorithms developed for known numbers of signals, which is highly explainable and extensible and get the state of the art under this framework.
&lt;/p&gt;</description></item><item><title>ApHMM &#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#26088;&#22312;&#26174;&#33879;&#20943;&#23569;Profile Hidden Markov Models&#20013;Baum-Welch&#31639;&#27861;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2207.09765</link><description>&lt;p&gt;
ApHMM: &#24555;&#36895;&#21644;&#33410;&#33021;&#30340;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#21152;&#36895;Profile Hidden Markov Models
&lt;/p&gt;
&lt;p&gt;
ApHMM: Accelerating Profile Hidden Markov Models for Fast and Energy-Efficient Genome Analysis. (arXiv:2207.09765v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09765
&lt;/p&gt;
&lt;p&gt;
ApHMM &#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#26088;&#22312;&#26174;&#33879;&#20943;&#23569;Profile Hidden Markov Models&#20013;Baum-Welch&#31639;&#27861;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Profile Hidden Markov Models (pHMMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#29983;&#29289;&#20449;&#24687;&#23398;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#29289;&#24207;&#21015;&#65288;&#22914;DNA&#25110;&#34507;&#30333;&#36136;&#24207;&#21015;&#65289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;pHMMs&#20013;&#65292;&#24207;&#21015;&#34987;&#34920;&#31034;&#20026;&#22270;&#24418;&#32467;&#26500;&#12290;&#36825;&#20123;&#27010;&#29575;&#38543;&#21518;&#34987;&#29992;&#20110;&#35745;&#31639;&#24207;&#21015;&#19982;pHMM&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#24471;&#20998;&#12290;Baum-Welch&#31639;&#27861;&#26159;&#19968;&#31181;&#24120;&#29992;&#19988;&#39640;&#24230;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#27010;&#29575;&#26469;&#20248;&#21270;&#21644;&#35745;&#31639;&#30456;&#20284;&#24615;&#24471;&#20998;&#12290;&#28982;&#32780;&#65292;Baum-Welch&#31639;&#27861;&#35745;&#31639;&#23494;&#38598;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#21482;&#25552;&#20379;&#36719;&#20214;&#26041;&#27861;&#65292;&#35201;&#20040;&#21482;&#25552;&#20379;&#30828;&#20214;&#26041;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#22266;&#23450;&#30340;pHMM&#35774;&#35745;&#12290;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#35774;&#35745;&#19968;&#31181;&#28789;&#27963;&#12289;&#39640;&#24615;&#33021;&#21644;&#33410;&#33021;&#30340;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;pHMM&#20013;Baum-Welch&#31639;&#27861;&#30340;&#20027;&#35201;&#20302;&#25928;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ApHMM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28789;&#27963;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#26088;&#22312;&#26174;&#33879;&#20943;&#23569;&#19982;Baum-Welch&#31639;&#27861;&#30456;&#20851;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Profile hidden Markov models (pHMMs) are widely employed in various bioinformatics applications to identify similarities between biological sequences, such as DNA or protein sequences. In pHMMs, sequences are represented as graph structures. These probabilities are subsequently used to compute the similarity score between a sequence and a pHMM graph. The Baum-Welch algorithm, a prevalent and highly accurate method, utilizes these probabilities to optimize and compute similarity scores. However, the Baum-Welch algorithm is computationally intensive, and existing solutions offer either software-only or hardware-only approaches with fixed pHMM designs. We identify an urgent need for a flexible, high-performance, and energy-efficient HW/SW co-design to address the major inefficiencies in the Baum-Welch algorithm for pHMMs.  We introduce ApHMM, the first flexible acceleration framework designed to significantly reduce both computational and energy overheads associated with the Baum-Welch al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.08012</link><description>&lt;p&gt;
&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#30340;&#32463;&#39564;&#21487;&#20197;&#20998;&#35299;&#20026;&#22522;&#26412;&#30340;&#21407;&#23376;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#20197;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#21442;&#19982;&#26032;&#39062;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#23398;&#20064;&#20197;&#32452;&#21512;&#26041;&#24335;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#30340;&#34892;&#20026;&#31216;&#20026;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#65288;CLBs&#65289;&#12290;&#23398;&#20064;CLBs&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#65288;BP&#65289;&#12290;&#23613;&#31649;&#36825;&#26159;&#20154;&#31867;&#36731;&#26494;&#23436;&#25104;&#30340;&#26234;&#33021;&#22766;&#20030;&#65292;&#20294;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#20195;&#29702;&#21830;&#36890;&#36807;&#35299;&#20915;BP&#30340;&#39046;&#22495;&#26080;&#20851;&#29256;&#26412;&#26469;&#23637;&#31034;CLBs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21463;&#21040;&#25351;&#20195;&#28216;&#25103;&#30340;&#35821;&#35328;&#28044;&#29616;&#21644;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#25193;&#23637;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24102;&#26377;&#22806;&#37096;&#36755;&#20837;&#30340;MDPs&#31639;&#27861;&#65292;&#21517;&#20026;&#36861;&#28335;&#23398;&#20064;&#65288;HL&#65289;&#12290;HL&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#21464;&#37327;&#26679;&#26412;&#20351;&#24471;&#36807;&#21435;&#30340;&#20915;&#31574;&#22312;&#22238;&#28335;&#20013;&#21487;&#20197;&#21152;&#36895;&#31574;&#30053;&#25913;&#36827;&#65292;&#22312;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.06272</link><description>&lt;p&gt;
&#24102;&#22806;&#37096;&#36755;&#20837;&#30340;MDPs&#30340;&#36861;&#28335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hindsight Learning for MDPs with Exogenous Inputs. (arXiv:2207.06272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06272
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24102;&#26377;&#22806;&#37096;&#36755;&#20837;&#30340;MDPs&#31639;&#27861;&#65292;&#21517;&#20026;&#36861;&#28335;&#23398;&#20064;&#65288;HL&#65289;&#12290;HL&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#21464;&#37327;&#26679;&#26412;&#20351;&#24471;&#36807;&#21435;&#30340;&#20915;&#31574;&#22312;&#22238;&#28335;&#20013;&#21487;&#20197;&#21152;&#36895;&#31574;&#30053;&#25913;&#36827;&#65292;&#22312;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#38656;&#35201;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20570;&#20986;&#36845;&#20195;&#20915;&#31574;&#65292;&#20854;&#20013;&#24433;&#21709;&#20915;&#31574;&#32467;&#26524;&#30340;&#21807;&#19968;&#19981;&#30830;&#23450;&#24615;&#26159;&#20915;&#31574;&#32773;&#25511;&#21046;&#20043;&#22806;&#30340;&#22806;&#37096;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#26377;&#22806;&#37096;&#36755;&#20837;&#30340;MDPs&#65288;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31867;&#21517;&#20026;&#36861;&#28335;&#23398;&#20064;&#65288;HL&#65289;&#30340;&#25968;&#25454;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;HL&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#20851;&#38190;&#27934;&#35265;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#29575;&#65306;&#36890;&#36807;&#22806;&#37096;&#21464;&#37327;&#30340;&#26679;&#26412;&#65292;&#36807;&#21435;&#30340;&#20915;&#31574;&#21487;&#20197;&#22312;&#22238;&#28335;&#20013;&#37325;&#26032;&#23457;&#35270;&#65292;&#20197;&#25512;&#26029;&#20986;&#21487;&#20197;&#21152;&#36895;&#31574;&#30053;&#25913;&#36827;&#30340;&#21453;&#20107;&#23454;&#21518;&#26524;&#12290;&#25105;&#20204;&#23558;HL&#19982;&#22810;&#20010;&#22522;&#32447;&#31639;&#27861;&#22312;&#22810;&#20010;&#27979;&#35797;&#26696;&#20363;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#21253;&#25324;&#22810;&#31192;&#20070;&#21644;&#33322;&#31354;&#20844;&#21496;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#21040;&#19994;&#21153;&#20851;&#38190;&#30340;&#20113;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#8212;&#8212;&#23558;&#34394;&#25311;&#26426;&#65288;VM&#65289;&#20998;&#37197;&#21040;&#29289;&#29702;&#26426;&#22120;&#19978;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#22823;&#22411;&#20844;&#20849;&#20113;&#25552;&#20379;&#21830;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#27169;&#25311;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;HL&#31639;&#27861;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many resource management problems require sequential decision-making under uncertainty, where the only uncertainty affecting the decision outcomes are exogenous variables outside the control of the decision-maker. We model these problems as Exo-MDPs (Markov Decision Processes with Exogenous Inputs) and design a class of data-efficient algorithms for them termed Hindsight Learning (HL). Our HL algorithms achieve data efficiency by leveraging a key insight: having samples of the exogenous variables, past decisions can be revisited in hindsight to infer counterfactual consequences that can accelerate policy improvements. We compare HL against classic baselines in the multi-secretary and airline revenue management problems. We also scale our algorithms to a business-critical cloud resource management problem -- allocating Virtual Machines (VMs) to physical machines, and simulate their performance with real datasets from a large public cloud provider. We find that HL algorithms outperform d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#26469;&#30830;&#23450;&#31867;&#39118;&#28287;&#24615;&#20851;&#33410;&#28814;&#23567;&#40736;&#27169;&#22411;&#30340;&#20813;&#30123;&#29366;&#24577;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#20813;&#30123;&#29366;&#24577;&#20013;&#30340;&#35843;&#33410;&#20316;&#29992;&#22240;&#23376;&#65292;&#20851;&#38381;&#33258;&#36523;&#20813;&#30123;&#21453;&#24212;&#20013;&#30340;&#33258;&#36523;&#20813;&#30123;&#36890;&#36335;&#12290;&#36890;&#36807;&#32771;&#34385;&#33014;&#21407;&#35825;&#23548;&#24615;&#20851;&#33410;&#28814;&#23567;&#40736;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#20316;&#32773;&#25506;&#32034;&#20102;&#22914;&#20309;&#30830;&#23450;&#31995;&#32479;&#29366;&#24577;&#20197;&#25552;&#39640;&#20813;&#30123;&#30103;&#27861;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.05882</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#30830;&#23450;&#31867;&#39118;&#28287;&#24615;&#20851;&#33410;&#28814;&#23567;&#40736;&#27169;&#22411;&#30340;&#20813;&#30123;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Employing Feature Selection Algorithms to Determine the Immune State of a Mouse Model of Rheumatoid Arthritis. (arXiv:2207.05882v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#26469;&#30830;&#23450;&#31867;&#39118;&#28287;&#24615;&#20851;&#33410;&#28814;&#23567;&#40736;&#27169;&#22411;&#30340;&#20813;&#30123;&#29366;&#24577;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#20813;&#30123;&#29366;&#24577;&#20013;&#30340;&#35843;&#33410;&#20316;&#29992;&#22240;&#23376;&#65292;&#20851;&#38381;&#33258;&#36523;&#20813;&#30123;&#21453;&#24212;&#20013;&#30340;&#33258;&#36523;&#20813;&#30123;&#36890;&#36335;&#12290;&#36890;&#36807;&#32771;&#34385;&#33014;&#21407;&#35825;&#23548;&#24615;&#20851;&#33410;&#28814;&#23567;&#40736;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#20316;&#32773;&#25506;&#32034;&#20102;&#22914;&#20309;&#30830;&#23450;&#31995;&#32479;&#29366;&#24577;&#20197;&#25552;&#39640;&#20813;&#30123;&#30103;&#27861;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20813;&#30123;&#21453;&#24212;&#26159;&#19968;&#20010;&#21160;&#24577;&#36807;&#31243;&#65292;&#36890;&#36807;&#35813;&#36807;&#31243;&#65292;&#26426;&#20307;&#30830;&#23450;&#25239;&#21407;&#26159;&#33258;&#36523;&#36824;&#26159;&#38750;&#33258;&#36523;&#12290;&#36825;&#20010;&#21160;&#24577;&#36807;&#31243;&#30340;&#29366;&#24577;&#30001;&#32452;&#25104;&#20915;&#31574;&#36807;&#31243;&#30340;&#28814;&#30151;&#21644;&#35843;&#33410;&#20316;&#29992;&#22240;&#23376;&#30340;&#30456;&#23545;&#24179;&#34913;&#21644;&#31181;&#32676;&#23450;&#20041;&#12290;&#20813;&#30123;&#30103;&#27861;&#24212;&#29992;&#20110;&#31867;&#39118;&#28287;&#24615;&#20851;&#33410;&#28814;&#31561;&#65292;&#20854;&#30446;&#26631;&#26159;&#23558;&#20813;&#30123;&#29366;&#24577;&#20559;&#21521;&#35843;&#33410;&#20316;&#29992;&#22240;&#23376;&#65292;&#20174;&#32780;&#20851;&#38381;&#33258;&#36523;&#20813;&#30123;&#21453;&#24212;&#20013;&#30340;&#33258;&#36523;&#20813;&#30123;&#36890;&#36335;&#12290;&#34429;&#28982;&#24050;&#30693;&#26377;&#20960;&#31181;&#20813;&#30123;&#30103;&#27861;&#26041;&#27861;&#65292;&#20294;&#35813;&#30103;&#27861;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#35813;&#24178;&#39044;&#22914;&#20309;&#25913;&#21464;&#35813;&#29366;&#24577;&#30340;&#28436;&#21464;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20010;&#36807;&#31243;&#19981;&#20165;&#30001;&#36807;&#31243;&#30340;&#21160;&#24577;&#24615;&#30830;&#23450;&#65292;&#36824;&#30001;&#24178;&#39044;&#26102;&#31995;&#32479;&#30340;&#29366;&#24577;&#30830;&#23450;&#65292;&#32780;&#24178;&#39044;&#21069;&#24456;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30830;&#23450;&#31995;&#32479;&#30340;&#29366;&#24577;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#31181;&#29366;&#24577;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#31867;&#39118;&#28287;&#24615;&#20851;&#33410;&#28814;&#23567;&#40736;&#27169;&#22411;&#65288;&#33014;&#21407;&#35825;&#23548;&#24615;&#20851;&#33410;&#28814;&#65289;&#65292;&#36827;&#34892;&#20102;&#20813;&#30123;&#30103;&#27861;&#65307;&#25910;&#38598;&#20102;&#39640;&#37327;&#32423;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
The immune response is a dynamic process by which the body determines whether an antigen is self or nonself. The state of this dynamic process is defined by the relative balance and population of inflammatory and regulatory actors which comprise this decision making process. The goal of immunotherapy as applied to, e.g. Rheumatoid Arthritis (RA), then, is to bias the immune state in favor of the regulatory actors - thereby shutting down autoimmune pathways in the response. While there are several known approaches to immunotherapy, the effectiveness of the therapy will depend on how this intervention alters the evolution of this state. Unfortunately, this process is determined not only by the dynamics of the process, but the state of the system at the time of intervention - a state which is difficult if not impossible to determine prior to application of the therapy. To identify such states we consider a mouse model of RA (Collagen-Induced Arthritis (CIA)) immunotherapy; collect high di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#22914;&#20309;&#39640;&#25928;&#22320;&#21024;&#38500;&#23458;&#25143;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#25191;&#34892;&#26412;&#22320;&#36951;&#24536;&#24182;&#32467;&#21512;&#23569;&#37327;&#36718;&#27425;&#30340;&#32852;&#37030;&#23398;&#20064;&#26469;&#33719;&#24471;&#36951;&#24536;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.05521</link><description>&lt;p&gt;
&#32852;&#37030;&#36951;&#24536;&#65306;&#22914;&#20309;&#39640;&#25928;&#22320;&#20174;FL&#20013;&#21024;&#38500;&#23458;&#25143;&#65311;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning: How to Efficiently Erase a Client in FL?. (arXiv:2207.05521v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#22914;&#20309;&#39640;&#25928;&#22320;&#21024;&#38500;&#23458;&#25143;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#25191;&#34892;&#26412;&#22320;&#36951;&#24536;&#24182;&#32467;&#21512;&#23569;&#37327;&#36718;&#27425;&#30340;&#32852;&#37030;&#23398;&#20064;&#26469;&#33719;&#24471;&#36951;&#24536;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38544;&#31169;&#27861;&#35268;&#36171;&#20104;&#29992;&#25143;&#34987;&#36951;&#24536;&#26435;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36951;&#24536;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#19978;&#30340;&#36951;&#24536;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#31561;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#22240;&#20026;&#23398;&#20064;&#21327;&#35758;&#30340;&#24046;&#24322;&#21644;&#22810;&#20010;&#21442;&#19982;&#32773;&#30340;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#21024;&#38500;&#23458;&#25143;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#21024;&#38500;&#23458;&#25143;&#30340;&#25972;&#20010;&#26412;&#22320;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#21024;&#38500;&#19968;&#20010;&#23458;&#25143;&#65292;&#25105;&#20204;&#24314;&#35758;&#39318;&#20808;&#22312;&#35201;&#21024;&#38500;&#30340;&#23458;&#25143;&#31471;&#25191;&#34892;&#26412;&#22320;&#36951;&#24536;&#65292;&#28982;&#21518;&#20351;&#29992;&#26412;&#22320;&#36951;&#24536;&#30340;&#27169;&#22411;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#22312;&#26381;&#21153;&#22120;&#21644;&#21097;&#20313;&#23458;&#25143;&#20043;&#38388;&#36827;&#34892;&#23569;&#37327;&#36718;&#27425;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#33719;&#24471;&#36951;&#24536;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#37319;&#29992;&#22810;&#31181;&#24615;&#33021;&#25351;&#26631;&#23545;&#25105;&#20204;&#30340;&#36951;&#24536;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With privacy legislation empowering the users with the right to be forgotten, it has become essential to make a model amenable for forgetting some of its training data. However, existing unlearning methods in the machine learning context can not be directly applied in the context of distributed settings like federated learning due to the differences in learning protocol and the presence of multiple actors. In this paper, we tackle the problem of federated unlearning for the case of erasing a client by removing the influence of their entire local data from the trained global model. To erase a client, we propose to first perform local unlearning at the client to be erased, and then use the locally unlearned model as the initialization to run very few rounds of federated learning between the server and the remaining clients to obtain the unlearned global model. We empirically evaluate our unlearning method by employing multiple performance measures on three datasets, and demonstrate that 
&lt;/p&gt;</description></item><item><title>QuASK&#26159;&#19968;&#20010;&#22522;&#20110;&#20869;&#26680;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12290;&#23427;&#35299;&#20915;&#20102;&#25163;&#21160;&#38598;&#25104;&#19981;&#21516;&#36719;&#20214;&#21253;&#21644;&#38271;&#20195;&#30721;&#33050;&#26412;&#23548;&#33268;&#30340;&#38169;&#35823;&#24212;&#29992;&#21644;&#20195;&#30721;&#21487;&#35835;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.15284</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#30340;&#37327;&#23376;&#20248;&#21183;&#23547;&#27714;&#32773;&#65288;QuASK&#65289;&#65306;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantum Advantage Seeker with Kernels (QuASK): a software framework to speed up the research in quantum machine learning. (arXiv:2206.15284v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15284
&lt;/p&gt;
&lt;p&gt;
QuASK&#26159;&#19968;&#20010;&#22522;&#20110;&#20869;&#26680;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12290;&#23427;&#35299;&#20915;&#20102;&#25163;&#21160;&#38598;&#25104;&#19981;&#21516;&#36719;&#20214;&#21253;&#21644;&#38271;&#20195;&#30721;&#33050;&#26412;&#23548;&#33268;&#30340;&#38169;&#35823;&#24212;&#29992;&#21644;&#20195;&#30721;&#21487;&#35835;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#20449;&#24687;&#30340;&#29305;&#24615;&#26469;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21463;&#30410;&#26159;&#37327;&#23376;&#35745;&#31639;&#20013;&#26368;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#20043;&#19968;&#12290;&#36825;&#31181;&#20852;&#36259;&#25512;&#21160;&#20102;&#22810;&#31181;&#36719;&#20214;&#26694;&#26550;&#65288;&#20363;&#22914;Qiskit&#12289;Pennylane&#12289;Braket&#65289;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#23454;&#29616;&#12289;&#27169;&#25311;&#21644;&#25191;&#34892;&#37327;&#23376;&#31639;&#27861;&#12290;&#22823;&#22810;&#25968;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#23450;&#20041;&#37327;&#23376;&#30005;&#36335;&#12289;&#36816;&#34892;&#22522;&#26412;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#26681;&#25454;&#30828;&#20214;&#35775;&#38382;&#20302;&#32423;&#21035;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#23454;&#39564;&#20013;&#65292;&#36825;&#20123;&#26694;&#26550;&#24517;&#39035;&#25163;&#21160;&#38598;&#25104;&#21040;&#26356;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#27969;&#27700;&#32447;&#20013;&#12290;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20102;&#35299;&#19981;&#21516;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24320;&#21457;&#38271;&#20195;&#30721;&#33050;&#26412;&#36827;&#34892;&#38598;&#25104;&#65292;&#20998;&#26512;&#32467;&#26524;&#24182;&#29983;&#25104;&#32472;&#22270;&#12290;&#38271;&#20195;&#30721;&#24448;&#24448;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#24179;&#22343;&#38169;&#35823;&#25968;&#37327;&#19982;&#31243;&#24207;&#38271;&#24230;&#25104;&#27491;&#27604;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20063;&#20250;&#38590;&#20197;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting the properties of quantum information to the benefit of machine learning models is perhaps the most active field of research in quantum computation. This interest has supported the development of a multitude of software frameworks (e.g. Qiskit, Pennylane, Braket) to implement, simulate, and execute quantum algorithms. Most of them allow us to define quantum circuits, run basic quantum algorithms, and access low-level primitives depending on the hardware such software is supposed to run. For most experiments, these frameworks have to be manually integrated within a larger machine learning software pipeline. The researcher is in charge of knowing different software packages, integrating them through the development of long code scripts, analyzing the results, and generating the plots. Long code often leads to erroneous applications, due to the average number of bugs growing proportional with respect to the program length. Moreover, other researchers will struggle to understand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#27604;&#29305;&#24065;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23558;&#30456;&#20851;&#36164;&#20135;&#12289;&#25216;&#26415;&#25351;&#26631;&#21644;Twitter&#20869;&#23481;&#20316;&#20026;&#36755;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#37329;&#34701;&#35789;&#27719;&#34920;&#30340;&#21477;&#32423;FinBERT&#23884;&#20837;&#65292;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25512;&#25991;&#20013;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2206.00648</link><description>&lt;p&gt;
PreBit -- &#19968;&#31181;&#21033;&#29992;Twitter FinBERT&#23884;&#20837;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#27604;&#29305;&#24065;&#30340;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin. (arXiv:2206.00648v2 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#27604;&#29305;&#24065;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23558;&#30456;&#20851;&#36164;&#20135;&#12289;&#25216;&#26415;&#25351;&#26631;&#21644;Twitter&#20869;&#23481;&#20316;&#20026;&#36755;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#37329;&#34701;&#35789;&#27719;&#34920;&#30340;&#21477;&#32423;FinBERT&#23884;&#20837;&#65292;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25512;&#25991;&#20013;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#29305;&#24065;&#20197;&#20854;&#19981;&#26029;&#22686;&#38271;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#23637;&#31034;&#20102;&#33258;&#20854;&#35806;&#29983;&#20197;&#26469;&#30340;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#24615;&#12290;&#36825;&#31181;&#27874;&#21160;&#24615;&#65292;&#21152;&#19978;&#20854;&#21435;&#20013;&#24515;&#21270;&#30340;&#24615;&#36136;&#65292;&#20351;&#27604;&#29305;&#24065;&#30456;&#23545;&#20110;&#26356;&#20256;&#32479;&#30340;&#36164;&#20135;&#26356;&#23481;&#26131;&#21463;&#21040;&#25237;&#26426;&#20132;&#26131;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#23558;&#21508;&#31181;&#30456;&#20851;&#36164;&#20135;&#12289;&#25216;&#26415;&#25351;&#26631;&#20197;&#21450;Twitter&#20869;&#23481;&#20316;&#20026;&#36755;&#20837;&#12290;&#22312;&#19968;&#39033;&#28145;&#20837;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26469;&#33258;&#22823;&#20247;&#23545;&#27604;&#29305;&#24065;&#30340;&#31038;&#20132;&#23186;&#20307;&#35752;&#35770;&#26159;&#21542;&#20855;&#26377;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20174;2015&#24180;&#21040;2021&#24180;&#27599;&#22825;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;&#27604;&#29305;&#24065;&#8221;&#30340;5,000&#26465;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;PreBit&#65292;&#24182;&#23558;&#20854;&#22312;&#32593;&#19978;&#25552;&#20379;&#12290;&#22312;&#25105;&#20204;&#30340;&#28151;&#21512;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22312;&#37329;&#34701;&#35789;&#27719;&#34920;&#19978;&#39044;&#35757;&#32451;&#30340;&#21477;&#32423;FinBERT&#23884;&#20837;&#65292;&#20197;&#20415;&#20197;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#25429;&#25417;&#25512;&#25991;&#30340;&#20840;&#37096;&#20869;&#23481;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#19968;&#31181;&#21367;&#31215;&#23618;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#25512;&#25991;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#36827;&#34892;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bitcoin, with its ever-growing popularity, has demonstrated extreme price volatility since its origin. This volatility, together with its decentralised nature, make Bitcoin highly subjective to speculative trading as compared to more traditional assets. In this paper, we propose a multimodal model for predicting extreme price fluctuations. This model takes as input a variety of correlated assets, technical indicators, as well as Twitter content. In an in-depth study, we explore whether social media discussions from the general public on Bitcoin have predictive power for extreme price movements. A dataset of 5,000 tweets per day containing the keyword `Bitcoin' was collected from 2015 to 2021. This dataset, called PreBit, is made available online. In our hybrid model, we use sentence-level FinBERT embeddings, pretrained on financial lexicons, so as to capture the full contents of the tweets and feed it to the model in an understandable way. By combining these embeddings with a Convoluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#37327;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20302;&#20301;&#23485;&#23384;&#20648;&#20840;&#31934;&#24230;&#20540;&#20197;&#23454;&#29616;&#33410;&#32422;&#20869;&#23384;&#21644;&#25805;&#20316;&#25104;&#26412;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#25991;&#31456;&#20998;&#31867;&#20171;&#32461;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#27169;&#22411;&#37327;&#21270;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#32570;&#28857;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2205.07877</link><description>&lt;p&gt;
&#27169;&#22411;&#37327;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Model Quantization for Deep Neural Networks. (arXiv:2205.07877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#37327;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20302;&#20301;&#23485;&#23384;&#20648;&#20840;&#31934;&#24230;&#20540;&#20197;&#23454;&#29616;&#33410;&#32422;&#20869;&#23384;&#21644;&#25805;&#20316;&#25104;&#26412;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#25991;&#31456;&#20998;&#31867;&#20171;&#32461;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#27169;&#22411;&#37327;&#21270;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#32570;&#28857;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#23384;&#20648;&#21644;&#36816;&#31639;&#20250;&#24102;&#26469;&#30828;&#20214;&#25104;&#26412;&#30340;&#22686;&#21152;&#21644;&#25361;&#25112;&#12290;&#23545;&#27492;&#65292;&#25552;&#20986;&#20102;&#21387;&#32553;&#26041;&#27861;&#20197;&#35774;&#35745;&#39640;&#25928;&#30340;&#21152;&#36895;&#22120;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#27861;&#26159;&#25226;&#20840;&#31934;&#24230;&#30340;&#20540;&#23384;&#20648;&#22312;&#20302;&#20301;&#23485;&#20013;&#65292;&#36825;&#23601;&#21487;&#20197;&#33410;&#32422;&#20869;&#23384;&#21516;&#26102;&#29992;&#20302;&#25104;&#26412;&#30340;&#31616;&#21333;&#36816;&#31639;&#20195;&#26367;&#21407;&#26412;&#30340;&#25805;&#20316;&#12290;&#30001;&#20110;&#27169;&#22411;&#37327;&#21270;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#35774;&#35745;&#39640;&#25928;&#30828;&#20214;&#30340;&#24433;&#21709;&#65292;&#26368;&#36817;&#20960;&#24180;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#26041;&#27861;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#27604;&#36739;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#37327;&#21270;&#27010;&#24565;&#24182;&#20174;&#19981;&#21516;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#36991;&#20813;&#31934;&#24230;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#23545;&#27169;&#22411;&#37327;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning by deep neural networks are significant. But using these networks has been accompanied by a huge number of parameters for storage and computations that leads to an increase in the hardware cost and posing challenges. Therefore, compression approaches have been proposed to design efficient accelerators. One important approach for deep neural network compression is quantization that full-precision values are stored in low bit-width. In this way, in addition to memory saving, the operations will be replaced by simple ones with low cost. Many methods are suggested for DNNs Quantization in recent years, because of flexibility and influence in designing efficient hardware. Therefore, an integrated report is essential for better understanding, analysis, and comparison. In this paper, we provide a comprehensive survey. We describe the quantization concepts and categorize the methods from different perspectives. We discuss using the scale factor to match the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2204.12723</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Information-theoretic limitations of data-based price discrimination. (arXiv:2204.12723v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20272;&#20540;&#21644;&#22806;&#29983;&#21464;&#37327;&#25968;&#25454;&#38543;&#26426;&#26679;&#26412;&#30340;&#31532;&#19977;&#24230;&#20215;&#26684;&#27495;&#35270;&#65288;3PD&#65289;&#65292;&#20854;&#20013;&#22806;&#29983;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#65292;&#25968;&#25454;&#20998;&#24067;&#23545;&#21334;&#26041;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#32467;&#26524;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#31532;&#19968;&#32452;&#32467;&#26524;&#26159;&#23450;&#20215;&#31574;&#30053;&#26080;&#20851;&#30340;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#20998;&#20026;3PD&#21644;&#22343;&#21248;&#23450;&#20215;&#20004;&#31181;&#24773;&#20917;&#12290;&#31532;&#20108;&#32452;&#32467;&#26524;&#25552;&#20986;&#20102;$K$-markets&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;$K$-markets ERM&#21644;&#22343;&#21248;ERM&#31574;&#30053;&#23454;&#29616;&#20102;&#25910;&#20837;&#25910;&#25947;&#21040;&#21508;&#33258;&#30495;&#23454;&#20998;&#24067;3PD&#21644;&#22343;&#21248;&#23450;&#20215;&#26368;&#20248;&#35299;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26679;&#26412;&#37327;&#36275;&#22815;&#23567;&#30340;&#26102;&#20505;&#65292;&#22343;&#21248;&#65288;&#21363;$1$-market&#65289;ERM&#31574;&#30053;&#20135;&#29983;&#30340;&#25910;&#20837;&#27604;$K$-markets ERM&#31574;&#30053;&#26356;&#39640;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies third-degree price discrimination (3PD) based on a random sample of valuation and covariate data, where the covariate is continuous, and the distribution of the data is unknown to the seller. The main results of this paper are twofold. The first set of results is pricing strategy independent and reveals the fundamental information-theoretic limitation of any data-based pricing strategy in revenue generation for two cases: 3PD and uniform pricing. The second set of results proposes the $K$-markets empirical revenue maximization (ERM) strategy and shows that the $K$-markets ERM and the uniform ERM strategies achieve the optimal rate of convergence in revenue to that generated by their respective true-distribution 3PD and uniform pricing optima. Our theoretical and numerical results suggest that the uniform (i.e., $1$-market) ERM strategy generates a larger revenue than the $K$-markets ERM strategy when the sample size is small enough, and vice versa.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#32806;Mixup&#65288;DM&#65289;&#30340;&#39640;&#25928;mixup&#30446;&#26631;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;&#26469;&#25366;&#25496;&#20855;&#26377;&#21028;&#21035;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2203.10761</link><description>&lt;p&gt;
&#21033;&#29992;&#35299;&#32806;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#24212;&#29992;&#20110;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Harnessing Hard Mixed Samples with Decoupled Regularizer. (arXiv:2203.10761v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#32806;Mixup&#65288;DM&#65289;&#30340;&#39640;&#25928;mixup&#30446;&#26631;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;&#26469;&#25366;&#25496;&#20855;&#26377;&#21028;&#21035;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#24179;&#28369;&#20915;&#31574;&#36793;&#30028;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#21160;&#24577;mixup&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#28151;&#21512;&#26679;&#26412;&#20013;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#26174;&#33879;&#21306;&#22495;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#38745;&#24577;&#31574;&#30053;&#65288;&#22914;&#32447;&#24615;&#25554;&#20540;&#65289;&#65292;&#20294;&#39069;&#22806;&#30340;&#26102;&#38388;&#25104;&#26412;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#36825;&#20123;&#39069;&#22806;&#30340;&#35745;&#31639;&#24320;&#38144;&#20027;&#35201;&#26469;&#33258;&#26681;&#25454;&#28151;&#21512;&#26631;&#31614;&#20248;&#21270;&#28151;&#21512;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#39069;&#22806;&#30340;&#20248;&#21270;&#27493;&#39588;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#65292;&#22240;&#20026;&#26631;&#31614;&#19981;&#21305;&#37197;&#30340;&#28151;&#21512;&#26679;&#26412;&#23545;&#20110;&#28145;&#24230;&#27169;&#22411;&#26469;&#23450;&#20301;&#26377;&#24046;&#24322;&#24615;&#29305;&#24449;&#26159;&#26377;&#20449;&#24687;&#37327;&#30340;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#32806;Mixup&#65288;DM&#65289;&#30340;&#39640;&#25928;mixup&#30446;&#26631;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#25552;&#20986;&#26356;&#22797;&#26434;&#30340;&#21160;&#24577;mixup&#31574;&#30053;&#12290;&#20854;&#20027;&#35201;&#25928;&#26524;&#26159;DM&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#36825;&#20123;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;&#26469;&#25366;&#25496;&#20855;&#26377;&#21028;&#21035;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is an efficient data augmentation approach that improves the generalization of neural networks by smoothing the decision boundary with mixed data. Recently, dynamic mixup methods have improved previous static policies effectively (e.g., linear interpolation) by maximizing target-related salient regions in mixed samples, but excessive additional time costs are not acceptable. These additional computational overheads mainly come from optimizing the mixed samples according to the mixed labels. However, we found that the extra optimizing step may be redundant because label-mismatched mixed samples are informative hard mixed samples for deep models to localize discriminative features. In this paper, we thus are not trying to propose a more complicated dynamic mixup policy but rather an efficient mixup objective function with a decoupled regularizer named Decoupled Mixup (DM). The primary effect is that DM can adaptively utilize those hard mixed samples to mine discriminative features 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#36229;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#24635;&#32467;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#21464;&#20307;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#21508;&#31181;&#19982;HGNN&#30456;&#20851;&#30340;&#24212;&#29992;&#21644;&#24403;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2202.13852</link><description>&lt;p&gt;
&#36229;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Graph Neural Networks: A Review of Methods and Applications. (arXiv:2202.13852v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#36229;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#24635;&#32467;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#21464;&#20307;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#21508;&#31181;&#19982;HGNN&#30456;&#20851;&#30340;&#24212;&#29992;&#21644;&#24403;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#20102;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#24182;&#22240;&#20854;&#20986;&#33394;&#30340;&#34920;&#24449;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#27431;&#20960;&#37324;&#24471;&#27169;&#22411;&#22312;&#19982;&#22270;&#30456;&#20851;&#30340;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#34920;&#24449;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#39640;&#24230;&#38750;&#27431;&#20960;&#37324;&#24471;&#28508;&#22312;&#35299;&#21078;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#65292;&#36229;&#36793;&#30028;&#31354;&#38388;&#22312;&#22788;&#29702;&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#21644;&#24130;&#24459;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36825;&#24402;&#21151;&#20110;&#20854;&#25351;&#25968;&#32423;&#30340;&#22686;&#38271;&#29305;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#24403;&#21069;&#36229;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#23558;&#23427;&#20204;&#32479;&#19968;&#20026;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#24635;&#32467;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#21464;&#20307;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21508;&#31181;&#19982;HGNN&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#25104;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#25104;&#23601;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks generalize conventional neural networks to graph-structured data and have received widespread attention due to their impressive representation ability. In spite of the remarkable achievements, the performance of Euclidean models in graph-related learning is still bounded and limited by the representation ability of Euclidean geometry, especially for datasets with highly non-Euclidean latent anatomy. Recently, hyperbolic space has gained increasing popularity in processing graph data with tree-like structure and power-law distribution, owing to its exponential growth property. In this survey, we comprehensively revisit the technical details of the current hyperbolic graph neural networks, unifying them into a general framework and summarizing the variants of each component. More importantly, we present various HGNN-related applications. Last, we also identify several challenges, which potentially serve as guidelines for further flourishing the achievements of graph
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#24191;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#39532;&#37324;&#20848;&#24030;&#21508;&#22320;&#65288;&#37038;&#25919;&#32534;&#30721;&#65289;&#28909;&#39118;&#38505;&#65292;&#36890;&#36807;&#26500;&#24314;&#29305;&#24449;&#21521;&#37327;&#37327;&#21270;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#23614;&#37096;&#25968;&#25454;&#28857;&#35745;&#31639;&#29305;&#23450;&#21442;&#32771;&#21521;&#37327;&#65292;&#20197;&#25512;&#24191;&#39118;&#38505;&#35780;&#20272;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2202.10963</link><description>&lt;p&gt;
&#20351;&#29992;&#24191;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#31354;&#38388;&#28909;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for spatial heat risk assessment using a generalized similarity measure. (arXiv:2202.10963v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#24191;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#39532;&#37324;&#20848;&#24030;&#21508;&#22320;&#65288;&#37038;&#25919;&#32534;&#30721;&#65289;&#28909;&#39118;&#38505;&#65292;&#36890;&#36807;&#26500;&#24314;&#29305;&#24449;&#21521;&#37327;&#37327;&#21270;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#23614;&#37096;&#25968;&#25454;&#28857;&#35745;&#31639;&#29305;&#23450;&#21442;&#32771;&#21521;&#37327;&#65292;&#20197;&#25512;&#24191;&#39118;&#38505;&#35780;&#20272;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20004;&#20010;&#24120;&#29992;&#30340;&#25351;&#26631;&#21363;&#26292;&#38706;&#24230;&#21644;&#33030;&#24369;&#24615;&#26469;&#35780;&#20272;&#39532;&#37324;&#20848;&#24030;&#21508;&#22320;&#21306;&#65288;&#37038;&#25919;&#32534;&#30721;&#65289;&#28909;&#28798;&#23475;&#25152;&#23548;&#33268;&#30340;&#20581;&#24247;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#30456;&#24212;&#30340;&#29305;&#24449;&#21521;&#37327;&#26469;&#37327;&#21270;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#24182;&#21033;&#29992;&#32858;&#31867;&#25968;&#25454;&#23614;&#37096;&#30340;&#25968;&#25454;&#28857;&#26469;&#35745;&#31639;&#25351;&#26631;&#29305;&#23450;&#30340;&#21442;&#32771;&#21521;&#37327;&#65292;&#36825;&#20123;&#21442;&#32771;&#21521;&#37327;&#34920;&#31034;&#39640;&#39118;&#38505;&#29615;&#22659;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#36991;&#20813;&#20102;&#20197;&#20449;&#24687;&#29702;&#35770;&#29109;&#20026;&#22522;&#30784;&#30340;&#32858;&#21512;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#21518;&#32773;&#30340;&#20351;&#29992;&#22240;&#29109;&#30340;&#19981;&#21516;&#35266;&#28857;&#32780;&#21464;&#21270;&#65292;&#32780;&#36825;&#20123;&#35266;&#28857;&#22312;&#26412;&#36136;&#19978;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#26410;&#30693;&#21442;&#32771;&#28857;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#26469;&#25512;&#24191;&#39118;&#38505;&#35780;&#20272;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we develop a novel framework to assess health risks due to heat hazards across various localities (zip codes) across the state of Maryland with the help of two commonly used indicators i.e. exposure and vulnerability. Our approach quantifies each of the two aforementioned indicators by developing their corresponding feature vectors and subsequently computes indicator-specific reference vectors that signify a high risk environment by clustering the data points at the tail-end of an empirical risk spectrum. The proposed framework circumvents the information-theoretic entropy based aggregation methods whose usage varies with different views of entropy that are subjective in nature and more importantly generalizes the notion of risk-valuation using cosine similarity with unknown reference points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24674;&#22797;&#19981;&#21516;&#22823;&#23567;&#31038;&#21306;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#20808;&#21069;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#20351;&#24471;&#21487;&#20197;&#24674;&#22797;&#30340;&#32858;&#31867;&#22823;&#23567;&#19982;&#22522;&#30784;&#32858;&#31867;&#25968;&#30446;&#26080;&#20851;&#12290;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#31639;&#27861;&#22312;&#31181;&#26893;&#30340;&#22242;&#31751;&#29468;&#24819;&#19979;&#23454;&#29616;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#32858;&#31867;&#24674;&#22797;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.08522</link><description>&lt;p&gt;
&#29992;&#20110;&#25925;&#38556;&#21442;&#29031;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#38750;&#24179;&#34913;&#31038;&#21306;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Recovering Unbalanced Communities in the Stochastic Block Model With Application to Clustering with a Faulty Oracle. (arXiv:2202.08522v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24674;&#22797;&#19981;&#21516;&#22823;&#23567;&#31038;&#21306;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#20808;&#21069;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#20351;&#24471;&#21487;&#20197;&#24674;&#22797;&#30340;&#32858;&#31867;&#22823;&#23567;&#19982;&#22522;&#30784;&#32858;&#31867;&#25968;&#30446;&#26080;&#20851;&#12290;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#31639;&#27861;&#22312;&#31181;&#26893;&#30340;&#22242;&#31751;&#29468;&#24819;&#19979;&#23454;&#29616;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#32858;&#31867;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#26159;&#30740;&#31350;&#32593;&#32476;&#20013;&#22270;&#32858;&#31867;&#25110;&#31038;&#21306;&#26816;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#23427;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23545;&#20110;&#24179;&#34913;&#24773;&#20917;&#65292;&#21363;&#20551;&#35774;&#25152;&#26377;&#31038;&#21306;&#37117;&#20855;&#26377;&#36739;&#22823;&#30340;&#22823;&#23567;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#38750;&#24179;&#34913;&#31038;&#21306;&#30340;SBM&#65288;&#22312;&#23454;&#36341;&#20013;&#26356;&#30456;&#20851;&#65289;&#65292;&#25105;&#20204;&#23545;&#20854;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;&#31038;&#21306;&#30340;SBM&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;Ailon&#65292;Chen&#21644;Xu&#22312;ICML 2013&#21644;JMLR 2015&#30340;&#32467;&#26524;&#65292;&#28040;&#38500;&#20102;&#23384;&#22312;&#19968;&#20010;&#36739;&#22823;&#21306;&#38388;&#20351;&#24471;&#32858;&#31867;&#30340;&#22823;&#23567;&#19981;&#22312;&#20854;&#20869;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#21487;&#24674;&#22797;&#32858;&#31867;&#30340;&#22823;&#23567;&#19982;&#22522;&#30784;&#32858;&#31867;&#25968;&#30446;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#25913;&#36827;&#12290;&#22312;&#31181;&#26893;&#30340;&#22242;&#31751;&#29468;&#24819;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#30340;&#32858;&#31867;&#22823;&#23567;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes. We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons. Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#30340;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;/&#36317;&#31163;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#30340;p&#20540;&#65292;&#23545;&#20110;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#26816;&#27979;&#31561;&#39640;&#39118;&#38505;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2202.06593</link><description>&lt;p&gt;
&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#36317;&#31163;&#30340;&#32479;&#35745;&#25512;&#26029;&#21450;&#20854;&#22312;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference for the Dynamic Time Warping Distance, with Application to Abnormal Time-Series Detection. (arXiv:2202.06593v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#30340;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;/&#36317;&#31163;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#30340;p&#20540;&#65292;&#23545;&#20110;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#26816;&#27979;&#31561;&#39640;&#39118;&#38505;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#31639;&#27861;&#30340;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;/&#36317;&#31163;&#30340;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#12290;&#30001;&#20110;DTW&#36317;&#31163;&#26159;&#22522;&#20110;DTW&#31639;&#27861;&#30340;&#35299;&#24471;&#21040;&#30340;&#65292;&#20854;&#37319;&#26679;&#20998;&#24067;&#24456;&#38590;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#36873;&#25321;&#25512;&#26029;&#26694;&#26550;&#65292;&#33021;&#22815;&#25512;&#23548;&#20986;&#19968;&#31181;&#23545;DTW&#36317;&#31163;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;p&#20540;&#26469;&#37327;&#21270;DTW&#36317;&#31163;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20687;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#26816;&#27979;&#31561;&#39640;&#39118;&#38505;&#20915;&#31574;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#25512;&#26029;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study statistical inference on the similarity/distance between two time-series under uncertain environment by considering a statistical hypothesis test on the distance obtained from Dynamic Time Warping (DTW) algorithm. The sampling distribution of the DTW distance is too difficult to derive because it is obtained based on the solution of the DTW algorithm, which is complicated. To circumvent this difficulty, we propose to employ the conditional selective inference framework, which enables us to derive a valid inference method on the DTW distance. To our knowledge, this is the first method that can provide a valid p-value to quantify the statistical significance of the DTW distance, which is helpful for high-stake decision making such as abnormal time-series detection problems. We evaluate the performance of the proposed inference method on both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#24615;&#33021;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#19982;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;CMD&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2202.06152</link><description>&lt;p&gt;
&#36890;&#36807;&#21367;&#31215;&#21453;&#23556;&#19979;&#38477;&#20998;&#26512;&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Analysis of Dual-Based PID Controllers through Convolutional Mirror Descent. (arXiv:2202.06152v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#24615;&#33021;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#19982;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;CMD&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#65292;&#22914;&#22312;&#32447;&#24191;&#21578;&#20013;&#30340;&#39044;&#31639;&#33410;&#22863;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#22120;&#22312;&#23454;&#36341;&#20013;&#20197;&#21551;&#21457;&#24335;&#26041;&#24335;&#20351;&#29992;&#65292;&#24182;&#19988;&#23545;&#20854;&#24615;&#33021;&#27809;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#39318;&#27425;&#25552;&#20379;&#20102;&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#24615;&#33021;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#19982;&#19968;&#31181;&#21517;&#20026;&#8220;&#21367;&#31215;&#21453;&#23556;&#19979;&#38477;&#8221;&#65288;Convolutional Mirror Descent&#65292;CMD&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;&#19968;&#38454;&#31639;&#27861;&#20043;&#38388;&#30340;&#22522;&#26412;&#32852;&#31995;&#12290;CMD&#26681;&#25454;&#36807;&#21435;&#26799;&#24230;&#30340;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#26356;&#26032;&#36845;&#20195;&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;CMD&#24674;&#22797;&#20102;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#19982;&#21160;&#37327;&#21644;&#20048;&#35266;&#38236;&#20687;&#19979;&#38477;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;CMD&#21487;&#23454;&#29616;&#23545;&#24102;&#26377;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#19968;&#33324;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20302;&#36951;&#25022;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#26032;&#32467;&#26524;&#32473;&#20986;&#20102;&#20851;&#20110;&#32593;&#32476;&#24191;&#21578;&#20013;&#39044;&#31639;&#33410;&#22863;&#38382;&#39064;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual-based proportional-integral-derivative (PID) controllers are often employed in practice to solve online allocation problems with global constraints, such as budget pacing in online advertising. However, controllers are used in a heuristic fashion and come with no provable guarantees on their performance. This paper provides the first regret bounds on the performance of dual-based PID controllers for online allocation problems. We do so by first establishing a fundamental connection between dual-based PID controllers and a new first-order algorithm for online convex optimization called \emph{Convolutional Mirror Descent} (CMD), which updates iterates based on a weighted moving average of past gradients. CMD recovers, in a special case, online mirror descent with momentum and optimistic mirror descent. We establish sufficient conditions under which CMD attains low regret for general online convex optimization problems with adversarial inputs. We leverage this new result to give the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#25968;&#25454;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;MICoL&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.05932</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#25968;&#25454;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. (arXiv:2202.05932v2 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#25968;&#25454;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;MICoL&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#25991;&#26723;&#19982;&#20854;&#30456;&#20851;&#26631;&#31614;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#20174;&#19968;&#20010;&#22823;&#30340;&#20505;&#36873;&#38598;&#20013;&#36873;&#25321;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#65288;&#21363;&#35768;&#22810;&#26631;&#31614;&#21482;&#20986;&#29616;&#20960;&#27425;&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65292;&#36825;&#19981;&#38656;&#35201;&#20219;&#20309;&#24102;&#26377;&#26631;&#31614;&#30340;&#27880;&#37322;&#25991;&#26723;&#65292;&#21482;&#20381;&#36182;&#20110;&#26631;&#31614;&#30340;&#34920;&#38754;&#21517;&#31216;&#21644;&#25551;&#36848;&#12290;&#20026;&#20102;&#35757;&#32451;&#19968;&#20010;&#35745;&#31639;&#25991;&#26723;&#19982;&#26631;&#31614;&#20043;&#38388;&#30456;&#20284;&#24230;&#24471;&#20998;&#30340;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#25968;&#25454;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;MICoL&#65289;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#19981;&#21516;&#65292;MICoL&#21033;&#29992;&#20102;&#24191;&#27867;&#21487;&#29992;&#20110;Web&#19978;&#30340;&#25991;&#26723;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#20316;&#32773;&#65292;&#20250;&#35758;&#65292;&#30740;&#31350;&#35770;&#25991;&#30340;&#24341;&#29992;&#65289;&#26469;&#25512;&#23548;&#20986;&#30456;&#20284;&#30340;&#25991;&#26723;&#23545;&#12290;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;
&lt;/p&gt;
&lt;p&gt;
Large-scale multi-label text classification (LMTC) aims to associate a document with its relevant labels from a large candidate set. Most existing LMTC approaches rely on massive human-annotated training data, which are often costly to obtain and suffer from a long-tailed label distribution (i.e., many labels occur only a few times in the training set). In this paper, we study LMTC under the zero-shot setting, which does not require any annotated documents with labels and only relies on label surface names and descriptions. To train a classifier that calculates the similarity score between a document and a label, we propose a novel metadata-induced contrastive learning (MICoL) method. Different from previous text-based contrastive learning techniques, MICoL exploits document metadata (e.g., authors, venues, and references of research papers), which are widely available on the Web, to derive similar document-document pairs. Experimental results on two large-scale datasets show that: (1)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23558;&#26032;&#25968;&#25454;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.05069</link><description>&lt;p&gt;
&#19981;&#21516;&#36755;&#20837;&#32500;&#24230;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#19979;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case. (arXiv:2202.05069v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23558;&#26032;&#25968;&#25454;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#20256;&#24863;&#22120;&#21644;&#30417;&#27979;&#35774;&#22791;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36825;&#20123;&#25968;&#25454;&#26082;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23558;&#36825;&#20123;&#26032;&#36755;&#20837;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35814;&#32454;&#30740;&#31350;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#26032;&#25968;&#25454;&#21644;&#21382;&#21490;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#19979;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23545;&#35813;&#26041;&#27861;&#30340;&#30410;&#22788;&#36827;&#34892;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#26159;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of new sensors and monitoring devices, more sources of data become available to be used as inputs for machine learning models. These can on the one hand help to improve the accuracy of a model. On the other hand however, combining these new inputs with historical data remains a challenge that has not yet been studied in enough detail. In this work, we propose a transfer-learning algorithm that combines the new and the historical data, that is especially beneficial when the new data is scarce. We focus the approach on the linear regression case, which allows us to conduct a rigorous theoretical study on the benefits of the approach. We show that our approach is robust against negative transfer-learning, and we confirm this result empirically with real and simulated data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.11104</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#25509;&#22270;&#20013;&#25214;&#21040;&#26368;&#20248;&#36335;&#24452;&#38656;&#35201;&#30830;&#23450;&#27839;&#30528;&#22270;&#30340;&#36793;&#32536;&#34892;&#36827;&#30340;&#26368;&#23567;&#24635;&#25104;&#26412;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20960;&#31181;&#32463;&#20856;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#36890;&#24120;&#25152;&#26377;&#36793;&#32536;&#30340;&#25104;&#26412;&#37117;&#26159;&#39044;&#20808;&#23450;&#20041;&#22909;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#24819;&#35201;&#26681;&#25454;&#26576;&#20010;&#20219;&#21153;&#30340;&#35201;&#27714;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#25913;&#21464;&#25104;&#26412;&#26102;&#65292;&#36890;&#24120;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31361;&#35302;&#26435;&#37325;&#26469;&#23450;&#20041;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#36825;&#20801;&#35768;&#20351;&#29992;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#24403;&#20174;&#19968;&#20010;&#21021;&#22987;&#27963;&#36291;&#24230;&#20540;&#20026;1&#24320;&#22987;&#26102;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#30340;&#27963;&#21160;&#20256;&#25773;&#23558;&#23548;&#33268;&#19982;Bellman-Ford&#31639;&#27861;&#25214;&#21040;&#30340;&#35299;&#30456;&#21516;&#30340;&#35299;&#12290;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19982;Bellman-Ford&#30456;&#21516;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#65288;&#22914;&#36203;&#24067;&#23398;&#20064;&#65289;&#21487;&#20197;&#35843;&#25972;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal paths in connected graphs requires determining the smallest total cost for traveling along the graph's edges. This problem can be solved by several classical algorithms where, usually, costs are predefined for all edges. Conventional planning methods can, thus, normally not be used when wanting to change costs in an adaptive way following the requirements of some task. Here we show that one can define a neural network representation of path finding problems by transforming cost values into synaptic weights, which allows for online weight adaptation using network learning mechanisms. When starting with an initial activity value of one, activity propagation in this network will lead to solutions, which are identical to those found by the Bellman-Ford algorithm. The neural network has the same algorithmic complexity as Bellman-Ford and, in addition, we can show that network learning mechanisms (such as Hebbian learning) can adapt the weights in the network augmenting the r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#39640;&#26031;&#20998;&#24067;&#20013;&#20272;&#35745;&#28385;&#36275;&#23436;&#20840;&#27491;&#24615;&#30340;&#31934;&#30830;&#24230;&#30697;&#38453;&#65288;$\mathrm{MTP}_2$&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#24230;&#37327;&#25237;&#24433;&#26041;&#27861;&#30340;&#26032;&#31639;&#27861;&#65292;&#22823;&#24133;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2112.01939</link><description>&lt;p&gt;
&#39640;&#31934;&#24230;&#30697;&#38453;&#20272;&#35745;&#19979;&#30340;&#24555;&#36895;&#25237;&#24433;&#29275;&#39039;&#26679;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Projected Newton-like Method for Precision Matrix Estimation under Total Positivity. (arXiv:2112.01939v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#39640;&#26031;&#20998;&#24067;&#20013;&#20272;&#35745;&#28385;&#36275;&#23436;&#20840;&#27491;&#24615;&#30340;&#31934;&#30830;&#24230;&#30697;&#38453;&#65288;$\mathrm{MTP}_2$&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#24230;&#37327;&#25237;&#24433;&#26041;&#27861;&#30340;&#26032;&#31639;&#27861;&#65292;&#22823;&#24133;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#28385;&#36275;&#20108;&#38454;&#22810;&#20803;&#23436;&#20840;&#27491;&#24615;&#65288;$\mathrm{MTP}_2$&#65289;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#20272;&#35745;&#31934;&#30830;&#24230;&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#20998;&#24067;&#20013;&#30340;&#31934;&#30830;&#24230;&#30697;&#38453;&#26159;M&#30697;&#38453;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#31526;&#21495;&#32422;&#26463;&#30340;&#23545;&#25968;&#34892;&#21015;&#24335;&#35268;&#21010;&#12290;&#30446;&#21069;&#30340;&#31639;&#27861;&#20351;&#29992;&#22359;&#22352;&#26631;&#19979;&#38477;&#27861;&#25110;&#32773;&#36817;&#31471;&#28857;&#31639;&#27861;&#26469;&#35774;&#35745;&#65292;&#20294;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#22823;&#37327;&#38750;&#36127;&#20108;&#27425;&#35268;&#21010;&#25110;&#22823;&#35268;&#27169;&#32447;&#24615;&#31995;&#32479;&#32780;&#21464;&#24471;&#35745;&#31639;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#24230;&#37327;&#25237;&#24433;&#26041;&#27861;&#30340;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#25628;&#32034;&#26041;&#21521;&#21644;&#21464;&#37327;&#20998;&#21306;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#20854;&#29702;&#35770;&#25910;&#25947;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating precision matrices in Gaussian distributions that are multivariate totally positive of order two ($\mathrm{MTP}_2$). The precision matrix in such a distribution is an M-matrix. This problem can be formulated as a sign-constrained log-determinant program. Current algorithms are designed using the block coordinate descent method or the proximal point algorithm, which becomes computationally challenging in high-dimensional cases due to the requirement to solve numerous nonnegative quadratic programs or large-scale linear systems. To address this issue, we propose a novel algorithm based on the two-metric projection method, incorporating a carefully designed search direction and variable partitioning scheme. Our algorithm substantially reduces computational complexity, and its theoretical convergence is established. Experimental results on synthetic and real-world datasets demonstrate that our proposed algorithm provides a significant improvement in compu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RATE&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#25972;&#21512;&#20854;&#20182;&#29305;&#24449;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#23454;&#26102;&#20301;&#32622;&#20272;&#35745;&#20013;&#30340;&#25991;&#26412;&#29305;&#24449;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.06515</link><description>&lt;p&gt;
RATE: &#20811;&#26381;&#23454;&#26102;&#20301;&#32622;&#20272;&#35745;&#20013;&#25991;&#26412;&#29305;&#24449;&#30340;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
RATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location Estimation. (arXiv:2111.06515v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RATE&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#25972;&#21512;&#20854;&#20182;&#29305;&#24449;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#23454;&#26102;&#20301;&#32622;&#20272;&#35745;&#20013;&#30340;&#25991;&#26412;&#29305;&#24449;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#23454;&#26102;&#20301;&#32622;&#25512;&#26029;&#26159;&#19968;&#20123;&#31354;&#38388;&#24212;&#29992;&#65288;&#22914;&#26412;&#22320;&#25628;&#32034;&#21644;&#20107;&#20214;&#26816;&#27979;&#65289;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#25512;&#25991;&#25991;&#26412;&#26159;&#20301;&#32622;&#20272;&#35745;&#20013;&#26368;&#24120;&#29992;&#30340;&#29305;&#24449;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#21463;&#21040;&#25991;&#26412;&#29305;&#24449;&#22122;&#22768;&#25110;&#31232;&#30095;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#26500;&#24314;&#27169;&#22359;&#65292;&#20197;&#34920;&#24449;&#22320;&#29702;&#20027;&#39064;&#21464;&#21270;&#21644;&#35789;&#27719;&#21464;&#21270;&#65292;&#20174;&#32780;&#19981;&#20877;&#30452;&#25509;&#20351;&#29992;&#8220;one-hot&#8221;&#32534;&#30721;&#21521;&#37327;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#36890;&#36807;Twitter&#27969;API&#25552;&#21462;&#30340;&#20854;&#20182;&#29305;&#24449;&#26469;&#20811;&#26381;&#22122;&#22768;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;RATE&#31639;&#27861;&#22312;&#22320;&#21306;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#32428;&#24230;&#32463;&#24230;&#22238;&#24402;&#30340;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that "one-hot" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MotifClass&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#39640;&#38454;&#20803;&#25968;&#25454;&#20449;&#24687;&#26469;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#27169;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#22270;&#26696;&#25551;&#36848;&#20803;&#25968;&#25454;&#32452;&#21512;&#20197;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#31867;&#21035;&#25351;&#31034;&#24847;&#20041;&#30340;&#22270;&#26696;&#23454;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2111.04022</link><description>&lt;p&gt;
MotifClass: &#22522;&#20110;&#39640;&#38454;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information. (arXiv:2111.04022v3 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.04022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MotifClass&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#39640;&#38454;&#20803;&#25968;&#25454;&#20449;&#24687;&#26469;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#27169;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#22270;&#26696;&#25551;&#36848;&#20803;&#25968;&#25454;&#32452;&#21512;&#20197;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#31867;&#21035;&#25351;&#31034;&#24847;&#20041;&#30340;&#22270;&#26696;&#23454;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#23558;&#25991;&#26412;&#25991;&#26723;&#20998;&#31867;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#65292;&#20165;&#20351;&#29992;&#31867;&#21035;&#34920;&#38754;&#21517;&#31216;&#65292;&#32780;&#26080;&#38656;&#25552;&#20379;&#20219;&#20309;&#26631;&#27880;&#30340;&#35757;&#32451;&#25991;&#26723;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#29616;&#26377;&#20998;&#31867;&#22120;&#21033;&#29992;&#27599;&#20010;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#25991;&#26723;&#38468;&#24102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#20316;&#32773;&#12289;&#20986;&#22788;&#21644;&#30740;&#31350;&#35770;&#25991;&#30340;&#24180;&#20221;&#65289;&#12290;&#36825;&#20123;&#20803;&#25968;&#25454;&#21450;&#20854;&#32452;&#21512;&#21487;&#33021;&#20316;&#20026;&#24378;&#26377;&#21147;&#30340;&#31867;&#21035;&#25351;&#26631;&#65292;&#29992;&#20110;&#36741;&#21161;&#25991;&#26412;&#20998;&#31867;&#12290;&#26412;&#25991;&#36890;&#36807;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#24314;&#27169;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#22270;&#26696;&#25551;&#36848;&#20803;&#25968;&#25454;&#32452;&#21512;&#20197;&#26377;&#25928;&#25429;&#25417;&#32593;&#32476;&#20013;&#30340;&#39640;&#38454;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MotifClass&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#65288;1&#65289;&#36873;&#25321;&#20855;&#26377;&#31867;&#21035;&#25351;&#31034;&#24847;&#20041;&#30340;&#22270;&#26696;&#23454;&#20363;&#65292;&#65288;2&#65289;&#26816;&#32034;&#21644;&#29983;&#25104;&#20803;&#25968;&#25454;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of weakly supervised text classification, which aims to classify text documents into a set of pre-defined categories with category surface names only and without any annotated training document provided. Most existing classifiers leverage textual information in each document. However, in many domains, documents are accompanied by various types of metadata (e.g., authors, venue, and year of a research paper). These metadata and their combinations may serve as strong category indicators in addition to textual contents. In this paper, we explore the potential of using metadata to help weakly supervised text classification. To be specific, we model the relationships between documents and metadata via a heterogeneous information network. To effectively capture higher-order structures in the network, we use motifs to describe metadata combinations. We propose a novel framework, named MotifClass, which (1) selects category-indicative motif instances, (2) retrieves and gen
&lt;/p&gt;</description></item><item><title>MLMOD&#26159;&#19968;&#20010;&#29992;&#20110;LAMMPS&#20013;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#34920;&#31034;&#12289;&#27169;&#25311;&#21160;&#21147;&#23398;&#12289;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#21644;&#35745;&#31639;&#29305;&#23450;&#31995;&#32479;&#29366;&#24577;&#30340;&#24863;&#20852;&#36259;&#37327;&#12290;</title><link>http://arxiv.org/abs/2107.14362</link><description>&lt;p&gt;
MLMOD: &#29992;&#20110;LAMMPS&#20013;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MLMOD: Machine Learning Methods for Data-Driven Modeling in LAMMPS. (arXiv:2107.14362v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14362
&lt;/p&gt;
&lt;p&gt;
MLMOD&#26159;&#19968;&#20010;&#29992;&#20110;LAMMPS&#20013;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#34920;&#31034;&#12289;&#27169;&#25311;&#21160;&#21147;&#23398;&#12289;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#21644;&#35745;&#31639;&#29305;&#23450;&#31995;&#32479;&#29366;&#24577;&#30340;&#24863;&#20852;&#36259;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MLMOD&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#27169;&#22411;&#24212;&#29992;&#20110;LAMMPS&#20013;&#30340;&#24494;&#35266;&#21147;&#23398;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20026;&#20174;&#23454;&#39564;&#25968;&#25454;&#21644;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#20013;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#34920;&#31034;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#35813;&#36719;&#20214;&#21253;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#21644;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26469;&#27169;&#25311;&#65306;(i)&#31995;&#32479;&#22312;&#36739;&#22823;&#31354;&#38388;-&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#21160;&#21147;&#23398;;(ii)&#31995;&#32479;&#32452;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;;(iii)&#20135;&#29983;&#36739;&#31895;&#30340;&#33258;&#30001;&#24230;&#30340;&#29305;&#24449;;(iv)&#25551;&#36848;&#31995;&#32479;&#34892;&#20026;&#30340;&#26032;&#24863;&#20852;&#36259;&#37327;&#30340;&#29305;&#24449;&#12290;MLMOD&#22312;LAMMPS&#20013;&#25552;&#20379;&#20102;&#29992;&#20110;&#65306;(i)&#24314;&#27169;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#27493;&#31215;&#20998;;(ii)&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;;(iii)&#35745;&#31639;&#25551;&#36848;&#31995;&#32479;&#29366;&#24577;&#30340;&#24863;&#20852;&#36259;&#37327;&#30340;&#25509;&#21475;&#12290;&#35813;&#36719;&#20214;&#21253;&#20801;&#35768;&#20351;&#29992;&#19968;&#33324;&#27169;&#22411;&#31867;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#12289;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#12289;&#26680;&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
MLMOD is a software package for incorporating machine learning approaches and models into simulations of microscale mechanics and molecular dynamics in LAMMPS. Recent machine learning approaches provide promising data-driven approaches for learning representations for system behaviors from experimental data and high fidelity simulations. The package faciliates learning and using data-driven models for (i) dynamics of the system at larger spatial-temporal scales (ii) interactions between system components, (iii) features yielding coarser degrees of freedom, and (iv) features for new quantities of interest characterizing system behaviors. MLMOD provides hooks in LAMMPS for (i) modeling dynamics and time-step integration, (ii) modeling interactions, and (iii) computing quantities of interest characterizing system states. The package allows for use of machine learning methods with general model classes including Neural Networks, Gaussian Process Regression, Kernel Models, and other approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;&#36763;&#32467;&#26500;&#30340;&#25913;&#36827;&#35757;&#32451;&#26041;&#27861;&#65292;&#35299;&#25918;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#19979;&#30028;&#65292;&#20351;&#20854;&#21487;&#20197;&#23398;&#20064;&#21040;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;HNNs&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.11753</link><description>&lt;p&gt;
&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#36763;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symplectic Learning for Hamiltonian Neural Networks. (arXiv:2106.11753v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;&#36763;&#32467;&#26500;&#30340;&#25913;&#36827;&#35757;&#32451;&#26041;&#27861;&#65292;&#35299;&#25918;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#19979;&#30028;&#65292;&#20351;&#20854;&#21487;&#20197;&#23398;&#20064;&#21040;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;HNNs&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#24314;&#27169;&#21644;&#39044;&#27979;&#29289;&#29702;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#34987;&#24403;&#20316;&#19981;&#22826;&#29702;&#35299;&#30340;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#24573;&#35270;&#20102;&#38382;&#39064;&#30340;&#25968;&#23398;&#32467;&#26500;&#21644;&#19981;&#21464;&#37327;&#12290;&#26368;&#36817;&#65292;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#65288;HNNs&#65289;&#30340;&#25552;&#20986;&#36808;&#20986;&#20102;&#36808;&#21521;&#32479;&#19968;&#30340;&#8220;&#28784;&#30418;&#23376;&#8221;&#26041;&#27861;&#30340;&#31532;&#19968;&#27493;&#65292;&#21033;&#29992;&#29289;&#29702;&#27934;&#23519;&#21147;&#26469;&#25552;&#39640;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#30340;HNNs&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#36763;&#32467;&#26500;&#21644;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#23558;&#25439;&#22833;&#20174;&#20154;&#20026;&#30340;&#19979;&#30028;&#20013;&#35299;&#25918;&#20986;&#26469;&#12290;&#25105;&#20204;&#25968;&#23398;&#19978;&#20445;&#35777;&#20102;HNNs&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#20989;&#25968;&#30340;&#23384;&#22312;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#21644;&#25968;&#20540;&#20998;&#26512;HNNs&#25152;&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#23436;&#20840;&#21487;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#21518;&#26657;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31163;&#25955;&#35266;&#27979;&#20013;&#33719;&#24471;&#30495;&#23454;&#30340;&#21704;&#23494;&#39039;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods are widely used in the natural sciences to model and predict physical systems from observation data. Yet, they are often used as poorly understood "black boxes," disregarding existing mathematical structure and invariants of the problem. Recently, the proposal of Hamiltonian Neural Networks (HNNs) took a first step towards a unified "gray box" approach, using physical insight to improve performance for Hamiltonian systems. In this paper, we explore a significantly improved training method for HNNs, exploiting the symplectic structure of Hamiltonian systems with a different loss function. This frees the loss from an artificial lower bound. We mathematically guarantee the existence of an exact Hamiltonian function which the HNN can learn. This allows us to prove and numerically analyze the errors made by HNNs which, in turn, renders them fully explainable. Finally, we present a novel post-training correction to obtain the true Hamiltonian only from discretized ob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35823;&#24046;&#26377;&#30028;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;SZ&#27169;&#22411;&#30340;&#35823;&#24046;&#26377;&#30028;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65307;&#65288;2&#65289;&#20248;&#21270;&#20102;&#35774;&#35745;&#30340;&#22522;&#20110;AE&#30340;&#35823;&#24046;&#26377;&#30028;&#21387;&#32553;&#26694;&#26550;&#20013;&#30340;&#20027;&#35201;&#38454;&#27573;&#30340;&#21387;&#32553;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2105.11730</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35823;&#24046;&#26377;&#30028;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35823;&#24046;&#26377;&#30028;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;SZ&#27169;&#22411;&#30340;&#35823;&#24046;&#26377;&#30028;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65307;&#65288;2&#65289;&#20248;&#21270;&#20102;&#35774;&#35745;&#30340;&#22522;&#20110;AE&#30340;&#35823;&#24046;&#26377;&#30028;&#21387;&#32553;&#26694;&#26550;&#20013;&#30340;&#20027;&#35201;&#38454;&#27573;&#30340;&#21387;&#32553;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#24046;&#26377;&#30028;&#30340;&#26377;&#25439;&#21387;&#32553;&#23545;&#20110;&#24403;&#20170;&#31185;&#23398;&#39033;&#30446;&#30340;&#25104;&#21151;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#27169;&#25311;&#25110;&#20202;&#22120;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#24040;&#22823;&#12290;&#23427;&#19981;&#20165;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#22823;&#23567;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#35823;&#24046;&#30028;&#38480;&#26469;&#25511;&#21046;&#21387;&#32553;&#35823;&#24046;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#27169;&#22411;&#22312;&#22270;&#20687;&#21387;&#32553;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#24456;&#23569;&#26377;&#22522;&#20110;AE&#30340;&#21387;&#32553;&#26041;&#27861;&#25903;&#25345;&#35823;&#24046;&#30028;&#38480;&#29305;&#24615;&#65292;&#32780;&#36825;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#26159;&#38750;&#24120;&#38656;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#25913;&#36827;&#35823;&#24046;&#26377;&#30028;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20197;&#19979;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#29305;&#24615;&#65292;&#24182;&#22312;SZ&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#35823;&#24046;&#26377;&#30028;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20248;&#21270;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#22522;&#20110;AE&#30340;&#35823;&#24046;&#26377;&#30028;&#21387;&#32553;&#26694;&#26550;&#20013;&#30340;&#20027;&#35201;&#38454;&#27573;&#30340;&#21387;&#32553;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error-bounded lossy compression is becoming an indispensable technique for the success of today's scientific projects with vast volumes of data produced during simulations or instrument data acquisitions. Not only can it significantly reduce data size, but it also can control the compression errors based on user-specified error bounds. Autoencoder (AE) models have been widely used in image compression, but few AE-based compression approaches support error-bounding features, which are highly required by scientific applications. To address this issue, we explore using convolutional autoencoders to improve error-bounded lossy compression for scientific data, with the following three key contributions. (1) We provide an in-depth investigation of the characteristics of various autoencoder models and develop an error-bounded autoencoder-based framework in terms of the SZ model. (2) We optimize the compression quality for the main stages in our designed AE-based error-bounded compression fram
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#27714;&#35299;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#33258;&#36866;&#24212;&#22797;&#26434;&#24615;&#21644;&#20989;&#25968;&#27714;&#20540;&#24635;&#27425;&#25968;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2102.08327</link><description>&lt;p&gt;
&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#27714;&#35299;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65306;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#33258;&#36866;&#24212;&#22797;&#26434;&#24615;&#30340;&#32452;&#21512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Submodular Maximization subject to a Knapsack Constraint: Combinatorial Algorithms with Near-optimal Adaptive Complexity. (arXiv:2102.08327v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.08327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#27714;&#35299;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#33258;&#36866;&#24212;&#22797;&#26434;&#24615;&#21644;&#20989;&#25968;&#27714;&#20540;&#24635;&#27425;&#25968;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#26368;&#22823;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#20110;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32463;&#20856;&#31639;&#27861;&#38382;&#39064;&#12290;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#22686;&#21152;&#20419;&#20351;&#20102;&#24179;&#34913;&#35299;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#30340;&#31639;&#27861;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;O(log n)&#33258;&#36866;&#24212;&#22797;&#26434;&#24615;&#30340;&#24120;&#25968;&#22240;&#23376;&#36817;&#20284;&#31639;&#27861;&#12290;&#32780;&#20165;&#20165;&#25317;&#26377;&#20302;&#33258;&#36866;&#24212;&#22797;&#26434;&#24615;&#26159;&#19981;&#22815;&#30340;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#20989;&#25968;&#27714;&#20540;&#30340;&#24635;&#27425;&#25968;&#65288;&#25110;&#20540;&#26597;&#35810;&#65289;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#38656;&#35201;&#32422;O(n^2)&#27425;&#20540;&#26597;&#35810;&#65292;&#20294;&#21487;&#20197;&#20462;&#25913;&#20026;&#20165;&#38656;&#35201;&#32422;O(n)&#27425;&#20540;&#26597;&#35810;&#65292;&#24182;&#20445;&#25345;O(log^2n)&#30340;&#20302;&#33258;&#36866;&#24212;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular maximization is a classic algorithmic problem with multiple applications in data mining and machine learning; there, the growing need to deal with massive instances motivates the design of algorithms balancing the quality of the solution with applicability. For the latter, an important measure is the adaptive complexity, which captures the number of sequential rounds of parallel computation needed by an algorithm to terminate. In this work we obtain the first constant factor approximation algorithm for non-monotone submodular maximization subject to a knapsack constraint with near-optimal $O(\log n)$ adaptive complexity. Low adaptivity by itself, however, is not enough: a crucial feature to account for is represented by the total number of function evaluations (or value queries). Our algorithm asks $\tilde{O}(n^2)$ value queries, but can be modified to run with only $\tilde{O}(n)$ instead, while retaining a low adaptive complexity of $O(\log^2n)$. Besides the above improveme
&lt;/p&gt;</description></item><item><title>MATCH&#26159;&#19968;&#20010;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#22823;&#22411;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#20803;&#25968;&#25454;&#21644;&#23618;&#27425;&#20449;&#24687;&#12290;&#23427;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23558;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#23884;&#20837;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#27880;&#24847;&#21147;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#26041;&#24335;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2102.07349</link><description>&lt;p&gt;
MATCH: &#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#22823;&#22411;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MATCH: Metadata-Aware Text Classification in A Large Hierarchy. (arXiv:2102.07349v2 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07349
&lt;/p&gt;
&lt;p&gt;
MATCH&#26159;&#19968;&#20010;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#22823;&#22411;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#20803;&#25968;&#25454;&#21644;&#23618;&#27425;&#20449;&#24687;&#12290;&#23427;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23558;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#23884;&#20837;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#27880;&#24847;&#21147;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#26041;&#24335;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26159;&#25351;&#23558;&#27599;&#20010;&#32473;&#23450;&#30340;&#25991;&#26723;&#20998;&#37197;&#32473;&#19982;&#26631;&#31614;&#38598;&#26368;&#30456;&#20851;&#30340;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#25552;&#20379;&#32473;&#23450;&#25991;&#26723;&#30340;&#20803;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#21482;&#20851;&#27880;&#24314;&#27169;&#25991;&#26412;&#20449;&#24687;&#65292;&#23545;&#20110;&#21033;&#29992;&#20803;&#25968;&#25454;&#25110;&#23618;&#27425;&#32467;&#26500;&#20449;&#21495;&#30340;&#23581;&#35797;&#36739;&#23569;&#65292;&#24182;&#38750;&#20004;&#32773;&#20860;&#39038;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35268;&#33539;&#21270;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#22823;&#22411;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#19979;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#25968;&#19975;&#20010;&#26631;&#31614;&#65289;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MATCH&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20803;&#25968;&#25454;&#21644;&#23618;&#27425;&#20449;&#24687;&#12290;&#20026;&#20102;&#32467;&#21512;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#22312;&#30456;&#21516;&#31354;&#38388;&#20013;&#39044;&#35757;&#32451;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#30340;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#20102;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#24335;&#26469;&#27491;&#21017;&#21270;&#21442;&#25968;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification refers to the problem of assigning each given document its most relevant labels from the label set. Commonly, the metadata of the given documents and the hierarchy of the labels are available in real-world applications. However, most existing studies focus on only modeling the text information, with a few attempts to utilize either metadata or hierarchy signals, but not both of them. In this paper, we bridge the gap by formalizing the problem of metadata-aware text classification in a large label hierarchy (e.g., with tens of thousands of labels). To address this problem, we present the MATCH solution -- an end-to-end framework that leverages both metadata and hierarchy information. To incorporate metadata, we pre-train the embeddings of text and metadata in the same space and also leverage the fully-connected attentions to capture the interrelations between them. To leverage the label hierarchy, we propose different ways to regularize the parameters and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#23558;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#25972;&#21512;&#36215;&#26469;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;HiMeCat&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#22312;&#21482;&#26377;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#25991;&#26723;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2010.13556</link><description>&lt;p&gt;
&#20998;&#23618;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#24369;&#30417;&#30563;&#19979;&#25991;&#26723;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Metadata-Aware Document Categorization under Weak Supervision. (arXiv:2010.13556v2 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.13556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#23558;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#25972;&#21512;&#36215;&#26469;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;HiMeCat&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#22312;&#21482;&#26377;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#25991;&#26723;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26222;&#36941;&#23384;&#22312;&#23618;&#27425;&#21270;&#20027;&#39064;&#32467;&#26500;&#65292;&#23558;&#25991;&#26723;&#20998;&#31867;&#21040;&#32473;&#23450;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20013;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#12290;&#23613;&#31649;&#30456;&#20851;&#30740;&#31350;&#22312;&#23436;&#20840;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26723;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21482;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#65288;1&#65289;&#26631;&#27880;&#38750;&#24120;&#26114;&#36149;&#65292;&#38590;&#20197;&#33719;&#21462;&#21040;&#24456;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#65307;&#65288;2&#65289;&#25991;&#26723;&#38468;&#24102;&#20803;&#25968;&#25454;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#25972;&#21512;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;HiMeCat&#65292;&#19968;&#20010;&#22522;&#20110;&#23884;&#20837;&#24335;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#65292;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#65292;&#29992;&#20110;&#20998;&#23618;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorizing documents into a given label hierarchy is intuitively appealing due to the ubiquity of hierarchical topic structures in massive text corpora. Although related studies have achieved satisfying performance in fully supervised hierarchical document classification, they usually require massive human-annotated training data and only utilize text information. However, in many domains, (1) annotations are quite expensive where very few training samples can be acquired; (2) documents are accompanied by metadata information. Hence, this paper studies how to integrate the label hierarchy, metadata, and text signals for document categorization under weak supervision. We develop HiMeCat, an embedding-based generative framework for our task. Specifically, we propose a novel joint representation learning module that allows simultaneous modeling of category dependencies, metadata information and textual semantics, and we introduce a data augmentation module that hierarchically synthesize
&lt;/p&gt;</description></item><item><title>Gasper&#26159;&#19968;&#20010;&#22312;R&#20013;&#36827;&#34892;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;&#30340;&#21253;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19982;SuiteSparse&#30697;&#38453;&#38598;&#21512;&#30340;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2007.10642</link><description>&lt;p&gt;
Gasper&#65306;&#22312;R&#20013;&#36827;&#34892;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Gasper: GrAph Signal ProcEssing in R. (arXiv:2007.10642v4 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.10642
&lt;/p&gt;
&lt;p&gt;
Gasper&#26159;&#19968;&#20010;&#22312;R&#20013;&#36827;&#34892;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;&#30340;&#21253;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19982;SuiteSparse&#30697;&#38453;&#38598;&#21512;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20351;&#29992;R&#35821;&#35328;&#30340;gasper&#21253;&#30340;&#31616;&#30701;&#25945;&#31243;&#12290;Gasper&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;&#30340;&#21253;&#65292;&#36824;&#25552;&#20379;&#20102;&#19982;SuiteSparse&#30697;&#38453;&#38598;&#21512;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a short tutorial on to the use of the \proglang{R} \pkg{gasper} package. Gasper is a package dedicated to signal processing on graphs. It also provides an interface to the SuiteSparse Matrix Collection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#20197;O(log n)&#30340;&#22797;&#26434;&#24230;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#23558;&#20989;&#25968;&#20540;&#35810;&#38382;&#30340;&#25968;&#37327;&#20943;&#23569;&#21040;O(n)&#32780;&#20445;&#25345;&#36739;&#20302;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2007.05014</link><description>&lt;p&gt;
&#24555;&#36895;&#36866;&#24212;&#24615;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack Constraint. (arXiv:2007.05014v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#20197;O(log n)&#30340;&#22797;&#26434;&#24230;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#23558;&#20989;&#25968;&#20540;&#35810;&#38382;&#30340;&#25968;&#37327;&#20943;&#23569;&#21040;O(n)&#32780;&#20445;&#25345;&#36739;&#20302;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#26368;&#22823;&#21270;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#22810;&#31181;&#24212;&#29992;&#12290;&#38024;&#23545;&#22823;&#35268;&#27169;&#23454;&#20363;&#30340;&#38656;&#27714;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#35299;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#31639;&#27861;&#25104;&#20026;&#21160;&#21147;&#12290;&#26412;&#25991;&#38024;&#23545;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#24120;&#25968;&#36924;&#36817;&#27604;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;O(log n)&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27714;&#35299;&#36807;&#31243;&#20013;&#35810;&#38382;&#30340;&#20989;&#25968;&#20540;&#25968;&#37327;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#21464;&#20026;O(n)&#32780;&#20445;&#25345;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#20026;O(log^2n)&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular maximization is a classic algorithmic problem with multiple applications in data mining and machine learning; there, the growing need to deal with massive instances motivates the design of algorithms balancing the quality of the solution with applicability. For the latter, an important measure is the \emph{adaptive complexity}, which captures the number of sequential rounds of parallel computation needed by an algorithm to terminate. In this work, we obtain the first \emph{constant factor} approximation algorithm for non-monotone submodular maximization subject to a knapsack constraint with \emph{near-optimal} $O(\log n)$ adaptive complexity. Low adaptivity by itself, however, is not enough: a crucial feature to account for is represented by the total number of function evaluations (or value queries). Our algorithm asks $\tilde{O}(n^2)$ value queries but can be modified to run with only $\tilde{O}(n)$ instead while retaining a low adaptive complexity of $O(\log^2n)$. Besides
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2005.00624</link><description>&lt;p&gt;
&#25991;&#26412;&#19982;&#20803;&#25968;&#25454;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Minimally Supervised Categorization of Text with Metadata. (arXiv:2005.00624v3 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#20998;&#31867;&#26159;&#23558;&#20027;&#39064;&#26631;&#31614;&#20998;&#37197;&#32473;&#27599;&#20010;&#25991;&#26723;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20256;&#32479;&#30417;&#30563;&#25991;&#26723;&#20998;&#31867;&#30740;&#31350;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36739;&#23569;&#20851;&#27880;&#20004;&#20010;&#23454;&#38469;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23384;&#22312;&#20803;&#25968;&#25454;&#65306;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#25991;&#26412;&#20276;&#38543;&#30528;&#21508;&#31181;&#38468;&#21152;&#20449;&#24687;&#65292;&#20363;&#22914;&#20316;&#32773;&#21644;&#26631;&#31614;&#12290;&#36825;&#20123;&#20803;&#25968;&#25454;&#20316;&#20026;&#26377;&#21147;&#30340;&#20027;&#39064;&#25351;&#31034;&#22120;&#65292;&#24212;&#35813;&#34987;&#21033;&#29992;&#21040;&#20998;&#31867;&#26694;&#26550;&#20013;&#65307;&#65288;2&#65289;&#26631;&#31614;&#31232;&#32570;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26377;&#26631;&#31614;&#30340;&#35757;&#32451;&#26679;&#26412;&#26159;&#26114;&#36149;&#30340;&#65292;&#38656;&#35201;&#21482;&#20351;&#29992;&#23569;&#37327;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#37492;&#20110;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#26469;&#25551;&#36848;&#21333;&#35789;&#12289;&#25991;&#26723;&#12289;&#26631;&#31614;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26681;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#23884;&#20837;&#21040;&#20998;&#31867;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1) the presence of metadata: in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2) label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into th
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#21644;&#21322;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/1912.05957</link><description>&lt;p&gt;
&#25991;&#26412;&#20316;&#20026;&#29615;&#22659;:&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model. (arXiv:1912.05957v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.05957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#21644;&#21322;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#21487;&#20197;&#26174;&#33879;&#20419;&#36827;&#20449;&#24687;&#30340;&#20934;&#30830;&#34920;&#36798;&#12290;&#25991;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;&#30340;&#21046;&#23450;&#28041;&#21450;&#23545;&#25991;&#26412;&#30340;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#36827;&#34892;&#35782;&#21035;&#65292;&#32780;&#19981;&#35770;&#20854;&#38271;&#24230;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#25991;&#26412;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#39640;&#25928;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#38382;&#39064;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36755;&#20837;&#25991;&#26412;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#20351;&#29992;&#21322;&#30417;&#30563;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26368;&#23569;&#30340;&#25991;&#26412;&#26469;&#30830;&#23450;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#12290;&#23558;&#35813;&#27169;&#22411;&#19982;Weebit&#21644;&#21073;&#26725;&#32771;&#35797;&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the readability of a text can significantly facilitate the precise expression of information in written form. The formulation of text readability assessment involves the identification of meaningful properties of the text regardless of its length. Sophisticated features and models are used to evaluate the comprehensibility of texts accurately. Despite this, the problem of assessing texts' readability efficiently remains relatively untouched. The efficiency of state-of-the-art text readability assessment models can be further improved using deep reinforcement learning models. Using a hard attention-based active inference technique, the proposed approach makes efficient use of input text and computational resources. Through the use of semi-supervised signals, the reinforcement learning model uses the minimum amount of text in order to determine text's readability. A comparison of the model on Weebit and Cambridge Exams with state-of-the-art models, such as the BERT text readab
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pani&#30340;&#36890;&#29992;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#38750;&#23616;&#37096;&#34920;&#31034;&#65292;&#24182;&#23558;&#37051;&#22495;&#34917;&#19969;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/1911.09307</link><description>&lt;p&gt;
Patch-level Neighborhood Interpolation: &#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy. (arXiv:1911.09307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.09307
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pani&#30340;&#36890;&#29992;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#38750;&#23616;&#37096;&#34920;&#31034;&#65292;&#24182;&#23558;&#37051;&#22495;&#34917;&#19969;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#19988;&#20165;&#32771;&#34385;&#24403;&#21069;&#26679;&#26412;&#30340;&#30693;&#35782;&#65292;&#27809;&#26377;&#21033;&#29992;&#26679;&#26412;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Patch-level Neighborhood Interpolation&#65288;Pani&#65289;&#8221;&#30340;&#36890;&#29992;&#27491;&#21017;&#21270;&#22120;&#65292;&#22312;&#32593;&#32476;&#35745;&#31639;&#20013;&#36827;&#34892;&#38750;&#23616;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26126;&#30830;&#22320;&#26500;&#24314;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#19969;&#32423;&#22270;&#65292;&#28982;&#21518;&#32447;&#24615;&#25554;&#20540;&#37051;&#22495;&#34917;&#19969;&#29305;&#24449;&#65292;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#21046;&#20026;&#20004;&#31181;&#27969;&#34892;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#65288;VAT&#65289;&#21644;MixUp&#20197;&#21450;&#20854;&#21464;&#20307;&#12290;&#39318;&#20808;&#27966;&#29983;&#30340;&#8220;Pani VAT&#8221;&#36890;&#36807;&#20351;&#29992;&#34917;&#19969;&#32423;&#25554;&#20540;&#25200;&#21160;&#26500;&#24314;&#38750;&#23616;&#37096;&#23545;&#25239;&#24179;&#28369;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly rely on the i.i.d. assumption and only consider the knowledge from the current sample, without the leverage of the neighboring relationship between samples. In this work, we propose a general regularizer called \textbf{Patch-level Neighborhood Interpolation~(Pani)} that conducts a non-local representation in the computation of networks. Our proposal explicitly constructs patch-level graphs in different layers and then linearly interpolates neighborhood patch features, serving as a general and effective regularization strategy. Further, we customize our approach into two kinds of popular regularization methods, namely Virtual Adversarial Training (VAT) and MixUp as well as its variants. The first derived \textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by employing patch-level interpolated perturbations. Th
&lt;/p&gt;</description></item><item><title>HiGitClass&#26159;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;GitHub&#20179;&#24211;&#30340;&#20998;&#23618;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#24102;&#26377;&#20851;&#38190;&#35789;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#30417;&#30563;&#12290;&#26694;&#26550;&#35299;&#20915;&#20102;&#22810;&#27169;&#24335;&#20449;&#21495;&#12289;&#30417;&#30563;&#31232;&#32570;&#24615;&#21644;&#30417;&#30563;&#26684;&#24335;&#19981;&#21305;&#37197;&#31561;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/1910.07115</link><description>&lt;p&gt;
HiGitClass: &#22522;&#20110;&#20851;&#38190;&#35789;&#30340;GitHub&#20179;&#24211;&#30340;&#20998;&#23618;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories. (arXiv:1910.07115v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.07115
&lt;/p&gt;
&lt;p&gt;
HiGitClass&#26159;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;GitHub&#20179;&#24211;&#30340;&#20998;&#23618;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#24102;&#26377;&#20851;&#38190;&#35789;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#30417;&#30563;&#12290;&#26694;&#26550;&#35299;&#20915;&#20102;&#22810;&#27169;&#24335;&#20449;&#21495;&#12289;&#30417;&#30563;&#31232;&#32570;&#24615;&#21644;&#30417;&#30563;&#26684;&#24335;&#19981;&#21305;&#37197;&#31561;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GitHub&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#20195;&#30721;&#20998;&#20139;&#21644;&#31185;&#23398;&#20132;&#27969;&#24179;&#21488;&#12290;&#30001;&#20110;&#21487;&#29992;&#30340;&#20179;&#24211;&#25968;&#37327;&#24222;&#22823;&#65292;&#38656;&#35201;&#22522;&#20110;&#20027;&#39064;&#36827;&#34892;&#25628;&#32034;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#20027;&#39064;&#26631;&#31614;&#21151;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;GitHub&#20179;&#24211;&#37117;&#27809;&#26377;&#20219;&#20309;&#26631;&#31614;&#65292;&#38480;&#21046;&#20102;&#25628;&#32034;&#21644;&#22522;&#20110;&#20027;&#39064;&#30340;&#20998;&#26512;&#30340;&#25928;&#29992;&#12290;&#26412;&#30740;&#31350;&#23558;&#33258;&#21160;&#20179;&#24211;&#20998;&#31867;&#38382;&#39064;&#23450;&#20301;&#20026;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#20998;&#23618;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#24102;&#26377;&#20851;&#38190;&#35789;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#30417;&#30563;&#12290;&#36825;&#31181;&#35774;&#32622;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#36866;&#24212;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#32771;&#34385;&#21040;&#20027;&#39064;&#26631;&#31614;&#30340;&#19981;&#21516;&#31890;&#24230;&#65292;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#65288;1&#65289;&#22810;&#27169;&#24335;&#20449;&#21495;&#30340;&#23384;&#22312;&#65307;&#65288;2&#65289;&#30417;&#30563;&#31232;&#32570;&#24615;&#21644;&#20559;&#35265;&#65307;&#65288;3&#65289;&#30417;&#30563;&#26684;&#24335;&#19981;&#21305;&#37197;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiGitClass&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;heterogeneou
&lt;/p&gt;
&lt;p&gt;
GitHub has become an important platform for code sharing and scientific exchange. With the massive number of repositories available, there is a pressing need for topic-based search. Even though the topic label functionality has been introduced, the majority of GitHub repositories do not have any labels, impeding the utility of search and topic-based analysis. This work targets the automatic repository classification problem as keyword-driven hierarchical classification. Specifically, users only need to provide a label hierarchy with keywords to supply as supervision. This setting is flexible, adaptive to the users' needs, accounts for the different granularity of topic labels and requires minimal human effort. We identify three key challenges of this problem, namely (1) the presence of multi-modal signals; (2) supervision scarcity and bias; (3) supervision format mismatch. In recognition of these challenges, we propose the HiGitClass framework, comprising of three modules: heterogeneou
&lt;/p&gt;</description></item><item><title>TimbreTron&#26159;&#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#38899;&#33394;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#39046;&#22495;&#30340;&#39118;&#26684;&#36716;&#25442;&#24212;&#29992;&#21040;&#38899;&#39057;&#20449;&#21495;&#30340;&#26102;&#39057;&#34920;&#31034;&#19978;&#65292;&#28982;&#21518;&#20351;&#29992;&#26465;&#20214;WaveNet&#21512;&#25104;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27874;&#24418;&#12290;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#35780;&#20272;&#65292;&#35777;&#23454;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/1811.09620</link><description>&lt;p&gt;
TimbreTron&#65306;&#29992;&#20110;&#38899;&#20048;&#38899;&#33394;&#36716;&#25442;&#30340;WaveNet&#65288;CycleGAN&#65288;CQT&#65288;Audio&#65289;&#65289;&#65289;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer. (arXiv:1811.09620v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1811.09620
&lt;/p&gt;
&lt;p&gt;
TimbreTron&#26159;&#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#38899;&#33394;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#39046;&#22495;&#30340;&#39118;&#26684;&#36716;&#25442;&#24212;&#29992;&#21040;&#38899;&#39057;&#20449;&#21495;&#30340;&#26102;&#39057;&#34920;&#31034;&#19978;&#65292;&#28982;&#21518;&#20351;&#29992;&#26465;&#20214;WaveNet&#21512;&#25104;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27874;&#24418;&#12290;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#35780;&#20272;&#65292;&#35777;&#23454;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#38899;&#20048;&#38899;&#33394;&#36716;&#25442;&#30340;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23558;&#19968;&#20010;&#20048;&#22120;&#30340;&#22768;&#38899;&#26679;&#26412;&#30340;&#38899;&#33394;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#20048;&#22120;&#30340;&#38899;&#33394;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20182;&#38899;&#20048;&#20869;&#23481;&#65292;&#22914;&#38899;&#39640;&#12289;&#33410;&#22863;&#21644;&#38899;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TimbreTron&#65292;&#19968;&#31181;&#38899;&#20048;&#38899;&#33394;&#36716;&#25442;&#26041;&#27861;&#65292;&#23427;&#23558;&#8220;&#22270;&#20687;&#8221;&#39046;&#22495;&#30340;&#39118;&#26684;&#36716;&#25442;&#24212;&#29992;&#21040;&#38899;&#39057;&#20449;&#21495;&#30340;&#26102;&#39057;&#34920;&#31034;&#19978;&#65292;&#28982;&#21518;&#20351;&#29992;&#26465;&#20214;WaveNet&#21512;&#25104;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Constant Q Transform&#65288;CQT&#65289;&#34920;&#31034;&#29305;&#21035;&#36866;&#21512;&#21367;&#31215;&#32467;&#26500;&#65292;&#22240;&#20854;&#36817;&#20284;&#30340;&#38899;&#39640;&#31561;&#21464;&#24615;&#12290;&#22522;&#20110;&#20154;&#31867;&#24863;&#30693;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#35748;TimbreTron&#21487;&#20197;&#34987;&#35782;&#21035;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies "image" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably tra
&lt;/p&gt;</description></item></channel></rss>