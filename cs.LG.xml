<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01588</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#32422;&#26463;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#25688;&#35201;&#32479;&#35745;&#37327;&#65288;&#22914;&#21151;&#29575;&#35889;&#65289;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#22797;&#26434;&#23431;&#23449;&#23398;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#25311;&#22871;&#20214;&#20013;&#30340;&#23376;&#32593;&#26684;&#29289;&#29702;&#23454;&#29616;&#21644;&#25968;&#20540;&#36924;&#36817;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#19968;&#20010;&#23431;&#23449;&#23398;&#27169;&#25311;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;&#22312;&#21478;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20250;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#24212;&#29992;&#20110;&#35266;&#27979;&#25968;&#25454;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#22871;&#20214;&#30340;CAMELS&#27700;&#21160;&#21147;&#23431;&#23449;&#23398;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DA-GNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#25429;&#25417;&#26469;&#33258;&#26143;&#31995;&#20998;&#24067;&#30340;&#32467;&#26500;&#26080;&#26631;&#24230;&#23431;&#23449;&#23398;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21253;&#25324;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#36866;&#37197;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#21464;&#20998;&#25512;&#26029;&#65288;DMVI&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#20013;&#36827;&#34892;&#33258;&#21160;&#36817;&#20284;&#25512;&#26029;&#12290;DMVI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#21518;&#39564;&#25512;&#26029;&#65292;&#32780;&#19988;&#26131;&#20110;&#23454;&#29616;&#21644;&#20351;&#29992;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27809;&#26377;&#20219;&#20309;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2311.00474</link><description>&lt;p&gt;
&#27010;&#29575;&#32534;&#31243;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion models for probabilistic programming. (arXiv:2311.00474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00474
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#21464;&#20998;&#25512;&#26029;&#65288;DMVI&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#20013;&#36827;&#34892;&#33258;&#21160;&#36817;&#20284;&#25512;&#26029;&#12290;DMVI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#21518;&#39564;&#25512;&#26029;&#65292;&#32780;&#19988;&#26131;&#20110;&#23454;&#29616;&#21644;&#20351;&#29992;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27809;&#26377;&#20219;&#20309;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#27169;&#22411;&#21464;&#20998;&#25512;&#26029;&#65288;DMVI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#65288;PPL&#65289;&#20013;&#36827;&#34892;&#33258;&#21160;&#36817;&#20284;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;DMVI&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#23545;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#21464;&#20998;&#36817;&#20284;&#65292;&#36890;&#36807;&#23548;&#20986;&#36125;&#21494;&#26031;&#24314;&#27169;&#20013;&#20351;&#29992;&#30340;&#36793;&#38469;&#20284;&#28982;&#30446;&#26631;&#30340;&#26032;&#32422;&#26463;&#12290;DMVI&#26131;&#20110;&#23454;&#29616;&#65292;&#22312;PPL&#20013;&#36827;&#34892;&#26080;&#38556;&#30861;&#25512;&#26029;&#65292;&#19981;&#20687;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#30340;&#21464;&#20998;&#25512;&#26029;&#37027;&#26679;&#20855;&#26377;&#32570;&#28857;&#65292;&#24182;&#19988;&#23545;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19981;&#20570;&#20219;&#20309;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#24120;&#35265;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;DMVI&#65292;&#24182;&#34920;&#26126;&#23427;&#30340;&#21518;&#39564;&#25512;&#26029;&#19968;&#33324;&#27604;PPL&#20013;&#20351;&#29992;&#30340;&#29616;&#20195;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#30340;&#35745;&#31639;&#25104;&#26412;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#25163;&#21160;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Diffusion Model Variational Inference (DMVI), a novel method for automated approximate inference in probabilistic programming languages (PPLs). DMVI utilizes diffusion models as variational approximations to the true posterior distribution by deriving a novel bound to the marginal likelihood objective used in Bayesian modelling. DMVI is easy to implement, allows hassle-free inference in PPLs without the drawbacks of, e.g., variational inference using normalizing flows, and does not make any constraints on the underlying neural network model. We evaluate DMVI on a set of common Bayesian models and show that its posterior inferences are in general more accurate than those of contemporary methods used in PPLs while having a similar computational cost and requiring less manual tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.15290</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#21487;&#38752;&#22320;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#20016;&#23500;&#30340;&#24739;&#32773;&#32423;&#25968;&#25454;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#39564;&#23460;&#26816;&#39564;&#12289;&#33647;&#29289;&#21644;&#35786;&#26029;&#65292;&#20026;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#38544;&#31169;&#30340;&#25285;&#24551;&#24120;&#24120;&#38480;&#21046;&#20102;&#23545;EHR&#30340;&#35775;&#38382;&#65292;&#38459;&#30861;&#20102;&#19979;&#28216;&#20998;&#26512;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20445;&#25252;&#38544;&#31169;&#30340;EHR&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#19971;&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#26469;&#22686;&#24378;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHRs) are rich sources of patient-level data, including laboratory tests, medications, and diagnoses, offering valuable resources for medical data analysis. However, concerns about privacy often restrict access to EHRs, hindering downstream analysis. Researchers have explored various methods for generating privacy-preserving EHR data. In this study, we introduce a new method for generating diverse and realistic synthetic EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six datasets, comparing our proposed method with seven existing methods. Our results demonstrate that our approach significantly outperforms all existing methods in terms of data utility while requiring less training effort. Our approach also enhances downstream medical data analysis by providing diverse and realistic synthetic EHR data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#31034;&#23454;&#29616;&#20102;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#20351;&#24471;&#19981;&#21516;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#36716;&#31227;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13458</link><description>&lt;p&gt;
&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#30340;&#20219;&#21153;&#28436;&#31034;&#23398;&#20064;&#19982;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Correspondence learning between morphologically different robots through task demonstrations. (arXiv:2310.13458v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#31034;&#23454;&#29616;&#20102;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#20351;&#24471;&#19981;&#21516;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#36716;&#31227;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#26426;&#22120;&#20154;&#22312;&#20854;&#26426;&#36523;&#12289;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#26041;&#38754;&#26377;&#30528;&#21508;&#31181;&#21508;&#26679;&#30340;&#24046;&#24322;&#12290;&#32771;&#34385;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#65292;&#29420;&#31435;&#22320;&#25945;&#23548;&#27599;&#20010;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;&#27599;&#20010;&#25216;&#33021;&#26159;&#20302;&#25928;&#19988;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;&#24863;&#23448;&#36816;&#21160;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#37027;&#20040;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#30340;&#25216;&#33021;&#21487;&#20197;&#26356;&#30452;&#25509;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#26426;&#22120;&#20154;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#20851;&#33410;&#25511;&#21046;&#30340;&#22266;&#23450;&#22522;&#24231;&#25805;&#32437;&#26426;&#22120;&#20154;&#21644;&#24046;&#21160;&#39537;&#21160;&#31227;&#21160;&#26426;&#22120;&#20154;&#20043;&#38388;&#23398;&#20064;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#35753;&#20004;&#20010;&#26426;&#22120;&#20154;&#36827;&#34892;&#25191;&#34892;&#30456;&#21516;&#20219;&#21153;&#30340;&#28436;&#31034;&#12290;&#22312;&#23398;&#20064;&#23545;&#24212;&#31574;&#30053;&#30340;&#21516;&#26102;&#24418;&#25104;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#21021;&#22987;&#23398;&#20064;&#38454;&#27573;&#20043;&#21518;&#65292;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#26032;&#20219;&#21153;&#25191;&#34892;&#23601;&#36275;&#20197;&#29983;&#25104;&#19968;&#20010;&#28508;&#21464;&#37327;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24212;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe a large variety of robots in terms of their bodies, sensors, and actuators. Given the commonalities in the skill sets, teaching each skill to each different robot independently is inefficient and not scalable when the large variety in the robotic landscape is considered. If we can learn the correspondences between the sensorimotor spaces of different robots, we can expect a skill that is learned in one robot can be more directly and easily transferred to the other robots. In this paper, we propose a method to learn correspondences between robots that have significant differences in their morphologies: a fixed-based manipulator robot with joint control and a differential drive mobile robot. For this, both robots are first given demonstrations that achieve the same tasks. A common latent representation is formed while learning the corresponding policies. After this initial learning stage, the observation of a new task execution by one robot becomes sufficient to generate a lat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12942</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#12290;Siegelmann&#21644;Sontag(1992)&#26366;&#32463;&#23637;&#31034;&#20102;&#20855;&#26377;&#26377;&#29702;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#30340;RNNs&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#28982;&#32780;&#65292;LMs&#19981;&#20165;&#23450;&#20041;&#20102;&#23383;&#31526;&#20018;&#19978;&#30340;&#21152;&#26435;&#65292;&#36824;&#23450;&#20041;&#20102;(&#38750;&#21152;&#26435;)&#35821;&#35328;&#25104;&#21592;&#20851;&#31995;&#65292;&#23545;RNN LMs&#65288;RLMs&#65289;&#30340;&#35745;&#31639;&#33021;&#21147;&#20998;&#26512;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23558;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26377;&#29702;&#26435;&#37325;&#30340;RLM&#21644;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#26469;&#27169;&#25311;&#20219;&#20309;&#27010;&#29575;&#22270;&#28789;&#26426;(PTM)&#12290;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#65292;RLMs&#23454;&#26102;&#24037;&#20316;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#19968;&#20010;&#31526;&#21495;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#19978;&#36848;&#32467;&#26524;&#20316;&#20026;RLMs&#34920;&#36798;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#22312;&#23454;&#26102;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#30830;&#23450;&#24615;&#23454;&#26102;&#26377;&#29702;PTMs&#26469;&#25552;&#20379;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#23454;&#38469;&#31639;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#32469;&#36807;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.12036</link><description>&lt;p&gt;
&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#29702;&#35299;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#23454;&#38469;&#31639;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#32469;&#36807;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#65306;&#31532;&#19968;&#20551;&#35774;&#21487;&#20197;&#29992;&#36880;&#28857;&#22870;&#21169;&#26367;&#20195;&#25104;&#23545;&#20559;&#22909;&#12290;&#31532;&#20108;&#20010;&#20551;&#35774;&#26159;&#22312;&#36825;&#20123;&#36880;&#28857;&#22870;&#21169;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#20174;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#27867;&#21270;&#21040;&#31574;&#30053;&#37319;&#26679;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32469;&#36807;&#20102;&#31532;&#20108;&#20010;&#36817;&#20284;&#65292;&#24182;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#31532;&#19968;&#20010;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23545;&#36825;&#20123;&#23454;&#38469;&#31639;&#27861;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#31216;&#20026;&#936;PO&#65292;&#29992;&#20110;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#35813;&#30446;&#26631;&#20197;&#25104;&#23545;&#20559;&#22909;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#22240;&#27492;&#32469;&#36807;&#20102;&#36825;&#20004;&#20010;&#36817;&#20284;&#12290;&#36825;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31181;&#26032;&#30340;&#20174;&#35757;&#32451;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#32780;&#26080;&#38656;&#36827;&#34892;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.  In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#22788;&#29702;&#20013;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#19979;&#30340;&#19968;&#33268;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.08897</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#30340;&#29305;&#33394;&#34701;&#21512;&#65306;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#34920;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#22788;&#29702;&#20013;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#19979;&#30340;&#19968;&#33268;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#29305;&#24449;&#23398;&#26159;&#19968;&#31181;&#36890;&#36807;&#22270;&#20687;&#25552;&#21462;&#23450;&#37327;&#25163;&#24037;&#29305;&#24449;&#26469;&#39044;&#27979;&#30142;&#30149;&#30340;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#20013;&#36827;&#34892;&#34701;&#21512;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#34701;&#21512;&#30340;&#26041;&#27861;&#21253;&#25324;&#26631;&#20934;&#21270;&#25104;&#20687;&#21327;&#35758;&#12289;&#32479;&#35745;&#35843;&#25972;&#21644;&#35780;&#20272;&#29305;&#24449;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#36229;&#22768;&#24515;&#21160;&#22270;&#21487;&#20197;&#35786;&#26029;&#24515;&#32908;&#30142;&#30149;&#65292;&#22914;&#24038;&#23460;&#32933;&#21402;(LVH)&#21644;&#39640;&#34880;&#21387;&#24515;&#33039;&#30149;(HHD)&#65292;&#20294;&#19981;&#21516;&#30340;&#25104;&#20687;&#35774;&#32622;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29305;&#24449;&#34701;&#21512;&#25216;&#26415;&#23545;&#20110;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#24212;&#29992;&#25163;&#24037;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;(SSl)&#36890;&#36807;&#38480;&#21046;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#25968;&#25454;&#29702;&#35299;&#65292;&#24182;&#36866;&#24212;&#22810;&#26679;&#30340;&#25968;&#25454;&#35774;&#32622;&#12290;ConvNeXt-V2&#23558;&#21367;&#31215;&#23618;&#38598;&#25104;&#21040;SSL&#20013;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;SSL&#20013;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23558;&#23427;&#20204;&#29992;&#20316;&#39044;&#22788;&#29702;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiomics, a medical imaging technique, extracts quantitative handcrafted features from images to predict diseases. Harmonization in those features ensures consistent feature extraction across various imaging devices and protocols. Methods for harmonization include standardized imaging protocols, statistical adjustments, and evaluating feature robustness. Myocardial diseases such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD) are diagnosed via echocardiography, but variable imaging settings pose challenges. Harmonization techniques are crucial for applying handcrafted features in disease diagnosis in such scenario. Self-supervised learning (SSL) enhances data understanding within limited datasets and adapts to diverse data settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying superior performance in various tasks. This study focuses on convolutional filters within SSL, using them as preprocessing to convert images into feature maps for h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#30340;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03789</link><description>&lt;p&gt;
&#22909;&#34920;&#31034;&#30340;&#28082;&#28404;&#65306;&#22312;&#20004;&#23618;&#32593;&#32476;&#20013; grokking &#20316;&#20026;&#19968;&#38454;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks. (arXiv:2310.03789v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#30340;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNN) &#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33021;&#22815;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#36259;&#26041;&#38754;&#22312;&#26368;&#36817;&#25253;&#36947;&#30340; Grokking &#29616;&#35937;&#20013;&#34920;&#29616;&#24471;&#26368;&#20026;&#26126;&#26174;&#12290;&#34429;&#28982;&#20027;&#35201;&#20307;&#29616;&#20026;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#31361;&#21464;&#22686;&#21152;&#65292;&#20294; Grokking &#20063;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36229;&#36234;&#25042;&#24816;&#23398;&#20064;/&#39640;&#26031;&#36807;&#31243; (GP) &#30340;&#29616;&#35937;&#65292;&#28041;&#21450;&#29305;&#24449;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20855;&#26377;&#31435;&#26041;&#22810;&#39033;&#24335;&#21644;&#27169;&#21152;&#27861;&#25945;&#24072;&#30340;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#20851;&#20110;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#24615;&#36136;&#30340;&#20998;&#26512;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312; Grokking &#20043;&#21518;&#65292;DNN &#30340;&#29366;&#24577;&#31867;&#20284;&#20110;&#19968;&#38454;&#30456;&#21464;&#21518;&#30340;&#28151;&#21512;&#30456;&#12290;&#22312;&#36825;&#20010;&#28151;&#21512;&#30456;&#20013;&#65292;DNN &#29983;&#25104;&#20102;&#19982;&#20043;&#21069;&#26126;&#26174;&#19981;&#21516;&#30340;&#25945;&#24072;&#30340;&#26377;&#29992;&#20869;&#37096;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key property of deep neural networks (DNNs) is their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to be a beyond lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here we apply a recent development in the theory of feature learning, the adaptive kernel approach, to two teacher-student models with cubic-polynomial and modular addition teachers. We provide analytical predictions on feature learning and Grokking properties of these models and demonstrate a mapping between Grokking and the theory of phase transitions. We show that after Grokking, the state of the DNN is analogous to the mixed phase following a first-order phase transition. In this mixed phase, the DNN generates useful internal representations of the teacher that are sharply distinct from those before the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2310.01012</link><description>&lt;p&gt;
CCA&#23478;&#26063;&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#26080;&#32422;&#26463;&#30446;&#26631;&#19982;&#26080;&#20559;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#23398;&#20064;&#20013;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#27491;&#21017;&#21270;&#32447;&#24615;CCA&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#30340;&#25512;&#24191;&#65292;&#24182;&#19982;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65288;GEP&#65289;&#26694;&#26550;&#32479;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#24615;&#26041;&#27861;&#30340;&#20256;&#32479;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;CCA&#30340;&#25193;&#23637;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#35757;&#32451;&#36807;&#31243;&#32531;&#24930;&#19988;&#22797;&#26434;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25551;&#36848;GEPs&#30340;&#39030;&#32423;&#23376;&#31354;&#38388;&#30340;&#26032;&#39062;&#26080;&#32422;&#26463;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24212;&#29992;&#20110;&#30456;&#24212;&#30340;CCA&#30446;&#26631;&#65292;&#20174;&#32780;&#33719;&#24471;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25152;&#26377;&#26631;&#20934;CCA&#21644;&#28145;&#24230;CCA&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;&#36825;&#26679;&#30340;&#36895;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39318;&#27425;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#29289;&#25968;&#25454;&#30340;PLS&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
&lt;/p&gt;</description></item><item><title>GeoCLIP&#26159;&#19968;&#31181;&#21463;Clip&#21551;&#21457;&#30340;&#22270;&#20687;&#21040;GPS&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#29699;&#22320;&#29702;&#23450;&#20301;&#12290;&#23427;&#36890;&#36807;&#23545;&#40784;&#22270;&#20687;&#21644;&#20854;&#23545;&#24212;&#30340;GPS&#20301;&#32622;&#26469;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#22266;&#23450;&#20998;&#31867;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16020</link><description>&lt;p&gt;
GeoCLIP&#65306;&#21463;Clip&#21551;&#21457;&#30340;&#22320;&#28857;&#21644;&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#20840;&#29699;&#22320;&#29702;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization. (arXiv:2309.16020v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16020
&lt;/p&gt;
&lt;p&gt;
GeoCLIP&#26159;&#19968;&#31181;&#21463;Clip&#21551;&#21457;&#30340;&#22270;&#20687;&#21040;GPS&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#29699;&#22320;&#29702;&#23450;&#20301;&#12290;&#23427;&#36890;&#36807;&#23545;&#40784;&#22270;&#20687;&#21644;&#20854;&#23545;&#24212;&#30340;GPS&#20301;&#32622;&#26469;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#22266;&#23450;&#20998;&#31867;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#22320;&#29702;&#23450;&#20301;&#26088;&#22312;&#30830;&#23450;&#36965;&#24863;&#22270;&#20687;&#30340;&#31934;&#30830;&#20301;&#32622;&#12290;&#30001;&#20110;&#22320;&#29702;&#26223;&#35266;&#30340;&#24040;&#22823;&#21464;&#21270;&#65292;&#36825;&#39033;&#20219;&#21153;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290; &#22522;&#20110;&#22270;&#20687;&#26816;&#32034;&#30340;&#26041;&#27861;&#26080;&#27861;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#26500;&#24314;&#28085;&#30422;&#25972;&#20010;&#19990;&#30028;&#30340;&#22823;&#22411;&#22270;&#20687;&#24211;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290; &#30456;&#21453;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#20840;&#29699;&#21010;&#20998;&#20026;&#31163;&#25955;&#30340;&#22320;&#29702;&#21333;&#20803;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20998;&#31867;&#20219;&#21153;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#39044;&#23450;&#20041;&#31867;&#21035;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#24403;&#22270;&#20687;&#30340;&#20301;&#32622;&#19982;&#20854;&#31867;&#21035;&#20013;&#24515;&#26174;&#33879;&#20559;&#31163;&#26102;&#65292;&#24448;&#24448;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#23450;&#20301;&#12290; &#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeoCLIP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;Clip&#21551;&#21457;&#30340;&#22270;&#20687;&#21040;GPS&#26816;&#32034;&#26041;&#27861;&#65292;&#24378;&#21046;&#36827;&#34892;&#22270;&#20687;&#19982;&#20854;&#23545;&#24212;&#30340;GPS&#20301;&#32622;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290; GeoCLIP&#30340;&#20301;&#32622;&#32534;&#30721;&#22120;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#23558;&#22320;&#29699;&#24314;&#27169;&#20026;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches divide the globe into discrete geographic cells, transforming the problem into a classification task. However, their performance is limited by the predefined classes and often results in inaccurate localizations when an image's location significantly deviates from its class center. To overcome these limitations, we propose GeoCLIP, a novel CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the image and its corresponding GPS locations. GeoCLIP's location encoder models the Earth as a continuous function by employing positional encoding through random Fourier features an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#20445;&#24207;&#24207;&#21015;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.11983</link><description>&lt;p&gt;
&#21464;&#20998;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#29992;&#20110;&#20445;&#24207;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling. (arXiv:2309.11983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#20445;&#24207;&#24207;&#21015;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#24120;&#34987;&#29992;&#20110;&#20445;&#24207;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#27604;&#22914;&#35821;&#38899;&#35782;&#21035;&#65292;&#20854;&#20013;&#20445;&#25345;&#36755;&#20837;&#21644;&#30446;&#26631;&#24207;&#21015;&#30340;&#39034;&#24207;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;CTC&#20165;&#24212;&#29992;&#20110;&#30830;&#23450;&#24615;&#24207;&#21015;&#27169;&#22411;&#65292;&#20854;&#20013;&#28508;&#22312;&#31354;&#38388;&#26159;&#19981;&#36830;&#32493;&#19988;&#31232;&#30095;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22788;&#29702;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#26041;&#38754;&#27604;&#21464;&#20998;&#27169;&#22411;&#33021;&#21147;&#26356;&#24369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;CTC&#19982;&#21464;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#23548;&#20986;&#20102;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#26356;&#20855;&#26222;&#36866;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20445;&#25345;&#39034;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#20004;&#20010;&#21512;&#29702;&#30340;&#20551;&#35774;&#23548;&#20986;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#26032;&#22411;&#21464;&#20998;CTC&#65292;&#31532;&#19968;&#20010;&#20551;&#35774;&#26159;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#21464;&#20998;&#28508;&#22312;&#21464;&#37327;&#22312;&#26465;&#20214;&#19979;&#26159;&#29420;&#31435;&#30340;&#65307;&#31532;&#20108;&#20010;&#20551;&#35774;&#26159;&#36825;&#20123;&#28508;&#22312;&#21464;&#37327;&#26159;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#37117;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#27169;&#22411;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35745;&#31639;&#19978;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist temporal classification (CTC) is commonly adopted for sequence modeling tasks like speech recognition, where it is necessary to preserve order between the input and target sequences. However, CTC is only applied to deterministic sequence models, where the latent space is discontinuous and sparse, which in turn makes them less capable of handling data variability when compared to variational models. In this paper, we integrate CTC with a variational model and derive loss functions that can be used to train more generalizable sequence models that preserve order. Specifically, we derive two versions of the novel variational CTC based on two reasonable assumptions, the first being that the variational latent variables at each time step are conditionally independent; and the second being that these latent variables are Markovian. We show that both loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and present computationally t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08549</link><description>&lt;p&gt;
&#22522;&#20110;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#30340;&#35757;&#32451;&#26469;&#25269;&#24481;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#26469;&#38450;&#27490;&#26469;&#33258;&#19981;&#21487;&#20449;&#25968;&#25454;&#28304;&#30340;&#28508;&#22312;&#27745;&#26579;&#25915;&#20987;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#36825;&#32473;&#20102;&#25915;&#20987;&#32773;&#35768;&#22810;&#21487;&#21033;&#29992;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#26377;&#25928;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#22914;&#20960;&#31181;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#37027;&#26679;&#21521;&#25152;&#26377;&#31034;&#20363;&#28155;&#21152;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#30340;&#23454;&#38469;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#26368;&#26032;&#25915;&#20987;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;H
&lt;/p&gt;
&lt;p&gt;
While numerous defense methods have been proposed to prohibit potential poisoning attacks from untrusted data sources, most research works only defend against specific attacks, which leaves many avenues for an adversary to exploit. In this work, we propose an efficient and robust training approach to defend against data poisoning attacks based on influence functions, named Healthy Influential-Noise based Training. Using influence functions, we craft healthy noise that helps to harden the classification model against poisoning attacks without significantly affecting the generalization ability on test data. In addition, our method can perform effectively when only a subset of the training data is modified, instead of the current method of adding noise to all examples that has been used in several previous works. We conduct comprehensive evaluations over two image datasets with state-of-the-art poisoning attacks under different realistic attack scenarios. Our empirical results show that H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807; emergent representation &#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#23553;&#24314;HRL&#31639;&#27861;&#26469;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.07675</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis. (arXiv:2309.07675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807; emergent representation &#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#23553;&#24314;HRL&#31639;&#27861;&#26469;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#34920;&#31034;&#32780;&#33719;&#30410;&#33391;&#22810;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30693;&#35782;&#20197;&#36827;&#34892;&#39640;&#25928;&#21644;&#21487;&#20256;&#36882;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20381;&#36182;&#31526;&#21495;&#25512;&#29702;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(HRL)&#26041;&#27861;&#36890;&#24120;&#21463;&#38480;&#20110;&#38656;&#35201;&#25163;&#21160;&#35774;&#32622;&#30446;&#26631;&#34920;&#31034;&#12290;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#30340;&#25361;&#25112;&#22312;&#20110;&#23427;&#24517;&#39035;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20363;&#22914;&#29615;&#22659;&#21160;&#21147;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#20986;&#29616;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#30446;&#26631;&#21457;&#29616;&#30340;&#21457;&#23637;&#26426;&#21046;&#65292;&#35813;&#34920;&#31034;&#23558;&#20855;&#26377;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#36827;&#34892;&#25277;&#35937;&#65288;&#21363;&#20998;&#32452;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#30340;&#23553;&#24314;HRL&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#31526;&#21495;&#21487;&#36798;&#24615;&#20998;&#26512;&#26469;&#36817;&#20284;&#29366;&#24577;&#38598;&#21512;&#20043;&#38388;&#30340;&#36807;&#28193;&#20851;&#31995;&#65292;&#24182;&#25913;&#36827;&#30446;&#26631;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We eval
&lt;/p&gt;</description></item><item><title>BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09936</link><description>&lt;p&gt;
BLIVA: &#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09936
&lt;/p&gt;
&lt;p&gt;
BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#35299;&#20915;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#35299;&#37322;&#23884;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#27969;&#31243;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19968;&#32452;&#22266;&#23450;&#30340;&#26597;&#35810;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23553;&#35013;&#22270;&#20687;&#19978;&#19979;&#25991;&#65292;&#24182;&#38543;&#21518;&#29992;&#20316;LLM&#20013;&#30340;&#36719;&#25552;&#31034;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21463;&#20196;&#29260;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#23545;&#25991;&#26412;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#22330;&#26223;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;BLIVA&#65306;InstructBLIP with Visual Assistant&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;BLIVA&#38598;&#25104;&#20102;&#26469;&#33258;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;&#24182;&#23558;&#32534;&#30721;&#30340;&#34917;&#19969;&#23884;&#20837;&#30452;&#25509;&#25237;&#24433;&#21040;LLM&#20013;&#65292;&#36825;&#26159;&#21463;&#21040;LLaVA&#30340;&#21551;&#21457;&#30340;&#19968;&#31181;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#25628;&#32034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13390</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#28508;&#31354;&#38388;&#30340;&#25628;&#32034;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space. (arXiv:2307.13390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#25628;&#32034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#26159;&#29992;&#20110;&#35299;&#20915;&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24037;&#20855;&#65306;1. &#26159;&#20160;&#20040;&#20851;&#38190;&#22240;&#32032;&#23548;&#33268;&#20102;&#33258;&#21160;&#39044;&#27979;/&#20915;&#31574;&#65311;2. &#22914;&#20309;&#25913;&#21464;&#36825;&#20123;&#22240;&#32032;&#20197;&#20174;&#29992;&#25143;&#35282;&#24230;&#33719;&#24471;&#26356;&#26377;&#21033;&#30340;&#32467;&#26524;&#65311;&#22240;&#27492;&#65292;&#36890;&#36807;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#21644;&#26131;&#20110;&#23454;&#29616;&#30340;&#21487;&#34892;&#21464;&#21270;&#26469;&#24341;&#23548;&#29992;&#25143;&#19982;AI&#31995;&#32479;&#30340;&#20132;&#20114;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#37319;&#29992;&#21644;&#38271;&#26399;&#25509;&#21463;AI&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;CEs&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#19981;&#21516;&#30340;&#36136;&#37327;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CEs&#30340;&#29983;&#25104;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#24314;&#35758;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#27492;&#19981;&#21487;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23558;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#24418;&#25104;&#20026;&#39640;&#26031;&#20998;&#24067;&#30340;&#28151;&#21512;&#65292;&#20026;&#39044;&#20808;&#35757;&#32451;&#30340;&#20108;&#20998;&#31867;&#22120;&#29983;&#25104;CEs&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11494</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#25913;&#36827;&#12289;&#21512;&#25104;&#65306;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39044;&#27979;&#25110;&#22635;&#34917;&#20219;&#21153;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38754;&#21521;&#22810;&#31181;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TSDiff&#65292;&#19968;&#31181;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#26465;&#20214;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#24341;&#23548;&#26426;&#21046;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#24471;TSDiff&#33021;&#22815;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#32593;&#32476;&#25110;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;TSDiff&#19982;&#20960;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26465;&#20214;&#39044;&#27979;&#26041;&#27861;&#30456;&#31454;&#20105;&#65288;&#39044;&#27979;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;TSDiff&#23398;&#21040;&#30340;&#38544;&#24615;&#27010;&#29575;&#23494;&#24230;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;p
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
&lt;/p&gt;</description></item><item><title>Epsilon*&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#39118;&#38505;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#33021;&#19982;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#38544;&#31169;&#25439;&#22833;&#30340;&#19979;&#30028;&#65292;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11280</link><description>&lt;p&gt;
Epsilon*: &#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11280
&lt;/p&gt;
&lt;p&gt;
Epsilon*&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#39118;&#38505;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#33021;&#19982;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#38544;&#31169;&#25439;&#22833;&#30340;&#19979;&#30028;&#65292;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Epsilon*&#65292;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38544;&#31169;&#20943;&#36731;&#31574;&#30053;&#37096;&#32626;&#20043;&#21069;&#12289;&#26399;&#38388;&#25110;&#20043;&#21518;&#65292;&#27979;&#37327;&#21333;&#20010;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#35813;&#24230;&#37327;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#37319;&#26679;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;Epsilon*&#26159;&#19968;&#20010;&#20851;&#20110;&#30495;&#38451;&#24615;&#21644;&#20551;&#38451;&#24615;&#29575;&#30340;&#20989;&#25968;&#65292;&#29992;&#20110;&#25932;&#25163;&#22312;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#20351;&#29992;&#30340;&#20551;&#35774;&#26816;&#39564;&#20013;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#37327;&#21270;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#25439;&#22833;&#21644;&#37327;&#21270;&#20135;&#29983;&#35813;&#27169;&#22411;&#23454;&#20363;&#30340;&#35757;&#32451;&#26426;&#21046;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#23457;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#20026;&#21518;&#32773;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#32780;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#20110;&#35757;&#32451;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#30340;&#65288;&#949;&#65292;&#948;&#65289;&#22411;&#37327;&#21270;&#65292;&#20026;&#21069;&#32773;&#25552;&#20379;&#20102;&#19979;&#30028;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#19979;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;Epsilon*&#20197;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#20998;&#26512;&#20102;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#24341;&#20837;&#20102;&#32452;&#21512;&#19968;&#33268;&#31283;&#23450;&#24615;&#27010;&#24565;&#24182;&#19982;SCO&#38382;&#39064;&#30340;&#27867;&#21270;&#24615;&#24314;&#31435;&#20102;&#23450;&#37327;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.03357</link><description>&lt;p&gt;
&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms. (arXiv:2307.03357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#20998;&#26512;&#20102;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#24341;&#20837;&#20102;&#32452;&#21512;&#19968;&#33268;&#31283;&#23450;&#24615;&#27010;&#24565;&#24182;&#19982;SCO&#38382;&#39064;&#30340;&#27867;&#21270;&#24615;&#24314;&#31435;&#20102;&#23450;&#37327;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#65288;SCO&#65289;&#38382;&#39064;&#65292;&#20363;&#22914;&#24378;&#21270;&#23398;&#20064;&#12289;AUC&#26368;&#22823;&#21270;&#21644;&#20803;&#23398;&#20064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#28041;&#21450;&#19982;&#26399;&#26395;&#30456;&#20851;&#30340;&#23884;&#22871;&#32452;&#21512;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;SCO&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#65292;&#21363;&#20174;&#35757;&#32451;&#31034;&#20363;&#26500;&#24314;&#30340;&#23398;&#20064;&#31639;&#27861;&#22312;&#26410;&#26469;&#30340;&#27979;&#35797;&#31034;&#20363;&#19978;&#30340;&#34892;&#20026;&#22914;&#20309;&#65292;&#21364;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#19979;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#25552;&#20379;&#20102;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#27010;&#24565;&#65292;&#31216;&#20026;&#32452;&#21512;&#19968;&#33268;&#31283;&#23450;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#19982;SCO&#38382;&#39064;&#30340;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#27969;&#34892;&#30340;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#24314;&#31435;&#20102;&#32452;&#21512;&#19968;&#33268;&#31283;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning tasks can be formulated as a stochastic compositional optimization (SCO) problem such as reinforcement learning, AUC maximization, and meta-learning, where the objective function involves a nested composition associated with an expectation. While a significant amount of studies has been devoted to studying the convergence behavior of SCO algorithms, there is little work on understanding their generalization, i.e., how these learning algorithms built from training examples would behave on future test examples. In this paper, we provide the stability and generalization analysis of stochastic compositional gradient descent algorithms through the lens of algorithmic stability in the framework of statistical learning theory. Firstly, we introduce a stability concept called compositional uniform stability and establish its quantitative relation with generalization for SCO problems. Then, we establish the compositional uniform stability results for two popular stochastic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#25506;&#35752;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#30340;&#35889;&#20998;&#26512;&#26469;&#29702;&#35299;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2306.17301</link><description>&lt;p&gt;
&#27973;&#23618;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65306;&#19968;&#20010;&#25968;&#20540;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study. (arXiv:2306.17301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#25506;&#35752;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#30340;&#35889;&#20998;&#26512;&#26469;&#29702;&#35299;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20998;&#26512;&#21644;&#23454;&#39564;&#30340;&#32508;&#21512;&#25968;&#20540;&#30740;&#31350;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#23454;&#38469;&#22240;&#32032;&#20013;&#65292;&#22788;&#29702;&#39640;&#39057;&#29575;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#20102;&#20197;&#19979;&#22522;&#26412;&#35745;&#31639;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22312;&#26377;&#38480;&#30340;&#26426;&#22120;&#31934;&#24230;&#19979;&#21487;&#20197;&#36798;&#21040;&#30340;&#26368;&#20339;&#31934;&#24230;&#65292;&#65288;2&#65289;&#23454;&#29616;&#32473;&#23450;&#31934;&#24230;&#25152;&#38656;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#65288;3&#65289;&#23545;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#30340;&#20851;&#38190;&#26159;&#30456;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#30340;&#35889;&#20998;&#26512;&#65292;&#35813;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#23646;&#24615;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a comprehensive numerical study involving analysis and experiments shows why a two-layer neural network has difficulties handling high frequencies in approximation and learning when machine precision and computation cost are important factors in real practice. In particular, the following fundamental computational issues are investigated: (1) the best accuracy one can achieve given a finite machine precision, (2) the computation cost to achieve a given accuracy, and (3) stability with respect to perturbations. The key to the study is the spectral analysis of the corresponding Gram matrix of the activation functions which also shows how the properties of the activation function play a role in the picture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;DNN&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#37327;&#21270;&#24352;&#37327;&#26041;&#27861;DNA-TEQ&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#22823;&#37327;&#24352;&#37327;&#31526;&#21512;&#25351;&#25968;&#20998;&#24067;&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#25968;&#20540;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.16430</link><description>&lt;p&gt;
DNA-TEQ&#65306;&#19968;&#31181;&#29992;&#20110;DNN&#25512;&#29702;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#37327;&#21270;&#24352;&#37327;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference. (arXiv:2306.16430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;DNN&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25351;&#25968;&#37327;&#21270;&#24352;&#37327;&#26041;&#27861;DNA-TEQ&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21457;&#29616;&#22823;&#37327;&#24352;&#37327;&#31526;&#21512;&#25351;&#25968;&#20998;&#24067;&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#25968;&#20540;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#38477;&#20302;&#28608;&#27963;&#21644;&#26435;&#37325;&#65288;&#21363;&#24352;&#37327;&#65289;&#30340;&#31639;&#26415;&#31934;&#24230;&#26469;&#20943;&#23569;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#39640;&#25928;&#30340;&#30828;&#20214;&#26550;&#26500;&#37319;&#29992;&#32447;&#24615;&#37327;&#21270;&#65292;&#20197;&#20415;&#23558;&#26368;&#26032;&#30340;DNN&#37096;&#32626;&#21040;&#23884;&#20837;&#24335;&#31995;&#32479;&#21644;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#22343;&#21248;&#37327;&#21270;&#36890;&#24120;&#26080;&#27861;&#23558;&#25968;&#20540;&#31934;&#24230;&#38477;&#20302;&#21040;&#23567;&#20110;8&#20301;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#24352;&#37327;&#24182;&#19981;&#26381;&#20174;&#22343;&#21248;&#20998;&#24067;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#24352;&#37327;&#31526;&#21512;&#25351;&#25968;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DNA-TEQ&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26041;&#26696;&#25351;&#25968;&#37327;&#21270;DNN&#24352;&#37327;&#65292;&#20197;&#23454;&#29616;&#25968;&#20540;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#20043;&#38388;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#26696;&#30456;&#27604;&#65292;DNA-TEQ&#25552;&#20379;&#20102;&#26356;&#20302;&#30340;&#37327;&#21270;&#20301;&#23485;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is commonly used in Deep Neural Networks (DNNs) to reduce the storage and computational complexity by decreasing the arithmetical precision of activations and weights, a.k.a. tensors. Efficient hardware architectures employ linear quantization to enable the deployment of recent DNNs onto embedded systems and mobile devices. However, linear uniform quantization cannot usually reduce the numerical precision to less than 8 bits without sacrificing high performance in terms of model accuracy. The performance loss is due to the fact that tensors do not follow uniform distributions. In this paper, we show that a significant amount of tensors fit into an exponential distribution. Then, we propose DNA-TEQ to exponentially quantize DNN tensors with an adaptive scheme that achieves the best trade-off between numerical precision and accuracy loss. The experimental results show that DNA-TEQ provides a much lower quantization bit-width compared to previous proposals, resulting in an av
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#20132;&#24207;&#21015;&#30340;FLORAS&#26041;&#27861;&#65292;&#21487;&#28040;&#38500;&#21457;&#36865;&#31471;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#39033;&#30446;&#32423;&#21644;&#23458;&#25143;&#32423;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;FLORAS&#21487;&#20197;&#28789;&#27963;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#24046;&#20998;&#38544;&#31169;&#31561;&#32423;&#65292;&#24182;&#19988;&#36890;&#36807;&#25512;&#23548;&#25910;&#25947;&#30028;&#38480;&#65292;&#23454;&#29616;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#20043;&#38388;&#30340;&#24179;&#31283;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.08280</link><description>&lt;p&gt;
&#20351;&#29992;&#27491;&#20132;&#24207;&#21015;&#30340;&#24046;&#20998;&#38544;&#31169;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Wireless Federated Learning Using Orthogonal Sequences. (arXiv:2306.08280v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#20132;&#24207;&#21015;&#30340;FLORAS&#26041;&#27861;&#65292;&#21487;&#28040;&#38500;&#21457;&#36865;&#31471;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#39033;&#30446;&#32423;&#21644;&#23458;&#25143;&#32423;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;FLORAS&#21487;&#20197;&#28789;&#27963;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#24046;&#20998;&#38544;&#31169;&#31561;&#32423;&#65292;&#24182;&#19988;&#36890;&#36807;&#25512;&#23548;&#25910;&#25947;&#30028;&#38480;&#65292;&#23454;&#29616;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#20043;&#38388;&#30340;&#24179;&#31283;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#19978;&#34892;&#31354;&#20013;&#35745;&#31639;&#26041;&#27861;FLORAS&#65292;&#29992;&#20110;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#65288;SISO&#65289;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#12290;FLORAS&#20174;&#36890;&#20449;&#35774;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21033;&#29992;&#27491;&#20132;&#24207;&#21015;&#30340;&#24615;&#36136;&#28040;&#38500;&#20102;&#21457;&#36865;&#31471;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSIT&#65289;&#35201;&#27714;&#12290;&#20174;&#38544;&#31169;&#20445;&#25252;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;FLORAS&#21487;&#20197;&#25552;&#20379;&#39033;&#30446;&#32423;&#21644;&#23458;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35843;&#25972;&#31995;&#32479;&#21442;&#25968;&#65292;FLORAS&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;DP&#31561;&#32423;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;FL&#25910;&#25947;&#30028;&#38480;&#65292;&#32467;&#21512;&#38544;&#31169;&#20445;&#35777;&#65292;&#21487;&#20197;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#24046;&#20998;&#38544;&#31169;&#32423;&#21035;&#20043;&#38388;&#23454;&#29616;&#24179;&#31283;&#30340;&#26435;&#34913;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;FLORAS&#30456;&#23545;&#20110;&#22522;&#20934;AirComp&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#21487;&#20197;&#25351;&#23548;&#19981;&#21516;&#26435;&#34913;&#26465;&#20214;&#19979;&#30340;&#38544;&#31169;&#20445;&#25252;FL&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel privacy-preserving uplink over-the-air computation (AirComp) method, termed FLORAS, for single-input single-output (SISO) wireless federated learning (FL) systems. From the communication design perspective, FLORAS eliminates the requirement of channel state information at the transmitters (CSIT) by leveraging the properties of orthogonal sequences. From the privacy perspective, we prove that FLORAS can offer both item-level and client-level differential privacy (DP) guarantees. Moreover, by adjusting the system parameters, FLORAS can flexibly achieve different DP levels at no additional cost. A novel FL convergence bound is derived which, combined with the privacy guarantees, allows for a smooth tradeoff between convergence rate and differential privacy levels. Numerical results demonstrate the advantages of FLORAS compared with the baseline AirComp method, and validate that our analytical results can guide the design of privacy-preserving FL with different tradeoff 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.06202</link><description>&lt;p&gt;
NeuroGraph:&#38754;&#21521;&#33041;&#36830;&#25509;&#32452;&#23398;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#20998;&#26512;&#39640;&#32500;&#21151;&#33021;&#24615;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#39044;&#27979;&#21508;&#31181;&#31070;&#32463;&#30142;&#30149;&#12289;&#31934;&#31070;&#38556;&#30861;&#21644;&#35748;&#30693;&#27169;&#24335;&#26377;&#25928;&#12290;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#30740;&#31350;&#20013;&#65292;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#35777;&#23454;&#65292;&#26631;&#24535;&#30528;&#25968;&#25454;&#35299;&#37322;&#21644;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#36716;&#21464;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#22270;&#24418;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#24191;&#27867;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#22823;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#20013;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#36716;&#25442;&#20173;&#28982;&#21463;&#21040;&#24847;&#22806;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroGraph(&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#38598;)&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#34892;&#20026;&#21644;&#35748;&#30693;&#29305;&#24449;&#31867;&#21035;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476; ShaDDR&#65292;&#21487;&#20197;&#36890;&#36807;&#20960;&#20309;&#32454;&#33410;&#21270;&#21644;&#26465;&#20214;&#32441;&#29702;&#29983;&#25104;&#24212;&#29992;&#20110;&#36755;&#20837;&#30340;&#31895;&#30053;&#20307;&#32032;&#24418;&#29366;&#65292;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#36148;&#22270;&#30340; 3D &#24418;&#29366;&#12290;&#29983;&#25104;&#23454;&#26102;&#19988;&#31934;&#24230;&#39640;&#65292;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#28508;&#22312;&#20195;&#30721;&#36827;&#34892;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04889</link><description>&lt;p&gt;
ShaDDR: &#22522;&#20110;&#31034;&#20363;&#30340;&#23454;&#26102;&#20960;&#20309;&#21644;&#32441;&#29702;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering. (arXiv:2306.04889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476; ShaDDR&#65292;&#21487;&#20197;&#36890;&#36807;&#20960;&#20309;&#32454;&#33410;&#21270;&#21644;&#26465;&#20214;&#32441;&#29702;&#29983;&#25104;&#24212;&#29992;&#20110;&#36755;&#20837;&#30340;&#31895;&#30053;&#20307;&#32032;&#24418;&#29366;&#65292;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#36148;&#22270;&#30340; 3D &#24418;&#29366;&#12290;&#29983;&#25104;&#23454;&#26102;&#19988;&#31934;&#24230;&#39640;&#65292;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#28508;&#22312;&#20195;&#30721;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; ShaDDR&#65292;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20960;&#20309;&#32454;&#33410;&#21270;&#21644;&#26465;&#20214;&#32441;&#29702;&#29983;&#25104;&#24212;&#29992;&#20110;&#36755;&#20837;&#30340;&#31895;&#30053;&#20307;&#32032;&#24418;&#29366;&#65292;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#36148;&#22270;&#30340; 3D &#24418;&#29366;&#12290;&#22312;&#23569;&#37327;&#35814;&#32454;&#21644;&#32441;&#29702;&#30340;&#33539;&#20363;&#24418;&#29366;&#19978;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#20307;&#32032;&#19978;&#37319;&#26679;&#23398;&#20064;&#20960;&#20309;&#32454;&#33410;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#20960;&#20010;&#35270;&#22270;&#30340;&#33539;&#20363;&#32441;&#29702;&#22270;&#20687;&#36827;&#34892;&#21487;&#24494;&#28210;&#26579;&#65292;&#22312;&#20307;&#32032;&#34920;&#38754;&#29983;&#25104;&#32441;&#29702;&#12290;&#29983;&#25104;&#26159;&#23454;&#26102;&#30340;&#65292;&#20165;&#38656;&#19981;&#21040; 1 &#31186;&#21363;&#21487;&#29983;&#25104;&#20998;&#36776;&#29575;&#39640;&#36798; 512^3 &#30340; 3D &#27169;&#22411;&#12290;&#29983;&#25104;&#30340;&#24418;&#29366;&#20445;&#30041;&#20102;&#36755;&#20837;&#31895;&#30053;&#20307;&#32032;&#27169;&#22411;&#30340;&#25972;&#20307;&#32467;&#26500;&#65292;&#32780;&#29983;&#25104;&#30340;&#20960;&#20309;&#32454;&#33410;&#21644;&#32441;&#29702;&#30340;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#28508;&#22312;&#20195;&#30721;&#36827;&#34892;&#25805;&#32437;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#27604;&#20197;&#21069;&#30340;&#20316;&#21697;&#26356;&#30495;&#23454;&#19988;&#20960;&#20309;&#32454;&#33410;&#21644;&#32441;&#29702;&#26356;&#24178;&#20928;&#30340;&#39640;&#20998;&#36776;&#29575;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 512^3. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#36873;&#25321;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#26041;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#29992;&#20110;2D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.04225</link><description>&lt;p&gt;
&#22522;&#20110;&#34917;&#19969;&#36873;&#25321;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#22312;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Vision Transformer for Human Pose Estimation via Patch Selection. (arXiv:2306.04225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#36873;&#25321;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#26041;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#29992;&#20110;2D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;2D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#26159;&#35270;&#35273;Transformer&#20316;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#21147;&#26367;&#20195;&#32773;&#65292;&#36890;&#36807;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#32780;&#23853;&#38706;&#22836;&#35282;&#12290; &#28982;&#32780;&#65292;&#35270;&#35273;Transformer&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#20854;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#38271;&#35270;&#39057;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;Transformer&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#22522;&#20110;&#36873;&#25321;&#21644;&#22788;&#29702;&#23569;&#37327;&#26368;&#20855;&#20449;&#24687;&#30340;&#34917;&#19969;&#65292;&#32780;&#24573;&#30053;&#20854;&#20182;&#22320;&#26041;&#30340;&#34917;&#19969;&#12290;&#25105;&#20204;&#21033;&#29992;&#36731;&#37327;&#32423;&#23039;&#24577;&#20272;&#35745;&#32593;&#32476;&#26469;&#25351;&#23548;&#34917;&#19969;&#36873;&#25321;&#36807;&#31243;&#65292;&#30830;&#20445;&#25152;&#36873;&#34917;&#19969;&#21253;&#21547;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;2D&#23039;&#24577;&#20272;&#35745;&#22522;&#20934;&#65288;&#21363;COCO&#12289;MPII&#21644;OCHuman&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#34429;&#28982;&#24615;&#33021;&#30053;&#26377;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Convolutional Neural Networks (CNNs) have been widely successful in 2D human pose estimation, Vision Transformers (ViTs) have emerged as a promising alternative to CNNs, boosting state-of-the-art performance. However, the quadratic computational complexity of ViTs has limited their applicability for processing high-resolution images and long videos. To address this challenge, we propose a simple method for reducing ViT's computational complexity based on selecting and processing a small number of most informative patches while disregarding others. We leverage a lightweight pose estimation network to guide the patch selection process, ensuring that the selected patches contain the most important information. Our experimental results on three widely used 2D pose estimation benchmarks, namely COCO, MPII and OCHuman, demonstrate the effectiveness of our proposed methods in significantly improving speed and reducing computational complexity with a slight drop in performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;hinge-Wasserstein&#65292;&#29992;&#20110;&#32531;&#35299;&#22238;&#24402;&#20219;&#21153;&#20013;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#26377;&#25928;&#25552;&#39640;&#20102;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.00560</link><description>&lt;p&gt;
Hinge-Wasserstein: &#36890;&#36807;&#20998;&#31867;&#36991;&#20813;&#22238;&#24402;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;
&lt;/p&gt;
&lt;p&gt;
Hinge-Wasserstein: Mitigating Overconfidence in Regression by Classification. (arXiv:2306.00560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;hinge-Wasserstein&#65292;&#29992;&#20110;&#32531;&#35299;&#22238;&#24402;&#20219;&#21153;&#20013;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#26377;&#25928;&#25552;&#39640;&#20102;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24615;&#33021;&#26041;&#38754;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#12290;&#22312;&#27169;&#31946;&#29978;&#33267;&#19981;&#21487;&#39044;&#27979;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#36807;&#24230;&#33258;&#20449;&#21487;&#33021;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#65292;&#37319;&#29992;&#22238;&#24402;-&#20998;&#31867;&#26041;&#27861;&#26377;&#28508;&#21147;&#32531;&#35299;&#36825;&#20123;&#27495;&#20041;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#25152;&#38656;&#36755;&#20986;&#30340;&#31163;&#25955;&#27010;&#29575;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#23494;&#24230;&#20272;&#35745;&#20173;&#28982;&#20542;&#21521;&#20110;&#36807;&#24230;&#33258;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#24120;&#35265;&#30340;NLL&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#26102;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;hinge-Wasserstein&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#27492;&#25439;&#22833;&#26174;&#30528;&#25552;&#39640;&#20102;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#65306; aleatoric&#19981;&#30830;&#23450;&#24615;&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26032;&#25439;&#22833;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#20998;&#21035;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#28436;&#31034;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks are prone to being overconfident despite their drastically improved performance. In ambiguous or even unpredictable real-world scenarios, this overconfidence can pose a major risk to the safety of applications. For regression tasks, the regression-by-classification approach has the potential to alleviate these ambiguities by instead predicting a discrete probability density over the desired output. However, a density estimator still tends to be overconfident when trained with the common NLL loss. To mitigate the overconfidence problem, we propose a loss function, hinge-Wasserstein, based on the Wasserstein Distance. This loss significantly improves the quality of both aleatoric and epistemic uncertainty, compared to previous work. We demonstrate the capabilities of the new loss on a synthetic dataset, where both types of uncertainty are controlled separately. Moreover, as a demonstration for real-world scenarios, we evaluate our approach on the benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.15851</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
On sampling determinantal and Pfaffian point processes on a quantum computer. (arXiv:2305.15851v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;(DPP) &#26368;&#26089;&#34987; Macchi &#20316;&#20026;&#37327;&#23376;&#20809;&#23398;&#27169;&#22411;&#24341;&#20837;&#65292;&#33258;&#37027;&#20197;&#21518;&#65292;&#23427;&#20204;&#24050;&#24191;&#27867;&#29992;&#20316;&#32479;&#35745;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#27169;&#22411;&#21644;&#23376;&#25277;&#26679;&#24037;&#20855;&#12290;&#22823;&#22810;&#25968;&#24212;&#29992;&#38656;&#35201;&#20174;DPP&#25277;&#26679;&#65292;&#32771;&#34385;&#21040;&#20854;&#37327;&#23376;&#36215;&#28304;&#65292;&#33258;&#28982;&#20250;&#24819;&#30693;&#36947;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25277;&#26679;DPP&#26159;&#21542;&#27604;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#26356;&#23481;&#26131;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#26377;&#38480;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;DPP&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;$\{1,\dots,N\}$&#23376;&#38598;&#19978;&#30340;&#20998;&#24067;&#65292;&#30001;&#19968;&#20010;$N\times N$&#30340;Hermite&#20869;&#26680;&#30697;&#38453;&#21442;&#25968;&#21270;&#12290;&#26368;&#22522;&#26412;&#30340;&#37319;&#26679;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#20998;&#21035;&#38656;&#35201; $\mathcal{O}(N^3)$ &#21644; $\mathcal{O}(Nr^2)$ &#30340;&#25805;&#20316;&#25104;&#26412;&#65292;&#20854;&#20013;$r$&#26159;&#20869;&#26680;&#30697;&#38453;&#30340;&#31209;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;DPP&#37319;&#26679;&#31639;&#27861;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since then, they have been widely used as models and subsampling tools in statistics and computer science. Most applications require sampling from a DPP, and given their quantum origin, it is natural to wonder whether sampling a DPP on a quantum computer is easier than on a classical one. We focus here on DPPs over a finite state space, which are distributions over the subsets of $\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanilla sampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and $\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rank of the kernel matrix. A large first part of the current paper consists in explaining why the state-of-the-art in quantum simulation of fermionic systems already yields quantum DPP sampling algorithms. We then modify existing quantum circuits, and discuss their insertion in a full DPP sampling pipeline that start
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14032</link><description>&lt;p&gt;
&#24102;&#26377;&#38899;&#39057;&#20809;&#35889;&#21464;&#25442;&#22120;&#30340; Patch-Mix &#23545;&#27604;&#23398;&#20064;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification. (arXiv:2305.14032v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#22768;&#21253;&#21547;&#26089;&#26399;&#35786;&#26029;&#33268;&#21629;&#32954;&#37096;&#30142;&#30149;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#33258; COVID-19 &#30123;&#24773;&#20197;&#26469;&#65292;&#22522;&#20110;&#30005;&#23376;&#21548;&#35786;&#22120;&#30340;&#26080;&#25509;&#35302;&#21307;&#30103;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#31232;&#32570;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340; Patch-Mix &#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#28151;&#21512;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#34917;&#19969;&#65292;&#19982; Audio Spectrogram Transformer (AST) &#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340; Patch-Mix &#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28151;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; ICBHI &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#39640;&#24471;&#20998; 4.08%&#12290;
&lt;/p&gt;
&lt;p&gt;
Respiratory sound contains crucial information for the early diagnosis of fatal lung diseases. Since the COVID-19 pandemic, there has been a growing interest in contact-free medical care based on electronic stethoscopes. To this end, cutting-edge deep learning models have been developed to diagnose lung diseases; however, it is still challenging due to the scarcity of medical data. In this study, we demonstrate that the pretrained model on large-scale visual and audio datasets can be generalized to the respiratory sound classification task. In addition, we introduce a straightforward Patch-Mix augmentation, which randomly mixes patches between different samples, with Audio Spectrogram Transformer (AST). We further propose a novel and effective Patch-Mix Contrastive Learning to distinguish the mixed representations in the latent space. Our method achieves state-of-the-art performance on the ICBHI dataset, outperforming the prior leading score by an improvement of 4.08%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#33258;&#23450;&#20041;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#20302;&#32500;&#32467;&#26500;&#21487;&#20197;&#22312;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13536</link><description>&lt;p&gt;
&#29992;&#20302;&#32500;&#21442;&#25968;&#23376;&#31354;&#38388;&#34920;&#31034;&#36755;&#20837;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Representing Input Transformations by Low-Dimensional Parameter Subspaces. (arXiv:2305.13536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#33258;&#23450;&#20041;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#20302;&#32500;&#32467;&#26500;&#21487;&#20197;&#22312;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#23545;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#21464;&#25442;&#65288;&#22914;&#26059;&#36716;&#12289;&#32553;&#25918;&#21644;&#24179;&#31227;&#65289;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#38500;&#38750;&#23427;&#20204;&#20855;&#26377;&#29305;&#23450;&#30340;&#19981;&#21464;&#32467;&#26500;&#25110;&#32463;&#36807;&#29305;&#23450;&#30340;&#35757;&#32451;&#21518;&#65288;&#20363;&#22914;&#20174;&#25968;&#25454;&#22686;&#24378;&#20013;&#23398;&#20064;&#25152;&#38656;&#30340;&#40065;&#26834;&#24615;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#21363;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#23376;&#31354;&#38388;&#21487;&#37197;&#32622;&#32593;&#32476;&#26469;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#23427;&#20204;&#22312;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#30340;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#12289;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#20013;&#30340;&#32467;&#26500;&#21644;&#24778;&#20154;&#30340;&#20302;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#36328;&#19981;&#21516;&#39046;&#22495;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#36716;&#31227;&#36755;&#20837;&#21464;&#25442;&#30693;&#35782;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#26356;&#20581;&#22766;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models lack robustness to simple input transformations such as rotation, scaling, and translation, unless they feature a particular invariant architecture or undergo specific training, e.g., learning the desired robustness from data augmentations. Alternatively, input transformations can be treated as a domain shift problem, and solved by post-deployment model adaptation. Although a large number of methods deal with transformed inputs, the fundamental relation between input transformations and optimal model weights is unknown. In this paper, we put forward the configuration subspace hypothesis that model weights optimal for parameterized continuous transformations can reside in low-dimensional linear subspaces. We introduce subspace-configurable networks to learn these subspaces and observe their structure and surprisingly low dimensionality on all tested transformations, datasets and architectures from computer vision and audio signal processing domains. Our findings enable effic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#38024;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#20102;&#26377;&#26465;&#20214;&#27169;&#25311;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13318</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#26041;&#27861;&#36827;&#34892;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A principled deep learning approach for geological facies generation. (arXiv:2305.13318v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#38024;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#20102;&#26377;&#26465;&#20214;&#27169;&#25311;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22320;&#29699;&#31185;&#23398;&#24212;&#29992;&#20013;&#65292;&#27169;&#25311;&#19981;&#21487;&#35266;&#27979;&#20307;&#31215;&#20013;&#30340;&#22320;&#36136;&#30456;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#32771;&#34385;&#21040;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#28145;&#24230;&#29983;&#25104;&#23398;&#20064;&#26159;&#20811;&#26381;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#23616;&#38480;&#24615;&#65288;&#29305;&#21035;&#26159;&#32570;&#20047;&#29289;&#29702;&#36924;&#30495;&#24615;&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#23545;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#36827;&#34892;&#24212;&#29992;&#65292;&#20197;&#20415;&#26377;&#26465;&#20214;&#22320;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#25239;&#24615;&#26041;&#27861;&#21644;&#26088;&#22312;&#20419;&#36827;&#20854;&#35757;&#32451;&#30340;&#31283;&#23450;&#21270;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20197;&#38543;&#26426;&#36807;&#31243;&#20026;&#22522;&#30784;&#30340;Flumy&#27169;&#22411;&#29983;&#25104;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#27169;&#25311;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#21033;&#29992;&#24418;&#24577;&#23398;&#25351;&#26631;&#27604;&#36739;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#36845;&#20195;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#31283;&#23450;&#25216;&#26415;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simulation of geological facies in an unobservable volume is essential in various geoscience applications. Given the complexity of the problem, deep generative learning is a promising approach to overcome the limitations of traditional geostatistical simulation models, in particular their lack of physical realism. This research aims to investigate the application of generative adversarial networks and deep variational inference for conditionally simulating meandering channels in underground volumes. In this paper, we review the generative deep learning approaches, in particular the adversarial ones and the stabilization techniques that aim to facilitate their training. The proposed approach is tested on 2D and 3D simulations generated by the stochastic process-based model Flumy. Morphological metrics are utilized to compare our proposed method with earlier iterations of generative adversarial networks. The results indicate that by utilizing recent stabilization techniques, generati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#21644;&#38548;&#31163;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#29616;Lueneberger&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#27531;&#30041;&#29983;&#25104;&#26816;&#27979;&#20256;&#24863;&#22120;&#25925;&#38556;&#24182;&#23454;&#29616;&#25925;&#38556;&#38548;&#31163;&#12290;</title><link>http://arxiv.org/abs/2304.08837</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#35266;&#27979;&#22120;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#19982;&#38548;&#31163;
&lt;/p&gt;
&lt;p&gt;
Sensor Fault Detection and Isolation in Autonomous Nonlinear Systems Using Neural Network-Based Observers. (arXiv:2304.08837v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#21644;&#38548;&#31163;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#29616;Lueneberger&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#27531;&#30041;&#29983;&#25104;&#26816;&#27979;&#20256;&#24863;&#22120;&#25925;&#38556;&#24182;&#23454;&#29616;&#25925;&#38556;&#38548;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#38548;&#31163;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#12290;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#65306;&#23436;&#20840;&#25925;&#38556;&#21644;&#20256;&#24863;&#22120;&#21155;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#32780;&#19981;&#38656;&#23545;&#20854;&#19977;&#35282;&#24418;&#21644;/&#25110;&#27491;&#24120;&#24418;&#24335;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#65292;&#36825;&#36890;&#24120;&#22312;&#35266;&#23519;&#32773;&#35774;&#35745;&#25991;&#29486;&#20013;&#32771;&#34385;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;Lueneberger&#35266;&#23519;&#22120;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35774;&#35745;&#65292;&#20854;&#20013;&#28041;&#21450;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#36716;&#21270;&#20026;&#20855;&#26377;&#36755;&#20986;&#27880;&#20837;&#30340;&#31283;&#23450;&#32447;&#24615;&#31995;&#32479;&#30340;&#21333;&#23556;&#26144;&#23556;&#12290;&#36825;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;Lueneberger&#35266;&#23519;&#22120;&#20934;&#30830;&#20272;&#35745;&#20102;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#36890;&#36807;&#27531;&#30041;&#29983;&#25104;&#23454;&#29616;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#26816;&#27979;&#12290;&#27531;&#24046;&#26159;&#36890;&#36807;&#35745;&#31639;&#31995;&#32479;&#27979;&#37327;&#36755;&#20986;&#21644;&#35266;&#23519;&#32773;&#39044;&#27979;&#36755;&#20986;&#21521;&#37327;&#20043;&#38388;&#24046;&#20540;&#30340;&#33539;&#25968;&#32780;&#24471;&#20986;&#30340;&#12290;&#25925;&#38556;&#38548;&#31163;&#26159;&#36890;&#36807;&#23558;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#36755;&#20986;&#19982;&#27531;&#24046;&#20449;&#21495;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#30340;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#36890;&#36807;&#23545;&#21452;&#32592;&#31995;&#32479;&#30340;&#27169;&#25311;&#32467;&#26524;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new observer-based approach to detect and isolate faulty sensors in industrial systems. Two types of sensor faults are considered: complete failure and sensor deterioration. The proposed method is applicable to general autonomous nonlinear systems without making any assumptions about its triangular and/or normal form, which is usually considered in the observer design literature. The key aspect of our approach is a learning-based design of the Luenberger observer, which involves using a neural network to approximate the injective map that transforms the nonlinear system into a stable linear system with output injection. This learning-based Luenberger observer accurately estimates the system's state, allowing for the detection of sensor faults through residual generation. The residual is computed as the norm of the difference between the system's measured output and the observer's predicted output vectors. Fault isolation is achieved by comparing each sensor's meas
&lt;/p&gt;</description></item><item><title>LASER&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#65292;&#36890;&#36807;&#36923;&#36753;&#35268;&#33539;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#23646;&#24615;&#65292;&#33021;&#22815;&#23545;&#40784;&#21407;&#22987;&#35270;&#39057;&#21644;&#35268;&#33539;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#32423;&#24863;&#30693;&#27169;&#22411;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#32423;&#35268;&#33539;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.07647</link><description>&lt;p&gt;
LASER&#65306;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LASER: Neuro-Symbolic Learning of Semantic Video Representations. (arXiv:2304.07647v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07647
&lt;/p&gt;
&lt;p&gt;
LASER&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#65292;&#36890;&#36807;&#36923;&#36753;&#35268;&#33539;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#23646;&#24615;&#65292;&#33021;&#22815;&#23545;&#40784;&#21407;&#22987;&#35270;&#39057;&#21644;&#35268;&#33539;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#32423;&#24863;&#30693;&#27169;&#22411;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#32423;&#35268;&#33539;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28041;&#21450;&#35270;&#39057;&#30340;AI&#24212;&#29992;&#65288;&#22914;&#35270;&#39057;-&#25991;&#26412;&#23545;&#40784;&#12289;&#35270;&#39057;&#25628;&#32034;&#21644;&#35270;&#39057;&#23383;&#24149;&#65289;&#21463;&#30410;&#20110;&#23545;&#35270;&#39057;&#35821;&#20041;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#65292;&#35201;&#20040;&#22522;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#36890;&#29992;&#23884;&#20837;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#37325;&#35201;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LASER&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#22815;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#26102;&#31354;&#23646;&#24615;&#30340;&#36923;&#36753;&#35268;&#33539;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21407;&#22987;&#35270;&#39057;&#19982;&#35268;&#33539;&#20043;&#38388;&#30340;&#23545;&#40784;&#26469;&#20844;&#24335;&#21270;&#38382;&#39064;&#12290;&#23545;&#40784;&#36807;&#31243;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;&#20302;&#23618;&#24863;&#30693;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#23618;&#35268;&#33539;&#30340;&#32454;&#31890;&#24230;&#35270;&#39057;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#24182;&#21487;&#32435;&#20837;&#20174;&#35268;&#33539;&#23548;&#20986;&#30340;&#23545;&#27604;&#21644;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#20016;&#23500;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern AI applications involving video, such as video-text alignment, video search, and video captioning, benefit from a fine-grained understanding of video semantics. Existing approaches for video understanding are either data-hungry and need low-level annotation, or are based on general embeddings that are uninterpretable and can miss important details. We propose LASER, a neuro-symbolic approach that learns semantic video representations by leveraging logic specifications that can capture rich spatial and temporal properties in video data. In particular, we formulate the problem in terms of alignment between raw videos and specifications. The alignment process efficiently trains low-level perception models to extract a fine-grained video representation that conforms to the desired high-level specification. Our pipeline can be trained end-to-end and can incorporate contrastive and semantic loss functions derived from specifications. We evaluate our method on two datasets with rich sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#23545;&#25239;&#39118;&#26684;&#25200;&#21160;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#25239;&#24615;&#25200;&#21160;&#29305;&#24449;&#39118;&#26684;&#36798;&#21040;&#22495;&#27867;&#21270;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#32467;&#21512;&#28151;&#21512;&#21407;&#22987;&#29305;&#24449;&#30340;&#26041;&#27861;&#32531;&#35299;&#25200;&#21160;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.01959</link><description>&lt;p&gt;
&#38543;&#26426;&#23545;&#25239;&#39118;&#26684;&#25200;&#21160;&#25216;&#26415;&#29992;&#20110;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Randomized Adversarial Style Perturbations for Domain Generalization. (arXiv:2304.01959v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#23545;&#25239;&#39118;&#26684;&#25200;&#21160;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#25239;&#24615;&#25200;&#21160;&#29305;&#24449;&#39118;&#26684;&#36798;&#21040;&#22495;&#27867;&#21270;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#32467;&#21512;&#28151;&#21512;&#21407;&#22987;&#29305;&#24449;&#30340;&#26041;&#27861;&#32531;&#35299;&#25200;&#21160;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#31216;&#20026;&#38543;&#26426;&#23545;&#25239;&#39118;&#26684;&#25200;&#21160;&#25216;&#26415;&#65288;RASP&#65289;&#65292;&#20854;&#21160;&#26426;&#22312;&#20110;&#29305;&#24449;&#32479;&#35745;&#23398;&#25429;&#25417;&#21040;&#27599;&#20010;&#22495;&#30340;&#29305;&#24449;&#12290;&#35813;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#26041;&#21521;&#19978;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#30340;&#39118;&#26684;&#65292;&#26397;&#30528;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#31867;&#21035;&#26041;&#21521;&#24182;&#20351;&#27169;&#22411;&#23398;&#20064;&#36991;&#20813;&#34987;&#22312;&#26410;&#30693;&#30446;&#26631;&#22495;&#20013;&#35266;&#23519;&#21040;&#30340;&#24847;&#22806;&#39118;&#26684;&#25152;&#35823;&#23548;&#12290;&#34429;&#28982;RASP&#33021;&#26377;&#25928;&#22788;&#29702;&#22495;&#28418;&#31227;&#65292;&#20294;&#23427;&#30340;&#31616;&#21333;&#34701;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#38477;&#20302;&#20174;&#28304;&#22495;&#23398;&#20064;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#24182;&#19981;&#38480;&#21046;&#34920;&#24449;&#30340;&#25200;&#21160;&#12290;&#36825;&#20010;&#25361;&#25112;&#30001;&#35268;&#19968;&#21270;&#29305;&#24449;Mixup&#65288;NFM&#65289;&#32531;&#35299;&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#28151;&#21512;&#26469;&#20419;&#36827;&#21407;&#22987;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23545;&#25200;&#21160;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel domain generalization technique, referred to as Randomized Adversarial Style Perturbation (RASP), which is motivated by the observation that the characteristics of each domain are captured by the feature statistics corresponding to style. The proposed algorithm perturbs the style of a feature in an adversarial direction towards a randomly selected class, and makes the model learn against being misled by the unexpected styles observed in unseen target domains. While RASP is effective to handle domain shifts, its naive integration into the training procedure might degrade the capability of learning knowledge from source domains because it has no restriction on the perturbations of representations. This challenge is alleviated by Normalized Feature Mixup (NFM), which facilitates the learning of the original features while achieving robustness to perturbed representations via their mixup during training. We evaluate the proposed algorithm via extensive experiments on var
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#19981;&#38656;&#35201;&#23545;&#30446;&#26631;&#20989;&#25968;&#27491;&#21017;&#21270;&#30340;&#26410;&#27491;&#21017;&#21270;PMD&#31639;&#27861;&#26063;&#65292;&#21487;&#20197;&#23454;&#29616;PI&#30340;&#32500;&#24230;&#33258;&#30001;&#30340;&#32447;&#24615;$\gamma$&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#20010;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#65292;&#19988;&#33258;&#36866;&#24212;&#27493;&#38271;&#26159;&#23454;&#29616;&#36825;&#20010;&#36895;&#29575;&#25152;&#24517;&#38656;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.11381</link><description>&lt;p&gt;
&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#31934;&#30830;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes. (arXiv:2302.11381v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#19981;&#38656;&#35201;&#23545;&#30446;&#26631;&#20989;&#25968;&#27491;&#21017;&#21270;&#30340;&#26410;&#27491;&#21017;&#21270;PMD&#31639;&#27861;&#26063;&#65292;&#21487;&#20197;&#23454;&#29616;PI&#30340;&#32500;&#24230;&#33258;&#30001;&#30340;&#32447;&#24615;$\gamma$&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#20010;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#65292;&#19988;&#33258;&#36866;&#24212;&#27493;&#38271;&#26159;&#23454;&#29616;&#36825;&#20010;&#36895;&#29575;&#25152;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;(Policy Mirror Descent&#65292;PMD)&#26159;&#19968;&#31181;&#24191;&#27867;&#30340;&#31639;&#27861;&#26063;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31995;&#21015;&#26032;&#39062;&#21644;&#22522;&#30784;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#30830;&#30340;&#31574;&#30053;&#35780;&#20215;&#26469;&#24314;&#31435;PI&#21644;PMD&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#19968;&#20010;&#19981;&#38656;&#35201;&#23545;&#30446;&#26631;&#20989;&#25968;&#27491;&#21017;&#21270;&#23601;&#21487;&#20197;&#23545;PI&#20013;&#30340;&#31574;&#30053;&#25913;&#36827;&#27493;&#39588;&#36827;&#34892;&#31639;&#27861;&#27491;&#21017;&#21270;&#30340;&#26410;&#27491;&#21017;&#21270;PMD&#31639;&#27861;&#26063;&#26469;&#23454;&#29616;PI&#30340;&#32500;&#24230;&#33258;&#30001;&#30340;&#32447;&#24615;$\gamma$-&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PMD&#30340;&#36825;&#31181;&#36895;&#29575;&#21644;&#27493;&#38271;&#37117;&#26159;&#26080;&#27861;&#25913;&#36827;&#30340;&#65306;&#25105;&#20204;&#25552;&#20379;&#30340;&#21305;&#37197;&#19979;&#30028;&#34920;&#26126;$\gamma$-&#29575;&#23545;&#20110;PMD&#26041;&#27861;&#21644;PI&#26041;&#27861;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#33258;&#36866;&#24212;&#27493;&#38271;&#26159;&#23454;&#29616;&#23427;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#22312;&#31934;&#30830;&#31574;&#30053;&#35780;&#20215;&#19979;&#30740;&#31350;PMD&#26041;&#27861;&#30340;&#26368;&#20248;&#35823;&#24046;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent (PMD) is a general family of algorithms that covers a wide range of novel and fundamental methods in reinforcement learning. Motivated by the instability of policy iteration (PI) with inexact policy evaluation, unregularised PMD algorithmically regularises the policy improvement step of PI without regularising the objective function. With exact policy evaluation, PI is known to converge linearly with a rate given by the discount factor $\gamma$ of a Markov Decision Process. In this work, we bridge the gap between PI and PMD with exact policy evaluation and show that the dimension-free $\gamma$-rate of PI can be achieved by the general family of unregularised PMD algorithms under an adaptive step-size. We show that both the rate and step-size are unimprovable for PMD: we provide matching lower bounds that demonstrate that the $\gamma$-rate is optimal for PMD methods as well as PI and that the adaptive step-size is necessary to achieve it. Our work is the first to r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#20351;&#24471;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#21035;&#26159;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#21487;&#25193;&#23637;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#65292;&#32780;&#26159;&#36890;&#36807;&#26435;&#34913;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2302.05326</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#36830;&#25509;&#21644;&#36873;&#25321;&#24615;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning. (arXiv:2302.05326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#20351;&#24471;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#21035;&#26159;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#21487;&#25193;&#23637;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#65292;&#32780;&#26159;&#36890;&#36807;&#26435;&#34913;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24863;&#30693;&#35266;&#23519;&#20013;&#26500;&#24314;&#29366;&#24577;&#26159;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#19968;&#31181;&#29992;&#20110;&#29366;&#24577;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290; BPTT&#21644;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#65288;RTRL&#65289;&#26159;&#20004;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24490;&#29615;&#23398;&#20064;&#26041;&#27861;&#12290; BPTT&#22312;&#35745;&#31639;&#26799;&#24230;&#20043;&#21069;&#38656;&#35201;&#23436;&#25972;&#30340;&#35266;&#23519;&#24207;&#21015;&#65292;&#19981;&#36866;&#21512;&#22312;&#32447;&#23454;&#26102;&#26356;&#26032;&#12290; RTRL&#21487;&#20197;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#32593;&#32476;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#65292;&#20351;RTRL&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;RTRL&#19982;&#21442;&#25968;&#25968;&#37327;&#21576;&#32447;&#24615;&#27604;&#20363;&#20851;&#31995;&#12290;&#19982;&#20808;&#21069;&#30340;&#21487;&#25193;&#23637;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#65288;&#20363;&#22914;UORO&#21644;Truncated-BPTT&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#26435;&#34913;&#20102;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires the complete sequence of observations before computing gradients and is unsuitable for online real-time updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules, or learning the network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade-off the functional capacity of the network to achieve scalable learning. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.12261</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#22312;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#20849;&#24773;&#24515;&#29702;&#30340;&#26381;&#21153;&#12290;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#32531;&#35299;&#21333;&#27169;&#24577;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20851;&#31995;&#24314;&#27169;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#12290;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#21462;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#36328;&#27169;&#24577;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;MMGCN&#65289;&#30452;&#25509;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#20887;&#20313;&#20449;&#24687;&#65292;&#19988;&#21487;&#33021;&#20002;&#22833;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#65288;GraphCFC&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#21644;&#20114;&#21160;&#20449;&#24687;&#12290;GraphCFC&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23376;&#31354;&#38388;&#25552;&#21462;&#22120;&#21644;&#25104;&#23545;&#36328;&#27169;&#24577;&#34917;&#20805;&#65288;PairCC&#65289;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#24230;&#25968;&#19981;&#21464;&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#23646;&#24615;&#22270;&#20013;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#30340;&#36793;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#36890;&#36807;&#20351;&#29992;Warner&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#21644;&#31574;&#30053;&#24615;&#36793;&#37319;&#26679;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#21516;&#26102;&#20445;&#30041;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2202.10209</link><description>&lt;p&gt;
&#20445;&#25345;&#24230;&#25968;&#19981;&#21464;&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Degree-Preserving Randomized Response for Graph Neural Networks under Local Differential Privacy. (arXiv:2202.10209v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#24230;&#25968;&#19981;&#21464;&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#23646;&#24615;&#22270;&#20013;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#30340;&#36793;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#36890;&#36807;&#20351;&#29992;Warner&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#21644;&#31574;&#30053;&#24615;&#36793;&#37319;&#26679;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#21516;&#26102;&#20445;&#30041;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;(Differentially private GNNs)&#26469;&#22312;&#22270;&#25968;&#25454;&#19978;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#21516;&#26102;&#24378;&#21147;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;(LDP)&#20445;&#25252;&#26377;&#23646;&#24615;&#30340;&#22270;&#20013;&#27599;&#20010;&#29992;&#25143;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#32780;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#24378;&#38544;&#31169;&#27010;&#24565;&#65292;&#19981;&#38656;&#35201;&#21487;&#20449;&#31532;&#19977;&#26041;&#12290;&#28982;&#32780;&#65292;&#35813;&#31639;&#27861;&#19981;&#20445;&#25252;&#31038;&#20132;&#22270;&#20013;&#30340;&#36793;&#65288;&#21451;&#35850;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#20445;&#25252;&#26080;&#23646;&#24615;&#22270;&#20013;&#30340;&#29992;&#25143;&#38544;&#31169;&#12290;&#22914;&#20309;&#22312;&#26080;&#23646;&#24615;&#22270;&#20013;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#30340;&#24378;&#38544;&#31169;&#20445;&#25252;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LDP&#31639;&#27861;&#65292;&#21517;&#20026;DPRR&#65288;Degree-Preserving Randomized Response&#65289;&#65292;&#29992;&#20110;&#22312;GNN&#20013;&#25552;&#20379;&#36793;&#30340;LDP&#12290;&#25105;&#20204;&#30340;DPRR&#22312;&#25552;&#20379;&#36793;&#30340;LDP&#30340;&#21516;&#26102;&#20445;&#30041;&#20102;&#27599;&#20010;&#29992;&#25143;&#30340;&#24230;&#25968;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#22270;&#32467;&#26500;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;DPRR&#20351;&#29992;&#20102;Warner&#30340;RR&#65288;Randomized Response&#65289;&#21644;&#31574;&#30053;&#24615;&#36793;&#37319;&#26679;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#30340;&#37319;&#26679;&#27010;&#29575;&#26159;&#36890;&#36807;Lapla&#36827;&#34892;&#33258;&#21160;&#35843;&#25972;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private GNNs (Graph Neural Networks) have been recently studied to provide high accuracy in various tasks on graph data while strongly protecting user privacy. In particular, a recent study proposes an algorithm to protect each user's feature vector in an attributed graph with LDP (Local Differential Privacy), a strong privacy notion without a trusted third party. However, this algorithm does not protect edges (friendships) in a social graph, hence cannot protect user privacy in unattributed graphs. How to provide strong privacy with high accuracy in unattributed graphs remains open.  In this paper, we propose a novel LDP algorithm called the DPRR (Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our DPRR preserves each user's degree hence a graph structure while providing edge LDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic edge sampling, where each user's sampling probability is automatically tuned using the Lapla
&lt;/p&gt;</description></item></channel></rss>