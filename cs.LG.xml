<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24076;&#23572;&#20271;&#29305;-&#26045;&#23494;&#29305;&#29420;&#31435;&#24615;&#20934;&#21017;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#33258;&#21160;&#35782;&#21035;&#25935;&#24863;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#39640;&#31038;&#20250;&#24433;&#21709;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#19981;&#20844;&#24179;&#32467;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06994</link><description>&lt;p&gt;
&#22312;&#32676;&#20307;&#20844;&#24179;&#35774;&#32622;&#20013;&#26816;&#27979;&#25935;&#24863;&#29305;&#24449;&#30340;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A statistical approach to detect sensitive features in a group fairness setting. (arXiv:2305.06994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24076;&#23572;&#20271;&#29305;-&#26045;&#23494;&#29305;&#29420;&#31435;&#24615;&#20934;&#21017;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#33258;&#21160;&#35782;&#21035;&#25935;&#24863;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#39640;&#31038;&#20250;&#24433;&#21709;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#19981;&#20844;&#24179;&#32467;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#39640;&#31038;&#20250;&#24433;&#21709;&#24212;&#29992;&#24341;&#36215;&#20102;&#23545;&#20110;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#19981;&#20844;&#24179;&#65288;&#19981;&#19968;&#33268;&#65289;&#32467;&#26524;&#30340;&#25285;&#24551;&#12290;&#24403;&#35780;&#20272;&#36825;&#26679;&#30340;&#19981;&#20844;&#24179;&#20915;&#31574;&#26102;&#65292;&#36890;&#24120;&#20250;&#20381;&#36182;&#20110;&#30001;&#19968;&#32452;&#29305;&#24449;&#30830;&#23450;&#30340;&#39044;&#23450;&#20041;&#32676;&#20307;&#65292;&#36825;&#20123;&#29305;&#24449;&#34987;&#35270;&#20026;&#25935;&#24863;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#20027;&#35266;&#30340;&#65292;&#19981;&#33021;&#20445;&#35777;&#36825;&#20123;&#29305;&#24449;&#26159;&#21807;&#19968;&#38656;&#35201;&#32771;&#34385;&#30340;&#25935;&#24863;&#29305;&#24449;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#23427;&#20204;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#65288;&#19981;&#19968;&#33268;&#65289;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#33258;&#21160;&#35782;&#21035;&#25935;&#24863;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#27169;&#22411;&#26469;&#39564;&#35777;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#22522;&#20110;&#24076;&#23572;&#20271;&#29305;-&#26045;&#23494;&#29305;&#29420;&#31435;&#24615;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#34913;&#37327;&#21464;&#37327;&#20998;&#24067;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#22914;&#26524;&#25935;&#24863;&#29305;&#24449;&#19982;&#26631;&#31614;&#21521;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#24456;&#39640;&#65292;&#37027;&#20040;&#36825;&#20010;&#29305;&#24449;&#25552;&#20379;&#30340;&#20449;&#24687;&#23558;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of machine learning models in decision support systems with high societal impact raised concerns about unfair (disparate) results for different groups of people. When evaluating such unfair decisions, one generally relies on predefined groups that are determined by a set of features that are considered sensitive. However, such an approach is subjective and does not guarantee that these features are the only ones to be considered as sensitive nor that they entail unfair (disparate) outcomes.  In this paper, we propose a preprocessing step to address the task of automatically recognizing sensitive features that does not require a trained model to verify unfair results. Our proposal is based on the Hilber-Schmidt independence criterion, which measures the statistical dependence of variable distributions. We hypothesize that if the dependence between the label vector and a candidate is high for a sensitive feature, then the information provided by this feature will entail disparate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#20102;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#36229;&#27969;&#65292;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;FermiNet&#27169;&#22411;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06989</link><description>&lt;p&gt;
&#36229;&#27969;&#20307;&#30340;&#31070;&#32463;&#27874;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Wave Functions for Superfluids. (arXiv:2305.06989v1 [cond-mat.quant-gas])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#20102;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#36229;&#27969;&#65292;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;FermiNet&#27169;&#22411;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36229;&#27969;&#24615;&#20173;&#28982;&#26159;&#20957;&#32858;&#24577;&#29289;&#29702;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#65288;FermiNet&#65289;&#27874;&#20989;&#25968;Ansatz&#36827;&#34892;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#24378;&#28872;&#30701;&#31243;&#21452;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;-- &#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#65292;&#35813;&#31995;&#32479;&#24050;&#30693;&#23384;&#22312;&#36229;&#27969;&#22522;&#24577;&#65292;&#20294;&#38590;&#20197;&#23450;&#37327;&#25551;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30740;&#31350;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#26102;FermiNet Ansatz&#30340;&#20851;&#38190;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20854;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;FermiNet&#65292;&#21487;&#20197;&#32473;&#20986;&#39640;&#24230;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25968;&#23398;&#35777;&#26126;&#20102;&#26032;&#30340;Ansatz&#26159;&#21407;&#22987;FermiNet&#20307;&#31995;&#32467;&#26500;&#30340;&#20005;&#26684;&#27010;&#25324;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;FermiNet&#20849;&#20139;&#20960;&#20010;&#20248;&#21183;:&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#28040;&#38500;&#20102;&#24213;&#23618;&#22522;&#32452;&#30340;&#38656;&#27714;;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#22312;&#21464;&#20998;&#37327;&#23376;Monte Carlo&#20013;&#20135;&#29983;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding superfluidity remains a major goal of condensed matter physics. Here we tackle this challenge utilizing the recently developed Fermionic neural network (FermiNet) wave function Ansatz for variational Monte Carlo calculations. We study the unitary Fermi gas, a system with strong, short-range, two-body interactions known to possess a superfluid ground state but difficult to describe quantitively. We demonstrate key limitations of the FermiNet Ansatz in studying the unitary Fermi gas and propose a simple modification that outperforms the original FermiNet significantly, giving highly accurate results. We prove mathematically that the new Ansatz is a strict generalization of the original FermiNet architecture, despite the use of fewer parameters. Our approach shares several advantanges with the FermiNet: the use of a neural network removes the need for an underlying basis set; and the flexiblity of the network yields extremely accurate results within a variational quantum Mon
&lt;/p&gt;</description></item><item><title>SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06988</link><description>&lt;p&gt;
&#33258;&#25105;&#38142;&#24335;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#19982;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06988
&lt;/p&gt;
&lt;p&gt;
SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#38382;&#31572;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20123;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#21551;&#21160;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#35270;&#39057;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#36827;&#34892;&#20018;&#25509;&#65292;&#32780;&#26410;&#36827;&#34892;&#26174;&#24335;&#30340;&#35821;&#35328;&#24863;&#30693;&#21644;&#26102;&#38388;&#24314;&#27169;&#12290;&#24403;&#35270;&#39057;&#36755;&#20837;&#20013;&#21482;&#26377;&#19968;&#37096;&#20998;&#19982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#26102;&#65292;&#36825;&#31181;&#22343;&#21248;&#24103;&#37319;&#26679;&#36890;&#24120;&#20250;&#23548;&#33268;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#20002;&#22833;&#12290;&#23613;&#31649;&#20154;&#31867;&#36890;&#24120;&#20250;&#25214;&#21040;&#35270;&#39057;&#20013;&#35201;&#20851;&#27880;&#30340;&#29255;&#27573;&#24182;&#20498;&#24102;&#29255;&#21051;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#35757;&#32451;&#19968;&#20010;&#26126;&#30830;&#30340;&#35270;&#39057;&#29255;&#27573;&#23616;&#37096;&#21270;&#22120;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeViLA&#26694;&#26550;&#65292;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65288;BLIP-2&#65289;&#26469;&#22788;&#29702;&#35270;&#39057;&#30340;&#26102;&#38388;&#20851;&#38190;&#24103;&#23450;&#20301;&#21644;&#38382;&#31572;&#12290;SeViLA&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#65292;&#20004;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24103;&#20197;&#22238;&#31572;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#22312;TVQA&#12289;TVR&#21644;How2QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SeViLA&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27880;&#37322;&#23601;&#33021;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown promising results on utilizing pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20855;&#26377;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#21487;&#35777;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#38480;&#21046;&#20102;&#30446;&#26631;&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.06986</link><description>&lt;p&gt;
&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#21487;&#35777;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. (arXiv:2305.06986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20855;&#26377;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#21487;&#35777;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#38480;&#21046;&#20102;&#30446;&#26631;&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#12290;&#28145;&#24230;&#32593;&#32476;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#30340;&#33021;&#21147;&#23545;&#20854;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20173;&#28982;&#19981;&#22815;&#28165;&#26224;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#20027;&#35201;&#23616;&#38480;&#20110;&#20004;&#23618;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#35777;&#26126;&#30340;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36890;&#36807;&#36880;&#23618;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#19977;&#23618;&#32593;&#32476;&#23398;&#20064;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#23427;&#19978;&#30028;&#20102;&#30446;&#26631;&#20855;&#26377;&#29305;&#23450;&#23618;&#27425;&#32467;&#26500;&#26102;&#23454;&#29616;&#20302;&#27979;&#35797;&#38169;&#35823;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#20363;&#21270;&#21040;&#29305;&#23450;&#30340;&#32479;&#35745;&#23398;&#23398;&#20064;&#35774;&#32622;&#20013;&#8212;&#8212;&#21333;&#25351;&#25968;&#27169;&#22411;&#21644;&#20108;&#27425;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20102;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06983</link><description>&lt;p&gt;
&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20102;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20855;&#26377;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#34394;&#20551;&#30340;&#21644;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#20174;&#22806;&#37096;&#30693;&#35782;&#36164;&#28304;&#20013;&#26816;&#32034;&#20449;&#24687;&#26469;&#22686;&#24378;LM&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;LM&#37319;&#29992;&#19968;&#31181;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#35774;&#32622;&#65292;&#20165;&#22522;&#20110;&#36755;&#20837;&#19968;&#27425;&#26816;&#32034;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#29983;&#25104;&#38271;&#25991;&#26412;&#30340;&#26356;&#26222;&#36941;&#30340;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#26029;&#22320;&#25910;&#38598;&#20449;&#24687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36807;&#21435;&#26377;&#19968;&#20123;&#26816;&#32034;&#20449;&#24687;&#65292;&#21516;&#26102;&#29983;&#25104;&#36755;&#20986;&#30340;&#21162;&#21147;&#65292;&#22823;&#22810;&#25968;&#37117;&#26159;&#20351;&#29992;&#21069;&#19968;&#20010;&#19978;&#19979;&#25991;&#20316;&#20026;&#26597;&#35810;&#65292;&#22312;&#22266;&#23450;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#26816;&#32034;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#24191;&#20041;&#35270;&#22270;&#65292;&#21363;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#20174;&#21738;&#37324;&#26816;&#32034;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21069;&#30651;&#24615;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;FLARE&#65289;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#29983;&#25104;&#22120;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#20027;&#21160;&#26597;&#35810;&#26816;&#32034;&#32452;&#20214;&#26469;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#12290;FLARE&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20027;&#21160;&#26816;&#32034;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval aug
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20132;&#21449;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#21644;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#19982;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06969</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20132;&#21449;&#20844;&#24179;&#24615;&#35843;&#26597;&#65306;&#27010;&#24565;&#12289;&#32531;&#35299;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges. (arXiv:2305.06969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20132;&#21449;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#21644;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#19982;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#20026;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#22914;&#21009;&#20107;&#21028;&#20915;&#21644;&#38134;&#34892;&#36151;&#27454;&#65292;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#30340;&#26356;&#22810;&#20851;&#27880;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#31639;&#27861;&#21644;&#25351;&#26631;&#26469;&#32531;&#35299;&#21644;&#34913;&#37327;&#36825;&#20123;&#27495;&#35270;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20559;&#35265;&#24418;&#24335;&#65292;&#31216;&#20026;&#20132;&#21449;&#20559;&#35265;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#65292;&#20363;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20132;&#21449;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#21449;&#20844;&#24179;&#24615;&#21644;&#32531;&#35299;&#30340;&#20998;&#31867;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25351;&#23548;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Machine Learning systems, especially in more decision-critical applications such as criminal sentencing and bank loans, has led to increased concerns about fairness implications. Algorithms and metrics have been developed to mitigate and measure these discriminations. More recently, works have identified a more challenging form of bias called intersectional bias, which encompasses multiple sensitive attributes, such as race and gender, together. In this survey, we review the state-of-the-art in intersectional fairness. We present a taxonomy for intersectional notions of fairness and mitigation. Finally, we identify the key challenges and provide researchers with guidelines for future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32423;&#32852;&#20132;&#21449;&#20851;&#27880;&#32593;&#32476;&#65288;CCAN&#65289;&#65292;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#22788;&#29702;&#22823;&#37327;&#25552;&#21462;&#20986;&#26469;&#30340;&#22270;&#20687;&#22359;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36716;&#25442;&#22120;&#35745;&#31639;&#22797;&#26434;&#24230;&#21576;&#24179;&#26041;&#32423;&#22686;&#38271;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#25972;&#24352;&#22270;&#20687;&#20998;&#31867;&#25928;&#26524;&#33267;&#23569;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.06963</link><description>&lt;p&gt;
&#22522;&#20110;&#32423;&#32852;&#20132;&#21449;&#20851;&#27880;&#32593;&#32476;&#30340;&#36716;&#25442;&#22120;&#29992;&#20110;&#25968;&#25454;&#26377;&#25928;&#30340;&#25972;&#24352;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers. (arXiv:2305.06963v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32423;&#32852;&#20132;&#21449;&#20851;&#27880;&#32593;&#32476;&#65288;CCAN&#65289;&#65292;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#22788;&#29702;&#22823;&#37327;&#25552;&#21462;&#20986;&#26469;&#30340;&#22270;&#20687;&#22359;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36716;&#25442;&#22120;&#35745;&#31639;&#22797;&#26434;&#24230;&#21576;&#24179;&#26041;&#32423;&#22686;&#38271;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#25972;&#24352;&#22270;&#20687;&#20998;&#31867;&#25928;&#26524;&#33267;&#23569;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#24352;&#22270;&#20687;&#25104;&#20687;&#25216;&#26415;&#20801;&#35768;&#23545;&#32452;&#32455;&#23398;&#26631;&#26412;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#21644;&#25968;&#23383;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#36825;&#20123;&#22270;&#20687;&#30340;&#33258;&#21160;&#20998;&#26512;&#22240;&#27492;&#38656;&#27714;&#26497;&#39640;&#12290;&#36716;&#25442;&#22120;&#26550;&#26500;&#34987;&#25552;&#20986;&#20316;&#20026;&#26377;&#25928;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#20449;&#24687;&#30340;&#21487;&#33021;&#20505;&#36873;&#32773;&#65292;&#25226;&#25972;&#24352;&#22270;&#20687;&#21010;&#20998;&#20026;&#22810;&#20010;&#22270;&#20687;&#22359;&#65292;&#20174;&#36825;&#20123;&#22270;&#20687;&#22359;&#20013;&#25552;&#21462;&#29305;&#24449;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#36716;&#25442;&#22120;&#20801;&#35768;&#21516;&#26102;&#22788;&#29702;&#22823;&#37327;&#36755;&#20837;&#26631;&#35760;&#65292;&#20294;&#35745;&#31639;&#37327;&#38543;&#30528;&#36755;&#20837;&#26631;&#35760;&#25968;&#21644;&#22270;&#20687;&#22359;&#25968;&#30340;&#22686;&#21152;&#21576;&#24179;&#26041;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#22411;&#32423;&#32852;&#20132;&#21449;&#20851;&#27880;&#32593;&#32476;&#65288;CCAN&#65289;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#25552;&#21462;&#30340;&#22270;&#20687;&#22359;&#25968;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26550;&#26500;&#33267;&#23569;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whole-Slide Imaging allows for the capturing and digitization of high-resolution images of histological specimen. An automated analysis of such images using deep learning models is therefore of high demand. The transformer architecture has been proposed as a possible candidate for effectively leveraging the high-resolution information. Here, the whole-slide image is partitioned into smaller image patches and feature tokens are extracted from these image patches. However, while the conventional transformer allows for a simultaneous processing of a large set of input tokens, the computational demand scales quadratically with the number of input tokens and thus quadratically with the number of image patches. To address this problem we propose a novel cascaded cross-attention network (CCAN) based on the cross-attention mechanism that scales linearly with the number of extracted patches. Our experiments demonstrate that this architecture is at least on-par with and even outperforms other at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38170;&#22270;&#36741;&#21161;&#30340;&#28145;&#24230;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65288;DMCAG&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;DMVSC&#26041;&#27861;&#23384;&#22312;&#30340;&#33258;&#32534;&#30721;&#22120;&#23884;&#20837;&#23376;&#20248;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#23567;&#23610;&#23544;&#38170;&#22270;&#26469;&#26174;&#33879;&#20943;&#23569;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06939</link><description>&lt;p&gt;
&#38170;&#22270;&#36741;&#21161;&#30340;&#28145;&#24230;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Multi-View Subspace Clustering with Anchor Graph. (arXiv:2305.06939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38170;&#22270;&#36741;&#21161;&#30340;&#28145;&#24230;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65288;DMCAG&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;DMVSC&#26041;&#27861;&#23384;&#22312;&#30340;&#33258;&#32534;&#30721;&#22120;&#23884;&#20837;&#23376;&#20248;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#23567;&#23610;&#23544;&#38170;&#22270;&#26469;&#26174;&#33879;&#20943;&#23569;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#28145;&#24230;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#65288;DMVSC&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DMVSC&#26041;&#27861;&#20173;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;(1)&#23427;&#20204;&#20027;&#35201;&#20381;&#38752;&#33258;&#32534;&#30721;&#22120;&#26469;&#38750;&#32447;&#24615;&#23884;&#20837;&#25968;&#25454;&#65292;&#32780;&#23884;&#20837;&#21487;&#33021;&#23545;&#20110;&#32858;&#31867;&#26469;&#35828;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#32858;&#31867;&#30446;&#26631;&#24456;&#23569;&#32771;&#34385;&#22312;&#33258;&#32534;&#30721;&#22120;&#20013;;(2)&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#20108;&#27425;&#25110;&#29978;&#33267;&#19977;&#27425;&#22797;&#26434;&#24230;&#65292;&#36825;&#20351;&#24471;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#21363;&#38170;&#22270;&#36741;&#21161;&#30340;&#28145;&#24230;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#65288;DMCAG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DMCAG&#39318;&#20808;&#29420;&#31435;&#22320;&#20026;&#27599;&#20010;&#35270;&#22270;&#23398;&#20064;&#23884;&#20837;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#33719;&#21462;&#23376;&#31354;&#38388;&#34920;&#31034;&#12290;&#20026;&#20102;&#26174;&#33879;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#35270;&#22270;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#23567;&#23610;&#23544;&#30340;&#38170;&#22270;&#12290;&#28982;&#21518;&#65292;&#22312;&#19968;&#20010;&#32508;&#21512;&#38170;&#22270;&#19978;&#25191;&#34892;&#35889;&#32858;&#31867;&#20197;&#33719;&#21462;&#20266;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36127;&#38754;&#24433;&#21709;......
&lt;/p&gt;
&lt;p&gt;
Deep multi-view subspace clustering (DMVSC) has recently attracted increasing attention due to its promising performance. However, existing DMVSC methods still have two issues: (1) they mainly focus on using autoencoders to nonlinearly embed the data, while the embedding may be suboptimal for clustering because the clustering objective is rarely considered in autoencoders, and (2) existing methods typically have a quadratic or even cubic complexity, which makes it challenging to deal with large-scale data. To address these issues, in this paper we propose a novel deep multi-view subspace clustering method with anchor graph (DMCAG). To be specific, DMCAG firstly learns the embedded features for each view independently, which are used to obtain the subspace representations. To significantly reduce the complexity, we construct an anchor graph with small size for each view. Then, spectral clustering is performed on an integrated anchor graph to obtain pseudo-labels. To overcome the negativ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#21322;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36827;&#31243;&#20013;&#65292;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#38477;&#20302;&#26102;&#38388;&#20998;&#36776;&#29575;&#26469;&#20943;&#23569;&#35745;&#21010;&#26102;&#38388;&#30340;&#36873;&#39033;&#20381;&#36182;&#19978;&#30028;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#36890;&#36807;&#35201;&#32032;&#26694;&#26550;&#23454;&#29616;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#21644;&#40065;&#26834;&#24615;&#25552;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06936</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#38388;&#21322;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36827;&#31243;&#20013;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#20381;&#36182;&#20110;&#36873;&#39033;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes. (arXiv:2305.06936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#21322;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36827;&#31243;&#20013;&#65292;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#38477;&#20302;&#26102;&#38388;&#20998;&#36776;&#29575;&#26469;&#20943;&#23569;&#35745;&#21010;&#26102;&#38388;&#30340;&#36873;&#39033;&#20381;&#36182;&#19978;&#30028;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#36890;&#36807;&#35201;&#32032;&#26694;&#26550;&#23454;&#29616;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#21644;&#40065;&#26834;&#24615;&#25552;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#37117;&#20855;&#26377;&#22797;&#26434;&#21644;&#24322;&#26500;&#30340;&#32467;&#26500;&#65292;&#23548;&#33268;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#24335;&#38590;&#20197;&#24212;&#29992;&#25110;&#29978;&#33267;&#19981;&#21487;&#34892;&#12290;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#36890;&#36807;&#26041;&#20415;&#30340;&#20219;&#21153;&#22810;&#32423;&#20998;&#35299;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#35299;&#20915;&#21464;&#24471;&#26131;&#20110;&#35775;&#38382;&#12290;&#34429;&#28982;&#23454;&#38469;&#24212;&#29992;&#20013;&#32463;&#24120;&#20351;&#29992;&#65292;&#20294;&#24456;&#23569;&#26377;&#20316;&#21697;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#26377;&#25928;&#22320;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#19982;&#26631;&#20934;&#24179;&#38754;&#26041;&#27861;&#30456;&#27604;&#20309;&#26102;&#26356;&#24895;&#24847;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#26377;&#38480;&#26102;&#38388;&#38382;&#39064;&#20013;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#36873;&#39033;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20998;&#23618;&#32467;&#26500;&#24341;&#36215;&#30340;&#26102;&#38388;&#25277;&#35937;&#25152;&#24102;&#26469;&#30340;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#65292;&#25512;&#21160;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#28982;&#21518;&#65292;&#19987;&#27880;&#20110;HRL&#26041;&#27861;&#30340;&#19968;&#20010;&#23376;&#38598;&#35201;&#32032;&#26694;&#26550;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35201;&#32032;&#22914;&#20309;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#23398;&#20064;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large variety of real-world Reinforcement Learning (RL) tasks is characterized by a complex and heterogeneous structure that makes end-to-end (or flat) approaches hardly applicable or even infeasible. Hierarchical Reinforcement Learning (HRL) provides general solutions to address these problems thanks to a convenient multi-level decomposition of the tasks, making their solution accessible. Although often used in practice, few works provide theoretical guarantees to justify this outcome effectively. Thus, it is not yet clear when to prefer such approaches compared to standard flat ones. In this work, we provide an option-dependent upper bound to the regret suffered by regret minimization algorithms in finite-horizon problems. We illustrate that the performance improvement derives from the planning horizon reduction induced by the temporal abstraction enforced by the hierarchical structure. Then, focusing on a sub-setting of HRL approaches, the options framework, we highlight how the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#23545;&#20256;&#32479;&#35748;&#35782;&#30340;&#39072;&#35206;&#24615;&#35266;&#28857;&#65306;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.06934</link><description>&lt;p&gt;
&#20154;&#31867;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#65306;&#20197;IEEEXtreme&#32534;&#31243;&#31454;&#36187;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition. (arXiv:2305.06934v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#23545;&#20256;&#32479;&#35748;&#35782;&#30340;&#39072;&#35206;&#24615;&#35266;&#28857;&#65306;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#21457;&#24067;&#20197;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#31361;&#20986;&#20102;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#23427;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#24448;&#24448;&#21487;&#20197;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#20154;&#31867;&#34920;&#29616;&#20248;&#24322;&#30340;&#23454;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#31435;&#30340;&#35266;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#23558;IEEExtreme&#32534;&#31243;&#25361;&#25112;&#36187;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#22797;&#26434;&#24230;&#38382;&#39064;&#30340;&#30693;&#21517;&#22269;&#38469;&#32534;&#31243;&#31454;&#36187;&#12290;&#20026;&#20102;&#36827;&#34892;&#24443;&#24213;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;102&#20010;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;IEEExtreme&#29256;&#26412;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;&#32534;&#31243;&#35821;&#35328;Python&#12289;Java&#21644;C++&#36827;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#20998;&#26512;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#35777;&#26126;&#19982;&#26222;&#36941;&#35748;&#20026;&#30340;&#30456;&#21453;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#22312;&#32534;&#31243;&#29615;&#22659;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#30340;&#26576;&#20123;&#26041;&#38754;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the release of ChatGPT, numerous studies have highlighted the remarkable performance of ChatGPT, which often rivals or even surpasses human capabilities in various tasks and domains. However, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming. We utilize the IEEExtreme Challenge competition as a benchmark, a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. To conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct IEEExtreme editions, using three major programming languages: Python, Java, and C++. Our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over ChatGPT in certain aspects of problem-solving within the programming context. In fact, we fou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#27010;&#29575;&#22320;&#20174;&#38750;&#20856;&#22411;&#38543;&#26426;&#21021;&#22987;&#21270;&#36798;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#30697;&#38453;&#20998;&#35299;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#21021;&#22987;&#21270;&#19981;&#20165;&#22312;&#29702;&#35770;&#19978;&#26377;&#30410;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06927</link><description>&lt;p&gt;
&#30697;&#38453;&#20998;&#35299;&#20013;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of Alternating Gradient Descent for Matrix Factorization. (arXiv:2305.06927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#27010;&#29575;&#22320;&#20174;&#38750;&#20856;&#22411;&#38543;&#26426;&#21021;&#22987;&#21270;&#36798;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#30697;&#38453;&#20998;&#35299;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#21021;&#22987;&#21270;&#19981;&#20165;&#22312;&#29702;&#35770;&#19978;&#26377;&#30410;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#24212;&#29992;&#20110;&#19981;&#23545;&#31216;&#30697;&#38453;&#20998;&#35299;&#30446;&#26631;&#30340;&#20855;&#26377;&#22266;&#23450;&#27493;&#38271;$\eta&gt;0$&#30340;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#65288;AGD&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23545;&#20110;&#31209;&#20026;$r$&#30340;&#30697;&#38453;$\mathbf {A}\in \mathbb {R} ^ {m \times n}$&#65292;$T=\left(\left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2\log(1/\epsilon)\right)$&#27425;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#21363;&#21487;&#20174;&#38750;&#20856;&#22411;&#38543;&#26426;&#21021;&#22987;&#21270;&#39640;&#27010;&#29575;&#22320;&#36798;&#21040;$\epsilon$-&#26368;&#20248;&#20998;&#35299;$\|\mathbf {A}\mathbf {X}_T^{\vphantom{\intercal}}\mathbf {Y}_T^{\intercal}\|_{\rm F}^2\le\epsilon\|\mathbf {A}\|_{\rm F}^2$&#12290;&#20998;&#35299;&#20013;&#22240;&#23376;&#30340;&#31209;&#20026;$d&gt;r$&#65292;&#22240;&#27492;$\mathbf{X}_T\in\mathbb{R}^{m \times d}$&#19988;$\mathbf{Y}_T\in\mathbb{R}^{n \times d}$&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#19981;&#20165;&#22312;&#29702;&#35770;&#19978;&#26377;&#30410;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#27010;&#24565;&#19978;&#24456;&#31616;&#21333;&#65306;&#19968;&#33268;&#30340;PL&#19981;&#31561;&#24335;&#21644;&#19968;&#33268;&#30340;Lipschitz&#24179;&#28369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider alternating gradient descent (AGD) with fixed step size $\eta &gt; 0$, applied to the asymmetric matrix factorization objective. We show that, for a rank-$r$ matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, $T = \left( \left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2 \log(1/\epsilon)\right)$ iterations of alternating gradient descent suffice to reach an $\epsilon$-optimal factorization $\| \mathbf{A} \mathbf{X}_T^{\vphantom{\intercal}} \mathbf{Y}_T^{\intercal} \|_{\rm F}^2 \leq \epsilon \| \mathbf{A} \|_{\rm F}^2$ with high probability starting from an atypical random initialization. The factors have rank $d&gt;r$ so that $\mathbf{X}_T\in\mathbb{R}^{m \times d}$ and $\mathbf{Y}_T \in\mathbb{R}^{n \times d}$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothne
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#23610;&#24230;&#19978;&#20934;&#30830;&#39044;&#27979;&#38146;&#37329;&#23646;&#34920;&#38754;&#21644;&#26377;&#38480;&#28201;&#24230;&#19979;&#30340;&#25209;&#37327;&#24615;&#36136;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35745;&#31639;&#30340;&#32570;&#38519;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#38146;&#37329;&#23646;&#22312;&#30005;&#27744;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.06925</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#23610;&#24230;&#19978;&#20934;&#30830;&#39044;&#27979;&#38146;&#37329;&#23646;&#34920;&#38754;&#21644;&#26377;&#38480;&#28201;&#24230;&#19979;&#30340;&#25209;&#37327;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Accurate Surface and Finite Temperature Bulk Properties of Lithium Metal at Large Scales using Machine Learning Interaction Potentials. (arXiv:2305.06925v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06925
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#23610;&#24230;&#19978;&#20934;&#30830;&#39044;&#27979;&#38146;&#37329;&#23646;&#34920;&#38754;&#21644;&#26377;&#38480;&#28201;&#24230;&#19979;&#30340;&#25209;&#37327;&#24615;&#36136;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35745;&#31639;&#30340;&#32570;&#38519;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#38146;&#37329;&#23646;&#22312;&#30005;&#27744;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#37329;&#23646;&#30340;&#24615;&#36136;&#26159;&#35774;&#35745;&#38146;&#31163;&#23376;&#21644;&#38146;&#37329;&#23646;&#30005;&#27744;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#30001;&#20110;&#38146;&#30340;&#39640;&#21453;&#24212;&#24615;&#21644;&#20302;&#29076;&#28857;&#20197;&#21450;&#38146;&#22312;&#30005;&#27744;&#20013;&#23384;&#22312;&#20110;&#24494;&#35266;&#23610;&#24230;&#19979;&#65292;&#24456;&#38590;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#25506;&#27979;&#12290;&#35745;&#31639;&#19978;&#65292;&#32570;&#23569;&#33021;&#22815;&#22312;&#25152;&#26377;&#24615;&#36136;&#19978;&#19968;&#33268;&#19988;&#23450;&#37327;&#20934;&#30830;&#30340;&#32463;&#39564;&#21183;&#65292;&#32780;&#20174;&#22836;&#35745;&#31639;&#30340;&#25104;&#26412;&#21448;&#22826;&#39640;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183; (MLIPs) &#23545;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770; (DFT) &#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22312;&#22823;&#38271;&#24230;&#21644;&#26102;&#38388;&#23610;&#24230;&#19979;&#20197;&#29366;&#20917;-of-the-art &#31934;&#24230;&#22797;&#29616;&#23454;&#39564;&#21644;&#20174;&#22836;&#35745;&#31639;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20934;&#30830;&#39044;&#27979;&#28909;&#21147;&#23398;&#24615;&#36136;&#12289;&#22768;&#23376;&#20809;&#35889;&#12289;&#24377;&#24615;&#24120;&#25968;&#30340;&#28201;&#24230;&#20381;&#36182;&#24615;&#20197;&#21450;&#21508;&#31181;&#34920;&#38754;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#22312; DFT &#20013;&#26080;&#27861;&#33719;&#24471;&#12290;&#25105;&#20204;&#35748;&#20026;&#19981;&#21516;&#30340; DFT &#27867;&#20989;&#23384;&#22312;&#24494;&#22937;&#20294;&#26174;&#30528;&#30340;&#23450;&#37327;&#24046;&#24322;&#65292;&#24433;&#21709;&#20102;&#20851;&#38190;&#24615;&#36136;&#22914;&#34920;&#38754;&#33021;&#12290;&#36890;&#36807; MLIPs &#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23454;&#29616;&#20102;&#23545;&#38146;&#37329;&#23646;&#30340;&#22823;&#23610;&#24230;&#22810;&#23610;&#24230;&#27169;&#25311;&#65292;&#36825;&#26159;&#30740;&#31350;&#30005;&#27744;&#20013;&#38146;&#37329;&#23646;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The properties of lithium metal are key parameters in the design of lithium ion and lithium metal batteries. They are difficult to probe experimentally due to the high reactivity and low melting point of lithium as well as the microscopic scales at which lithium exists in batteries where it is found to have enhanced strength, with implications for dendrite suppression strategies. Computationally, there is a lack of empirical potentials that are consistently quantitatively accurate across all properties and ab-initio calculations are too costly. In this work, we train Machine Learning Interaction Potentials (MLIPs) on Density Functional Theory (DFT) data to state-of-the-art accuracy in reproducing experimental and ab-initio results across a wide range of simulations at large length and time scales. We accurately predict thermodynamic properties, phonon spectra, temperature dependence of elastic constants and various surface properties inaccessible using DFT. We establish that there exis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#23454;&#29616;&#20102;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;&#65292;&#38024;&#23545;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20808;&#39564;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.06924</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#22312;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#23454;&#29616;&#20808;&#39564;&#30693;&#35782;&#20256;&#36882;&#20197;&#36827;&#34892;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation. (arXiv:2305.06924v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#23454;&#29616;&#20102;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;&#65292;&#38024;&#23545;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20808;&#39564;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#24066;&#22330;&#30340;&#25237;&#26631;&#28216;&#25103;&#20013;&#65292;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#20272;&#35745;&#26159;&#21457;&#30005;&#20844;&#21496;&#65288;GENCO&#65289;&#36827;&#34892;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#29420;&#31435;&#31995;&#32479;&#36816;&#33829;&#21830;&#65288;ISO&#65289;&#36827;&#34892;&#24066;&#22330;&#30417;&#35270;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NE&#20272;&#35745;&#26041;&#27861;&#22312;&#26032;&#20852;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#65288;FEM&#65289;&#20013;&#26159;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22312;&#20219;&#20309;&#29615;&#22659;&#21464;&#21270;&#20043;&#21069;&#65292;&#22914;&#36127;&#36733;&#38656;&#27714;&#21464;&#21270;&#12289;&#32593;&#32476;&#25317;&#22581;&#21644;&#24066;&#22330;&#35774;&#35745;&#30340;&#20462;&#25913;&#65292;&#25237;&#26631;&#31574;&#30053;&#30340;&#20808;&#39564;&#30693;&#35782;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#38024;&#23545;FEM&#24320;&#21457;&#20102;Bayes&#33258;&#36866;&#24212;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;BAMDP-FEM&#65289;&#65292;&#20197;&#32771;&#34385;&#20808;&#39564;&#30693;&#35782;&#26469;&#24314;&#27169;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#12290;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65288;MAGAIL-FEM&#65289;&#65292;&#20351;GENCO&#33021;&#22815;&#21516;&#26102;&#20174;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#24471;&#21040;&#30340;NE&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#65288;BNE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Nash Equilibrium (NE) estimation in bidding games of electricity markets is the key concern of both generation companies (GENCOs) for bidding strategy optimization and the Independent System Operator (ISO) for market surveillance. However, existing methods for NE estimation in emerging modern electricity markets (FEM) are inaccurate and inefficient because the priori knowledge of bidding strategies before any environment changes, such as load demand variations, network congestion, and modifications of market design, is not fully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM (BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategy optimization considering the priori knowledge. A novel Multi-Agent Generative Adversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed to enable GENCOs to learn simultaneously from priori knowledge and interactions with changing environments. The obtained NE is a Bayesian Nash Equilibrium (BNE) with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26469;&#32852;&#21512;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#21644;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#20135;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2305.06921</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#26410;&#26469;&#30340;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#65311;&#31532;&#20108;&#37096;&#20998;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications. (arXiv:2305.06921v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26469;&#32852;&#21512;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#21644;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#20135;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20026;&#20004;&#37096;&#20998;&#30340;&#35770;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#33539;&#24335;&#29702;&#35770;&#21644;&#35814;&#32454;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#25311;&#26469;&#32852;&#21512;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#36890;&#36807;&#38416;&#36848;&#35814;&#32454;&#30340;&#26041;&#27861;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#65288;ESM&#65289;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#65288;ASM&#65289;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#65288;RC&#65289;&#21644;&#37329;&#34701;&#24066;&#22330;&#65288;FM&#65289;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#65288;VB&#65289;&#20135;&#21697;&#26469;&#36827;&#19968;&#27493;&#28436;&#31034;&#36825;&#19968;&#29702;&#35770;&#12290;&#26681;&#25454;&#31532;&#19968;&#37096;&#20998;&#25552;&#20986;&#30340;&#29702;&#35770;&#65292;&#39318;&#20808;&#30830;&#23450;&#32852;&#21512;&#24066;&#22330;&#20013;&#30340;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;&#25509;&#30528;&#65292;&#24320;&#21457;&#20102;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#21644;&#19981;&#30830;&#23450;&#39118;&#38505;&#32435;&#20837;&#27169;&#22411;&#20844;&#24335;&#20013;&#12290;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#36817;&#31471;&#20248;&#21270;&#65288;MAPPO&#65289;&#31639;&#27861;&#65292;&#20316;&#20026;&#31532;&#19968;&#37096;&#20998;&#24320;&#21457;&#30340;&#24191;&#20041;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20123;&#24066;&#22330;&#36816;&#34892;&#32489;&#25928;&#25351;&#26631;&#65292;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This two-part paper develops a paradigmatic theory and detailed methods of the joint electricity market design using reinforcement-learning (RL)-based simulation. In Part 2, this theory is further demonstrated by elaborating detailed methods of designing an electricity spot market (ESM), together with a reserved capacity product (RC) in the ancillary service market (ASM) and a virtual bidding (VB) product in the financial market (FM). Following the theory proposed in Part 1, firstly, market design options in the joint market are specified. Then, the Markov game model is developed, in which we show how to incorporate market design options and uncertain risks in model formulation. A multi-agent policy proximal optimization (MAPPO) algorithm is elaborated, as a practical implementation of the generalized market simulation method developed in Part 1. Finally, the case study demonstrates how to pick the best market design options by using some of the market operation performance indicators 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#21463;&#26410;&#30693;&#24178;&#25200;&#21644;&#38459;&#23612;&#24433;&#21709;&#26102;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#35299;&#26512;&#39033;&#30340;&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#65292;&#21363;&#20351;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#65292;&#20063;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#21160;&#24577;&#65292;&#23545;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06920</link><description>&lt;p&gt;
&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian system identification. (arXiv:2305.06920v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#21463;&#26410;&#30693;&#24178;&#25200;&#21644;&#38459;&#23612;&#24433;&#21709;&#26102;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#35299;&#26512;&#39033;&#30340;&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#65292;&#21363;&#20351;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#65292;&#20063;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#21160;&#24577;&#65292;&#23545;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#26102;&#65292;&#30830;&#23450;&#29289;&#29702;&#31995;&#32479;&#30340;&#22522;&#26412;&#21160;&#24577;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20551;&#35774;&#19968;&#23450;&#30340;&#20266;&#21704;&#23494;&#39039;&#24418;&#24335;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#31995;&#32479;&#21463;&#21040;&#26410;&#30693;&#38459;&#23612;&#21644;&#22806;&#25200;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#20063;&#33021;&#22815;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#30340;&#35299;&#26512;&#39033;&#12290;&#22312;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#39033;&#30340;&#28151;&#21512;&#27169;&#22411;&#20173;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#23601;&#20687;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#19968;&#26679;&#12290;&#36825;&#20351;&#24471;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#65292;&#36991;&#20813;&#35757;&#32451;&#20013;&#30340;&#23454;&#38469;&#31215;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#22914;&#20309;&#25552;&#39640;&#24615;&#33021;&#30340;&#21508;&#31181;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the underlying dynamics of physical systems can be challenging when only provided with observational data. In this work, we consider systems that can be modelled as first-order ordinary differential equations. By assuming a certain pseudo-Hamiltonian formulation, we are able to learn the analytic terms of internal dynamics even if the model is trained on data where the system is affected by unknown damping and external disturbances. In cases where it is difficult to find analytic terms for the disturbances, a hybrid model that uses a neural network to learn these can still accurately identify the dynamics of the system as if under ideal conditions. This makes the models applicable in situations where other system identification models fail. Furthermore, we propose to use a fourth-order symmetric integration scheme in the loss function and avoid actual integration in the training, and demonstrate on varied examples how this leads to increased performance on noisy data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#25104;&#20687;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#36890;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#24230;&#37327;&#30340;&#20803;&#23398;&#20064;&#22312;&#23567;&#25968;&#25454;&#22330;&#26223;&#19979;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.06912</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation. (arXiv:2305.06912v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06912
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#25104;&#20687;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#36890;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#24230;&#37327;&#30340;&#20803;&#23398;&#20064;&#22312;&#23567;&#25968;&#25454;&#22330;&#26223;&#19979;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20803;&#23398;&#20064;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#23616;&#38480;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#20854;&#20182;&#20219;&#21153;&#65292;&#22914;&#20998;&#21106;&#21644;&#26816;&#27979;&#30340;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#27604;&#36739;&#20998;&#26512;&#20102;&#19981;&#21516;&#33539;&#24335;&#30340;&#20803;&#23398;&#20064;&#22120;&#22312;&#19981;&#21516;&#31232;&#30095;&#26631;&#27880;&#25918;&#23556;&#24615;&#20219;&#21153;&#20013;&#36866;&#29992;&#20110;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#30340;&#25928;&#26524;&#12290;&#25104;&#20687;&#27169;&#24577;&#21253;&#25324;2D&#33016;&#37096;&#12289;&#20083;&#33146;&#21644;&#29273;&#31185;X&#20809;&#20197;&#21450;&#24425;&#36229;&#21644;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;2D&#28369;&#21160;&#20999;&#29255;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32771;&#34385;&#20102;9&#31181;&#20803;&#23398;&#20064;&#22120;&#12289;4&#31181;&#20027;&#24178;&#32593;&#32476;&#21644;&#22810;&#20010;&#30446;&#26631;&#22120;&#23448;&#20998;&#21106;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#20855;&#26377;&#19981;&#21516;&#24369;&#26631;&#27880;&#26679;&#24335;&#21644;&#23494;&#24230;&#30340;&#21307;&#23398;&#23567;&#25968;&#25454;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#36739;&#23567;&#30340;&#39046;&#22495;&#21464;&#21270;&#19979;&#65292;&#22522;&#20110;&#24230;&#37327;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#21106;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#22312;&#20803;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#22521;&#35757;&#65292;&#32780;&#26377;&#20123;&#26041;&#27861;&#21017;&#21487;&#33021;&#22312;&#38754;&#23545;&#36739;&#22823;&#30340;&#39046;&#22495;&#21464;&#21270;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most uses of Meta-Learning in visual recognition are very often applied to image classification, with a relative lack of works in other tasks {such} as segmentation and detection. We propose a generic Meta-Learning framework for few-shot weakly-supervised segmentation in medical imaging domains. We conduct a comparative analysis of meta-learners from distinct paradigms adapted to few-shot image segmentation in different sparsely annotated radiological tasks. The imaging modalities include 2D chest, mammographic and dental X-rays, as well as 2D slices of volumetric tomography and resonance images. Our experiments consider a total of 9 meta-learners, 4 backbones and multiple target organ segmentation tasks. We explore small-data scenarios in radiology with varying weak annotation styles and densities. Our analysis shows that metric-based meta-learning approaches achieve better segmentation results in tasks with smaller domain shifts in comparison to the meta-training datasets, while some
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25552;&#20986;&#23558;&#22240;&#26524;&#21457;&#29616;&#35270;&#20026;&#39044;&#27979;&#26410;&#35266;&#23519;&#21040;&#32852;&#21512;&#32479;&#35745;&#37327;&#30340;&#20219;&#21153;&#65292;&#36825;&#26679;&#21487;&#20197;&#26356;&#22909;&#22320;&#25512;&#26029;&#26410;&#35266;&#23519;&#21040;&#38598;&#21512;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06894</link><description>&lt;p&gt;
&#23558;&#22240;&#26524;&#21457;&#29616;&#37325;&#26032;&#35299;&#37322;&#20026;&#39044;&#27979;&#26410;&#35266;&#23519;&#21040;&#30340;&#32852;&#21512;&#32479;&#35745;&#37327;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Reinterpreting causal discovery as the task of predicting unobserved joint statistics. (arXiv:2305.06894v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06894
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25552;&#20986;&#23558;&#22240;&#26524;&#21457;&#29616;&#35270;&#20026;&#39044;&#27979;&#26410;&#35266;&#23519;&#21040;&#32852;&#21512;&#32479;&#35745;&#37327;&#30340;&#20219;&#21153;&#65292;&#36825;&#26679;&#21487;&#20197;&#26356;&#22909;&#22320;&#25512;&#26029;&#26410;&#35266;&#23519;&#21040;&#38598;&#21512;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;$X,Y,Z$&#34920;&#31034;&#38543;&#26426;&#21464;&#37327;&#38598;&#65292;&#19981;&#21516;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#21253;&#21547;$P_{X,Y}$&#21644;$P_{Y,Z}$&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#22240;&#26524;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#25512;&#26029;&#8220;&#26410;&#35266;&#23519;&#21040;&#30340;&#32852;&#21512;&#20998;&#24067;&#8221;$P_{X,Y,Z}$&#25110;$P_{X,Z}$&#30340;&#24615;&#36136;&#12290;&#36825;&#20123;&#24615;&#36136;&#21487;&#20197;&#26159;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;&#22914;&#8220;&#25972;&#21512;&#22240;&#26524;&#25512;&#29702;&#8221;&#20013;&#37027;&#26679;&#65289;&#65292;&#20063;&#21487;&#20197;&#26159;&#20851;&#20110;&#20381;&#36182;&#24615;&#30340;&#23450;&#37327;&#35828;&#26126;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#21464;&#37327;&#30340;&#23376;&#38598;&#65292;&#26631;&#31614;&#26159;&#35813;&#23376;&#38598;&#30340;&#26576;&#20123;&#32479;&#35745;&#23646;&#24615;&#12290;&#20849;&#21516;&#35266;&#27979;&#21464;&#37327;&#38598;&#23450;&#20041;&#20102;&#35757;&#32451;&#28857;&#65292;&#32780;&#26410;&#35266;&#23519;&#21040;&#30340;&#38598;&#21512;&#26159;&#21487;&#33021;&#30340;&#27979;&#35797;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#20219;&#21153;&#65292;&#25105;&#20204;&#20174;&#35266;&#23519;&#32467;&#26524;&#20013;&#25512;&#26029;&#20986;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#36825;&#20010;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#24471;&#21040;&#26410;&#35266;&#23519;&#21040;&#38598;&#21512;&#30340;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23450;&#20041;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#31867;&#30340;VC&#32500;&#65292;&#24182;&#20026;&#39044;&#27979;&#25512;&#23548;&#20986;&#27867;&#21270;&#30028;&#38480;&#12290;&#22240;&#27492;&#65292;&#22240;&#26524;&#21457;&#29616;&#21464;&#24471;&#26356;&#21152;&#35878;&#36874;&#21644;&#26131;&#20110;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
If $X,Y,Z$ denote sets of random variables, two different data sources may contain samples from $P_{X,Y}$ and $P_{Y,Z}$, respectively. We argue that causal discovery can help inferring properties of the `unobserved joint distributions' $P_{X,Y,Z}$ or $P_{X,Z}$. The properties may be conditional independences (as in `integrative causal inference') or also quantitative statements about dependences.  More generally, we define a learning scenario where the input is a subset of variables and the label is some statistical property of that subset. Sets of jointly observed variables define the training points, while unobserved sets are possible test points. To solve this learning task, we infer, as an intermediate step, a causal model from the observations that then entails properties of unobserved sets. Accordingly, we can define the VC dimension of a class of causal models and derive generalization bounds for the predictions.  Here, causal discovery becomes more modest and better accessible 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23384;&#22312;&#30340;&#21435;&#21367;&#31215;&#23450;&#20041;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;&#65292;&#23558;&#31515;&#21345;&#20799;&#31215;&#21644;&#24186;&#27169;&#31215;&#30340;&#27010;&#24565;&#24212;&#35813;&#26500;&#25104;&#21435;&#21367;&#31215;&#30340;&#26680;&#24515;&#65292;&#24182;&#23637;&#29616;&#20102;&#22788;&#29702;&#20989;&#25968;&#12289;&#31561;&#21464;&#26144;&#23556;&#12289;&#20851;&#31995;&#21644;&#38543;&#26426;&#26144;&#23556;&#30340;&#30456;&#20284;&#24615;&#21644;&#20851;&#38190;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06886</link><description>&lt;p&gt;
&#23545;&#8220;&#21435;&#21367;&#31215;&#8221;&#23450;&#20041;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Category-theoretical Meta-analysis of Definitions of Disentanglement. (arXiv:2305.06886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23384;&#22312;&#30340;&#21435;&#21367;&#31215;&#23450;&#20041;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;&#65292;&#23558;&#31515;&#21345;&#20799;&#31215;&#21644;&#24186;&#27169;&#31215;&#30340;&#27010;&#24565;&#24212;&#35813;&#26500;&#25104;&#21435;&#21367;&#31215;&#30340;&#26680;&#24515;&#65292;&#24182;&#23637;&#29616;&#20102;&#22788;&#29702;&#20989;&#25968;&#12289;&#31561;&#21464;&#26144;&#23556;&#12289;&#20851;&#31995;&#21644;&#38543;&#26426;&#26144;&#23556;&#30340;&#30456;&#20284;&#24615;&#21644;&#20851;&#38190;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23558;&#25968;&#25454;&#30340;&#21464;&#21270;&#22240;&#32032;&#20998;&#31163;&#26159;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#30740;&#31350;&#20154;&#21592;&#20197;&#21508;&#31181;&#26041;&#24335;&#30740;&#31350;&#23427;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#30340;&#23450;&#20041;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#32463;&#39564;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#20173;&#38656;&#35201;&#26356;&#22810;&#30340;&#29702;&#35770;&#30740;&#31350;&#26469;&#20805;&#20998;&#29702;&#35299;&#21435;&#21367;&#31215;&#30340;&#23450;&#20041;&#23646;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#23450;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23384;&#22312;&#30340;&#21435;&#21367;&#31215;&#23450;&#20041;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;&#65292;&#23558;&#33539;&#30068;&#35770;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#32780;&#20005;&#35880;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#31515;&#21345;&#20799;&#31215;&#21644;&#24186;&#27169;&#31215;&#30340;&#27010;&#24565;&#24212;&#35813;&#26500;&#25104;&#21435;&#21367;&#31215;&#30340;&#26680;&#24515;&#12290;&#26377;&#20102;&#36825;&#20123;&#26680;&#24515;&#27010;&#24565;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22788;&#29702;&#65288;i&#65289;&#20989;&#25968;&#65292;&#65288;ii&#65289;&#31561;&#21464;&#26144;&#23556;&#65292;&#65288;iii&#65289;&#20851;&#31995;&#21644;&#65288;iv&#65289;&#38543;&#26426;&#26144;&#23556;&#30340;&#30456;&#20284;&#24615;&#21644;&#20851;&#38190;&#21306;&#21035;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#33539;&#30068;&#35770;&#20803;&#20998;&#26512;&#28145;&#21270;&#20102;&#25105;&#20204;&#23545;&#21435;&#21367;&#31215;&#21450;&#20854;&#19981;&#21516;&#21046;&#23450;&#30340;&#29702;&#35299;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#19981;&#21516;&#30340;&#23450;&#20041;&#20043;&#38388;&#36827;&#34892;&#23548;&#33322;&#65292;&#24182;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling the factors of variation in data is a fundamental concept in machine learning and has been studied in various ways by different researchers, leading to a multitude of definitions. Despite the numerous empirical studies, more theoretical research is needed to fully understand the defining properties of disentanglement and how different definitions relate to each other. This paper presents a meta-analysis of existing definitions of disentanglement, using category theory as a unifying and rigorous framework. We propose that the concepts of the cartesian and monoidal products should serve as the core of disentanglement. With these core concepts, we show the similarities and crucial differences in dealing with (i) functions, (ii) equivariant maps, (iii) relations, and (iv) stochastic maps. Overall, our meta-analysis deepens our understanding of disentanglement and its various formulations and can help researchers navigate different definitions and choose the most appropriate o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#21152;&#26435;&#25277;&#26679;&#20449;&#24515;&#24207;&#21015;&#65292;&#23545;N&#20010;&#26410;&#30693;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#30340;&#39118;&#38505;&#38480;&#21046;&#36130;&#21153;&#23457;&#35745;&#65288;RLFA&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#24182;&#26410;&#30693;&#20540;&#30340;&#38468;&#21152;&#20449;&#24687;&#25552;&#39640;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06884</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#26367;&#25442;&#21152;&#26435;&#25277;&#26679;&#36827;&#34892;&#39118;&#38505;&#38480;&#21046;&#36130;&#21153;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Risk-limiting Financial Audits via Weighted Sampling without Replacement. (arXiv:2305.06884v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#21152;&#26435;&#25277;&#26679;&#20449;&#24515;&#24207;&#21015;&#65292;&#23545;N&#20010;&#26410;&#30693;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#30340;&#39118;&#38505;&#38480;&#21046;&#36130;&#21153;&#23457;&#35745;&#65288;RLFA&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#24182;&#26410;&#30693;&#20540;&#30340;&#38468;&#21152;&#20449;&#24687;&#25552;&#39640;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#39118;&#38505;&#38480;&#21046;&#36130;&#21153;&#23457;&#35745;&#65288;RLFA&#65289;&#30340;&#27010;&#24565;&#65306;&#22312;&#32473;&#23450;&#35823;&#24046;$\epsilon$&#21644;&#32622;&#20449;&#24230;$1-\delta$&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#21152;&#26435;&#25277;&#26679;&#20449;&#24515;&#24207;&#21015;&#65288;CSs&#65289;&#65292;&#23545;$N$&#20010;&#26410;&#30693;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#37325;&#35201;&#26435;&#37325;&#30340;&#24819;&#27861;&#26500;&#24314;&#27979;&#35797;&#38789;&#65292;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#26500;&#24314;&#20219;&#24847;&#25277;&#26679;&#31574;&#30053;&#30340;CSs&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#36890;&#36807;&#21512;&#24182;&#19982;&#27599;&#20010;&#39033;&#30446;&#20851;&#32852;&#30340;&#26410;&#30693;&#20540;&#30340;&#38468;&#21152;&#20449;&#24687;&#26469;&#25552;&#39640;CSs&#30340;&#36136;&#37327;&#12290;&#24403;&#38468;&#21152;&#20449;&#24687;&#36275;&#22815;&#20855;&#26377;&#39044;&#27979;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;&#23427;&#21487;&#20197;&#30452;&#25509;&#39537;&#21160;&#25277;&#26679;&#12290;&#23545;&#20110;&#31934;&#24230;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#36890;&#36807;&#25511;&#21046;&#21464;&#37327;&#20351;&#29992;&#38468;&#21152;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26500;&#24314;&#26159;&#33258;&#36866;&#24212;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of a risk-limiting financial auditing (RLFA): given $N$ transactions, the goal is to estimate the total misstated monetary fraction~($m^*$) to a given accuracy $\epsilon$, with confidence $1-\delta$. We do this by constructing new confidence sequences (CSs) for the weighted average of $N$ unknown values, based on samples drawn without replacement according to a (randomized) weighted sampling scheme. Using the idea of importance weighting to construct test martingales, we first develop a framework to construct CSs for arbitrary sampling strategies. Next, we develop methods to improve the quality of CSs by incorporating side information about the unknown values associated with each item. We show that when the side information is sufficiently predictive, it can directly drive the sampling. Addressing the case where the accuracy is unknown a priori, we introduce a method that incorporates side information via control variates. Crucially, our construction is adaptive
&lt;/p&gt;</description></item><item><title>&#24314;&#31435;&#20102;&#22522;&#20110;&#24191;&#20041;&#21704;&#23494;&#23572;&#39039;&#23454;&#25968;&#35745;&#31639;&#30340;&#20984;&#22235;&#20803;&#25968;&#20248;&#21270;&#22522;&#26412;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21028;&#21035;&#23450;&#29702;&#21644;&#21028;&#21035;&#26631;&#20934;&#65292;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#24212;&#29992;&#39564;&#35777;&#20102;&#29702;&#35770;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06879</link><description>&lt;p&gt;
&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#30340;&#20984;&#22235;&#20803;&#25968;&#20248;&#21270;&#65306;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convex Quaternion Optimization for Signal Processing: Theory and Applications. (arXiv:2305.06879v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06879
&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20102;&#22522;&#20110;&#24191;&#20041;&#21704;&#23494;&#23572;&#39039;&#23454;&#25968;&#35745;&#31639;&#30340;&#20984;&#22235;&#20803;&#25968;&#20248;&#21270;&#22522;&#26412;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21028;&#21035;&#23450;&#29702;&#21644;&#21028;&#21035;&#26631;&#20934;&#65292;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#24212;&#29992;&#39564;&#35777;&#20102;&#29702;&#35770;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20984;&#20248;&#21270;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#20449;&#21644;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22235;&#20803;&#25968;&#20248;&#21270;&#29702;&#35770;&#30446;&#21069;&#27809;&#26377;&#20687;&#22797;&#25968;&#21644;&#23454;&#25968;&#20248;&#21270;&#29702;&#35770;&#37027;&#26679;&#23436;&#20840;&#21457;&#23637;&#21644;&#31995;&#32479;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#20041;&#21704;&#23494;&#23572;&#39039;&#23454;&#25968;(GHR)&#35745;&#31639;&#65292;&#24314;&#31435;&#20102;&#20984;&#22235;&#20803;&#25968;&#20248;&#21270;&#30340;&#22522;&#26412;&#29702;&#35770;&#65292;&#36825;&#26159;&#19982;&#20256;&#32479;&#30340;&#22797;&#25968;&#21644;&#23454;&#25968;&#20248;&#21270;&#29702;&#35770;&#31526;&#21512;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#20010;&#21028;&#21035;&#20984;&#22235;&#20803;&#25968;&#20989;&#25968;&#30340;&#23450;&#29702;&#65292;&#21644;&#22235;&#20010;&#24378;&#20984;&#22235;&#20803;&#25968;&#20989;&#25968;&#30340;&#21028;&#21035;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#26412;&#23450;&#29702;&#65292;&#29992;&#20110;&#35828;&#26126;&#20984;&#22235;&#20803;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#22235;&#20803;&#25968;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#26469;&#28436;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#20984;&#22235;&#20803;&#25968;&#20248;&#21270;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#20026;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#24320;&#21551;&#20102;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convex optimization methods have been extensively used in the fields of communications and signal processing. However, the theory of quaternion optimization is currently not as fully developed and systematic as that of complex and real optimization. To this end, we establish an essential theory of convex quaternion optimization for signal processing based on the generalized Hamilton-real (GHR) calculus. This is achieved in a way which conforms with traditional complex and real optimization theory. For rigorous, We present five discriminant theorems for convex quaternion functions, and four discriminant criteria for strongly convex quaternion functions. Furthermore, we provide a fundamental theorem for the optimality of convex quaternion optimization problems, and demonstrate its utility through three applications in quaternion signal processing. These results provide a solid theoretical foundation for convex quaternion optimization and open avenues for further developments in signal pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SocFedCS&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#23458;&#25143;&#31471;&#36873;&#25321;&#65292;&#22312;&#31227;&#21160;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06865</link><description>&lt;p&gt;
&#31227;&#21160;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#30340;&#22810;&#23618;&#23458;&#25143;&#31471;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-Tier Client Selection for Mobile Federated Learning Networks. (arXiv:2305.06865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SocFedCS&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#23458;&#25143;&#31471;&#36873;&#25321;&#65292;&#22312;&#31227;&#21160;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20197;&#20998;&#24067;&#26041;&#24335;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#24050;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#65288;MFLNs&#65289;&#20013;&#20248;&#21270;FL&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#8220;SocFedCS&#8221;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#65292;&#20197;&#26368;&#23567;&#21270;&#25104;&#26412;&#24182;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;FL&#27169;&#22411;&#12290;SocFedCS&#36890;&#36807;&#20351;&#25968;&#25454;&#25152;&#26377;&#32773;&#22312;&#20182;&#20204;&#30340;&#20449;&#20219;&#26412;&#22320;&#32593;&#32476;&#20013;&#20256;&#25773;FL&#20219;&#21153;&#20449;&#24687;&#26469;&#20016;&#23500;&#20505;&#36873;&#30340;FL&#23458;&#25143;&#31471;&#27744;&#65292;&#21363;&#20351;&#35774;&#22791;&#30456;&#20114;&#31227;&#21160;&#65292;&#20063;&#33021;&#22815;&#20445;&#25345;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), which addresses data privacy issues by training models on resource-constrained mobile devices in a distributed manner, has attracted significant research attention. However, the problem of optimizing FL client selection in mobile federated learning networks (MFLNs), where devices move in and out of each others' coverage and no FL server knows all the data owners, remains open. To bridge this gap, we propose a first-of-its-kind \underline{Soc}ially-aware \underline{Fed}erated \underline{C}lient \underline{S}election (SocFedCS) approach to minimize costs and train high-quality FL models. SocFedCS enriches the candidate FL client pool by enabling data owners to propagate FL task information through their local networks of trust, even as devices are moving into and out of each others' coverage. Based on Lyapunov optimization, we first transform this time-coupled problem into a step-by-step optimization problem. Then, we design a method based on alternating minimiza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#21487;&#35270;&#21270;&#20219;&#20309;&#31070;&#32463;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20013;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#38170;&#23450;&#26041;&#21521;&#65292;&#21487;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#21644;&#21407;&#22987;&#36755;&#20837;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#12290;&#26412;&#25991;&#35828;&#26126;&#20102;&#22312;&#19968;&#20010;&#22522;&#20110;&#35282;&#24230;&#20449;&#24687;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20943;&#23569;&#20449;&#24687;&#20002;&#22833;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06862</link><description>&lt;p&gt;
&#22522;&#20110;&#35282;&#24230;&#20449;&#24687;&#30340;&#31070;&#32463;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#21487;&#35270;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Visualizing Embedding Spaces of Neural Survival Analysis Models Based on Angular Information. (arXiv:2305.06862v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#21487;&#35270;&#21270;&#20219;&#20309;&#31070;&#32463;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20013;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#38170;&#23450;&#26041;&#21521;&#65292;&#21487;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#21644;&#21407;&#22987;&#36755;&#20837;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#12290;&#26412;&#25991;&#35828;&#26126;&#20102;&#22312;&#19968;&#20010;&#22522;&#20110;&#35282;&#24230;&#20449;&#24687;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20943;&#23569;&#20449;&#24687;&#20002;&#22833;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#35270;&#21270;&#31070;&#32463;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20219;&#20309;&#20013;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25152;&#35859;&#38170;&#23450;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#32858;&#31867;&#25110;&#29992;&#25143;&#25552;&#20379;&#30340;&#8220;&#27010;&#24565;&#8221;&#26469;&#23450;&#20041;&#21407;&#22987;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#26469;&#33258;&#22899;&#24615;&#24739;&#32773;&#30340;&#29305;&#24449;&#21521;&#37327;&#21487;&#20197;&#32534;&#30721;&#8220;&#22899;&#24615;&#8221;&#27010;&#24565;&#65289;&#26469;&#20272;&#35745;&#36825;&#20123;&#38170;&#23450;&#26041;&#21521;&#12290;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#20197;&#26174;&#31034;&#38170;&#23450;&#26041;&#21521;&#19982;&#21407;&#22987;&#20020;&#24202;&#29305;&#24449;&#21644;&#29983;&#23384;&#26102;&#38388;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21487;&#35270;&#21270;&#24605;&#24819;&#22914;&#20309;&#25193;&#23637;&#21040;&#22788;&#29702;&#22270;&#20687;&#31561;&#21407;&#22987;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#22522;&#20110;&#26597;&#30475;&#23884;&#20837;&#31354;&#38388;&#20013;&#21521;&#37327;&#20043;&#38388;&#30340;&#35282;&#24230;&#24314;&#31435;&#30340;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;&#24133;&#24230;&#20449;&#24687;&#32780;&#21487;&#33021;&#23384;&#22312;&#8220;&#20449;&#24687;&#20002;&#22833;&#8221;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#22914;&#20309;&#20943;&#23569;&#36825;&#31181;&#20449;&#24687;&#20002;&#22833;&#23548;&#33268;&#30340;&#8220;&#32858;&#38598;&#8221;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a general framework for visualizing any intermediate embedding representation used by any neural survival analysis model. Our framework is based on so-called anchor directions in an embedding space. We show how to estimate these anchor directions using clustering or, alternatively, using user-supplied "concepts" defined by collections of raw inputs (e.g., feature vectors all from female patients could encode the concept "female"). For tabular data, we present visualization strategies that reveal how anchor directions relate to raw clinical features and to survival time distributions. We then show how these visualization ideas extend to handling raw inputs that are images. Our framework is built on looking at angles between vectors in an embedding space, where there could be "information loss" by ignoring magnitude information. We show how this loss results in a "clumping" artifact that appears in our visualizations, and how to reduce this information loss in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#26032;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#65292;&#21363;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#25351;&#20986;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25506;&#32034;&#30340;&#23454;&#36136;&#26159;&#35745;&#31639;&#24403;&#21069;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.06851</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30340;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Algorithms Implicitly Optimize by Continuation. (arXiv:2305.06851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#26032;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#65292;&#21363;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#26041;&#24335;&#38544;&#24335;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#25351;&#20986;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25506;&#32034;&#30340;&#23454;&#36136;&#26159;&#35745;&#31639;&#24403;&#21069;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#36890;&#24120;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#20248;&#21270;&#31574;&#30053;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#24314;&#31435;&#22312;&#20248;&#21270;&#36830;&#32493;&#26694;&#26550;&#19979;&#12290;&#21518;&#32773;&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#38750;&#20984;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20197;&#36830;&#32493;&#30340;&#26367;&#20195;&#30446;&#26631;&#20989;&#25968;&#24207;&#21015;&#20026;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#21270;&#20223;&#23556;&#39640;&#26031;&#31574;&#30053;&#24182;&#25191;&#34892;&#29109;&#27491;&#21017;&#21270;&#21487;&#20197;&#35299;&#37322;&#20026;&#36890;&#36807;&#36830;&#32493;&#38544;&#24335;&#22320;&#20248;&#21270;&#30830;&#23450;&#24615;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#30340;&#25506;&#32034;&#21253;&#25324;&#35745;&#31639;&#24403;&#21069;&#30340;&#31574;&#30053;&#25910;&#30410;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#31574;&#30053;&#30340;&#26041;&#24046;&#24212;&#35813;&#26159;&#21382;&#21490;&#20381;&#36182;&#24615;&#20989;&#25968;&#65292;&#20197;&#36991;&#20813;&#23616;&#37096;&#26368;&#20540;&#32780;&#19981;&#26159;&#20165;&#20165;&#26368;&#22823;&#21270;&#25919;&#31574;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#32452;&#20214;&#34701;&#20837;&#39044;&#27979;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22330;&#26469;&#34920;&#31034;&#36741;&#21161;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22312;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#26102;&#23384;&#22312;&#30340;&#19968;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06827</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#31070;&#32463;&#22330;&#22312;&#26102;&#31354;&#39044;&#27979;&#20013;&#23558;&#26102;&#38388;&#34701;&#20837;&#21040;&#36890;&#29992;&#26041;&#27861;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Generic Approach to Integrating Time into Spatial-Temporal Forecasting via Conditional Neural Fields. (arXiv:2305.06827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#32452;&#20214;&#34701;&#20837;&#39044;&#27979;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22330;&#26469;&#34920;&#31034;&#36741;&#21161;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22312;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#26102;&#23384;&#22312;&#30340;&#19968;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#24847;&#35782;&#26159;&#33258;&#20027;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#20363;&#22914;&#33258;&#20027;&#39550;&#39542;&#32593;&#32476;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#25512;&#26029;&#29615;&#22659;&#30340;&#26410;&#26469;&#29366;&#24577;&#20197;&#21450;&#20854;&#38543;&#26102;&#38388;&#25512;&#31227;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#22823;&#37327;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#31639;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20197;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#32479;&#35745;&#26041;&#27861;&#19978;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#20294;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#34920;&#31034;&#23395;&#33410;&#24615;&#27169;&#24335;&#30340;&#20840;&#23616;&#20449;&#24687;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#32452;&#20214;&#25972;&#21512;&#21040;&#39044;&#27979;&#27169;&#22411;&#20013;&#20197;&#25552;&#39640;&#20854;&#31934;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#32452;&#20214;&#34701;&#20837;&#39044;&#27979;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22330;&#26469;&#34920;&#31034;&#36741;&#21161;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-awareness is the key capability of autonomous systems, e.g., autonomous driving network, which relies on highly efficient time series forecasting algorithm to enable the system to reason about the future state of the environment, as well as its effect on the system behavior as time progresses. Recently, a large number of forecasting algorithms using either convolutional neural networks or graph neural networks have been developed to exploit the complex temporal and spatial dependencies present in the time series. While these solutions have shown significant advantages over statistical approaches, one open question is to effectively incorporate the global information which represents the seasonality patterns via the time component of time series into the forecasting models to improve their accuracy. This paper presents a general approach to integrating the time component into forecasting models. The main idea is to employ conditional neural fields to represent the auxiliary feature
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#21450;&#20854;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.06807</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Information Design in Multi-Agent Reinforcement Learning. (arXiv:2305.06807v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#21450;&#20854;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#20223;&#20154;&#31867;&#21644;&#21160;&#29289;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#23454;&#38469;&#29615;&#22659;&#20013;&#23384;&#22312;&#20854;&#20182;&#26377;&#33258;&#24049;&#30446;&#26631;&#30340;&#26234;&#33021;&#20307;&#65292;&#23427;&#20204;&#20250;&#36866;&#24212;&#22320;&#19982;&#33258;&#24049;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#25104;&#21151;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#38656;&#35201;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#20197;&#20351;&#23427;&#20204;&#30340;&#34892;&#20026;&#26356;&#26377;&#30410;&#12290;&#20449;&#24687;&#35774;&#35745;&#26159;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#19968;&#32452;RL&#20195;&#29702;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) mimics how humans and animals interact with the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receivers are willing to respect. We formulate the Markov signaling game, and develop the notions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#25919;&#31574;&#32454;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#29702;&#39564;&#35777;&#20854;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#30028;&#38480;&#12289;&#27867;&#21270;&#35823;&#24046;&#21644;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06796</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#32454;&#21270;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Theoretical Understanding of Data-Driven Policy Refinement. (arXiv:2305.06796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#25919;&#31574;&#32454;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#29702;&#39564;&#35777;&#20854;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#30028;&#38480;&#12289;&#27867;&#21270;&#35823;&#24046;&#21644;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#32454;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#26469;&#22686;&#24378;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#21644;&#20248;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#36825;&#20010;&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#32454;&#21270;&#27010;&#24565;&#30340;&#25968;&#23398;&#34920;&#36848;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25968;&#25454;&#39537;&#21160;&#39564;&#35777;&#20013;&#28014;&#29616;&#30340;&#21453;&#20363;&#23398;&#20064;&#65292;&#31995;&#32479;&#22320;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#29702;&#65292;&#38416;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#30028;&#38480;&#12289;&#27867;&#21270;&#35823;&#24046;&#21644;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24377;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#19981;&#20165;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#36825;&#20010;&#26041;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach for data-driven policy refinement in reinforcement learning, specifically designed for safety-critical applications. Our methodology leverages the strengths of data-driven optimization and reinforcement learning to enhance policy safety and optimality through iterative refinement. Our principal contribution lies in the mathematical formulation of this data-driven policy refinement concept. This framework systematically improves reinforcement learning policies by learning from counterexamples surfaced during data-driven verification. Furthermore, we present a series of theorems elucidating key theoretical properties of our approach, including convergence, robustness bounds, generalization error, and resilience to model mismatch. These results not only validate the effectiveness of our methodology but also contribute to a deeper understanding of its behavior in different environments and scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;NNCI&#65292;&#29992;&#20110;&#23558;&#26368;&#36817;&#37051;&#23621;&#20449;&#24687;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06789</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#38598;&#25104;&#26368;&#36817;&#37051;&#23621;&#20197;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Integrating nearest neighbors on neural network models for treatment effect estimation. (arXiv:2305.06789v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;NNCI&#65292;&#29992;&#20110;&#23558;&#26368;&#36817;&#37051;&#23621;&#20449;&#24687;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#23545;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#26469;&#35828;&#20855;&#26377;&#39640;&#24230;&#37325;&#35201;&#24615;&#12290;&#35266;&#23519;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#20351;&#23427;&#20204;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#23384;&#22312;&#20559;&#24046;&#21644;&#20854;&#20182;&#24369;&#28857;&#65292;&#23548;&#33268;&#22914;&#26524;&#19981;&#27491;&#30830;&#22788;&#29702;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#20250;&#19981;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#37117;&#19987;&#27880;&#20110;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20197;&#36798;&#21040;&#26356;&#31934;&#30830;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#36817;&#37051;&#23621;&#20449;&#24687;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#65288;NNCI&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26377;&#20215;&#20540;&#30340;&#26368;&#36817;&#37051;&#23621;&#20449;&#24687;&#38598;&#25104;&#21040;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#20013;&#65292;&#20197;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#25552;&#20986;&#30340;NNCI&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#19968;&#20123;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation is of high-importance for both researchers and practitioners across many scientific and industrial domains. The abundance of observational data makes them increasingly used by researchers for the estimation of causal effects. However, these data suffer from biases, from several weaknesses, leading to inaccurate causal effect estimations, if not handled properly. Therefore, several machine learning techniques have been proposed, most of them focusing on leveraging the predictive power of neural network models to attain more precise estimation of causal effects. In this work, we propose a new methodology, named Nearest Neighboring Information for Causal Inference (NNCI), for integrating valuable nearest neighboring information on neural network-based models for estimating treatment effects. The proposed NNCI methodology is applied to some of the most well established neural network-based models for treatment effect estimation with the use of observational data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.06784</link><description>&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#28040;&#36153;&#32773;&#30340;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning. (arXiv:2305.06784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;AFL&#65289;&#22240;&#36890;&#36807;&#32463;&#27982;&#25163;&#27573;&#28608;&#21169;&#25968;&#25454;&#25317;&#26377;&#32773;&#21152;&#20837;FL&#32780;&#21463;&#21040;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#29616;&#26377;&#24037;&#20316;&#20551;&#35774;&#22312;AFL&#24066;&#22330;&#19978;&#20165;&#23384;&#22312;&#19968;&#20010;&#25968;&#25454;&#28040;&#36153;&#32773;&#21644;&#22810;&#20010;&#25968;&#25454;&#25317;&#26377;&#32773;&#65288;&#21363;&#22404;&#26029;&#24066;&#22330;&#65289;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#25317;&#26377;&#32773;&#31454;&#26631;&#21152;&#20837;&#25968;&#25454;&#28040;&#36153;&#32773;&#36827;&#34892;FL&#12290;&#20294;&#26159;&#65292;&#22312;&#23454;&#38469;&#30340;AFL&#24066;&#22330;&#20013;&#65292;&#22810;&#20010;&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#33021;&#20250;&#31454;&#20105;&#20197;&#21560;&#24341;&#25968;&#25454;&#25317;&#26377;&#32773;&#21152;&#20837;&#20182;&#20204;&#21508;&#33258;&#30340;FL&#20219;&#21153;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#22815;&#23481;&#32435;&#19981;&#21516;&#24066;&#22330;&#21160;&#24577;&#30340;&#21508;&#31181;&#33719;&#32988;&#20989;&#25968;&#30340;&#25928;&#29992;&#20272;&#35745;&#33021;&#21147;&#12290;&#22522;&#20110;&#20845;&#20010;&#24120;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auction-based Federated Learning (AFL) has attracted extensive research interest due to its ability to motivate data owners to join FL through economic means. Existing works assume that only one data consumer and multiple data owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data owners bid to join the data consumer for FL. However, this assumption is not realistic in practical AFL marketplaces in which multiple data consumers can compete to attract data owners to join their respective FL tasks. In this paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing bidding strategy for data consumers in federated learning (Fed-Bidder). It enables multiple FL data consumers to compete for data owners via AFL effectively and efficiently by providing with utility estimation capabilities which can accommodate diverse forms of winning functions, each reflecting different market dynamics. Extensive experiments based on six commonly adopted benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#37319;&#38598;&#21644;&#21453;&#23556;&#26657;&#27491;&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3DMPCs&#65292;&#29992;&#20110;&#39640;&#25928;&#26893;&#29289;&#34920;&#22411;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.06777</link><description>&lt;p&gt;
&#37319;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#37319;&#38598;&#21644;&#22522;&#20110;NeREF&#30340;&#21453;&#23556;&#26657;&#27491;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3DMPCs&#65292;&#20419;&#36827;&#39640;&#25928;&#26893;&#29289;&#34920;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generating high-quality 3DMPCs by adaptive data acquisition and NeREF-based reflectance correction to facilitate efficient plant phenotyping. (arXiv:2305.06777v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#37319;&#38598;&#21644;&#21453;&#23556;&#26657;&#27491;&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3DMPCs&#65292;&#29992;&#20110;&#39640;&#25928;&#26893;&#29289;&#34920;&#22411;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#21644;&#22810;&#20809;&#35889;&#25968;&#25454;&#23545;&#26893;&#29289;&#34920;&#22411;&#29305;&#24449;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#35780;&#20272;&#21487;&#20197;&#21152;&#28145;&#32946;&#31181;&#32773;&#23545;&#26893;&#29289;&#29983;&#38271;&#30340;&#29702;&#35299;&#65292;&#24182;&#20351;&#20182;&#20204;&#33021;&#22815;&#20570;&#20986;&#30693;&#24773;&#31649;&#29702;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#20809;&#29031;&#26465;&#20214;&#19979;&#65292;&#20027;&#35266;&#35270;&#35282;&#36873;&#25321;&#21644;&#22797;&#26434;&#30340;&#20809;&#29031;&#25928;&#24212;&#20250;&#38477;&#20302;&#25968;&#25454;&#36136;&#37327;&#65292;&#22686;&#21152;&#35299;&#20915;&#34920;&#22411;&#21442;&#25968;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#37319;&#38598;&#21644;&#21453;&#23556;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#20998;&#21035;&#29983;&#25104;&#26893;&#29289;&#30340;&#39640;&#36136;&#37327;3D&#22810;&#20809;&#35889;&#28857;&#20113;&#65288;3DMPCs&#65289;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#22411;UGV&#24179;&#21488;&#21644;&#22810;&#20256;&#24863;&#22120;&#35013;&#22791;&#30340;&#26426;&#22120;&#20154;&#33218;&#30340;&#39640;&#25928;&#30340;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#35282;&#65288;NBV&#65289;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#21442;&#32771;&#22330;&#65288;NeREF&#65289;&#26469;&#39044;&#27979;&#21442;&#32771;&#30340;&#25968;&#23383;&#65288;DN&#65289;&#26469;&#28040;&#38500;&#20809;&#29031;&#25928;&#24212;&#12290;&#25105;&#20204;&#22312;6&#20010;&#32043;&#33487;&#21644;6&#20010;&#30058;&#33540;&#26893;&#26666;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#36873;&#25321;&#20102;2&#29255;&#21487;&#35265;&#21494;&#21644;4&#20010;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-destructive assessments of plant phenotypic traits using high-quality three-dimensional (3D) and multispectral data can deepen breeders' understanding of plant growth and allow them to make informed managerial decisions. However, subjective viewpoint selection and complex illumination effects under natural light conditions decrease the data quality and increase the difficulty of resolving phenotypic parameters. We proposed methods for adaptive data acquisition and reflectance correction respectively, to generate high-quality 3D multispectral point clouds (3DMPCs) of plants. In the first stage, we proposed an efficient next-best-view (NBV) planning method based on a novel UGV platform with a multi-sensor-equipped robotic arm. In the second stage, we eliminated the illumination effects by using the neural reference field (NeREF) to predict the digital number (DN) of the reference. We tested them on 6 perilla and 6 tomato plants, and selected 2 visible leaves and 4 regions of interest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;K&#22343;&#20540;&#32858;&#31867;&#12289;OPTICS&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#32858;&#31867;&#31639;&#27861;&#23545;&#25391;&#21160;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#21457;&#29616;&#24179;&#22343;&#20540;&#21644;&#26041;&#24046;&#31867;&#29305;&#24449;&#29305;&#21035;&#37325;&#35201;&#65292;PCA&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#65292;GMM&#26159;&#26368;&#20248;&#31639;&#27861;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2305.06753</link><description>&lt;p&gt;
&#25391;&#21160;&#25968;&#25454;&#38598;&#32479;&#35745;&#29305;&#24449;&#30340;&#32858;&#31867;&#31639;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Clustering Algorithms for Statistical Features of Vibration Data Sets. (arXiv:2305.06753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;K&#22343;&#20540;&#32858;&#31867;&#12289;OPTICS&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#32858;&#31867;&#31639;&#27861;&#23545;&#25391;&#21160;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#21457;&#29616;&#24179;&#22343;&#20540;&#21644;&#26041;&#24046;&#31867;&#29305;&#24449;&#29305;&#21035;&#37325;&#35201;&#65292;PCA&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#65292;GMM&#26159;&#26368;&#20248;&#31639;&#27861;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25391;&#21160;&#30340;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#30001;&#20110;&#25429;&#33719;&#23485;&#39057;&#33539;&#22260;&#20869;&#30340;&#21160;&#24577;&#29305;&#24449;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#19981;&#21516;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#25391;&#21160;&#25968;&#25454;&#20013;&#32858;&#31867;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#65292;&#25152;&#24471;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#21482;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#20110;&#25391;&#21160;&#25968;&#25454;&#38598;&#26102;&#38388;&#21644;&#39057;&#22495;&#30340;&#32479;&#35745;&#29305;&#24449;&#37319;&#29992;&#20102;K&#22343;&#20540;&#32858;&#31867;&#12289;OPTICS&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#32858;&#31867;&#65288;GMM&#65289;&#36827;&#34892;&#24191;&#27867;&#30340;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29305;&#24449;&#32452;&#21512;&#12289;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36827;&#34892;&#30340;&#29305;&#24449;&#36873;&#25321;&#20197;&#21450;&#25351;&#23450;&#25968;&#37327;&#30340;&#32858;&#31867;&#23545;&#32858;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36890;&#36807;&#32593;&#26684;&#25628;&#32034;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22343;&#20540;&#65288;&#24179;&#22343;&#20540;&#65292;&#20013;&#20301;&#25968;&#65289;&#21644;&#22522;&#20110;&#26041;&#24046;&#65288;&#26631;&#20934;&#24046;&#65289;&#30340;&#29305;&#24449;&#23545;&#20110;&#32858;&#31867;&#25391;&#21160;&#25968;&#25454;&#38750;&#24120;&#20851;&#38190;&#65292;&#32780;PCA&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;GMM&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;K&#22343;&#20540;&#21644;OPTICS&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vibration-based condition monitoring systems are receiving increasing attention due to their ability to accurately identify different conditions by capturing dynamic features over a broad frequency range. However, there is little research on clustering approaches in vibration data and the resulting solutions are often optimized for a single data set. In this work, we present an extensive comparison of the clustering algorithms K-means clustering, OPTICS, and Gaussian mixture model clustering (GMM) applied to statistical features extracted from the time and frequency domains of vibration data sets. Furthermore, we investigate the influence of feature combinations, feature selection using principal component analysis (PCA), and the specified number of clusters on the performance of the clustering algorithms. We conducted this comparison in terms of a grid search using three different benchmark data sets. Our work showed that averaging (Mean, Median) and variance-based features (Standard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;(RBM)&#30340;&#29983;&#25104;&#21160;&#21147;&#23398;&#65292;&#21457;&#29616;&#20174;&#23884;&#21512;&#24577;&#24320;&#22987;&#33258;&#19978;&#32780;&#19979;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#21407;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06745</link><description>&lt;p&gt;
&#30740;&#31350;&#22522;&#20110;&#33021;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Investigating the generative dynamics of energy-based neural networks. (arXiv:2305.06745v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;(RBM)&#30340;&#29983;&#25104;&#21160;&#21147;&#23398;&#65292;&#21457;&#29616;&#20174;&#23884;&#21512;&#24577;&#24320;&#22987;&#33258;&#19978;&#32780;&#19979;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#21407;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#20854;&#25152;&#35757;&#32451;&#30340;&#20998;&#24067;&#30340;&#32479;&#35745;&#23646;&#24615;&#29983;&#25104;&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#20010;&#29305;&#24615;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#29616;&#20195;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20551;&#35828;&#65292;&#21363;&#33258;&#21457;&#30340;&#33041;&#27963;&#21160;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#29983;&#25104;&#22788;&#29702;&#30340;&#25903;&#25345;&#12290;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30740;&#31350;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;(RBMs)&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#23427;&#30340;&#29983;&#25104;&#21160;&#21147;&#23398;&#65292;&#34920;&#24449;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#25277;&#26679;&#36807;&#31243;&#20013;&#35775;&#38382;&#30340;&#29366;&#24577;&#25968;&#37327;&#65292;&#24182;&#25506;&#31350;&#20102;&#20174;&#26377;&#20559;&#21521;&#38544;&#23618;&#29366;&#24577;&#24320;&#22987;&#20135;&#29983;&#29983;&#25104;&#36807;&#31243;&#26159;&#21542;&#33021;&#22686;&#21152;&#35775;&#38382;&#21560;&#24341;&#23376;&#30340;&#24322;&#36136;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#22312;&#32463;&#20856;&#25163;&#20889;&#25968;&#23383;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;RBMs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20174;&#23884;&#21512;&#24577;&#24320;&#22987;&#33258;&#19978;&#32780;&#19979;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#21407;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural networks can produce data samples according to the statistical properties of their training distribution. This feature can be used to test modern computational neuroscience hypotheses suggesting that spontaneous brain activity is partially supported by top-down generative processing. A widely studied class of generative models is that of Restricted Boltzmann Machines (RBMs), which can be used as building blocks for unsupervised deep learning architectures. In this work, we systematically explore the generative dynamics of RBMs, characterizing the number of states visited during top-down sampling and investigating whether the heterogeneity of visited attractors could be increased by starting the generation process from biased hidden states. By considering an RBM trained on a classic dataset of handwritten digits, we show that the capacity to produce diverse data prototypes can be increased by initiating top-down sampling from chimera states, which encode high-level vis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.06743</link><description>&lt;p&gt;
&#38024;&#23545;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#38544;&#24335;&#33539;&#25968;&#39044;&#27979;&#22120;&#30340;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits. (arXiv:2305.06743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#38544;&#24335;&#33539;&#25968;&#39044;&#27979;&#22120;&#65288;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65292;&#20197;Tsallis&#29109;&#20316;&#20026;prox&#20989;&#25968;&#65289;&#26159;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65288;MAB&#65289;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#22797;&#26434;&#24615;&#32467;&#26524;&#37117;&#20381;&#36182;&#20110;&#26377;&#30028;&#22870;&#21169;&#25110;&#20854;&#20182;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#26368;&#36817;&#26377;&#20851;&#26368;&#20339;&#20108;&#32773;&#32467;&#21512;&#31639;&#27861;&#30340;&#30740;&#31350;&#24050;&#32463;&#38024;&#23545;&#23545;&#25163;&#24615;&#21644;&#38543;&#26426;&#37325;&#23614;MAB&#35774;&#32622;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#20294;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#25552;&#20986;&#20102;&#24102;&#21098;&#36753;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#22312;&#22870;&#21169;&#20998;&#24067;&#19978;&#25552;&#20986;&#28176;&#36827;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#26368;&#22909;&#30340;&#20108;&#32773;&#32467;&#21512;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicitly Normalized Forecaster (online mirror descent with Tsallis entropy as prox-function) is known to be an optimal algorithm for adversarial multi-armed problems (MAB). However, most of the complexity results rely on bounded rewards or other restrictive assumptions. Recently closely related best-of-both-worlds algorithm were proposed for both adversarial and stochastic heavy-tailed MAB settings. This algorithm is known to be optimal in both settings, but fails to exploit data fully. In this paper, we propose Implicitly Normalized Forecaster with clipping for MAB problems with heavy-tailed distribution on rewards. We derive convergence results under mild assumptions on rewards distribution and show that the proposed method is optimal for both linear and non-linear heavy-tailed stochastic MAB problems. Also we show that algorithm usually performs better compared to best-of-two-worlds algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06741</link><description>&lt;p&gt;
IVP-VAE: &#21033;&#29992;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#23545;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;ODE&#21644;&#31070;&#32463;&#27969;&#37327;&#65289;&#22312;&#20998;&#26512;&#30005;&#23376;&#30149;&#21382;&#20013;&#24120;&#35265;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#36890;&#36807;&#21021;&#20540;&#38382;&#39064;&#65288;IVP&#65289;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#22788;&#29702;&#12290; &#39034;&#24207;&#27714;&#35299;IVP&#20351;&#24471;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#19981;&#22815;&#39640;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#20351;&#29992;&#36830;&#32493;&#36807;&#31243;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#29366;&#24577;&#28436;&#21464;&#21487;&#20197;&#36890;&#36807;IVP&#30452;&#25509;&#36817;&#20284;&#12290; &#36825;&#28040;&#38500;&#20102;&#36882;&#24402;&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#24182;&#20801;&#35768;&#22810;&#20010;&#29366;&#24577;&#24182;&#34892;&#28436;&#21464;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#20854;&#21487;&#36870;&#24615;&#30340;IVP&#27714;&#35299;&#22120;&#34701;&#21512;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#36825;&#23548;&#33268;&#21442;&#25968;&#26356;&#23569;&#65292;&#25910;&#25947;&#26356;&#24555;&#12290; &#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#33719;&#24471;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#38024;&#23545;MRI&#20013;&#30340;&#36816;&#21160;&#38382;&#39064;&#65292;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23545;&#36816;&#21160;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#20849;&#21516;&#28857;&#65292;&#22312;&#26410;&#26469;&#30340;&#26041;&#21521;&#19978;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.06739</link><description>&lt;p&gt;
MRI&#20013;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#22238;&#39038;&#24615;&#36816;&#21160;&#26657;&#27491;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review. (arXiv:2305.06739v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06739
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#38024;&#23545;MRI&#20013;&#30340;&#36816;&#21160;&#38382;&#39064;&#65292;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23545;&#36816;&#21160;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#20849;&#21516;&#28857;&#65292;&#22312;&#26410;&#26469;&#30340;&#26041;&#21521;&#19978;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#26159;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#30001;&#20110;MR&#20449;&#21495;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#33719;&#21462;&#65292;&#20219;&#20309;&#25104;&#20687;&#29289;&#20307;&#30340;&#36816;&#21160;&#37117;&#20250;&#23548;&#33268;&#22797;&#26434;&#30340;&#20266;&#24433;&#65292;&#27492;&#22806;&#36824;&#20250;&#20135;&#29983;&#20854;&#20182;MR&#25104;&#20687;&#20266;&#24433;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#32463;&#24120;&#34987;&#25552;&#20986;&#29992;&#20110;&#37325;&#24314;&#36807;&#31243;&#30340;&#22810;&#20010;&#38454;&#27573;&#20013;&#30340;&#36816;&#21160;&#26657;&#27491;&#12290;MRI&#37319;&#38598;&#24207;&#21015;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#23398;&#21644;&#30149;&#29702;&#23398;&#20197;&#21450;&#36816;&#21160;&#27169;&#24335;&#65288;&#21018;&#24615;vs&#21487;&#21464;&#24418;&#21644;&#38543;&#26426;vs&#35268;&#24459;&#24615;&#65289;&#20351;&#24471;&#20840;&#38754;&#24615;&#35299;&#20915;&#26041;&#26696;&#19981;&#22826;&#21487;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#30340;&#24605;&#24819;&#20256;&#36882;&#65292;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#32454;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;MRI&#36816;&#21160;&#26657;&#27491;&#26041;&#27861;&#27010;&#36848;&#21450;&#20854;&#24120;&#35265;&#25361;&#25112;&#21644;&#28508;&#21147;&#12290;&#35813;&#32508;&#36848;&#35782;&#21035;&#20102;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#30340;&#25968;&#25454;&#20351;&#29992;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#35780;&#20272;&#31574;&#30053;&#30340;&#24046;&#24322;&#21644;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#35752;&#35770;&#20102;&#22823;&#36235;&#21183;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#26088;&#22312;&#22686;&#24378;MRI&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#22238;&#39038;&#24615;&#36816;&#21160;&#26657;&#27491;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion represents one of the major challenges in magnetic resonance imaging (MRI). Since the MR signal is acquired in frequency space, any motion of the imaged object leads to complex artefacts in the reconstructed image in addition to other MR imaging artefacts. Deep learning has been frequently proposed for motion correction at several stages of the reconstruction process. The wide range of MR acquisition sequences, anatomies and pathologies of interest, and motion patterns (rigid vs. deformable and random vs. regular) makes a comprehensive solution unlikely. To facilitate the transfer of ideas between different applications, this review provides a detailed overview of proposed methods for learning-based motion correction in MRI together with their common challenges and potentials. This review identifies differences and synergies in underlying data usage, architectures and evaluation strategies. We critically discuss general trends and outline future directions, with the aim to enhan
&lt;/p&gt;</description></item><item><title>NUBO&#26159;&#19968;&#20010;&#36879;&#26126;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#65292;&#23427;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#20570;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#33719;&#21462;&#20989;&#25968;&#26469;&#25351;&#23548;&#36873;&#25321;&#20505;&#36873;&#28857;&#65292;&#19987;&#27880;&#20110;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.06709</link><description>&lt;p&gt;
NUBO&#65306;&#19968;&#20010;&#36879;&#26126;&#30340; Python &#21253;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
NUBO: A Transparent Python Package for Bayesian Optimisation. (arXiv:2305.06709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06709
&lt;/p&gt;
&lt;p&gt;
NUBO&#26159;&#19968;&#20010;&#36879;&#26126;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#65292;&#23427;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#20570;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#33719;&#21462;&#20989;&#25968;&#26469;&#25351;&#23548;&#36873;&#25321;&#20505;&#36873;&#28857;&#65292;&#19987;&#27880;&#20110;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NUBO&#65288;Newcastle University Bayesian Optimisation&#65289;&#26159;&#19968;&#20010;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#65292;&#27604;&#22914;&#29289;&#29702;&#23454;&#39564;&#21644;&#35745;&#31639;&#26426;&#27169;&#25311;&#22120;&#12290;&#23427;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#20570;&#20195;&#29702;&#27169;&#22411;&#12289;&#24182;&#36890;&#36807;&#33719;&#21462;&#20989;&#25968;&#26469;&#36873;&#25321;&#29992;&#20110;&#20840;&#23616;&#26368;&#20248;&#21270;&#30340;&#20505;&#36873;&#28857;&#12290;NUBO&#19987;&#27880;&#20110;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20307;&#39564;&#65292;&#20197;&#20415;&#35753;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26356;&#23481;&#26131;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
NUBO, short for Newcastle University Bayesian Optimisation, is a Bayesian optimisation framework for the optimisation of expensive-to-evaluate black-box functions, such as physical experiments and computer simulators. Bayesian optimisation is a cost-efficient optimisation strategy that uses surrogate modelling via Gaussian processes to represent an objective function and acquisition functions to guide the selection of candidate points to approximate the global optimum of the objective function. NUBO itself focuses on transparency and user experience to make Bayesian optimisation easily accessible to researchers from all disciplines. Clean and understandable code, precise references, and thorough documentation ensure transparency, while user experience is ensured by a modular and flexible design, easy-to-write syntax, and careful selection of Bayesian optimisation algorithms. NUBO allows users to tailor Bayesian optimisation to their specific problem by writing the optimisation loop the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#39044;&#27979;&#19981;&#21516;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#65292;&#37319;&#29992;&#22797;&#26434;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#20803;&#32032;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;RCO&#20462;&#27491;&#30340;ELM&#31639;&#27861;&#36827;&#34892;&#30701;&#26102;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.06707</link><description>&lt;p&gt;
&#22522;&#20110;RIOHTrack&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23454;&#29616;&#27813;&#38738;&#36335;&#38754;&#36710;&#36761;&#28145;&#24230;&#30340;&#30701;&#26102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A data-driven rutting depth short-time prediction model with metaheuristic optimization for asphalt pavements based on RIOHTrack. (arXiv:2305.06707v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#39044;&#27979;&#19981;&#21516;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#65292;&#37319;&#29992;&#22797;&#26434;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#20803;&#32032;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;RCO&#20462;&#27491;&#30340;ELM&#31639;&#27861;&#36827;&#34892;&#30701;&#26102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21508;&#31181;&#36947;&#36335;&#35774;&#35745;&#25351;&#21335;&#26469;&#35828;&#65292;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#26631;&#20934;&#12290;&#26412;&#25991;&#35797;&#22270;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26469;&#20272;&#35745;&#19981;&#21516;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#21098;&#36753;&#65292;&#28201;&#24230;&#20197;&#21450;&#36127;&#36733;&#36724;&#20316;&#20026;&#20027;&#35201;&#29305;&#24449;&#12290;&#23454;&#39564;&#25968;&#25454;&#26469;&#33258;&#26045;&#21152;&#19981;&#21516;&#21407;&#27833;&#26469;&#28304;&#30340;19&#20010;&#27813;&#38738;&#36335;&#38754;&#22312;&#36890;&#24030;&#24066;&#30340;2.038&#20844;&#37324;&#38271;&#30340;&#20840;&#23610;&#23544;&#21152;&#36895;&#36335;&#38754;&#35797;&#39564;&#32447;&#36335;(RIOHTrack, Road Track Institute)&#20013;&#25152;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#36890;&#36807;&#22797;&#26434;&#32593;&#32476;&#26041;&#27861;&#21644;Louvain&#31639;&#27861;&#23545;&#19981;&#21516;&#30340;&#36335;&#38754;&#36710;&#36761;&#28145;&#24230;&#26500;&#24314;&#22797;&#26434;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#20013;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#32467;&#26500;&#20803;&#32032;&#65292;&#24182;&#25214;&#21040;&#30456;&#20284;&#30340;&#32467;&#26500;&#20803;&#32032;&#12290;&#37319;&#29992;&#24102;&#27531;&#24046;&#20462;&#27491;&#20248;&#21270;(Residual Corrective Optimization, RCO)&#30340;&#26497;&#38480;&#23398;&#20064;&#26426;&#31639;&#27861;(ELM)&#29992;&#20110;&#30701;&#26102;&#38388;&#20869;&#39044;&#27979;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rutting of asphalt pavements is a crucial design criterion in various pavement design guides. A good road transportation base can provide security for the transportation of oil and gas in road transportation. This study attempts to develop a robust artificial intelligence model to estimate different asphalt pavements' rutting depth clips, temperature, and load axes as primary characteristics. The experiment data were obtained from 19 asphalt pavements with different crude oil sources on a 2.038 km long full-scale field accelerated pavement test track (RIOHTrack, Road Track Institute) in Tongzhou, Beijing. In addition, this paper also proposes to build complex networks with different pavement rutting depths through complex network methods and the Louvain algorithm for community detection. The most critical structural elements can be selected from different asphalt pavement rutting data, and similar structural elements can be found. An extreme learning machine algorithm with residual cor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#24378;&#21270;&#20102;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06704</link><description>&lt;p&gt;
&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#30340;&#40065;&#26834;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models. (arXiv:2305.06704v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#24378;&#21270;&#20102;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#65292;&#21487;&#20197;&#33719;&#24471;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#25351;&#30340;&#26159;&#20004;&#20010;&#30456;&#23545;&#26102;&#38388;&#20114;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#29992;&#20110;&#25511;&#21046;&#12289;&#39044;&#27979;&#25110;&#32858;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25152;&#35774;&#24819;&#30340;&#31649;&#36947;&#25509;&#25910;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20174;&#27599;&#20010;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#32452;&#23376;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#65288;&#20363;&#22914;K-means++&#21644;&#35889;&#32858;&#31867;&#65289;&#65292;&#37319;&#29992;&#21508;&#31181;&#25104;&#23545;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#19968;&#26086;&#32858;&#31867;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#36328;&#32858;&#31867;&#30340;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#34987;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;&#30001;&#20110;&#22810;
&lt;/p&gt;
&lt;p&gt;
In multivariate time series systems, key insights can be obtained by discovering lead-lag relationships inherent in the data, which refer to the dependence between two time series shifted in time relative to one another, and which can be leveraged for the purposes of control, forecasting or clustering. We develop a clustering-driven methodology for the robust detection of lead-lag relationships in lagged multi-factor models. Within our framework, the envisioned pipeline takes as input a set of time series, and creates an enlarged universe of extracted subsequence time series from each input time series, by using a sliding window approach. We then apply various clustering techniques (e.g, K-means++ and spectral clustering), employing a variety of pairwise similarity measures, including nonlinear ones. Once the clusters have been extracted, lead-lag estimates across clusters are aggregated to enhance the identification of the consistent relationships in the original universe. Since multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#35843;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#27599;&#31181;&#31454;&#20105;&#29983;&#23384;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#30830;&#20445;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#26368;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.06703</link><description>&lt;p&gt;
&#31454;&#20105;&#39118;&#38505;&#30340;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#65306;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Fine-Gray: Monotonic neural networks for competing risks. (arXiv:2305.06703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#35843;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#27599;&#31181;&#31454;&#20105;&#29983;&#23384;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#30830;&#20445;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#26368;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#19968;&#31181;&#22788;&#29702;&#24739;&#32773;&#22240;&#26410;&#32463;&#21382;&#24863;&#20852;&#36259;&#20107;&#20214;&#32780;&#20986;&#29616;&#30340;&#8220;censoring&#8221;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#27169;&#22411;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#27492;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#24448;&#24448;&#24573;&#30053;&#20102;&#31454;&#20105;&#39118;&#38505;&#23545;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23548;&#33268;&#29983;&#23384;&#29575;&#20272;&#35745;&#23384;&#22312;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#35843;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#27599;&#31181;&#31454;&#20105;&#29983;&#23384;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#30830;&#20445;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#33021;&#22815;&#22312;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#26368;&#20248;&#21270;&#12290;&#36890;&#36807;&#25928;&#26524;&#23454;&#39564;&#23545;&#27604;&#23436;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#21644;&#19977;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#29983;&#23384;&#25968;&#25454;&#20998;&#26512;&#12290;&#26368;&#21518;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#21307;&#30103;&#23454;&#36341;&#39118;&#38505;&#35780;&#20272;&#25351;&#26631;&#26102;&#32771;&#34385;&#31454;&#20105;&#39118;&#38505;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-to-event modelling, known as survival analysis, differs from standard regression as it addresses censoring in patients who do not experience the event of interest. Despite competitive performances in tackling this problem, machine learning methods often ignore other competing risks that preclude the event of interest. This practice biases the survival estimation. Extensions to address this challenge often rely on parametric assumptions or numerical estimations leading to sub-optimal survival approximations. This paper leverages constrained monotonic neural networks to model each competing survival distribution. This modelling choice ensures the exact likelihood maximisation at a reduced computational cost by using automatic differentiation. The effectiveness of the solution is demonstrated on one synthetic and three medical datasets. Finally, we discuss the implications of considering competing risks when developing risk scores for medical practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06695</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#29992;&#20110;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#65292;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#29289;&#31181;&#21644;&#20010;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#19978;&#22686;&#24378;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#31232;&#26377;&#31867;&#21035;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#38754;&#65292;&#35813;&#39046;&#22495;&#23578;&#26410;&#36827;&#34892;&#23581;&#35797;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#65292;&#26088;&#22312;&#38544;&#24335;&#32534;&#30721;&#36328;&#22495;&#20851;&#32852;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36825;&#31181;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#30452;&#25509;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20110;32&#20010;&#29289;&#31181;&#12289;&#36229;&#36807;30,000&#20010;&#28014;&#28216;&#26377;&#23380;&#34411;&#22771;&#30340;&#26174;&#24494;&#22270;&#20687;&#24182;&#19982;&#29420;&#31435;&#30340;&#36951;&#20256;&#25968;&#25454;&#26679;&#26412;&#19968;&#36215;&#20351;&#29992;&#26469;&#23454;&#39564;&#23460;&#23637;&#29616;&#20102;&#35813;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;&#20174;&#19994;&#32773;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35270;&#35273;-&#36951;&#20256;&#23545;&#40784;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06677</link><description>&lt;p&gt;
INGENIOUS&#65306;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#30528;&#29305;&#28857;&#26159;&#22312;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#26032;&#33021;&#21147;&#26041;&#38754;&#38543;&#30528;&#27169;&#22411;&#23481;&#37327;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780; achieved. &#28982;&#32780;&#65292;&#24517;&#39035;&#35748;&#35782;&#21040;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12289;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26377;&#23475;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#26159;&#21542;&#21487;&#33021;&#20165;&#20351;&#29992;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#35757;&#32451; PTLM&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#20854;&#19979;&#28216;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#65306;&#22312;Exp3&#31639;&#27861;&#20013;&#65292;&#22914;&#26524;&#23398;&#20064;&#29575;&#26159;&#24658;&#23450;&#30340;&#65292;&#21017;MLE&#19981;&#33021;&#26377;&#25928;&#20272;&#35745;&#23398;&#20064;&#29575;&#65307;&#32780;&#22914;&#26524;&#23398;&#20064;&#29575;&#38543;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#21576;&#22810;&#39033;&#24335;&#19979;&#38477;&#65292;&#21017;MLE&#30340;&#39044;&#27979;&#35823;&#24046;&#22312;&#27010;&#29575;&#19978;&#28385;&#36275;&#38543;&#26679;&#26412;&#37327;&#22810;&#39033;&#24335;&#19979;&#38477;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.06660</link><description>&lt;p&gt;
&#20851;&#20110;MLE&#20316;&#20026;Exp3&#31639;&#27861;&#23398;&#20064;&#29575;&#20272;&#35745;&#22120;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of the MLE as an estimator of the learning rate in the Exp3 algorithm. (arXiv:2305.06660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#65306;&#22312;Exp3&#31639;&#27861;&#20013;&#65292;&#22914;&#26524;&#23398;&#20064;&#29575;&#26159;&#24658;&#23450;&#30340;&#65292;&#21017;MLE&#19981;&#33021;&#26377;&#25928;&#20272;&#35745;&#23398;&#20064;&#29575;&#65307;&#32780;&#22914;&#26524;&#23398;&#20064;&#29575;&#38543;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#21576;&#22810;&#39033;&#24335;&#19979;&#38477;&#65292;&#21017;MLE&#30340;&#39044;&#27979;&#35823;&#24046;&#22312;&#27010;&#29575;&#19978;&#28385;&#36275;&#38543;&#26679;&#26412;&#37327;&#22810;&#39033;&#24335;&#19979;&#38477;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25226;&#20010;&#20307;&#30340;&#23398;&#20064;&#25968;&#25454;&#25311;&#21512;&#25104;&#31867;&#20284;&#31639;&#27861;&#30340;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#35266;&#27979;&#20540;&#38750;&#24120;&#30456;&#20114;&#20381;&#36182;&#19988;&#38750;&#24179;&#31283;&#65292;&#20197;&#33267;&#20110;&#20154;&#20204;&#21487;&#33021;&#20250;&#24819;&#30693;&#36947;&#21363;&#20351;&#26159;&#23454;&#39564;&#24615;&#35748;&#30693;&#30340;&#24120;&#35268;&#24037;&#20855;&#8212;&#8212;&#32463;&#20856;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65288;MLE&#65289;&#33021;&#22815;&#20570;&#20123;&#20160;&#20040;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#23637;&#31034;&#22914;&#26524;Exp3&#31639;&#27861;&#20013;&#30340;&#23398;&#20064;&#29575;&#26159;&#24658;&#23450;&#30340;&#65292;&#21017;&#23398;&#20064;&#29575;&#30340;&#20272;&#35745;&#19981;&#21487;&#33021;&#26159;&#26377;&#25928;&#30340;&#65292;&#21478;&#22806;&#65292;&#22914;&#26524;&#23398;&#20064;&#29575;&#38543;&#30528;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#21576;&#22810;&#39033;&#24335;&#19979;&#38477;&#65292;&#21017;MLE&#30340;&#39044;&#27979;&#35823;&#24046;&#22312;&#27010;&#29575;&#19978;&#28385;&#36275;&#38543;&#26679;&#26412;&#37327;&#22810;&#39033;&#24335;&#19979;&#38477;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
When fitting the learning data of an individual to algorithm-like learning models, the observations are so dependent and non-stationary that one may wonder what the classical Maximum Likelihood Estimator (MLE) could do, even if it is the usual tool applied to experimental cognition. Our objective in this work is to show that the estimation of the learning rate cannot be efficient if the learning rate is constant in the classical Exp3 (Exponential weights for Exploration and Exploitation) algorithm. Secondly, we show that if the learning rate decreases polynomially with the sample size, then the prediction error and in some cases the estimation error of the MLE satisfy bounds in probability that decrease at a polynomial rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;</title><link>http://arxiv.org/abs/2305.06657</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#30456;&#37051;&#19981;&#30830;&#23450;&#24615;&#38598;&#21644;&#21452;&#20195;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#19978;&#20248;&#21270;&#26368;&#24046;&#24615;&#33021;&#12290;&#32473;&#23450;&#19968;&#20010;&#20135;&#29983;&#35757;&#32451;&#26679;&#26412;&#30340;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;N-MDP&#65289;&#65292;&#35813;&#38598;&#21512;&#21253;&#21547;&#36890;&#36807;&#23545;N-MDP&#36827;&#34892;&#26576;&#20123;&#25200;&#21160;&#32780;&#33719;&#24471;&#30340;MDP&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27604;&#29616;&#26377;&#38598;&#21512;&#26356;&#23454;&#38469;&#30340;MDP&#12290;&#20351;&#29992;&#36825;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#21517;&#20026;ARQ-Learning&#65292;&#29992;&#20110;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#35823;&#24046;&#30028;&#24182;&#35777;&#26126;&#23427;&#19982;Q-Learning&#21644;&#40065;&#26834;Q-Learning&#65288;&#21363;&#29616;&#26377;&#30340;&#40065;&#26834;RL&#26041;&#27861;&#65289;&#19968;&#26679;&#24555;&#22320;&#25910;&#25947;&#65292;&#21516;&#26102;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23558;ARQ-Learning&#25193;&#23637;&#21040;&#22823;&#22411;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#19968;&#25216;&#26415;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;PRQ-Learning&#12290;&#25509;&#30528;&#65292;&#23558;&#20854;&#19982;DQN&#21644;DDPG&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#20998;&#21035;&#24320;&#21457;&#20102;PR-DQN&#21644;PR-DDPG&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21450;&#20854;&#21464;&#20307;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#65292;&#20854;&#27867;&#21270;&#30028;&#38480;&#19982;&#36830;&#32493;&#26435;&#37325;&#24046;&#24322;&#22823;&#23567;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.06648</link><description>&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#19982;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization bounds for neural ordinary differential equations and deep residual networks. (arXiv:2305.06648v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21450;&#20854;&#21464;&#20307;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#65292;&#20854;&#27867;&#21270;&#30028;&#38480;&#19982;&#36830;&#32493;&#26435;&#37325;&#24046;&#24322;&#22823;&#23567;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#36830;&#32493;&#28145;&#24230;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#30001;&#36830;&#32493;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;ODE&#21450;&#26102;&#21464;&#30340;&#31070;&#32463;ODE&#32452;&#25104;&#30340;&#22823;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;Lipschitz&#26041;&#27861;&#25512;&#23548;&#20102;&#36825;&#20010;&#31867;&#21035;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;ODE&#21644;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#20043;&#38388;&#30340;&#31867;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#19968;&#20010;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36825;&#20010;&#30028;&#38480;&#19982;&#36830;&#32493;&#26435;&#37325;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#22823;&#23567;&#26377;&#20851;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#28436;&#31034;&#20102;&#36825;&#20010;&#37327;&#26159;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (neural ODEs) are a popular family of continuous-depth deep learning models. In this work, we consider a large family of parameterized ODEs with continuous-in-time parameters, which include time-dependent neural ODEs. We derive a generalization bound for this class by a Lipschitz-based argument. By leveraging the analogy between neural ODEs and deep residual networks, our approach yields in particular a generalization bound for a class of deep residual networks. The bound involves the magnitude of the difference between successive weight matrices. We illustrate numerically how this quantity affects the generalization capability of neural networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21464;&#28857;&#30417;&#27979;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#27604;&#29616;&#26377;&#30340;&#22312;&#32447;&#30417;&#27979;&#26041;&#27861;&#26356;&#22909;&#22320;&#25511;&#21046;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;ARIMA&#27169;&#22411;&#21644;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06630</link><description>&lt;p&gt;
&#24322;&#36136;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predictive change point detection for heterogeneous data. (arXiv:2305.06630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21464;&#28857;&#30417;&#27979;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#27604;&#29616;&#26377;&#30340;&#22312;&#32447;&#30417;&#27979;&#26041;&#27861;&#26356;&#22909;&#22320;&#25511;&#21046;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;ARIMA&#27169;&#22411;&#21644;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36741;&#21161;&#30340;&#21464;&#28857;&#26816;&#27979;&#65288;CPD&#65289;&#26694;&#26550;&#65292;&#24182;&#19982;&#20854;&#20182;&#22312;&#32447;CPD&#20363;&#31243;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;&#35813;&#26041;&#27861;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#39044;&#27979;&#27493;&#39588;&#65289;&#20195;&#26367;&#36890;&#24120;&#20351;&#29992;&#30340;&#36235;&#21183;&#20272;&#35745;&#20989;&#25968;&#65288;&#22914;&#28369;&#21160;&#24179;&#22343;&#65289;&#65292;&#24182;&#23558;&#20854;&#39044;&#27979;&#19982;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65288;&#27604;&#36739;&#27493;&#39588;&#65289;&#65292;&#20174;&#32780;&#25913;&#21892;&#39034;&#24207;&#20998;&#26512;&#20013;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20363;&#22914;CUSUM&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#36825;&#20123;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
A change point detection (CPD) framework assisted by a predictive machine learning model called ''Predict and Compare'' is introduced and characterised in relation to other state-of-the-art online CPD routines which it outperforms in terms of false positive rate and out-of-control average run length. The method's focus is on improving standard methods from sequential analysis such as the CUSUM rule in terms of these quality measures.  This is achieved by replacing typically used trend estimation functionals such as the running mean with more sophisticated predictive models (Predict step), and comparing their prognosis with actual data (Compare step). The two models used in the Predict step are the ARIMA model and the LSTM recursive neural network. However, the framework is formulated in general terms, so as to allow the use of other prediction or comparison methods than those tested here. The power of the method is demonstrated in a tribological case study in which change points separa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21452;&#25351;&#25968;&#26063;&#30340;&#25193;&#23637;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;dropout&#27491;&#21017;&#21270;&#65292;dropout&#27491;&#21017;&#21270;&#20559;&#22909;&#32597;&#35265;&#20294;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22312;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#26041;&#38754;&#37117;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06625</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#25351;&#25968;&#26063;&#30340;&#25193;&#23637;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;Dropout&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dropout Regularization in Extended Generalized Linear Models based on Double Exponential Families. (arXiv:2305.06625v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21452;&#25351;&#25968;&#26063;&#30340;&#25193;&#23637;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;dropout&#27491;&#21017;&#21270;&#65292;dropout&#27491;&#21017;&#21270;&#20559;&#22909;&#32597;&#35265;&#20294;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22312;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#26041;&#38754;&#37117;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;dropout&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21452;&#25351;&#25968;&#26063;&#30340;&#25193;&#23637;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;dropout&#27491;&#21017;&#21270;&#65292;&#20854;&#20013;&#31163;&#25955;&#21442;&#25968;&#21487;&#20197;&#38543;&#29305;&#24449;&#21464;&#21270;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;dropout&#27491;&#21017;&#21270;&#20559;&#22909;&#32597;&#35265;&#20294;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22312;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#26041;&#38754;&#37117;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#36825;&#25193;&#23637;&#20102;&#20043;&#21069;&#38024;&#23545;&#20256;&#32479;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#32467;&#26524; &#12290;&#37319;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;dropout&#24212;&#29992;&#20110;&#33258;&#36866;&#24212;B&#26679;&#26465;&#24179;&#28369;&#65292;&#20854;&#20013;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#21442;&#25968;&#37117;&#34987;&#28789;&#27963;&#22320;&#24314;&#27169;&#12290;&#37325;&#35201;&#30340;B&#26679;&#26465;&#22522;&#30784;&#20989;&#25968;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#32597;&#35265;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#65292;dropout&#26159;&#19968;&#31181;&#25913;&#21892;&#20102;&#32602;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#30340;&#26174;&#24335;&#24179;&#28369;&#24615;&#30340;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#21442;&#25968;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though dropout is a popular regularization technique, its theoretical properties are not fully understood. In this paper we study dropout regularization in extended generalized linear models based on double exponential families, for which the dispersion parameter can vary with the features. A theoretical analysis shows that dropout regularization prefers rare but important features in both the mean and dispersion, generalizing an earlier result for conventional generalized linear models. Training is performed using stochastic gradient descent with adaptive learning rate. To illustrate, we apply dropout to adaptive smoothing with B-splines, where both the mean and dispersion parameters are modelled flexibly. The important B-spline basis functions can be thought of as rare features, and we confirm in experiments that dropout is an effective form of regularization for mean and dispersion parameters that improves on a penalized maximum likelihood approach with an explicit smoothness p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28909;&#24102;&#21322;&#29615;&#19978;&#30340;&#30697;&#38453;&#19977;&#22240;&#24335;&#20998;&#35299;&#31639;&#27861; triFastSTMF&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#37096;&#20998;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#22312;&#22235;&#37096;&#20998;&#32593;&#32476;&#32467;&#26500;&#20998;&#26512;&#20013;&#24674;&#22797;&#32593;&#32476;&#30340;&#36793;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06624</link><description>&lt;p&gt;
&#28909;&#24102;&#21322;&#29615;&#19978;&#30340;&#30697;&#38453;&#19977;&#22240;&#24335;&#20998;&#35299;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matrix tri-factorization over the tropical semiring. (arXiv:2305.06624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28909;&#24102;&#21322;&#29615;&#19978;&#30340;&#30697;&#38453;&#19977;&#22240;&#24335;&#20998;&#35299;&#31639;&#27861; triFastSTMF&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#37096;&#20998;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#22312;&#22235;&#37096;&#20998;&#32593;&#32476;&#32467;&#26500;&#20998;&#26512;&#20013;&#24674;&#22797;&#32593;&#32476;&#30340;&#36793;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#24102;&#21322;&#29615;&#22312;&#20248;&#21270;&#25511;&#21046;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#25110;&#20915;&#31574;&#38382;&#39064;&#31561;&#20960;&#20010;&#30740;&#31350;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28909;&#24102;&#21322;&#29615;&#19978;&#30340;&#30697;&#38453;&#19977;&#22240;&#24335;&#20998;&#35299;&#31639;&#27861; triFastSTMF&#65292;&#29992;&#20110;&#20998;&#26512;&#22312;&#28909;&#24102;&#21322;&#29615;&#19978;&#22810;&#37096;&#20998;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#22235;&#37096;&#20998;&#32593;&#32476;&#32467;&#26500;&#20998;&#26512;&#65292;&#20197;&#24674;&#22797;&#32593;&#32476;&#30340;&#36793;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tropical semiring has proven successful in several research areas, including optimal control, bioinformatics, discrete event systems, or solving a decision problem. In previous studies, a matrix two-factorization algorithm based on the tropical semiring has been applied to investigate bipartite and tripartite networks. Tri-factorization algorithms based on standard linear algebra are used for solving tasks such as data fusion, co-clustering, matrix completion, community detection, and more. However, there is currently no tropical matrix tri-factorization approach, which would allow for the analysis of multipartite networks with a high number of parts. To address this, we propose the triFastSTMF algorithm, which performs tri-factorization over the tropical semiring. We apply it to analyze a four-partition network structure and recover the edge lengths of the network. We show that triFastSTMF performs similarly to Fast-NMTF in terms of approximation and prediction performance when fitted
&lt;/p&gt;</description></item><item><title>V2Meow&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;O(100K)&#38899;&#39057;&#29255;&#27573;&#37197;&#23545;&#30340;&#35270;&#39057;&#24103;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#19982;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#39057;&#36755;&#20837;&#30340;&#35270;&#35273;&#35821;&#20041;&#30456;&#21305;&#37197;&#30340;&#39640;&#36136;&#37327;&#38899;&#39057;&#65292;&#26080;&#38656;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.06594</link><description>&lt;p&gt;
V2Meow: &#36890;&#36807;&#38899;&#20048;&#29983;&#25104;&#22120;&#36319;&#38543;&#35270;&#35273;&#33410;&#25293;&#36827;&#34892;&#8220;&#21941;&#21483;&#8221;(arXiv:2305.06594v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
V2Meow: Meowing to the Visual Beat via Music Generation. (arXiv:2305.06594v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06594
&lt;/p&gt;
&lt;p&gt;
V2Meow&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;O(100K)&#38899;&#39057;&#29255;&#27573;&#37197;&#23545;&#30340;&#35270;&#39057;&#24103;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#19982;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#39057;&#36755;&#20837;&#30340;&#35270;&#35273;&#35821;&#20041;&#30456;&#21305;&#37197;&#30340;&#39640;&#36136;&#37327;&#38899;&#39057;&#65292;&#26080;&#38656;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#19982;&#35270;&#39057;&#35270;&#35273;&#20869;&#23481;&#30456;&#21305;&#37197;&#30340;&#39640;&#36136;&#37327;&#38899;&#20048;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35270;&#35273;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#65292;&#22914;MIDI&#25991;&#20214;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;&#38899;&#39057;&#27874;&#24418;&#12290;&#32771;&#34385;&#21040;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#20026;&#23569;&#25968;&#20048;&#22120;&#25110;&#29305;&#23450;&#31867;&#22411;&#30340;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#38899;&#20048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;V2Meow&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#19982;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#39057;&#36755;&#20837;&#30340;&#35270;&#35273;&#35821;&#20041;&#30456;&#21305;&#37197;&#30340;&#39640;&#36136;&#37327;&#38899;&#39057;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23427;&#26159;&#36890;&#36807;&#19982;&#20174;&#37326;&#29983;&#38899;&#20048;&#35270;&#39057;&#20013;&#25366;&#25496;&#30340;O(100K)&#38899;&#20048;&#38899;&#39057;&#29255;&#27573;&#37197;&#23545;&#30340;&#35270;&#39057;&#24103;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#27809;&#26377;&#28041;&#21450;&#20219;&#20309;&#24182;&#34892;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#12290;V2Meow&#33021;&#22815;&#20165;&#22312;&#20808;&#21069;&#35757;&#32451;&#30340;&#20174;&#20219;&#24847;&#38745;&#24577;&#35270;&#39057;&#25552;&#21462;&#30340;&#35270;&#35273;&#29305;&#24449;&#30340;&#26465;&#20214;&#19979;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#38899;&#39057;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating high quality music that complements the visual content of a video is a challenging task. Most existing visual conditioned music generation systems generate symbolic music data, such as MIDI files, instead of raw audio waveform. Given the limited availability of symbolic music data, such methods can only generate music for a few instruments or for specific types of visual input. In this paper, we propose a novel approach called V2Meow that can generate high-quality music audio that aligns well with the visual semantics of a diverse range of video input types. Specifically, the proposed music generation system is a multi-stage autoregressive model which is trained with a number of O(100K) music audio clips paired with video frames, which are mined from in-the-wild music videos, and no parallel symbolic music data is involved. V2Meow is able to synthesize high-fidelity music audio waveform solely conditioned on pre-trained visual features extracted from an arbitrary silent vide
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HAHE&#27169;&#22411;&#65292;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#20102;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#39034;&#24207;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06588</link><description>&lt;p&gt;
HAHE: &#22522;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22411;&#29992;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level. (arXiv:2305.06588v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HAHE&#27169;&#22411;&#65292;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#20102;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#39034;&#24207;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#26159;&#20540;&#24471;&#23581;&#35797;&#30340;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#8212;&#8212;HAHE&#65292;&#21253;&#25324;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34920;&#31034;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#37319;&#29992;&#36229;&#22270;&#21452;&#37325;&#27880;&#24847;&#21147;&#23618;&#65292;&#20840;&#23616;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21487;&#20197;&#24314;&#27169;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#65307;&#32780;&#37319;&#29992;&#24322;&#36136;&#24615;&#33258;&#27880;&#24847;&#23618;&#65292;&#23616;&#37096;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21017;&#21487;&#20197;&#23398;&#20064;H-Facts&#20869;&#37096;&#30340;&#39034;&#24207;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HAHE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06587</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22823;&#22810;&#25968;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25277;&#35937;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#35889;&#26102;GNN&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#21147;&#21463;&#21040;&#25105;&#20204;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#23454;&#36341;&#20013;&#26377;&#29992;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#30456;&#20851;&#38480;&#21046;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#35889;&#22495;&#20013;&#35774;&#35745;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#22359;&#30340;&#29702;&#35770;&#34013;&#22270;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#24182;&#20026;&#20102;&#23637;&#31034;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#35889;&#26102;GNN&#26377;&#22810;&#20040;&#24378;&#22823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Temporal Graph GegenConv (TGC) &#30340;&#31616;&#21333;&#23454;&#20363;&#65292;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#26102;&#21464;&#22270;&#33410;&#28857;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#28155;&#21152;&#20102;&#31751;&#26631;&#31614;&#24179;&#28369;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.06576</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#26631;&#31614;&#24179;&#28369;&#24615;&#30340;&#26102;&#21464;&#22270;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Clustering of Time-Varying Graphs Based on Temporal Label Smoothness. (arXiv:2305.06576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#26102;&#21464;&#22270;&#33410;&#28857;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#28155;&#21152;&#20102;&#31751;&#26631;&#31614;&#24179;&#28369;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31751;&#26631;&#31614;&#38543;&#26102;&#38388;&#24179;&#28369;&#21464;&#21270;&#30340;&#33410;&#28857;&#32858;&#31867;&#26041;&#27861;&#12290;&#32858;&#31867;&#26159;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#22522;&#30784;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#21495;&#22788;&#29702;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#24050;&#26377;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#38745;&#24577;&#22270;&#20013;&#33410;&#28857;&#30340;&#32858;&#31867;&#65292;&#20294;&#25105;&#20204;&#32463;&#24120;&#36935;&#21040;&#26102;&#21464;&#22270;&#65292;&#22914;&#31038;&#20132;&#32593;&#32476;&#12289;&#33041;&#21151;&#33021;&#36830;&#25509;&#21644;&#28857;&#20113;&#31561;&#26102;&#24207;&#25968;&#25454;&#12290;&#26412;&#25991;&#23558;&#26102;&#21464;&#22270;&#30340;&#33410;&#28857;&#32858;&#31867;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#28155;&#21152;&#20102;&#31751;&#26631;&#31614;&#24179;&#28369;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#25286;&#20998;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a node clustering method for time-varying graphs based on the assumption that the cluster labels are changed smoothly over time. Clustering is one of the fundamental tasks in many science and engineering fields including signal processing, machine learning, and data mining. Although most existing studies focus on the clustering of nodes in static graphs, we often encounter time-varying graphs for time-series data, e.g., social networks, brain functional connectivity, and point clouds. In this paper, we formulate a node clustering of time-varying graphs as an optimization problem based on spectral clustering, with a smoothness constraint of the node labels. We solve the problem with a primal-dual splitting algorithm. Experiments on synthetic and real-world time-varying graphs are performed to validate the effectiveness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270;Tucker&#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#24341;&#20837;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06563</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270; Tucker &#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270;Tucker&#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#24341;&#20837;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;(STDI)&#26159;&#25968;&#25454;&#39537;&#21160;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#19981;&#21487;&#36991;&#20813;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22312;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#20132;&#36890;&#25968;&#25454;&#20013;&#20272;&#35745;&#20002;&#22833;&#25968;&#25454;&#12290;&#30001;&#20110;&#20132;&#36890;&#25968;&#25454;&#20855;&#26377;&#22810;&#32500;&#21644;&#26102;&#31354;&#24615;&#36136;&#65292;&#25105;&#20204;&#23558;&#20002;&#22833;&#25968;&#25454;&#22635;&#20805;&#35270;&#20026;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#35768;&#22810;&#20851;&#20110;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340; STDI &#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#24320;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#24615;&#21644;&#26680;&#24352;&#37327;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#22635;&#20805;&#24615;&#33021;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#12290;&#26412;&#25991;&#37325;&#26032;&#26500;&#36896;&#20102;3/4&#38454;&#27721;&#20811;&#23572;&#24352;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270; Tucker &#20998;&#35299;(maniRTD)&#27169;&#22411;&#29992;&#20110;STDI&#12290;&#26126;&#30830;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22810;&#32500;&#24310;&#36831;&#23884;&#20837;&#21464;&#25442;&#23558;&#20256;&#24863;&#20132;&#36890;&#29366;&#24577;&#25968;&#25454;&#34920;&#31034;&#20026;3/4&#38454;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;ManiRTD&#20351;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#20351;&#29992;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#30340;Open Long-Tailed QA (OLTQA)&#27169;&#22411;&#65292;&#40723;&#21169;&#22836;&#37096;&#12289;&#23614;&#37096;&#21644;&#26410;&#30693;&#20219;&#21153;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#24182;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#25366;&#25496;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;QA&#26041;&#27861;&#20013;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06557</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Question Answering in an Open World. (arXiv:2305.06557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#30340;Open Long-Tailed QA (OLTQA)&#27169;&#22411;&#65292;&#40723;&#21169;&#22836;&#37096;&#12289;&#23614;&#37096;&#21644;&#26410;&#30693;&#20219;&#21153;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#24182;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#25366;&#25496;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;QA&#26041;&#27861;&#20013;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#24320;&#25918;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;QA&#27169;&#22411;&#20197;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#23545;&#20110;&#23454;&#38469;&#30340;QA&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25193;&#23637;&#20197;&#21069;&#30340;QA&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#35775;&#38382;&#36275;&#22815;&#26679;&#26412;&#30340;&#24050;&#30693;&#20219;&#21153;&#65292;&#35201;&#20040;&#19981;&#26126;&#30830;&#22320;&#23545;&#26410;&#30693;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Open Long-Tailed QA (OLTQA)&#23450;&#20041;&#20026;&#23398;&#20064;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;QA&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;OLTQA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#40723;&#21169;&#22836;&#37096;&#12289;&#23614;&#37096;&#21644;&#26410;&#30693;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#24182;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#25366;&#25496;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#32452;&#32454;&#31890;&#24230;&#30340;&#32452;&#20214;&#26469;&#32452;&#32455;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#21160;&#24577;&#32452;&#21512;&#36825;&#20123;&#32452;&#20214;&#20197;&#26041;&#20415;&#30693;&#35782;&#20849;&#20139;&#12290;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#26816;&#32034;-&#37325;&#25490;&#26694;&#26550;&#26469;&#36873;&#25321;&#19978;&#19979;&#25991;&#20363;&#23376;&#65292;&#36825;&#20123;&#20363;&#23376;&#25351;&#23548;LM&#29983;&#25104;&#34920;&#36798;QA&#20219;&#21153;&#30693;&#35782;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#27744;&#21270;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM). Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing. A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generate text that express knowledge for QA tasks. Moreover, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diana&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#20854;&#20013;&#65292;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#23454;&#20363;&#32423;&#25552;&#31034;&#29992;&#20110;&#23398;&#20064;&#36328;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06555</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#19979;&#30340;&#39046;&#22495;&#22686;&#37327;&#29983;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain Incremental Lifelong Learning in an Open World. (arXiv:2305.06555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diana&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#20854;&#20013;&#65292;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#23454;&#20363;&#32423;&#25552;&#31034;&#29992;&#20110;&#23398;&#20064;&#36328;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26159;NLP&#27169;&#22411;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#22522;&#20110;&#26550;&#26500;&#30340;&#26041;&#27861;&#34987;&#25253;&#36947;&#20026;LL&#27169;&#22411;&#30340;&#26377;&#25928;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#20808;&#21069;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#39046;&#22495;&#22686;&#37327;LL&#22330;&#26223;&#24182;&#38750;&#26131;&#20107;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#22312;&#27979;&#35797;&#38454;&#27573;&#35775;&#38382;&#20219;&#21153;&#36523;&#20221;&#65292;&#35201;&#20040;&#26080;&#27861;&#22788;&#29702;&#26469;&#33258;&#26410;&#35265;&#20219;&#21153;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Diana&#65306;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#35797;&#22270;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290; Diana&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose \textbf{Diana}: a \underline{d}ynam\underline{i}c \underline{a}rchitecture-based lifelo\underline{n}g le\underline{a}rning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model's generalization performance. Moreover, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#21051;&#30011;&#21560;&#24341;&#22495;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.06547</link><description>&lt;p&gt;
&#31163;&#25955;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Lyapunov Control for Discrete-Time Systems. (arXiv:2305.06547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#21051;&#30011;&#21560;&#24341;&#22495;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32447;&#24615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#24050;&#32463;&#34987;&#20805;&#20998;&#20102;&#35299;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19968;&#33324;&#26041;&#27861;&#26159;&#21033;&#29992;&#26446;&#38597;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#35745;&#31639;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#20989;&#25968;&#21644;&#30456;&#20851;&#25511;&#21046;&#31574;&#30053;&#30340;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#23547;&#25214;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#36866;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#23398;&#20064;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#30340;&#26041;&#27861;&#12290;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#31532;&#19968;&#20010;&#26159;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#20013;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#26159;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21051;&#30011;&#20102;&#21560;&#24341;&#22495;&#30340;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
While ensuring stability for linear systems is well understood, it remains a major challenge for systems with nonlinear dynamics. A general approach in such cases is to leverage Lyapunov stability theory to compute a combination of a Lyapunov control function and an associated control policy. However, finding Lyapunov functions for general nonlinear systems is a challenging task. To address this challenge, several methods have been recently proposed that represent Lyapunov functions using neural networks. However, such approaches have been designed exclusively for continuous-time systems. We propose the first approach for learning neural Lyapunov control in discrete-time systems. Three key ingredients enable us to effectively learn provably stable control policies. The first is a novel mixed-integer linear programming approach for verifying the stability conditions in discrete-time systems. The second is a novel approach for computing sub-level sets which characterize the region of att
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#35889;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#20174;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#25552;&#21462;&#22823;&#37327;&#28857;&#26102;&#30340;&#35889;&#32858;&#31867;&#38382;&#39064;&#24471;&#21040;&#20102;&#19968;&#23450;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2305.06541</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#35889;&#32858;&#31867;&#65306;&#22312;&#20309;&#26102;&#26377;&#25928;&#65311;&#26469;&#33258;&#36830;&#32493;&#32858;&#31867;&#21644;&#23494;&#24230;Cheeger-Buser&#29702;&#35770;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser. (arXiv:2305.06541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#35889;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#20174;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#25552;&#21462;&#22823;&#37327;&#28857;&#26102;&#30340;&#35889;&#32858;&#31867;&#38382;&#39064;&#24471;&#21040;&#20102;&#19968;&#23450;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#32858;&#31867;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#65292;&#24050;&#32463;&#32463;&#21463;&#20102;&#26102;&#38388;&#30340;&#32771;&#39564;&#12290;&#23427;&#26131;&#20110;&#25551;&#36848;&#65292;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#32447;&#24615;&#20195;&#25968;&#23454;&#29616;&#65292;&#24182;&#19988;&#36890;&#24120;&#27604;&#20256;&#32479;&#32858;&#31867;&#31639;&#27861;&#22914;K-means&#21644;K-centers&#25214;&#21040;&#26356;&#22909;&#30340;&#32858;&#31867;&#12290;&#30001;Shi&#21644;Malik&#24320;&#21457;&#30340;&#20004;&#21521;&#35889;&#32858;&#31867;&#22522;&#30784;&#31639;&#27861;&#20174;&#25968;&#25454;&#20013;&#21019;&#24314;&#20960;&#20309;&#22270;&#24418;&#65292;&#24182;&#25214;&#21040;&#22270;&#24418;&#30340;&#35889;&#20999;&#21106;&#12290;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#34987;&#24314;&#27169;&#20026;&#20174;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#25552;&#21462;&#30340;&#22823;&#37327;&#28857;&#12290;&#23545;&#20110;&#36825;&#31181;&#26041;&#24335;&#22914;&#20309;&#36827;&#34892;&#35889;&#32858;&#31867;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#32773;&#36890;&#36807;&#24341;&#29992;&#22270;Cheeger&#19981;&#31561;&#24335;&#35777;&#26126;&#20102;&#35889;&#32858;&#31867;&#30340;&#27491;&#30830;&#24615;&#65288;&#21363;&#34920;&#31034;&#22270;&#35889;&#20999;&#21106;&#36924;&#36817;&#8220;&#24402;&#19968;&#21270;&#20999;&#21106;&#8221;&#65289;&#65292;&#20294;&#36825;&#31181;&#35777;&#26126;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#23849;&#28291;&#20102;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#20174;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#25552;&#21462;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering is one of the most popular clustering algorithms that has stood the test of time. It is simple to describe, can be implemented using standard linear algebra, and often finds better clusters than traditional clustering algorithms like $k$-means and $k$-centers. The foundational algorithm for two-way spectral clustering, by Shi and Malik, creates a geometric graph from data and finds a spectral cut of the graph.  In modern machine learning, many data sets are modeled as a large number of points drawn from a probability density function. Little is known about when spectral clustering works in this setting -- and when it doesn't. Past researchers justified spectral clustering by appealing to the graph Cheeger inequality (which states that the spectral cut of a graph approximates the ``Normalized Cut''), but this justification is known to break down on large data sets.  We provide theoretically-informed intuition about spectral clustering on large data sets drawn from pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#35821;&#20041;&#38543;&#26426;&#28216;&#36208;&#30340;&#23646;&#24615;&#22270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20004;&#31181;&#24322;&#26500;&#28304;&#65292;&#26500;&#24314;&#36741;&#21161;&#21152;&#26435;&#22270;&#36827;&#34892;&#39640;&#38454;&#36317;&#31163;&#23398;&#20064;&#65292;&#20174;&#32780;&#34920;&#24449;&#33410;&#28857;&#21644;&#23646;&#24615;&#65292;&#24182;&#25429;&#33719;&#22270;&#32467;&#26500;&#21644;&#35821;&#20041;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#39640;&#38454;&#20869;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06531</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#38543;&#26426;&#28216;&#36208;&#30340;&#23646;&#24615;&#22270;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic Random Walk for Graph Representation Learning in Attributed Graphs. (arXiv:2305.06531v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#35821;&#20041;&#38543;&#26426;&#28216;&#36208;&#30340;&#23646;&#24615;&#22270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20004;&#31181;&#24322;&#26500;&#28304;&#65292;&#26500;&#24314;&#36741;&#21161;&#21152;&#26435;&#22270;&#36827;&#34892;&#39640;&#38454;&#36317;&#31163;&#23398;&#20064;&#65292;&#20174;&#32780;&#34920;&#24449;&#33410;&#28857;&#21644;&#23646;&#24615;&#65292;&#24182;&#25429;&#33719;&#22270;&#32467;&#26500;&#21644;&#35821;&#20041;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#39640;&#38454;&#20869;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#23646;&#24615;&#22270;&#20013;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;"&#35821;&#20041;&#22270;&#34920;&#31034;"&#26041;&#27861;&#65292;&#23558;&#20004;&#20010;&#24322;&#26500;&#28304;&#30340;&#32852;&#21512;&#20248;&#21270;&#36716;&#21270;&#20026;&#24120;&#35265;&#30340;&#39640;&#38454;&#36317;&#31163;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36741;&#21161;&#21152;&#26435;&#22270;&#65292;&#35813;&#22270;&#20840;&#38754;&#32534;&#30721;&#21407;&#22987;&#22270;&#20013;&#33410;&#28857;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#21516;&#26500;&#21644;&#24322;&#26500;&#20851;&#31995;&#65292;&#28982;&#21518;&#23545;&#26032;&#26500;&#24314;&#30340;&#22270;&#24212;&#29992;&#20256;&#32479;&#30340;&#39640;&#38454;&#25299;&#25169;&#36317;&#31163;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#33410;&#28857;&#21644;&#23646;&#24615;&#30340;&#34920;&#31034;&#65292;&#24182;&#25429;&#33719;&#22270;&#32467;&#26500;&#21644;&#35821;&#20041;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#39640;&#38454;&#20869;&#22312;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#23398;&#20064;&#21040;&#30340;&#23646;&#24615;&#23884;&#20837;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#33719;&#27599;&#20010;&#33410;&#28857;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#20110;&#19979;&#28216;&#30340;&#22270;&#20998;&#26512;&#20219;&#21153;&#65292;&#22914;&#32858;&#31867;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the graph representation learning (a.k.a. network embedding) in attributed graphs. Different from existing embedding methods that treat the incorporation of graph structure and semantic as the simple combination of two optimization objectives, we propose a novel semantic graph representation (SGR) method to formulate the joint optimization of the two heterogeneous sources into a common high-order proximity based framework. Concretely, we first construct an auxiliary weighted graph, where the complex homogeneous and heterogeneous relations among nodes and attributes in the original graph are comprehensively encoded. Conventional embedding methods that consider high-order topology proximities can then be easily applied to the newly constructed graph to learn the representations of both node and attribute while capturing the nonlinear high-order intrinsic correlation inside or among graph structure and semantic. The learned attribute embeddings can also effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30053;&#20302;&#12290;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#12290;</title><link>http://arxiv.org/abs/2305.06530</link><description>&lt;p&gt;
&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Good are Commercial Large Language Models on African Languages?. (arXiv:2305.06530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30053;&#20302;&#12290;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20419;&#20351;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#26410;&#30693;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#20063;&#34987;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#21830;&#19994;API&#25152;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#30340;&#34920;&#29616;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#23545;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#20004;&#39033;&#20219;&#21153;&#65288;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#20998;&#31867;&#65289;&#19978;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#30340;&#24615;&#33021;&#30053;&#20302;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#65292;&#36825;&#20063;&#26159;&#38750;&#27954;&#35821;&#31181;&#36880;&#28176;&#27969;&#34892;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) has led to the proliferation of large pretrained language models. These models have been shown to yield good performance, using in-context learning, even on unseen tasks and languages. They have also been exposed as commercial APIs as a form of language-model-as-a-service, with great adoption. However, their performance on African languages is largely unknown. We present a preliminary analysis of commercial large language models on two tasks (machine translation and text classification) across eight African languages, spanning different language families and geographical areas. Our results suggest that commercial language models produce below-par performance on African languages. We also find that they perform better on text classification than machine translation. In general, our findings present a call-to-action to ensure African languages are well represented in commercial large language models, given their growing popularity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#25299;&#25169;&#26041;&#27861;&#26469;&#39044;&#27979;&#26102;&#21464;&#22270;&#20013;&#30340;&#24322;&#24120;&#28857;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#29305;&#24449;&#30340;&#32452;&#21512;&#29190;&#28856;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#26102;&#21464;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06523</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#30340;&#25299;&#25169;&#26041;&#27861;&#39044;&#27979;&#26102;&#21464;&#22270;&#30340;&#24322;&#24120;&#28857;
&lt;/p&gt;
&lt;p&gt;
A fast topological approach for predicting anomalies in time-varying graphs. (arXiv:2305.06523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#25299;&#25169;&#26041;&#27861;&#26469;&#39044;&#27979;&#26102;&#21464;&#22270;&#20013;&#30340;&#24322;&#24120;&#28857;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#29305;&#24449;&#30340;&#32452;&#21512;&#29190;&#28856;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#26102;&#21464;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26102;&#21464;&#22270;&#22312;&#37329;&#34701;&#12289;&#31038;&#20250;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#26377;&#25928;&#25552;&#21462;&#32534;&#30721;&#31232;&#30095;&#12289;&#22810;&#23618;&#12289;&#21160;&#24577;&#22270;&#22797;&#26434;&#32467;&#26500;&#30340;&#29305;&#24449;&#23384;&#22312;&#35745;&#31639;&#21644;&#26041;&#27861;&#19978;&#30340;&#25361;&#25112;&#12290;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#20013;&#30340;&#25345;&#20037;&#21270;&#22270;&#65288;PD&#65289;&#24050;&#25104;&#20026;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#36317;&#31163;&#30340;&#25968;&#25454;&#24418;&#29366;&#25551;&#36848;&#31526;&#30340;&#28909;&#38376;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;TDA&#22312;&#22270;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#26469;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#24418;&#29366;&#20449;&#24687;&#65292;&#22635;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#35859;&#30340;&#19979;&#26143;&#36807;&#28388;&#22120;&#65288;lower-star filtration&#65289;&#20351;&#29992;&#23450;&#37327;&#33410;&#28857;&#23646;&#24615;&#35745;&#31639;PD&#65292;&#28982;&#21518;&#22312;&#19968;&#32500;&#32593;&#26684;&#19978;&#36890;&#36807;&#24179;&#22343;&#30456;&#20851;&#30340;Betti&#20989;&#25968;&#26469;&#30690;&#37327;&#21270;&#23427;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#20256;&#32479;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#20013;&#29305;&#24449;&#30340;&#32452;&#21512;&#29190;&#28856;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#26102;&#21464;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large time-varying graphs are increasingly common in financial, social and biological settings. Feature extraction that efficiently encodes the complex structure of sparse, multi-layered, dynamic graphs presents computational and methodological challenges. In the past decade, a persistence diagram (PD) from topological data analysis (TDA) has become a popular descriptor of shape of data with a well-defined distance between points. However, applications of TDA to graphs, where there is no intrinsic concept of distance between the nodes, remain largely unexplored. This paper addresses this gap in the literature by introducing a computationally efficient framework to extract shape information from graph data. Our framework has two main steps: first, we compute a PD using the so-called lower-star filtration which utilizes quantitative node attributes, and then vectorize it by averaging the associated Betti function over successive scale values on a one-dimensional grid. Our approach avoids
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22788;&#29702;&#32570;&#22833;&#20540;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.06480</link><description>&lt;p&gt;
ST-GIN:&#19968;&#31181;&#20855;&#26377;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#24490;&#29615;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ST-GIN: An Uncertainty Quantification Approach in Traffic Data Imputation with Spatio-temporal Graph Attention and Bidirectional Recurrent United Neural Networks. (arXiv:2305.06480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22788;&#29702;&#32570;&#22833;&#20540;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25968;&#25454;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20174;&#29615;&#36335;&#26816;&#27979;&#22120;&#25110;&#31867;&#20284;&#26469;&#28304;&#25910;&#38598;&#30340;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#32570;&#22833;&#20540;(MVs)&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#30456;&#20851;&#24212;&#29992;&#21644;&#30740;&#31350;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#24674;&#22797;&#36825;&#20123;&#32570;&#22833;&#20540;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#25968;&#23383;&#32479;&#35745;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#25454;&#25554;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25554;&#20540;&#32570;&#22833;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22270;&#27880;&#24847;&#26550;&#26500;&#26469;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#65292;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic data serves as a fundamental component in both research and applications within intelligent transportation systems. However, real-world transportation data, collected from loop detectors or similar sources, often contain missing values (MVs), which can adversely impact associated applications and research. Instead of discarding this incomplete data, researchers have sought to recover these missing values through numerical statistics, tensor decomposition, and deep learning techniques. In this paper, we propose an innovative deep-learning approach for imputing missing data. A graph attention architecture is employed to capture the spatial correlations present in traffic data, while a bidirectional neural network is utilized to learn temporal information. Experimental results indicate that our proposed method outperforms all other benchmark techniques, thus demonstrating its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29992;&#25143;&#35780;&#20998;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#19982;&#20256;&#32479;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;&#32467;&#26524;&#21457;&#29616;LLMs&#33021;&#22815;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#24456;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.06474</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#33021;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#65311;&#22312;&#29992;&#25143;&#35780;&#20998;&#39044;&#27979;&#20219;&#21153;&#20013;&#23545;LLM&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. (arXiv:2305.06474v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29992;&#25143;&#35780;&#20998;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#19982;&#20256;&#32479;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;&#32467;&#26524;&#21457;&#29616;LLMs&#33021;&#22815;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#26480;&#20986;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#22522;&#20110;&#29992;&#25143;&#20197;&#21069;&#30340;&#34892;&#20026;&#25512;&#26029;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#33021;&#21147;&#30340;&#31243;&#24230;&#36824;&#26159;&#19968;&#20010;&#23578;&#19981;&#28165;&#26970;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#65292;&#21327;&#21516;&#36807;&#28388;(CF)&#26159;&#36825;&#20123;&#20219;&#21153;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35780;&#20998;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;LLMs&#36890;&#24120;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#21448;&#20445;&#25345;&#20102;&#27599;&#20010;&#39033;&#30446;(&#22914;&#30005;&#24433;&#25110;&#20135;&#21697;)&#30340;&#35814;&#23613;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29992;&#25143;&#35780;&#20998;&#39044;&#27979;&#36825;&#19968;&#32463;&#20856;&#20219;&#21153;&#20013;&#30340;CF&#21644;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;&#36825;&#19968;&#20219;&#21153;&#28041;&#21450;&#22522;&#20110;&#29992;&#25143;&#36807;&#21435;&#30340;&#35780;&#20998;&#39044;&#27979;&#20505;&#36873;&#39033;&#30446;&#30340;&#35780;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#65292;&#20174;250M&#21040;540B&#20010;&#21442;&#25968;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional capabilities in generalizing to new tasks in a zero-shot or few-shot manner. However, the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question. Traditionally, Collaborative Filtering (CF) has been the most effective method for these tasks, predominantly relying on the extensive volume of rating data. In contrast, LLMs typically demand considerably less data while maintaining an exhaustive world knowledge about each item, such as movies or products. In this paper, we conduct a thorough examination of both CF and LLMs within the classic task of user rating prediction, which involves predicting a user's rating for a candidate item based on their past ratings. We investigate various LLMs in different sizes, ranging from 250M to 540B parameters and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We conduct comprehen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38450;&#27490;&#26799;&#24230;&#27844;&#28431;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;SGD&#26356;&#20855;&#38544;&#31169;&#12289;&#20934;&#30830;&#24615;&#21644;&#25915;&#20987;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06473</link><description>&lt;p&gt;
&#38024;&#23545;&#26799;&#24230;&#27844;&#28431;&#23041;&#32961;&#30340;&#20998;&#24067;&#24335;SGD&#23433;&#20840;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Securing Distributed SGD against Gradient Leakage Threats. (arXiv:2305.06473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38450;&#27490;&#26799;&#24230;&#27844;&#28431;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;SGD&#26356;&#20855;&#38544;&#31169;&#12289;&#20934;&#30830;&#24615;&#21644;&#25915;&#20987;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38450;&#27490;&#26799;&#24230;&#27844;&#28431;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#23433;&#20840;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#38544;&#31169;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65306;&#65288;i&#65289;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#25110;&#20302;&#31209;&#28388;&#27874;&#30340;&#26799;&#24230;&#21098;&#26525;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20351;&#29992;&#21152;&#24615;&#38543;&#26426;&#22122;&#22768;&#25110;&#24046;&#20998;&#38544;&#31169;&#22122;&#22768;&#30340;&#26799;&#24230;&#25200;&#21160;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#22266;&#26377;&#38480;&#21046;&#20197;&#21450;&#23427;&#20204;&#23545;&#38544;&#31169;&#20445;&#35777;&#12289;&#27169;&#22411;&#20934;&#30830;&#24230;&#21644;&#25915;&#20987;&#38887;&#24615;&#30340;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#25511;&#21046;&#22122;&#22768;&#30340;&#26799;&#24230;&#27844;&#28431;&#20445;&#25252;&#26041;&#27861;-&#20445;&#25252;&#20998;&#24067;&#24335;SGD&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36319;&#36394;&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#26356;&#26032;&#36235;&#21183;&#65292;&#20351;&#24471;&#33258;&#36866;&#24212;&#22122;&#22768;&#27880;&#20837;&#22312;&#32852;&#37030;&#27169;&#22411;&#35757;&#32451;&#20013;&#20445;&#25345;&#32039;&#23494;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#38544;&#31169;&#20445;&#25252;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a holistic approach to gradient leakage resilient distributed Stochastic Gradient Descent (SGD). First, we analyze two types of strategies for privacy-enhanced federated learning: (i) gradient pruning with random selection or low-rank filtering and (ii) gradient perturbation with additive random noise or differential privacy noise. We analyze the inherent limitations of these approaches and their underlying impact on privacy guarantee, model accuracy, and attack resilience. Next, we present a gradient leakage resilient approach to securing distributed SGD in federated learning, with differential privacy controlled noise as the tool. Unlike conventional methods with the per-client federated noise injection and fixed noise parameter strategy, our approach keeps track of the trend of per-example gradient updates. It makes adaptive noise injection closely aligned throughout the federated model training. Finally, we provide an empirical privacy analysis on the privacy gu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.06472</link><description>&lt;p&gt;
ChatGPT&#24335;&#30340;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#25216;&#26415;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#35774;&#22791;&#32500;&#25252;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;PHM&#25216;&#26415;&#35782;&#21035;&#21644;&#39044;&#27979;&#35774;&#22791;&#25925;&#38556;&#21644;&#25439;&#22351;&#12290;&#29616;&#22312;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;AI&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#36825;&#31181;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24037;&#19994;&#39046;&#22495;&#65292;&#22914;&#38081;&#36335;&#12289;&#33021;&#28304;&#21644;&#33322;&#31354;&#31561;&#65292;&#20197;&#25552;&#39640;&#35774;&#22791;&#30340;&#26381;&#21153;&#23551;&#21629;&#21644;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#29983;&#20135;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#25345;&#32493;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;ConFER&#65289;&#8221;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#22312;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#65292;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#33021;&#22815;&#19981;&#26029;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#12289;&#22330;&#26223;&#31561;&#22810;&#21464;&#29615;&#22659;&#30340;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06448</link><description>&lt;p&gt;
&#25345;&#32493;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65306;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Facial Expression Recognition: A Benchmark. (arXiv:2305.06448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#25345;&#32493;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;ConFER&#65289;&#8221;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#22312;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#65292;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#33021;&#22815;&#19981;&#26029;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#12289;&#22330;&#26223;&#31561;&#22810;&#21464;&#29615;&#22659;&#30340;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#24773;&#24863;&#34892;&#20026;&#65288;&#23588;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#20013;&#65289;&#65292;&#38656;&#35201;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#27169;&#22411;&#33021;&#22815;&#19981;&#26029;&#36866;&#24212;&#20010;&#20307;&#34920;&#24773;&#12289;&#29615;&#22659;&#21450;&#32972;&#26223;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;FER&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23396;&#31435;&#39044;&#35757;&#32451;&#26080;&#27861;&#25429;&#25417;&#21040;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#32780;&#19968;&#20123;&#29983;&#21629;&#23398;&#20064;&#65292;&#22914;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#65292;&#21487;&#20197;&#20351;&#26234;&#33021;&#20307;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#38598;&#25104;&#26032;&#20449;&#24687;&#32780;&#19981;&#20250;&#24178;&#25200;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#22240;&#20026;&#29983;&#21629;&#23398;&#20064;&#22312;FER&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#25345;&#32493;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;ConFER&#65289;&#8221;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#27969;&#34892;&#30340;CL&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding human affective behaviour, especially in the dynamics of real-world settings, requires Facial Expression Recognition (FER) models to continuously adapt to individual differences in user expression, contextual attributions, and the environment. Current (deep) Machine Learning (ML)-based FER approaches pre-trained in isolation on benchmark datasets fail to capture the nuances of real-world interactions where data is available only incrementally, acquired by the agent or robot during interactions. New learning comes at the cost of previous knowledge, resulting in catastrophic forgetting. Lifelong or Continual Learning (CL), on the other hand, enables adaptability in agents by being sensitive to changing data distributions, integrating new information without interfering with previously learnt knowledge. Positing CL as an effective learning paradigm for FER, this work presents the Continual Facial Expression Recognition (ConFER) benchmark that evaluates popular CL techniques 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Transformer&#36827;&#34892;&#25233;&#37057;&#30151;&#31579;&#26597;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#29305;&#24449;&#24037;&#31243;&#20381;&#36182;&#21644;&#24573;&#30053;&#26102;&#21464;&#22240;&#32032;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.06447</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#36827;&#34892;&#25233;&#37057;&#30151;&#31579;&#26597;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Representation Learning for Depression Screening with Transformer. (arXiv:2305.06447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06447
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Transformer&#36827;&#34892;&#25233;&#37057;&#30151;&#31579;&#26597;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#29305;&#24449;&#24037;&#31243;&#20381;&#36182;&#21644;&#24573;&#30053;&#26102;&#21464;&#22240;&#32032;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#29616;&#24515;&#29702;&#38556;&#30861;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26679;&#21487;&#20197;&#21450;&#26102;&#24178;&#39044;&#21644;&#27835;&#30103;&#65292;&#20174;&#32780;&#22823;&#22823;&#25913;&#21892;&#24739;&#26377;&#20005;&#37325;&#24515;&#29702;&#30142;&#30149;&#30340;&#20010;&#20307;&#30340;&#39044;&#21518;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26368;&#36817;&#20986;&#29616;&#30340;&#24515;&#29702;&#20581;&#24247;&#35752;&#35770;&#30340;&#28608;&#22686;&#20026;&#30740;&#31350;&#24515;&#29702;&#20581;&#24247;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#24182;&#26377;&#21487;&#33021;&#26816;&#27979;&#21040;&#24515;&#29702;&#30142;&#30149;&#30340;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#32780;&#21463;&#21040;&#38480;&#21046;&#65306;(1)&#20381;&#36182;&#20110;&#29305;&#24449;&#24037;&#31243;&#65292;(2)&#27809;&#26377;&#32771;&#34385;&#26102;&#21464;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#20854;&#20013;&#20005;&#37325;&#20381;&#36182;&#20110;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#25968;&#37327;&#12289;&#36136;&#37327;&#21644;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#26102;&#21464;&#22240;&#32032;&#23545;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#20363;&#22914;&#31038;&#20132;&#23186;&#20307;&#19978;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20135;&#29983;&#30340;&#35821;&#35328;&#27169;&#24335;&#21644;&#20154;&#38469;&#20114;&#21160;&#34892;&#20026;&#30340;&#21160;&#24577;&#21464;&#21270;(&#20363;&#22914;&#22238;&#22797;&#12289;&#25552;&#21450;&#21644;&#24341;&#29992;&#25512;&#25991;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of mental disorder is crucial as it enables prompt intervention and treatment, which can greatly improve outcomes for individuals suffering from debilitating mental affliction. The recent proliferation of mental health discussions on social media platforms presents research opportunities to investigate mental health and potentially detect instances of mental illness. However, existing depression detection methods are constrained due to two major limitations: (1) the reliance on feature engineering and (2) the lack of consideration for time-varying factors. Specifically, these methods require extensive feature engineering and domain knowledge, which heavily rely on the amount, quality, and type of user-generated content. Moreover, these methods ignore the important impact of time-varying factors on depression detection, such as the dynamics of linguistic patterns and interpersonal interactive behaviors over time on social media (e.g., replies, mentions, and quote-tweets)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.06446</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;: &#24322;&#27493;&#36890;&#20449;&#21644;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35774;&#32622;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20197;&#21512;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#21516;&#26102;&#30830;&#20445;&#21512;&#20316;&#20248;&#21183;&#19988;&#36890;&#20449;&#24320;&#38144;&#20302;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377; $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ &#30340;&#36951;&#25022;&#20540;&#21644; $\tilde{\mathcal{O}}(dHM^2)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013; $d$ &#26159;&#29305;&#24449;&#32500;&#25968;&#65292;$H$ &#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$M$ &#26159;&#26234;&#33021;&#20307;&#24635;&#25968;&#65292;$K$ &#26159;&#24635;&#24773;&#33410;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#38480;&#35777;&#26126;&#65292;&#34920;&#26126;&#36890;&#36807;&#21327;&#20316;&#33267;&#23569;&#38656;&#35201; $\Omega(dM)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#25165;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;K-12&#25945;&#32946;&#20013;&#23398;&#29983;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23398;&#20064;&#38590;&#24230;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#23398;&#27010;&#24565;&#65292;&#21516;&#26102;&#27880;&#37325;&#27010;&#24565;&#29702;&#35299;&#19982;&#23454;&#38469;&#24212;&#29992;&#65292;&#22522;&#20110;&#20915;&#31574;&#26641;&#23398;&#20064;&#65292;&#26088;&#22312;&#36171;&#20104;&#23398;&#29983;&#31215;&#26497;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#21453;&#24605;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06442</link><description>&lt;p&gt;
&#25968;&#25454;&#12289;&#26641;&#21644;&#26862;&#26519;&#8212;&#8212;K-12&#25945;&#32946;&#20013;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data, Trees, and Forests -- Decision Tree Learning in K-12 Education. (arXiv:2305.06442v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06442
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;K-12&#25945;&#32946;&#20013;&#23398;&#29983;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23398;&#20064;&#38590;&#24230;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#23398;&#27010;&#24565;&#65292;&#21516;&#26102;&#27880;&#37325;&#27010;&#24565;&#29702;&#35299;&#19982;&#23454;&#38469;&#24212;&#29992;&#65292;&#22522;&#20110;&#20915;&#31574;&#26641;&#23398;&#20064;&#65292;&#26088;&#22312;&#36171;&#20104;&#23398;&#29983;&#31215;&#26497;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#21453;&#24605;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#23545;&#25105;&#20204;&#29983;&#27963;&#30340;&#24433;&#21709;&#36234;&#26469;&#36234;&#22823;&#65292;&#27599;&#20010;&#20154;&#37117;&#38656;&#35201;&#29702;&#35299;&#30456;&#24212;&#29616;&#35937;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#21442;&#19982;&#22609;&#36896;&#25105;&#20204;&#30340;&#19990;&#30028;&#24182;&#20570;&#20986;&#20851;&#20110;&#23545;&#31038;&#20250;&#24433;&#21709;&#30340;&#30693;&#24773;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#22312;K-12&#25945;&#32946;&#20013;&#65292;&#23398;&#29983;&#38656;&#35201;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#30340;&#26680;&#24515;&#24605;&#24819;&#21644;&#21407;&#21017;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#30446;&#26631;&#32676;&#20307;&#26469;&#35828;&#65292;&#23454;&#29616;&#25152;&#26377;&#19978;&#36848;&#30446;&#26631;&#37117;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#23398;&#27010;&#24565;&#65292;&#23558;&#27880;&#37325;&#27010;&#24565;&#29702;&#35299;&#30340;&#26377;&#36259;&#26131;&#25026;&#30340;&#38750;&#25554;&#20214;&#26041;&#27861;&#19982;&#36171;&#20104;&#23398;&#29983;&#31215;&#26497;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#21453;&#24605;&#20854;&#23545;&#31038;&#20250;&#24433;&#21709;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#22522;&#20110;&#20915;&#31574;&#26641;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a consequence of the increasing influence of machine learning on our lives, everyone needs competencies to understand corresponding phenomena, but also to get involved in shaping our world and making informed decisions regarding the influences on our society. Therefore, in K-12 education, students need to learn about core ideas and principles of machine learning. However, for this target group, achieving all of the aforementioned goals presents an enormous challenge. To this end, we present a teaching concept that combines a playful and accessible unplugged approach focusing on conceptual understanding with empowering students to actively apply machine learning methods and reflect their influence on society, building upon decision tree learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.06435</link><description>&lt;p&gt;
&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#29616;&#22312;&#38750;&#24120;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#23569;&#23450;&#37327;&#35299;&#37322;&#26368;&#20339;&#23567;&#25209;&#37327;&#22823;&#23567;&#24212;&#35813;&#26159;&#22810;&#22823;&#30340;&#29702;&#35770;&#12290;&#26412;&#25991;&#23581;&#35797;&#31995;&#32479;&#22320;&#29702;&#35299;&#23567;&#25209;&#37327;&#22823;&#23567;&#22312;&#35757;&#32451;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#25945;&#24072;-&#23398;&#29983;&#24773;&#22659;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#25945;&#24072;&#65292;&#24182;&#32858;&#28966;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;m&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#30340;&#27867;&#21270;&#24615;&#33021;&#24378;&#28872;&#20381;&#36182;&#20110;m&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#20020;&#30028;&#20540;mc&#22788;&#32463;&#21382;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#36825;&#26679;&#24403;m&lt; mc&#26102;&#65292;&#35757;&#32451;&#36807;&#31243;&#22833;&#36133;&#65292;&#32780;&#24403;m&gt; mc&#26102;&#65292;&#23398;&#29983;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;&#25110;&#24456;&#22909;&#22320;&#27867;&#21270;&#25945;&#24072;&#12290;&#30456;&#21464;&#26159;&#30001;&#32479;&#35745;&#21147;&#23398;&#39318;&#27425;&#21457;&#29616;&#30340;&#38598;&#20307;&#29616;&#35937;&#65292;&#24182;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#35266;&#23519;&#21040;&#12290;&#25214;&#21040;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;&#65292;&#21487;&#20197;&#38416;&#26126;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m&lt;m_c$ the training process fails, while for $m&gt;m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#35821;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#22270;&#22806;&#25991;&#26723;&#26102;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#65307;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#34920;&#26126;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#32852;&#21512;&#24314;&#27169;&#35789;&#32423;&#21644;&#25991;&#26723;&#32423;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06434</link><description>&lt;p&gt;
&#22522;&#20110;&#35789;&#35821;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Word Grounded Graph Convolutional Network. (arXiv:2305.06434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#35821;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#22270;&#22806;&#25991;&#26723;&#26102;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#65307;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#34920;&#26126;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#32852;&#21512;&#24314;&#27169;&#35789;&#32423;&#21644;&#25991;&#26723;&#32423;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#24314;&#27169;&#22270;&#32467;&#26500;&#25968;&#25454;&#65288;&#22914;&#25991;&#29486;&#24341;&#29992;&#32593;&#32476;&#65289;&#26041;&#38754;&#65292;&#23545;&#20110;&#21508;&#31181;&#20219;&#21153;&#22914;&#25991;&#26412;&#20998;&#31867;&#31561;&#37117;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCNs&#20165;&#38480;&#20110;&#22788;&#29702;&#39044;&#23450;&#20041;&#22270;&#20013;&#30340;&#25991;&#26723;&#65292;&#21363;&#19981;&#33021;&#25512;&#24191;&#21040;&#22270;&#22806;&#25991;&#26723;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#26723;&#22270;&#36716;&#21270;&#20026;&#35789;&#22270;&#65292;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#20110;&#25991;&#26723;&#30340;&#22270;&#26469;&#35299;&#32806;&#25968;&#25454;&#26679;&#26412;&#65288;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#25991;&#26723;&#65289;&#21644;GCN&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#35789;&#32423;&#30340;GCN&#22240;&#27492;&#21487;&#20197;&#33258;&#28982;&#22320;&#24402;&#32435;&#22320;&#25512;&#29702;&#20986;&#22270;&#22806;&#25991;&#26723;&#12290;&#25552;&#20986;&#20102;&#22522;&#20110;WGraph&#30340;&#24402;&#32435;&#35789;&#35821;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;WGCN&#65289;&#65292;&#23427;&#21487;&#20197;&#23545;&#21333;&#35789;&#32423;&#25991;&#26412;&#23454;&#20363;&#65288;&#22914;&#19981;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26723;&#65289;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WGCN&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#26041;&#27861;&#32852;&#21512;&#24314;&#27169;&#35789;&#32423;&#21644;&#25991;&#26723;&#32423;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) have shown strong performance in learning text representations for various tasks such as text classification, due to its expressive power in modeling graph structure data (e.g., a literature citation network). Most existing GCNs are limited to deal with documents included in a pre-defined graph, i.e., it cannot be generalized to out-of-graph documents. To address this issue, we propose to transform the document graph into a word graph, to decouple data samples (i.e., documents in training and test sets) and a GCN model by using a document-independent graph. Such word-level GCN could therefore naturally inference out-of-graph documents in an inductive way. The proposed Word-level Graph (WGraph) can not only implicitly learning word presentation with commonly-used word co-occurrences in corpora, but also incorporate extra global semantic dependency derived from inter-document relationships (e.g., literature citations). An inductive Word-grounded Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;MC&#26041;&#27861;&#19982;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.06432</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#39118;&#38505;&#27010;&#29575;&#20272;&#35745;&#30340;&#21487;&#25512;&#24191;&#12289;&#29289;&#29702;&#23398;&#22522;&#30784;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalizable Physics-informed Learning Framework for Risk Probability Estimation. (arXiv:2305.06432v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;MC&#26041;&#27861;&#19982;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#23545;&#20110;&#35768;&#22810;&#38543;&#26426;&#23433;&#20840;&#25511;&#21046;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#21644;&#26410;&#30693;&#25110;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#35745;&#31639;&#36825;&#20123;&#39118;&#38505;&#27010;&#29575;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#28385;&#36275;&#26576;&#20123;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#20107;&#23454;&#65292;&#35813;&#26041;&#31243;&#34920;&#24449;&#20102;&#27010;&#29575;&#20043;&#38388;&#30340;&#37051;&#36817;&#20851;&#31995;&#65292;&#20197;&#23558;MC&#26041;&#27861;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#29305;&#23450;&#35757;&#32451;&#37197;&#32622;&#19979;&#32473;&#20986;&#20272;&#35745;&#35823;&#24046;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#30693;&#21306;&#22495;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#65292;&#30456;&#27604;MC&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23427;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate estimates of long-term risk probabilities and their gradients are critical for many stochastic safe control methods. However, computing such risk probabilities in real-time and in unseen or changing environments is challenging. Monte Carlo (MC) methods cannot accurately evaluate the probabilities and their gradients as an infinitesimal devisor can amplify the sampling noise. In this paper, we develop an efficient method to evaluate the probabilities of long-term risk and their gradients. The proposed method exploits the fact that long-term risk probability satisfies certain partial differential equations (PDEs), which characterize the neighboring relations between the probabilities, to integrate MC methods and physics-informed neural networks. We provide theoretical guarantees of the estimation error given certain choices of training configurations. Numerical results show the proposed method has better sample efficiency, generalizes well to unseen regions, and can adapt to sys
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#27169;&#22411;&#36827;&#34892;&#21307;&#38498;&#36807;&#31243;&#23567;&#32467;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#20197;&#32531;&#35299;&#21307;&#29983;&#36807;&#21171;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24615;&#19978;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#30450;&#35780;&#20272;&#34920;&#26126;62%&#30340;&#33258;&#21160;&#21270;&#25688;&#35201;&#31526;&#21512;&#26631;&#20934;&#65292;&#20855;&#26377;&#20020;&#24202;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06416</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#31070;&#32463;&#31185;&#24739;&#32773;&#20986;&#38498;&#23567;&#32467;&#21307;&#38498;&#36807;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method to Automate the Discharge Summary Hospital Course for Neurology Patients. (arXiv:2305.06416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06416
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#27169;&#22411;&#36827;&#34892;&#21307;&#38498;&#36807;&#31243;&#23567;&#32467;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#20197;&#32531;&#35299;&#21307;&#29983;&#36807;&#21171;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24615;&#19978;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#30450;&#35780;&#20272;&#34920;&#26126;62%&#30340;&#33258;&#21160;&#21270;&#25688;&#35201;&#31526;&#21512;&#26631;&#20934;&#65292;&#20855;&#26377;&#20020;&#24202;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20020;&#24202;&#31508;&#35760;&#30340;&#29983;&#25104;&#34987;&#25552;&#20986;&#20316;&#20026;&#32531;&#35299;&#21307;&#29983;&#36807;&#21171;&#30340;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30149;&#20154;&#20303;&#38498;&#26399;&#38388;&#33258;&#21160;&#21270;&#21465;&#36848;&#24615;&#25688;&#35201;&#21487;&#20316;&#20026;&#20303;&#38498;&#21307;&#29983;&#22312;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#35760;&#24405;&#30340;&#20986;&#38498;&#23567;&#32467;&#20013;&#30340;&#21307;&#38498;&#36807;&#31243;&#37096;&#20998;&#30340;&#34917;&#20805;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#27169;&#22411;&#36827;&#34892;&#21307;&#38498;&#36807;&#31243;&#23567;&#32467;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;BERT&#21644;BART&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#24182;&#36890;&#36807;&#38480;&#21046;&#27874;&#26463;&#25628;&#32034;&#36827;&#34892;&#20102;&#23454;&#38469;&#24615;&#20248;&#21270;&#65292;&#37319;&#29992;&#20102;&#20174;&#23398;&#26415;&#21307;&#30103;&#20013;&#24515;&#31070;&#32463;&#31185;&#30149;&#20154;&#30340;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;ROUGE&#20998;&#25968;&#21644;13.76&#30340;R-2&#12290;&#22312;&#30450;&#35780;&#20272;&#20013;&#65292;&#20004;&#20301;&#33891;&#20107;&#20250;&#35748;&#35777;&#30340;&#21307;&#29983;&#35780;&#20215;62%&#30340;&#33258;&#21160;&#21270;&#25688;&#35201;&#31526;&#21512;&#26631;&#20934;&#65292;&#36825;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#33021;&#22312;&#20020;&#24202;&#19978;&#26377;&#29992;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#30740;&#31350;&#26159;&#20854;&#20013;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of automated clinical notes have been posited as a strategy to mitigate physician burnout. In particular, an automated narrative summary of a patient's hospital stay could supplement the hospital course section of the discharge summary that inpatient physicians document in electronic health record (EHR) systems. In the current study, we developed and evaluated an automated method for summarizing the hospital course section using encoder-decoder sequence-to-sequence transformer models. We fine tuned BERT and BART models and optimized for factuality through constraining beam search, which we trained and tested using EHR data from patients admitted to the neurology unit of an academic medical center. The approach demonstrated good ROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certified physicians rated 62% of the automated summaries as meeting the standard of care, which suggests the method may be useful clinically. To our knowledge, this study is among th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21363;&#27704;&#32493;&#24615;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#65292;&#36890;&#36807;&#20559;&#21521;&#20808;&#21069;&#26631;&#35760;&#38598;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#22238;&#25918;&#26041;&#26696;&#65292;&#21253;&#25324;&#27169;&#22411;&#33976;&#39311;&#21644;&#20174;&#21382;&#21490;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#21644;&#19981;&#30830;&#23450;&#30340;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAL&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06408</link><description>&lt;p&gt;
&#20351;&#29992;&#27704;&#32493;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#25209;&#27425;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Batch Active Learning Using Continual Learning Techniques. (arXiv:2305.06408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21363;&#27704;&#32493;&#24615;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#65292;&#36890;&#36807;&#20559;&#21521;&#20808;&#21069;&#26631;&#35760;&#38598;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#22238;&#25918;&#26041;&#26696;&#65292;&#21253;&#25324;&#27169;&#22411;&#33976;&#39311;&#21644;&#20174;&#21382;&#21490;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#21644;&#19981;&#30830;&#23450;&#30340;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAL&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#35757;&#32451;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#27169;&#22411;&#36890;&#24120;&#22312;&#27599;&#20010;&#26597;&#35810;&#36718;&#20043;&#21518;&#37117;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#39318;&#20808;&#28436;&#31034;&#20102;&#20351;&#29992;&#39044;&#28909;&#21551;&#21160;&#30340;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;AL&#30340;&#22833;&#36133;&#65292;&#26082;&#19981;&#33021;&#21152;&#36895;&#35757;&#32451;&#65292;&#21448;&#19981;&#33021;&#36991;&#20813;&#22312;AL&#26597;&#35810;&#36718;&#19978;&#20351;&#29992;&#24494;&#35843;&#26102;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31867;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20559;&#21521;&#20808;&#21069;&#26631;&#35760;&#38598;&#26469;&#21152;&#36895;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#21644;&#21457;&#23637;&#26032;&#30340;&#22522;&#20110;&#22238;&#25918;&#30340;&#27704;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31639;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#30693;&#35782;&#32780;&#19981;&#36951;&#24536;&#32769;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#25968;&#25454;&#26469;&#33258;&#19981;&#26029;&#21464;&#21270;&#30340;&#20998;&#24067;&#26102;&#29305;&#21035;&#26377;&#25928;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#33539;&#20363;&#31216;&#20026;&#8220;&#27704;&#32493;&#24615;&#20027;&#21160;&#23398;&#20064;&#8221;&#65288;CAL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CAL&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#22238;&#25918;&#26041;&#26696;&#26469;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#36825;&#20123;&#26041;&#26696;&#20351;&#29992;&#27169;&#22411;&#33976;&#39311;&#24182;&#20174;&#21382;&#21490;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#21644;&#19981;&#30830;&#23450;&#30340;&#28857;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#25968;&#25454;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#33258;&#28982;
&lt;/p&gt;
&lt;p&gt;
A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm Continual Active Learning (CAL). We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse, uncertain points from the history. We conduct experiments across many data domains, including natura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.06395</link><description>&lt;p&gt;
ACTC: &#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;(KGC)&#20381;&#36182;&#20110;&#20272;&#35745;&#24471;&#20998;&#27169;&#22411;(&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23454;&#20307;)-&#20803;&#32452;&#65292;&#20363;&#22914;&#65292;&#36890;&#36807;&#23884;&#20837;&#21021;&#22987;&#30693;&#35782;&#22270;&#12290;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#38408;&#20540;(&#20351;&#29992;&#25163;&#21160;&#27880;&#37322;&#30340;&#31034;&#20363;)&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#27979;&#36136;&#37327;&#12290;&#26412;&#25991;&#23581;&#35797;&#39318;&#27425;&#38024;&#23545;KGC&#36827;&#34892;&#20919;&#21551;&#21160;&#26657;&#20934;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#21021;&#22987;&#27809;&#26377;&#27880;&#37322;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#21482;&#33021;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;ACTC&#22522;&#20110;&#26377;&#38480;&#30340;&#27880;&#37322;&#20803;&#32452;&#26377;&#25928;&#22320;&#25214;&#21040;&#22909;&#30340;&#27599;&#20010;&#20851;&#31995;&#30340;&#38408;&#20540;&#12290;&#38500;&#20102;&#19968;&#20123;&#27880;&#37322;&#30340;&#20803;&#32452;&#22806;&#65292;ACTC&#36824;&#21033;&#29992;Logistic&#22238;&#24402;&#25110;&#39640;&#26031;&#36807;&#31243;&#20998;&#31867;&#22120;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20803;&#32452;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23494;&#24230;&#21644;&#38543;&#26426;&#36873;&#25321;&#31561;&#19981;&#21516;&#26041;&#27861;&#36873;&#25321;&#20505;&#36873;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#35780;&#20998;&#27169;&#22411;&#21644;&#19968;&#20010;oracle&#27880;&#37322;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;"Text-To-Concept"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#36716;&#25442;&#20026;&#21487;&#19982;&#25991;&#26412;&#32534;&#30721;&#22120;&#27604;&#36739;&#30340;&#26631;&#20934;&#21270;&#24418;&#24335;&#65292;&#24182;&#20813;&#36153;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#36716;&#25442;&#20026;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.06386</link><description>&lt;p&gt;
&#36328;&#27169;&#22411;&#23545;&#40784;&#23454;&#29616;&#25991;&#26412;&#21040;&#27010;&#24565;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Text-To-Concept (and Back) via Cross-Model Alignment. (arXiv:2305.06386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;"Text-To-Concept"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#36716;&#25442;&#20026;&#21487;&#19982;&#25991;&#26412;&#32534;&#30721;&#22120;&#27604;&#36739;&#30340;&#26631;&#20934;&#21270;&#24418;&#24335;&#65292;&#24182;&#20813;&#36153;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#36716;&#25442;&#20026;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35266;&#23519;&#21040;&#21363;&#20351;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#65292;&#22270;&#20687;&#34920;&#31034;&#30340;&#26144;&#23556;&#20063;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#32447;&#24615;&#23618;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;"Text-To-Concept"&#65292;&#20854;&#20013;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#19982;CLIP&#31354;&#38388;&#36827;&#34892;&#32447;&#24615;&#23545;&#40784;&#65292;&#20351;&#24471;&#26469;&#33258;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#25991;&#26412;&#23884;&#20837;&#21487;&#30452;&#25509;&#19982;&#23545;&#40784;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;Text-To-Concept&#36716;&#25442;&#65292;&#21487;&#20813;&#36153;&#23558;&#22266;&#23450;&#30340;&#29616;&#25104;&#35270;&#35273;&#32534;&#30721;&#22120;&#36716;&#25442;&#20026;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;CLIP&#30340;&#31934;&#24230;&#65292;&#21363;&#20351;&#36825;&#20123;&#32534;&#30721;&#22120;&#27604;CLIP&#23567;&#24471;&#22810;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;CLIP&#65292;&#35757;&#32451;&#25968;&#25454;&#20165;&#21344;&#24456;&#23567;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;Text-To-Concept&#30340;&#20854;&#20182;&#30452;&#25509;&#24212;&#29992;&#65306;&#22914;&#26500;&#24314;&#19981;&#38656;&#35201;&#27010;&#24565;&#30417;&#30563;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#31867;&#27010;&#24565;&#35786;&#26029;&#20998;&#24067;&#31227;&#20301;&#65292;&#24182;&#26816;&#32034;&#28385;&#36275;&#19968;&#32452;&#22522;&#20110;&#25991;&#26412;&#30340;&#32422;&#26463;&#26465;&#20214;&#30340;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;&#29305;&#24449;&#23545;&#20934;&#30830;&#23454;&#29616;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe that the mapping between an image's representation in one model to its representation in another can be learned surprisingly well with just a linear layer, even across diverse models. Building on this observation, we propose $\textit{text-to-concept}$, where features from a fixed pretrained model are aligned linearly to the CLIP space, so that text embeddings from CLIP's text encoder become directly comparable to the aligned features. With text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly strong zero-shot classifiers for free, with accuracy at times even surpassing that of CLIP, despite being much smaller models and trained on a small fraction of the data compared to CLIP. We show other immediate use-cases of text-to-concept, like building concept bottleneck models with no concept supervision, diagnosing distribution shifts in terms of human concepts, and retrieving images satisfying a set of text-based constraints. Lastly, we demonstrate the fe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#21457;&#25496;&#26368;&#20339;&#37327;&#23376;&#32416;&#38169;&#30721;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#20195;&#29702;&#20197;&#26368;&#22823;&#21270;&#32534;&#30721;&#36317;&#31163;&#25110;&#26368;&#23567;&#21270;&#22312;&#20559;&#32622;Pauli&#22122;&#22768;&#19979;&#30340;&#36923;&#36753;&#38169;&#35823;&#27010;&#29575;&#12290;&#20316;&#32773;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;CSS&#20195;&#30721;&#30456;&#27604;&#65292;&#20182;&#20204;&#21487;&#20197;&#38480;&#21046;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#24182;&#33719;&#24471;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06378</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21457;&#25496;&#26368;&#20339;&#37327;&#23376;&#32416;&#38169;&#30721;&#30340;&#26041;&#27861;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovery of Optimal Quantum Error Correcting Codes via Reinforcement Learning. (arXiv:2305.06378v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#21457;&#25496;&#26368;&#20339;&#37327;&#23376;&#32416;&#38169;&#30721;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#20195;&#29702;&#20197;&#26368;&#22823;&#21270;&#32534;&#30721;&#36317;&#31163;&#25110;&#26368;&#23567;&#21270;&#22312;&#20559;&#32622;Pauli&#22122;&#22768;&#19979;&#30340;&#36923;&#36753;&#38169;&#35823;&#27010;&#29575;&#12290;&#20316;&#32773;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;CSS&#20195;&#30721;&#30456;&#27604;&#65292;&#20182;&#20204;&#21487;&#20197;&#38480;&#21046;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#24182;&#33719;&#24471;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#37327;&#23376;&#20048;&#39640;&#26694;&#26550;&#20026;&#21033;&#29992;&#31616;&#21333;&#30340;&#26041;&#27861;&#29983;&#25104;&#22797;&#26434;&#30340;&#37327;&#23376;&#32416;&#38169;&#30721;&#65288;QECC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#21464;&#25104;&#20102;&#19968;&#20010;&#28216;&#25103;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#38145;&#20102;&#19968;&#31181;&#35774;&#35745;&#21644;&#21457;&#25496;&#32534;&#30721;&#30340;&#26032;&#36884;&#24452;&#12290; RL&#30340;&#19968;&#20010;&#22909;&#22788;&#26159;&#25105;&#20204;&#21487;&#20197;&#25351;&#23450;&#24453;&#20248;&#21270;&#30340;&#32534;&#30721;&#30340;\textit{&#20219;&#24847;}&#23646;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#31181;&#36825;&#26679;&#30340;&#23646;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#22823;&#21270;&#32534;&#30721;&#36317;&#31163;&#21644;&#22312;&#20559;&#32622;Pauli&#22122;&#22768;&#19979;&#26368;&#23567;&#21270;&#36923;&#36753;&#38169;&#35823;&#30340;&#27010;&#29575;&#12290;&#38024;&#23545;&#31532;&#19968;&#20010;&#23646;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35757;&#32451;&#30340;&#20195;&#29702;&#33021;&#22815;&#35782;&#21035;&#22686;&#21152;&#32534;&#30721;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36229;&#36807;&#21407;&#22987;&#30340;&#32423;&#32852;&#26041;&#27861;&#65292;&#22312;13&#20010;&#37327;&#23376;&#27604;&#29305;&#19978;&#39281;&#21644;&#32447;&#24615;&#32534;&#31243;&#30028;&#38480;&#30340;CSS&#32534;&#30721;&#12290;&#23545;&#20110;&#23398;&#20064;&#30446;&#26631;&#26159;&#22312;&#20559;&#32622;Pauli&#22122;&#22768;&#19979;&#26368;&#23567;&#21270;&#36923;&#36753;&#38169;&#35823;&#27010;&#29575;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#26368;&#22909;&#30340;&#24050;&#30693;CSS&#20195;&#30721;&#65292;&#38024;&#23545;$\lesssim 20$&#20010;&#37327;&#23376;&#27604;&#29305;&#12290;&#19982;&#20854;&#20182;&#65288;&#23616;&#37096;&#21464;&#24418;&#30340;&#65289;CSS&#20195;&#30721;&#65292;&#21253;&#25324;Surface&#65292;XZZX&#21644;2D Color codes&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;$[[17,1,3]]$&#32534;&#30721;&#26500;&#36896;&#23454;&#38469;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently introduced Quantum Lego framework provides a powerful method for generating complex quantum error correcting codes (QECCs) out of simple ones. We gamify this process and unlock a new avenue for code design and discovery using reinforcement learning (RL). One benefit of RL is that we can specify \textit{arbitrary} properties of the code to be optimized. We train on two such properties, maximizing the code distance, and minimizing the probability of logical error under biased Pauli noise. For the first, we show that the trained agent identifies ways to increase code distance beyond naive concatenation, saturating the linear programming bound for CSS codes on 13 qubits. With a learning objective to minimize the logical error probability under biased Pauli noise, we find the best known CSS code at this task for $\lesssim 20$ qubits. Compared to other (locally deformed) CSS codes, including Surface, XZZX, and 2D Color codes, our $[[17,1,3]]$ code construction actually has \text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#35745;&#21010;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38271;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06361</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Multi-task Neural Solver with Multi-armed Bandits. (arXiv:2305.06361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#35745;&#21010;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38271;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22914;&#20309;&#39640;&#25928;&#22320;&#20026;&#21508;&#31181;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064; (COP) &#35757;&#32451;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#19979;&#30340;&#22810;&#20219;&#21153;&#29702;&#35770;&#25439;&#22833;&#20998;&#35299;&#65292;&#36890;&#36807;&#19968;&#20010;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36890;&#36807;&#27491;&#30830;&#30340;&#36172;&#21338;&#31639;&#27861;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#30456;&#27604;&#26631;&#20934;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#27573;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#36825;&#21487;&#20197;&#20026;&#20854;&#20182;&#22810;&#20219;&#21153;&#22823;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#25552;&#20379;&#25351;&#23548;&#65292;&#27492;&#22806;&#65292;&#24433;&#21709;&#30697;&#38453;&#21487;&#20197;&#25552;&#20379;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#24120;&#35265;&#23454;&#36341;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#20174;&#32780;&#25903;&#25345;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. In this paper, we propose a general and efficient training paradigm based on multi-armed bandits to deliver a unified multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. Our method achieves much higher overall performance with either limited training budgets or the same training epochs, compared to standard training schedules, which can be promising for advising efficient training of other multi-task large models. Additionally, the influence matrix can provide empirical evidence of some common practices in the area of learning to optimize, which in turn supports the validity of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#20855;&#26377;4D&#21160;&#24577;&#22330;&#26223;&#34920;&#31034;&#30340;HumanRF&#33021;&#22815;&#20174;&#22810;&#35270;&#35282;&#35270;&#39057;&#36755;&#20837;&#20013;&#25429;&#25417;&#20840;&#36523;&#22806;&#35980;&#65292;&#20197;&#39640;&#21387;&#32553;&#29575;&#25429;&#25417;&#31934;&#32454;&#32454;&#33410;&#24182;&#25903;&#25345;&#39640;&#20998;&#36776;&#29575;&#12290;ActorsHQ&#25552;&#20379;&#20102;12MP&#30340;&#38236;&#22836;&#65292;&#20026;&#38271;&#24207;&#21015;&#33719;&#24471;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#20154;&#29289;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.06356</link><description>&lt;p&gt;
HumanRF&#65306;&#29992;&#20110;&#36816;&#21160;&#20013;&#20154;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion. (arXiv:2305.06356v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06356
&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;4D&#21160;&#24577;&#22330;&#26223;&#34920;&#31034;&#30340;HumanRF&#33021;&#22815;&#20174;&#22810;&#35270;&#35282;&#35270;&#39057;&#36755;&#20837;&#20013;&#25429;&#25417;&#20840;&#36523;&#22806;&#35980;&#65292;&#20197;&#39640;&#21387;&#32553;&#29575;&#25429;&#25417;&#31934;&#32454;&#32454;&#33410;&#24182;&#25903;&#25345;&#39640;&#20998;&#36776;&#29575;&#12290;ActorsHQ&#25552;&#20379;&#20102;12MP&#30340;&#38236;&#22836;&#65292;&#20026;&#38271;&#24207;&#21015;&#33719;&#24471;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#20154;&#29289;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#22914;&#30005;&#24433;&#21046;&#20316;&#12289;&#30005;&#33041;&#28216;&#25103;&#25110;&#35270;&#39057;&#20250;&#35758;&#20013;&#65292;&#39640;&#20445;&#30495;&#22320;&#34920;&#29616;&#20154;&#31867;&#34920;&#29616;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26500;&#24314;&#22359;&#12290;&#20026;&#20102;&#25509;&#36817;&#29983;&#20135;&#32423;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HumanRF&#65292;&#36825;&#26159;&#19968;&#20010;4D&#21160;&#24577;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#65292;&#20174;&#22810;&#35270;&#35282;&#35270;&#39057;&#36755;&#20837;&#20013;&#25429;&#25417;&#36816;&#21160;&#20013;&#30340;&#20840;&#36523;&#22806;&#35980;&#65292;&#24182;&#20351;&#20854;&#21487;&#20197;&#22312;&#26032;&#30340;&#12289;&#30475;&#19981;&#35265;&#30340;&#35270;&#35282;&#19979;&#25773;&#25918;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#34920;&#31034;&#20316;&#20026;&#19968;&#20010;&#21160;&#24577;&#35270;&#39057;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#26102;&#31354;&#20998;&#35299;&#20026;&#19968;&#20010;&#26102;&#38388;&#30697;&#38453;&#21521;&#37327;&#20998;&#35299;&#65292;&#20197;&#39640;&#21387;&#32553;&#29575;&#25429;&#25417;&#31934;&#32454;&#32454;&#33410;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#20026;&#38271;&#24207;&#21015;&#33719;&#24471;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#20154;&#29289;&#37325;&#24314;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21160;&#20316;&#24773;&#20917;&#19979;&#34920;&#31034;&#39640;&#20998;&#36776;&#29575;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#21512;&#25104;4MP&#25110;&#26356;&#20302;&#20998;&#36776;&#29575;&#65292;&#20294;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;12MP&#19978;&#25805;&#20316;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ActorsHQ&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#65292;&#20026;160&#20010;&#25668;&#20687;&#26426;&#25552;&#20379;&#20102;12MP&#30340;&#38236;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#37327;&#23376;&#21464;&#20998;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QVK-SVM&#65289;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#12289;&#25439;&#22833;&#21644;&#28151;&#28102;&#30697;&#38453;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#23427;&#24212;&#35813;&#25104;&#20026;&#26410;&#26469;QML&#30740;&#31350;&#20013;&#30340;&#21487;&#38752;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.06063</link><description>&lt;p&gt;
&#36890;&#36807;&#35823;&#24046;&#20989;&#25968;&#26680;&#35757;&#32451;&#22686;&#24378;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Enhancing Quantum Support Vector Machines through Variational Kernel Training. (arXiv:2305.06063v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#37327;&#23376;&#21464;&#20998;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QVK-SVM&#65289;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#12289;&#25439;&#22833;&#21644;&#28151;&#28102;&#30697;&#38453;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#23427;&#24212;&#35813;&#25104;&#20026;&#26410;&#26469;QML&#30740;&#31350;&#20013;&#30340;&#21487;&#38752;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#25104;&#20026;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#29616;&#26377;&#30340;QSVM&#26041;&#27861;&#65306;&#37327;&#23376;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QK-SVM&#65289;&#21644;&#37327;&#23376;&#21464;&#20998;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QV-SVM&#65289;&#12290;&#34429;&#28982;&#20004;&#31181;&#26041;&#27861;&#22343;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#37327;&#23376;&#21464;&#20998;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QVK-SVM&#65289;&#65292;&#20197;&#22686;&#24378;&#20934;&#30830;&#24615;&#65292;&#34701;&#21512;&#20102;QK-SVM&#21644;QV-SVM&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#22312;&#40482;&#23614;&#33457;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;QVK-SVM&#22312;&#20934;&#30830;&#24615;&#12289;&#25439;&#22833;&#21644;&#28151;&#28102;&#30697;&#38453;&#25351;&#26631;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;QVK-SVM&#20316;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#12289;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;QML&#24212;&#29992;&#24037;&#20855;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#26410;&#26469;&#30340;QML&#30740;&#31350;&#20013;&#37319;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning (QML) has witnessed immense progress recently, with quantum support vector machines (QSVMs) emerging as a promising model. This paper focuses on the two existing QSVM methods: quantum kernel SVM (QK-SVM) and quantum variational SVM (QV-SVM). While both have yielded impressive results, we present a novel approach that synergizes the strengths of QK-SVM and QV-SVM to enhance accuracy. Our proposed model, quantum variational kernel SVM (QVK-SVM), leverages the quantum kernel and quantum variational algorithm. We conducted extensive experiments on the Iris dataset and observed that QVK-SVM outperforms both existing models in terms of accuracy, loss, and confusion matrix indicators. Our results demonstrate that QVK-SVM holds tremendous potential as a reliable and transformative tool for QML applications. Hence, we recommend its adoption in future QML research endeavors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#20998;&#36776;&#29575;&#26465;&#20214;&#19979;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28304;&#22270;&#20687;&#30340;&#21487;&#35270;&#29305;&#24449;&#19982;&#20302;&#20998;&#36776;&#29575;&#30446;&#26631;&#22270;&#20687;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05023</link><description>&lt;p&gt;
&#20302;&#20998;&#36776;&#29575;&#26465;&#20214;&#19979;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning. (arXiv:2305.05023v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#20998;&#36776;&#29575;&#26465;&#20214;&#19979;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28304;&#22270;&#20687;&#30340;&#21487;&#35270;&#29305;&#24449;&#19982;&#20302;&#20998;&#36776;&#29575;&#30446;&#26631;&#22270;&#20687;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#36328;&#39046;&#22495;&#30340;&#26144;&#23556;&#65292;&#20551;&#23450;&#29992;&#20110;&#32763;&#35793;&#30340;&#22270;&#20687;&#20849;&#20139;&#20869;&#23481;&#65292;&#20294;&#20855;&#26377;&#33258;&#24049;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#65288;&#21363;&#39118;&#26684;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32454;&#31890;&#24230;&#38382;&#39064;&#65292;&#20854;&#20013;&#39046;&#22495;&#30456;&#20851;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#39046;&#22495;&#26080;&#20851;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#19968;&#24133;&#22270;&#20687;&#65292;&#23558;&#28304;&#22270;&#20687;&#30340;&#21487;&#35270;&#29305;&#24449;&#19982;&#20302;&#39057;&#20449;&#24687;&#65288;&#20363;&#22914;&#23039;&#21183;&#12289;&#39068;&#33394;&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#20135;&#29983;&#21516;&#26102;&#20855;&#26377;&#28304;&#22270;&#20687;&#30340;&#29420;&#29305;&#20449;&#24687;&#21644;&#20302;&#20998;&#36776;&#29575;&#30446;&#26631;&#22270;&#20687;&#20449;&#24687;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generally, image-to-image translation (i2i) methods aim at learning mappings across domains with the assumption that the images used for translation share content (e.g., pose) but have their own domain-specific information (a.k.a. style). Conditioned on a target image, such methods extract the target style and combine it with the source image content, keeping coherence between the domains. In our proposal, we depart from this traditional view and instead consider the scenario where the target domain is represented by a very low-resolution (LR) image, proposing a domain-agnostic i2i method for fine-grained problems, where the domains are related. More specifically, our domain-agnostic approach aims at generating an image that combines visual features from the source image with low-frequency information (e.g. pose, color) of the LR target image. To do so, we present a novel approach that relies on training the generative model to produce images that both share distinctive information of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.04866</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#20840;&#36523;&#25805;&#20316;&#30340;&#22240;&#26524;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19979;&#19968;&#20195;&#23478;&#24237;&#26426;&#22120;&#20154;&#21161;&#25163;&#38656;&#35201;&#32467;&#21512;&#26426;&#21160;&#21644;&#20132;&#20114;&#33021;&#21147;&#65292;&#21363;&#36890;&#24120;&#25152;&#35828;&#30340;&#31227;&#21160;&#25805;&#20316;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#21644;&#20219;&#21153;&#24120;&#35265;&#30340;&#22810;&#30446;&#26631;&#24615;&#36136;&#65292;&#20363;&#22914;&#33021;&#22815;&#26377;&#25928;&#22320;&#36798;&#21040;&#30446;&#26631;&#19988;&#36991;&#20813;&#38556;&#30861;&#65292;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#24456;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#20154;&#24037;&#21305;&#37197;&#21160;&#20316;&#31354;&#38388;&#30340;&#37096;&#20998;&#21040;&#31227;&#21160;&#25805;&#20316;&#23376;&#30446;&#26631;&#65288;&#20363;&#22914;&#29992;&#20110;&#31227;&#21160;&#30446;&#26631;&#30340;&#22522;&#30784;&#21160;&#20316;&#21644;&#29992;&#20110;&#25805;&#20316;&#30340;&#25163;&#33218;&#21160;&#20316;&#65289;&#23558;&#20219;&#21153;&#20998;&#20026;&#19981;&#24102;&#25805;&#20316;&#30340;&#23548;&#33322;&#21644;&#19981;&#24102;&#26426;&#21160;&#30340;&#22266;&#23450;&#25805;&#20316;&#12290;&#27492;&#35299;&#20915;&#26041;&#26696;&#38450;&#27490;&#20102;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#24182;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#35813;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;PCA&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20302;&#32500;&#31354;&#38388;&#26469;&#38477;&#20302;&#24494;&#20998;&#29109;&#20272;&#35745;&#30340;&#25351;&#25968;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#23884;&#20837;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#32467;&#26500;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04712</link><description>&lt;p&gt;
&#36890;&#36807;&#38477;&#32500;&#23454;&#29616;&#39640;&#32500;&#24179;&#28369;&#29109;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction. (arXiv:2305.04712v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;PCA&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20302;&#32500;&#31354;&#38388;&#26469;&#38477;&#20302;&#24494;&#20998;&#29109;&#20272;&#35745;&#30340;&#25351;&#25968;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#23884;&#20837;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#32467;&#26500;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#39640;&#26031;&#21367;&#31215;&#19979;&#20811;&#26381;&#24494;&#20998;&#29109;&#20272;&#35745;&#30340;&#25351;&#25968;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;$X$&#30340;$n$&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#26469;&#20272;&#35745;&#24494;&#20998;&#29109;$h(X+Z)$&#65292;&#20854;&#20013;$X$&#21644;$Z$&#26159;&#29420;&#31435;&#30340;$D$&#32500;&#38543;&#26426;&#21464;&#37327;&#65292;&#20854;&#20013;$X$&#26159;&#27425;&#39640;&#26031;&#30340;&#65292;&#20108;&#38454;&#30697;&#26377;&#30028;&#65292;&#32780;$Z\sim\mathcal{N}(0,\sigma^2I_D)$&#12290;&#22312;&#32477;&#23545;&#35823;&#24046;&#25439;&#22833;&#19979;&#65292;&#19978;&#36848;&#38382;&#39064;&#20855;&#26377;$\frac{c^D}{\sqrt{n}}$&#30340;&#21442;&#25968;&#20272;&#35745;&#36895;&#29575;&#65292;&#20854;&#22312;&#25968;&#25454;&#32500;&#24230;$D$&#20013;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24120;&#24120;&#22312;&#24212;&#29992;&#20013;&#24341;&#21457;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29109;&#20272;&#35745;&#20043;&#21069;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#23558;$X$&#25237;&#24433;&#21040;&#20302;&#32500;&#31354;&#38388;&#26469;&#20811;&#26381;&#36825;&#31181;&#25351;&#25968;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;PCA&#30340;&#26410;&#35299;&#37322;&#26041;&#24046;&#28040;&#22833;&#26102;&#28176;&#36817;&#35823;&#24046;&#30340;&#24320;&#38144;&#20063;&#23558;&#28040;&#22833;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#23884;&#20837;&#26412;&#36136;&#19978;&#20302;&#32500;&#32467;&#26500;&#30340;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of overcoming exponential sample complexity in differential entropy estimation under Gaussian convolutions. Specifically, we consider the estimation of the differential entropy $h(X+Z)$ via $n$ independently and identically distributed samples of $X$, where $X$ and $Z$ are independent $D$-dimensional random variables with $X$ sub-Gaussian with bounded second moment and $Z\sim\mathcal{N}(0,\sigma^2I_D)$. Under the absolute-error loss, the above problem has a parametric estimation rate of $\frac{c^D}{\sqrt{n}}$, which is exponential in data dimension $D$ and often problematic for applications. We overcome this exponential sample complexity by projecting $X$ to a low-dimensional space via principal component analysis (PCA) before the entropy estimation, and show that the asymptotic error overhead vanishes as the unexplained variance of the PCA vanishes. This implies near-optimal performance for inherently low-dimensional structures embedded in high-dimensional spaces,
&lt;/p&gt;</description></item><item><title>MO-DEHB&#26159;&#19968;&#31181;&#25193;&#23637;&#33258;DEHB&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04502</link><description>&lt;p&gt;
MO-DEHB:&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36827;&#21270;&#36229;&#24102;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MO-DEHB: Evolutionary-based Hyperband for Multi-Objective Optimization. (arXiv:2305.04502v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04502
&lt;/p&gt;
&lt;p&gt;
MO-DEHB&#26159;&#19968;&#31181;&#25193;&#23637;&#33258;DEHB&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#33258;&#21160;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#24615;&#21482;&#26159;&#24517;&#39035;&#32771;&#34385;&#30340;&#22810;&#20010;&#24615;&#33021;&#26631;&#20934;&#20043;&#19968;&#12290;&#22312;&#22797;&#26434;&#22810;&#20803;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#21516;&#26102;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MO-DEHB&#65292;&#19968;&#31181;&#26377;&#25928;&#28789;&#27963;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#65292;&#23427;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#36827;&#21270;Hyperband&#26041;&#27861;DEHB&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#22871;&#21253;&#25324;15&#20010;&#19981;&#21516;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#30340;&#20840;&#38754;&#22522;&#20934;&#22871;&#20214;&#26469;&#39564;&#35777;MO-DEHB&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#21644;&#32852;&#21512;NAS&#21644;HPO&#65292;&#30446;&#26631;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#34920;&#26126;&#65292;MO-DEHB&#22312;&#36825;15&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a powerful technique for automating the tuning of machine learning (ML) models. However, in many real-world applications, accuracy is only one of multiple performance criteria that must be considered. Optimizing these objectives simultaneously on a complex and diverse search space remains a challenging task. In this paper, we propose MO-DEHB, an effective and flexible multi-objective (MO) optimizer that extends the recent evolutionary Hyperband method DEHB. We validate the performance of MO-DEHB using a comprehensive suite of 15 benchmarks consisting of diverse and challenging MO problems, including HPO, neural architecture search (NAS), and joint NAS and HPO, with objectives including accuracy, latency and algorithmic fairness. A comparative study against state-of-the-art MO optimizers demonstrates that MO-DEHB clearly achieves the best performance across our 15 benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#20998;&#24067;&#24335;&#20869;&#37096;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22312;&#32447;Hebbian-Bayesian&#23398;&#20064;&#21644;&#37325;&#36830;&#26426;&#21046;&#65292;&#24182;&#34920;&#29616;&#20986;&#21644;&#20256;&#32479;&#30340;&#38750;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03866</link><description>&lt;p&gt;
&#24102;&#26377;Hebbian&#21487;&#22609;&#24615;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks with Hebbian plasticity for unsupervised representation learning. (arXiv:2305.03866v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#20998;&#24067;&#24335;&#20869;&#37096;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22312;&#32447;Hebbian-Bayesian&#23398;&#20064;&#21644;&#37325;&#36830;&#26426;&#21046;&#65292;&#24182;&#34920;&#29616;&#20986;&#21644;&#20256;&#32479;&#30340;&#38750;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26080;&#30417;&#30563;&#36807;&#31243;&#20013;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20998;&#24067;&#24335;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#38750;&#33033;&#20914;&#21069;&#39304;&#36125;&#21494;&#26031;&#32622;&#20449;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#65288;BCPNN&#65289;&#27169;&#22411;&#36716;&#21270;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;Hebbian-Bayesian&#23398;&#20064;&#21644;&#37325;&#36830;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#20808;&#21069;&#24050;&#34920;&#29616;&#20986;&#25191;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#33033;&#20914;&#27169;&#22411;&#37319;&#29992;&#27850;&#26494;&#32479;&#35745;&#21644;&#20302;&#21457;&#28779;&#29575;&#19982;&#22312;&#20307;&#26143;&#24418;&#31070;&#32463;&#20803;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#33033;&#20914;&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#38750;&#33033;&#20914;BCPNN&#25509;&#36817;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#19982;&#20854;&#20182;&#22522;&#20110;Hebbian&#30340;&#33033;&#20914;&#32593;&#32476;&#22312;MNIST&#21644;F-MNIST&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel spiking neural network model for learning distributed internal representations from data in an unsupervised procedure. We achieved this by transforming the non-spiking feedforward Bayesian Confidence Propagation Neural Network (BCPNN) model, employing an online correlation-based Hebbian-Bayesian learning and rewiring mechanism, shown previously to perform representation learning, into a spiking neural network with Poisson statistics and low firing rate comparable to in vivo cortical pyramidal neurons. We evaluated the representations learned by our spiking model using a linear classifier and show performance close to the non-spiking BCPNN, and competitive with other Hebbian-based spiking networks when trained on MNIST and F-MNIST machine learning benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#36719;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#25104;&#21151;&#23558;SMLM&#24212;&#29992;&#20110;&#21463;&#38480;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.03530</link><description>&lt;p&gt;
&#25506;&#32034;&#36719;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#22312;&#21487;&#25511;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Softly Masked Language Modelling for Controllable Symbolic Music Generation. (arXiv:2305.03530v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#36719;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#25104;&#21151;&#23558;SMLM&#24212;&#29992;&#20110;&#21463;&#38480;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#36719;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#65288;SMLM&#65289;&#24212;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#26089;&#26399;&#25506;&#32034;&#12290;SMLM&#21487;&#35270;&#20026;&#25513;&#27169;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#20854;&#20013;&#36755;&#20837;&#38598;&#21512;&#30340;&#27599;&#20010;&#20803;&#32032;&#21487;&#20197;&#26159;&#37096;&#20998;&#24050;&#30693;&#30340;&#65292;&#32780;&#19981;&#26159;&#24050;&#30693;&#25110;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#23637;&#31034;&#20102;&#23558;SMLM&#24212;&#29992;&#20110;&#21463;&#38480;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#21487;&#22312;https://erl-j.github.io/smlm-web-supplement/&#19978;&#25214;&#21040;&#33509;&#24178;&#38899;&#39057;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document presents some early explorations of applying Softly Masked Language Modelling (SMLM) to symbolic music generation. SMLM can be seen as a generalisation of masked language modelling (MLM), where instead of each element of the input set being either known or unknown, elements can be partly known. We demonstrate some results of applying SMLM to constrained symbolic music generation using a transformer encoder architecture. Several audio examples are available at https://erl-j.github.io/smlm-web-supplement/
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.02993</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;7: &#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#20219;&#21153;7&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#20027;&#35201;&#28041;&#21450;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#20013;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI4CT&#65289;&#65292;&#30001;&#20004;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#19968;&#20010;&#26159;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#65292;&#21478;&#19968;&#20010;&#26159;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#21307;&#23398;&#21644;&#25968;&#23383;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#21307;&#30103;&#35777;&#25454;&#35299;&#37322;&#21644;&#26816;&#32034;&#12289;&#25552;&#20379;&#20010;&#24615;&#21270;&#22522;&#20110;&#35777;&#25454;&#30340;&#20445;&#20581;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#31532;1&#20010;&#23376;&#20219;&#21153;&#8220;&#34164;&#21547;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;40&#20301;&#21442;&#36187;&#32773;&#30340;643&#20221;&#25552;&#20132;&#65292;&#31532;2&#20010;&#23376;&#20219;&#21153;&#8220;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;23&#20301;&#21442;&#36187;&#32773;&#30340;364&#20221;&#25552;&#20132;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#37096;&#20998;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#34164;&#21547;&#20219;&#21153;&#19978;&#26410;&#33021;&#26126;&#26174;&#20248;&#20110;&#22823;&#22810;&#25968;&#31867;&#22522;&#32447;&#65292;&#32780;&#25105;&#20204;&#35266;&#23519;&#21040;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#34164;&#21547;&#20219;&#21153;&#12290;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;LayerNorm&#22312;Transformers&#30340;Attention&#23618;&#20013;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#21521;&#37327;&#36827;&#34892;&#25237;&#24433;&#24182;&#23545;&#25152;&#26377;&#21521;&#37327;&#36827;&#34892;&#32553;&#25918;&#65292;LayerNorm&#21487;&#20197;&#24110;&#21161;&#27880;&#24847;&#21147;&#26426;&#21046;&#26356;&#22909;&#22320;&#22788;&#29702;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2305.02582</link><description>&lt;p&gt;
&#20851;&#20110;LayerNorm&#22312;Transformers&#30340;Attention&#20013;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Expressivity Role of LayerNorm in Transformers' Attention. (arXiv:2305.02582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;LayerNorm&#22312;Transformers&#30340;Attention&#23618;&#20013;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#21521;&#37327;&#36827;&#34892;&#25237;&#24433;&#24182;&#23545;&#25152;&#26377;&#21521;&#37327;&#36827;&#34892;&#32553;&#25918;&#65292;LayerNorm&#21487;&#20197;&#24110;&#21161;&#27880;&#24847;&#21147;&#26426;&#21046;&#26356;&#22909;&#22320;&#22788;&#29702;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Layer Normalization&#65288;LayerNorm&#65289;&#26159;&#25152;&#26377;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#37117;&#20855;&#26377;&#30340;&#32452;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;LayerNorm&#23545;&#38543;&#21518;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#19982;&#36890;&#24120;&#35748;&#20026;LayerNorm&#20165;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#24402;&#19968;&#21270;&#28608;&#27963;&#20540;&#21644;&#22312;&#21453;&#21521;&#20256;&#25773;&#26399;&#38388;&#24402;&#19968;&#21270;&#26799;&#24230;&#30340;&#20849;&#35782;&#19981;&#21516;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;LayerNorm&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#35748;&#20026;&#23427;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;a&#65289;&#23558;&#36755;&#20837;&#21521;&#37327;&#25237;&#24433;&#21040;&#27491;&#20132;&#20110;$\left[1,1,...,1\right]$&#21521;&#37327;&#30340;$d-1$&#31354;&#38388;&#65292;&#24182;&#19988;&#65288;b&#65289;&#23558;&#25152;&#26377;&#21521;&#37327;&#32553;&#25918;&#21040;&#30456;&#21516;&#30340;$\sqrt{d}$&#33539;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#20010;&#32452;&#20214;&#23545;&#38543;&#21518;&#30340;Transformers&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#37117;&#24456;&#37325;&#35201;&#65306;&#65288;a&#65289;&#25237;&#24433;&#20801;&#35768;&#27880;&#24847;&#26426;&#21046;&#21019;&#24314;&#19968;&#20010;&#20851;&#27880;&#25152;&#26377;&#38190;&#31561;&#37327;&#30340;&#27880;&#24847;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#27492;&#25805;&#20316;&#30340;&#38656;&#35201;&#65307;&#65288;b&#65289;&#32553;&#25918;&#20351;&#27599;&#20010;&#38190;&#37117;&#26377;&#21487;&#33021;&#25509;&#25910;&#21040;&#26368;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#37197;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a $d-1$ space that is orthogonal to the $\left[1,1,...,1\right]$ vector, and (b) scaling of all vectors to the same norm of $\sqrt{d}$. We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24471;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#20271;&#21162;&#21033;&#36807;&#31243;&#26399;&#26395;&#26368;&#22823;&#20540;&#30340;&#19978;&#38480;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#30001;&#25351;&#25968;&#38598;&#22312;&#22343;&#21248;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#31867;&#19979;&#30340;&#23646;&#24615;&#21644;&#20989;&#25968;&#31867;&#24471;&#20986;&#12290;</title><link>http://arxiv.org/abs/2304.14474</link><description>&lt;p&gt;
&#19968;&#39033;&#38024;&#23545;&#20271;&#21162;&#21033;&#36807;&#31243;&#26399;&#26395;&#26368;&#22823;&#20540;&#30340;&#38142;&#24335;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Chain Rule for the Expected Suprema of Bernoulli Processes. (arXiv:2304.14474v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24471;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#20271;&#21162;&#21033;&#36807;&#31243;&#26399;&#26395;&#26368;&#22823;&#20540;&#30340;&#19978;&#38480;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#30001;&#25351;&#25968;&#38598;&#22312;&#22343;&#21248;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#31867;&#19979;&#30340;&#23646;&#24615;&#21644;&#20989;&#25968;&#31867;&#24471;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#20271;&#21162;&#21033;&#36807;&#31243;&#26399;&#26395;&#26368;&#22823;&#20540;&#30340;&#19978;&#38480;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#30001;&#25351;&#25968;&#38598;&#22312;&#22343;&#21248;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#31867;&#19979;&#30340;&#23646;&#24615;&#21644;&#20989;&#25968;&#31867;&#24471;&#20986;&#65292;&#25299;&#23637;&#20102;Maurer&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#30340;&#26089;&#26399;&#32467;&#26524;&#12290;&#35777;&#26126;&#24517;&#39035;&#22522;&#20110;Bednorz&#21644;Latala&#38024;&#23545;&#20271;&#21162;&#21033;&#36807;&#31243;&#26377;&#30028;&#24615;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We obtain an upper bound on the expected supremum of a Bernoulli process indexed by the image of an index set under a uniformly Lipschitz function class in terms of properties of the index set and the function class, extending an earlier result of Maurer for Gaussian processes. The proof makes essential use of recent results of Bednorz and Latala on the boundedness of Bernoulli processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2304.13836</link><description>&lt;p&gt;
&#35770;RemOve-And-Retrain&#30340;&#38519;&#38449;&#65306;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#21327;&#35758;&#29992;&#20110;&#27979;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#32972;&#26223;&#21644;&#23454;&#35777;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#26377;&#20851;&#20915;&#31574;&#21151;&#33021;&#30340;&#20449;&#24687;&#30340;&#23646;&#24615;&#22312;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#19982;ROAR&#30340;&#21407;&#22987;&#30446;&#30340;&#30456;&#30683;&#30462;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#20986;&#29616;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#21464;&#20307;RemOve-And-Debias&#65288;ROAD&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROAR&#24402;&#22240;&#24230;&#37327;&#20013;&#27611;&#31961;&#24230;&#20559;&#24046;&#30340;&#19968;&#33268;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#30450;&#30446;&#20381;&#36182;ROAR&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#21644;&#29702;&#35299;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#20132;&#20114;&#20013;&#37325;&#29616;&#36825;&#20123;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.13787</link><description>&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Surrogate Assisted Generation of Human-Robot Interaction Scenarios. (arXiv:2304.13787v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#21644;&#29702;&#35299;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#20132;&#20114;&#20013;&#37325;&#29616;&#36825;&#20123;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#19981;&#21516;&#29615;&#22659;&#21644;&#29992;&#25143;&#19979;&#35780;&#20272;&#21644;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#20248;&#32570;&#28857;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#20849;&#20139;&#25511;&#21046;&#36965;&#25805;&#20316;&#20219;&#21153;&#30340;&#31995;&#32479;&#22833;&#25928;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#31574;&#30053;&#21644;&#20154;&#31867;&#34892;&#20026;&#26469;&#30452;&#25509;&#35780;&#20272;&#29983;&#25104;&#30340;&#22330;&#26223;&#12290;&#36825;&#20123;&#35780;&#20272;&#25152;&#38656;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#26367;&#20195;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#20026;&#26469;&#22686;&#24378;&#22330;&#26223;&#29983;&#25104;&#31995;&#32479;&#30340;&#24314;&#35758;&#12290;&#22312;&#20849;&#20139;&#25511;&#21046;&#36965;&#25805;&#20316;&#22495;&#21644;&#26356;&#22797;&#26434;&#30340;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26367;&#20195;&#27169;&#22411;&#36741;&#21161;&#30340;&#22330;&#26223;&#29983;&#25104;&#21487;&#20197;&#39640;&#25928;&#22320;&#21512;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25925;&#38556;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20132;&#20114;&#20013;&#26159;&#21487;&#37325;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#23545;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#36827;&#34892;&#25311;&#21512;&#21487;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13761</link><description>&lt;p&gt;
&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization. (arXiv:2304.13761v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13761
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#23545;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#36827;&#34892;&#25311;&#21512;&#21487;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDT)&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#26410;&#35265;&#25968;&#25454;&#20013;&#30340;&#23567;&#21327;&#21464;&#37327;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#24212;&#29992;&#29420;&#28909;&#32534;&#30721;&#23558;GBDT&#27169;&#22411;&#36716;&#25442;&#20026;&#32447;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26641;&#21494;&#32534;&#30721;&#20026;&#19968;&#20010;&#34394;&#25311;&#21464;&#37327;&#12290;&#36825;&#20801;&#35768;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#20998;&#35299;&#26041;&#27861;&#26469;&#35780;&#20272;GBDT&#27169;&#22411;&#23545;&#21327;&#21464;&#37327;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#37325;&#26032;&#25311;&#21512;&#20854;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#65292;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#20102;&#27491;&#21017;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29420;&#28909;&#32534;&#30721;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-boosted decision trees (GBDT) are widely used and highly effective machine learning approach for tabular data modeling. However, their complex structure may lead to low robustness against small covariate perturbation in unseen data. In this study, we apply one-hot encoding to convert a GBDT model into a linear framework, through encoding of each tree leaf to one dummy variable. This allows for the use of linear regression techniques, plus a novel risk decomposition for assessing the robustness of a GBDT model against covariate perturbations. We propose to enhance the robustness of GBDT models by refitting their linear regression forms with $L_1$ or $L_2$ regularization. Theoretical results are obtained about the effect of regularization on the model performance and robustness. It is demonstrated through numerical experiments that the proposed regularization approach can enhance the robustness of the one-hot-encoded GBDT models.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#65292;&#20026;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.13081</link><description>&lt;p&gt;
&#26032;&#20852;&#25216;&#26415;&#30340;&#32452;&#32455;&#27835;&#29702;&#65306;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Organizational Governance of Emerging Technologies: AI Adoption in Healthcare. (arXiv:2304.13081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#65292;&#20026;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#33829;&#21644;&#20844;&#20849;&#37096;&#38376;&#30340;&#32467;&#26500;&#21644;&#35268;&#33539;&#31934;&#32454;&#21270;&#20102;&#26032;&#20852;&#25216;&#26415;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#23613;&#31649;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;AI&#37319;&#29992;&#26041;&#24335;&#65292;&#20294;&#26159;&#20854;&#20351;&#29992;&#21644;&#25972;&#21512;&#21608;&#22260;&#30340;&#32452;&#32455;&#27835;&#29702;&#24448;&#24448;&#34987;&#35748;&#20026;&#19981;&#21487;&#34892;&#12290;&#20581;&#24247;AI&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65288;HAIP&#65289;&#26088;&#22312;&#36890;&#36807;&#27492;&#30740;&#31350;&#26356;&#22909;&#22320;&#23450;&#20041;&#21307;&#30103;&#20445;&#20581;&#20013;AI&#31995;&#32479;&#30340;&#20805;&#20998;&#32452;&#32455;&#27835;&#29702;&#35201;&#27714;&#65292;&#24182;&#25903;&#25345;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#35201;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#37319;&#29992;&#30340;&#26631;&#20934;&#22914;&#20309;&#26131;&#20110;&#20351;&#29992;&#21644;&#39640;&#25928;&#36816;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#29305;&#23450;&#30340;&#21355;&#29983;&#31995;&#32479;&#20013;&#65292;&#32472;&#21046;&#20986;&#23454;&#38469;&#26426;&#26500;&#37319;&#29992;AI&#25216;&#26415;&#30340;&#20855;&#20307;&#20915;&#31574;&#28857;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#21512;&#20316;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working w
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#65292;&#34920;&#26126;&#26356;&#39057;&#32321;&#22320;&#19982;&#21442;&#25968;&#26381;&#21153;&#22120;&#36890;&#20449;&#20250;&#36127;&#38754;&#24433;&#21709;&#27492;&#31867;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12216</link><description>&lt;p&gt;
&#26356;&#22810;&#36890;&#20449;&#19981;&#20250;&#20351;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#21464;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
More Communication Does Not Result in Smaller Generalization Error in Federated Learning. (arXiv:2304.12216v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12216
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#65292;&#34920;&#26126;&#26356;&#39057;&#32321;&#22320;&#19982;&#21442;&#25968;&#26381;&#21153;&#22120;&#36890;&#20449;&#20250;&#36127;&#38754;&#24433;&#21709;&#27492;&#31867;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#19979;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26377;$K$&#20010;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#65292;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#19968;&#20010;&#22823;&#23567;&#20026;$n$&#30340;&#29420;&#31435;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26412;&#22320;&#23398;&#20064;&#30340;&#20010;&#20307;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#32858;&#21512;&#65288;&#24179;&#22343;&#65289;&#65292;&#28982;&#21518;&#21457;&#36865;&#22238;&#35774;&#22791;&#12290;&#25105;&#20204;&#32771;&#34385;&#22810;&#27425;&#65288;&#27604;&#22914;&#35828;$R\in \mathbb{N}^*$&#65289;&#27169;&#22411;&#32858;&#21512;&#24182;&#30740;&#31350;$R$&#23545;&#26368;&#32456;&#32858;&#21512;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19978;&#30028;&#65292;&#26126;&#30830;&#32771;&#34385;&#20102;$R$&#65288;&#38500;&#20102;&#21442;&#19982;&#35774;&#22791;&#30340;&#25968;&#37327;$K$&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;$n$&#65289;&#30340;&#24433;&#21709;&#12290;&#35266;&#23519;&#21040;&#23545;&#20110;&#22266;&#23450;&#30340;$(n,K)$&#65292;&#19978;&#30028;&#38543;$R$&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;&#26356;&#39057;&#32321;&#22320;&#19982;&#21442;&#25968;&#26381;&#21153;&#22120;&#36890;&#20449;&#20250;&#36127;&#38754;&#24433;&#21709;&#27492;&#31867;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#32463;&#39564;&#39118;&#38505;&#36890;&#24120;&#21482;&#38543;&#30528;$n$&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;&#20026;&#20102;&#22312;FL&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#38656;&#35201;&#26435;&#34913;&#26412;&#22320;&#23398;&#20064;&#21644;&#20840;&#23616;&#32858;&#21512;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the generalization error of statistical learning models in a Federated Learning (FL) setting. Specifically, there are $K$ devices or clients, each holding an independent own dataset of size $n$. Individual models, learned locally via Stochastic Gradient Descent, are aggregated (averaged) by a central server into a global model and then sent back to the devices. We consider multiple (say $R \in \mathbb N^*$) rounds of model aggregation and study the effect of $R$ on the generalization error of the final aggregated model. We establish an upper bound on the generalization error that accounts explicitly for the effect of $R$ (in addition to the number of participating devices $K$ and dataset size $n$). It is observed that, for fixed $(n, K)$, the bound increases with $R$, suggesting that the generalization of such learning algorithms is negatively affected by more frequent communication with the parameter server. Combined with the fact that the empirical risk, however, generally d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.11116</link><description>&lt;p&gt;
Graph-ToolFormer: &#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#65292;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. (arXiv:2304.11116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#23545;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#24403;&#21069;&#65292;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#25193;&#23637;&#20063;&#24050;&#34987;&#24212;&#29992;&#20110;&#30740;&#31350;&#20855;&#26377;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;LLMs&#30001;&#20110;&#22312;&#25191;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12289;&#31934;&#30830;&#30340;&#25968;&#23398;&#35745;&#31639;&#20197;&#21450;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#24369;&#28857;&#65292;&#22240;&#27492;&#21576;&#29616;&#20986;&#38750;&#24120;&#20005;&#37325;&#30340;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#35843;&#26597;&#25506;&#32034;&#36171;&#20104;&#29616;&#26377;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#30340;&#21407;&#29702;&#12289;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#36825;&#23558;&#23545;LLMs&#21644;&#22270;&#24418;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#21463;&#26368;&#26032;&#30340;ChatGPT&#21644;Toolformer&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Graph-ToolFormer&#65288;&#38754;&#21521;&#22270;&#24418;&#25512;&#29702;&#30340;Toolformer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#33258;&#36523;&#65292;&#26088;&#22312;&#22521;&#20859;&#20182;&#20204;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05292</link><description>&lt;p&gt;
MC-ViViT: &#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-ViViT&#29992;&#20110;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#12289;&#38382;&#21367;&#21644;&#35270;&#39057;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;(MCI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-&#35270;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;(MC-ViViT)&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#21306;&#20998;MCI&#21644;&#27491;&#24120;&#35748;&#30693;&#12290;&#25968;&#25454;&#26469;&#33258;I-CONECT&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#39057;&#32321;&#35270;&#39057;&#32842;&#22825;&#26469;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#34892;&#20026;&#24178;&#39044;&#35797;&#39564;&#12290;MC-ViViT&#22312;&#19968;&#20010;&#20998;&#25903;&#20013;&#25552;&#21462;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;MC&#27169;&#22359;&#22686;&#24378;&#34920;&#31034;&#12290;&#30001;&#20110;I-CONECT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21253;&#21547;&#38590;&#26131;&#21644;&#27491;&#36127;&#26679;&#26412;&#65289;&#65292;&#36825;&#20351;MC-ViViT&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Hard-Easy&#21644;Positive-Negative&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;HP Loss&#65289;&#26469;&#32467;&#21512;&#23545;&#27604;&#24230;&#35843;&#33410;&#25439;&#22833;Focal loss&#21644;AD-CORRE loss&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;I-CONECT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#30740;&#31350;&#21644;&#35299;&#20915;&#20102;&#27491;&#21017;&#21270;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#65292;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#28789;&#24863;&#65292;&#20351;&#29992;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#22312;&#36755;&#20837;&#31232;&#30095;&#26102;&#38388;&#20869;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.15725</link><description>&lt;p&gt;
&#27714;&#35299;&#27491;&#21017;&#21270;&#30340;exp&#12289;cosh&#21644;sinh&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Regularized Exp, Cosh and Sinh Regression Problems. (arXiv:2303.15725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15725
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#30740;&#31350;&#21644;&#35299;&#20915;&#20102;&#27491;&#21017;&#21270;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#65292;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#28789;&#24863;&#65292;&#20351;&#29992;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#22312;&#36755;&#20837;&#31232;&#30095;&#26102;&#38388;&#20869;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27880;&#24847;&#21147;&#35745;&#31639;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Transformer&#12289;GPT-4&#21644;ChatGPT&#65289;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#35813;&#25991;&#30740;&#31350;&#20102;&#21463;softmax/exp&#21333;&#20803;&#21551;&#21457;&#30340;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#12290;&#26631;&#20934;&#25351;&#25968;&#22238;&#24402;&#26159;&#38750;&#20984;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#20984;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#20197;&#36755;&#20837;&#31232;&#30095;&#26102;&#38388;&#35299;&#20915;&#38382;&#39064;&#12290;&#24418;&#24335;&#19978;&#65292;&#32473;&#23450;&#30697;&#38453;$A\in \mathbb{R}^{n\times d}$&#65292;$b\in \mathbb{R}^n$&#65292;$w\in\mathbb{R}^n$&#21644;&#20219;&#20309;&#20989;&#25968;$\exp,\cosh$&#21644;$\sinh$&#65292;&#35760;&#20316;$f$&#12290;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#20248;$x$&#65292;&#20351;&#24471;$0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$&#26368;&#23567;&#21270;&#12290;&#30740;&#31350;&#24182;&#35299;&#20915;&#20102;&#27491;&#21017;&#21270;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#20989;&#25968;&#20026;$\exp, \cosh$&#21644;$\sinh$&#65292;&#24182;&#20351;&#29992;&#36817;&#20284;&#29275;&#39039;&#27861;&#22312;&#36755;&#20837;&#31232;&#30095;&#26102;&#38388;&#20869;&#27714;&#35299;&#65292;&#28789;&#24863;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern machine learning, attention computation is a fundamental task for training large language models such as Transformer, GPT-4 and ChatGPT. In this work, we study exponential regression problem which is inspired by the softmax/exp unit in the attention mechanism in large language models. The standard exponential regression is non-convex. We study the regularization version of exponential regression problem which is a convex problem. We use approximate newton method to solve in input sparsity time.  Formally, in this problem, one is given matrix $A \in \mathbb{R}^{n \times d}$, $b \in \mathbb{R}^n$, $w \in \mathbb{R}^n$ and any of functions $\exp, \cosh$ and $\sinh$ denoted as $f$. The goal is to find the optimal $x$ that minimize $ 0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$. The straightforward method is to use the naive Newton's method. Let $\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let $\omega$ denote the exponent of matrix multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#20986;&#22810;&#31181;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30740;&#31350;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#12290;</title><link>http://arxiv.org/abs/2303.12706</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#36328;&#22810;&#31181;&#25104;&#20687;&#27169;&#24577;&#36827;&#34892;&#35268;&#33539;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities. (arXiv:2303.12706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#20986;&#22810;&#31181;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30740;&#31350;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24120;&#35265;&#31070;&#32463;&#30142;&#30149;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30142;&#30149;&#24322;&#36136;&#24615;&#65292;&#21253;&#25324;&#30149;&#22240;&#12289;&#31070;&#32463;&#25104;&#20687;&#29305;&#24449;&#12289;&#21512;&#24182;&#30151;&#25110;&#22522;&#22240;&#21464;&#24322;&#30340;&#24046;&#24322;&#12290;&#35268;&#33539;&#24314;&#27169;&#24050;&#25104;&#20026;&#30740;&#31350;&#36825;&#31181;&#20154;&#32676;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#20854;&#20013;&#23545;&#29983;&#29702;&#31995;&#32479;&#30340;&#8220;&#27491;&#24120;&#8221;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#20010;&#20307;&#23618;&#38754;&#19978;&#26816;&#27979;&#19982;&#30142;&#30149;&#30149;&#29702;&#30456;&#20851;&#30340;&#20559;&#24046;&#12290;&#23545;&#20110;&#35768;&#22810;&#24322;&#36136;&#24615;&#30142;&#30149;&#65292;&#25105;&#20204;&#39044;&#35745;&#20250;&#35266;&#23519;&#21040;&#22810;&#31181;&#31070;&#32463;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#30340;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35268;&#33539;&#27169;&#22411;&#20027;&#35201;&#26159;&#20026;&#20102;&#30740;&#31350;&#21333;&#19968;&#25104;&#20687;&#27169;&#24577;&#32780;&#24320;&#21457;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#27169;&#24577;&#30340;&#21464;&#37327;&#20013;&#32858;&#21512;&#24322;&#24120;&#24615;&#65292;&#24182;&#19988;&#27604;&#21333;&#27169;&#24335;&#22522;&#32447;&#26356;&#33021;&#26816;&#27979;&#21040;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#26816;&#27979;T1&#21644;DTI&#25968;&#25454;&#20013;&#30340;&#20010;&#20307;&#23618;&#38754;&#20559;&#24046;&#30340;&#22810;&#27169;&#24577;VAE&#35268;&#33539;&#27169;&#22411;&#12290;&#19982;&#21333;&#27169;&#24335;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#21040;&#36731;&#24230;&#35748;&#30693;&#21463;&#25439;&#30340;&#21463;&#35797;&#32773;&#20013;&#30340;&#20559;&#24046;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#29992;&#20110;&#30142;&#30149;&#24322;&#36136;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges of studying common neurological disorders is disease heterogeneity including differences in causes, neuroimaging characteristics, comorbidities, or genetic variation. Normative modelling has become a popular method for studying such cohorts where the 'normal' behaviour of a physiological system is modelled and can be used at subject level to detect deviations relating to disease pathology. For many heterogeneous diseases, we expect to observe abnormalities across a range of neuroimaging and biological variables. However, thus far, normative models have largely been developed for studying a single imaging modality. We aim to develop a multi-modal normative modelling framework where abnormality is aggregated across variables of multiple modalities and is better able to detect deviations than uni-modal baselines. We propose two multi-modal VAE normative models to detect subject level deviations across T1 and DTI data. Our proposed models were better able to detect di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#29289;&#29702;&#24863;&#30693;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#31561;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.07647</link><description>&lt;p&gt;
&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#24212;&#29992;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review. (arXiv:2303.07647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#29289;&#29702;&#24863;&#30693;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#31561;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#22312;&#34920;&#24449;&#21644;&#29702;&#35299;&#22825;&#28982;&#21644;&#26032;&#26448;&#26009;&#30340;&#21147;&#23398;&#24615;&#36136;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#21253;&#25324;&#23454;&#39564;&#35774;&#35745;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21453;&#38382;&#39064;&#12290;&#30001;&#20110;&#36817;&#24180;&#26469;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#65292;&#22240;&#27492;&#21450;&#26102;&#36827;&#34892;&#20840;&#38754;&#21644;&#26356;&#26032;&#30340;&#32508;&#36848;&#65292;&#23545;&#20110;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#19982;&#35813;&#32508;&#36848;&#30456;&#20851;&#30340;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#26415;&#35821;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#29289;&#29702;&#23398;&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20840;&#38754;&#28085;&#30422;&#20102;&#23454;&#39564;&#21147;&#23398;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#21253;&#25324;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#21069;&#27963;&#36291;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#26410;&#26469;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many decades, experimental solid mechanics has played a crucial role in characterizing and understanding the mechanical properties of natural and novel materials. Recent advances in machine learning (ML) provide new opportunities for the field, including experimental design, data analysis, uncertainty quantification, and inverse problems. As the number of papers published in recent years in this emerging field is exploding, it is timely to conduct a comprehensive and up-to-date review of recent ML applications in experimental solid mechanics. Here, we first provide an overview of common ML algorithms and terminologies that are pertinent to this review, with emphasis placed on physics-informed and physics-based ML methods. Then, we provide thorough coverage of recent ML applications in traditional and emerging areas of experimental mechanics, including fracture mechanics, biomechanics, nano- and micro-mechanics, architected materials, and 2D material. Finally, we highlight some curr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;VAE&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#25674;&#38144;&#28508;&#22312;&#21464;&#37327;&#30340;&#29305;&#24615;&#19982;&#20256;&#32479;&#26174;&#24335;&#34920;&#31034;&#26041;&#27861;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2303.07487</link><description>&lt;p&gt;
&#20351;&#29992;VAE&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#65306;&#22312;cryo-EM&#20013;&#30340;&#24212;&#29992;&#35266;&#23519;&#65288;arXiv:2303.07487v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
Using VAEs to Learn Latent Variables: Observations on Applications in cryo-EM. (arXiv:2303.07487v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;VAE&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#25674;&#38144;&#28508;&#22312;&#21464;&#37327;&#30340;&#29305;&#24615;&#19982;&#20256;&#32479;&#26174;&#24335;&#34920;&#31034;&#26041;&#27861;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#36817;&#20284;&#20998;&#24067;&#12290;VAE&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#29992;&#20110;&#35748;&#35777;&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#65292;&#20026;&#25968;&#25454;&#26679;&#26412;&#29983;&#25104;&#28508;&#22312;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;VAEs&#24050;&#29992;&#20110;&#34920;&#24449;&#29289;&#29702;&#21644;&#29983;&#29289;&#31995;&#32479;&#12290;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#24615;&#22320;&#30740;&#31350;&#20102;VAE&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#30340;&#25674;&#38144;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#24212;&#29992;&#20013;&#65292;&#32534;&#30721;&#22120;&#19982;&#26356;&#20256;&#32479;&#30340;&#26174;&#24335;&#28508;&#22312;&#21464;&#37327;&#34920;&#31034;&#20855;&#26377;&#23450;&#24615;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are a popular generative model used to approximate distributions. The encoder part of the VAE is used in amortized learning of latent variables, producing a latent representation for data samples. Recently, VAEs have been used to characterize physical and biological systems. In this case study, we qualitatively examine the amortization properties of a VAE used in biological applications. We find that in this application the encoder bears a qualitative resemblance to more traditional explicit representation of latent variables.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#22330;&#26223;&#27979;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#31867;&#20284;&#30340;&#20445;&#25252;&#21644;&#38750;&#20445;&#25252;&#23454;&#20363;&#26469;&#26816;&#27979;&#20998;&#31867;&#22120;&#20013;&#30340;&#27495;&#35270;&#65292;&#36890;&#36807;&#27604;&#36739;&#32452;&#38388;&#20915;&#31574;&#32467;&#26524;&#24046;&#24322;&#65292;&#26469;&#21457;&#29616;&#20010;&#20154;&#27495;&#35270;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#12300;&#32473;&#23450;&#24046;&#24322;&#30340;&#20844;&#24179;&#21407;&#21017;&#12301;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#25581;&#31034;&#22312;&#20844;&#24179;&#21407;&#21017;&#19979;&#30340;&#27495;&#35270;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.11944</link><description>&lt;p&gt;
&#27979;&#35797;&#21453;&#20107;&#23454;&#22330;&#26223;&#65306;&#25581;&#31034;&#22312;&#20844;&#24179;&#21407;&#21017;&#19979;&#30340;&#27495;&#35270;&#24046;&#24322; (arXiv:2302.11944v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference. (arXiv:2302.11944v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11944
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#22330;&#26223;&#27979;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#31867;&#20284;&#30340;&#20445;&#25252;&#21644;&#38750;&#20445;&#25252;&#23454;&#20363;&#26469;&#26816;&#27979;&#20998;&#31867;&#22120;&#20013;&#30340;&#27495;&#35270;&#65292;&#36890;&#36807;&#27604;&#36739;&#32452;&#38388;&#20915;&#31574;&#32467;&#26524;&#24046;&#24322;&#65292;&#26469;&#21457;&#29616;&#20010;&#20154;&#27495;&#35270;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#12300;&#32473;&#23450;&#24046;&#24322;&#30340;&#20844;&#24179;&#21407;&#21017;&#12301;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#25581;&#31034;&#22312;&#20844;&#24179;&#21407;&#21017;&#19979;&#30340;&#27495;&#35270;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#21453;&#20107;&#23454;&#22330;&#26223;&#27979;&#35797;(CST)&#30340;&#22240;&#26524;&#25968;&#25454;&#25366;&#25496;&#26694;&#26550;&#26469;&#26816;&#27979;&#20998;&#31867;&#22120;&#20013;&#30340;&#27495;&#35270;&#24773;&#20917;&#12290;CST&#26088;&#22312;&#20197;&#21487;&#25805;&#20316;&#19988;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#22238;&#31572;&#19968;&#31181;&#30452;&#35266;&#38382;&#39064;&#65306;&#8220;&#22914;&#26524;&#20010;&#20154;&#25110;&#25237;&#35785;&#20154;&#25152;&#23646;&#30340;&#21463;&#20445;&#25252;&#36523;&#20221;&#19981;&#21516;&#65292;&#27169;&#22411;&#30340;&#32467;&#26524;&#23558;&#20250;&#26159;&#20160;&#20040;&#65311;&#8221;&#23427;&#36890;&#36807;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#23545;&#27861;&#24459;&#22522;&#30784;&#30340;&#24773;&#26223;&#27979;&#35797;&#36827;&#34892;&#25193;&#23637;&#65292;&#20197;&#25805;&#20316;&#8220;&#32473;&#23450;&#24046;&#24322;&#30340;&#20844;&#24179;&#21407;&#21017;&#8221;&#30340;&#27010;&#24565;&#12290;&#23545;&#20110;&#20219;&#20309;&#25237;&#35785;&#20154;&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#22120;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#25214;&#21040;&#24182;&#27604;&#36739;&#30456;&#20284;&#30340;&#21463;&#20445;&#25252;&#21644;&#38750;&#21463;&#20445;&#25252;&#23454;&#20363;&#65292;&#26500;&#36896;&#25511;&#21046;&#32452;&#21644;&#27979;&#35797;&#32452;&#65292;&#20004;&#32452;&#30340;&#20915;&#31574;&#32467;&#26524;&#24046;&#24322;&#24847;&#21619;&#30528;&#28508;&#22312;&#30340;&#20010;&#20154;&#27495;&#35270;&#12290;&#19982;&#24773;&#22659;&#27979;&#35797;&#19981;&#21516;&#65292;&#24773;&#22659;&#27979;&#35797;&#26159;&#22260;&#32469;&#25237;&#35785;&#20154;&#26500;&#24314;&#20004;&#32452;&#65292;&#25105;&#20204;&#26681;&#25454;&#22240;&#26524;&#30693;&#35782;&#22312;&#25237;&#35785;&#20154;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27979;&#35797;&#32452;&#12290;&#21453;&#20107;&#23454;&#26088;&#22312;&#21453;&#26144;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present counterfactual situation testing (CST), a causal data mining framework for detecting discrimination in classifiers. CST aims to answer in an actionable and meaningful way the intuitive question "what would have been the model outcome had the individual, or complainant, been of a different protected status?" It extends the legally-grounded situation testing of Thanh et al. (2011) by operationalizing the notion of fairness given the difference using counterfactual reasoning. For any complainant, we find and compare similar protected and non-protected instances in the dataset used by the classifier to construct a control and test group, where a difference between the decision outcomes of the two groups implies potential individual discrimination. Unlike situation testing, which builds both groups around the complainant, we build the test group on the complainant's counterfactual generated using causal knowledge. The counterfactual is intended to reflect how the protected attrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26657;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#19981;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.11835</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#30340;&#25628;&#32034;&#26041;&#27861;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs. (arXiv:2302.11835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26657;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#19981;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#23398;&#21644;&#37329;&#34701;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#36890;&#24120;&#28041;&#21450;&#21040;&#23545;&#38750;&#24120;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#26080;&#23548;&#25968;&#25628;&#32034;&#12290;&#26412;&#25991;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23545;&#20247;&#25152;&#21608;&#30693;&#30340;&#23439;&#35266;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#33509;&#24178;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#26041;&#27861;&#25152;&#20570;&#20986;&#30340;&#8220;&#28151;&#21512;&#31574;&#30053;&#8221;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26367;&#20195;&#27169;&#22411;&#30340;&#26041;&#27861;&#29305;&#21035;&#39640;&#25928;&#65292;&#24182;&#19988;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#36890;&#24120;&#20250;&#22686;&#21152;&#24615;&#33021;&#65292;&#22240;&#20026;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#30340;&#20559;&#24046;&#37117;&#20250;&#34987;&#32531;&#35299;&#12290;&#36890;&#36807;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#22312;&#26657;&#20934;&#36816;&#34892;&#36807;&#31243;&#20013;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#12290;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20154;&#21482;&#26377;&#22312;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#26102;&#25165;&#32487;&#32493;&#21033;&#29992;&#29305;&#23450;&#26041;&#27861;&#65292;&#20294;&#22312;&#35813;&#26041;&#27861;&#36798;&#21040;&#24615;&#33021;&#24179;&#21488;&#26102;&#25506;&#32034;&#26032;&#31574;&#30053;&#12290;&#24471;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#25628;&#32034;&#26041;&#26696;&#22312;&#20219;&#20309;&#20854;&#20182;&#27979;&#35797;&#30340;&#26041;&#27861;&#25110;&#26041;&#27861;&#32452;&#21512;&#19978;&#37117;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#19987;&#19994;&#30340;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating agent-based models (ABMs) in economics and finance typically involves a derivative-free search in a very large parameter space. In this work, we benchmark a number of search methods in the calibration of a well-known macroeconomic ABM on real data, and further assess the performance of "mixed strategies" made by combining different methods. We find that methods based on random-forest surrogates are particularly efficient, and that combining search methods generally increases performance since the biases of any single method are mitigated. Moving from these observations, we propose a reinforcement learning (RL) scheme to automatically select and combine search methods on-the-fly during a calibration run. The RL agent keeps exploiting a specific method only as long as this keeps performing well, but explores new strategies when the specific method reaches a performance plateau. The resulting RL search scheme outperforms any other method or method combination tested, and does 
&lt;/p&gt;</description></item><item><title>&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2302.09656</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Imprecise Bayesian Neural Networks. (arXiv:2302.09656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09656
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;, &#30830;&#23450;&#19981;&#30830;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#26159;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20351;&#24471;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#33021;&#22815;&#34987;&#35780;&#20272;&#65292;&#19981;&#21516;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;IBNNs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#27010;&#25324;&#21644;&#20811;&#26381;&#26631;&#20934;BNNs&#30340;&#26576;&#20123;&#32570;&#28857;&#12290;&#26631;&#20934;BNNs&#20351;&#29992;&#21333;&#19968;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;IBNNs&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#20204;&#20801;&#35768;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#27604;&#26631;&#20934;BNNs&#26356;&#21152;&#40065;&#26834;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;PAC&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#38598;&#12290;&#25105;&#20204;&#23558;IBNNs&#24212;&#29992;&#20110;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#19968;&#20010;&#26159;&#20026;&#20102;&#20154;&#24037;&#33008;&#33146;&#25511;&#21046;&#27169;&#25311;&#34880;&#31958;&#21644;&#33008;&#23707;&#32032;&#21160;&#21147;&#23398;&#65292;&#21478;&#19968;&#20010;&#26159;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian neural networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. We present imprecise Bayesian neural networks (IBNNs); they generalize and overcome some of the drawbacks of standard BNNs. These latter are trained using a single prior and likelihood distributions, whereas IBNNs are trained using credal prior and likelihood sets. They allow to distinguish between aleatoric and epistemic uncertainties, and to quantify them. In addition, IBNNs are robust in the sense of Bayesian sensitivity analysis, and are more robust than BNNs to distribution shift. They can also be used to compute sets of outcomes that enjoy PAC-like properties. We apply IBNNs to two case studies. One, to model blood glucose and insulin dynamics for artificial pancreas control, and two, for motion p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09195</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#26131;&#20110;&#23398;&#20064;&#30340;&#26679;&#26412;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#35782;&#21035;&#23545;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#26368;&#26377;&#29992;&#30340;&#31034;&#20363;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;SSL&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;SSL&#30340;&#20215;&#20540;&#22914;&#20309;&#37327;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#65292;&#23545;&#27604;SSL&#20013;&#23545;&#23398;&#20064;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#31034;&#20363;&#26159;&#20855;&#26377;&#26368;&#30456;&#20284;&#25968;&#25454;&#22686;&#24378;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;SSL&#30340;&#24191;&#20041;&#24615;&#33021;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;SSL&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#23376;&#38598;&#26159;&#23545;&#30417;&#30563;&#23398;&#20064;&#20570;&#20986;&#26368;&#23567;&#36129;&#29486;&#30340;&#23376;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23376;&#38598;&#22312;CIFAR100&#12289;CIFAR&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#23376;&#38598;3%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROE&#30340;&#26032;&#22411;&#25968;&#25454;&#27745;&#26579;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22522;&#26412;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36816;&#34892;&#24335;&#36873;&#20030;&#65292;&#26377;&#25928;&#21033;&#29992;logits&#23618;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MNIST&#25968;&#25454;&#38598;&#21644;CIFAR-10&#19978;&#24471;&#21040;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#38598;&#25104;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;</title><link>http://arxiv.org/abs/2302.02300</link><description>&lt;p&gt;
&#36816;&#34892;&#24335;&#36873;&#20030;&#65306;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#25913;&#36827;&#21487;&#35777;&#26126;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Run-Off Election: Improved Provable Defense against Data Poisoning Attacks. (arXiv:2302.02300v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROE&#30340;&#26032;&#22411;&#25968;&#25454;&#27745;&#26579;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22522;&#26412;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36816;&#34892;&#24335;&#36873;&#20030;&#65292;&#26377;&#25928;&#21033;&#29992;logits&#23618;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MNIST&#25968;&#25454;&#38598;&#21644;CIFAR-10&#19978;&#24471;&#21040;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#38598;&#25104;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#12289;&#20462;&#25913;&#25110;&#21024;&#38500;&#26679;&#26412;&#26469;&#25913;&#21464;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#38598;&#25104;&#26041;&#27861;&#30340;&#21487;&#35777;&#26126;&#25968;&#25454;&#27745;&#26579;&#38450;&#24481;&#26041;&#27861;&#65292;&#20854;&#20013;&#39044;&#27979;&#26159;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#36827;&#34892;&#22810;&#25968;&#34920;&#20915;&#26469;&#23436;&#25104;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#32771;&#34385;&#38598;&#25104;&#38450;&#24481;&#20013;&#30340;&#22823;&#22810;&#25968;&#34920;&#20915;&#26159;&#28010;&#36153;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26377;&#25928;&#22320;&#21033;&#29992;&#22522;&#26412;&#27169;&#22411;&#20013;&#30340;logits&#23618;&#20013;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36816;&#34892;&#24335;&#36873;&#20030;&#65288;ROE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22522;&#26412;&#27169;&#22411;&#20043;&#38388;&#30340;&#20004;&#36718;&#36873;&#20030;&#30340;&#26032;&#22411;&#32858;&#21512;&#26041;&#27861;&#65306;&#22312;&#31532;&#19968;&#36718;&#20013;&#65292;&#27169;&#22411;&#20026;&#23427;&#20204;&#39318;&#36873;&#30340;&#31867;&#21035;&#25237;&#31080;&#65292;&#28982;&#21518;&#22312;&#31532;&#19968;&#36718;&#20013;&#25490;&#21517;&#21069;&#20004;&#30340;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#31532;&#20108;&#36718;&#8220;Run-Off&#8221;&#36873;&#20030;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;Deep Partition Aggregation&#65288;DPA&#65289;&#21644;Finite Aggregation&#65288;FA&#65289;&#26041;&#27861;&#30340;DPA+ROE&#21644;FA+ROE&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#21644;CIFAR-10&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;ROE&#30340;&#38450;&#24481;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
In data poisoning attacks, an adversary tries to change a model's prediction by adding, modifying, or removing samples in the training data. Recently, ensemble-based approaches for obtaining provable defenses against data poisoning have been proposed where predictions are done by taking a majority vote across multiple base models. In this work, we show that merely considering the majority vote in ensemble defenses is wasteful as it does not effectively utilize available information in the logits layers of the base models. Instead, we propose Run-Off Election (ROE), a novel aggregation method based on a two-round election across the base models: In the first round, models vote for their preferred class and then a second, Run-Off election is held between the top two classes in the first round. Based on this approach, we propose DPA+ROE and FA+ROE defense methods based on Deep Partition Aggregation (DPA) and Finite Aggregation (FA) approaches from prior work. We evaluate our methods on MN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Seq2Seq-attn&#21644;Transformer&#30340;&#20449;&#36947;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#21453;&#21521;&#25216;&#26415;&#20197;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.00341</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#21450;&#21453;&#21521;&#25216;&#26415;&#30340;&#20449;&#36947;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reverse Ordering Techniques for Attention-Based Channel Prediction. (arXiv:2302.00341v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Seq2Seq-attn&#21644;Transformer&#30340;&#20449;&#36947;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#21453;&#21521;&#25216;&#26415;&#20197;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65288;Seq2Seq-attn&#65289;&#21644;Transformer&#27169;&#22411;&#65292;&#22522;&#20110;&#22122;&#22768;&#35266;&#27979;&#26469;&#39044;&#27979;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#12290;&#20004;&#31181;&#27169;&#22411;&#37117;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25913;&#32534;&#32780;&#26469;&#65292;&#20197;&#24212;&#23545;&#20449;&#36947;&#39044;&#27979;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21453;&#21521;&#20301;&#32622;&#32534;&#30721;&#8221;&#30340;&#26032;&#25216;&#26415;&#20197;&#25552;&#39640;Transformer&#27169;&#22411;&#22312;&#19981;&#21516;&#24207;&#21015;&#38271;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#24212;&#29992;&#27880;&#24847;&#21147;&#20043;&#21069;&#65292;Seq2Seq-attn&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#36755;&#20986;&#20063;&#20250;&#34987;&#32763;&#36716;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21453;&#21521;&#25216;&#26415;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#24207;&#21015;&#20013;&#20449;&#36947;&#30636;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26080;&#35770;&#24207;&#21015;&#38271;&#24230;&#22914;&#20309;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to predict channels in wireless communication systems based on noisy observations, utilizing sequence-to-sequence models with attention (Seq2Seq-attn) and transformer models. Both models are adapted from natural language processing to tackle the complex challenge of channel prediction. Additionally, a new technique called reverse positional encoding is introduced in the transformer model to improve the robustness of the model against varying sequence lengths. Similarly, the encoder outputs of the Seq2Seq-attn model are reversed before applying attention. Simulation results demonstrate that the proposed ordering techniques allow the models to better capture the relationships between the channel snapshots within the sequence, irrespective of the sequence length, as opposed to existing methods.
&lt;/p&gt;</description></item><item><title>NeSyFOLD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#33258;&#21160;&#26144;&#23556;&#31639;&#27861;&#26469;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#12290;</title><link>http://arxiv.org/abs/2301.12667</link><description>&lt;p&gt;
NeSyFOLD: &#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks. (arXiv:2301.12667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12667
&lt;/p&gt;
&lt;p&gt;
NeSyFOLD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#33258;&#21160;&#26144;&#23556;&#31639;&#27861;&#26469;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;NeSyFOLD&#65292;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#21019;&#24314;&#19968;&#20010;NeSyFOLD&#27169;&#22411;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;NeSyFOLD&#30340;&#23398;&#20064;&#27969;&#31243;&#22914;&#19979;&#65306;&#65288;i&#65289;&#25105;&#20204;&#39318;&#20808;&#22312;&#36755;&#20837;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;CNN&#65292;&#24182;&#25552;&#21462;&#26368;&#21518;&#19968;&#23618;&#26680;&#30340;&#28608;&#27963;&#20316;&#20026;&#20108;&#36827;&#21046;&#20540;&#65307;&#65288;ii&#65289;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#33021;&#22815;&#20998;&#31867;&#22270;&#20687;&#30340;&#36923;&#36753;&#31243;&#24207;&#8212;&#8212;&#34920;&#31034;&#20026;&#27599;&#20010;&#26680;&#23545;&#24212;&#30340;&#20108;&#36827;&#21046;&#28608;&#27963;&#21521;&#37327;&#65292;&#21516;&#26102;&#20135;&#29983;&#36923;&#36753;&#35299;&#37322;&#12290;&#30001;FOLD-SE-M&#31639;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#20855;&#26377;&#26680;&#32534;&#21495;&#20316;&#20026;&#35859;&#35789;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#27010;&#24565;&#12290;&#36825;&#20010;&#26144;&#23556;&#34987;&#29992;&#26469;&#23558;&#35268;&#21017;&#38598;&#20013;&#30340;&#35859;&#35789;&#21517;&#65288;&#26680;&#32534;&#21495;&#65289;&#26367;&#25442;&#20026;&#23545;&#24212;&#30340;&#35821;&#20041;&#27010;&#24565;&#26631;&#31614;&#12290;&#32467;&#26524;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#21487;&#20197;&#34987;&#20154;&#31867;&#30452;&#35266;&#22320;&#29702;&#35299;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;NeSyFOLD&#26694;&#26550;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#21644;&#35299;&#37322;&#24615;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel neurosymbolic framework called NeSyFOLD to extract logic rules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD's learning pipeline is as follows: (i) We first pre-train a CNN on the input image dataset and extract activations of the last layer kernels as binary values; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithm to generate a logic program that can classify an image -- represented as a vector of binary activations corresponding to each kernel -- while producing a logical explanation. The rules generated by the FOLD-SE-M algorithm have kernel numbers as predicates. We have devised a novel algorithm for automatically mapping the CNN kernels to semantic concepts in the images. This mapping is used to replace predicate names (kernel numbers) in the rule-set with corresponding semantic concept labels. The resulting rule-set is interpretable, and can be intuitively understood by humans. We compare our NeSyFOLD framework with th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38899;&#39057;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#35821;&#38899;&#39537;&#21160;&#35270;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#22120;&#23454;&#29616;&#23545;&#21767;&#37096;&#21644;&#19979;&#24052;&#36816;&#21160;&#30340;&#21516;&#27493;&#65292;&#36991;&#20813;&#20102;&#23545;&#20013;&#38388;&#32467;&#26500;&#34920;&#31034;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2301.04474</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#39537;&#21160;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Speech Driven Video Editing via an Audio-Conditioned Diffusion Model. (arXiv:2301.04474v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38899;&#39057;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#35821;&#38899;&#39537;&#21160;&#35270;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#22120;&#23454;&#29616;&#23545;&#21767;&#37096;&#21644;&#19979;&#24052;&#36816;&#21160;&#30340;&#21516;&#27493;&#65292;&#36991;&#20813;&#20102;&#23545;&#20013;&#38388;&#32467;&#26500;&#34920;&#31034;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#21040;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#35821;&#38899;&#39537;&#21160;&#35270;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#35828;&#35805;&#32773;&#30340;&#35270;&#39057;&#21644;&#21333;&#29420;&#30340;&#38899;&#39057;&#24405;&#38899;&#65292;&#36890;&#36807;&#23558;&#38899;&#39057;&#26757;&#23572;&#39057;&#35889;&#29305;&#24449;&#20316;&#20026;&#26465;&#20214;&#65292;&#37325;&#26032;&#21516;&#27493;&#21767;&#37096;&#21644;&#19979;&#24052;&#36816;&#21160;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20013;&#38388;&#30340;&#32467;&#26500;&#34920;&#31034;&#65292;&#22914;&#38754;&#37096;&#26631;&#24535;&#25110;3D&#38754;&#37096;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;CREMA-D&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#21333;&#35828;&#35805;&#20154;&#21644;&#22810;&#35828;&#35805;&#20154;&#35270;&#39057;&#32534;&#36753;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;&#21644;&#39564;&#35777;&#24212;&#29992;&#31471;&#21040;&#31471;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21040;&#35821;&#38899;&#39537;&#21160;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronized without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26680;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#29305;&#24449;&#23376;&#31354;&#38388;&#21644;&#26680;&#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29109;&#24230;&#37327;&#26041;&#27861;&#29992;&#20110;&#26680;&#30340;&#27604;&#36739;&#12290;&#29305;&#21035;&#22320;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26497;&#22823;&#30456;&#20851;&#26680;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#20449;&#24687;&#29109;&#24230;&#37327;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#25226;Fisher&#26680;&#35299;&#37322;&#20026;&#19968;&#31181;&#29305;&#27530;&#30340;&#26497;&#22823;&#30456;&#20851;&#26680;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.01410</link><description>&lt;p&gt;
&#26680;&#23376;&#31354;&#38388;&#21644;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Kernel Subspace and Feature Extraction. (arXiv:2301.01410v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26680;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#29305;&#24449;&#23376;&#31354;&#38388;&#21644;&#26680;&#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29109;&#24230;&#37327;&#26041;&#27861;&#29992;&#20110;&#26680;&#30340;&#27604;&#36739;&#12290;&#29305;&#21035;&#22320;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26497;&#22823;&#30456;&#20851;&#26680;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#20449;&#24687;&#29109;&#24230;&#37327;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#25226;Fisher&#26680;&#35299;&#37322;&#20026;&#19968;&#31181;&#29305;&#27530;&#30340;&#26497;&#22823;&#30456;&#20851;&#26680;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29305;&#24449;&#23376;&#31354;&#38388;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26680;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#29305;&#24449;&#23376;&#31354;&#38388;&#21644;&#26680;&#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29109;&#24230;&#37327;&#26041;&#27861;&#29992;&#20110;&#26680;&#30340;&#27604;&#36739;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;Hirschfeld-Gebelein-R\'{e}nyi&#26497;&#22823;&#30456;&#20851;&#20989;&#25968;&#26500;&#25104;&#30340;&#26680;&#65292;&#31216;&#20043;&#20026;&#26497;&#22823;&#30456;&#20851;&#26680;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20449;&#24687;&#29109;&#24230;&#37327;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#20197;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#20026;&#20363;&#65292;&#23558;&#26680;&#26041;&#27861;&#19982;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#34920;&#26126;&#22312;&#26497;&#22823;&#30456;&#20851;&#26680;&#19978;&#30340;&#26680;SVM&#20855;&#26377;&#26368;&#23567;&#39044;&#27979;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25226;Fisher&#26680;&#35299;&#37322;&#20026;&#19968;&#31181;&#29305;&#27530;&#30340;&#26497;&#22823;&#30456;&#20851;&#26680;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study kernel methods in machine learning from the perspective of feature subspace. We establish a one-to-one correspondence between feature subspaces and kernels and propose an information-theoretic measure for kernels. In particular, we construct a kernel from Hirschfeld--Gebelein--R\'{e}nyi maximal correlation functions, coined the maximal correlation kernel, and demonstrate its information-theoretic optimality. We use the support vector machine (SVM) as an example to illustrate a connection between kernel methods and feature extraction approaches. We show that the kernel SVM on maximal correlation kernel achieves minimum prediction error. Finally, we interpret the Fisher kernel as a special maximal correlation kernel and establish its optimality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;NLP&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;CF&#39044;&#38450;&#12289;&#30693;&#35782;&#36801;&#31227;&#21644;&#36328;&#20219;&#21153;&#31867;&#20998;&#31163;&#31561;&#26041;&#38754;&#23545;NLP&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.12701</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;NLP&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;CF&#39044;&#38450;&#12289;&#30693;&#35782;&#36801;&#31227;&#21644;&#36328;&#20219;&#21153;&#31867;&#20998;&#31163;&#31561;&#26041;&#38754;&#23545;NLP&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#27169;&#25311;&#20154;&#31867;&#19981;&#26029;&#23398;&#20064;&#21644;&#31215;&#32047;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#23398;&#36807;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#23398;&#21040;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#26032;&#20219;&#21153;&#26356;&#22909;&#22320;&#23398;&#20064;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;NLP&#20013;CL&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23427;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;CL&#26377;&#26174;&#30528;&#21306;&#21035;&#12290;&#23427;&#28085;&#30422;&#20102;&#65288;1&#65289;&#25152;&#26377;CL&#35774;&#32622;&#21450;&#29616;&#26377;&#25216;&#26415;&#20998;&#31867;&#65307;&#65288;2&#65289;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65307;&#65288;3&#65289;&#30693;&#35782;&#36801;&#31227;&#65288;KT&#65289;&#65292;&#23545;NLP&#20219;&#21153;&#23588;&#20854;&#37325;&#35201;&#65307;&#20197;&#21450;&#65288;4&#65289;&#19968;&#20123;&#29702;&#35770;&#21644;&#20132;&#20219;&#21153;&#31867;&#20998;&#31163;&#65288;ICS&#65289;&#30340;&#38544;&#21547;&#25361;&#25112;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#19968;&#20123;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is a learning paradigm that emulates the human capability of learning and accumulating knowledge continually without forgetting the previously learned knowledge and also transferring the learned knowledge to help learn new tasks better. This survey presents a comprehensive review and analysis of the recent progress of CL in NLP, which has significant differences from CL in computer vision and machine learning. It covers (1) all CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting (CF) prevention, (3) knowledge transfer (KT), which is particularly important for NLP tasks; and (4) some theory and the hidden challenge of inter-task class separation (ICS). (1), (3) and (4) have not been included in the existing survey. Finally, a list of future directions is discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#24191;&#27867;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#23548;&#33268;&#20998;&#25968;&#21305;&#37197;&#30340;&#21407;&#22987;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2211.03595</link><description>&lt;p&gt;
&#20174;&#21435;&#22122;&#25193;&#25955;&#21040;&#21435;&#22122;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Denoising Diffusions to Denoising Markov Models. (arXiv:2211.03595v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#24191;&#27867;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#23548;&#33268;&#20998;&#25968;&#21305;&#37197;&#30340;&#21407;&#22987;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#26159;&#23637;&#29616;&#20986;&#21331;&#36234;&#23454;&#39564;&#24615;&#33021;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#20182;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#25193;&#25955;&#21040;&#39640;&#26031;&#20998;&#24067;&#65292;&#28982;&#21518;&#23398;&#20064;&#36870;&#36716;&#36825;&#20010;&#22122;&#22768;&#36807;&#31243;&#20197;&#33719;&#21462;&#21512;&#25104;&#25968;&#25454;&#28857;&#12290;&#21435;&#22122;&#25193;&#25955;&#20381;&#36182;&#20110;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#23545;&#22122;&#22768;&#25968;&#25454;&#23494;&#24230;&#30340;&#23545;&#25968;&#23548;&#25968;&#30340;&#36924;&#36817;&#12290;&#24403;&#21482;&#33021;&#20174;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20989;&#25968;&#20013;&#36827;&#34892;&#25277;&#26679;&#26102;&#65292;&#36825;&#31181;&#27169;&#22411;&#20063;&#21487;&#29992;&#20110;&#25191;&#34892;&#36817;&#20284;&#21518;&#39564;&#27169;&#25311;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#27492;&#26041;&#27861;&#25512;&#24191;&#21040;&#19968;&#31867;&#24191;&#27867;&#30340;&#31354;&#38388;&#65292;&#24182;&#23548;&#33268;&#20998;&#25968;&#21305;&#37197;&#30340;&#21407;&#22987;&#25193;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#35828;&#26126;&#20102;&#25152;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalising this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#25215;&#35834;&#21644;&#22122;&#22768;&#35266;&#27979;&#30340;$2\times 2$&#38646;&#21644;&#21338;&#24328;&#65292;&#21457;&#29616;&#24179;&#34913;&#28857;&#24635;&#26159;&#23384;&#22312;&#30340;&#65307;&#39046;&#23548;&#32773;&#30340;&#21160;&#20316;&#35266;&#27979;&#32467;&#26524;&#23545;&#20110;&#36861;&#38543;&#32773;&#26469;&#35828;&#35201;&#20040;&#26159;&#26377;&#30410;&#30340;&#65292;&#35201;&#20040;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65307;&#35813;&#21338;&#24328;&#30340;&#25910;&#30410;&#22312;&#22343;&#34913;&#28857;&#19978;&#34987;&#19978;&#30028;&#38480;&#21046;&#20026;&#32431;&#31574;&#30053;&#19979;&#30340;SE&#30340;&#25910;&#30410;&#65292;&#19979;&#30028;&#20026;&#28151;&#21512;&#31574;&#30053;&#19979;&#30340;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2211.01703</link><description>&lt;p&gt;
&#24102;&#25215;&#35834;&#21644;&#22122;&#22768;&#35266;&#27979;&#30340;$2\times 2$&#38646;&#21644;&#21338;&#24328;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
$2 \times 2$ Zero-Sum Games with Commitments and Noisy Observations. (arXiv:2211.01703v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#25215;&#35834;&#21644;&#22122;&#22768;&#35266;&#27979;&#30340;$2\times 2$&#38646;&#21644;&#21338;&#24328;&#65292;&#21457;&#29616;&#24179;&#34913;&#28857;&#24635;&#26159;&#23384;&#22312;&#30340;&#65307;&#39046;&#23548;&#32773;&#30340;&#21160;&#20316;&#35266;&#27979;&#32467;&#26524;&#23545;&#20110;&#36861;&#38543;&#32773;&#26469;&#35828;&#35201;&#20040;&#26159;&#26377;&#30410;&#30340;&#65292;&#35201;&#20040;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65307;&#35813;&#21338;&#24328;&#30340;&#25910;&#30410;&#22312;&#22343;&#34913;&#28857;&#19978;&#34987;&#19978;&#30028;&#38480;&#21046;&#20026;&#32431;&#31574;&#30053;&#19979;&#30340;SE&#30340;&#25910;&#30410;&#65292;&#19979;&#30028;&#20026;&#28151;&#21512;&#31574;&#30053;&#19979;&#30340;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20197;&#19979;&#20551;&#35774;&#19979;&#30340;$2\times 2$&#38646;&#21644;&#21338;&#24328;&#65306;$(1)$&#20854;&#20013;&#19968;&#20301;&#29609;&#23478;&#65288;&#39046;&#23548;&#32773;&#65289;&#25215;&#35834;&#36890;&#36807;&#37319;&#26679;&#32473;&#23450;&#30340;&#27010;&#29575;&#20998;&#24067;&#65288;&#31574;&#30053;&#65289;&#26469;&#36873;&#25321;&#20182;&#30340;&#21160;&#20316;;$(2)$&#39046;&#23548;&#32773;&#23459;&#24067;&#20182;&#30340;&#21160;&#20316;&#65292;&#36825;&#20010;&#21160;&#20316;&#36890;&#36807;&#20108;&#36827;&#21046;&#20449;&#36947;&#34987;&#23545;&#25163;&#65288;&#36861;&#38543;&#32773;&#65289;&#35266;&#23519;&#21040;;$(3)$&#36861;&#38543;&#32773;&#22522;&#20110;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#21644;&#39046;&#23548;&#32773;&#21160;&#20316;&#30340;&#22122;&#22768;&#35266;&#27979;&#26469;&#36873;&#25321;&#22905;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#24179;&#34913;&#28857;&#34987;&#35777;&#26126;&#24635;&#26159;&#23384;&#22312;&#30340;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#39046;&#23548;&#32773;&#30340;&#34892;&#21160;&#23545;&#36861;&#38543;&#32773;&#26469;&#35828;&#23454;&#36136;&#19978;&#35201;&#20040;&#26159;&#26377;&#30410;&#30340;&#65292;&#35201;&#20040;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#20010;&#21338;&#24328;&#30340;&#22343;&#34913;&#28857;&#19978;&#65292;&#25910;&#30410;&#34987;&#19978;&#30028;&#38480;&#21046;&#20026;&#32431;&#31574;&#30053;&#19979;SE&#30340;&#25910;&#30410;&#65307;&#24182;&#19988;&#19979;&#30028;&#20026;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#30410;&#65292;&#36825;&#31561;&#20215;&#20110;&#28151;&#21512;&#31574;&#30053;&#19979;&#30340;SE&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#26465;&#20214;&#26469;&#35266;&#23519;&#22343;&#34913;&#28857;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, $2\times2$ zero-sum games are studied under the following assumptions: $(1)$ One of the players (the leader) commits to choose its actions by sampling a given probability measure (strategy); $(2)$ The leader announces its action, which is observed by its opponent (the follower) through a binary channel; and $(3)$ the follower chooses its strategy based on the knowledge of the leader's strategy and the noisy observation of the leader's action. Under these conditions, the equilibrium is shown to always exist. Interestingly, even subject to noise, observing the actions of the leader is shown to be either beneficial or immaterial for the follower. More specifically, the payoff at the equilibrium of this game is upper bounded by the payoff at the Stackelberg equilibrium (SE) in pure strategies; and lower bounded by the payoff at the Nash equilibrium, which is equivalent to the SE in mixed strategies.Finally, necessary and sufficient conditions for observing the payoff at equi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#23436;&#20840;&#24066;&#22330;&#19979;&#23545;&#20914;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#30340;&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#21644;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.00948</link><description>&lt;p&gt;
&#19981;&#28789;&#27963;&#30340;&#22810;&#36164;&#20135;&#23545;&#20914;&#19981;&#23436;&#20840;&#24066;&#22330;
&lt;/p&gt;
&lt;p&gt;
Inflexible Multi-Asset Hedging of incomplete market. (arXiv:2211.00948v2 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#23436;&#20840;&#24066;&#22330;&#19979;&#23545;&#20914;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#30340;&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#21644;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23436;&#20840;&#24066;&#22330;&#30340;&#20551;&#35774;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#22312;&#19981;&#23436;&#20840;&#24066;&#22330;&#20013;&#19981;&#36215;&#20316;&#29992;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23384;&#22312;&#39118;&#38505;&#22240;&#32032;&#12289;&#26080;&#27969;&#21160;&#24615;&#21644;&#31163;&#25955;&#20132;&#26131;&#26085;&#26399;&#19977;&#31181;&#19981;&#23436;&#25972;&#24615;&#30340;&#19981;&#23436;&#20840;&#24066;&#22330;&#23545;&#20914;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36339;&#36291;&#25193;&#25955;&#27169;&#22411;&#25551;&#36848;&#20102;&#38543;&#26426;&#36164;&#20135;&#20215;&#26684;&#12290;&#37319;&#29992;&#20102;&#21253;&#25324;RNN&#12289;LSTM&#21644;Mogrifier-LSTM&#22312;&#20869;&#30340;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#23545;&#20914;&#31574;&#30053;&#65292;&#24182;&#23454;&#26045;&#21644;&#27604;&#36739;&#20102;MSE Loss&#21644;Huber Loss&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Mogrifier-LSTM&#26159;&#36895;&#24230;&#26368;&#24555;&#12289;&#22312;MSE&#21644;Huber Loss&#19979;&#32467;&#26524;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained under assumptions in the complete market usually don't take effect in the incomplete market. This paper solves the hedging problem in incomplete market with three sources of incompleteness: risk factor, illiquidity, and discrete transaction dates. A new jump-diffusion model is proposed to describe stochastic asset prices. Three neutral networks, including RNN, LSTM, Mogrifier-LSTM are used to attain hedging strategies with MSE Loss and Huber Loss implemented and compared.As a result, Mogrifier-LSTM is the fastest model with the best results under MSE and Huber Loss.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Adversarial Discriminator Ensemble Network&#65288;ADE-Net&#65289;&#26469;&#22686;&#24378;&#39640;&#20809;&#35889;&#22270;&#20687;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#20351;&#29992;&#30456;&#24212;&#30340;&#25915;&#20987;&#19987;&#23478;&#38598;&#21512;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2210.16346</link><description>&lt;p&gt;
&#22312;&#22810;&#27425;&#25915;&#20987;&#19979;&#25552;&#39640;&#39640;&#20809;&#35889;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Hyperspectral Adversarial Robustness Under Multiple Attacks. (arXiv:2210.16346v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16346
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Adversarial Discriminator Ensemble Network&#65288;ADE-Net&#65289;&#26469;&#22686;&#24378;&#39640;&#20809;&#35889;&#22270;&#20687;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#20351;&#29992;&#30456;&#24212;&#30340;&#25915;&#20987;&#19987;&#23478;&#38598;&#21512;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#21333;&#20010;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#20294;&#26159;&#22312;&#36935;&#21040;&#22810;&#31181;&#25915;&#20987;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#27604;&#21333;&#29420;&#35757;&#32451;&#27599;&#20010;&#32593;&#32476;&#37117;&#35201;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Adversarial Discriminator Ensemble Network&#65288;ADE-Net&#65289;&#65292;&#35813;&#32593;&#32476;&#19987;&#27880;&#20110;&#25915;&#20987;&#31867;&#22411;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#19979;&#20445;&#30041;&#26368;&#20339;&#30340;&#27599;&#20010;&#25968;&#25454;&#31867;&#22411;&#26435;&#37325;&#65292;&#21516;&#26102;&#22686;&#24378;&#25972;&#20010;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#37492;&#21035;&#22120;&#32593;&#32476;&#23558;&#25968;&#25454;&#25353;&#25915;&#20987;&#31867;&#22411;&#36827;&#34892;&#20998;&#31163;&#65292;&#36827;&#32780;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#31867;&#22411;&#20351;&#29992;&#30456;&#24212;&#30340;&#25915;&#20987;&#19987;&#23478;&#38598;&#21512;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation models classifying hyperspectral images (HSI) are vulnerable to adversarial examples. Traditional approaches to adversarial robustness focus on training or retraining a single network on attacked data, however, in the presence of multiple attacks these approaches decrease in performance compared to networks trained individually on each attack. To combat this issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net) which focuses on attack type detection and adversarial robustness under a unified model to preserve per data-type weight optimally while robustifiying the overall network. In the proposed method, a discriminator network is used to separate data by attack type into their specific attack-expert ensemble network.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32852;&#21512;&#20998;&#31867;&#21644;&#22810;&#20010;&#26126;&#30830;&#26816;&#27979;&#31867;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#22312;&#20445;&#35777;&#21487;&#39564;&#35777;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#30784;&#19978;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#22810;&#20010;&#26126;&#30830;&#24323;&#26435;&#31867;&#21035;&#30340;&#32593;&#32476;&#30340;&#20445;&#38556;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#25239;&#20102;&#27169;&#22411;&#30340;&#36864;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#26377;&#21033;&#30340;&#26631;&#20934;&#21644;&#40065;&#26834;&#24615;&#39564;&#35777;&#20934;&#30830;&#24615;&#24179;&#34913;&#28857;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.14410</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#20998;&#31867;&#21644;&#22810;&#20010;&#26126;&#30830;&#26816;&#27979;&#31867;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes. (arXiv:2210.14410v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32852;&#21512;&#20998;&#31867;&#21644;&#22810;&#20010;&#26126;&#30830;&#26816;&#27979;&#31867;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#22312;&#20445;&#35777;&#21487;&#39564;&#35777;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#30784;&#19978;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#22810;&#20010;&#26126;&#30830;&#24323;&#26435;&#31867;&#21035;&#30340;&#32593;&#32476;&#30340;&#20445;&#38556;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#25239;&#20102;&#27169;&#22411;&#30340;&#36864;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#26377;&#21033;&#30340;&#26631;&#20934;&#21644;&#40065;&#26834;&#24615;&#39564;&#35777;&#20934;&#30830;&#24615;&#24179;&#34913;&#28857;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#24320;&#21457;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#26377;&#20445;&#38556;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#32852;&#21512;&#40065;&#26834;&#24615;&#20998;&#31867;&#21644;&#26816;&#27979;&#34987;&#26368;&#36817;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#21487;&#39564;&#35777;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#27491;&#30830;&#20998;&#31867;&#25110;&#20998;&#37197;&#21040;&#8220;&#24323;&#26435;&#8221;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#26679;&#30340;&#21487;&#35777;&#26126;&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#26126;&#30830;&#24323;&#26435;&#31867;&#21035;&#30340;&#32593;&#32476;&#20013;&#32780;&#33719;&#30410;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#36866;&#24212;&#22320;&#20998;&#37197;&#21040;&#37027;&#20123;&#31867;&#21035;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#31616;&#21333;&#22320;&#28155;&#21152;&#22810;&#20010;&#24323;&#26435;&#31867;&#21035;&#21487;&#33021;&#20250;&#23548;&#33268;&#8220;&#27169;&#22411;&#36864;&#21270;&#8221;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#31181;&#36864;&#21270;&#65292;&#36890;&#36807;&#20419;&#36827;&#20805;&#20998;&#20351;&#29992;&#22810;&#20010;&#24323;&#26435;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19968;&#33268;&#22320;&#23454;&#29616;&#20102;&#26377;&#21033;&#30340;&#26631;&#20934;&#21644;&#40065;&#26834;&#24615;&#39564;&#35777;&#20934;&#30830;&#24615;&#24179;&#34913;&#28857;&#65292;&#24182;&#22312;&#21508;&#31181;&#36873;&#25321;&#24323;&#26435;&#31867;&#21035;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work concerns the development of deep networks that are certifiably robust to adversarial attacks. Joint robust classification-detection was recently introduced as a certified defense mechanism, where adversarial examples are either correctly classified or assigned to the "abstain" class. In this work, we show that such a provable framework can benefit by extension to networks with multiple explicit abstain classes, where the adversarial examples are adaptively assigned to those. We show that naively adding multiple abstain classes can lead to "model degeneracy", then we propose a regularization approach and a training method to counter this degeneracy by promoting full use of the multiple abstain classes. Our experiments demonstrate that the proposed approach consistently achieves favorable standard vs. robust verified accuracy tradeoffs, outperforming state-of-the-art algorithms for various choices of number of abstain classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2210.13011</link><description>&lt;p&gt;
&#35770;&#22810;&#21160;&#20316;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
On Many-Actions Policy Gradient. (arXiv:2210.13011v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#22810;&#21160;&#20316;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#23427;&#20915;&#23450;&#20102;&#24403;&#19982;&#27604;&#20363;&#25193;&#23637;&#36712;&#36857;&#30340;&#21333;&#21160;&#20316;&#20195;&#29702;&#30456;&#27604;&#65292;&#22810;&#21160;&#20316;SPG&#20135;&#29983;&#27604;&#36739;&#20302;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#65288;MBMA&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;SPG&#32972;&#26223;&#19979;&#21033;&#29992;&#21160;&#24577;&#27169;&#22411;&#36827;&#34892;&#22810;&#21160;&#20316;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#21160;&#20316;SPG&#23454;&#29616;&#25152;&#28041;&#21450;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#22411;&#27169;&#25311;&#30340;&#22238;&#21512;&#20013;&#25552;&#20379;&#19982;SPG&#30456;&#24403;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MBMA&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#32467;&#26500;&#19982;&#29702;&#35770;&#39044;&#27979;&#30340;&#30456;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#21160;&#20316;&#29615;&#22659;&#20013;&#65292;MBMA&#19982;&#26080;&#27169;&#22411;&#65292;&#22810;&#21160;&#20316;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#22522;&#32447;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the variance of stochastic policy gradients (SPGs) with many action samples per state. We derive a many-actions optimality condition, which determines when many-actions SPG yields lower variance as compared to a single-action agent with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA), an approach leveraging dynamics models for many-actions sampling in the context of SPG. MBMA addresses issues associated with existing implementations of many-actions SPG and yields lower bias and comparable variance to SPG estimated from states in model-simulated rollouts. We find that MBMA bias and variance structure matches that predicted by theory. As a result, MBMA achieves improved sample efficiency and higher returns on a range of continuous action environments as compared to model-free, many-actions, and model-based on-policy SPG baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#29992;&#20110;&#30697;&#38453;&#23436;&#25104;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24471;&#21040;&#20102;&#40654;&#26364;&#20960;&#20309;&#21644;&#35757;&#32451;&#28176;&#36817;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#39640;&#29366;&#24577;&#31354;&#38388;&#20307;&#31215;&#20559;&#35265;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12497</link><description>&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#29992;&#20110;&#30697;&#38453;&#23436;&#25104;&#8212;&#8212;&#26080;&#31351;&#28145;&#24230;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Linear Networks for Matrix Completion -- An Infinite Depth Limit. (arXiv:2210.12497v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#29992;&#20110;&#30697;&#38453;&#23436;&#25104;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24471;&#21040;&#20102;&#40654;&#26364;&#20960;&#20309;&#21644;&#35757;&#32451;&#28176;&#36817;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#39640;&#29366;&#24577;&#31354;&#38388;&#20307;&#31215;&#20559;&#35265;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#65288;DLN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#36229;&#21442;&#25968;&#23398;&#20064;&#32467;&#26500;&#30340;&#26799;&#24230;&#19979;&#38477;&#38544;&#24335;&#27491;&#21017;&#21270;&#27169;&#22411;&#12290;&#35757;&#32451;DLN&#23545;&#24212;&#20110;&#40654;&#26364;&#26799;&#24230;&#27969;&#65292;&#20854;&#20013;&#40654;&#26364;&#24230;&#37327;&#30001;&#32593;&#32476;&#32467;&#26500;&#23450;&#20041;&#65292;&#25439;&#22833;&#20989;&#25968;&#30001;&#23398;&#20064;&#20219;&#21153;&#23450;&#20041;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#19968;&#20960;&#20309;&#26694;&#26550;&#65292;&#24471;&#21040;&#20102;&#21367;&#31215;&#20307;&#31215;&#24418;&#24335;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#65292;&#21253;&#25324;&#32593;&#32476;&#20855;&#26377;&#26080;&#31351;&#28145;&#24230;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#30740;&#31350;&#20102;&#40654;&#26364;&#20960;&#20309;&#21644;&#30697;&#38453;&#23436;&#25104;&#35757;&#32451;&#30340;&#28176;&#36817;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#24314;&#35758;&#65292;&#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#39640;&#29366;&#24577;&#31354;&#38388;&#20307;&#31215;&#20559;&#35265;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deep linear network (DLN) is a model for implicit regularization in gradient based optimization of overparametrized learning architectures. Training the DLN corresponds to a Riemannian gradient flow, where the Riemannian metric is defined by the architecture of the network and the loss function is defined by the learning task. We extend this geometric framework, obtaining explicit expressions for the volume form, including the case when the network has infinite depth. We investigate the link between the Riemannian geometry and the training asymptotics for matrix completion with rigorous analysis and numerics. We propose that implicit regularization is a result of bias towards high state space volume.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#35299;&#20915;&#22823;&#26102;&#38388;&#38271;&#24230;&#19979;&#30340;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#20854;&#35745;&#31639;&#25104;&#26412;&#19981;&#21253;&#25324;&#20381;&#36182;&#20110;&#26102;&#38388;&#38271;&#24230;&#30340;&#39033;&#12290;</title><link>http://arxiv.org/abs/2210.07513</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Continuous-in-time Limit for Bayesian Bandits. (arXiv:2210.07513v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#35299;&#20915;&#22823;&#26102;&#38388;&#38271;&#24230;&#19979;&#30340;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#20854;&#35745;&#31639;&#25104;&#26412;&#19981;&#21253;&#25324;&#20381;&#36182;&#20110;&#26102;&#38388;&#38271;&#24230;&#30340;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#19979;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36172;&#21338;&#26426;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#23547;&#25214;&#26368;&#20248;&#31574;&#30053;&#20197;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;&#38754;&#23545;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#65292;&#24403;&#38382;&#39064;&#30340;&#26102;&#38388;&#38271;&#24230;&#25110;&#33218;&#25968;&#36739;&#22823;&#26102;&#65292;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#36866;&#24403;&#30340;&#37325;&#32553;&#25918;&#19979;&#65292;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#25910;&#25947;&#20110;&#19968;&#20010;&#36830;&#32493;&#30340;&#21704;&#23494;&#23572;&#39039; - &#38597;&#21508;&#27604; - &#36125;&#23572;&#26364;&#65288;HJB&#65289;&#26041;&#31243;&#12290;&#23545;&#20110;&#24120;&#35265;&#30340;&#19968;&#20123;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21487;&#20197;&#26126;&#30830;&#33719;&#24471;&#26497;&#38480;HJB&#26041;&#31243;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#26080;&#27861;&#26126;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35299;&#20915;HJB&#26041;&#31243;&#30340;&#25968;&#23383;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#35299;&#20915;&#22823;&#26102;&#38388;&#38271;&#24230;&#19979;&#30340;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#19981;&#21253;&#25324;&#20381;&#36182;&#20110;&#26102;&#38388;&#38271;&#24230;&#30340;&#39033;&#65292;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the bandit problem in the Bayesian setting. The Bayesian approach formulates the bandit problem as an optimization problem, and the goal is to find the optimal policy which minimizes the Bayesian regret. One of the main challenges facing the Bayesian approach is that computation of the optimal policy is often intractable, especially when the length of the problem horizon or the number of arms is large. In this paper, we first show that under a suitable rescaling, the Bayesian bandit problem converges toward a continuous Hamilton-Jacobi-Bellman (HJB) equation. The optimal policy for the limiting HJB equation can be explicitly obtained for several common bandit problems, and we give numerical methods to solve the HJB equation when an explicit solution is not available. Based on these results, we propose an approximate Bayes-optimal policy for solving Bayesian bandit problems with large horizons. Our method has the added benefit that its computational cost does not inc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#20070;&#31821;&#30340;&#20840;&#25991;&#20869;&#23481;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#30740;&#31350;&#39044;&#27979;&#20070;&#31821;&#26159;&#21542;&#20250;&#25104;&#20026;&#30021;&#38144;&#20070;&#12290;&#20351;&#29992;&#20102; SemAxis &#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#25968;&#25454;&#21021;&#27493;&#25506;&#32034;&#65292;&#37319;&#29992;&#22810;&#31181;&#20998;&#31867;&#22120;&#33719;&#24471;&#23450;&#37327;&#21644;&#26356;&#21152;&#23458;&#35266;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.02334</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#25991;&#20869;&#23481;&#34920;&#24449;&#21644;&#35782;&#21035;&#30021;&#38144;&#20070;
&lt;/p&gt;
&lt;p&gt;
Using Full-Text Content to Characterize and Identify Best Seller Books. (arXiv:2210.02334v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#20070;&#31821;&#30340;&#20840;&#25991;&#20869;&#23481;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#30740;&#31350;&#39044;&#27979;&#20070;&#31821;&#26159;&#21542;&#20250;&#25104;&#20026;&#30021;&#38144;&#20070;&#12290;&#20351;&#29992;&#20102; SemAxis &#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#25968;&#25454;&#21021;&#27493;&#25506;&#32034;&#65292;&#37319;&#29992;&#22810;&#31181;&#20998;&#31867;&#22120;&#33719;&#24471;&#23450;&#37327;&#21644;&#26356;&#21152;&#23458;&#35266;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#20316;&#21697;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#23427;&#20204;&#22312;&#35835;&#32773;&#20013;&#30340;&#25509;&#21463;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25991;&#33402;&#20316;&#21697;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#29305;&#21035;&#26159;&#35780;&#20272;&#39044;&#27979;&#20070;&#31821;&#26159;&#21542;&#20250;&#25104;&#20026;&#30021;&#38144;&#20070;&#30340;&#20219;&#21153;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20070;&#31821;&#30340;&#20840;&#25991;&#20869;&#23481;&#65292;&#24182;&#32771;&#34385;&#20102;&#21487;&#35270;&#21270;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992; SemAxis &#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#25968;&#25454;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#33719;&#24471;&#23450;&#37327;&#21644;&#26356;&#21152;&#23458;&#35266;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#25968;&#25454;&#38598;&#19968;&#36215;&#20351;&#29992;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;1895&#24180;&#21040;1924&#24180;&#20986;&#29256;&#30340;&#20070;&#31821;&#65292;&#24182;&#34987;&#12298;&#20986;&#29256;&#21608;&#21002;&#30021;&#38144;&#20070;&#27036;&#12299;&#30830;&#23450;&#20026;&#30021;&#38144;&#20070;&#21644;&#22312;&#21516;&#19968;&#26102;&#26399;&#20986;&#29256;&#20294;&#26410;&#34987;&#25552;&#21450;&#30340;&#25991;&#23398;&#20316;&#21697;&#12290;&#25105;&#20204;&#26041;&#27861;&#27604;&#36739;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#22909;&#30340;&#25104;&#32489;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Artistic pieces can be studied from several perspectives, one example being their reception among readers over time. In the present work, we approach this interesting topic from the standpoint of literary works, particularly assessing the task of predicting whether a book will become a best seller. Dissimilarly from previous approaches, we focused on the full content of books and considered visualization and classification tasks. We employed visualization for the preliminary exploration of the data structure and properties, involving SemAxis and linear discriminant analyses. Then, to obtain quantitative and more objective results, we employed various classifiers. Such approaches were used along with a dataset containing (i) books published from 1895 to 1924 and consecrated as best sellers by the Publishers Weekly Bestseller Lists and (ii) literary works published in the same period but not being mentioned in that list. Our comparison of methods revealed that the best-achieved result 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.06049</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65306;&#20197;&#21360;&#24230;&#27861;&#24459;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22686;&#22810;&#65292;&#29305;&#21035;&#26159;&#22312;&#27431;&#32654;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#65292;PLMs&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#31561;&#20854;&#20182;&#22269;&#23478;&#30340;&#27861;&#24459;&#25991;&#26412;&#20855;&#26377;&#24456;&#22810;&#29305;&#27530;&#29305;&#24449;&#65292;&#22240;&#27492;&#20063;&#38656;&#35201;&#22312;&#36825;&#20123;&#26041;&#38754;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21360;&#24230;&#27861;&#24459;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65288;&#32487;&#32493;&#39044;&#35757;&#32451;&#65289;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#27861;&#24459;PLMs, LegalBERT&#21644;CaseLawBERT&#65292;&#20197;&#21450;&#20351;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;PLMs&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#8212;&#8212;&#20174;&#20107;&#23454;&#20013;&#35782;&#21035;&#27861;&#24459;&#27861;&#35268;&#12289;&#23545;&#27861;&#38498;&#21028;&#20915;&#25991;&#20214;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#21450;&#39044;&#27979;&#27861;&#38498;&#19978;&#35785;&#21028;&#20915;--&#22312;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#25991;&#26412;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#31867;&#26234;&#24935;&#22522;&#20934;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22522;&#20934;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#21019;&#24314;ML&#22522;&#20934;&#26102;&#24212;&#32771;&#34385;&#21644;&#35760;&#24405;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2209.00692</link><description>&lt;p&gt;
&#21019;&#36896;&#26234;&#33021;&#65306;&#26234;&#21830;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#30340;&#20262;&#29702;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Making Intelligence: Ethical Values in IQ and ML Benchmarks. (arXiv:2209.00692v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#31867;&#26234;&#24935;&#22522;&#20934;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22522;&#20934;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#21019;&#24314;ML&#22522;&#20934;&#26102;&#24212;&#32771;&#34385;&#21644;&#35760;&#24405;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#23450;&#20041;&#21644;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19968;&#20123;&#20154;&#22312;&#20851;&#27880;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;ML&#30740;&#31350;&#30340;&#20262;&#29702;&#36947;&#24503;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20262;&#29702;&#21644;&#30475;&#20284;&#8220;&#25216;&#26415;&#8221;&#25110;&#8220;&#31185;&#23398;&#8221;&#20915;&#31574;&#22312;ML&#22522;&#20934;&#35774;&#35745;&#20013;&#30340;&#32416;&#32544;&#12290;&#25105;&#20204;&#30340;&#20986;&#21457;&#28857;&#26159;&#20154;&#31867;&#26234;&#21147;&#22522;&#20934;&#21644;ML&#22522;&#20934;&#20043;&#38388;&#23384;&#22312;&#22810;&#37325;&#34987;&#24573;&#35270;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#36825;&#20004;&#31181;&#22522;&#20934;&#37117;&#35774;&#23450;&#20102;&#29992;&#20110;&#25551;&#36848;&#12289;&#35780;&#20272;&#21644;&#27604;&#36739;&#26234;&#21147;&#30456;&#20851;&#20219;&#21153;&#34920;&#29616;&#30340;&#26631;&#20934;&#65292;&#35768;&#22810;&#20154;&#31867;&#26234;&#21147;&#23398;&#32773;&#38271;&#26399;&#20197;&#26469;&#24050;&#32463;&#35748;&#35782;&#21040;&#36825;&#20123;&#26631;&#20934;&#26159;&#20215;&#20540;&#35266;&#36127;&#36733;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#22899;&#24615;&#20027;&#20041;&#31185;&#23398;&#21746;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#21402;&#27010;&#24565;&#35270;&#35282;&#26469; &#35770;&#35777;&#65292;&#22312;&#21019;&#24314;ML&#22522;&#20934;&#26102;&#38656;&#35201;&#32771;&#34385;&#21644;&#35760;&#24405;&#20215;&#20540;&#35266;&#12290;&#36890;&#36807;&#21019;&#24314;&#20851;&#20110;&#25968;&#25454;&#38598;&#26412;&#36523;&#30340;&#35268;&#33539;&#65292;&#32780;&#19981;&#26159;&#38544;&#21547;&#30340;&#20262;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, ML researchers have wrestled with defining and improving machine learning (ML) benchmarks and datasets. In parallel, some have trained a critical lens on the ethics of dataset creation and ML research. In this position paper, we highlight the entanglement of ethics with seemingly ``technical'' or ``scientific'' decisions about the design of ML benchmarks. Our starting point is the existence of multiple overlooked structural similarities between human intelligence benchmarks and ML benchmarks. Both types of benchmarks set standards for describing, evaluating, and comparing performance on tasks relevant to intelligence -- standards that many scholars of human intelligence have long recognized as value-laden. We use perspectives from feminist philosophy of science on IQ benchmarks and thick concepts in social science to argue that values need to be considered and documented when creating ML benchmarks. It is neither possible nor desirable to avoid this choice by creating 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#23383;&#24149;&#29983;&#25104;&#25216;&#26415;&#65292;&#33258;&#21160;&#21457;&#29616;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#20294;&#36924;&#30495;&#30340;&#36755;&#20837;&#65292;&#32858;&#31867;&#21644;&#25551;&#36848;&#65292;&#35780;&#20272;&#21644;&#21457;&#29616;&#20998;&#31867;&#22120;&#30340;&#22833;&#36133;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#26500;&#24314;&#23454;&#29992;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2208.08831</link><description>&lt;p&gt;
&#21033;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#23383;&#24149;&#29983;&#25104;&#25216;&#26415;&#21457;&#29616;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning. (arXiv:2208.08831v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#23383;&#24149;&#29983;&#25104;&#25216;&#26415;&#65292;&#33258;&#21160;&#21457;&#29616;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#20294;&#36924;&#30495;&#30340;&#36755;&#20837;&#65292;&#32858;&#31867;&#21644;&#25551;&#36848;&#65292;&#35780;&#20272;&#21644;&#21457;&#29616;&#20998;&#31867;&#22120;&#30340;&#22833;&#36133;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#26500;&#24314;&#23454;&#29992;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#19979;&#33258;&#21160;&#21457;&#29616;&#35270;&#35273;&#27169;&#22411;&#30340;&#22833;&#36133;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#29616;&#25104;&#22823;&#22411;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26469;&#33258;&#21160;&#21457;&#29616;&#36825;&#20123;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#23558;&#26465;&#20214;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#20294;&#36924;&#30495;&#30340;&#36755;&#20837;&#12290;&#20998;&#31867;&#38169;&#35823;&#30340;&#36755;&#20837;&#23558;&#34987;&#32858;&#31867;&#24182;&#20351;&#29992;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#26469;&#25551;&#36848;&#27599;&#20010;&#32858;&#31867;&#12290;&#27599;&#20010;&#32858;&#31867;&#30340;&#25551;&#36848;&#20381;&#27425;&#29992;&#20110;&#29983;&#25104;&#26356;&#22810;&#36755;&#20837;&#24182;&#35780;&#20272;&#26159;&#21542;&#29305;&#23450;&#32858;&#31867;&#24341;&#36215;&#30340;&#22833;&#36133;&#36229;&#20986;&#20102;&#39044;&#26399;&#12290;&#25105;&#20204;&#21033;&#29992;&#27492;&#27969;&#31243;&#28436;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#26597;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#25214;&#21040;&#29305;&#23450;&#22833;&#36133;&#26696;&#20363;&#24182;&#21457;&#29616;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#25193;&#23637;&#27492;&#26041;&#27861;&#65292;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#20998;&#31867;&#22120;&#26550;&#26500;&#30340;&#23545;&#25239;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#20316;&#20026;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#30340;&#27010;&#24565;&#39564;&#35777;&#65292;&#29992;&#20110;&#21457;&#29616;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#22833;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically discovering failures in vision models under real-world settings remains an open challenge. This work demonstrates how off-the-shelf, large-scale, image-to-text and text-to-image models, trained on vast amounts of data, can be leveraged to automatically find such failures. In essence, a conditional text-to-image generative model is used to generate large amounts of synthetic, yet realistic, inputs given a ground-truth label. Misclassified inputs are clustered and a captioning model is used to describe each cluster. Each cluster's description is used in turn to generate more inputs and assess whether specific clusters induce more failures than expected. We use this pipeline to demonstrate that we can effectively interrogate classifiers trained on ImageNet to find specific failure cases and discover spurious correlations. We also show that we can scale the approach to generate adversarial datasets targeting specific classifier architectures. This work serves as a proof-of-co
&lt;/p&gt;</description></item><item><title>FedOBD&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#35299;&#20026;&#35821;&#20041;&#22359;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#26426;&#20250;&#20027;&#20041;&#22320;&#19978;&#20256;&#37327;&#21270;&#30340;&#22359;&#26469;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#27492;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25805;&#20316;&#27169;&#22411;&#21442;&#25968;&#25152;&#20135;&#29983;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.05174</link><description>&lt;p&gt;
FedOBD&#65306;&#26426;&#20250;&#20027;&#20041;&#22359;&#20002;&#24323;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#39640;&#25928;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning. (arXiv:2208.05174v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05174
&lt;/p&gt;
&lt;p&gt;
FedOBD&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#35299;&#20026;&#35821;&#20041;&#22359;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#26426;&#20250;&#20027;&#20041;&#22320;&#19978;&#20256;&#37327;&#21270;&#30340;&#22359;&#26469;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#27492;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25805;&#20316;&#27169;&#22411;&#21442;&#25968;&#25152;&#20135;&#29983;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#38750;&#24120;&#36866;&#21512;&#34892;&#19994;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#24403;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#19979;&#35757;&#32451;&#23384;&#22312;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#27169;&#22411;&#21442;&#25968;dropout&#65292;&#20294;&#21333;&#29420;&#25805;&#20316;&#27169;&#22411;&#21442;&#25968;&#19981;&#20165;&#22312;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#26080;&#27861;&#26377;&#25928;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#23545;&#32553;&#25918;&#21644;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#26426;&#20250;&#20027;&#20041;&#22359;&#20002;&#24323;&#65288;FedOBD&#65289;&#26041;&#27861;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#28857;&#22312;&#20110;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#35299;&#20026;&#35821;&#20041;&#22359;&#65292;&#20197;&#20415;&#32852;&#37030;&#23398;&#20064;&#21442;&#19982;&#32773;&#21487;&#20197;&#26426;&#20250;&#20027;&#20041;&#22320;&#19978;&#20256;&#37327;&#21270;&#30340;&#22359;&#65292;&#36825;&#20123;&#22359;&#34987;&#35748;&#20026;&#23545;&#20110;&#35757;&#32451;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#36827;&#34892;&#32858;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;FedOBD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural networks possess considerable expressive power. They are well-suited for complex learning tasks in industrial applications. However, large-scale models pose significant challenges for training under the current Federated Learning (FL) paradigm. Existing approaches for efficient FL training often leverage model parameter dropout. However, manipulating individual model parameters is not only inefficient in meaningfully reducing the communication overhead when training large-scale FL models, but may also be detrimental to the scaling efforts and model performance as shown by recent research. To address these issues, we propose the Federated Opportunistic Block Dropout (FedOBD) approach. The key novelty is that it decomposes large-scale models into semantic blocks so that FL participants can opportunistically upload quantized blocks, which are deemed to be significant towards training the model, to the FL server for aggregation. Extensive experiments evaluating FedOBD ag
&lt;/p&gt;</description></item><item><title>PointConvFormer &#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#26500;&#24314;&#27169;&#22359;&#65292;&#23427;&#21033;&#29992;&#29305;&#24449;&#24046;&#24322;&#35745;&#31639;&#20986;&#30340;&#27880;&#24847;&#21147;&#26469;&#20462;&#25913;&#21367;&#31215;&#26435;&#37325;&#65292;&#20445;&#30041;&#20102;&#28857;&#21367;&#31215;&#30340;&#19981;&#21464;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#36873;&#25321;&#20855;&#26377;&#30456;&#20851;&#24615;&#30340;&#28857;&#36827;&#34892;&#21367;&#31215;&#25805;&#20316;&#65292;&#36866;&#29992;&#20110;&#28857;&#32423;&#32454;&#33410;&#30340;&#22810;&#20010;&#20219;&#21153;&#65292;&#26082;&#25552;&#39640;&#20102;&#31934;&#24230;&#65292;&#21448;&#25552;&#39640;&#20102;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2208.02879</link><description>&lt;p&gt;
PointConvFormer&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28857;&#20113;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
PointConvFormer: Revenge of the Point-based Convolution. (arXiv:2208.02879v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02879
&lt;/p&gt;
&lt;p&gt;
PointConvFormer &#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#26500;&#24314;&#27169;&#22359;&#65292;&#23427;&#21033;&#29992;&#29305;&#24449;&#24046;&#24322;&#35745;&#31639;&#20986;&#30340;&#27880;&#24847;&#21147;&#26469;&#20462;&#25913;&#21367;&#31215;&#26435;&#37325;&#65292;&#20445;&#30041;&#20102;&#28857;&#21367;&#31215;&#30340;&#19981;&#21464;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#36873;&#25321;&#20855;&#26377;&#30456;&#20851;&#24615;&#30340;&#28857;&#36827;&#34892;&#21367;&#31215;&#25805;&#20316;&#65292;&#36866;&#29992;&#20110;&#28857;&#32423;&#32454;&#33410;&#30340;&#22810;&#20010;&#20219;&#21153;&#65292;&#26082;&#25552;&#39640;&#20102;&#31934;&#24230;&#65292;&#21448;&#25552;&#39640;&#20102;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;PointConvFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#28857;&#20113;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#30340;&#26032;&#39062;&#26500;&#24314;&#27169;&#22359;&#12290;PointConvFormer&#32467;&#21512;&#20102;&#28857;&#21367;&#31215;&#21644;Transformer&#24605;&#24819;&#65292;&#21033;&#29992;&#29305;&#24449;&#24046;&#24322;&#35745;&#31639;&#20986;&#30340;&#27880;&#24847;&#21147;&#26469;&#20462;&#25913;&#27599;&#20010;&#28857;&#30340;&#21367;&#31215;&#26435;&#37325;&#12290;&#23427;&#36866;&#29992;&#20110;&#38656;&#35201;&#28857;&#32423;&#32454;&#33410;&#30340;&#22810;&#20010;&#20219;&#21153;&#65292;&#22914;&#20998;&#21106;&#21644;&#22330;&#26223;&#27969;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;ScanNet&#12289;SemanticKitti&#12289;FlyingThings3D&#21644;KITTI&#65292;&#32467;&#26524;&#34920;&#26126;PointConvFormer&#27604;&#32463;&#20856;&#21367;&#31215;&#12289;&#24120;&#35268;Transformer&#21644;&#20307;&#32032;&#21367;&#31215;&#25552;&#20379;&#26356;&#22909;&#30340;&#31934;&#24230;-&#36895;&#24230;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PointConvFormer, a novel building block for point cloud based deep network architectures. Inspired by generalization theory, PointConvFormer combines ideas from point convolution, where filter weights are only based on relative position, and Transformers which utilize feature-based attention. In PointConvFormer, attention computed from feature difference between points in the neighborhood is used to modify the convolutional weights at each point. Hence, we preserved the invariances from point convolution, whereas attention helps to select relevant points in the neighborhood for convolution. PointConvFormer is suitable for multiple tasks that require details at the point level, such as segmentation and scene flow estimation tasks. We experiment on both tasks with multiple datasets including ScanNet, SemanticKitti, FlyingThings3D and KITTI. Our results show that PointConvFormer offers a better accuracy-speed tradeoff than classic convolutions, regular transformers, and voxel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#20998;&#24067;&#30340;&#20934;&#30830;&#24615;&#19982;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#24378;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#36866;&#29992;&#20110;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22312;&#32447;&#19968;&#33268;&#24615;&#8221;&#36825;&#19968;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#20272;&#31639;&#27809;&#26377;&#26631;&#35760;&#30340;OOD&#39564;&#35777;&#38598;&#26102;&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.13089</link><description>&lt;p&gt;
&#8220;&#22312;&#32447;&#19968;&#33268;&#24615;&#8221;&#65306;&#39044;&#27979;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift. (arXiv:2206.13089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#20998;&#24067;&#30340;&#20934;&#30830;&#24615;&#19982;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#24378;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#36866;&#29992;&#20110;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22312;&#32447;&#19968;&#33268;&#24615;&#8221;&#36825;&#19968;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#20272;&#31639;&#27809;&#26377;&#26631;&#35760;&#30340;OOD&#39564;&#35777;&#38598;&#26102;&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Miller&#31561;&#20154;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#20854;&#22312;&#20960;&#20010;OOD&#22522;&#20934;&#19978;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#24378;&#28872;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#20182;&#20204;&#31216;&#20043;&#20026;&#8220;&#20934;&#30830;&#24615;&#22312;&#32447;&#8221;&#12290; &#34429;&#28982;&#36825;&#23545;&#20110;&#27169;&#22411;&#36873;&#25321;&#65288;&#21363;&#65292;ID&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#27169;&#22411;&#26368;&#26377;&#21487;&#33021;&#34920;&#29616;&#26368;&#22909;OOD&#65289;&#26159;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#36825;&#19968;&#20107;&#23454;&#26080;&#27861;&#24110;&#21161;&#20272;&#35745;&#27809;&#26377;&#26631;&#35760;&#30340;OOD&#39564;&#35777;&#38598;&#30340;&#27169;&#22411;&#23454;&#38469;OOD&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#29616;&#35937;&#20063;&#36866;&#29992;&#20110;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65306;&#27599;&#24403;&#20934;&#30830;&#24615;&#22312;&#32447;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20219;&#24847;&#20004;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;&#20855;&#26377;&#28508;&#22312;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#65289;&#30340;&#39044;&#27979;&#22312;OOD&#19978;&#30340;&#21327;&#35758;&#20063;&#19982;&#20854;ID&#21327;&#35758;&#20043;&#38388;&#20855;&#26377;&#24378;&#28872;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;OOD&#19982;ID&#21327;&#35758;&#30340;&#26012;&#29575;&#21644;&#20559;&#24046;&#19982;OOD&#19982;ID&#20934;&#30830;&#24230;&#38750;&#24120;&#25509;&#36817;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#25105;&#20204;&#31216;&#20026;&#8220;&#22312;&#32447;&#19968;&#33268;&#24615;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;OOD&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#26631;&#35760;&#30340;OOD&#39564;&#35777;&#38598;&#12290; &#25105;&#20204;&#36890;&#36807;&#22312;&#20960;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#65288;ImageNet&#12289;CIFAR-10-C&#21644;-10-P&#65289;&#19978;&#37096;&#32626;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#23637;&#31034;&#36825;&#19968;&#28857;&#65292;&#24182;&#23637;&#31034;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;OOD&#26816;&#27979;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Miller et al. showed that a model's in-distribution (ID) accuracy has a strong linear correlation with its out-of-distribution (OOD) accuracy on several OOD benchmarks -- a phenomenon they dubbed ''accuracy-on-the-line''. While a useful tool for model selection (i.e., the model most likely to perform the best OOD is the one with highest ID accuracy), this fact does not help estimate the actual OOD performance of models without access to a labeled OOD validation set. In this paper, we show a similar but surprising phenomenon also holds for the agreement between pairs of neural network classifiers: whenever accuracy-on-the-line holds, we observe that the OOD agreement between the predictions of any two pairs of neural networks (with potentially different architectures) also observes a strong linear correlation with their ID agreement. Furthermore, we observe that the slope and bias of OOD vs ID agreement closely matches that of OOD vs ID accuracy. This phenomenon, which we call
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36845;&#20195;CVaR&#21644;&#26368;&#22351;&#36335;&#24452;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26377;&#25928;&#31639;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#36991;&#20813;&#39118;&#38505;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2206.02678</link><description>&lt;p&gt;
&#35777;&#26126;&#26377;&#25928;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#65306;&#36845;&#20195;CVaR&#21644;&#26368;&#22351;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR and Worst Path. (arXiv:2206.02678v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36845;&#20195;CVaR&#21644;&#26368;&#22351;&#36335;&#24452;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26377;&#25928;&#31639;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#36991;&#20813;&#39118;&#38505;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#36845;&#20195;CVaR&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27599;&#27493;&#22238;&#25253;&#30340;&#23614;&#37096;&#65292;&#24182;&#19987;&#27880;&#20110;&#32039;&#23494;&#25511;&#21046;&#27599;&#20010;&#38454;&#27573;&#36827;&#20837;&#28798;&#38590;&#24615;&#24773;&#20917;&#30340;&#39118;&#38505;&#12290;&#36825;&#31181;&#21046;&#23450;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#24378;&#28872;&#36991;&#20813;&#39118;&#38505;&#30340;&#29616;&#23454;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20020;&#24202;&#27835;&#30103;&#35268;&#21010;&#21644;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36845;&#20195;CVaR&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#24615;&#33021;&#25351;&#26631;&#65292;&#21363;&#36951;&#25022;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#25105;&#20204;&#20998;&#21035;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;ICVaR-RM&#21644;ICVaR-BPI&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#21095;&#38598;&#25968;$K$&#30456;&#21305;&#37197;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36845;&#20195;CVaR&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26377;&#36259;&#30340;&#26497;&#38480;&#24773;&#20917;&#65292;&#31216;&#20026;&#26368;&#22351;&#36335;&#24452;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#21487;&#33021;&#30340;&#26368;&#23567;&#32047;&#31215;&#22238;&#25253;&#12290;&#23545;&#20110;&#26368;&#22351;&#36335;&#24452;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20005;&#26684;&#24615;&#33021;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study a novel episodic risk-sensitive Reinforcement Learning (RL) problem, named Iterated CVaR RL, which aims to maximize the tail of the reward-to-go at each step, and focuses on tightly controlling the risk of getting into catastrophic situations at each stage. This formulation is applicable to real-world tasks that demand strong risk avoidance throughout the decision process, such as autonomous driving, clinical treatment planning and robotics. We investigate two performance metrics under Iterated CVaR RL, i.e., Regret Minimization and Best Policy Identification. For both metrics, we design efficient algorithms ICVaR-RM and ICVaR-BPI, respectively, and provide nearly matching upper and lower bounds with respect to the number of episodes $K$. We also investigate an interesting limiting case of Iterated CVaR RL, called Worst Path RL, where the objective becomes to maximize the minimum possible cumulative reward. For Worst Path RL, we propose an efficient algorithm wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SF&#30340;LoRa RSSI&#25351;&#32441;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20197;&#24212;&#23545;LoRa&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#23450;&#20301;&#31934;&#24230;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;6.67&#65285;&#21040;48.10&#65285;&#19981;&#31561;&#12290;</title><link>http://arxiv.org/abs/2205.11428</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#39057;&#22240;&#23376;&#30340;LoRa&#23450;&#20301;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spreading Factor assisted LoRa Localization with Deep Reinforcement Learning. (arXiv:2205.11428v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SF&#30340;LoRa RSSI&#25351;&#32441;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20197;&#24212;&#23545;LoRa&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#23450;&#20301;&#31934;&#24230;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;6.67&#65285;&#21040;48.10&#65285;&#19981;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#23454;&#29616;&#23450;&#20301;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;RSSI&#25351;&#32441;&#12290;&#28982;&#32780;&#65292;&#22312;LoRa&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#32593;&#32476;&#35774;&#32622;&#20013;&#30340;&#25193;&#39057;&#22240;&#23376;&#65288;SF&#65289;&#65292;&#20256;&#32479;&#25351;&#32441;&#21487;&#33021;&#32570;&#20047;&#26080;&#32447;&#30005;&#22320;&#22270;&#30340;&#20195;&#34920;&#24615;&#65292;&#23548;&#33268;&#20301;&#32622;&#20272;&#35745;&#19981;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;SF&#30340;&#26032;&#22411;LoRa RSSI&#25351;&#32441;&#26041;&#27861;&#12290;&#24615;&#33021;&#35780;&#20272;&#26174;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#65292;&#22240;&#20026;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#22810;6.67&#65285;&#30340;&#23450;&#20301;&#31934;&#24230;&#25552;&#39640;&#12290;&#35780;&#20272;&#26159;&#20351;&#29992;&#22522;&#32447;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;LoRa&#32593;&#32476;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#24212;&#23545;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;&#25152;&#33719;&#24471;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#22522;&#32447;DNN&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#23450;&#20301;&#31934;&#24230;&#26041;&#38754;&#25552;&#39640;&#20102;48.10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the developed localization solutions rely on RSSI fingerprinting. However, in the LoRa networks, due to the spreading factor (SF) in the network setting, traditional fingerprinting may lack representativeness of the radio map, leading to inaccurate position estimates. As such, in this work, we propose a novel LoRa RSSI fingerprinting approach that takes into account the SF. The performance evaluation shows the prominence of our proposed approach since we achieved an improvement in localization accuracy by up to 6.67% compared to the state-of-the-art methods. The evaluation has been done using a fully connected deep neural network (DNN) set as the baseline. To further improve the localization accuracy, we propose a deep reinforcement learning model that captures the ever-growing complexity of LoRa networks and copes with their scalability. The obtained results show an improvement of 48.10% in the localization accuracy compared to the baseline DNN model.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;LGBTQ&#22312;&#32447;&#31038;&#32676;&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#32463;&#21382;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26816;&#27979;Twitter&#19978;&#34920;&#29616;&#20986;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#65292;&#27604;&#36739;&#30123;&#24773;&#21069;&#21518;&#30340;&#35821;&#35328;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2205.09511</link><description>&lt;p&gt;
LGBTQ&#22312;&#32447;&#31038;&#32676;&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#32463;&#21382;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;
&lt;/p&gt;
&lt;p&gt;
Minority Stress Experienced by LGBTQ Online Communities during the COVID-19 Pandemic. (arXiv:2205.09511v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09511
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;LGBTQ&#22312;&#32447;&#31038;&#32676;&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#32463;&#21382;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26816;&#27979;Twitter&#19978;&#34920;&#29616;&#20986;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#65292;&#27604;&#36739;&#30123;&#24773;&#21069;&#21518;&#30340;&#35821;&#35328;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#30340;&#24433;&#21709;&#22312;&#23569;&#25968;&#26063;&#32676;&#20013;&#20135;&#29983;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#65292;&#22914;LGBTQ&#31038;&#32676;&#65288;&#22899;&#21516;&#24615;&#24651;&#12289;&#30007;&#21516;&#24615;&#24651;&#12289;&#21452;&#24615;&#24651;&#12289;&#36328;&#24615;&#21035;&#21644;&#37239;&#20799;&#65289;&#30340;&#25104;&#21592;&#65292;&#22240;&#20026;&#20182;&#20204;&#26412;&#36523;&#23384;&#22312;&#31038;&#20250;&#19981;&#21033;&#21644;&#20581;&#24247;&#24046;&#24322;&#12290;&#34429;&#28982;&#23545;&#20110;COVID-19&#30123;&#24773;&#23545;&#22823;&#20247;&#29983;&#27963;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;LGBTQ&#26063;&#32676;&#12290;&#26412;&#25991;&#21033;&#29992;&#30123;&#24773;&#21069;&#21644;&#30123;&#24773;&#26399;&#38388;&#30340;&#25968;&#25454;&#38598;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#20004;&#32452;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20197;&#35782;&#21035;&#22312;Twitter&#19978;&#34920;&#29616;&#20986;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#30340;&#24086;&#23376;&#12290;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#26159;LGBTQ&#26063;&#32676;&#25104;&#21592;&#30001;&#20110;&#20854;&#24615;&#21035;&#21644;&#24615;&#21035;&#35748;&#21516;&#32780;&#38754;&#20020;&#30340;&#29420;&#29305;&#21387;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26368;&#20339;&#30340;&#30123;&#24773;&#21069;&#21644;&#30123;&#24773;&#26399;&#38388;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#26816;&#27979;&#21253;&#21547;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#30340;&#24086;&#23376;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30123;&#24773;&#21069;&#21644;&#30123;&#24773;&#26399;&#38388;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#24086;&#23376;&#30340;&#35821;&#35328;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24868;&#24594;&#26159;&#22312;&#30123;&#24773;&#26399;&#38388;&#26368;&#26174;&#33879;&#30340;&#24773;&#32490;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has disproportionately impacted the lives of minorities, such as members of the LGBTQ community (lesbian, gay, bisexual, transgender, and queer) due to pre-existing social disadvantages and health disparities. Although extensive research has been carried out on the impact of the COVID-19 pandemic on different aspects of the general population's lives, few studies are focused on the LGBTQ population. In this paper, we develop and evaluate two sets of machine learning classifiers using a pre-pandemic and a during-pandemic dataset to identify Twitter posts exhibiting minority stress, which is a unique pressure faced by the members of the LGBTQ population due to their sexual and gender identities. We demonstrate that our best pre- and during-pandemic models show strong and stable performance for detecting posts that contain minority stress. We investigate the linguistic differences in minority stress posts across pre- and during-pandemic periods. We find that anger wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2204.07182</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29992;&#20110;&#27861;&#38498;&#25991;&#20214;&#30340;&#30456;&#20284;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysing similarities between legal court documents using natural language processing approaches based on Transformers. (arXiv:2204.07182v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25104;&#20026;&#27861;&#24459;&#39046;&#22495;&#20013;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#26412;&#25991;&#20197;&#24052;&#35199;&#21496;&#27861;&#31995;&#32479;&#30340;&#26696;&#20363;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36816;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#12290;&#21253;&#25324;BERT&#12289;GPT-2&#12289;RoBERTa&#31561;NLP&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#12290;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#25991;&#26723;&#30340;&#23884;&#20837;&#21521;&#37327;&#34920;&#24449;&#65292;&#36816;&#29992;&#32858;&#31867;&#26041;&#27861;&#23545;&#35785;&#35772;&#26696;&#20214;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#27169;&#22411;&#30340;&#21697;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Artificial Intelligence (AI) have leveraged promising results in solving complex problems in the area of Natural Language Processing (NLP), being an important tool to help in the expeditious resolution of judicial proceedings in the legal area. In this context, this work targets the problem of detecting the degree of similarity between judicial documents that can be achieved in the inference group, by applying six NLP techniques based on the transformers architecture to a case study of legal proceedings in the Brazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2 and RoBERTa, were pre-trained using a general purpose corpora of the Brazilian Portuguese language, and then were fine-tuned and specialised for the legal sector using 210,000 legal proceedings. Vector representations of each legal document were calculated based on their embeddings, which were used to cluster the lawsuits, calculating the quality of each model based on the cosine of
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#31350;&#20102;&#23545;&#27604;&#34920;&#31034;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;Time Control(TC)&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#30041;&#38271;&#25991;&#26412;&#30340;&#32467;&#26500;&#65292;&#24182;&#22312;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20197;&#33719;&#24471;&#35805;&#35821;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2203.11370</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Language modeling via stochastic processes. (arXiv:2203.11370v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#31350;&#20102;&#23545;&#27604;&#34920;&#31034;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;Time Control(TC)&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#30041;&#38271;&#25991;&#26412;&#30340;&#32467;&#26500;&#65292;&#24182;&#22312;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20197;&#33719;&#24471;&#35805;&#35821;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#30701;&#25991;&#26412;&#65292;&#20294;&#26159;&#24403;&#23427;&#20204;&#29983;&#25104;&#36739;&#38271;&#25991;&#26412;&#26102;&#65292;&#24448;&#24448;&#26174;&#24471;&#20887;&#26434;&#25110;&#32773;&#19981;&#36830;&#36143;&#12290;&#36825;&#20123;&#38382;&#39064;&#28304;&#33258;next-token-only&#30340;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24037;&#20316;&#34920;&#26126;&#65292;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21040;&#22909;&#30340;&#28508;&#22312;&#34920;&#24449;&#65292;&#36825;&#23545;&#20110;&#21306;&#20998;&#24615;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20998;&#26512;&#20102;&#23545;&#27604;&#34920;&#31034;&#24212;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#65288;&#22914;&#38271;&#25991;&#26412;&#29983;&#25104;&#65289;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Time Control (TC)&#12290;TC&#39318;&#20808;&#23398;&#20064;&#30446;&#26631;&#25991;&#26412;&#39046;&#22495;&#30340;&#23545;&#27604;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#36825;&#20123;&#34920;&#31034;&#35299;&#30721;&#29983;&#25104;&#25991;&#26412;&#12290;&#19982;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#26041;&#27861;&#21644;&#36328;&#36234;&#21508;&#31181;&#25991;&#26412;&#39046;&#22495;&#30340;fine-tuning GPT2&#30456;&#27604;&#65292;TC&#22312;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20197;&#33719;&#24471;&#35805;&#35821;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#35774;&#32622;&#20013;&#65292;TC&#20445;&#30041;&#20102;&#25991;&#26412;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. Recent work in self-supervised learning suggests that models can learn good latent representations via contrastive learning, which can be effective for discriminative tasks. Our work analyzes the application of contrastive representations for generative tasks, like long text generation. We propose one approach for leveraging constrastive representations, which we call Time Control (TC). TC first learns a contrastive representation of the target text domain, then generates text by decoding from these representations. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC performs competitively to methods specific for learning sentence representations on discourse coherence. On long text generation settings, TC preserves the text structure bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.11155</link><description>&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#22312;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#34920;&#31034;&#25972;&#20010;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#23558;&#23494;&#24230;&#30697;&#38453;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#20219;&#21153;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#12290;&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#26032;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#36755;&#20837;&#20026;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;QA&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#20197;&#22686;&#24378;&#32463;&#20856;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#26694;&#26550;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20154;&#31867;&#39550;&#39542;&#21592;&#28151;&#21512;&#20132;&#36890;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#33258;&#20027;&#20195;&#29702;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#21487;&#25193;&#23637;&#19988;&#23433;&#20840;&#30340;&#30417;&#31649;&#65292;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;AV&#21512;&#20316;&#21487;&#20197;&#23558;&#30417;&#31649;&#21487;&#38752;&#24615;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#30417;&#31649;&#21592;&#12290;</title><link>http://arxiv.org/abs/2112.07569</link><description>&lt;p&gt;
&#28151;&#21512;&#20132;&#36890;&#20013;&#21487;&#25193;&#23637;&#30417;&#31649;&#33258;&#20027;&#24615;&#30340;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Cooperation for Scalable Supervision of Autonomy in Mixed Traffic. (arXiv:2112.07569v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07569
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20154;&#31867;&#39550;&#39542;&#21592;&#28151;&#21512;&#20132;&#36890;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#33258;&#20027;&#20195;&#29702;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#21487;&#25193;&#23637;&#19988;&#23433;&#20840;&#30340;&#30417;&#31649;&#65292;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;AV&#21512;&#20316;&#21487;&#20197;&#23558;&#30417;&#31649;&#21487;&#38752;&#24615;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#30417;&#31649;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24615;&#30340;&#36827;&#27493;&#20026;&#35768;&#22810;&#39046;&#22495;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#31215;&#26497;&#25104;&#26524;&#65292;&#20294;&#23454;&#29616;&#20854;&#23433;&#20840;&#37096;&#32626;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20197;&#22914;&#19979;&#21160;&#26426;&#20026;&#20986;&#21457;&#28857;&#65306;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33021;&#21542;&#36991;&#20813;&#38656;&#35201;&#19968;&#20010;&#20154;&#22987;&#32456;&#30417;&#30563;&#19968;&#20010;&#26426;&#22120;&#65311;&#36890;&#36807;&#32771;&#34385;&#36828;&#31243;&#20154;&#31867;&#30417;&#31649;&#21592;&#30340;&#24773;&#20917;&#65292;&#26412;&#25991;&#35268;&#33539;&#21270;&#20102;&#36825;&#20010;&#21487;&#25193;&#23637;&#30417;&#31649;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#33258;&#20027;&#20195;&#29702;&#22914;&#20309;&#21512;&#20316;&#20197;&#23454;&#29616;&#23433;&#20840;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#22312;&#30001;AVs&#21644;&#20154;&#31867;&#39550;&#39542;&#21592;&#28151;&#21512;&#32452;&#25104;&#30340;&#20132;&#36890;&#20013;&#21512;&#24182;&#30340;&#23433;&#20840;&#20851;&#38190;&#19978;&#19979;&#25991;&#12290;&#20998;&#26512;&#32467;&#26524;&#30830;&#23450;&#20102;&#20154;&#31867;&#30417;&#31649;&#35201;&#27714;&#30340;&#39640;&#21487;&#38752;&#24615;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#23427;&#36827;&#19968;&#27493;&#34920;&#26126;AV&#21512;&#20316;&#21487;&#20197;&#23558;&#30417;&#31649;&#21487;&#38752;&#24615;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#20196;&#20154;&#24847;&#24819;&#19981;&#21040;&#30340;&#38656;&#35201;&#26356;&#23569;&#30340;&#30417;&#31649;&#21592;&#65288;&#27599;&#20010;AV&#65289;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;AV&#37319;&#29992;&#12290;&#36825;&#20123;&#20998;&#26512;&#32467;&#26524;&#21033;&#29992;&#20102;&#25490;&#38431;&#29702;&#35770;&#20998;&#26512;&#12289;&#39034;&#24207;&#32479;&#35745;&#23398;&#20197;&#21450;&#20445;&#23432;&#30340;&#21453;&#24212;&#25511;&#21046;&#31574;&#30053;&#8212;&#8212;&#26102;&#38388;&#21040;&#30896;&#25758;&#36991;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in autonomy offer the potential for dramatic positive outcomes in a number of domains, yet enabling their safe deployment remains an open problem. This work's motivating question is: In safety-critical settings, can we avoid the need to have one human supervise one machine at all times? The work formalizes this scalable supervision problem by considering remotely located human supervisors and investigating how autonomous agents can cooperate to achieve safety. This article focuses on the safety-critical context of autonomous vehicles (AVs) merging into traffic consisting of a mixture of AVs and human drivers. The analysis establishes high reliability upper bounds on human supervision requirements. It further shows that AV cooperation can improve supervision reliability by orders of magnitude and counterintuitively requires fewer supervisors (per AV) as more AVs are adopted. These analytical results leverage queuing-theoretic analysis, order statistics, and a conservative, reac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;&#39640;&#26031;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#24050;&#30693;&#25511;&#21046;&#26102;&#22495;&#22823;&#23567;$N$&#30340;&#19978;&#38480;&#32622;&#20449;&#24230;&#31574;&#30053;&#30340;&#26497;&#38480;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2112.06423</link><description>&lt;p&gt;
&#39640;&#26031;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;UCB&#31574;&#30053;&#30340;&#26497;&#38480;&#25551;&#36848;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stochastic differential equations for limiting description of UCB rule for Gaussian multi-armed bandits. (arXiv:2112.06423v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;&#39640;&#26031;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#24050;&#30693;&#25511;&#21046;&#26102;&#22495;&#22823;&#23567;$N$&#30340;&#19978;&#38480;&#32622;&#20449;&#24230;&#31574;&#30053;&#30340;&#26497;&#38480;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#24050;&#30693;&#25511;&#21046;&#26102;&#22495;&#22823;&#23567;$N$&#30340;&#39640;&#26031;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#30340;&#19978;&#38480;&#32622;&#20449;&#24230;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#24314;&#31435;&#20102;&#20854;&#26497;&#38480;&#25551;&#36848;&#12290;&#20551;&#35774;&#33218;&#30340;&#22870;&#21169;&#20855;&#26377;&#26410;&#30693;&#30340;&#26399;&#26395;&#20540;&#21644;&#24050;&#30693;&#30340;&#26041;&#24046;&#12290;&#36827;&#34892;&#20102;&#19968;&#32452;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#25509;&#36817;&#30340;&#24773;&#20917;&#65288;&#21363;&#22343;&#20540;&#22870;&#21169;&#30456;&#24046;$N^{-1/2}$&#25968;&#37327;&#32423;&#65292;&#22240;&#20026;&#36825;&#20250;&#20135;&#29983;&#26368;&#22823;&#30340;&#26631;&#20934;&#21270;&#36951;&#25022;&#65289;&#65292;&#20197;&#39564;&#35777;&#25152;&#24471;&#21040;&#30340;&#25551;&#36848;&#30340;&#26377;&#25928;&#24615;&#12290;&#20272;&#35745;&#20102;&#24403;&#26631;&#20934;&#21270;&#36951;&#25022;&#19981;&#26126;&#26174;&#22823;&#20110;&#26368;&#22823;&#20540;&#26102;&#30340;&#25511;&#21046;&#26102;&#22495;&#30340;&#26368;&#23567;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the upper confidence bound strategy for Gaussian multi-armed bandits with known control horizon sizes $N$ and build its limiting description with a system of stochastic differential equations and ordinary differential equations. Rewards for the arms are assumed to have unknown expected values and known variances. A set of Monte-Carlo simulations was performed for the case of close distributions of rewards, when mean rewards differ by the magnitude of order $N^{-1/2}$, as it yields the highest normalized regret, to verify the validity of the obtained description. The minimal size of the control horizon when the normalized regret is not noticeably larger than maximum possible was estimated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#21487;&#20197;&#20005;&#26684;&#35745;&#31639;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;Koopman&#31639;&#23376;&#30340;&#35889;&#20449;&#24687;&#12290;&#35813;&#31639;&#27861;&#36798;&#21040;&#20102;&#39640;&#38454;&#25910;&#25947;&#65292;&#36866;&#29992;&#20110;&#28151;&#27788;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2111.14889</link><description>&lt;p&gt;
&#21160;&#21147;&#31995;&#32479;&#30340; Koopman &#31639;&#23376;&#35889;&#29305;&#24615;&#30340;&#20005;&#35880;&#25968;&#25454;&#39537;&#21160;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Rigorous data-driven computation of spectral properties of Koopman operators for dynamical systems. (arXiv:2111.14889v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.14889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#21487;&#20197;&#20005;&#26684;&#35745;&#31639;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;Koopman&#31639;&#23376;&#30340;&#35889;&#20449;&#24687;&#12290;&#35813;&#31639;&#27861;&#36798;&#21040;&#20102;&#39640;&#38454;&#25910;&#25947;&#65292;&#36866;&#29992;&#20110;&#28151;&#27788;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman &#31639;&#23376;&#26159;&#26080;&#38480;&#32500;&#31639;&#23376;&#65292;&#20840;&#23616;&#32447;&#24615;&#21270;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#35889;&#20449;&#24687;&#23545;&#20110;&#29702;&#35299;&#21160;&#21147;&#23398;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;Koopman &#31639;&#23376;&#21487;&#33021;&#20855;&#26377;&#36830;&#32493;&#30340;&#35889;&#21644;&#26080;&#38480;&#32500;&#19981;&#21464;&#23376;&#31354;&#38388;&#65292;&#20351;&#24471;&#35745;&#31639;&#23427;&#20204;&#30340;&#35889;&#20449;&#24687;&#25104;&#20026;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#23427;&#20855;&#26377;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21487;&#20197;&#35745;&#31639;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340; Koopman &#31639;&#23376;&#30340;&#35889;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21097;&#20313;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;ResDMD&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20174;&#24555;&#29031;&#25968;&#25454;&#20013;&#35745;&#31639;&#26222;&#36890; Koopman &#31639;&#23376;&#30340;&#35889;&#21644;&#20266;&#35889;&#30340;&#26041;&#26696;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#35889;&#27745;&#26579;&#12290;&#21033;&#29992;&#20849;&#36717;&#31639;&#23376;&#21644; ResDMD&#65292;&#25105;&#20204;&#35745;&#31639;&#19982;&#26222;&#36890;&#30340;&#20445;&#27979;&#24230;&#21160;&#21147;&#31995;&#32479;&#30456;&#20851;&#30340;&#35889;&#27979;&#37327;&#30340;&#24179;&#28369;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26174;&#24335;&#25910;&#25947;&#23450;&#29702;&#65292;&#21363;&#20351;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#21487;&#20197;&#36798;&#21040;&#39640;&#38454;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Koopman operators are infinite-dimensional operators that globally linearize nonlinear dynamical systems, making their spectral information valuable for understanding dynamics. However, Koopman operators can have continuous spectra and infinite-dimensional invariant subspaces, making computing their spectral information a considerable challenge. This paper describes data-driven algorithms with rigorous convergence guarantees for computing spectral information of Koopman operators from trajectory data. We introduce residual dynamic mode decomposition (ResDMD), which provides the first scheme for computing the spectra and pseudospectra of general Koopman operators from snapshot data without spectral pollution. Using the resolvent operator and ResDMD, we compute smoothed approximations of spectral measures associated with general measure-preserving dynamical systems. We prove explicit convergence theorems for our algorithms, which can achieve high-order convergence even for chaotic system
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25293;&#21334;&#21644;&#21516;&#20276;&#39044;&#27979;&#26426;&#21046;&#65292;&#21516;&#26102;&#28608;&#21169;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#21644;&#35780;&#23457;&#65292;&#26469;&#25913;&#21892;&#21516;&#34892;&#35780;&#23457;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2109.00923</link><description>&lt;p&gt;
&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#20013;&#30340;&#25293;&#21334;&#21644;&#21516;&#20276;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Auctions and Peer Prediction for Academic Peer Review. (arXiv:2109.00923v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.00923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25293;&#21334;&#21644;&#21516;&#20276;&#39044;&#27979;&#26426;&#21046;&#65292;&#21516;&#26102;&#28608;&#21169;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#21644;&#35780;&#23457;&#65292;&#26469;&#25913;&#21892;&#21516;&#34892;&#35780;&#23457;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#34892;&#35780;&#35758;&#30340;&#21457;&#34920;&#34987;&#35748;&#20026;&#26159;&#35748;&#35777;&#21644;&#20256;&#25773;&#30740;&#31350;&#30028;&#35748;&#20026;&#26377;&#20215;&#20540;&#30340;&#24605;&#24819;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#31995;&#32479;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;1&#65289;&#30001;&#20110;&#25552;&#20132;&#25968;&#37327;&#24222;&#22823;&#65292;&#23545;&#23457;&#31295;&#20154;&#30340;&#38656;&#27714;&#21387;&#20498;&#24615;&#65292;&#65288;2&#65289;&#32570;&#20047;&#28608;&#21169;&#26426;&#21046;&#20419;&#20351;&#23457;&#31295;&#20154;&#21442;&#19982;&#24182;&#20184;&#20986;&#24517;&#35201;&#30340;&#21162;&#21147;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#23457;&#31295;&#24847;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26426;&#21046;&#35774;&#35745;&#26041;&#27861;&#25552;&#20986;&#20102;&#25913;&#36827;&#21516;&#34892;&#35780;&#23457;&#27969;&#31243;&#30340;&#26041;&#26696;&#65292;&#23558;&#25991;&#29486;&#25552;&#20132;&#21644;&#23457;&#31295;&#27969;&#31243;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#24182;&#21516;&#26102;&#28608;&#21169;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#21644;&#35780;&#23457;&#12290;&#22312;&#25552;&#20132;&#38454;&#27573;&#65292;&#20316;&#32773;&#36890;&#36807;&#25552;&#20132;&#20854;&#35770;&#25991;&#21644;&#20195;&#34920;&#20182;&#20204;&#26399;&#26395;&#24471;&#21040;&#23457;&#31295;&#24847;&#35265;&#30340;&#20986;&#20215;&#21442;&#21152;VCG&#25293;&#21334;&#65292;&#31454;&#20215;&#20105;&#21462;&#23457;&#26680;&#21517;&#39069;&#12290;&#23545;&#20110;&#23457;&#31295;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#20276;&#39044;&#27979;&#26426;&#21046;&#65288;H-DIPP&#65289;&#65292;&#24314;&#31435;&#22312;&#36817;&#26399;&#20449;&#24687;&#25366;&#25496;&#30740;&#31350;&#25104;&#26524;&#22522;&#30784;&#19978;&#65292;&#36825;&#31181;&#26426;&#21046;&#28608;&#21169;&#23457;&#31295;&#20154;&#25552;&#20986;&#20934;&#30830;&#30340;&#23457;&#26680;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer reviewed publications are considered the gold standard in certifying and disseminating ideas that a research community considers valuable. However, we identify two major drawbacks of the current system: (1) the overwhelming demand for reviewers due to a large volume of submissions, and (2) the lack of incentives for reviewers to participate and expend the necessary effort to provide high-quality reviews. In this work, we adopt a mechanism-design approach to propose improvements to the peer review process, tying together the paper submission and review processes and simultaneously incentivizing high-quality submissions and reviews. In the submission stage, authors participate in a VCG auction for review slots by submitting their papers along with a bid that represents their expected value for having their paper reviewed. For the reviewing stage, we propose a novel peer prediction mechanism (H-DIPP) building on recent work in the information elicitation literature, which incentivize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#22343;&#20540;&#21327;&#26041;&#24046;&#36172;&#21338;&#26426;&#65288;CMCB&#65289;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#36873;&#39033;&#30456;&#20851;&#24615;&#12290;&#31639;&#27861;&#20855;&#26377;&#26368;&#20248;&#36951;&#25022;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2102.12090</link><description>&lt;p&gt;
&#36830;&#32493;&#22343;&#20540;&#21327;&#26041;&#24046;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Continuous Mean-Covariance Bandits. (arXiv:2102.12090v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#22343;&#20540;&#21327;&#26041;&#24046;&#36172;&#21338;&#26426;&#65288;CMCB&#65289;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#36873;&#39033;&#30456;&#20851;&#24615;&#12290;&#31639;&#27861;&#20855;&#26377;&#26368;&#20248;&#36951;&#25022;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#39118;&#38505;&#24863;&#30693;&#22810;&#33218;&#36172;&#21338;&#27169;&#22411;&#36890;&#24120;&#20851;&#27880;&#20110;&#21333;&#20010;&#36873;&#39033;&#30340;&#39118;&#38505;&#24230;&#37327;&#65292;&#20363;&#22914;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20851;&#36873;&#39033;&#30340;&#37325;&#35201;&#30340;&#23454;&#38469;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#22343;&#20540;&#21327;&#26041;&#24046;&#36172;&#21338;&#26426;&#65288;CMCB&#65289;&#27169;&#22411;&#65292;&#20197;&#26174;&#24335;&#32771;&#34385;&#36873;&#39033;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;CMCB&#20013;&#65292;&#26377;&#19968;&#20010;&#23398;&#20064;&#32773;&#39034;&#24207;&#36873;&#25321;&#32473;&#23450;&#36873;&#39033;&#19978;&#30340;&#26435;&#37325;&#21521;&#37327;&#65292;&#24182;&#26681;&#25454;&#20915;&#31574;&#35266;&#23519;&#38543;&#26426;&#21453;&#39304;&#12290;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#22312;&#34913;&#37327;&#36873;&#39033;&#21327;&#26041;&#24046;&#30340;&#22870;&#21169;&#21644;&#39118;&#38505;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#12290;&#20026;&#20102;&#25429;&#25417;&#23454;&#36341;&#20013;&#30340;&#19981;&#21516;&#22870;&#21169;&#35266;&#23519;&#22330;&#26223;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#21453;&#39304;&#35774;&#32622;&#65292;&#21363;&#23436;&#20840;&#20449;&#24687;&#12289;&#21322;&#36172;&#24466;&#21644;&#20840;&#36172;&#24466;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#20248;&#36951;&#25022;&#65288;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#65289;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#19979;&#30028;&#26469;&#39564;&#35777;&#23427;&#20204;&#30340;&#26368;&#20248;&#24615;&#30340;&#26032;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing risk-aware multi-armed bandit models typically focus on risk measures of individual options such as variance. As a result, they cannot be directly applied to important real-world online decision making problems with correlated options. In this paper, we propose a novel Continuous Mean-Covariance Bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent's objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, we consider three feedback settings, i.e., full-information, semi-bandit and full-bandit feedback. We propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#20013;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37319;&#29992;&#38543;&#26426;&#36755;&#20837;&#22635;&#20805;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2102.09057</link><description>&lt;p&gt;
&#38024;&#23545;&#30005;&#21147;&#31995;&#32479;&#20013;&#34394;&#20551;&#25968;&#25454;&#25915;&#20987;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarial-Resilient Deep Neural Networks for False Data Injection Attack Detection in Power Grids. (arXiv:2102.09057v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.09057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#20013;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37319;&#29992;&#38543;&#26426;&#36755;&#20837;&#22635;&#20805;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65288;FDIA&#65289;&#23545;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#36896;&#25104;&#20102;&#37325;&#22823;&#23433;&#20840;&#23041;&#32961;&#12290;&#20026;&#20102;&#26816;&#27979;&#36825;&#31181;&#25915;&#20987;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#22823;&#22810;&#25968;&#26410;&#33021;&#32771;&#34385;&#23545;&#25239;&#24615;&#27979;&#37327;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#21361;&#21450;&#21508;&#31181;ML&#24212;&#29992;&#20013;DNNs&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#29992;&#20110;FDIA&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;&#20960;&#31181;&#23545;&#25239;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;FDIA&#26816;&#27979;&#20013;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#24377;&#24615;&#30340;DNN&#26816;&#27979;&#26694;&#26550;&#26469;&#26816;&#27979;FDIA&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37117;&#37319;&#29992;&#38543;&#26426;&#36755;&#20837;&#22635;&#20805;&#25216;&#26415;&#12290;&#25105;&#20204;&#22522;&#20110;IEEE&#26631;&#20934;&#30005;&#21147;&#31995;&#32479;&#36827;&#34892;&#30340;&#20223;&#30495;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26694;&#26550;&#26174;&#33879;&#38477;&#20302;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#23545;DNN&#22312;FDIA&#26816;&#27979;&#20013;&#30340;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#38750;&#24120;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
False data injection attacks (FDIAs) pose a significant security threat to power system state estimation. To detect such attacks, recent studies have proposed machine learning (ML) techniques, particularly deep neural networks (DNNs). However, most of these methods fail to account for the risk posed by adversarial measurements, which can compromise the reliability of DNNs in various ML applications. In this paper, we present a DNN-based FDIA detection approach that is resilient to adversarial attacks. We first analyze several adversarial defense mechanisms used in computer vision and show their inherent limitations in FDIA detection. We then propose an adversarial-resilient DNN detection framework for FDIA that incorporates random input padding in both the training and inference phases. Our simulations, based on an IEEE standard power system, demonstrate that this framework significantly reduces the effectiveness of adversarial attacks while having a negligible impact on the DNNs' dete
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#39033;Logit&#36873;&#25321;&#27169;&#22411;&#30340;&#23398;&#20064;&#25490;&#24207;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#22312;&#25972;&#20010;&#39033;&#30446;&#21015;&#34920;&#20013;&#30340;&#36873;&#25321;&#34892;&#20026;&#65292;&#20026;&#32593;&#31449;&#35774;&#35745;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25490;&#24207;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2009.03207</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#22810;&#39033;Logit&#36873;&#25321;&#19979;&#36827;&#34892;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank under Multinomial Logit Choice. (arXiv:2009.03207v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.03207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#39033;Logit&#36873;&#25321;&#27169;&#22411;&#30340;&#23398;&#20064;&#25490;&#24207;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#22312;&#25972;&#20010;&#39033;&#30446;&#21015;&#34920;&#20013;&#30340;&#36873;&#25321;&#34892;&#20026;&#65292;&#20026;&#32593;&#31449;&#35774;&#35745;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25490;&#24207;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#31449;&#35774;&#35745;&#20013;&#65292;&#23398;&#20064;&#26368;&#20339;&#20869;&#23481;&#25490;&#24207;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26694;&#26550;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#36873;&#25321;&#20869;&#23481;&#21015;&#34920;&#24182;&#35266;&#23519;&#29992;&#25143;&#20915;&#23450;&#28857;&#20987;&#30340;&#39034;&#24207;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;LTR&#24037;&#20316;&#20551;&#35774;&#29992;&#25143;&#22312;&#21015;&#34920;&#20013;&#29420;&#31435;&#32771;&#34385;&#27599;&#20010;&#39033;&#30446;&#65292;&#24182;&#23545;&#27599;&#20010;&#39033;&#30446;&#36827;&#34892;&#20108;&#36873;&#19968;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#39033;&#24335;Logit&#65288;MNL&#65289;&#36873;&#25321;&#27169;&#22411;&#21040;LTR&#26694;&#26550;&#20013;&#65292;&#23427;&#25429;&#25417;&#21040;&#29992;&#25143;&#23558;&#26377;&#24207;&#30340;&#39033;&#30446;&#21015;&#34920;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#65292;&#20174;&#25152;&#26377;&#39033;&#30446;&#21644;&#27809;&#26377;&#28857;&#20987;&#36873;&#39033;&#20013;&#20570;&#20986;&#19968;&#20010;&#36873;&#25321;&#30340;&#34892;&#20026;&#12290;&#22312;MNL&#27169;&#22411;&#19979;&#65292;&#29992;&#25143;&#26356;&#21916;&#27426;&#26412;&#36136;&#19978;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#39033;&#30446;&#65292;&#25110;&#32773;&#22788;&#20110;&#21015;&#34920;&#20013;&#26356;&#21487;&#21462;&#30340;&#20301;&#32622;&#30340;&#39033;&#30446;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#32622;&#20449;&#30028;&#65288;UCB&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#20301;&#32622;&#20381;&#36182;&#21442;&#25968;&#30340;&#20004;&#31181;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#23548;&#33268;&#20102;&#23545;&#38382;&#39064;&#30340;$\Omega&#65288;\sqrt{JT}&#65289;$&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the optimal ordering of content is an important challenge in website design. The learning to rank (LTR) framework models this problem as a sequential problem of selecting lists of content and observing where users decide to click. Most previous work on LTR assumes that the user considers each item in the list in isolation, and makes binary choices to click or not on each. We introduce a multinomial logit (MNL) choice model to the LTR framework, which captures the behaviour of users who consider the ordered list of items as a whole and make a single choice among all the items and a no-click option. Under the MNL model, the user favours items which are either inherently more attractive, or placed in a preferable position within the list. We propose upper confidence bound (UCB) algorithms to minimise regret in two settings where the position dependent parameters are known, and unknown. We present theoretical analysis leading to an $\Omega(\sqrt{JT})$ lower bound for the problem
&lt;/p&gt;</description></item></channel></rss>