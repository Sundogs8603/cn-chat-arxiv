<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.02058</link><description>&lt;p&gt;
LOTUS&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02058
&lt;/p&gt;
&lt;p&gt;
LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOTUS&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#20351;&#24471;&#29289;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#32780;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;LOTUS&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#26032;&#20219;&#21153;&#30340;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#26500;&#24314;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#12290;LOTUS&#39318;&#20808;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#20174;&#26410;&#20998;&#27573;&#30340;&#28436;&#31034;&#20013;&#25552;&#21462;&#37325;&#22797;&#20986;&#29616;&#30340;&#25216;&#33021;&#27169;&#24335;&#12290;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#26356;&#26032;&#29616;&#26377;&#25216;&#33021;&#20197;&#36991;&#20813;&#23545;&#20197;&#21069;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#28155;&#21152;&#26032;&#25216;&#33021;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;LOTUS&#35757;&#32451;&#19968;&#20010;&#20803;&#25511;&#21046;&#22120;&#65292;&#22312;&#32456;&#36523;&#23398;&#20064;&#36807;&#31243;&#20013;&#28789;&#27963;&#22320;&#32452;&#21512;&#21508;&#31181;&#25216;&#33021;&#26469;&#35299;&#20915;&#22522;&#20110;&#35270;&#35273;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;LOTUS&#22312;&#25104;&#21151;&#29575;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#26041;&#27861;11&#65285;&#20197;&#19978;&#65292;&#26174;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#26500;&#36896;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#36890;&#36807;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;&#38382;&#39064;&#21644;&#25968;&#20540;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#26368;&#32456;&#32467;&#26524;&#26377;&#21161;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2311.01404</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#23558;&#24402;&#19968;&#21270;&#27969;&#20316;&#20026;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs. (arXiv:2311.01404v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#26500;&#36896;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#36890;&#36807;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;&#38382;&#39064;&#21644;&#25968;&#20540;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#26368;&#32456;&#32467;&#26524;&#26377;&#21161;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#24402;&#19968;&#21270;&#27969;"&#19968;&#35789;&#19982;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#30456;&#20851;&#12290;&#26412;&#25991;&#32771;&#34385;&#23558;$W_2$-&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;$T$&#24674;&#22797;&#20026;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#21512;&#36866;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#32477;&#23545;&#36830;&#32493;&#27979;&#24230;$\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$&#21644;&#21463;&#25511;&#21521;&#37327;&#22330;&#65292;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#21253;&#21547;&#22312;&#31995;&#32479;&#20135;&#29983;&#30340;&#27969;&#21160;&#30340;$C^0_c$&#38381;&#21253;&#20013;&#12290;&#20551;&#35774;&#21407;&#22987;&#27979;&#24230;$\mu,\nu$&#30340;&#31163;&#25955;&#36817;&#20284;$\mu_N,\nu_N$&#21487;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;$\gamma_N$&#26469;&#23450;&#20041;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;$\Gamma$-&#25910;&#25947;&#35770;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20854;&#35299;&#23545;&#24212;&#20110;&#36817;&#20284;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;$T$&#30340;&#27969;&#21160;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;Pontryagin&#26368;&#22823;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#20540;&#26041;&#26696;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "Normalizing Flows" is related to the task of constructing invertible transport maps between probability measures by means of deep neural networks. In this paper, we consider the problem of recovering the $W_2$-optimal transport map $T$ between absolutely continuous measures $\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural ODE. We first show that, under suitable assumptions on $\mu,\nu$ and on the controlled vector fields, the optimal transport map is contained in the $C^0_c$-closure of the flows generated by the system. Assuming that discrete approximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available, we use a discrete optimal coupling $\gamma_N$ to define an optimal control problem. With a $\Gamma$-convergence argument, we prove that its solutions correspond to flows that approximate the optimal transport map $T$. Finally, taking advantage of the Pontryagin Maximum Principle, we propose an iterative numerical scheme for the reso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#34892;&#20026;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#20998;&#26512;&#20102;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.00690</link><description>&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#22312;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#34892;&#20026;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#20998;&#26512;&#20102;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#30340;&#29702;&#35299;&#21487;&#20197;&#20174;&#22810;&#20010;&#26041;&#38754;&#21463;&#30410;&#20110;&#21487;&#35270;&#21270;&#30740;&#31350;&#20154;&#21592;&#65292;&#21253;&#25324;&#25913;&#36827;&#21487;&#35270;&#21270;&#35774;&#35745;&#21644;&#24320;&#21457;&#20808;&#36827;&#30340;&#20132;&#20114;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24863;&#30693;&#30340;&#22797;&#26434;&#24615;&#21644;&#25105;&#20204;&#23545;&#30456;&#20851;&#29992;&#25143;&#34892;&#20026;&#30340;&#32570;&#20047;&#20102;&#35299;&#65292;&#29992;&#25143;&#34892;&#20026;&#30340;&#26085;&#24535;&#25991;&#20214;&#20173;&#28982;&#38590;&#20197;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#20840;&#38754;&#25968;&#25454;&#37319;&#38598;&#30340;&#30740;&#31350;&#65292;&#24182;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#32463;&#20856;&#30340;&#21487;&#35270;&#21270;&#24212;&#29992;&#65292;Covid-19&#25968;&#25454;&#20998;&#26512;&#65292;&#28085;&#30422;&#22320;&#29702;&#31354;&#38388;&#12289;&#26102;&#38388;&#24207;&#21015;&#21644;&#22810;&#23646;&#24615;&#30340;&#24120;&#35265;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#25910;&#38598;&#20102;&#20851;&#20110;&#22810;&#20010;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#29992;&#25143;&#34892;&#20026;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#21487;&#27604;&#36739;&#30340;&#31995;&#32479;&#65292;&#26700;&#38754;&#21644;&#27785;&#28024;&#24335;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#20004;&#20010;&#23610;&#24230;&#19978;&#20351;&#29992;&#19977;&#31181;&#26102;&#38388;&#24207;&#21015;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#34892;&#20026;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of visual analytics process can benefit visualization researchers from multiple aspects, including improving visual designs and developing advanced interaction functions. However, the log files of user behaviors are still hard to analyze due to the complexity of sensemaking and our lack of knowledge on the related user behaviors. This work presents a study on a comprehensive data collection of user behaviors, and our analysis approach with time-series classification methods. We have chosen a classical visualization application, Covid-19 data analysis, with common analysis tasks covering geo-spatial, time-series and multi-attributes. Our user study collects user behaviors on a diverse set of visualization tasks with two comparable systems, desktop and immersive visualizations. We summarize the classification results with three time-series machine learning algorithms at two scales, and explore the influences of behavior features. Our results reveal that user behaviors c
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;AutoDiff&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#21644;&#29305;&#24449;&#38388;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15479</link><description>&lt;p&gt;
AutoDiff:&#32467;&#21512;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing. (arXiv:2310.15479v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15479
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;AutoDiff&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#21644;&#29305;&#24449;&#38388;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#35768;&#22810;&#23376;&#39046;&#22495;&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#35328;&#27169;&#22411;&#25110;&#35821;&#38899;&#21512;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#21147;&#37327;&#26469;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#19968;&#30452;&#26159;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#21512;&#25104;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#34920;&#26684;&#22312;&#32479;&#35745;&#19978;&#19982;&#30495;&#23454;&#25968;&#25454;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;15&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#28789;&#27963;&#22320;&#25429;&#25417;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22914;&#33509;&#25509;&#32435;&#20102;&#35770;&#25991;&#65292;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#26681;&#25454;&#35201;&#27714;&#25552;&#20379;&#65292;&#24182;&#19988;&#23558;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model has become a main paradigm for synthetic data generation in many subfields of modern machine learning, including computer vision, language model, or speech synthesis. In this paper, we leverage the power of diffusion model for generating synthetic tabular data. The heterogeneous features in tabular data have been main obstacles in tabular data synthesis, and we tackle this problem by employing the auto-encoder architecture. When compared with the state-of-the-art tabular synthesizers, the resulting synthetic tables from our model show nice statistical fidelities to the real data, and perform well in downstream tasks for machine learning utilities. We conducted the experiments over 15 publicly available datasets. Notably, our model adeptly captures the correlations among features, which has been a long-standing challenge in tabular data synthesis. Our code is available upon request and will be publicly released if paper is accepted.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#39640;&#32500;&#24230;&#20302;&#26679;&#26412;(HDLSS)&#20998;&#31867;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30456;&#20284;&#24230;&#30340;&#23398;&#20064;&#39044;&#35745;&#31639;SVM&#26680;&#26041;&#27861;(RFSVM)&#65292;&#36890;&#36807;&#22312;40&#20010;&#20844;&#20849;HDLSS&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;HDLSS&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#19988;&#20445;&#25345;&#20102;&#38750;&#24120;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14710</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#20302;&#26679;&#26412;&#20998;&#31867;&#30340;&#38543;&#26426;&#26862;&#26519;&#24046;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Random Forest Dissimilarity for High-Dimension Low Sample Size Classification. (arXiv:2310.14710v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14710
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32500;&#24230;&#20302;&#26679;&#26412;(HDLSS)&#20998;&#31867;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30456;&#20284;&#24230;&#30340;&#23398;&#20064;&#39044;&#35745;&#31639;SVM&#26680;&#26041;&#27861;(RFSVM)&#65292;&#36890;&#36807;&#22312;40&#20010;&#20844;&#20849;HDLSS&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;HDLSS&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#19988;&#20445;&#25345;&#20102;&#38750;&#24120;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#24230;&#20302;&#26679;&#26412;(HDLSS)&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#20174;&#21307;&#23398;&#24433;&#20687;&#21040;&#25991;&#26412;&#22788;&#29702;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#26080;&#27861;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26368;&#20339;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#24322;&#24615;&#30340;&#22810;&#35270;&#35282;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;&#38543;&#26426;&#26862;&#26519;&#24046;&#24322;&#24615;(RFD)&#65292;&#35813;&#26041;&#27861;&#22312;&#36825;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#21407;&#21017;&#36716;&#21270;&#20026;&#35299;&#20915;HDLSS&#20998;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#30456;&#20284;&#24230;&#20316;&#20026;&#23398;&#20064;&#30340;&#39044;&#35745;&#31639;SVM&#26680;(RFSVM)&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#23398;&#20064;&#30456;&#20284;&#24230;&#24230;&#37327;&#22312;&#36825;&#31181;&#20998;&#31867;&#19978;&#29305;&#21035;&#36866;&#29992;&#21644;&#20934;&#30830;&#12290;&#36890;&#36807;&#23545;40&#20010;&#20844;&#20849;HDLSS&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#37197;&#21512;&#20005;&#26684;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;RFSVM&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;HDLSS&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#21516;&#26102;&#38750;&#24120;&#36830;&#36143;&#12290;
&lt;/p&gt;
&lt;p&gt;
High dimension, low sample size (HDLSS) problems are numerous among real-world applications of machine learning. From medical images to text processing, traditional machine learning algorithms are usually unsuccessful in learning the best possible concept from such data. In a previous work, we proposed a dissimilarity-based approach for multi-view classification, the Random Forest Dissimilarity (RFD), that perfoms state-of-the-art results for such problems. In this work, we transpose the core principle of this approach to solving HDLSS classification problems, by using the RF similarity measure as a learned precomputed SVM kernel (RFSVM). We show that such a learned similarity measure is particularly suited and accurate for this classification context. Experiments conducted on 40 public HDLSS classification datasets, supported by rigorous statistical analyses, show that the RFSVM method outperforms existing methods for the majority of HDLSS problems and remains at the same time very co
&lt;/p&gt;</description></item><item><title>CrossCodeEval&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#65292;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.11248</link><description>&lt;p&gt;
CrossCodeEval: &#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion. (arXiv:2310.11248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11248
&lt;/p&gt;
&lt;p&gt;
CrossCodeEval&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#65292;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#24403;&#21069;&#27969;&#34892;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22914;HumanEval&#21644;MBPP&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#25991;&#20214;&#20869;&#30340;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#19978;&#12290;&#36825;&#31181;&#36807;&#20110;&#31616;&#21270;&#30340;&#35774;&#32622;&#26080;&#27861;&#20934;&#30830;&#22320;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#65292;&#20854;&#20013;&#23384;&#20648;&#24211;&#36328;&#36234;&#22810;&#20010;&#25991;&#20214;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#36328;&#25991;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#38656;&#35201;&#35775;&#38382;&#21644;&#29702;&#35299;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#25165;&#33021;&#27491;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrossCodeEval&#65292;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#20195;&#30721;&#34917;&#20840;&#22522;&#20934;&#27979;&#35797;&#65292;&#38656;&#35201;&#28145;&#20837;&#30340;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;CrossCodeEval&#22522;&#20110;&#22235;&#31181;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;Python&#65292;Java&#65292;TypeScript&#21644;C#&#65289;&#20013;&#30340;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#12289;&#24320;&#28304;&#12289;&#26435;&#38480;&#35768;&#21487;&#30340;&#23384;&#20648;&#24211;&#38598;&#21512;&#26500;&#24314;&#12290;&#20026;&#20102;&#21019;&#24314;&#20005;&#26684;&#35201;&#27714;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#36827;&#34892;&#20934;&#30830;&#23436;&#25104;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#38745;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09299</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#30340;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Digital Twin Assisted Deep Reinforcement Learning for Online Optimization of Network Slicing Admission Control. (arXiv:2310.09299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21450;&#20197;&#19978;&#32593;&#32476;&#20013;&#22810;&#26679;&#21270;&#30340;&#32593;&#32476;&#26381;&#21153;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#32593;&#32476;&#20999;&#29255;&#25216;&#26415;&#30340;&#20986;&#29616;&#12290;&#22312;&#20854;&#20013;&#65292;&#20837;&#22330;&#25511;&#21046;&#36890;&#36807;&#36873;&#25321;&#24615;&#25509;&#21463;&#26381;&#21153;&#35831;&#27714;&#26469;&#23454;&#29616;&#29305;&#23450;&#30340;&#20248;&#21270;&#30446;&#26631;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#35768;&#22810;&#20837;&#22330;&#25511;&#21046;&#26041;&#27861;&#20013;&#36215;&#30528;&#22522;&#30784;&#21644;&#28789;&#27963;&#24615;&#30340;&#20316;&#29992;&#65292;&#20294;DRL&#27169;&#22411;&#30340;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;(DT)&#36741;&#21161;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20837;&#22330;&#20915;&#31574;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#38543;&#21518;&#31616;&#21270;&#20026;&#31561;&#20215;&#30340;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#20415;&#23454;&#26045;DRL&#26041;&#27861;&#12290;DT&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#30340;&#65292;&#24182;&#29992;&#20110;&#36741;&#21161;DRL&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;DT&#20316;&#20026;&#19968;&#31181;&#36741;&#21161;&#25163;&#27573;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DRL&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of diverse network services in 5G and beyond networks has led to the emergence of network slicing technologies. Among these, admission control plays a crucial role in achieving specific optimization goals through the selective acceptance of service requests. Although Deep Reinforcement Learning (DRL) forms the foundation in many admission control approaches for its effectiveness and flexibility, the initial instability of DRL models hinders their practical deployment in real-world networks. In this work, we propose a digital twin (DT) assisted DRL solution to address this issue. Specifically, we first formulate the admission decision-making process as a semi-Markov decision process, which is subsequently simplified into an equivalent discrete-time Markov decision process to facilitate the implementation of DRL methods. The DT is established through supervised learning and employed to assist the training phase of the DRL model. Extensive simulations show that the DT-as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07958</link><description>&lt;p&gt;
&#36808;&#21521;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#37325;&#35201;&#25361;&#25112;&#26159;&#27169;&#22411;&#22312;&#25200;&#21160;&#19979;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24212;&#29992;&#21040;&#26410;&#35265;&#36807;&#30340;&#39033;&#30446;&#19978;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#38750;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#21464;&#37327;&#21517;&#65292;&#19982;&#26631;&#31614;&#20855;&#26377;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#24403;&#25200;&#21160;&#21644;OOD&#25968;&#25454;&#38598;&#19981;&#20877;&#20855;&#26377;&#30456;&#21516;&#30340;&#34394;&#20551;&#29305;&#24449;&#26102;&#65292;&#27169;&#22411;&#39044;&#27979;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#24615;&#24341;&#20837;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CausalVul&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#25200;&#21160;&#26469;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19978;&#24212;&#29992;&#20102;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;do-&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to sys
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20854;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#19982;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#33021;&#37327;&#20989;&#25968;&#31561;&#25928;&#12290;&#36825;&#31181;&#31561;&#25928;&#24615;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.17290</link><description>&lt;p&gt;
&#25628;&#32034;&#20998;&#25955;&#30340;&#35760;&#24518;&#65306;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#26159;&#20851;&#32852;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In search of dispersed memories: Generative diffusion models are associative memory networks. (arXiv:2309.17290v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20854;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#19982;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#33021;&#37327;&#20989;&#25968;&#31561;&#25928;&#12290;&#36825;&#31181;&#31561;&#25928;&#24615;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hopfield&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20316;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#31616;&#21270;&#29702;&#35770;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#29289;&#20851;&#32852;&#35760;&#24518;&#12290;&#21407;&#22987;&#30340;Hopfield&#32593;&#32476;&#36890;&#36807;&#32534;&#30721;&#20108;&#20803;&#20851;&#32852;&#27169;&#24335;&#26469;&#23384;&#20648;&#35760;&#24518;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31181;&#31216;&#20026;Hebbian&#23398;&#20064;&#35268;&#21017;&#30340;&#31361;&#35302;&#23398;&#20064;&#26426;&#21046;&#12290;&#29616;&#20195;&#30340;Hopfield&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#23454;&#29616;&#25351;&#25968;&#32423;&#23481;&#37327;&#25193;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#27169;&#22411;&#30340;&#33021;&#37327;&#20989;&#25968;&#19981;&#33021;&#30452;&#25509;&#21387;&#32553;&#20026;&#20108;&#20803;&#31361;&#35302;&#32806;&#21512;&#65292;&#24182;&#19988;&#20063;&#19981;&#33021;&#30452;&#25509;&#25552;&#20379;&#26032;&#30340;&#31361;&#35302;&#23398;&#20064;&#35268;&#21017;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#29616;&#20195;&#30340;Hopfield&#32593;&#32476;&#30456;&#31561;&#12290;&#36825;&#31181;&#31561;&#20215;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hopfield networks are widely used in neuroscience as simplified theoretical models of biological associative memory. The original Hopfield networks store memories by encoding patterns of binary associations, which result in a synaptic learning mechanism known as Hebbian learning rule. Modern Hopfield networks can achieve exponential capacity scaling by using highly non-linear energy functions. However, the energy function of these newer models cannot be straightforwardly compressed into binary synaptic couplings and it does not directly provide new synaptic learning rules. In this work we show that generative diffusion models can be interpreted as energy-based models and that, when trained on discrete patterns, their energy function is equivalent to that of modern Hopfield networks. This equivalence allows us to interpret the supervised training of diffusion models as a synaptic learning process that encodes the associative dynamics of a modern Hopfield network in the weight structure 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.16397</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#20027;&#21160;&#20132;&#20114;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#26694;&#26550;&#65292;&#22240;&#27492;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#26368;&#36817;Transformers&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#23558;&#31163;&#32447;RL&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#36825;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#36807;&#20110;&#20048;&#35266;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#30456;&#21516;&#30340;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#30456;&#21516;&#30340;&#21160;&#20316;&#19968;&#33268;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#36716;&#25442;&#27169;&#22411;&#25110;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UNREST&#36890;&#36807;&#36716;&#25442;&#19982;&#22238;&#25253;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#12290;&#36890;&#36807;&#21457;&#29616;&#39550;&#39542;&#29615;&#22659;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#32047;&#31215;&#8221;&#21644;&#8220;&#26102;&#38388;&#23616;&#37096;&#24615;&#8221;&#29305;&#24615;&#65292;UNREST&#23558;&#20915;&#31574;Transformer&#20013;&#30340;&#20840;&#23616;&#22238;&#25253;&#26367;&#25442;&#20026;&#36739;&#23569;&#30340;&#37096;&#20998;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#24212;&#30340;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.08835</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#65292;&#26234;&#33021;&#26426;&#22120;&#22312;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
Intelligent machines work in unstructured environments by differential neural computing. (arXiv:2309.08835v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#24212;&#30340;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24076;&#26395;&#26234;&#33021;&#26426;&#22120;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#25928;&#22320;&#24037;&#20316;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#22320;&#29702;&#35299;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26080;&#32467;&#26500;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#23601;&#20687;&#20154;&#31867;&#19968;&#26679;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#38459;&#24615;&#31070;&#32463;&#35745;&#31639;&#30340;&#24863;&#30693;&#20449;&#21495;&#24046;&#20998;&#22788;&#29702;&#21644;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#20851;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#33719;&#24471;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#22914;&#26426;&#26800;&#21050;&#28608;&#30340;&#25918;&#22823;&#65288;&gt;720%&#65289;&#21644;&#36866;&#24212;&#65288;&lt;50%&#65289;&#12290;&#35813;&#26041;&#27861;&#36824;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#26234;&#33021;&#26426;&#22120;&#30340;&#20004;&#20010;&#20856;&#22411;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65306;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#22312;&#29289;&#20307;&#25235;&#21462;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;1&#27627;&#31186;&#20869;&#20351;&#29992;&#21333;&#20010;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#23398;&#20064;&#26410;&#30693;&#29289;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#23574;&#38160;&#30340;&#35282;&#21644;&#20809;&#28369;&#30340;&#34920;&#38754;&#65289;&#65292;&#19968;&#20010;&#26426;&#22120;&#25163;&#23454;&#29616;&#20102;&#23433;&#20840;&#31283;&#23450;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expecting intelligent machines to efficiently work in real world requires a new method to understand unstructured information in unknown environments with good accuracy, scalability and generalization, like human. Here, a memristive neural computing based perceptual signal differential processing and learning method for intelligent machines is presented, via extracting main features of environmental information and applying associated encoded stimuli to memristors, we successfully obtain human-like ability in processing unstructured environmental information, such as amplification (&gt;720%) and adaptation (&lt;50%) of mechanical stimuli. The method also exhibits good scalability and generalization, validated in two typical applications of intelligent machines: object grasping and autonomous driving. In the former, a robot hand experimentally realizes safe and stable grasping, through learning unknown object features (e.g., sharp corner and smooth surface) with a single memristor in 1 ms. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#22312;&#29305;&#23450;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#36816;&#29992;&#31639;&#23376;&#26041;&#31243;&#36827;&#34892;&#31163;&#32447;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#21644;&#32454;&#21270;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07383</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#36817;&#20284;&#30340;&#26576;&#20123;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#22312;&#29305;&#23450;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#36816;&#29992;&#31639;&#23376;&#26041;&#31243;&#36827;&#34892;&#31163;&#32447;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#21644;&#32454;&#21270;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#32452;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;$H(\Omega)$&#20013;&#20986;&#29616;&#30340;&#19968;&#20123;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#31867;&#30340;&#26412;&#22320;&#31354;&#38388;&#20013;&#26500;&#24314;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31163;&#32447;&#36817;&#20284;&#30340;&#31639;&#23376;&#26041;&#31243;&#30340;&#24378;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#20010;&#31639;&#23376;&#26041;&#31243;&#20986;&#29616;&#22312;&#31574;&#30053;&#36845;&#20195;&#20013;&#12290;&#21033;&#29992;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;$H_N$&#22312;&#26412;&#22320;&#31354;&#38388;$H(\Omega)$&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;$\Pwr_{H,N}$&#65292;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#26174;&#24335;&#19978;&#30028;&#12290;&#36825;&#20123;&#19978;&#30028;&#20855;&#26377;&#20960;&#20309;&#24615;&#36136;&#65292;&#24182;&#23545;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#26377;&#20102;&#19968;&#20123;&#25913;&#36827;&#21644;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies convergence rates for some value function approximations that arise in a collection of reproducing kernel Hilbert spaces (RKHS) $H(\Omega)$. By casting an optimal control problem in a specific class of native spaces, strong rates of convergence are derived for the operator equation that enables offline approximations that appear in policy iteration. Explicit upper bounds on error in value function approximations are derived in terms of power function $\Pwr_{H,N}$ for the space of finite dimensional approximants $H_N$ in the native space $H(\Omega)$. These bounds are geometric in nature and refine some well-known, now classical results concerning convergence of approximations of value functions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#25955;&#23556;&#39044;&#27979;&#65292;&#35777;&#26126;&#20102;&#22312;&#37327;&#23376;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#20004;&#20010;&#20855;&#20307;&#38382;&#39064;&#20013;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.14789</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#30340;&#25955;&#23556;
&lt;/p&gt;
&lt;p&gt;
Scattering with Neural Operators. (arXiv:2308.14789v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#25955;&#23556;&#39044;&#27979;&#65292;&#35777;&#26126;&#20102;&#22312;&#37327;&#23376;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#20004;&#20010;&#20855;&#20307;&#38382;&#39064;&#20013;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#30830;&#31435;&#20102;&#19968;&#31867;&#31216;&#20026;&#31070;&#32463;&#31639;&#23376;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33021;&#22815;&#36817;&#20284;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#21463;&#21040;&#23558;&#20854;&#24212;&#29992;&#20110;&#22522;&#30784;&#29289;&#29702;&#23398;&#30340;&#21069;&#26223;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20854;&#22312;&#37327;&#23376;&#21147;&#23398;&#20013;&#25955;&#23556;&#36807;&#31243;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#36845;&#20195;&#21464;&#20307;&#26469;&#23398;&#20064;&#34203;&#23450;&#35860;&#31639;&#23376;&#30340;&#29289;&#29702;&#23398;&#65292;&#35813;&#31639;&#23376;&#23558;&#21021;&#22987;&#27874;&#20989;&#25968;&#21644;&#21183;&#22330;&#26144;&#23556;&#21040;&#26368;&#32456;&#27874;&#20989;&#25968;&#12290;&#36825;&#20123;&#28145;&#24230;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#24605;&#24819;&#22312;&#20004;&#20010;&#20855;&#20307;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#19968;&#20010;&#31070;&#32463;&#31639;&#23376;&#39044;&#27979;&#19968;&#20010;&#22312;$1+1$&#32500;&#24230;&#20013;&#19982;&#20013;&#24515;&#21183;&#22330;&#21457;&#29983;&#25955;&#23556;&#30340;&#27874;&#21253;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#20197;&#21450;$2+1$&#32500;&#24230;&#20013;&#30340;&#21452;&#32541;&#23454;&#39564;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#26377;&#38480;&#24046;&#20998;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#31070;&#32463;&#31639;&#23376;&#21487;&#20197;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning establish the ability of certain neural-network architectures called neural operators to approximate maps between function spaces. Motivated by a prospect of employing them in fundamental physics, we examine applications to scattering processes in quantum mechanics. We use an iterated variant of Fourier neural operators to learn the physics of Schr\"odinger operators, which map from the space of initial wave functions and potentials to the final wave functions. These deep operator learning ideas are put to test in two concrete problems: a neural operator predicting the time evolution of a wave packet scattering off a central potential in $1+1$ dimensions, and the double-slit experiment in $2+1$ dimensions. At inference, neural operators can become orders of magnitude more efficient compared to traditional finite-difference solvers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20027;&#21160;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#32771;&#34385;&#20449;&#21495;&#22312;&#22266;&#23450;&#22270;&#19978;&#30340;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#23545;&#31216;&#24615;&#27010;&#24565;&#65292;&#36890;&#36807;&#22270;&#31895;&#21270;&#23454;&#29616;&#12290;&#36825;&#31687;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#24046;-&#26041;&#24046;&#20844;&#24335;&#26469;&#34913;&#37327;&#36817;&#20284;&#23545;&#31216;&#24615;...</title><link>http://arxiv.org/abs/2308.10436</link><description>&lt;p&gt;
&#36817;&#20284;&#31561;&#21464;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Approximately Equivariant Graph Networks. (arXiv:2308.10436v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20027;&#21160;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#32771;&#34385;&#20449;&#21495;&#22312;&#22266;&#23450;&#22270;&#19978;&#30340;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#23545;&#31216;&#24615;&#27010;&#24565;&#65292;&#36890;&#36807;&#22270;&#31895;&#21270;&#23454;&#29616;&#12290;&#36825;&#31687;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#24046;-&#26041;&#24046;&#20844;&#24335;&#26469;&#34913;&#37327;&#36817;&#20284;&#23545;&#31216;&#24615;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#37325;&#26032;&#25490;&#24207;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;GNNs&#30340;&#36825;&#31181;&#23545;&#31216;&#24615;&#24120;&#34987;&#19982;&#27431;&#20960;&#37324;&#24471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#23545;&#31216;&#24615;&#26412;&#36136;&#19978;&#26159;&#19981;&#21516;&#30340;&#65306;CNNs&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#23545;&#24212;&#20110;&#20316;&#29992;&#20110;&#22270;&#20687;&#20449;&#21495;&#30340;&#22266;&#23450;&#22495;&#30340;&#23545;&#31216;&#24615;&#65288;&#26377;&#26102;&#31216;&#20026;&#20027;&#21160;&#23545;&#31216;&#24615;&#65289;&#65292;&#32780;&#22312;GNNs&#20013;&#65292;&#20219;&#20309;&#32622;&#25442;&#37117;&#20316;&#29992;&#20110;&#22270;&#20449;&#21495;&#21644;&#22270;&#22495;&#65288;&#26377;&#26102;&#25551;&#36848;&#20026;&#34987;&#21160;&#23545;&#31216;&#24615;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;GNNs&#30340;&#20027;&#21160;&#23545;&#31216;&#24615;&#65292;&#32771;&#34385;&#20449;&#21495;&#22312;&#19968;&#20010;&#22266;&#23450;&#22270;&#19978;&#36827;&#34892;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;GNNs&#30340;&#33258;&#28982;&#23545;&#31216;&#24615;&#26159;&#22270;&#30340;&#33258;&#21516;&#26500;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#24448;&#24448;&#26159;&#38750;&#23545;&#31216;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#21270;&#22270;&#31895;&#21270;&#26469;&#25918;&#26494;&#23545;&#31216;&#24615;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#24046;-&#26041;&#24046;&#20844;&#24335;&#26469;&#34913;&#37327;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance symmetry of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to symmetries of the fixed domain acting on the image signal (sometimes known as active symmetries), whereas in GNNs any permutation acts on both the graph signals and the graph domain (sometimes described as passive symmetries). In this work, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We present a bias-variance formula that qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;&#23545;&#31216;&#21644;&#20998;&#37327;&#32452;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#19982;&#32032;&#25551;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#36817;&#20284;&#28857;&#38598;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#20855;&#20307;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.00273</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#31181;&#23545;&#31216;&#21644;&#20998;&#37327;&#32452;&#19981;&#21464;&#21151;&#33021;&#30340;&#36890;&#29992;&#26550;&#26500;&#36817;&#20284;Wasserstein&#36317;&#31163;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions. (arXiv:2308.00273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;&#23545;&#31216;&#21644;&#20998;&#37327;&#32452;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#19982;&#32032;&#25551;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#36817;&#20284;&#28857;&#38598;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#20855;&#20307;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22797;&#26434;&#23545;&#35937;&#20043;&#38388;&#30340;&#36317;&#31163;&#20989;&#25968;&#65292;&#27604;&#22914;&#29992;&#20110;&#27604;&#36739;&#28857;&#38598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#22797;&#26434;&#23545;&#35937;&#65288;&#22914;&#28857;&#38598;&#21644;&#22270;&#24418;&#65289;&#65292;&#20989;&#25968;&#24448;&#24448;&#38656;&#35201;&#23545;&#21508;&#31181;&#32676;&#25805;&#20316;&#65288;&#22914;&#25490;&#21015;&#25110;&#21018;&#24615;&#21464;&#25442;&#65289;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22797;&#26434;&#23545;&#35937;&#19978;&#30340;&#36830;&#32493;&#23545;&#31216;&#20056;&#31215;&#20989;&#25968;&#65288;&#20363;&#22914;&#36317;&#31163;&#20989;&#25968;&#65289;&#20063;&#24517;&#39035;&#23545;&#36825;&#20123;&#32676;&#25805;&#20316;&#30340;&#20056;&#31215;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20989;&#25968;&#31216;&#20026;&#23545;&#31216;&#21644;&#20998;&#37327;&#32452;&#19981;&#21464;&#20989;&#25968;&#65288;&#31616;&#31216;SFGI&#20989;&#25968;&#65289;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;SFGI&#20989;&#25968;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23558;&#36825;&#20010;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#19968;&#20010;&#32032;&#25551;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#20855;&#20307;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36817;&#20284;&#28857;&#38598;&#20043;&#38388;&#30340;$p$-th Wasserstein&#36317;&#31163;&#12290;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#38656;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#19982;&#28857;&#38598;&#30340;&#22823;&#23567;&#21644;&#32500;&#24230;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning distance functions between complex objects, such as the Wasserstein distance to compare point sets, is a common goal in machine learning applications. However, functions on such complex objects (e.g., point sets and graphs) are often required to be invariant to a wide variety of group actions e.g. permutation or rigid transformation. Therefore, continuous and symmetric product functions (such as distance functions) on such complex objects must also be invariant to the product of such group actions. We call these functions symmetric and factor-wise group invariant (or SFGI functions in short). In this paper, we first present a general neural network architecture for approximating SFGI functions. The main contribution of this paper combines this general neural network with a sketching idea to develop a specific and efficient neural network which can approximate the $p$-th Wasserstein distance between point sets. Very importantly, the required model complexity is independent of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#25345;&#20037;&#24615;&#25193;&#23637;&#21040;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10865</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22270;&#30340;&#25345;&#20037;&#24615;&#35299;&#20915;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#25345;&#20037;&#24615;&#25193;&#23637;&#21040;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25345;&#20037;&#24615;&#26159;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#25552;&#20986;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#26032;&#20852;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#25105;&#20204;&#21457;&#29616;&#65292;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#20960;&#23618;&#20013;&#27809;&#26377;&#30456;&#20851;&#30340;&#31354;&#38388;&#32467;&#26500;&#65292;&#20351;&#24471;&#31070;&#32463;&#25345;&#20037;&#24615;&#22823;&#33268;&#31561;&#20110;&#26435;&#37325;&#30340;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#25552;&#20986;&#30340;&#23618;&#38388;&#24179;&#22343;&#36807;&#31243;&#27809;&#26377;&#32771;&#34385;&#23618;&#38388;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#31070;&#32463;&#25345;&#20037;&#24615;&#22522;&#30784;&#32467;&#26500;&#30340;&#25193;&#23637;&#65292;&#20174;&#21333;&#23618;&#25913;&#20026;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#30456;&#24403;&#20110;&#22312;&#19968;&#20010;&#29305;&#23450;&#30697;&#38453;&#19978;&#35745;&#31639;&#31070;&#32463;&#25345;&#20037;&#24615;&#12290;&#36825;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measur
&lt;/p&gt;</description></item><item><title>DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16058</link><description>&lt;p&gt;
DUET: 2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16058
&lt;/p&gt;
&lt;p&gt;
DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;(MSSL)&#22522;&#20110;&#23398;&#20064;&#30456;&#23545;&#20110;&#19968;&#32452;&#36755;&#20837;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#21464;&#24615;&#20174;&#34920;&#31034;&#20013;&#37096;&#20998;&#25110;&#23436;&#20840;&#31227;&#38500;&#19982;&#21464;&#25442;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#38656;&#35201;&#36825;&#20123;&#20449;&#24687;&#30340;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#36896;&#25104;&#25439;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;2D&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#34920;&#31034;&#65292;&#31216;&#20026;DUET&#65292;&#23427;&#20204;&#26159;&#20197;&#30697;&#38453;&#32467;&#26500;&#32452;&#32455;&#30340;2D&#34920;&#31034;&#65292;&#24182;&#19988;&#23545;&#20316;&#29992;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#25442;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;DUET&#34920;&#31034;&#20445;&#30041;&#26377;&#20851;&#36755;&#20837;&#21464;&#25442;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#34920;&#36798;&#33021;&#21147;&#12290;&#19982;SimCLR&#65288;Chen&#31561;&#65292;2020&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#19981;&#21464;&#24615;&#65289;&#21644;ESSL&#65288;Dangovski&#31561;&#65292;2022&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#31561;&#21464;&#24615;&#65289;&#30456;&#27604;&#65292;DUET&#34920;&#31034;&#30340;&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#24615;&#20351;&#24471;&#29983;&#25104;&#20855;&#26377;&#26356;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#21487;&#25511;&#24615;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;SimCLR&#25110;ESSL&#21017;&#26080;&#27861;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;DUET&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy fo
&lt;/p&gt;</description></item><item><title>&#26641;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(TreeVAE)&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#28789;&#27963;&#30340;&#26641;&#29366;&#28508;&#21464;&#37327;&#21518;&#39564;&#20998;&#24067;&#65292;&#23618;&#27425;&#21010;&#20998;&#25968;&#25454;&#26679;&#26412;&#24182;&#25581;&#31034;&#38544;&#34255;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26641;&#30340;&#29983;&#25104;&#24335;&#26550;&#26500;&#36827;&#34892;&#36731;&#37327;&#32423;&#26465;&#20214;&#25512;&#29702;&#65292;&#21516;&#26102;&#36890;&#36807;&#19987;&#38376;&#30340;&#21494;&#23376;&#35299;&#30721;&#22120;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;TreeVAE&#21457;&#29616;&#20102;&#28508;&#22312;&#31751;&#24182;&#25214;&#21040;&#20102;&#26377;&#24847;&#20041;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#19982;&#39034;&#24207;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;TreeVAE&#25552;&#20379;&#20102;&#26356;&#20855;&#31454;&#20105;&#21147;&#30340;&#23545;&#25968;&#20284;&#28982;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.08984</link><description>&lt;p&gt;
&#26641;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tree Variational Autoencoders. (arXiv:2306.08984v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08984
&lt;/p&gt;
&lt;p&gt;
&#26641;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(TreeVAE)&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#28789;&#27963;&#30340;&#26641;&#29366;&#28508;&#21464;&#37327;&#21518;&#39564;&#20998;&#24067;&#65292;&#23618;&#27425;&#21010;&#20998;&#25968;&#25454;&#26679;&#26412;&#24182;&#25581;&#31034;&#38544;&#34255;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26641;&#30340;&#29983;&#25104;&#24335;&#26550;&#26500;&#36827;&#34892;&#36731;&#37327;&#32423;&#26465;&#20214;&#25512;&#29702;&#65292;&#21516;&#26102;&#36890;&#36807;&#19987;&#38376;&#30340;&#21494;&#23376;&#35299;&#30721;&#22120;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;TreeVAE&#21457;&#29616;&#20102;&#28508;&#22312;&#31751;&#24182;&#25214;&#21040;&#20102;&#26377;&#24847;&#20041;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#19982;&#39034;&#24207;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;TreeVAE&#25552;&#20379;&#20102;&#26356;&#20855;&#31454;&#20105;&#21147;&#30340;&#23545;&#25968;&#20284;&#28982;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#22522;&#20110;&#26641;&#30340;&#28508;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25552;&#20986;&#30340;&#26641;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(TreeVAE)&#26681;&#25454;&#25968;&#25454;&#30340;&#22266;&#26377;&#29305;&#24449;&#23545;&#26679;&#26412;&#36827;&#34892;&#23618;&#27425;&#21010;&#20998;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#32467;&#26500;&#12290;&#23427;&#26681;&#25454;&#28508;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#35843;&#25972;&#20854;&#32467;&#26500;&#20197;&#21457;&#29616;&#26368;&#20248;&#30340;&#32534;&#30721;&#26641;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#26641;&#30340;&#29983;&#25104;&#24335;&#26550;&#26500;&#20801;&#35768;&#36731;&#37327;&#32423;&#30340;&#26465;&#20214;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#19987;&#38376;&#30340;&#21494;&#23376;&#35299;&#30721;&#22120;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TreeVAE&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#28508;&#22312;&#31751;&#65292;&#24182;&#25214;&#21040;&#20102;&#19981;&#21516;&#32452;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#39034;&#24207;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;TreeVAE&#25552;&#20379;&#20102;&#26356;&#20855;&#31454;&#20105;&#21147;&#30340;&#23545;&#25968;&#20284;&#28982;&#19979;&#30028;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#20854;&#29983;&#25104;&#24615;&#36136;&#65292;TreeVAE&#33021;&#22815;&#20174;&#24050;&#23398;&#20064;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new generative hierarchical clustering model that learns a flexible tree-based posterior distribution over latent variables. The proposed Tree Variational Autoencoder (TreeVAE) hierarchically divides samples according to their intrinsic characteristics, shedding light on hidden structure in the data. It adapts its architecture to discover the optimal tree for encoding dependencies between latent variables. The proposed tree-based generative architecture permits lightweight conditional inference and improves generative performance by utilizing specialized leaf decoders. We show that TreeVAE uncovers underlying clusters in the data and finds meaningful hierarchical relations between the different groups on a variety of datasets, including real-world imaging data. We present empirically that TreeVAE provides a more competitive log-likelihood lower bound than the sequential counterparts. Finally, due to its generative nature, TreeVAE is able to generate new samples from the di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24494;&#35843;&#23545;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#36873;&#25321;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;CLIP-based &#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.06048</link><description>&lt;p&gt;
&#24494;&#35843;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#26159;&#24590;&#26679;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?. (arXiv:2306.06048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24494;&#35843;&#23545;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#36873;&#25321;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;CLIP-based &#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#22806;&#20998;&#24067;&#26816;&#27979;&#21644;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#20869;&#20998;&#24067;&#20934;&#30830;&#24615;&#24448;&#24448;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;CLIP&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#65292;&#24050;&#32463;&#22312;&#23384;&#22312;&#22806;&#20998;&#24067;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#20869;&#20998;&#24067;&#20998;&#31867;&#21644;&#22806;&#20998;&#24067;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#23545;&#20110;&#27809;&#26377;&#22806;&#20998;&#24067;&#26631;&#31614;&#30340;&#35821;&#20041;&#36716;&#31227;&#26159;&#21542;&#21487;&#38752;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#23545;&#24494;&#35843;&#23545;&#20110;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#36890;&#36807;&#23558;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#21270;&#20026;&#22810;&#27169;&#24335;&#27010;&#24565;&#21305;&#37197;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24494;&#35843;&#26041;&#27861;&#21644;&#21508;&#31181;&#22806;&#20998;&#24067;&#20998;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;&#22522;&#20110;CLIP&#30340;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large vision-language models such as CLIP have shown remarkable out-of-distribution (OOD) detection and generalization performance. However, their zero-shot in-distribution (ID) accuracy is often limited for downstream datasets. Recent CLIP-based fine-tuning methods such as prompt learning have demonstrated significant improvements in ID classification and OOD generalization where OOD labels are available. Nonetheless, it remains unclear whether the model is reliable to semantic shifts without OOD labels. In this paper, we aim to bridge the gap and present a comprehensive study to understand how fine-tuning impact OOD detection for few-shot downstream tasks. By framing OOD detection as multi-modal concept matching, we establish a connection between fine-tuning methods and various OOD scores. Our results suggest that a proper choice of OOD scores is essential for CLIP-based fine-tuning. In particular, the maximum concept matching (MCM) score provides a promising solution consiste
&lt;/p&gt;</description></item><item><title>SAM&#31639;&#27861;&#20013;&#65292;&#20165;&#25200;&#21160;&#35268;&#33539;&#21270;&#23618;&#21487;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#20013;&#37117;&#36866;&#29992;&#65292;&#31232;&#30095;&#25200;&#21160;&#26041;&#27861;&#19981;&#34892;&#12290;&#36825;&#21457;&#29616;&#23545;SAM&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04226</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#23618;&#26159;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Normalization Layers Are All That Sharpness-Aware Minimization Needs. (arXiv:2306.04226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04226
&lt;/p&gt;
&lt;p&gt;
SAM&#31639;&#27861;&#20013;&#65292;&#20165;&#25200;&#21160;&#35268;&#33539;&#21270;&#23618;&#21487;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#20013;&#37117;&#36866;&#29992;&#65292;&#31232;&#30095;&#25200;&#21160;&#26041;&#27861;&#19981;&#34892;&#12290;&#36825;&#21457;&#29616;&#23545;SAM&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#26088;&#22312;&#20943;&#23569;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#65292;&#24182;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;SAM&#30340;&#23545;&#25239;&#27493;&#39588;&#20013;&#21482;&#25200;&#21160;&#20223;&#23556;&#35268;&#33539;&#21270;&#21442;&#25968;&#65288;&#20165;&#21344;&#24635;&#21442;&#25968;&#30340;0.1%&#20197;&#19979;&#65289;&#20248;&#20110;&#25200;&#21160;&#25152;&#26377;&#21442;&#25968;&#12290;&#36825;&#19968;&#21457;&#29616;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;SAM&#21464;&#20307;&#21644;ResNet&#65288;&#25209;&#37327;&#24402;&#19968;&#21270;&#65289;&#20197;&#21450;Vision Transformer&#65288;&#23618;&#24402;&#19968;&#21270;&#65289;&#26550;&#26500;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26367;&#20195;&#30340;&#31232;&#30095;&#25200;&#21160;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#22914;&#27492;&#26497;&#31471;&#30340;&#31232;&#30095;&#27700;&#24179;&#19979;&#26080;&#27861;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#26126;&#36825;&#31181;&#34892;&#20026;&#26159;&#35268;&#33539;&#21270;&#23618;&#29305;&#26377;&#30340;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#21457;&#29616;&#37325;&#26032;&#35777;&#23454;&#20102;SAM&#22312;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#20943;&#23569;&#38160;&#24230;&#26159;&#21542;&#21807;&#19968;&#23548;&#33268;&#24615;&#33021;&#25552;&#39640;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312; https://github.com/mueller-mp/SAM-ON &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima and has been shown to enhance generalization performance in various settings. In this work we show that perturbing only the affine normalization parameters (comprising less than 0.1% of the total parameters) in the adversarial step of SAM outperforms perturbing all of the parameters. This finding generalizes to different SAM variants and both ResNet (Batch Normalization) and Vision Transformer (Layer Normalization) architectures. We consider alternative sparse perturbation approaches and find that these do not achieve similar performance enhancement at such extreme sparsity levels, showing that this behaviour is unique to the normalization layers. Although our findings reaffirm the effectiveness of SAM in improving generalization performance, they cast doubt on whether this is solely caused by reduced sharpness. The code for our experiments is publicly available at https://github.com/mueller-mp/SAM-ON.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#21487;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.05433</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#29992;&#20110;&#37327;&#23376;&#24577;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Transformer Networks for Quantum State Tomography. (arXiv:2305.05433v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#21487;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33391;&#22909;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#31070;&#32463;&#32593;&#32476;&#19968;&#30452;&#34987;&#29992;&#20110;&#37327;&#23376;&#24577;&#37325;&#26500;&#65288;QST&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#37325;&#26500;&#37327;&#23376;&#24577;&#30340;&#25928;&#29575;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#24314;&#27169;&#19982;&#37327;&#23376;&#24577;&#37325;&#26500;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20174;&#27979;&#37327;&#32479;&#35745;&#25968;&#25454;&#20013;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#36741;&#21161;&#20351;&#29992;&#32508;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#24110;&#21161;&#26368;&#23567;&#21270;&#23454;&#38469;&#24577;&#19982;&#26816;&#32034;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#36319;&#36394;&#20102;&#28041;&#21450;&#21508;&#31181;&#21442;&#25968;&#35843;&#25972;&#30340;&#24120;&#35265;&#35757;&#32451;&#31574;&#30053;&#23545;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; QST &#26041;&#27861;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#32467;&#21512;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#32447;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#26500;&#32431;&#24577;&#21644;&#28151;&#21512;&#24577;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been actively explored for quantum state tomography (QST) due to their favorable expressibility. To further enhance the efficiency of reconstructing quantum states, we explore the similarity between language modeling and quantum state tomography and propose an attention-based QST method that utilizes the Transformer network to capture the correlations between measured results from different measurements. Our method directly retrieves the density matrices of quantum states from measured statistics, with the assistance of an integrated loss function that helps minimize the difference between the actual states and the retrieved states. Then, we systematically trace different impacts within a bag of common training strategies involving various parameter adjustments on the attention-based QST method. Combining these techniques, we establish a robust baseline that can efficiently reconstruct pure and mixed quantum states. Furthermore, by comparing the performance of thre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#19981;&#30830;&#23450;&#22810;&#21464;&#37327;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25512;&#26029;&#30697;&#25551;&#36848;&#20998;&#24067;&#39044;&#35745;&#22914;&#20309;&#21709;&#24212;&#26032;&#20449;&#24687;&#65292;&#29305;&#21035;&#20851;&#27880;&#25512;&#26029;&#20559;&#24046;&#65292;&#20197;&#25913;&#21892;&#24773;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01841</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#22810;&#21464;&#37327;&#31995;&#32479;&#30340;&#25512;&#26029;&#30697;
&lt;/p&gt;
&lt;p&gt;
Inferential Moments of Uncertain Multivariable Systems. (arXiv:2305.01841v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#19981;&#30830;&#23450;&#22810;&#21464;&#37327;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25512;&#26029;&#30697;&#25551;&#36848;&#20998;&#24067;&#39044;&#35745;&#22914;&#20309;&#21709;&#24212;&#26032;&#20449;&#24687;&#65292;&#29305;&#21035;&#20851;&#27880;&#25512;&#26029;&#20559;&#24046;&#65292;&#20197;&#25913;&#21892;&#24773;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31216;&#20026;&#8220;&#25512;&#26029;&#30697;&#8221;&#30340;&#19968;&#32452;&#37327;&#26469;&#20998;&#26512;&#19981;&#30830;&#23450;&#22810;&#21464;&#37327;&#31995;&#32479;&#34892;&#20026;&#30340;&#26032;&#33539;&#24335;&#12290;&#36793;&#32536;&#21270;&#26159;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36807;&#31243;&#65292;&#23427;&#36890;&#36807;&#24179;&#22343;&#26465;&#20214;&#27010;&#29575;&#26469;&#37327;&#21270;&#25152;&#20851;&#27880;&#27010;&#29575;&#30340;&#26399;&#26395;&#20540;&#12290;&#25512;&#26029;&#30697;&#26159;&#25551;&#36848;&#20998;&#24067;&#39044;&#35745;&#22914;&#20309;&#21709;&#24212;&#26032;&#20449;&#24687;&#30340;&#39640;&#38454;&#26465;&#20214;&#27010;&#29575;&#30697;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#26029;&#20559;&#24046;&#65292;&#23427;&#26159;&#26399;&#26395;&#30340;&#27010;&#29575;&#27874;&#21160;&#65292;&#38543;&#30528;&#25512;&#26029;&#26356;&#26032;&#21478;&#19968;&#20010;&#21464;&#37327;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#20197;&#25512;&#26029;&#30697;&#30340;&#24418;&#24335;&#25214;&#21040;&#20102;&#20114;&#20449;&#24687;&#30340;&#24130;&#32423;&#25968;&#23637;&#24320;&#24335;&#65292;&#36825;&#24847;&#21619;&#30528;&#25512;&#26029;&#30697;&#36923;&#36753;&#21487;&#33021;&#23545;&#36890;&#24120;&#20351;&#29992;&#20449;&#24687;&#35770;&#24037;&#20855;&#25191;&#34892;&#30340;&#20219;&#21153;&#26377;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24212;&#29992;&#20013;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#25512;&#26029;&#20559;&#24046;&#65292;&#20197;&#25913;&#21892;&#24773;&#22659;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article offers a new paradigm for analyzing the behavior of uncertain multivariable systems using a set of quantities we call \emph{inferential moments}. Marginalization is an uncertainty quantification process that averages conditional probabilities to quantify the \emph{expected value} of a probability of interest. Inferential moments are higher order conditional probability moments that describe how a distribution is expected to respond to new information. Of particular interest in this article is the \emph{inferential deviation}, which is the expected fluctuation of the probability of one variable in response to an inferential update of another. We find a power series expansion of the Mutual Information in terms of inferential moments, which implies that inferential moment logic may be useful for tasks typically performed with information theoretic tools. We explore this in two applications that analyze the inferential deviations of a Bayesian Network to improve situational aw
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20276;&#38543;&#26041;&#27861;&#22312;AEM&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#20248;&#21270;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; NeuLag&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#23637;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13860</link><description>&lt;p&gt;
&#21033;&#29992;&#20934;&#30830;&#30340;&#27169;&#25311;&#22120;&#21644;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#26041;&#26696;&#22686;&#24378;&#21453;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators and Promising Candidates. (arXiv:2304.13860v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13860
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20276;&#38543;&#26041;&#27861;&#22312;AEM&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#20248;&#21270;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; NeuLag&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#23637;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21453;&#38382;&#39064;&#25216;&#26415;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#20854;&#20013;&#31070;&#32463;&#20276;&#38543;&#65288;NA&#65289;&#26041;&#27861;&#37319;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#65292;&#22312;&#20154;&#24037;&#30005;&#30913;&#26448;&#26009;&#65288;AEM&#65289;&#35774;&#35745;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#22120;&#20934;&#30830;&#24615;&#23545;NA&#26041;&#27861;&#35299;&#20915;&#26041;&#26696;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#24403;&#27169;&#25311;&#22120;&#24222;&#22823;&#19988;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#26102;&#65292;&#22312;&#35813;&#26041;&#27861;&#20013;&#23454;&#29616;&#36275;&#22815;&#30340;&#20248;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20174;&#24037;&#31243;&#35282;&#24230;&#26469;&#30475;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#34892;&#20026;&#23578;&#26410;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#25311;&#22120;&#20934;&#30830;&#24615;&#23545;&#35299;&#20915;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#35299;&#20915;&#26041;&#26696;&#36234;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;NA&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21517;&#20026;&#31070;&#32463;&#25289;&#26684;&#26391;&#26085;&#65288;NeuLag&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning inverse techniques have attracted significant attention in recent years. Among them, the neural adjoint (NA) method, which employs a neural network surrogate simulator, has demonstrated impressive performance in the design tasks of artificial electromagnetic materials (AEM). However, the impact of the surrogate simulators' accuracy on the solutions in the NA method remains uncertain. Furthermore, achieving sufficient optimization becomes challenging in this method when the surrogate simulator is large, and computational resources are limited. Additionally, the behavior under constraints has not been studied, despite its importance from the engineering perspective. In this study, we investigated the impact of surrogate simulators' accuracy on the solutions and discovered that the more accurate the surrogate simulator is, the better the solutions become. We then developed an extension of the NA method, named Neural Lagrangian (NeuLag) method, capable of efficiently optimizi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21069;&#39044;&#27979;&#21019;&#20260;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12737</link><description>&lt;p&gt;
ICU&#21019;&#20260;&#24739;&#32773;&#26089;&#26399;&#33043;&#27602;&#30151;&#21457;&#20316;&#39044;&#27979;&#30340;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NPRL: Nightly Profile Representation Learning for Early Sepsis Onset Prediction in ICU Trauma Patients. (arXiv:2304.12737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21069;&#39044;&#27979;&#21019;&#20260;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33043;&#27602;&#30151;&#26159;&#19968;&#31181;&#28304;&#20110;&#24863;&#26579;&#65292;&#20197;&#20005;&#37325;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#20026;&#29305;&#24449;&#30340;&#32508;&#21512;&#30151;&#65292;&#24182;&#19988;&#26159;&#20840;&#29699;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;(ICU)&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#36890;&#36807;&#26089;&#26399;&#24212;&#29992;&#25239;&#29983;&#32032;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#24182;&#21457;&#30151;&#65292;&#22240;&#27492;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#21457;&#20316;&#26102;&#38388;&#23545;&#24739;&#32773;&#30340;&#29983;&#23384;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#22312;&#21307;&#30103;&#22522;&#30784;&#35774;&#26045;&#20869;&#37096;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#65292;&#19981;&#36275;&#20197;&#26089;&#26399;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#21457;&#29983;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24739;&#32773;&#29983;&#29702;&#21644;&#20020;&#24202;&#25968;&#25454;&#30340;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;(NPRL)&#65292;&#20197;&#25429;&#25417;&#24739;&#32773;&#29366;&#24577;&#38543;&#26102;&#38388;&#21160;&#24577;&#25913;&#21464;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#36825;&#20123;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#26102;&#38388;&#65292;&#24182;&#36229;&#36234;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sepsis is a syndrome that develops in response to the presence of infection. It is characterized by severe organ dysfunction and is one of the leading causes of mortality in Intensive Care Units (ICUs) worldwide. These complications can be reduced through early application of antibiotics, hence the ability to anticipate the onset of sepsis early is crucial to the survival and well-being of patients. Current machine learning algorithms deployed inside medical infrastructures have demonstrated poor performance and are insufficient for anticipating sepsis onset early. In recent years, deep learning methodologies have been proposed to predict sepsis, but some fail to capture the time of onset (e.g., classifying patients' entire visits as developing sepsis or not) and others are unrealistic to be deployed into medical facilities (e.g., creating training instances using a fixed time to onset where the time of onset needs to be known apriori). Therefore, in this paper, we first propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24418;&#29366;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;21%&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11247</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22312;&#22797;&#26434;&#24418;&#29366;&#20013;&#27169;&#25311;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes. (arXiv:2304.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24418;&#29366;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;21%&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#27969;&#20307;&#30340;&#36895;&#24230;&#21644;&#21387;&#21147;&#20998;&#24067;&#65288;&#36890;&#36807;&#35299;&#20915;&#32435;&#32500;&#23572;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65289;&#26159;&#21270;&#23398;&#12289;&#33021;&#28304;&#12289;&#21046;&#33647;&#24037;&#19994;&#20197;&#21450;&#26426;&#26800;&#24037;&#31243;&#21644;&#31649;&#36947;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#27714;&#35299;&#22120;&#65288;&#22914;OpenFOAM&#21644;Ansys&#65289;&#22312;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#27599;&#24403;&#20960;&#20309;&#21442;&#25968;&#25110;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#34987;&#25913;&#21464;&#12290;&#29289;&#29702;&#23398;&#20449;&#36182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26159;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#27969;&#20307;&#27969;&#21160;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36866;&#24212;&#20960;&#20309;&#24418;&#29366;&#21644;&#32593;&#26684;&#23450;&#20041;&#30340;&#21464;&#21270;&#65292;&#20801;&#35768;&#36328;&#19981;&#21516;&#24418;&#29366;&#36827;&#34892;&#27010;&#25324;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28151;&#21512;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#27169;&#25311;&#19977;&#32500; Y &#22411;&#28151;&#21512;&#22120;&#20013;&#30340;&#23618;&#27969;&#27969;&#20307;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#37327;&#23376;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982; PINN &#30340;&#28789;&#27963;&#24615;&#30456;&#32467;&#21512;&#65292;&#31934;&#24230;&#27604;&#26222;&#36890; PINN &#25552;&#39640;&#20102; 21&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the distribution of the velocities and pressures of a fluid (by solving the Navier-Stokes equations) is a principal task in the chemical, energy, and pharmaceutical industries, as well as in mechanical engineering and the design of pipeline systems. With existing solvers, such as OpenFOAM and Ansys, simulations of fluid dynamics in intricate geometries are computationally expensive and require re-simulation whenever the geometric parameters or the initial and boundary conditions are altered. Physics-informed neural networks (PINNs) are a promising tool for simulating fluid flows in complex geometries, as they can adapt to changes in the geometry and mesh definitions, allowing for generalization across different shapes. We present a hybrid quantum physics-informed neural network that simulates laminar fluid flows in 3D Y-shaped mixers. Our approach combines the expressive power of a quantum model with the flexibility of a PINN, resulting in a 21% higher accuracy compared to a pu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#20256;&#32479;&#38889;&#21307;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.17807</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#32479;&#38889;&#21307;&#20013;&#30340;&#28508;&#21147;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#25991;&#21270;&#36866;&#24212;&#20445;&#20581;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare. (arXiv:2303.17807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#20256;&#32479;&#38889;&#21307;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#38889;&#21307;&#27880;&#37325;&#20010;&#20307;&#21270;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#25968;&#25454;&#26377;&#38480;&#19988;&#36807;&#31243;&#38544;&#24615;&#65292;&#20351;AI&#24314;&#27169;&#22256;&#38590;&#12290;GPT-3.5&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23613;&#31649;&#32570;&#20047;&#21307;&#23398;&#19987;&#19994;&#22521;&#35757;&#65292;&#20294;&#24050;&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#21307;&#30103;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#21462;&#24471;&#20102;42.06%&#21644;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;GPT-4&#25509;&#36817;&#21450;&#26684;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Traditional Korean medicine (TKM) emphasizes individualized diagnosis and treatment, making AI modeling difficult due to limited data and implicit processes. GPT-3.5 and GPT-4, large language models, have shown impressive medical knowledge despite lacking medicine-specific training. This study aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using the Korean National Licensing Examination for Korean Medicine Doctors. Methods: GPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questions from the 2022 examination across 12 subjects. Each question was independently evaluated five times in an initialized session. Results: GPT-3.5 and GPT-4 achieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passing performance. There were significant differences in accuracy by subjects, with 83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine (2). Both models showed high accuracy in recall-based and diagnosis-based questi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;MvCAN&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17245</link><description>&lt;p&gt;
&#30740;&#31350;&#21644;&#20943;&#36731;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios. (arXiv:2303.17245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;MvCAN&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;MvC&#65289;&#26088;&#22312;&#25506;&#32034;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31867;&#21035;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#26631;&#31614;&#30417;&#30563;&#12290;&#22810;&#35270;&#22270;&#27604;&#21333;&#35270;&#22270;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#22240;&#27492;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22914;&#26524;&#35270;&#22270;&#22024;&#26434;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20005;&#37325;&#36864;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#30740;&#31350;&#20102;&#22024;&#26434;&#35270;&#22270;&#30340;&#32570;&#28857;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;&#31216;&#20026;MvCAN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MvC&#30446;&#26631;&#65292;&#20351;&#24471;&#19981;&#20849;&#20139;&#21442;&#25968;&#21644;&#19981;&#19968;&#33268;&#30340;&#32858;&#31867;&#39044;&#27979;&#21487;&#20197;&#36328;&#36234;&#22810;&#20010;&#35270;&#22270;&#65292;&#20197;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#36845;&#20195;&#36807;&#31243;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#31283;&#20581;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#25366;&#25496;&#22810;&#20010;&#35270;&#22270;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;MvCAN&#30340;&#24037;&#20316;&#26159;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#65292;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#23454;&#29616;&#30340;&#12290;&#26368;&#21518;&#65292;&#23545;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26032;&#25910;&#38598;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MvCAN&#22312;&#22788;&#29702;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#22024;&#26434;&#35270;&#22270;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering (MvC) aims at exploring the category structure among multi-view data without label supervision. Multiple views provide more information than single views and thus existing MvC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MvC method (namely MvCAN) to address this issue. Specifically, we propose a novel MvC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a non-parametric iterative process is designed to generate a robust learning target for mining multiple views' useful information. Theoretical analysis reveals that MvCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on publ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.16047</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#25506;&#32034;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#25972;&#20010;&#20248;&#31168;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#39046;&#22495;&#19987;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#29983;&#25104;&#21333;&#20010;&#27169;&#22411;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#19981;&#21033;&#20110;&#27492;&#31867;&#20132;&#20114;&#12290;&#36817;&#20284;&#21644;&#25506;&#32034;Rashomon&#38598;&#65292;&#21363;&#25152;&#26377;&#36817;&#20046;&#26368;&#20248;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#25552;&#20379;&#29992;&#25143;&#21487;&#25628;&#32034;&#30340;&#31354;&#38388;&#21253;&#21547;&#22810;&#26679;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#23454;&#38469;&#25361;&#25112;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#20174;&#20013;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#36817;&#20284;&#20855;&#26377;&#22266;&#23450;&#25903;&#25345;&#38598;&#30340;GAMs&#30340;Rashomon&#38598;&#30340;&#26925;&#29699;&#24418;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26925;&#29699;&#24418;&#36817;&#20284;&#20102;&#35768;&#22810;&#19981;&#21516;&#25903;&#25345;&#38598;&#30340;Rashomon&#38598;&#12290;&#36817;&#20284;&#30340;Rashomon&#38598;&#20026;&#35299;&#20915;&#23454;&#38469;&#25361;&#25112;&#65292;&#20363;&#22914;&#65288;1&#65289;&#30740;&#31350;&#27169;&#22411;&#31867;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65307;&#65288;2&#65289;&#22312;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;&#26597;&#25214;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
&lt;/p&gt;</description></item><item><title>GCondNet&#21033;&#29992;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#36890;&#36807;&#21019;&#24314;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#26465;&#20214;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.06302</link><description>&lt;p&gt;
GCondNet: &#19968;&#31181;&#25913;&#36827;&#23567;&#22411;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data. (arXiv:2211.06302v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06302
&lt;/p&gt;
&lt;p&gt;
GCondNet&#21033;&#29992;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#36890;&#36807;&#21019;&#24314;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#26465;&#20214;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#22788;&#29702;&#39640;&#32500;&#20294;&#26679;&#26412;&#25968;&#37327;&#36739;&#23567;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#26102;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#24403;&#21069;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#27861;&#20551;&#23450;&#26435;&#37325;&#20043;&#38388;&#30456;&#20114;&#29420;&#31435;&#65292;&#24403;&#26679;&#26412;&#19981;&#36275;&#20197;&#20934;&#30830;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#23567;&#25968;&#25454;&#22330;&#26223;&#19979;&#65292;&#21033;&#29992;&#20854;&#20182;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GCondNet&#65292;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#32467;&#26500;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#32500;&#24230;&#22312;&#26679;&#26412;&#20043;&#38388;&#21019;&#24314;&#19968;&#20010;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#25552;&#21462;&#36825;&#31181;&#38544;&#21547;&#32467;&#26500;&#65292;&#20197;&#21450;&#35843;&#25972;&#28508;&#22312;&#39044;&#27979; MLP &#32593;&#32476;&#30340;&#31532;&#19968;&#23618;&#21442;&#25968;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#12290;&#36890;&#36807;&#21019;&#24314;&#35768;&#22810;&#23567;&#22270;&#65292;GCondNet &#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#39640;&#32500;&#29305;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models often struggle with high-dimensional but small sample-size tabular datasets. One reason is that current weight initialisation methods assume independence between weights, which can be problematic when there are insufficient samples to estimate the model's parameters accurately. In such small data scenarios, leveraging additional structures can improve the model's training stability and performance. To address this, we propose GCondNet, a general approach to enhance neural networks by leveraging implicit structures present in tabular data. We create a graph between samples for each data dimension, and utilise Graph Neural Networks (GNNs) for extracting this implicit structure, and for conditioning the parameters of the first layer of an underlying predictor MLP network. By creating many small graphs, GCondNet exploits the data's high-dimensionality, and thus improves the performance of an underlying predictor network. We demonstrate the effectiveness of our method 
&lt;/p&gt;</description></item><item><title>&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#24050;&#32463;&#20174;&#20197;&#20934;&#30830;&#24615;&#20026;&#23548;&#21521;&#36716;&#21464;&#20026;&#20197;&#36879;&#26126;&#12289;&#20844;&#27491;&#12289;&#31283;&#20581;&#24615;&#20026;&#29305;&#28857;&#30340;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2208.06265</link><description>&lt;p&gt;
&#21487;&#20449;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Recommender Systems. (arXiv:2208.06265v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06265
&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#24050;&#32463;&#20174;&#20197;&#20934;&#30830;&#24615;&#20026;&#23548;&#21521;&#36716;&#21464;&#20026;&#20197;&#36879;&#26126;&#12289;&#20844;&#27491;&#12289;&#31283;&#20581;&#24615;&#20026;&#29305;&#28857;&#30340;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#24222;&#22823;&#30340;&#30446;&#24405;&#20013;&#26377;&#25928;&#22320;&#26816;&#32034;&#24863;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#33268;&#21147;&#20110;&#24320;&#21457;&#20934;&#30830;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#36234;&#26469;&#36234;&#22810;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26469;&#33258;&#25915;&#20987;&#12289;&#31995;&#32479;&#21644;&#29992;&#25143;&#20135;&#29983;&#30340;&#24178;&#25200;&#20197;&#21450;&#31995;&#32479;&#30340;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#24050;&#32463;&#19981;&#22815;&#65292;&#30740;&#31350;&#24517;&#39035;&#32771;&#34385;&#20854;&#20182;&#37325;&#35201;&#22240;&#32032;&#65292;&#22914;&#21487;&#20449;&#24230;&#12290;&#23545;&#20110;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#65292;&#19968;&#20010;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#35201;&#20934;&#30830;&#65292;&#32780;&#19988;&#36824;&#35201;&#36879;&#26126;&#12289;&#26080;&#20559;&#35265;&#12289;&#20844;&#27491;&#65292;&#24182;&#19988;&#23545;&#24178;&#25200;&#25110;&#25915;&#20987;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#35266;&#23519;&#23454;&#38469;&#19978;&#23548;&#33268;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#30340;&#33539;&#24335;&#36716;&#21464;: &#20174;&#20197;&#20934;&#30830;&#24615;&#20026;&#23548;&#21521;&#30340;&#25512;&#33616;&#31995;&#32479;&#36716;&#21521;&#20102;&#20197;&#21487;&#20449;&#24230;&#20026;&#23548;&#21521;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#32570;&#20047;&#23545;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#25991;&#29486;&#30340;&#31995;&#32479;&#27010;&#36848;&#21644;&#35752;&#35770;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35813;&#26032;&#20852;&#19988;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25991;&#29486;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RSs) aim to help users to effectively retrieve items of their interests from a large catalogue. For a quite long period of time, researchers and practitioners have been focusing on developing accurate RSs. Recent years have witnessed an increasing number of threats to RSs, coming from attacks, system and user generated noise, system bias. As a result, it has become clear that a strict focus on RS accuracy is limited and the research must consider other important factors, e.g., trustworthiness. For end users, a trustworthy RS (TRS) should not only be accurate, but also transparent, unbiased and fair as well as robust to noise or attacks. These observations actually led to a paradigm shift of the research on RSs: from accuracy-oriented RSs to TRSs. However, researchers lack a systematic overview and discussion of the literature in this novel and fast developing field of TRSs. To this end, in this paper, we provide an overview of TRSs, including a discussion of the mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#30340;&#23384;&#26723;&#65292;&#38598;&#20013;&#25910;&#38598;&#20102;&#21508;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#23545;&#38382;&#39064;&#30340;&#25551;&#36848;&#12289;&#26684;&#24335;&#21644;&#29305;&#24615;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#21450;&#21015;&#20030;&#20102;&#30456;&#20851;&#31639;&#27861;&#12290;&#35813;&#23384;&#26723;&#26088;&#22312;&#26041;&#20415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#24050;&#26377;&#24037;&#20316;&#30340;&#27604;&#36739;&#65292;&#24182;&#27426;&#36814;&#25552;&#20132;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.03574</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#23384;&#26723;
&lt;/p&gt;
&lt;p&gt;
Structured Prediction Problem Archive. (arXiv:2202.03574v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03574
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#30340;&#23384;&#26723;&#65292;&#38598;&#20013;&#25910;&#38598;&#20102;&#21508;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#23545;&#38382;&#39064;&#30340;&#25551;&#36848;&#12289;&#26684;&#24335;&#21644;&#29305;&#24615;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#21450;&#21015;&#20030;&#20102;&#30456;&#20851;&#31639;&#27861;&#12290;&#35813;&#23384;&#26723;&#26088;&#22312;&#26041;&#20415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#24050;&#26377;&#24037;&#20316;&#30340;&#27604;&#36739;&#65292;&#24182;&#27426;&#36814;&#25552;&#20132;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#20043;&#19968;&#12290;&#20026;&#20102;&#20415;&#20110;&#31639;&#27861;&#24320;&#21457;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22320;&#26041;&#25910;&#38598;&#20102;&#22823;&#37327;&#26131;&#20110;&#38405;&#35835;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#25968;&#25454;&#38598;&#30340;&#23384;&#26723;&#38142;&#25509;&#65292;&#38382;&#39064;&#25551;&#36848;&#21644;&#38382;&#39064;&#26684;&#24335;&#65292;&#20197;&#21450;&#38382;&#39064;&#29305;&#24615;&#30340;&#31616;&#35201;&#24635;&#32467;&#65292;&#21253;&#25324;&#22823;&#23567;&#65292;&#23454;&#20363;&#25968;&#37327;&#31561;&#12290;&#20026;&#20102;&#21442;&#32771;&#65292;&#25105;&#20204;&#36824;&#21015;&#20030;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#19968;&#20123;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#20013;&#22830;&#23384;&#20648;&#24211;&#33021;&#22815;&#26356;&#23481;&#26131;&#22320;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#24050;&#26377;&#24037;&#20316;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#27426;&#36814;&#25552;&#20132;&#26377;&#36259;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#65292;&#20197;&#20415;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#30340;&#23384;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured prediction problems are one of the fundamental tools in machine learning. In order to facilitate algorithm development for their numerical solution, we collect in one place a large number of datasets in easy to read formats for a diverse set of problem classes. We provide archival links to datasets, description of the considered problems and problem formats, and a short summary of problem characteristics including size, number of instances etc. For reference we also give a non-exhaustive selection of algorithms proposed in the literature for their solution. We hope that this central repository will make benchmarking and comparison to established works easier. We welcome submission of interesting new datasets and algorithms for inclusion in our archive.
&lt;/p&gt;</description></item></channel></rss>