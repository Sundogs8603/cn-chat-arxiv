<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#20223;&#30495;&#22120;&#22312;&#32447;&#35757;&#32451;&#20122;&#32593;&#26684;&#21442;&#25968;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#25439;&#22833;&#20989;&#25968;&#36866;&#24212;&#38750;&#21487;&#24494;&#20998;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#24182;&#36890;&#36807;&#26102;&#38388;&#31215;&#20998;&#27493;&#39588;&#20801;&#35768;&#26799;&#24230;&#20256;&#25773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#31070;&#32463;&#20223;&#30495;&#22120;&#21644;&#21442;&#25968;&#21270;&#32452;&#20214;&#20998;&#21035;&#29992;&#30456;&#24212;&#30340;&#25439;&#22833;&#37327;&#36827;&#34892;&#35757;&#32451;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#26576;&#20123;&#36817;&#20284;&#20559;&#24046;&#30340;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2310.19385</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20223;&#30495;&#22120;&#30340;&#26080;&#26799;&#24230;&#22312;&#32447;&#23398;&#20064;&#20122;&#32593;&#26684;&#23610;&#24230;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Gradient-free online learning of subgrid-scale dynamics with neural emulators. (arXiv:2310.19385v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#20223;&#30495;&#22120;&#22312;&#32447;&#35757;&#32451;&#20122;&#32593;&#26684;&#21442;&#25968;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#25439;&#22833;&#20989;&#25968;&#36866;&#24212;&#38750;&#21487;&#24494;&#20998;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#24182;&#36890;&#36807;&#26102;&#38388;&#31215;&#20998;&#27493;&#39588;&#20801;&#35768;&#26799;&#24230;&#20256;&#25773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#31070;&#32463;&#20223;&#30495;&#22120;&#21644;&#21442;&#25968;&#21270;&#32452;&#20214;&#20998;&#21035;&#29992;&#30456;&#24212;&#30340;&#25439;&#22833;&#37327;&#36827;&#34892;&#35757;&#32451;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#26576;&#20123;&#36817;&#20284;&#20559;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20122;&#32593;&#26684;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#25439;&#22833;&#20989;&#25968;&#36866;&#24212;&#38750;&#21487;&#24494;&#20998;&#25968;&#20540;&#27714;&#35299;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#20223;&#30495;&#22120;&#35757;&#32451;&#31616;&#21270;&#29366;&#24577;&#31354;&#38388;&#27714;&#35299;&#22120;&#30340;&#36817;&#20284;&#20540;&#65292;&#28982;&#21518;&#36890;&#36807;&#26102;&#38388;&#31215;&#20998;&#27493;&#39588;&#20801;&#35768;&#26799;&#24230;&#20256;&#25773;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#19981;&#35745;&#31639;&#21407;&#22987;&#27714;&#35299;&#22120;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#22823;&#37096;&#20998;&#22312;&#32447;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#31070;&#32463;&#20223;&#30495;&#22120;&#21644;&#21442;&#25968;&#21270;&#32452;&#20214;&#20998;&#21035;&#29992;&#30456;&#24212;&#30340;&#25439;&#22833;&#37327;&#36827;&#34892;&#35757;&#32451;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#26576;&#20123;&#36817;&#20284;&#20559;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a generic algorithm to train machine learning-based subgrid parametrizations online, i.e., with $\textit{a posteriori}$ loss functions for non-differentiable numerical solvers. The proposed approach leverage neural emulators to train an approximation of the reduced state-space solver, which is then used to allows gradient propagation through temporal integration steps. The algorithm is able to recover most of the benefit of online strategies without having to compute the gradient of the original solver. It is demonstrated that training the neural emulator and parametrization components separately with respective loss quantities is necessary in order to minimize the propagation of some approximation bias.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13139</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#23436;&#20840;&#30001;&#36866;&#24403;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#26469;&#25551;&#36848;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20219;&#20309;&#22312;&#26631;&#35760;&#22270;&#19978;&#35299;&#37322;&#30340;&#20851;&#20110;&#20108;&#20803;&#36923;&#36753;&#29255;&#27573;&#65288;GC2&#65289;&#30340;&#26597;&#35810;&#37117;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#22823;&#23567;&#20165;&#21462;&#20915;&#20110;&#26597;&#35810;&#28145;&#24230;&#30340;GNN&#26469;&#34920;&#31034;&#12290;&#27491;&#22914;[Barcelo&#65286;Al&#12290;&#65292;2020&#65292;Grohe&#65292;2021]&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#36825;&#20010;&#25551;&#36848;&#36866;&#29992;&#20110;&#19968;&#32452;&#28608;&#27963;&#20989;&#25968;&#30340;&#23478;&#26063;&#65292;&#36825;&#34920;&#26126;GNN&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#26469;&#34920;&#36798;&#19981;&#21516;&#30340;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#23618;&#27425;&#32467;&#26500;&#30340;&#23384;&#22312;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#26080;&#27861;&#34920;&#31034;GC2&#26597;&#35810;&#12290;&#36825;&#24847;&#21619;&#30528;&#22810;&#39033;&#24335;&#21644;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#12289;sigmoid&#12289;&#21452;&#26354;&#27491;&#20999;&#31561;&#65289;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20998;&#31163;&#65292;&#24182;&#22238;&#31572;&#20102;[Grohe&#65292;2021]&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo &amp; Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25913;&#36827;&#20102;SCGAN&#27169;&#22411;&#20013;&#30340;&#30456;&#20284;&#24615;&#32422;&#26463;&#65292;&#20351;&#29992;SSIM&#24230;&#37327;&#22270;&#20687;&#30456;&#20284;&#24615;&#24182;&#24212;&#29992;&#23545;&#27604;&#25439;&#22833;&#21407;&#21017;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12262</link><description>&lt;p&gt;
&#25913;&#36827;SCGAN&#30340;&#30456;&#20284;&#24615;&#32422;&#26463;&#24182;&#23398;&#20064;&#26356;&#22909;&#30340;&#35299;&#32806;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving SCGAN's Similarity Constraint and Learning a Better Disentangled Representation. (arXiv:2310.12262v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25913;&#36827;&#20102;SCGAN&#27169;&#22411;&#20013;&#30340;&#30456;&#20284;&#24615;&#32422;&#26463;&#65292;&#20351;&#29992;SSIM&#24230;&#37327;&#22270;&#20687;&#30456;&#20284;&#24615;&#24182;&#24212;&#29992;&#23545;&#27604;&#25439;&#22833;&#21407;&#21017;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SCGAN&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#30456;&#20284;&#24615;&#32422;&#26463;&#65292;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#26465;&#20214;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#12290;&#30456;&#20284;&#24615;&#32422;&#26463;&#20316;&#20026;&#23548;&#24072;&#65292;&#25351;&#23548;&#29983;&#25104;&#22120;&#32593;&#32476;&#29702;&#35299;&#22522;&#20110;&#26465;&#20214;&#30340;&#34920;&#31034;&#24046;&#24322;&#12290;&#25105;&#20204;&#28145;&#20837;&#29702;&#35299;&#20102;SCGAN&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#36825;&#31181;&#29702;&#35299;&#20351;&#25105;&#20204;&#24847;&#35782;&#21040;&#30456;&#20284;&#24615;&#32422;&#26463;&#30340;&#21151;&#33021;&#31867;&#20284;&#20110;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#20855;&#26377;&#39640;&#24230;&#29702;&#35299;&#21644;&#26234;&#33021;&#30340;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#22270;&#20687;&#30340;&#32467;&#26500;&#21644;&#39640;&#32423;&#29305;&#24449;&#26469;&#24230;&#37327;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#23601;&#20687;&#20154;&#31867;&#19968;&#26679;&#12290;&#25105;&#20204;&#23545;SCGAN&#36827;&#34892;&#20102;&#20004;&#20010;&#20027;&#35201;&#25913;&#21464;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#25913;&#36827;&#30340;&#27169;&#22411;&#65306;&#20351;&#29992;SSIM&#26469;&#24230;&#37327;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#23558;&#23545;&#27604;&#25439;&#22833;&#21407;&#21017;&#24212;&#29992;&#20110;&#30456;&#20284;&#24615;&#32422;&#26463;&#12290;&#25913;&#36827;&#30340;&#27169;&#22411;&#22312;FID&#21644;FactorVAE&#25351;&#26631;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25913;&#36827;&#30340;&#27169;&#22411;&#36824;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
SCGAN adds a similarity constraint between generated images and conditions as a regularization term on generative adversarial networks. Similarity constraint works as a tutor to instruct the generator network to comprehend the difference of representations based on conditions. We understand how SCGAN works on a deeper level. This understanding makes us realize that the similarity constraint functions like the contrastive loss function. We believe that a model with high understanding and intelligence measures the similarity between images based on their structure and high level features, just like humans do. Two major changes we applied to SCGAN in order to make a modified model are using SSIM to measure similarity between images and applying contrastive loss principles to the similarity constraint. The modified model performs better using FID and FactorVAE metrics. The modified model also has better generalisability compared to other models. Keywords Generative Adversarial Nets, Unsupe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#21253;&#25324;&#23545;&#21518;&#39564;&#20998;&#24067;&#12289;&#37325;&#26500;&#25439;&#22833;&#21644;&#36755;&#20837;&#19982;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.04935</link><description>&lt;p&gt;
&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#32473;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory. (arXiv:2310.04935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04935
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#21253;&#25324;&#23545;&#21518;&#39564;&#20998;&#24067;&#12289;&#37325;&#26500;&#25439;&#22833;&#21644;&#36755;&#20837;&#19982;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#23427;&#20204;&#30340;&#38382;&#19990;&#20197;&#26469;&#65292;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#29702;&#35770;&#24615;&#36136;&#20173;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#12290;&#26412;&#25991;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;VAEs&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;&#29420;&#31435;&#26679;&#26412;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#39318;&#20010;PAC-Bayesian&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#20026;VAE&#30340;&#37325;&#26500;&#25439;&#22833;&#25552;&#20379;&#20102;&#27867;&#21270;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36755;&#20837;&#20998;&#24067;&#19982;VAE&#29983;&#25104;&#27169;&#22411;&#23450;&#20041;&#30340;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36755;&#20837;&#20998;&#24067;&#19982;VAE&#29983;&#25104;&#27169;&#22411;&#23450;&#20041;&#30340;&#20998;&#24067;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their inception, Variational Autoencoders (VAEs) have become central in machine learning. Despite their widespread use, numerous questions regarding their theoretical properties remain open. Using PAC-Bayesian theory, this work develops statistical guarantees for VAEs. First, we derive the first PAC-Bayesian bound for posterior distributions conditioned on individual samples from the data-generating distribution. Then, we utilize this result to develop generalization guarantees for the VAE's reconstruction loss, as well as upper bounds on the distance between the input and the regenerated distributions. More importantly, we provide upper bounds on the Wasserstein distance between the input distribution and the distribution defined by the VAE's generative model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#30340;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.16620</link><description>&lt;p&gt;
&#27531;&#24046;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36229;&#21442;&#25968;&#36716;&#31227;&#65306;&#21160;&#24577;&#21644;&#32553;&#25918;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. (arXiv:2309.16620v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16620
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#30340;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#20419;&#20351;&#20174;&#19994;&#32773;&#23547;&#25214;&#20351;&#29992;&#36739;&#23567;&#32593;&#32476;&#30340;&#20195;&#29702;&#26041;&#27861;&#36827;&#34892;&#35843;&#25972;&#12290;&#20854;&#20013;&#19968;&#20010;&#24314;&#35758;&#20351;&#29992;$\mu$P&#21442;&#25968;&#21270;&#32593;&#32476;&#65292;&#20854;&#20013;&#23567;&#23485;&#24230;&#32593;&#32476;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#36716;&#31227;&#21040;&#20219;&#24847;&#23485;&#24230;&#30340;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26041;&#26696;&#20013;&#65292;&#36229;&#21442;&#25968;&#19981;&#20250;&#22312;&#19981;&#21516;&#28145;&#24230;&#20043;&#38388;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;$1/\sqrt{\text{depth}}$&#30340;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#21442;&#25968;&#21270;&#35757;&#32451;&#30340;&#27531;&#24046;&#32467;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;ResNet&#21644;Vision Transformer&#65292;&#22312;CIFAR-10&#21644;ImageNet&#19978;&#23637;&#31034;&#20102;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#21457;&#29616;&#24471;&#21040;&#20102;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#21160;&#26426;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#25551;&#36848;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks transfer to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and Vision Transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#20840;&#23616;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.15730</link><description>&lt;p&gt;
&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Temporal graph models fail to capture global temporal dynamics. (arXiv:2309.15730v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15730
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#20840;&#23616;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#38142;&#25509;&#23646;&#24615;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#26102;&#38388;&#22270;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26368;&#36817;&#27969;&#34892;&#33410;&#28857;"&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#22270;&#22522;&#20934;&#30340;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20004;&#20010;&#24230;&#37327;&#65292;&#21487;&#20197;&#37327;&#21270;&#25968;&#25454;&#38598;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20840;&#23616;&#21160;&#24577;&#30340;&#24378;&#24230;&#12290;&#36890;&#36807;&#20998;&#26512;&#25105;&#20204;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#22522;&#32447;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#36127;&#37319;&#26679;&#35780;&#20272;&#26041;&#27861;&#22312;&#20855;&#26377;&#24378;&#28872;&#26102;&#38388;&#21160;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36864;&#21270;&#65292;&#23548;&#33268;&#26080;&#27861;&#23545;&#26102;&#38388;&#22270;&#32593;&#32476;&#36827;&#34892;&#25490;&#24207;&#30340;&#39044;&#27979;&#23436;&#20840;&#39281;&#21644;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36127;&#37319;&#26679;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;&#26080;&#36127;&#37319;&#26679;&#30340;&#38750;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of "recently popular nodes" outperforming other methods on all medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14348</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#25269;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;LLMs&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#24694;&#24847;&#20869;&#23481;&#12290;&#23613;&#31649;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#24182;&#38450;&#27490;&#23427;&#20204;&#29983;&#25104;&#19981;&#36866;&#24403;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#23545;&#40784;&#36890;&#24120;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20248;&#21270;&#25110;&#25163;&#24037;&#26500;&#24314;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#20197;&#38450;&#33539;&#28508;&#22312;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#26500;&#24314;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#65292;&#36890;&#36807;&#20855;&#26377;&#31283;&#20581;&#23545;&#40784;&#26816;&#26597;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;LLM&#36827;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;RA-LLM&#22312;&#38450;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#19982;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.11955</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Forward-Forward Algorithm for Self-Supervised Learning. (arXiv:2309.11955v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#19982;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20986;&#26377;&#29992;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#20102;&#21453;&#21521;&#20256;&#25773;&#20316;&#20026;&#35757;&#32451;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#26368;&#36817;&#65292;Geoffrey Hinton&#25552;&#20986;&#20102;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#20102;&#20004;&#27425;&#21521;&#21069;&#20256;&#36882;&#21644;&#27599;&#23618;&#37117;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#32593;&#32476;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#21453;&#21521;&#20256;&#25773;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#19982;&#21453;&#21521;&#20256;&#25773;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#31354;&#38388;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20351;&#29992;&#20102;&#22235;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;MNIST&#12289;F-MNIST&#12289;SVHN&#21644;CIFAR-10&#65292;&#20197;&#21450;&#19977;&#31181;&#24120;&#29992;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21363;&#26059;&#36716;&#12289;&#32763;&#36716;&#21644;&#25340;&#22270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21521;&#21069;-&#21521;&#21069;&#31639;&#27861;&#19982;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning has seen remarkable progress in the last few years, with some of the recent methods being able to learn useful image representations without labels. These methods are trained using backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the forward-forward algorithm as an alternative training method. It utilizes two forward passes and a separate loss function for each layer to train the network without backpropagation.  In this study, for the first time, we study the performance of forward-forward vs. backpropagation for self-supervised representation learning and provide insights into the learned representation spaces. Our benchmark employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and three commonly used self-supervised representation learning techniques, namely rotation, flip and jigsaw.  Our main finding is that while the forward-forward algorithm performs comparably to backpropagation during (self-)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03886</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21151;&#33021;&#35299;&#37322;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21487;&#35835;&#30340;&#25551;&#36848;&#26631;&#35760;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22359;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#36825;&#20123;&#25551;&#36848;&#21487;&#20197;&#26292;&#38706;&#22833;&#36133;&#12289;&#24341;&#23548;&#24178;&#39044;&#65292;&#29978;&#33267;&#21487;&#20197;&#35299;&#37322;&#37325;&#35201;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#26800;&#21407;&#29702;&#30340;&#24050;&#35757;&#32451;&#32593;&#32476;&#25551;&#36848;&#37117;&#28041;&#21450;&#21040;&#23567;&#27169;&#22411;&#12289;&#29421;&#20041;&#29616;&#35937;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#20013;&#26631;&#35760;&#20986;&#25152;&#26377;&#20154;&#21487;&#35299;&#37322;&#30340;&#23376;&#35745;&#31639;&#20960;&#20046;&#32943;&#23450;&#38656;&#35201;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#39564;&#35777;&#25551;&#36848;&#30340;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#30340;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26377;&#38480;&#19988;&#20020;&#26102;&#12290;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#39564;&#35777;&#21644;&#27604;&#36739;&#24320;&#25918;&#24335;&#26631;&#35760;&#24037;&#20855;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;FIND&#65288;&#20989;&#25968;&#35299;&#37322;&#21644;&#25551;&#36848;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#26041;&#27861;&#26500;&#24314;&#27169;&#22359;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;FIND&#21253;&#21547;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#30340;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03251</link><description>&lt;p&gt;
&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#25193;&#23637;&#65292;&#34701;&#20837;&#20102;&#26102;&#38388;&#32500;&#24230;&#12290;&#22312;TKGs&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#25581;&#31034;&#21382;&#21490;&#23376;&#22270;&#21644;&#26102;&#38388;&#27169;&#24335;&#20013;&#30340;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#38752;&#23454;&#20307;&#24314;&#27169;&#26469;&#27169;&#25311;TKGs&#65292;&#22240;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#22312;&#30693;&#35782;&#34920;&#31034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#20986;&#29616;&#26032;&#23454;&#20307;&#12290;&#36825;&#20351;&#24471;&#20381;&#36182;&#20110;&#23454;&#20307;&#30340;&#26041;&#27861;&#24456;&#38590;&#24212;&#23545;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#26377;&#25928;&#22788;&#29702;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#20063;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#65292;&#23427;&#20197;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#23545;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TiPNN&#37319;&#29992;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#65292;&#21517;&#20026;&#21382;&#21490;&#26102;&#38388;&#22270;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#65292;&#36890;&#36807;&#28857;&#27880;&#37322;&#21644;&#24418;&#29366;&#20808;&#39564;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#19988;&#20173;&#33021;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#29992;&#20110;&#20998;&#21106;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#29983;&#25104;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.09835</link><description>&lt;p&gt;
&#36890;&#36807;&#28857;&#21644;&#24418;&#29366;&#27491;&#21017;&#21270;&#30340;&#25968;&#25454;&#21512;&#25104;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#65292;&#36890;&#36807;&#28857;&#27880;&#37322;&#21644;&#24418;&#29366;&#20808;&#39564;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#19988;&#20173;&#33021;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#29992;&#20110;&#20998;&#21106;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#29983;&#25104;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#38656;&#35201;&#23494;&#38598;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#25104;&#26412;&#39640;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#19982;&#23436;&#25972;&#26631;&#27880;&#25152;&#25551;&#36848;&#30340;&#23545;&#35937;&#30340;&#23436;&#25972;&#36718;&#24275;&#30456;&#27604;&#65292;&#28857;&#27880;&#37322;&#65292;&#29305;&#21035;&#26159;&#23545;&#35937;&#36136;&#24515;&#65292;&#26356;&#23481;&#26131;&#33719;&#21462;&#65292;&#24182;&#19988;&#20173;&#28982;&#20026;&#21518;&#32493;&#20998;&#21106;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#28857;&#27880;&#37322;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#20351;&#29992;&#24418;&#29366;&#20808;&#39564;&#32422;&#26463;&#37319;&#26679;&#19968;&#20010;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20197;&#38750;&#37197;&#23545;&#30340;&#26041;&#24335;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#65307;&#65288;3&#65289;&#20266;&#25513;&#30721;&#21644;&#21512;&#25104;&#22270;&#20687;&#20849;&#21516;&#26500;&#25104;&#20102;&#35757;&#32451;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Bayesian causal discovery&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25512;&#26029;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;DAG&#27491;&#21017;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13917</link><description>&lt;p&gt;
BayesDAG&#65306;&#22522;&#20110;&#26799;&#24230;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#21518;&#39564;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery. (arXiv:2307.13917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Bayesian causal discovery&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25512;&#26029;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;DAG&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26088;&#22312;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#37327;&#21270;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#21644;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#31354;&#38388;&#30340;&#32852;&#21512;&#25512;&#29702;&#32780;&#24102;&#26469;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;DAG&#19978;&#30340;&#39640;&#25928;&#21518;&#39564;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20165;&#38480;&#20110;&#23545;&#32447;&#24615;&#22240;&#26524;&#27169;&#22411;&#30340;&#33410;&#28857;&#25490;&#21015;&#30697;&#38453;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#23548;&#33268;&#25512;&#26029;&#20934;&#30830;&#24615;&#21463;&#25439;&#65292;&#35201;&#20040;&#26159;&#22312;&#21463;DAG&#27491;&#21017;&#21270;&#32422;&#26463;&#30340;&#37051;&#25509;&#30697;&#38453;&#19978;&#36827;&#34892;&#36830;&#32493;&#26494;&#24347;&#65292;&#32780;&#19981;&#33021;&#30830;&#20445;&#24471;&#21040;&#30340;&#22270;&#26159;DAGs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;SG-MCMC&#65289;&#30340;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20174;&#21518;&#39564;&#20013;&#37319;&#26679;DAG&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;DAG&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#36824;&#32472;&#21046;&#20989;&#25968;&#21442;&#25968;&#26679;&#26412;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#38543;&#26426;&#28857;&#31215;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#38750;&#20984;&#20248;&#21270;&#25216;&#26415;&#25913;&#36827;&#20102;&#22312;&#35266;&#23519;&#22270;&#20013;&#20272;&#35745;&#33410;&#28857;&#28508;&#22312;&#21521;&#37327;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#38454;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#35299;&#20915;&#23884;&#20837;&#38382;&#39064;&#65292;&#24182;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#23454;&#29992;&#32593;&#32476;&#23884;&#20837;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.13818</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#38543;&#26426;&#28857;&#31215;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Gradient-Based Spectral Embeddings of Random Dot Product Graphs. (arXiv:2307.13818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#38543;&#26426;&#28857;&#31215;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#38750;&#20984;&#20248;&#21270;&#25216;&#26415;&#25913;&#36827;&#20102;&#22312;&#35266;&#23519;&#22270;&#20013;&#20272;&#35745;&#33410;&#28857;&#28508;&#22312;&#21521;&#37327;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#38454;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#35299;&#20915;&#23884;&#20837;&#38382;&#39064;&#65292;&#24182;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#23454;&#29992;&#32593;&#32476;&#23884;&#20837;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#28857;&#31215;&#22270;&#35889;&#65288;RDPG&#65289;&#26159;&#19968;&#20010;&#20851;&#31995;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#33410;&#28857;&#36890;&#36807;&#22312;&#20302;&#32500;&#27431;&#27663;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#34920;&#31034;&#12290;RDPG&#20851;&#38190;&#22320;&#20551;&#35774;&#36793;&#30340;&#24418;&#25104;&#27010;&#29575;&#30001;&#30456;&#24212;&#30340;&#28508;&#22312;&#20301;&#32622;&#30340;&#28857;&#31215;&#32473;&#20986;&#12290;&#22240;&#27492;&#65292;&#20174;&#35266;&#23519;&#21040;&#30340;&#22270;&#20013;&#20272;&#35745;&#36825;&#20123;&#21521;&#37327;&#30340;&#23884;&#20837;&#20219;&#21153;&#36890;&#24120;&#34987;&#35774;&#23450;&#20026;&#19968;&#20010;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#12290;&#32463;&#20856;&#30340;&#37051;&#25509;&#35889;&#23884;&#20837;&#65288;ASE&#65289;&#20855;&#26377;&#21487;&#38752;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#20294;&#23427;&#22312;&#24418;&#24335;&#19978;&#35299;&#20915;&#30340;&#26159;&#19968;&#20010;&#20195;&#29702;&#38382;&#39064;&#65292;&#24182;&#19988;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#38750;&#20984;&#20248;&#21270;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#23545;RDPG&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;&#19968;&#38454;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#35299;&#20915;&#23884;&#20837;&#38382;&#39064;&#65292;&#24182;&#33258;&#28982;&#22320;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#23454;&#29992;&#32593;&#32476;&#23884;&#20837;&#24212;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;RDPG&#23884;&#20837;&#26377;&#21521;&#22270;&#22833;&#21435;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#38500;&#38750;...
&lt;/p&gt;
&lt;p&gt;
The Random Dot Product Graph (RDPG) is a generative model for relational data, where nodes are represented via latent vectors in low-dimensional Euclidean space. RDPGs crucially postulate that edge formation probabilities are given by the dot product of the corresponding latent positions. Accordingly, the embedding task of estimating these vectors from an observed graph is typically posed as a low-rank matrix factorization problem. The workhorse Adjacency Spectral Embedding (ASE) enjoys solid statistical properties, but it is formally solving a surrogate problem and can be computationally intensive. In this paper, we bring to bear recent advances in non-convex optimization and demonstrate their impact to RDPG inference. We advocate first-order gradient descent methods to better solve the embedding problem, and to organically accommodate broader network embedding applications of practical relevance. Notably, we argue that RDPG embeddings of directed graphs loose interpretability unless 
&lt;/p&gt;</description></item><item><title>ECSIC&#26159;&#19968;&#31181;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24038;&#21491;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#36827;&#34892;&#32852;&#21512;&#21387;&#32553;&#65292;&#24182;&#20351;&#29992;&#26032;&#39062;&#30340;&#31435;&#20307;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#31435;&#20307;&#19978;&#19979;&#25991;&#27169;&#22359;&#23454;&#29616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ECSIC&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#31435;&#20307;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#32534;&#30721;&#21644;&#35299;&#30721;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10284</link><description>&lt;p&gt;
ECSIC: &#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#30340;&#26497;&#32447;&#20132;&#21449;&#27880;&#24847;&#21147;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ECSIC: Epipolar Cross Attention for Stereo Image Compression. (arXiv:2307.10284v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10284
&lt;/p&gt;
&lt;p&gt;
ECSIC&#26159;&#19968;&#31181;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24038;&#21491;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#36827;&#34892;&#32852;&#21512;&#21387;&#32553;&#65292;&#24182;&#20351;&#29992;&#26032;&#39062;&#30340;&#31435;&#20307;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#31435;&#20307;&#19978;&#19979;&#25991;&#27169;&#22359;&#23454;&#29616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ECSIC&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#31435;&#20307;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#32534;&#30721;&#21644;&#35299;&#30721;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26041;&#27861;ECSIC&#65292;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#31435;&#20307;&#22270;&#20687;&#23545;&#24038;&#21491;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31435;&#20307;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;SCA&#65289;&#27169;&#22359;&#21644;&#20004;&#20010;&#31435;&#20307;&#19978;&#19979;&#25991;&#27169;&#22359;&#65292;&#20197;&#32852;&#21512;&#26041;&#24335;&#21387;&#32553;&#24038;&#21491;&#22270;&#20687;&#12290;SCA&#27169;&#22359;&#22312;&#20004;&#20010;&#22270;&#20687;&#30340;&#23545;&#24212;&#26497;&#32447;&#33539;&#22260;&#20869;&#36827;&#34892;&#20132;&#21449;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#24182;&#19988;&#24182;&#34892;&#22788;&#29702;&#23427;&#20204;&#12290;&#31435;&#20307;&#19978;&#19979;&#25991;&#27169;&#22359;&#36890;&#36807;&#20351;&#29992;&#31532;&#19968;&#20010;&#22270;&#20687;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#25913;&#21892;&#23545;&#31532;&#20108;&#20010;&#32534;&#30721;&#22270;&#20687;&#30340;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21076;&#38500;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#27604;&#36739;&#12290;ECSIC&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;&#31435;&#20307;&#22270;&#20687;&#25968;&#25454;&#38598;Cityscapes&#21644;InStereo2k&#19978;&#36798;&#21040;&#20102;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#24555;&#36895;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#38750;&#24120;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present ECSIC, a novel learned method for stereo image compression. Our proposed method compresses the left and right images in a joint manner by exploiting the mutual information between the images of the stereo image pair using a novel stereo cross attention (SCA) module and two stereo context modules. The SCA module performs cross-attention restricted to the corresponding epipolar lines of the two images and processes them in parallel. The stereo context modules improve the entropy estimation of the second encoded image by using the first image as a context. We conduct an extensive ablation study demonstrating the effectiveness of the proposed modules and a comprehensive quantitative and qualitative comparison with existing methods. ECSIC achieves state-of-the-art performance among stereo image compression models on the two popular stereo image datasets Cityscapes and InStereo2k while allowing for fast encoding and decoding, making it highly practical for real-time
&lt;/p&gt;</description></item><item><title>FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2307.04684</link><description>&lt;p&gt;
FreeDrag: &#28857;&#36861;&#36394;&#24182;&#19981;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing. (arXiv:2307.04684v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04684
&lt;/p&gt;
&lt;p&gt;
FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#22270;&#20687;&#32534;&#36753;&#30340;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#38656;&#27714;&#65292;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#31934;&#30830;&#21644;&#28789;&#27963;&#30340;&#25805;&#32437;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#65292;DragGAN&#36890;&#36807;&#22522;&#20110;&#28857;&#30340;&#25805;&#32437;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;DragGAN&#22312;&#28857;&#30340;&#36861;&#36394;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#21253;&#25324;&#38169;&#35823;&#36861;&#36394;&#21644;&#27169;&#31946;&#36861;&#36394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FreeDrag&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;DragGAN&#20013;&#28857;&#36861;&#36394;&#30340;&#36127;&#25285;&#12290;FreeDrag&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;DragGAN&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#22256;&#38590;&#24773;&#26223;&#19979;&#23454;&#29616;&#31283;&#23450;&#30340;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable. Recently, DragGAN has achieved impressive editing results through point-based manipulation. However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points. To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN. The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing. Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar st
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#22240;&#26524;&#25512;&#35770;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312;&#32473;&#23450;&#22240;&#26524;&#25490;&#24207;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#24674;&#22797;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#21487;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05415</link><description>&lt;p&gt;
&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#65306;&#20174;&#29702;&#35770;&#21040;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Causal normalizing flows: from theory to practice. (arXiv:2306.05415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#22240;&#26524;&#25512;&#35770;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312;&#32473;&#23450;&#22240;&#26524;&#25490;&#24207;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#24674;&#22797;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#21487;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#22240;&#26524;&#25512;&#35770;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#38750;&#32447;&#24615;ICA&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#26174;&#31034;&#20986;&#22312;&#32473;&#23450;&#22240;&#26524;&#25490;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#37492;&#21035;&#20986;&#26469;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#33258;&#22238;&#24402;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#24674;&#22797;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#25429;&#25417;&#28508;&#22312;&#22240;&#26524;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#21516;&#35774;&#35745;&#21644;&#23398;&#20064;&#36873;&#25321;&#30340;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#22312;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#20013;&#23454;&#29616;do-operator&#65292;&#20174;&#32780;&#22238;&#31572;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#36873;&#25321;&#65307;&#23558;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#19982;&#20854;&#20182;&#36924;&#36817;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65307;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#21487;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;&#25968;&#25454;&#21644;&#22240;&#26524;&#22270;&#37096;&#20998;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#20195;&#30721;&#21487;&#20197;&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we deepen on the use of normalizing flows for causal reasoning. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows (NFs). Second, we analyze different design and learning choices for causal normalizing flows to capture the underlying causal data-generating process. Third, we describe how to implement the do-operator in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems, where the presence of mixed discrete-continuous data and partial knowledge on the causal graph is the norm. The code for this work can be f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G$^2$uardFL&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23646;&#24615;&#21270;&#23458;&#25143;&#31471;&#22270;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#21363;&#20351;&#24694;&#24847;&#23458;&#25143;&#31471;&#25968;&#37327;&#39640;&#36798;50&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.04984</link><description>&lt;p&gt;
G$^2$uardFL: &#36890;&#36807;&#23646;&#24615;&#21270;&#23458;&#25143;&#31471;&#22270;&#32858;&#31867;&#26469;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering. (arXiv:2306.04984v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G$^2$uardFL&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23646;&#24615;&#21270;&#23458;&#25143;&#31471;&#22270;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#21363;&#20351;&#24694;&#24847;&#23458;&#25143;&#31471;&#25968;&#37327;&#39640;&#36798;50&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21327;&#21516;&#33539;&#24335;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#23458;&#25143;&#31471;&#33021;&#22815;&#36827;&#34892;&#38598;&#20307;&#27169;&#22411;&#35757;&#32451;&#32780;&#19981;&#20132;&#25442;&#21508;&#33258;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;FL&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#20250;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#26435;&#37325;&#27880;&#20837;&#26377;&#27602;&#25968;&#25454;&#65292;&#20174;&#32780;&#24471;&#21040;&#38024;&#23545;&#29305;&#23450;&#26679;&#26412;&#30340;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#23545;&#31574;&#20027;&#35201;&#22522;&#20110;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#30001;&#20110;&#37327;&#21270;&#23458;&#25143;&#27169;&#22411;&#30456;&#20284;&#24615;&#30340;&#19981;&#36275;&#65292;&#36825;&#20123;&#23545;&#31574;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#25298;&#32477;&#21512;&#27861;&#26435;&#37325;&#65292;&#21516;&#26102;&#25509;&#21463;&#24694;&#24847;&#26435;&#37325;&#12290;&#20854;&#20182;&#38450;&#24481;&#26426;&#21046;&#20165;&#22312;&#38754;&#23545;&#23569;&#37327;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#20363;&#22914;&#23569;&#20110;10&#65285;&#30340;&#24694;&#24847;&#23458;&#25143;&#31471;&#26102;&#25165;&#26377;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G$^2$uardFL&#65292;&#36825;&#26159;&#19968;&#20010;&#20445;&#25252;&#26694;&#26550;&#65292;&#23427;&#23558;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#31471;&#35270;&#20026;&#19968;&#20010;&#23646;&#24615;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#20174;&#32780;&#20445;&#25252;FL&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#23458;&#25143;&#31471;&#22270;&#32858;&#31867;&#25216;&#26415;&#65292;&#26681;&#25454;&#27169;&#22411;&#26435;&#37325;&#30340;&#30456;&#20284;&#24615;&#23558;&#23458;&#25143;&#31471;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24694;&#24847;&#12290;&#36890;&#36807;&#37319;&#29992;&#23545;&#23458;&#25143;&#31471;&#22266;&#26377;&#23646;&#24615;&#36827;&#34892;&#32534;&#30721;&#30340;&#23646;&#24615;&#26631;&#31614;&#65292;G$^2$uardFL&#22312;&#35782;&#21035;&#21463;&#25439;&#23458;&#25143;&#31471;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#32780;&#19981;&#25490;&#38500;&#21512;&#27861;&#23458;&#25143;&#31471;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;50&#65285;&#30340;&#23458;&#25143;&#31471;&#26159;&#24694;&#24847;&#30340;&#65292;G$^2$uardFL&#20063;&#33021;&#26174;&#33879;&#38477;&#20302;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a collaborative paradigm, Federated Learning (FL) empowers clients to engage in collective model training without exchanging their respective local data. Nevertheless, FL remains vulnerable to backdoor attacks in which an attacker compromises malicious clients, and injects poisoned model weights into the aggregation process to yield attacker-chosen predictions for particular samples. Existing countermeasures, mainly based on anomaly detection, may erroneously reject legitimate weights while accepting malicious ones, which is due to inadequacies in quantifying client model similarities. Other defense mechanisms prove effective exclusively when confronted with a restricted number of malicious clients, e.g., less than 10%. To address these vulnerabilities, we present G$^2$uardFL, a protective framework that reframes the detection of malicious clients as an attributed graph clustering problem, thereby safeguarding FL systems. This framework employs a client graph clustering technique to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#65292;&#22312;&#21512;&#25104;MOVi&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#27169;&#22411;&#26159;&#39318;&#20010;&#33021;&#22815;&#25193;&#23637;&#21040;&#26080;&#32422;&#26463;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04829</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#27979;&#26102;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities. (arXiv:2306.04829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#65292;&#22312;&#21512;&#25104;MOVi&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#27169;&#22411;&#26159;&#39318;&#20010;&#33021;&#22815;&#25193;&#23637;&#21040;&#26080;&#32422;&#26463;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26159;&#20174;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#35270;&#39057;&#38598;&#21512;&#20013;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#22312;&#21463;&#38480;&#39046;&#22495;&#20869;&#32553;&#25918;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#30340;&#37325;&#24314;&#20250;&#23548;&#33268;&#22312;&#19981;&#21463;&#32422;&#26463;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#24418;&#24335;&#20026;&#26102;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#12290;&#35813;&#25439;&#22833;&#32534;&#30721;&#22270;&#20687;&#22359;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#33258;&#28982;&#22320;&#24341;&#20837;&#36816;&#21160;&#20559;&#24046;&#26469;&#21457;&#29616;&#29289;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#25439;&#22833;&#23548;&#33268;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;MOVi&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#24403;&#19982;&#29305;&#24449;&#37325;&#24314;&#25439;&#22833;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#39318;&#20010;&#33021;&#22815;&#25193;&#23637;&#21040;&#26080;&#32422;&#26463;&#35270;&#39057;&#25968;&#25454;&#38598;&#65288;&#22914;YouTube-VIS&#65289;&#30340;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised video-based object-centric learning is a promising avenue to learn structured representations from large, unlabeled video collections, but previous approaches have only managed to scale to real-world datasets in restricted domains. Recently, it was shown that the reconstruction of pre-trained self-supervised features leads to object-centric representations on unconstrained real-world image datasets. Building on this approach, we propose a novel way to use such pre-trained features in the form of a temporal feature similarity loss. This loss encodes temporal correlations between image patches and is a natural way to introduce a motion bias for object discovery. We demonstrate that this loss leads to state-of-the-art performance on the challenging synthetic MOVi datasets. When used in combination with the feature reconstruction loss, our model is the first object-centric video model that scales to unconstrained video datasets such as YouTube-VIS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#19968;&#31181;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#36827;&#34892;&#20102;&#33033;&#20914;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03623</link><description>&lt;p&gt;
&#32463;&#20856;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Spike-based computation using classical recurrent neural networks. (arXiv:2306.03623v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#19968;&#31181;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#36827;&#34892;&#20102;&#33033;&#20914;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36890;&#20449;&#20165;&#30001;&#20107;&#20214;&#25110;&#25152;&#35859;&#30340;&#33033;&#20914;&#32452;&#25104;&#12290;&#36825;&#31181;&#29305;&#24615;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36827;&#34892;&#24322;&#27493;&#21644;&#31232;&#30095;&#35745;&#31639;&#65292;&#24182;&#22240;&#27492;&#22312;&#19987;&#29992;&#30828;&#20214;&#19978;&#36816;&#34892;&#26102;&#22823;&#24133;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#37319;&#29992;&#19968;&#31181;&#23545;&#31216;&#30340;&#26041;&#27861;&#65306;&#20462;&#25913;&#19968;&#31181;&#24050;&#30693;&#30340;&#12289;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#12290;&#36890;&#36807;&#26126;&#30830;&#24341;&#20837;&#33033;&#20914;&#38408;&#20540;&#21644;&#37325;&#32622;&#26426;&#21046;&#65292;&#25105;&#20204;&#20351;&#32593;&#32476;&#33021;&#22815;&#20165;&#20351;&#29992;&#33033;&#20914;&#26469;&#25191;&#34892;&#21069;&#21521;&#21644;&#24490;&#29615;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20462;&#25913;&#21518;&#30340;&#26500;&#26550;&#26082;&#21487;&#20197;&#23454;&#29616;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;ImageNet&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks are a type of artificial neural networks in which communication between neurons is only made of events, also called spikes. This property allows neural networks to make asynchronous and sparse computations and therefore to drastically decrease energy consumption when run on specialized hardware. However, training such networks is known to be difficult, mainly due to the non-differentiability of the spike activation, which prevents the use of classical backpropagation. This is because state-of-the-art spiking neural networks are usually derived from biologically-inspired neuron models, to which are applied machine learning methods for training. Nowadays, research about spiking neural networks focuses on the design of training algorithms whose goal is to obtain networks that compete with their non-spiking version on specific tasks. In this paper, we attempt the symmetrical approach: we modify the dynamics of a well-known, easily trainable type of recurrent neural 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#30340;&#24341;&#20837;&#65292;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#29289;&#20307;&#24212;&#26144;&#23556;&#21040;&#19981;&#21516;&#27133;&#20301;&#30340;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02204</link><description>&lt;p&gt;
&#24490;&#29615;&#19968;&#33268;&#24615;&#39537;&#21160;&#30340;&#29289;&#20307;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cycle Consistency Driven Object Discovery. (arXiv:2306.02204v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#30340;&#24341;&#20837;&#65292;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#29289;&#20307;&#24212;&#26144;&#23556;&#21040;&#19981;&#21516;&#27133;&#20301;&#30340;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#26550;&#26500;&#20808;&#39564;&#25110;&#36741;&#21161;&#20449;&#24687;&#65288;&#20363;&#22914;&#28145;&#24230;&#22270;&#25110;&#27969;&#22330;&#22270;&#65289;&#26469;&#25506;&#32034;&#22522;&#20110;&#27133;&#20301;&#30340;&#26041;&#27861;&#65292;&#20197;&#34920;&#31034;&#23545;&#35937;&#20026;&#31216;&#20026;&#8220;&#27133;&#20301;&#8221;&#25110;&#8220;&#23545;&#35937;&#25991;&#20214;&#8221;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#65292;&#20174;&#32780;&#20419;&#36827;&#29289;&#20307;&#21457;&#29616;&#12290; &#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#26550;&#26500;&#20808;&#39564;&#20250;&#24341;&#20837;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#25165;&#33021;&#35782;&#21035;&#27491;&#30830;&#30340;&#23545;&#35937;&#12290; &#21516;&#26679;&#65292;&#20381;&#36182;&#36741;&#21161;&#20449;&#24687;&#30340;&#26041;&#27861;&#20063;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#36825;&#31181;&#20449;&#24687;&#36890;&#24120;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#23545;&#35937;&#24212;&#26144;&#23556;&#21040;&#19968;&#20010;&#19981;&#21516;&#27133;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#24418;&#24335;&#21270;&#36825;&#20010;&#32422;&#26463;&#65292;&#31216;&#20043;&#20026;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290; &#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#29289;&#20307;&#21457;&#29616;&#21644;&#23569;&#26679;&#26412;&#29289;&#20307;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches have explored slot-based methods utilizing architectural priors or auxiliary information such as depth maps or flow maps to facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. However, reliance on architectural priors introduces unreliability and requires meticulous engineering to identify the correct objects. Likewise, methods relying on auxiliary information are suboptimal as such information is often unavailable for most natural scenes. To address these limitations, we propose a method that explicitly optimizes the constraint that each object in a scene should be mapped to a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. We refer to them as the \textit{cycle-consistency} objectives. By applying these con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>J-UNIWARD &#26159;&#19968;&#31181;&#23558;&#31192;&#23494;&#20449;&#24687;&#38544;&#34255;&#22312;JPEG&#22270;&#20687;&#20013;&#30340;&#38544;&#20889;&#26041;&#27861;&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#20854;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010; off-by-one &#38169;&#35823;&#65292;&#20351;&#19968;&#20123;&#22270;&#20687;&#22359;&#34987;&#39640;&#20272;&#65292;&#21478;&#19968;&#20123;&#34987;&#20302;&#20272;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#29992;&#20110;&#26816;&#27979;&#27492;&#31181;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.19776</link><description>&lt;p&gt;
J-UNIWARD&#20013;&#30340;&#19968;&#20010;&#23454;&#29616;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Off-By-One Implementation Error in J-UNIWARD. (arXiv:2305.19776v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19776
&lt;/p&gt;
&lt;p&gt;
J-UNIWARD &#26159;&#19968;&#31181;&#23558;&#31192;&#23494;&#20449;&#24687;&#38544;&#34255;&#22312;JPEG&#22270;&#20687;&#20013;&#30340;&#38544;&#20889;&#26041;&#27861;&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#20854;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010; off-by-one &#38169;&#35823;&#65292;&#20351;&#19968;&#20123;&#22270;&#20687;&#22359;&#34987;&#39640;&#20272;&#65292;&#21478;&#19968;&#20123;&#34987;&#20302;&#20272;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#29992;&#20110;&#26816;&#27979;&#27492;&#31181;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
J-UNIWARD&#26159;&#19968;&#31181;&#23558;&#31192;&#23494;&#20449;&#24687;&#38544;&#34255;&#22312;JPEG&#30422;&#26495;&#22270;&#20687;&#20013;&#30340;&#27969;&#34892;&#38544;&#20889;&#26415;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;J-UNIWARD&#26088;&#22312;&#23884;&#20837;&#21040;&#32441;&#29702;&#22270;&#20687;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#30340;&#21464;&#21270;&#38590;&#20197;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;J-UNIWARD&#39318;&#20808;&#20026;&#27599;&#20010;DCT&#31995;&#25968;&#20998;&#37197;&#19968;&#20010;&#23884;&#20837;&#25104;&#26412;&#65292;&#35813;&#25104;&#26412;&#22522;&#20110;&#22270;&#20687;&#30340;&#23567;&#27874;&#27531;&#24046;&#35745;&#31639;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#32534;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23884;&#20837;&#25152;&#38656;&#30340;&#26377;&#25928;&#36733;&#33655;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;&#26356;&#25913;&#19968;&#20010;DCT&#31995;&#25968;&#20250;&#24433;&#21709;23x23&#20010;&#23567;&#27874;&#31995;&#25968;&#31383;&#21475;&#12290;&#20026;&#20102;&#21152;&#36895;&#25104;&#26412;&#22270;&#30340;&#35745;&#31639;&#65292;&#21407;&#22987;&#23454;&#29616;&#39044;&#20808;&#35745;&#31639;&#23567;&#27874;&#27531;&#24046;&#65292;&#28982;&#21518;&#23545;&#20110;&#27599;&#20010;&#26356;&#25913;&#30340;DCT&#31995;&#25968;&#65292;&#32771;&#34385;&#19968;&#20010;23x23&#30340;&#23567;&#27874;&#27531;&#24046;&#31383;&#21475;&#12290;&#28982;&#32780;&#65292;&#35813;&#23454;&#29616;&#38169;&#35823;&#22320;&#23558;&#31383;&#21475;&#20559;&#31227;&#20102;&#19968;&#20010;&#20687;&#32032;&#21040;&#21491;&#19979;&#26041;&#12290;&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20010;off-by-one&#38169;&#35823;&#23545;&#29983;&#25104;&#30340;&#25104;&#26412;&#22270;&#30340;&#24433;&#21709;&#12290;&#19968;&#20123;&#22270;&#20687;&#22359;&#34987;&#39640;&#20272;&#65292;&#32780;&#20854;&#20182;&#22270;&#20687;&#22359;&#21017;&#34987;&#20302;&#20272;&#65292;&#20294;&#24046;&#24322;&#30456;&#23545;&#36739;&#23567;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35828;&#26126;&#22914;&#20309;&#26816;&#27979;&#20351;&#29992;&#24102;&#26377;&#20559;&#31227;&#38169;&#35823;&#30340;J-UNIWARD&#38544;&#34255;&#30340;&#38544;&#20889;&#26415;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
J-UNIWARD is a popular steganography method for hiding secret messages in JPEG cover images. As a content-adaptive method, J-UNIWARD aims to embed into textured image regions where changes are difficult to detect. To this end, J-UNIWARD first assigns to each DCT coefficient an embedding cost calculated based on the image's Wavelet residual, and then uses a coding method that minimizes the cost while embedding the desired payload. Changing one DCT coefficient affects a 23x23 window of Wavelet coefficients. To speed up the costmap computation, the original implementation pre-computes the Wavelet residual and then considers per changed DCT coefficient a 23x23 window of the Wavelet residual. However, the implementation accesses a window accidentally shifted by one pixel to the bottom right. In this report, we evaluate the effect of this off-by-one error on the resulting costmaps. Some image blocks are over-priced while other image blocks are under-priced, but the difference is relatively s
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20248;&#21270;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#21542;&#33021;&#22312;&#21463;&#38480;&#30340;&#39044;&#27979;&#22120;&#26063;&#20013;&#24471;&#21040;&#26657;&#20934;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#23616;&#37096;&#26368;&#20248;&#26465;&#20214;&#21462;&#20195;&#20840;&#23616;&#26368;&#20248;&#24615;&#26465;&#20214;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.18764</link><description>&lt;p&gt;
&#20248;&#21270;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#21542;&#33021;&#24471;&#21040;&#26657;&#20934;&#30340;&#39044;&#27979;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Does Optimizing a Proper Loss Yield Calibration?. (arXiv:2305.18764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18764
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20248;&#21270;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#21542;&#33021;&#22312;&#21463;&#38480;&#30340;&#39044;&#27979;&#22120;&#26063;&#20013;&#24471;&#21040;&#26657;&#20934;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#23616;&#37096;&#26368;&#20248;&#26465;&#20214;&#21462;&#20195;&#20840;&#23616;&#26368;&#20248;&#24615;&#26465;&#20214;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#34987;&#24191;&#27867;&#35748;&#20026;&#20250;&#24471;&#21040;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#29305;&#24615;&#30340;&#39044;&#27979;&#22120;&#65292;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#26679;&#30340;&#25439;&#22833;&#65292;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#39044;&#27979;&#30495;&#23454;&#27010;&#29575;&#65292;&#36825;&#30830;&#23454;&#26159;&#26657;&#20934;&#30340;&#12290;&#20294;&#26159;&#65292;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#35757;&#32451;&#26469;&#36817;&#20284;&#22320;&#26368;&#23567;&#21270;&#22312;&#21463;&#38480;&#21046;&#30340;&#39044;&#27979;&#22120;&#26063;&#20013;&#30340;&#25439;&#22833;&#65292;&#36825;&#20123;&#39044;&#27979;&#22120;&#26063;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#30495;&#23454;&#30340;&#27010;&#29575;&#12290;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#21463;&#38480;&#21046;&#30340;&#39044;&#27979;&#22120;&#26063;&#20013;&#36866;&#24403;&#30340;&#25439;&#22833;&#21487;&#20197;&#24471;&#21040;&#26657;&#20934;&#30340;&#27169;&#22411;&#65311;&#23427;&#25552;&#20379;&#20102;&#20160;&#20040;&#31934;&#30830;&#30340;&#26657;&#20934;&#20445;&#35777;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#20005;&#26684;&#31572;&#26696;&#12290;&#25105;&#20204;&#29992;&#23616;&#37096;&#26368;&#20248;&#26465;&#20214;&#26367;&#25442;&#20840;&#23616;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#35268;&#23450;&#20102;&#39044;&#27979;&#22120;&#65288;&#36866;&#24403;&#30340;&#65289;&#25439;&#22833;&#19981;&#33021;&#36890;&#36807;&#20351;&#29992;&#19968;&#23450;&#26063;&#32676;&#30340;Lipschitz&#20989;&#25968;&#21518;&#22788;&#29702;&#20854;&#39044;&#27979;&#32780;&#38477;&#20302;&#22826;&#22810;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#36825;&#31181;&#23616;&#37096;&#26368;&#20248;&#24615;&#36136;&#30340;&#20219;&#20309;&#39044;&#27979;&#22120;&#37117;&#28385;&#36275;Kakade-Foster(2008)&#12289;B&#322;asiok&#31561;&#20154;(2023)&#20013;&#23450;&#20041;&#30340;&#24179;&#31283;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing proper loss functions is popularly believed to yield predictors with good calibration properties; the intuition being that for such losses, the global optimum is to predict the ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are trained to approximately minimize loss over restricted families of predictors, that are unlikely to contain the ground truth. Under what circumstances does optimizing proper loss over a restricted family yield calibrated models? What precise calibration guarantees does it give? In this work, we provide a rigorous answer to these questions. We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions. We show that any predictor with this local optimality satisfies smooth calibration as defined in Kakade-Foster (2008), B{\l}asiok et al. (2023). L
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#25439;&#22833;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#65292;&#20197;&#25552;&#20379;&#20844;&#24179;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09424</link><description>&lt;p&gt;
&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26657;&#20934;&#21487;&#26368;&#23567;&#21270;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Loss minimization yields multicalibration for large neural networks. (arXiv:2304.09424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#25439;&#22833;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#65292;&#20197;&#25552;&#20379;&#20844;&#24179;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26657;&#20934;&#26159;&#19968;&#31181;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#20379;&#36328;&#22823;&#37327;&#22242;&#20307;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#20989;&#25968;&#65292;&#22810;&#26657;&#20934;&#20063;&#34987;&#35748;&#20026;&#26159;&#19982;&#26368;&#23567;&#21270;&#25439;&#22833;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#65288;&#20960;&#20046;&#25152;&#26377;&#30340;&#65289;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#24179;&#26041;&#35823;&#24046;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#26041;&#38754;&#65292;&#32780;&#19981;&#26159;&#20851;&#20110;&#31639;&#27861;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#32771;&#34385;&#12290;&#20197;&#21069;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#20960;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#39044;&#27979;&#22120;&#65292;&#22240;&#27492;&#26159;&#34920;&#24449;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#36866;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#31639;&#27861;&#65292;&#22914; SGD&#65292;&#24182;&#19988;&#19981;&#24212;&#35299;&#37322;&#20026;&#8220;&#20844;&#24179;&#24615;&#20174;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33719;&#24471;&#20813;&#36153;&#30340;&#22909;&#22788;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multicalibration is a notion of fairness that aims to provide accurate predictions across a large set of groups. Multicalibration is known to be a different goal than loss minimization, even for simple predictors such as linear functions. In this note, we show that for (almost all) large neural network sizes, optimally minimizing squared error leads to multicalibration. Our results are about representational aspects of neural networks, and not about algorithmic or sample complexity considerations. Previous such results were known only for predictors that were nearly Bayes-optimal and were therefore representation independent. We emphasize that our results do not apply to specific algorithms for optimizing neural networks, such as SGD, and they should not be interpreted as "fairness comes for free from optimizing neural networks".
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32806;&#21512;&#22810;&#21464;&#37327;&#26144;&#23556;&#38382;&#39064;&#30340;&#38590;&#28857;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.02304</link><description>&lt;p&gt;
&#38024;&#23545;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Coupled Multiwavelet Neural Operator Learning for Coupled Partial Differential Equations. (arXiv:2303.02304v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32806;&#21512;&#22810;&#21464;&#37327;&#26144;&#23556;&#38382;&#39064;&#30340;&#38590;&#28857;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#25551;&#36848;&#35768;&#22810;&#29289;&#29702;&#36807;&#31243;&#22797;&#26434;&#21160;&#24577;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#23637;&#31034;&#20986;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;/&#23567;&#27874;&#31354;&#38388;&#30452;&#25509;&#23398;&#20064;&#31215;&#20998;&#26680;&#26469;&#35299;&#20915;PDE&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#32806;&#21512;PDE&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#38590;&#28857;&#22312;&#20110;&#22788;&#29702;&#20989;&#25968;&#20043;&#38388;&#30340;&#32806;&#21512;&#26144;&#23556;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#65288;CMWNO&#65289;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#23567;&#27874;&#31354;&#38388;&#20013;&#36827;&#34892;&#22810;&#23567;&#27874;&#20998;&#35299;&#21644;&#37325;&#26500;&#36807;&#31243;&#20013;&#35299;&#32806;&#21512;&#31215;&#20998;&#26680;&#12290;&#22312;&#35299;&#20915;Gray-Scott&#65288;GS&#65289;&#26041;&#31243;&#21644;&#38750;&#23616;&#37096;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#31561;&#32806;&#21512;PDE&#26041;&#38754;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#20808;&#21069;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;$L^2$&#35823;&#24046;&#34920;&#29616;&#20986;&#20102;$2\times \sim 4\times$&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coupled partial differential equations (PDEs) are key tasks in modeling the complex dynamics of many physical processes. Recently, neural operators have shown the ability to solve PDEs by learning the integral kernel directly in Fourier/Wavelet space, so the difficulty for solving the coupled PDEs depends on dealing with the coupled mappings between the functions. Towards this end, we propose a \textit{coupled multiwavelets neural operator} (CMWNO) learning scheme by decoupling the coupled integral kernels during the multiwavelet decomposition and reconstruction procedures in the Wavelet space. The proposed model achieves significantly higher accuracy compared to previous learning-based solvers in solving the coupled PDEs including Gray-Scott (GS) equations and the non-local mean field game (MFG) problem. According to our experimental results, the proposed model exhibits a $2\times \sim 4\times$ improvement relative $L$2 error compared to the best results from the state-of-the-art mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#22797;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26368;&#21518;&#20960;&#23618;&#36827;&#34892;&#27010;&#29575;&#21270;&#22788;&#29702;&#65292;&#37327;&#21270;&#21644;&#32435;&#20837;&#19981;&#30830;&#23450;&#24615;&#24182;&#26377;&#21161;&#20110;&#20915;&#23450;&#35745;&#31639;&#39044;&#31639;&#30340;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2302.06359</link><description>&lt;p&gt;
&#20462;&#22797;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fixing Overconfidence in Dynamic Neural Networks. (arXiv:2302.06359v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#22797;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26368;&#21518;&#20960;&#23618;&#36827;&#34892;&#27010;&#29575;&#21270;&#22788;&#29702;&#65292;&#37327;&#21270;&#21644;&#32435;&#20837;&#19981;&#30830;&#23450;&#24615;&#24182;&#26377;&#21161;&#20110;&#20915;&#23450;&#35745;&#31639;&#39044;&#31639;&#30340;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26681;&#25454;&#36755;&#20837;&#38590;&#24230;&#21160;&#24577;&#35843;&#25972;&#35745;&#31639;&#20195;&#20215;&#65292;&#25215;&#35834;&#32531;&#35299;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;uncertainty estimates&#30340;&#36136;&#37327;&#36739;&#24046;&#65292;&#24456;&#38590;&#21306;&#20998;hard&#21644;easy&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#21518;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#35745;&#31639;&#26377;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#26368;&#21518;&#20960;&#23618;&#36827;&#34892;&#27010;&#29575;&#21270;&#22788;&#29702;&#65292;&#20805;&#20998;&#37327;&#21270;&#21644;&#32435;&#20837;aleatoric&#21644;epistemic uncertainty&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#30830;&#23450;&#35745;&#31639;&#39044;&#31639;&#26102;&#26377;&#21161;&#20110;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;CIFAR-100&#12289;ImageNet&#21644;Caltech-256&#26041;&#38754;&#23637;&#31034;&#20102;&#20934;&#30830;&#24615;&#12289;&#25429;&#33719;&#19981;&#30830;&#23450;&#24615;&#21644;&#26657;&#20934;&#35823;&#24046;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic neural networks are a recent technique that promises a remedy for the increasing size of modern deep learning models by dynamically adapting their computational cost to the difficulty of the inputs. In this way, the model can adjust to a limited computational budget. However, the poor quality of uncertainty estimates in deep learning models makes it difficult to distinguish between hard and easy samples. To address this challenge, we present a computationally efficient approach for post-hoc uncertainty quantification in dynamic neural networks. We show that adequately quantifying and accounting for both aleatoric and epistemic uncertainty through a probabilistic treatment of the last layers improves the predictive performance and aids decision-making when determining the computational budget. In the experiments, we show improvements on CIFAR-100, ImageNet, and Caltech-256 in terms of accuracy, capturing uncertainty, and calibration error.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#24182;&#20419;&#36827;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01842</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#20998;&#23618;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars. (arXiv:2211.01842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#24182;&#20419;&#36827;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21333;&#30340;&#26500;&#24314;&#22359;&#20013;&#21457;&#29616;&#31070;&#32463;&#32467;&#26500;&#26159;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27493;&#39588;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#24120;&#20165;&#25628;&#32034;&#19968;&#20123;&#38480;&#23450;&#26041;&#38754;&#30340;&#26550;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#32479;&#19968;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#33258;&#28982;&#32780;&#32039;&#20945;&#22320;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#27604;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#31354;&#38388;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36890;&#36807;&#22686;&#24378;&#21644;&#21033;&#29992;&#23427;&#20204;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#24182;&#20419;&#36827;&#20102;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#23618;&#26680;&#35774;&#35745;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#39640;&#25928;&#25628;&#32034;&#22914;&#27492;&#24222;&#22823;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#25628;&#32034;&#31574;&#30053;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#30340;&#32467;&#26500;&#21270;&#33258;&#36866;&#24212;&#28145;&#24230;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#20540;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#24314;&#27169;&#12290;&#24212;&#29992;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#21487;&#20197;&#24471;&#21040;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20855;&#26377;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#29305;&#24449;&#25353;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#24207;&#12290;&#22312;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25130;&#26029;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#34920;&#31034;&#38271;&#24230;&#27604;&#39046;&#20808;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30701;16&#20493;&#65292;&#21516;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12637</link><description>&lt;p&gt;
&#31070;&#32463;&#29305;&#24449;&#21521;&#37327;&#26159;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Eigenfunctions Are Structured Representation Learners. (arXiv:2210.12637v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#30340;&#32467;&#26500;&#21270;&#33258;&#36866;&#24212;&#28145;&#24230;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#20540;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#24314;&#27169;&#12290;&#24212;&#29992;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#21487;&#20197;&#24471;&#21040;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20855;&#26377;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#29305;&#24449;&#25353;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#24207;&#12290;&#22312;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25130;&#26029;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#34920;&#31034;&#38271;&#24230;&#27604;&#39046;&#20808;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30701;16&#20493;&#65292;&#21516;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#30340;&#32467;&#26500;&#21270;&#33258;&#36866;&#24212;&#28145;&#24230;&#34920;&#31034;&#12290;&#19982;&#20808;&#21069;&#30340;&#35889;&#26041;&#27861;&#65288;&#22914;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#65289;&#20197;&#38750;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25805;&#20316;&#19981;&#21516;&#65292;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#20540;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#29305;&#24449;&#20540;&#20989;&#25968;&#26469;&#33258;&#20110;&#25968;&#25454;&#25193;&#22686;&#35774;&#32622;&#20013;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#26102;&#65292;&#24212;&#29992;&#31070;&#32463;&#29305;&#24449;&#26144;&#23556;&#20250;&#20135;&#29983;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#29305;&#24449;&#25353;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#20013;&#28436;&#31034;&#20102;&#20351;&#29992;&#36825;&#26679;&#30340;&#33258;&#36866;&#24212;&#38271;&#24230;&#32534;&#30721;&#26469;&#34920;&#31034;&#12290;&#36890;&#36807;&#26681;&#25454;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25130;&#26029;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#34920;&#31034;&#38271;&#24230;&#27604;&#39046;&#20808;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30701;16&#20493;&#65292;&#21516;&#26102;&#36798;&#21040;&#30456;&#20284;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#24418;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a structured, adaptive-length deep representation called Neural Eigenmap. Unlike prior spectral methods such as Laplacian Eigenmap that operate in a nonparametric manner, Neural Eigenmap leverages NeuralEF to parametrically model eigenfunctions using a neural network. We show that, when the eigenfunction is derived from positive relations in a data augmentation setup, applying NeuralEF results in an objective function that resembles those of popular self-supervised learning methods, with an additional symmetry-breaking property that leads to structured representations where features are ordered by importance. We demonstrate using such representations as adaptive-length codes in image retrieval systems. By truncation according to feature importance, our method requires up to $16\times$ shorter representation length than leading self-supervised learning ones to achieve similar retrieval performance. We further apply our method to graph data and report strong results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#36890;&#36807;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#21644;&#22788;&#29702;&#25928;&#24212;&#65292;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.07898</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#21327;&#21516;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#36890;&#36807;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#21644;&#22788;&#29702;&#25928;&#24212;&#65292;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#20934;&#23454;&#39564;&#65288;DC-QE&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#39318;&#20808;&#65292;&#26412;&#22320;&#21508;&#26041;&#20174;&#31169;&#26377;&#25968;&#25454;&#20013;&#26500;&#24314;&#38477;&#32500;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#20182;&#20204;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#21518;&#65292;&#20174;&#20849;&#20139;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#12290;&#26368;&#21518;&#65292;&#20174;&#20542;&#21521;&#20998;&#25968;&#20013;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#38543;&#26426;&#35823;&#24046;&#21644;&#20559;&#24046;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#20943;&#23569;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#38543;&#26426;&#35823;&#24046;&#12290;&#36890;&#36807;&#22312;&#20154;&#24037;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#27604;&#21333;&#29420;&#20998;&#26512;&#26356;&#22909;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of technologies for causal inference with the privacy preservation of distributed data has attracted considerable attention in recent years. To address this issue, we propose a data collaboration quasi-experiment (DC-QE) that enables causal inference from distributed data with privacy preservation. In our method, first, local parties construct dimensionality-reduced intermediate representations from the private data. Second, they share intermediate representations, instead of private data for privacy preservation. Third, propensity scores were estimated from the shared intermediate representations. Finally, the treatment effects were estimated from propensity scores. Our method can reduce both random errors and biases, whereas existing methods can only reduce random errors in the estimation of treatment effects. Through numerical experiments on both artificial and real-world data, we confirmed that our method can lead to better estimation results than individual analyse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;MLE&#20844;&#24335;&#24182;&#20174;&#22810;&#20010;&#39057;&#29575;&#30340;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#30456;&#23545;&#30456;&#20301;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#30456;&#20301;&#21516;&#27493;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.12276</link><description>&lt;p&gt;
&#22810;&#39057;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#30456;&#20301;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Multi-Frequency Joint Community Detection and Phase Synchronization. (arXiv:2206.12276v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;MLE&#20844;&#24335;&#24182;&#20174;&#22810;&#20010;&#39057;&#29575;&#30340;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#30456;&#23545;&#30456;&#20301;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#30456;&#20301;&#21516;&#27493;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two simple and efficient algorithms that leverage the MLE formulation and benefit from the information across multiple frequencies to solve the joint community detection and phase synchronization problem on the stochastic block model with relative phase.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30456;&#23545;&#30456;&#20301;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#30456;&#20301;&#21516;&#27493;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#19982;&#19968;&#20010;&#26410;&#30693;&#30340;&#30456;&#20301;&#35282;&#30456;&#20851;&#32852;&#12290;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#65292;&#26088;&#22312;&#21516;&#26102;&#24674;&#22797;&#31751;&#32467;&#26500;&#21644;&#30456;&#20851;&#30340;&#30456;&#20301;&#35282;&#12290;&#25105;&#20204;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;&#20854;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#20844;&#24335;&#65292;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#21576;&#29616;&#20986;&#8220;&#22810;&#39057;&#8221;&#32467;&#26500;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#24182;&#38750;&#28304;&#20110;&#36825;&#20010;&#35282;&#24230;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;MLE&#20844;&#24335;&#24182;&#20174;&#22810;&#20010;&#39057;&#29575;&#30340;&#20449;&#24687;&#20013;&#21463;&#30410;&#12290;&#21069;&#32773;&#26159;&#22522;&#20110;&#26032;&#39062;&#30340;&#22810;&#39057;&#21015;&#20027;&#20803;QR&#20998;&#35299;&#30340;&#35889;&#26041;&#27861;&#12290;&#24212;&#29992;&#20110;&#35266;&#27979;&#30697;&#38453;&#30340;&#21069;&#20960;&#20010;&#29305;&#24449;&#21521;&#37327;&#30340;&#20998;&#35299;&#25552;&#20379;&#20102;&#26377;&#20851;&#31751;&#32467;&#26500;&#21644;&#30456;&#20851;&#30456;&#20301;&#35282;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#36845;&#20195;&#30340;&#22810;&#39057;&#29575;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the joint community detection and phase synchronization problem on the stochastic block model with relative phase, where each node is associated with an unknown phase angle. This problem, with a variety of real-world applications, aims to recover the cluster structure and associated phase angles simultaneously. We show this problem exhibits a ``multi-frequency'' structure by closely examining its maximum likelihood estimation (MLE) formulation, whereas existing methods are not originated from this perspective. To this end, two simple yet efficient algorithms that leverage the MLE formulation and benefit from the information across multiple frequencies are proposed. The former is a spectral method based on the novel multi-frequency column-pivoted QR factorization. The factorization applied to the top eigenvectors of the observation matrix provides key information about the cluster structure and associated phase angles. The second approach is an iterative multi-frequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#24182;&#21462;&#24471;&#20102; ROAM &#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#21453;&#24212;&#29575;&#35758;&#35770;&#12290;</title><link>http://arxiv.org/abs/2108.11328</link><description>&lt;p&gt;
&#29992;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#21152;&#24615;&#27169;&#22411;&#21644;&#32467;&#26500;&#20132;&#20114;&#39044;&#27979;&#20154;&#21475;&#26222;&#26597;&#35843;&#26597;&#21453;&#24212;&#29575;
&lt;/p&gt;
&lt;p&gt;
Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions. (arXiv:2108.11328v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#24182;&#21462;&#24471;&#20102; ROAM &#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#21453;&#24212;&#29575;&#35758;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#19968;&#31995;&#21015;&#28789;&#27963;&#19988;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#27169;&#22411;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#33879;&#21517;&#30340; ROAM &#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#35813;&#24212;&#29992;&#20351;&#29992;&#22312;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#35268;&#21010;&#25968;&#25454;&#24211;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#35782;&#21035;&#38590;&#20197;&#35843;&#26597;&#30340;&#21306;&#22495;&#12290;&#21313;&#24180;&#21069;&#32452;&#32455;&#30340;&#19968;&#22330;&#20247;&#21253;&#31454;&#36187;&#34920;&#26126;&#65292;&#22522;&#20110;&#22238;&#24402;&#26641;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#30456;&#24212;&#30340;&#27169;&#22411;&#19981;&#33021;&#29992;&#20110;&#25311;&#23450;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992; $\ell_0$-based &#24809;&#32602;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#23569;&#25968;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#24378;&#23618;&#27425;&#20132;&#20114;&#21512;&#24182;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#22312;Github &#19978;&#24320;&#28304;&#65289;&#20801;&#35768;&#25105;&#20204;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#35843;&#26597;&#21453;&#24212;&#29575;&#30340;&#21487;&#34892;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312; ROAM &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#25913;&#36827;&#35843;&#26597;&#21453;&#24212;&#29575;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the problem of predicting survey response rates using a family of flexible and interpretable nonparametric models. The study is motivated by the US Census Bureau's well-known ROAM application which uses a linear regression model trained on the US Census Planning Database data to identify hard-to-survey areas. A crowdsourcing competition organized around ten years ago revealed that machine learning methods based on ensembles of regression trees led to the best performance in predicting survey response rates; however, the corresponding models could not be adopted for the intended application due to their black-box nature. We consider nonparametric additive models with small number of main and pairwise interaction effects using $\ell_0$-based penalization. From a methodological viewpoint, we study both computational and statistical aspects of our estimator; and discuss variants that incorporate strong hierarchical interactions. Our algorithms (opensourced on gith
&lt;/p&gt;</description></item></channel></rss>