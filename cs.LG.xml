<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#20010;&#20307;&#25968;&#25454;&#32780;&#19981;&#26159;&#25972;&#20307;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20110;HuMob Challenge&#31454;&#36187;&#20013;&#12290;&#37319;&#29992;&#20102;&#29305;&#24449;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;SVR&#65292;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#21644;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#39044;&#27979;&#20934;&#30830;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12900</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#21450;&#20854;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Personalized human mobility prediction for HuMob challenge. (arXiv:2310.12900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12900
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#20010;&#20307;&#25968;&#25454;&#32780;&#19981;&#26159;&#25972;&#20307;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20110;HuMob Challenge&#31454;&#36187;&#20013;&#12290;&#37319;&#29992;&#20102;&#29305;&#24449;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;SVR&#65292;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#21644;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#39044;&#27979;&#20934;&#30830;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21019;&#24314;&#25552;&#20132;&#32473;HuMob Challenge&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#25968;&#25454;&#20998;&#26512;&#31454;&#36187;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#22522;&#20110;&#20010;&#20307;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#20854;&#36816;&#21160;&#36712;&#36857;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#25972;&#20307;&#36816;&#21160;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#20154;&#31867;&#36816;&#21160;&#23545;&#20110;&#27599;&#20010;&#20154;&#32780;&#35328;&#26159;&#29420;&#29305;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#24449;&#65292;&#22914;&#26085;&#26399;&#21644;&#26102;&#38388;&#65292;&#27963;&#21160;&#26102;&#38388;&#65292;&#21608;&#20960;&#65292;&#19968;&#22825;&#20013;&#30340;&#26102;&#38388;&#21644;POI&#65288;&#20852;&#36259;&#28857;&#65289;&#35775;&#38382;&#39057;&#29575;&#31561;&#12290;&#20316;&#20026;&#39069;&#22806;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#26469;&#34701;&#21512;&#20855;&#26377;&#30456;&#20284;&#34892;&#20026;&#27169;&#24335;&#30340;&#20854;&#20182;&#20010;&#20307;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#36827;&#34892;&#20934;&#30830;&#24615;&#26816;&#39564;&#65292;&#24182;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#12290;&#23613;&#31649;&#24635;&#20307;&#25968;&#25454;&#38598;&#21253;&#21547;10&#19975;&#21517;&#29992;&#25143;&#30340;&#36712;&#36857;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#20102;2&#19975;&#21517;&#30446;&#26631;&#29992;&#25143;&#30340;&#25968;&#25454;&#65292;&#24182;&#19981;&#38656;&#35201;&#20351;&#29992;&#20854;&#20182;8&#19975;&#21517;&#29992;&#25143;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explain the methodology used to create the data submitted to HuMob Challenge, a data analysis competition for human mobility prediction. We adopted a personalized model to predict the individual's movement trajectory from their data, instead of predicting from the overall movement, based on the hypothesis that human movement is unique to each person. We devised the features such as the date and time, activity time, days of the week, time of day, and frequency of visits to POI (Point of Interest). As additional features, we incorporated the movement of other individuals with similar behavior patterns through the employment of clustering. The machine learning model we adopted was the Support Vector Regression (SVR). We performed accuracy through offline assessment and carried out feature selection and parameter tuning. Although overall dataset provided consists of 100,000 users trajectory, our method use only 20,000 target users data, and do not need to use other 80,000 data. Despite 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12688</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#21387;&#32553;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compression of Recurrent Neural Networks using Matrix Factorization. (arXiv:2310.12688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#26102;&#25110;&#23884;&#20837;&#24335;&#24212;&#29992;&#20013;&#37096;&#32626;&#27169;&#22411;&#26102;&#65292;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20351;&#29992;&#20302;&#31209;&#36817;&#20284;&#23545;&#27169;&#22411;&#30340;&#30697;&#38453;&#36827;&#34892;&#20998;&#35299;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#34429;&#28982;&#22312;&#35757;&#32451;&#20043;&#21069;&#21487;&#20197;&#35774;&#32622;&#31209;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26082;&#19981;&#28789;&#27963;&#20063;&#19981;&#26368;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#30697;&#38453;&#36873;&#25321;&#19981;&#21516;&#30340;&#31209;&#12290;&#32467;&#21512;&#35757;&#32451;&#36866;&#24212;&#24615;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#25110;&#32773;&#26377;&#24456;&#23569;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#22312;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#33267;&#26368;&#22810;14&#20493;&#65292;&#19988;&#30456;&#23545;&#24615;&#33021;&#38477;&#20302;&#26368;&#22810;&#20026;1.4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressing neural networks is a key step when deploying models for real-time or embedded applications. Factorizing the model's matrices using low-rank approximations is a promising method for achieving compression. While it is possible to set the rank before training, this approach is neither flexible nor optimal. In this work, we propose a post-training rank-selection method called Rank-Tuning that selects a different rank for each matrix. Used in combination with training adaptations, our method achieves high compression rates with no or little performance degradation. Our numerical experiments on signal processing tasks show that we can compress recurrent neural networks up to 14x with at most 1.4% relative performance reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F2GNN&#30340;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#22686;&#24378;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.12350</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#32467;&#26500;&#24863;&#30693;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F2GNN&#30340;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#22686;&#24378;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#22270;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#20219;&#21153;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#30417;&#31649;&#38480;&#21046;&#65292;&#23545;&#38598;&#20013;&#24335;&#22270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#36235;&#21183;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GNN&#21487;&#33021;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#21382;&#21490;&#20559;&#35265;&#24182;&#23548;&#33268;&#27495;&#35270;&#24615;&#39044;&#27979;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#23616;&#37096;&#27169;&#22411;&#30340;&#20559;&#35265;&#24456;&#23481;&#26131;&#20256;&#25773;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#32473;&#22312;&#32852;&#37030;GNN&#20013;&#20943;&#36731;&#20559;&#35265;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;F2GNN&#65292;&#19968;&#31181;&#22686;&#24378;&#32852;&#37030;GNN&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#20559;&#35265;&#21487;&#33021;&#26469;&#33258;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;F2GNN&#26088;&#22312;&#22312;&#32852;&#37030;&#29615;&#22659;&#19979;&#20943;&#23569;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a tra
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;MDP&#20013;&#23398;&#20064;&#969;-regular&#30446;&#26631;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.12248</link><description>&lt;p&gt;
MDP&#20013;LTL&#21644;&#969;-regular&#30446;&#26631;&#30340;PAC&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;MDP&#20013;&#23398;&#20064;&#969;-regular&#30446;&#26631;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#65288;LTL&#65289;&#21644;&#969;-regular&#30446;&#26631;&#26159;&#36817;&#26399;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#36798;&#38750;&#39532;&#23572;&#21487;&#22827;&#30446;&#26631;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;MDP&#20013;&#30340;&#969;-regular&#30446;&#26631;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#31995;&#32479;&#30340;&#37319;&#26679;&#36712;&#36857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#26088;&#22312;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#20855;&#20307;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.12069</link><description>&lt;p&gt;
&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#30340;Transformer&#65306;&#22825;&#25991;&#23398;&#23478;&#30340;&#25945;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers for scientific data: a pedagogical review for astronomers. (arXiv:2310.12069v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#26088;&#22312;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#20855;&#20307;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;ChatGPT&#21644;&#30456;&#20851;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30456;&#20851;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#34987;&#31216;&#20026;Transformer&#12290;&#26368;&#21021;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;Transformer&#21644;&#23427;&#20204;&#21033;&#29992;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25945;&#23398;&#21644;&#38750;&#27491;&#24335;&#32508;&#36848;&#30340;&#30446;&#26631;&#26159;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#12290;&#25105;&#20204;&#30340;&#25945;&#23398;&#21644;&#38750;&#27491;&#24335;&#32508;&#36848;&#21253;&#25324;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23545;&#21407;&#22987;Transformer&#26550;&#26500;&#30340;&#25551;&#36848;&#65292;&#20197;&#21450;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#19968;&#33410;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#65292;&#20379;&#37027;&#20123;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24863;&#20852;&#36259;&#24182;&#24076;&#26395;&#24320;&#22987;&#20351;&#29992;Transformer&#36827;&#34892;&#30740;&#31350;&#30340;&#35835;&#32773;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. Our pedagogical and informal review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include with a Frequently Asked Questions section for readers who are curious about generative AI and interested in getting started with transformers for their research problem.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11971</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#20307;&#19981;&#21464;&#24615;&#23398;&#20064;&#25552;&#39640;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;AI&#21161;&#25163;&#30340;&#25104;&#21151;&#22312;&#20110;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;, &#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#26356;&#21152;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;. &#20316;&#20026;&#36890;&#29992;AI&#21161;&#25163;, &#20154;&#20204;&#36234;&#26469;&#36234;&#26399;&#26395;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#34920;&#29616;&#19968;&#33268;. &#28982;&#32780;, &#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;,&#24378;&#21270;&#23398;&#20064;(RL)&#32463;&#24120;&#21033;&#29992;&#25463;&#24452;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#22870;&#21169;, &#24573;&#30053;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;. &#36825;&#31181;&#23545;&#24555;&#36895;&#22870;&#21169;&#25910;&#30410;&#30340;&#20851;&#27880;&#19981;&#20165;&#21066;&#24369;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;, &#20063;&#21066;&#24369;&#20102;&#27169;&#22411;&#23545;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;. &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;, &#21487;&#20197;&#36890;&#36807;RL&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#31574;&#30053;. &#37492;&#20110;&#33719;&#24471;&#32676;&#20307;&#26631;&#27880;&#30340;&#25361;&#25112;, &#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#33258;&#21160;&#23558;&#25968;&#25454;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;, &#26377;&#24847;&#22320;&#26368;&#22823;&#21270;&#24615;&#33021;&#24046;&#24322;. &#28982;&#21518;, &#25105;&#20204;&#20248;&#21270;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#20013;&#34920;&#29616;&#33391;&#22909;. &#26368;&#21518;, &#21033;&#29992;&#24050;&#24314;&#31435;&#30340;
&lt;/p&gt;
&lt;p&gt;
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11762</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11762
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20219;&#21153;&#20013;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#33410;&#28857;&#30340;&#65292;&#21363;&#20351;&#33410;&#28857;&#23884;&#20837;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#30001;&#20110;&#22270;&#32467;&#26500;&#30340;&#23384;&#22312;&#32780;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#65288;QW&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20511;&#21161;&#20110;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#26368;&#20248;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#23548;GNN&#30340;&#26032;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#8220;&#20934;&#29926;&#29380;&#26031;&#22374;&#8221;&#36317;&#31163;&#65292;&#29992;&#20110;&#35266;&#27979;&#21040;&#30340;&#22810;&#32500;&#33410;&#28857;&#26631;&#31614;&#21644;&#23427;&#20204;&#30340;&#20272;&#35745;&#20043;&#38388;&#65292;&#36890;&#36807;&#20248;&#21270;&#22312;&#22270;&#36793;&#19978;&#23450;&#20041;&#30340;&#26631;&#31614;&#20256;&#36755;&#12290;&#36825;&#20123;&#20272;&#35745;&#26159;&#30001;&#19968;&#20010;GNN&#21442;&#25968;&#21270;&#30340;&#65292;&#20854;&#20013;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#30830;&#23450;&#22270;&#36793;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#26631;&#31614;&#20256;&#36755;&#30340;&#20005;&#26684;&#32422;&#26463;&#37325;&#26032;&#34920;&#36798;&#20026;&#22522;&#20110;Bregman&#25955;&#24230;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25152;&#25552;&#20986;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#65292;&#20851;&#32852;&#20004;&#20010;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#23398;&#20064;GNN&#20197;&#21450;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11569</link><description>&lt;p&gt;
&#24403;&#21018;&#24615;&#25104;&#20026;&#38382;&#39064;&#65306;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2310.11569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#20307;&#65292;&#20854;&#30446;&#26631;&#26159;&#23545;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#26410;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#20063;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#20013;&#26045;&#21152;&#23618;&#27425;&#20851;&#31995;&#65292;&#20294;&#26410;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20063;&#40664;&#35748;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#23618;&#27425;&#20851;&#31995;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#26410;&#36866;&#24212;&#26174;&#31034;&#20986;&#20559;&#31163;&#27492;&#20551;&#35774;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHiT&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23545;&#25972;&#20010;&#23618;&#27425;&#30340;&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;PROFHiT&#20351;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulariz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.11466</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#19977;&#32500;&#22270;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#31283;&#20581;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction. (arXiv:2310.11466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#29289;&#23398;&#20219;&#21153;&#65288;&#22914;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#21644;&#20122;&#32454;&#32990;&#23450;&#20301;&#20272;&#35745;&#65289;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#23454;&#39564;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;AlphaFold2&#65289;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20570;&#27861;&#65292;&#21363;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#20934;&#30830;&#39044;&#27979;&#30340;&#32467;&#26500;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#26126;&#26174;&#19979;&#38477;&#12290;&#34429;&#28982;&#31867;&#20284;&#29616;&#35937;&#24050;&#32463;&#22312;&#19968;&#33324;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#39044;&#27979;&#30340;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.11009</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#36335;&#39044;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#32463;&#20856;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#21551;&#21457;&#24335;&#24230;&#37327;&#34987;&#36873;&#25321;&#20026;&#22312;&#19982;&#38142;&#36335;&#24418;&#25104;&#30456;&#20851;&#30340;&#22522;&#26412;&#22240;&#32032;&#19978;&#19982;&#20043;&#30456;&#20851;&#33391;&#22909;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#19968;&#31867;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#20248;&#21183;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#20197;&#21450;&#25429;&#25417;&#20505;&#36873;&#38142;&#36335;&#20013;&#33410;&#28857;&#20043;&#38388;&#20851;&#31995;&#30340;&#8220;&#23545;&#21521;&#32534;&#30721;&#8221;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#23427;&#20204;&#24050;&#32463;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#21521;&#32534;&#30721;&#24448;&#24448;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#22522;&#26412;&#22240;&#32032;&#26469;&#20998;&#31867;&#25152;&#26377;&#38142;&#36335;&#12290;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#22914;&#20309;&#27491;&#30830;&#20998;&#31867;&#21487;&#33021;&#30001;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#21508;&#31181;&#19981;&#21516;&#38142;&#36335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
&lt;/p&gt;</description></item><item><title>ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10692</link><description>&lt;p&gt;
ACES: &#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#30340;&#32534;&#31243;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10692
&lt;/p&gt;
&lt;p&gt;
ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#21644;&#36873;&#25321;&#26032;&#39062;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#22909;&#22855;&#24515;&#12289;&#31185;&#23398;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#22312;Python&#32534;&#31243;&#38590;&#39064;&#30340;&#26080;&#38480;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26088;&#22312;&#24314;&#27169;&#21442;&#32771;&#20998;&#24067;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;&#20854;&#20182;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#25163;&#24037;&#32534;&#30721;&#34920;&#31034;&#31354;&#38388;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#30830;&#20248;&#21270;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#23884;&#20837;&#31354;&#38388;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#26377;&#36259;&#21464;&#21270;&#30340;&#24863;&#30693;&#19981;&#31526;&#12290;&#36890;&#36807;ACES&#65288;&#33258;&#25105;&#30446;&#26631;&#20195;&#30721;&#25506;&#32034;&#19982;&#35821;&#20041;&#25551;&#36848;&#31526;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#30452;&#25509;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;&#27599;&#20010;&#38590;&#39064;&#37117;&#26631;&#35760;&#26377;10&#20010;&#32500;&#24230;&#65292;&#27599;&#20010;&#32500;&#24230;&#25429;&#25417;&#20102;&#35299;&#20915;&#23427;&#25152;&#38656;&#30340;&#32534;&#31243;&#25216;&#33021;&#12290;ACES&#29983;&#25104;&#24182;&#36861;&#27714;&#26032;&#39062;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.10537</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#25193;&#23637;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Microscaling Data Formats for Deep Learning. (arXiv:2310.10537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31364;&#20301;&#23485;&#25968;&#25454;&#26684;&#24335;&#23545;&#20110;&#38477;&#20302;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23558;&#27599;&#20010;&#22359;&#30340;&#32553;&#25918;&#22240;&#23376;&#19982;&#31364;&#28014;&#28857;&#21644;&#25972;&#25968;&#31867;&#22411;&#30456;&#32467;&#21512;&#30340;&#24494;&#25193;&#23637;&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#65292;&#20197;&#28385;&#36275;&#30828;&#20214;&#25928;&#29575;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#25705;&#25830;&#20043;&#38388;&#30340;&#31454;&#20105;&#38656;&#27714;&#12290;&#23545;&#20110;AI&#25512;&#29702;&#21644;&#35757;&#32451;&#65292;MX&#25968;&#25454;&#26684;&#24335;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#26102;&#29992;&#25143;&#25705;&#25830;&#23567;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21644;&#26080;&#38656;&#20462;&#25913;&#35757;&#32451;&#37197;&#26041;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#35757;&#32451;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#23567;&#20110;8&#20301;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#28176;&#21464;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
&lt;/p&gt;</description></item><item><title>Rank-DETR&#26159;&#19968;&#31181;&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25490;&#21517;&#23548;&#21521;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.08854</link><description>&lt;p&gt;
&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#30340;Rank-DETR&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08854
&lt;/p&gt;
&lt;p&gt;
Rank-DETR&#26159;&#19968;&#31181;&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25490;&#21517;&#23548;&#21521;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26816;&#27979;&#21464;&#25442;&#22120;&#65288;DETR&#65289;&#20351;&#29992;&#19968;&#32452;&#23545;&#35937;&#26597;&#35810;&#26469;&#39044;&#27979;&#36793;&#30028;&#26694;&#21015;&#34920;&#65292;&#36890;&#36807;&#23558;&#20854;&#20998;&#31867;&#32622;&#20449;&#24230;&#24471;&#20998;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#36873;&#25321;&#25490;&#21517;&#38752;&#21069;&#30340;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#26368;&#32456;&#26816;&#27979;&#32467;&#26524;&#12290;&#24615;&#33021;&#21331;&#36234;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#38656;&#35201;&#23545;&#36793;&#30028;&#26694;&#39044;&#27979;&#36827;&#34892;&#20934;&#30830;&#30340;&#25490;&#24207;&#12290;&#23545;&#20110;&#22522;&#20110;DETR&#30340;&#26816;&#27979;&#22120;&#65292;&#25490;&#21517;&#38752;&#21069;&#30340;&#36793;&#30028;&#26694;&#30001;&#20110;&#20998;&#31867;&#24471;&#20998;&#19982;&#23450;&#20301;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#32780;&#23548;&#33268;&#23450;&#20301;&#36136;&#37327;&#36739;&#24046;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#39640;&#36136;&#37327;&#26816;&#27979;&#22120;&#30340;&#26500;&#24314;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#38754;&#21521;&#25490;&#21517;&#30340;&#35774;&#35745;&#65292;&#20849;&#21516;&#31216;&#20026;Rank-DETR&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#20010;&#38754;&#21521;&#25490;&#21517;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#20419;&#36827;&#27491;&#38754;&#39044;&#27979;&#24182;&#25233;&#21046;&#36127;&#38754;&#39044;&#27979;&#65292;&#20197;&#30830;&#20445;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#19968;&#20010;&#38754;&#21521;&#25490;&#21517;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#21305;&#37197;&#25104;&#26412;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25910;&#25947;&#36895;&#24230;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.08237</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32479;&#19968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25910;&#25947;&#36895;&#24230;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21327;&#21464;&#37327;&#28418;&#31227;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#21363;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#36755;&#20837;&#20998;&#24067;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#20851;&#27880;&#20110;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#27809;&#26377;&#22312;&#29702;&#35770;&#19978;&#21644;&#25968;&#20540;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#23646;&#20110;&#19968;&#20010;&#20016;&#23500;&#30340;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#30340;&#19968;&#33324;&#25439;&#22833;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22914;&#22343;&#20540;&#22238;&#24402;&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#12289;&#22522;&#20110;&#20284;&#28982;&#30340;&#20998;&#31867;&#21644;&#22522;&#20110;&#36793;&#32536;&#30340;&#20998;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31867;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#24314;&#31435;&#20102;&#23574;&#38160;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35813;&#32467;&#26524;&#19982;&#25991;&#29486;&#20013;&#30340;&#26368;&#20248;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate shift occurs prevalently in practice, where the input distributions of the source and target data are substantially different. Despite its practical importance in various learning problems, most of the existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature wher
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#30740;&#31350;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#31561;&#26041;&#27861;&#19982;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07940</link><description>&lt;p&gt;
&#25104;&#26412;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines. (arXiv:2310.07940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#30740;&#31350;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#31561;&#26041;&#27861;&#19982;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23459;&#25196;&#30528;&#30001;&#29289;&#32852;&#32593;&#35774;&#22791;&#65288;&#21253;&#25324;&#26234;&#33021;&#20256;&#24863;&#22120;&#65292;&#23478;&#23621;&#21644;&#22478;&#24066;&#65289;&#25512;&#21160;&#30340;&#26410;&#26469;&#24895;&#26223;&#12290;&#36234;&#26469;&#36234;&#22810;&#22320;&#65292;&#23558;&#26234;&#33021;&#23884;&#20837;&#36825;&#20123;&#35774;&#22791;&#20013;&#28041;&#21450;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23384;&#20648;&#21644;&#22788;&#29702;&#38656;&#27714;&#20351;&#23427;&#20204;&#23545;&#20110;&#24265;&#20215;&#30340;&#29616;&#25104;&#24179;&#21488;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#20811;&#26381;&#36825;&#20123;&#35201;&#27714;&#23545;&#20110;&#23454;&#29616;&#24191;&#27867;&#36866;&#29992;&#30340;&#26234;&#33021;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#20351;&#27169;&#22411;&#21464;&#24471;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#29305;&#23450;&#22330;&#26223;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#32570;&#20047;&#29702;&#35299;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#23545;&#20110;&#36793;&#32536;&#24179;&#21488;&#65292;&#36825;&#20123;&#36873;&#25321;&#19981;&#33021;&#19982;&#25104;&#26412;&#21644;&#29992;&#25143;&#20307;&#39564;&#30456;&#21106;&#31163;&#22320;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#35282;&#24230;&#20840;&#38754;&#25506;&#32034;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#19982;&#23384;&#20648;&#12289;&#20256;&#24863;&#22120;&#21644;&#22788;&#29702;&#22120;&#31561;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#30340;&#35282;&#24230;&#36827;&#34892;&#65292;&#32771;&#34385;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and d
&lt;/p&gt;</description></item><item><title>KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.07488</link><description>&lt;p&gt;
KwaiYiiMath: &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07488
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KwaiYiiMath&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22686;&#24378;&#20102;KwaiYiiBase1&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20013;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#38598;&#65288;&#21629;&#21517;&#20026;KMath&#65289;&#65292;&#21253;&#21547;188&#20010;&#20363;&#23376;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;KwaiYiiMath&#22312;GSM8k&#12289;CMath&#21644;KMath&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#30637;&#35299;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#20915;&#26041;&#26696;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2310.03178</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Digital Ethics in Federated Learning. (arXiv:2310.03178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#30637;&#35299;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#20915;&#26041;&#26696;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#24341;&#21457;&#20102;&#23545;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#28389;&#29992;&#38480;&#21046;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20849;&#20139;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#26159;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#20419;&#36827;&#20102;&#22810;&#26041;&#20043;&#38388;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#24182;&#22240;&#20854;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#23398;&#20064;&#25928;&#29575;&#25552;&#21319;&#26041;&#38754;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;FL&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65292;FL&#38754;&#20020;&#30528;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#35282;&#24230;&#65292;&#20197;&#21450;&#38598;&#20013;&#24335;&#21644;&#20998;&#25955;&#24335;FL&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;FL&#20013;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Things (IoT) consistently generates vast amounts of data, sparking increasing concern over the protection of data privacy and the limitation of data misuse. Federated learning (FL) facilitates collaborative capabilities among multiple parties by sharing machine learning (ML) model parameters instead of raw user data, and it has recently gained significant attention for its potential in privacy preservation and learning efficiency enhancement. In this paper, we highlight the digital ethics concerns that arise when human-centric devices serve as clients in FL. More specifically, challenges of game dynamics, fairness, incentive, and continuity arise in FL due to differences in perspectives and objectives between clients and the server. We analyze these challenges and their solutions from the perspectives of both the client and the server, and through the viewpoints of centralized and decentralized FL. Finally, we explore the opportunities in FL for human-centric IoT as dir
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02227</link><description>&lt;p&gt;
SNIP: &#29992;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#36830;&#25509;&#25968;&#23398;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training. (arXiv:2310.02227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02227
&lt;/p&gt;
&lt;p&gt;
SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26080;&#27861;&#32570;&#23569;&#31526;&#21495;&#25968;&#23398;&#26041;&#31243;&#26469;&#24314;&#27169;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#30340;&#26102;&#20195;&#65292;&#31185;&#23398;&#25506;&#31350;&#24448;&#24448;&#28041;&#21450;&#21040;&#25910;&#38598;&#35266;&#23519;&#25968;&#25454;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#27934;&#23519;&#21147;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#29305;&#21270;&#20110;&#25968;&#20540;&#39046;&#22495;&#25110;&#31526;&#21495;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#20026;&#29305;&#23450;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#30417;&#30563;&#24335;&#35757;&#32451;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#31526;&#21495;&#26041;&#31243;&#21644;&#20854;&#25968;&#20540;&#23545;&#24212;&#29289;&#20043;&#38388;&#21487;&#33021;&#20135;&#29983;&#30340;&#37325;&#22823;&#22909;&#22788;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SNIP&#65292;&#19968;&#31181;&#31526;&#21495;-&#25968;&#20540;&#38598;&#25104;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#23884;&#20837;&#20013;&#30340;&#30456;&#20114;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#36827;&#34892;&#28508;&#31354;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SNIP&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic unified understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic 
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2310.01225</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#20195;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65306;&#24433;&#21709;&#12289;&#21069;&#26223;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33021;&#22815;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#36866;&#29992;&#20110;&#26368;&#24191;&#27867;&#30340;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24674;&#22797;&#25110;&#36229;&#36234;&#24050;&#30693;&#30340;&#27492;&#31867;&#33539;&#25968;&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#36335;&#24452;&#33539;&#25968;&#36824;&#20139;&#26377;&#36335;&#24452;&#33539;&#25968;&#30340;&#24120;&#35268;&#20248;&#28857;&#65306;&#35745;&#31639;&#31616;&#20415;&#12289;&#23545;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#27604;&#25805;&#20316;&#31526;&#33539;&#25968;&#30340;&#20056;&#31215;&#65288;&#21478;&#19968;&#31181;&#24120;&#29992;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#38160;&#24230;&#12290;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26131;&#20110;&#23454;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#22312;ImageNet&#19978;&#23545;ResNet&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#26469;&#25361;&#25112;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10194</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
The Kernel Density Integral Transformation. (arXiv:2309.10194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#20110;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#32487;&#32493;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32508;&#21512;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#26041;&#27861;&#20316;&#20026;&#26497;&#38480;&#24773;&#20917;&#65306;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#19981;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#21487;&#20197;&#20316;&#20026;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#35843;&#25972;&#19968;&#20010;&#36830;&#32493;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#32463;&#24120;&#20248;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#26680;&#23494;&#24230;&#36716;&#25442;&#21487;&#20197;&#26377;&#30410;&#22320;&#24212;&#29992;&#20110;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#20851;&#24615;&#20998;&#26512;&#21644;&#21333;&#21464;&#37327;&#32858;&#31867;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#23436;&#25104;&#24341;&#25806;&#26469;&#36827;&#19968;&#27493;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.00608</link><description>&lt;p&gt;
Copiloting the Copilots: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23436;&#25104;&#24341;&#25806;&#34701;&#21512;&#29992;&#20110;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair. (arXiv:2309.00608v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00608
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#23436;&#25104;&#24341;&#25806;&#26469;&#36827;&#19968;&#27493;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#65292;&#23545;&#20110;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#23454;&#38469;&#31995;&#32479;&#21512;&#25104;&#27491;&#30830;&#30340;&#20462;&#34917;&#31243;&#24207;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#24320;&#21457;&#20154;&#21592;&#22312;&#21508;&#31181;&#32534;&#30721;&#20219;&#21153;&#20013;&#20855;&#26377;&#24110;&#21161;&#65292;&#24182;&#19988;&#24050;&#30452;&#25509;&#24212;&#29992;&#20110;&#20462;&#34917;&#31243;&#24207;&#30340;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLMs&#23558;&#31243;&#24207;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#30446;&#26631;&#32534;&#31243;&#35821;&#35328;&#30340;&#24213;&#23618;&#35821;&#20041;&#32422;&#26463;&#19968;&#26080;&#25152;&#30693;&#12290;&#36825;&#23548;&#33268;&#29983;&#25104;&#20102;&#22823;&#37327;&#38745;&#24577;&#26080;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#65292;&#38459;&#30861;&#20102;&#35813;&#25216;&#26415;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Repilot&#65292;&#19968;&#31181;&#22312;&#20462;&#22797;&#36807;&#31243;&#20013;&#36890;&#36807;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#20462;&#34917;&#31243;&#24207;&#20174;&#32780;&#36827;&#19968;&#27493;&#25903;&#25345;AI&#8220;&#21103;&#39550;&#39542;&#21592;&#8221;&#65288;&#21363;LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#35768;&#22810;LLMs&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#36755;&#20986;&#65288;&#21363;&#36880;&#20010;&#20196;&#29260;&#29983;&#25104;&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#32534;&#20889;&#31243;&#24207;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23436;&#25104;&#24341;&#25806;&#26174;&#33879;&#25552;&#21319;&#21644;&#24341;&#23548;&#12290;Repilot&#21327;&#21516;&#21512;&#25104;&#20102;&#20462;&#34917;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;</title><link>http://arxiv.org/abs/2308.06368</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Topic-Level Bayesian Surprise and Serendipity for Recommender Systems. (arXiv:2308.06368v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#20854;&#25512;&#33616;&#20165;&#36866;&#21512;&#29992;&#25143;&#23545;&#24050;&#28040;&#36153;&#29289;&#21697;&#30340;&#35780;&#32423;&#21382;&#21490;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#28388;&#27873;&#65292;&#29992;&#25143;&#26080;&#27861;&#20174;&#26032;&#39062;&#12289;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#20013;&#20307;&#39564;&#29289;&#21697;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#24847;&#22806;&#24615;&#24418;&#24335;&#65292;&#20197;&#36125;&#21494;&#26031;&#24778;&#21916;&#20026;&#22522;&#30784;&#65292;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#28040;&#36153;&#24182;&#35780;&#32423;&#21518;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#12290;&#32467;&#21512;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#30340;&#21327;&#21516;&#36807;&#28388;&#32452;&#20214;&#65292;&#21487;&#20197;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#24847;&#22806;&#24615;&#30340;&#29289;&#21697;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#20027;&#39064;&#32423;&#21035;&#30340;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;Goodreads&#20013;&#25552;&#21462;&#30340;&#22270;&#20070;&#38405;&#35835;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;26&#21315;&#20010;&#29992;&#25143;&#21644;&#36817;130&#19975;&#26412;&#20070;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;449&#31687;&#20070;&#36827;&#34892;&#20102;&#25163;&#21160;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 
&lt;/p&gt;</description></item><item><title>Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.09688</link><description>&lt;p&gt;
Amazon-M2: &#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09688
&lt;/p&gt;
&lt;p&gt;
Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#26469;&#35828;&#65292;&#24314;&#27169;&#23458;&#25143;&#36141;&#29289;&#24847;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#21644;&#21442;&#19982;&#24230;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#29702;&#35299;&#23458;&#25143;&#30340;&#20559;&#22909;&#23545;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25216;&#26415;&#21033;&#29992;&#23458;&#25143;&#20250;&#35805;&#25968;&#25454;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#20114;&#21160;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20250;&#35805;&#25968;&#25454;&#38598;&#22312;&#39033;&#30446;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#20840;&#38754;&#22320;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20559;&#22909;&#30340;&#35889;&#31995;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Amazon Multilingual Multi-locale Shopping Session Dataset&#65292;&#21363;Amazon-M2&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#30001;&#26469;&#33258;&#20845;&#20010;&#19981;&#21516;&#21306;&#22495;&#30340;&#25968;&#30334;&#19975;&#29992;&#25143;&#20250;&#35805;&#32452;&#25104;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20135;&#21697;&#30340;&#20027;&#35201;&#35821;&#35328;&#26159;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#26085;&#35821;&#12289;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06125</link><description>&lt;p&gt;
&#23398;&#20064;&#23618;&#27425;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20197;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#33258;&#30001;&#36335;&#24452;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#32463;&#24120;&#38656;&#35201;&#25805;&#25511;&#29615;&#22659;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#25171;&#24320;&#38376;&#20197;&#27983;&#35272;&#25151;&#38388;&#65292;&#24182;&#22312;&#27249;&#26588;&#21644;&#25277;&#23625;&#20869;&#25628;&#32034;&#30446;&#26631;&#29289;&#21697;&#12290;&#36825;&#20123;&#26032;&#25361;&#25112;&#38656;&#35201;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#32467;&#21512;&#25805;&#25511;&#21644;&#23548;&#33322;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HIMOS&#65292;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#32452;&#21512;&#25506;&#32034;&#12289;&#23548;&#33322;&#21644;&#25805;&#25511;&#25216;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#35821;&#20041;&#22320;&#22270;&#35760;&#24518;&#30340;&#25277;&#35937;&#39640;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#25506;&#32034;&#36807;&#30340;&#29615;&#22659;&#20316;&#20026;&#23454;&#20363;&#23548;&#33322;&#28857;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;HIMOS&#21487;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#20855;&#22791;&#22810;&#31181;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05141</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#36816;&#21160;&#21407;&#29702;&#19982;&#36125;&#21494;&#26031;&#32858;&#21512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Deep Probabilistic Movement Primitives with a Bayesian Aggregator. (arXiv:2307.05141v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05141
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#20855;&#22791;&#22810;&#31181;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#21407;&#29702;&#26159;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#28436;&#31034;&#38598;&#21512;&#20013;&#22797;&#21046;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#20801;&#35768;&#26102;&#38388;&#35843;&#21046;&#36816;&#21160;&#65288;&#21152;&#36895;&#25110;&#20943;&#36895;&#22797;&#21046;&#36816;&#21160;&#65289;&#12289;&#28151;&#21512;&#65288;&#23558;&#20004;&#20010;&#36816;&#21160;&#21512;&#24182;&#20026;&#19968;&#20010;&#65289;&#12289;&#36890;&#36807;&#28857;&#35843;&#33410;&#65288;&#23558;&#36816;&#21160;&#32422;&#26463;&#21040;&#29305;&#23450;&#30340;&#36890;&#36807;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#35843;&#33410;&#65288;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#29983;&#25104;&#36816;&#21160;&#65292;&#20363;&#22914;&#29289;&#20307;&#30340;&#20301;&#32622;&#65289;&#23637;&#31034;&#20986;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19968;&#20123;&#24418;&#24335;&#30340;&#36755;&#20837;&#35843;&#33410;&#25110;&#26102;&#38388;&#35843;&#21046;&#34920;&#36798;&#20013;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#25552;&#20986;&#19968;&#20010;&#21333;&#19968;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20855;&#22791;&#25152;&#26377;&#20808;&#21069;&#30340;&#25805;&#20316;&#65292;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#36816;&#21160;&#21407;&#29702;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive arch
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.04228</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#22320;&#36136;&#22797;&#26434;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#27931;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;MCMC&#65289;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#20808;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#21051;&#30011;&#21644;&#20284;&#28982;&#20989;&#25968;&#30340;&#39640;&#25928;&#35780;&#20272;&#12290;&#22312;&#23618;&#26512;&#25104;&#20687;&#30340;&#36125;&#21494;&#26031;&#30740;&#31350;&#20013;&#65292;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26041;&#20415;&#22320;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#21516;&#26102;&#20511;&#21161;&#22522;&#20110;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#65288;PCE&#65289;&#30340;&#20934;&#30830;&#20195;&#29702;&#27169;&#22411;&#26469;&#26367;&#20195;&#35745;&#31639;&#23494;&#38598;&#30340;&#20840;&#29289;&#29702;&#27491;&#21521;&#27714;&#35299;&#22120;&#12290;&#24403;PCA&#26080;&#27861;&#30452;&#25509;&#25552;&#20379;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#30340;&#26041;&#24335;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#31561;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;VAE&#30340;&#28508;&#22312;&#21442;&#25968;&#19982;&#27491;&#21521;&#24314;&#27169;&#36755;&#20986;&#20043;&#38388;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
&lt;/p&gt;</description></item><item><title>URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03810</link><description>&lt;p&gt;
URL&#65306;&#19968;&#31181;&#21487;&#36716;&#31227;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03810
&lt;/p&gt;
&lt;p&gt;
URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#21457;&#23637;&#20986;&#33021;&#22815;&#20316;&#20026;&#20174;&#38646;&#24320;&#22987;&#36801;&#31227;&#21040;&#26032;&#25968;&#25454;&#38598;&#26102;&#30340;&#26377;&#20215;&#20540;&#36215;&#28857;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#38543;&#30528;&#23545;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20165;&#33021;&#25552;&#20379;&#23884;&#20837;&#21521;&#37327;&#65292;&#36824;&#33021;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20026;&#20102;&#24341;&#23548;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;URL&#65288;Uncertainty-aware Representation Learning&#65289;&#22522;&#20934;&#12290;&#38500;&#20102;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20043;&#22806;&#65292;&#23427;&#36824;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38646;&#26679;&#26412;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;URL&#26469;&#35780;&#20272;11&#31181;&#22312;ImageNet&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36716;&#31227;&#21040;8&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30528;&#37325;&#20110;&#34920;&#31034;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#19978;&#28216;&#31867;&#21035;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertaint
&lt;/p&gt;</description></item><item><title>Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15687</link><description>&lt;p&gt;
Voicebox&#65306;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15687
&lt;/p&gt;
&lt;p&gt;
Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;GPT&#21644;DALL-E&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25110;&#22270;&#20687;&#36755;&#20986;&#65292;&#32780;&#19988;&#36824;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#34987;&#26126;&#30830;&#25945;&#25480;&#30340;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#20219;&#21153;&#36890;&#29992;&#21270;&#26041;&#38754;&#20173;&#28982;&#27604;&#36739;&#21407;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Voicebox&#65292;&#36825;&#26159;&#26368;&#22810;&#21151;&#33021;&#30340;&#38754;&#21521;&#35268;&#27169;&#30340;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;Voicebox&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#38899;&#39057;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;50,000&#23567;&#26102;&#30340;&#26410;&#32463;&#36807;&#28388;&#25110;&#22686;&#24378;&#30340;&#35821;&#38899;&#36827;&#34892;&#22635;&#20805;&#12290;&#19982;GPT&#31867;&#20284;&#65292;Voicebox&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#22810;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#26356;&#21152;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#36824;&#21487;&#20197;&#23545;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;Voicebox&#21487;&#20197;&#29992;&#20110;&#21333;&#35821;&#25110;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;Voicebox
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2306.12129</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors. (arXiv:2306.12129v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#32455;&#20256;&#24863;&#22120;&#32463;&#24120;&#21463;&#21040;&#22266;&#26377;&#25928;&#24212;&#65288;&#22914;&#20559;&#31227;&#12289;&#26494;&#24347;&#21644;&#28418;&#31227;&#65289;&#30340;&#24433;&#21709;&#32780;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#23646;&#24615;&#30340;&#32467;&#21512;&#20351;&#24471;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#21040;&#29289;&#29702;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#23567;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#32467;&#21512;&#31616;&#21333;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#36827;&#34892;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#37325;&#26032;&#37319;&#26679;&#36807;&#30340;&#20256;&#24863;&#22120;&#20449;&#21495;&#19978;&#24212;&#29992;&#20102;&#22810;&#20010;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#65292;&#20197;&#20135;&#29983;&#20445;&#30041;&#19981;&#21516;&#21382;&#21490;&#20256;&#24863;&#22120;&#25968;&#25454;&#27700;&#24179;&#30340;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#34920;&#31034;&#20197;&#21069;&#20256;&#24863;&#22120;&#25191;&#34892;&#20805;&#20998;&#29366;&#24577;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#19977;&#23618;ANN&#65292;&#24635;&#20849;&#26377;8&#20010;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23545;&#20110;&#26448;&#26009;&#21644;&#32467;&#26500;&#30456;&#24403;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#20063;&#26159;&#36866;&#29992;&#30340;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#30456;&#20851;&#30340;&#29289;&#29702;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knitted sensors frequently suffer from inconsistencies due to innate effects such as offset, relaxation, and drift. These properties, in combination, make it challenging to reliably map from sensor data to physical actuation. In this paper, we demonstrate a method for counteracting this by applying processing using a minimal artificial neural network (ANN) in combination with straightforward pre-processing. We apply a number of exponential smoothing filters on a re-sampled sensor signal, to produce features that preserve different levels of historical sensor data and, in combination, represent an adequate state of previous sensor actuation. By training a three-layer ANN with a total of 8 neurons, we manage to significantly improve the mapping between sensor reading and actuation force. Our findings also show that our technique translates to sensors of reasonably different composition in terms of material and structure, and it can furthermore be applied to related physical features such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;</title><link>http://arxiv.org/abs/2306.06344</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#22330;&#26223;&#32423;&#20132;&#36890;&#20223;&#30495;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#20223;&#30495;&#26159;&#21152;&#36895;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#21457;&#23637;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#23398;&#20064;&#30340;&#20132;&#36890;&#27169;&#22411;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#24456;&#38590;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CTG++&#65292;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;&#38656;&#35201;&#19968;&#20010;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#27169;&#22411;&#39592;&#24178;&#32467;&#26500;&#65292;&#24182;&#19988;&#35201;&#26377;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#35821;&#35328;&#19982;&#20132;&#36890;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#26102;&#31354;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#22330;&#26223;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#29983;&#25104;&#20102;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#25442;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#26397;&#30528;&#26597;&#35810;&#21512;&#35268;&#30340;&#29983;&#25104;&#26041;&#21521;&#21069;&#36827;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;</title><link>http://arxiv.org/abs/2306.04542</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#21644;&#21024;&#38500;&#22122;&#22768;&#26469;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#20197;&#29983;&#25104;&#25968;&#25454;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#34987;&#25552;&#20986;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#39640;&#23618;&#27425;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#32452;&#20214;&#30340;&#35774;&#35745;&#22522;&#30784;&#35206;&#30422;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#32780;&#36830;&#36143;&#30340;&#32508;&#36848;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#20214;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32508;&#36848;&#25353;&#29031;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#32452;&#32455;&#65292;&#21363;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#25193;&#25955;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#30740;&#31350;&#20998;&#26512;&#20010;&#20307;&#32452;&#20214;&#12289;&#35774;&#35745;&#36873;&#25321;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>AdANNS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#30456;&#20284;&#24230;&#35745;&#31639;&#36234;&#25509;&#36817;&#30340;&#25968;&#25454;&#28857;&#23558;&#20351;&#29992;&#26356;&#20302;&#23481;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#35745;&#31639;&#65292;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.19435</link><description>&lt;p&gt;
AdANNS: &#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AdANNS: A Framework for Adaptive Semantic Search. (arXiv:2305.19435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19435
&lt;/p&gt;
&lt;p&gt;
AdANNS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#30456;&#20284;&#24230;&#35745;&#31639;&#36234;&#25509;&#36817;&#30340;&#25968;&#25454;&#28857;&#23558;&#20351;&#29992;&#26356;&#20302;&#23481;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#35745;&#31639;&#65292;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#35268;&#27169;&#30340;&#25628;&#32034;&#31995;&#32479;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#23884;&#20837;&#19968;&#20010;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#28982;&#21518;&#23558;&#20854;&#36830;&#25509;&#21040;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#31649;&#36947;&#20013;&#26469;&#26816;&#32034;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#25429;&#25417;&#23614;&#37096;&#26597;&#35810;&#21644;&#25968;&#25454;&#28857;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36890;&#24120;&#26159;&#21018;&#24615;&#30340;&#12289;&#39640;&#32500;&#30340;&#21521;&#37327;&#65292;&#36890;&#24120;&#22312;&#25972;&#20010;ANNS&#31649;&#36947;&#20013;&#19968;&#25104;&#19981;&#21464;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#26816;&#32034;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#19982;&#20854;&#20351;&#29992;&#21018;&#24615;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;ANNS&#30340;&#19981;&#21516;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#21363;&#21487;&#20197;&#36827;&#34892;&#26356;&#21152;&#36817;&#20284;&#35745;&#31639;&#30340;ANNS&#38454;&#27573;&#24212;&#35813;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#28857;&#30340;&#20302;&#23481;&#37327;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AdANNS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;ANNS&#35774;&#35745;&#26694;&#26550;&#65292;&#26126;&#30830;&#21033;&#29992;Matryoshka&#34920;&#31034;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;AdANNS&#30340;&#26032;&#22411;&#20851;&#38190;ANNS&#26500;&#24314;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;LSTM&#21644;BLSTM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21147;&#28040;&#32791;&#30701;&#26399;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22235;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;BLSTM&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.16546</link><description>&lt;p&gt;
&#27604;&#36739;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21452;&#21521;LSTM&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Comparing Long Short-Term Memory (LSTM) and Bidirectional LSTM Deep Neural Networks for power consumption prediction. (arXiv:2305.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;LSTM&#21644;BLSTM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21147;&#28040;&#32791;&#30701;&#26399;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22235;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;BLSTM&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#26041;&#27861;&#26159;&#20026;&#20102;&#20915;&#31574;&#33410;&#33021;&#20197;&#21450;&#22312;&#33021;&#28304;&#24066;&#22330;&#20013;&#39044;&#27979;&#38656;&#27714;&#31561;&#22810;&#31181;&#21407;&#22240;&#32780;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21452;&#21521;LSTM&#65288;BLSTM&#65289;&#65292;&#29992;&#20110;&#21333;&#21464;&#37327;&#30005;&#24230;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#30701;&#26399;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36873;&#25321;&#20102;&#22235;&#20010;&#19981;&#21516;&#19978;&#19979;&#25991;&#21644;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#22235;&#20010;&#25968;&#25454;&#38598;&#20998;&#21035;&#26159;&#65306;&#65288;a&#65289;&#27861;&#22269;&#23478;&#24237;&#30340;&#29992;&#30005;&#37327;&#65307;&#65288;b&#65289;&#24052;&#35199;Santa&#233;m&#30340;&#19968;&#24231;&#22823;&#23398;&#24314;&#31569;&#30340;&#29992;&#30005;&#37327;&#65307;&#65288;c&#65289;&#25705;&#27931;&#21733;T&#233;touan&#24066;&#30340;&#29992;&#30005;&#38656;&#27714;&#65307;&#65288;d&#65289;&#26032;&#21152;&#22369;&#32858;&#21512;&#30005;&#21147;&#38656;&#27714;&#12290;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;&#20132;&#21449;&#39564;&#35777;&#26041;&#26696;&#35745;&#31639;&#20102;RMSE&#12289;MAE&#12289;MAPE&#21644;R2&#31561;&#25351;&#26631;&#12290;&#23545;&#24402;&#19968;&#21270;RMSE&#65288;NRMSE&#65289;&#30340;&#32467;&#26524;&#24212;&#29992;&#20102;Friedman&#26816;&#39564;&#65292;&#34920;&#26126;BLSTM&#27604;LSTM&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric consumption prediction methods are investigated for many reasons such as decision-making related to energy efficiency as well as for anticipating demand in the energy market dynamics. The objective of the present work is the comparison between two Deep Learning models, namely the Long Short-Term Memory (LSTM) and Bi-directional LSTM (BLSTM) for univariate electric consumption Time Series (TS) short-term forecast. The Data Sets (DSs) were selected for their different contexts and scales, aiming the assessment of the models' robustness. Four DSs were used, related to the power consumption of: (a) a household in France; (b) a university building in Santar\'em, Brazil; (c) the T\'etouan city zones, in Morocco; and (c) the Singapore aggregated electric demand. The metrics RMSE, MAE, MAPE and R2 were calculated in a TS cross-validation scheme. The Friedman's test was applied to normalized RMSE (NRMSE) results, showing that BLSTM outperforms LSTM with statistically significant differ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#31526;&#21512;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;</title><link>http://arxiv.org/abs/2305.15538</link><description>&lt;p&gt;
&#25913;&#21892;&#36873;&#25321;&#24615;&#24230;&#37327;&#30340;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Post-processing Private Synthetic Data for Improving Utility on Selected Measures. (arXiv:2305.15538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#31526;&#21512;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#24573;&#30053;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#26159;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26377;&#29305;&#23450;&#30340;&#38656;&#27714;&#65292;&#21512;&#25104;&#25968;&#25454;&#24517;&#39035;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#25968;&#25454;&#30340;&#19979;&#28216;&#29992;&#36884;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#28041;&#21450;&#20174;&#21512;&#25104;&#25968;&#25454;&#20013;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#28385;&#36275;&#25152;&#36873;&#25928;&#29992;&#24230;&#37327;&#30340;&#26679;&#26412;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;&#37325;&#26032;&#25277;&#26679;&#26435;&#37325;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22987;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#20013;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing private synthetic data generation algorithms are agnostic to downstream tasks. However, end users may have specific requirements that the synthetic data must satisfy. Failure to meet these requirements could significantly reduce the utility of the data for downstream use. We introduce a post-processing technique that improves the utility of the synthetic data with respect to measures selected by the end user, while preserving strong privacy guarantees and dataset quality. Our technique involves resampling from the synthetic data to filter out samples that do not meet the selected utility measures, using an efficient stochastic first-order algorithm to find optimal resampling weights. Through comprehensive numerical experiments, we demonstrate that our approach consistently improves the utility of synthetic data across multiple benchmark datasets and state-of-the-art synthetic data generation algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.14381</link><description>&lt;p&gt;
&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;MCR&#65289;&#23398;&#20064;&#26088;&#22312;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#32534;&#30721;&#21040;&#19968;&#20010;&#35821;&#20041;&#23545;&#40784;&#30340;&#20849;&#20139;&#31354;&#38388;&#20013;&#12290;&#35813;&#33539;&#20363;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#30340;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#20854;&#22312;&#26356;&#22810;&#27169;&#24577;&#19978;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#35757;&#32451;&#39640;&#25928;&#26041;&#27861;&#65292;&#31216;&#20026;&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;C-MCR&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#27169;&#24577;&#23545;&#19978;&#39044;&#35757;&#32451;&#20004;&#20010;&#29616;&#26377;&#30340;MCR&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#22312;&#26032;&#31354;&#38388;&#20013;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#27169;&#24577;&#23545;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#22312;&#27599;&#20010;MCR&#20869;&#24050;&#32463;&#23545;&#40784;&#65292;&#22240;&#27492;&#36890;&#36807;&#37325;&#21472;&#27169;&#24577;&#23398;&#20064;&#21040;&#30340;&#36830;&#25509;&#20063;&#21487;&#20197;&#36716;&#31227;&#21040;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#12290;&#20026;&#20102;&#21457;&#25381;C-MCR&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#22686;&#24378;&#30340;int
&lt;/p&gt;
&lt;p&gt;
Multi-modal Contrastive Representation (MCR) learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project them to a new space and use the data from the overlapping modality B to aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A, B) and (B, C) are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair (A, C). To unleash the potential of C-MCR, we further introduce a semantic-enhanced int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12467</link><description>&lt;p&gt;
&#29702;&#35299;ReLU&#32593;&#32476;&#30340;&#22810;&#38454;&#27573;&#20248;&#21270;&#21160;&#24577;&#21644;&#20016;&#23500;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#32463;&#24120;&#34920;&#29616;&#20986;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#21644;&#25439;&#22833;&#30340;&#38750;&#20984;&#24615;&#20026;&#29702;&#35770;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#29305;&#23450;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25429;&#33719;&#20102;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#21040;&#26368;&#32456;&#25910;&#25947;&#30340;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#12290;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#20063;&#21487;&#20197;&#34987;&#31934;&#30830;&#22320;&#35782;&#21035;&#21644;&#29702;&#35770;&#19978;&#25429;&#33719;&#65292;&#20363;&#22914;...
&lt;/p&gt;
&lt;p&gt;
The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21095;&#38598;&#24335;MDP&#27169;&#22411;&#65292;&#25552;&#20986;&#22312;&#38754;&#20020;&#36716;&#25442;&#21644;&#22870;&#21169;&#29305;&#24615;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#25552;&#20379;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .</title><link>http://arxiv.org/abs/2305.10744</link><description>&lt;p&gt;
&#38754;&#21521;&#21095;&#38598;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation in Episodic Markov Decision Processes. (arXiv:2305.10744v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21095;&#38598;&#24335;MDP&#27169;&#22411;&#65292;&#25552;&#20986;&#22312;&#38754;&#20020;&#36716;&#25442;&#21644;&#22870;&#21169;&#29305;&#24615;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#25552;&#20379;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38271;&#26399;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#23427;&#38656;&#35201;&#22312;&#22810;&#20010;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#22810;&#38454;&#27573;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21095;&#38598;&#24335;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#20854;&#20013;&#36716;&#25442;&#21644;&#22870;&#21169;&#20197;&#21450;&#27599;&#19968;&#27425;&#30340;&#36164;&#28304;&#28040;&#32791;&#20989;&#25968;&#37117;&#26159;&#38750;&#23450;&#24577;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31561;&#25928;&#30340;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#37325;&#26500;&#26041;&#27861;&#65292;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#65292;&#20026;&#27492;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36164;&#28304;&#20998;&#37197;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22788;&#29702;&#20102;&#22312;&#20272;&#31639;&#30495;&#23454;&#21487;&#34892;&#38598;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#35823;&#24046;&#65292;&#36825;&#26159;&#30456;&#23545;&#29420;&#31435;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#38543;&#26426;&#22870;&#21169;&#21644;&#36164;&#28304;&#28040;&#32791;&#20989;&#25968;&#65292;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463;&#65292;&#20854;&#30028;&#38480;&#21463;&#21040; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ &#30340;&#32422;&#26463;&#65292;&#20854;&#20013; $\rho\in(0,1)$ &#26159;&#39044;&#31639;&#21442;&#25968;&#65292;$H$ &#26159;&#22320;&#24179;&#32447;&#38271;&#24230;&#65292;$S$ &#21644; $A$ &#26159;. . .
&lt;/p&gt;
&lt;p&gt;
This paper studies a long-term resource allocation problem over multiple periods where each period requires a multi-stage decision-making process. We formulate the problem as an online resource allocation problem in an episodic finite-horizon Markov decision process with unknown non-stationary transitions and stochastic non-stationary reward and resource consumption functions for each episode. We provide an equivalent online linear programming reformulation based on occupancy measures, for which we develop an online mirror descent algorithm. Our online dual mirror descent algorithm for resource allocation deals with uncertainties and errors in estimating the true feasible set, which is of independent interest. We prove that under stochastic reward and resource consumption functions, the expected regret of the online mirror descent algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$ is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20854;&#23454;&#29616;&#22810;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05799</link><description>&lt;p&gt;
&#22810;&#21151;&#33021;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#21452;&#37325;&#35270;&#35273;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Seeing double with a multifunctional reservoir computer. (arXiv:2305.05799v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20854;&#23454;&#29616;&#22810;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21151;&#33021;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22810;&#37325;&#31283;&#23450;&#24615;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#32780;&#19981;&#25913;&#21464;&#20219;&#20309;&#32593;&#32476;&#23646;&#24615;&#12290;&#20351;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#33719;&#24471;&#26576;&#20123;&#22810;&#31283;&#23450;&#24615;&#20197;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#19982;&#32593;&#32476;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#29305;&#23450;&#21560;&#24341;&#23376;&#30456;&#20851;&#32852;&#65292;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#33258;&#28982;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#12290;&#22240;&#20026;&#19982;&#22810;&#31283;&#23450;&#24615;&#26377;&#20851;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20648;&#22791;&#35745;&#31639;&#26426;&#65288;RC&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#20648;&#22791;&#35745;&#31639;&#26426;&#26159;&#19968;&#31181;&#20197;ANN&#24418;&#24335;&#21576;&#29616;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#8220;&#21452;&#37325;&#35270;&#35273;&#25928;&#24212;&#8221;&#38382;&#39064;&#26469;&#31995;&#32479;&#22320;&#30740;&#31350;&#24403;&#20004;&#20010;&#21560;&#24341;&#23376;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#26102;RC&#22914;&#20309;&#37325;&#26500;&#21560;&#24341;&#23376;&#30340;&#20849;&#23384;&#12290;&#38543;&#30528;&#37325;&#21472;&#37327;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#21457;&#29616;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multifunctional biological neural networks exploit multistability in order to perform multiple tasks without changing any network properties. Enabling artificial neural networks (ANNs) to obtain certain multistabilities in order to perform several tasks, where each task is related to a particular attractor in the network's state space, naturally has many benefits from a machine learning perspective. Given the association to multistability, in this paper we explore how the relationship between different attractors influences the ability of a reservoir computer (RC), which is a dynamical system in the form of an ANN, to achieve multifunctionality. We construct the `seeing double' problem to systematically study how a RC reconstructs a coexistence of attractors when there is an overlap between them. As the amount of overlap increases, we discover that for multifunctionality to occur, there is a critical dependence on a suitable choice of the spectral radius for the RC's internal network c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.04934</link><description>&lt;p&gt;
&#24212;&#29992;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20998;&#26512;&#21644;&#21457;&#29616;&#26032;&#22411;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#20010;&#25972;&#21512;&#20102;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#22240;&#26524;&#22810;&#22836;&#22270;&#26426;&#21046;&#20013;&#23454;&#29616;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#39044;&#27979;&#20108;&#32423;&#32467;&#26500;&#20869;&#23481;&#65288;&#27599;&#20010;&#27531;&#22522;&#30340;&#27700;&#24179;&#21644;&#24635;&#20307;&#20869;&#23481;&#65289;&#12289;&#34507;&#30333;&#36136;&#21487;&#28342;&#24615;&#21644;&#27979;&#24207;&#20219;&#21153;&#12290;&#36827;&#19968;&#27493;&#22312;&#21453;&#21521;&#20219;&#21153;&#19978;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#36825;&#20123;&#24615;&#36136;&#20316;&#20026;&#30446;&#26631;&#29305;&#24449;&#30340;&#34507;&#30333;&#36136;&#12290;&#35813;&#27169;&#22411;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23436;&#20840;&#22522;&#20110;&#25552;&#31034;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#39069;&#22806;&#20219;&#21153;&#20250;&#20135;&#29983;&#30456;&#20114;&#21327;&#21516;&#20316;&#29992;&#65292;&#20351;&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#24471;&#21040;&#25552;&#39640;&#65292;&#36229;&#36807;&#20165;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#26696;&#20363;&#30740;&#31350;&#29992;&#20110;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#65292;&#21253;&#25324;&#31283;&#23450;&#24615;&#21644;&#21487;&#28342;&#24615;&#30340;&#34507;&#30333;&#36136;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03495</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#26799;&#24230;&#19979;&#38477;&#8221;&#19982; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03495
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20854;&#33021;&#21147;&#20173;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#20889;&#30340;&#25552;&#31034;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#20351;&#29992;&#25968;&#20540;&#26799;&#24230;&#19979;&#38477;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
&lt;/p&gt;</description></item><item><title>Patch Diffusion &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#23558;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#33267;&#23569;&#19968;&#20493;&#65292;&#24182;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12526</link><description>&lt;p&gt;
Patch Diffusion: &#26356;&#24555;&#26356;&#39640;&#25928;&#30340;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12526
&lt;/p&gt;
&lt;p&gt;
Patch Diffusion &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#23558;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#33267;&#23569;&#19968;&#20493;&#65292;&#24182;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550; Patch Diffusion&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#26102;&#38388;&#25104;&#26412;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26680;&#24515;&#26159;&#26032;&#30340;&#26465;&#20214;&#35780;&#20998;&#20989;&#25968;&#65292;&#23427;&#22312;&#22359;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#23558;&#21407;&#22987;&#22270;&#20687;&#20013;&#30340;&#22359;&#20301;&#32622;&#20316;&#20026;&#38468;&#21152;&#22352;&#26631;&#36890;&#36947;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#21270;&#21644;&#22810;&#26679;&#21270;&#22359;&#22823;&#23567;&#26469;&#32534;&#30721;&#22810;&#23610;&#24230;&#30340;&#36328;&#21306;&#22495;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#37319;&#26679;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21407;&#22987;&#25193;&#25955;&#27169;&#22411;&#19968;&#26679;&#31616;&#21333;&#26131;&#29992;&#12290;&#36890;&#36807; Patch Diffusion&#65292;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616; $\mathbf{\ge 2\times}$ &#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;Patch Diffusion &#25552;&#39640;&#20102;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#20165; 5,000 &#24352;&#22270;&#20687;&#36827;&#34892;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.12410</link><description>&lt;p&gt;
PEFT-Ref: &#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#21442;&#32771;&#26550;&#26500;&#21644;&#31867;&#22411;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;(PEFT)&#25216;&#26415;&#26088;&#22312;&#25913;&#21892;&#23436;&#20840;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#19981;&#26029;&#20986;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(i)&#23427;&#20204;&#28155;&#21152;&#21040;PLM&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;(ii)&#19981;&#21516;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#25928;&#29575;&#25913;&#36827;&#65292;(iii)&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;(iv)&#32467;&#26500;&#21644;&#21151;&#33021;&#24046;&#24322;&#22914;&#20309;&#19982;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#24046;&#24322;&#38548;&#31163;&#21040;&#19982;&#26631;&#20934;&#32452;&#20214;&#30340;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26631;&#20934;&#21270;&#21644;&#38548;&#31163;&#24046;&#24322;&#30340;&#36807;&#31243;&#65292;&#20986;&#29616;&#20102;PEFT&#25216;&#26415;&#30340;&#27169;&#22359;&#21270;&#35270;&#22270;&#65292;&#19981;&#20165;&#25903;&#25345;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#31216;&#20026;PEFT-Ref&#65292;&#21253;&#25324;&#19971;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#22788;&#29702;PEFT&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#21487;&#29992;&#20316;&#24320;&#21457;&#26032;PEFT&#25216;&#26415;&#21644;&#27604;&#36739;&#29616;&#26377;&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10398</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#36825;&#20123;&#36827;&#23637;&#22312;&#22810;&#31867;&#20998;&#31867;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#23637;&#31034;&#65292;&#20294;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29616;&#23454;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#33410;&#28857;&#21487;&#33021;&#26377;&#22810;&#20010;&#26631;&#31614;&#65292;&#19968;&#30452;&#20197;&#26469;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#36827;&#34892;&#20851;&#20110;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#30340;&#37325;&#28857;&#30740;&#31350;&#30340;&#39318;&#35201;&#25361;&#25112;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#26631;&#31614;&#22270;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19977;&#20010;&#30495;&#23454;&#30340;&#29983;&#29289;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#26631;&#31614;&#22270;&#29983;&#25104;&#22120;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#21487;&#35843;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#39640;&#26631;&#31614;&#30456;&#20284;&#24615;&#65288;&#39640;&#21516;&#31867;&#20559;&#22909;&#65289;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;GNN&#30340;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#24182;&#19981;&#36981;&#24490;&#30446;&#21069;&#20026;&#22810;&#31867;&#22330;&#26223;&#23450;&#20041;&#30340;&#21516;&#31867;&#20559;&#22909;&#21644;&#24322;&#31867;&#20559;&#22909;&#30340;&#24120;&#35268;&#35821;&#20041;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#38500;&#20102;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#23450;&#20041;&#21516;&#31867;&#20559;&#22909;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;MLGCN&#65288;&#22810;&#26631;&#31614;&#22270;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#26469;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, besides defining homophily for the multi-label scenario, we dev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.09310</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; $\tau$-Lasso&#65306;&#20854;&#20581;&#22766;&#24615;&#21644;&#26368;&#20248;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#40065;&#26834; $\tau$-&#22238;&#24402;&#20272;&#35745;&#22120;&#65292;&#20197;&#24212;&#23545;&#21709;&#24212;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#30340;&#20005;&#37325;&#27745;&#26579;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20272;&#35745;&#22120;&#20026;&#33258;&#36866;&#24212; $\tau$-Lasso&#65292;&#23427;&#23545;&#24322;&#24120;&#20540;&#21644;&#39640;&#26464;&#26438;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#26469;&#20943;&#23569;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20026;&#27599;&#20010;&#22238;&#24402;&#31995;&#25968;&#20998;&#37197;&#19968;&#20010;&#26435;&#37325;&#12290;&#23545;&#20110;&#22266;&#23450;&#25968;&#37327;&#30340;&#39044;&#27979;&#21464;&#37327; $p$&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#33258;&#36866;&#24212; $\tau$-Lasso &#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#26029;&#28857;&#21644;&#24433;&#21709;&#20989;&#25968;&#26469;&#34920;&#24449;&#20854;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07063</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#20844;&#24335;&#26159;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28165;&#26224;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#32452;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#35270;&#20026;&#38598;&#21512;&#36816;&#31639;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#30740;&#31350;&#36981;&#24490;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#20174;&#36923;&#36753;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20808;&#21069;&#30740;&#31350;&#35843;&#26597;&#30340;&#26597;&#35810;&#33539;&#22260;&#65292;&#24182;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#23427;&#19982;&#25972;&#20010;&#23384;&#22312;&#24615;&#20844;&#24335;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#26032;&#20844;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#20102;&#21516;&#26102;&#20986;&#29616;&#30340;&#26032;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#23558;&#29702;&#35770;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#24182;&#32467;&#21512;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#37325;&#23376;&#25955;&#23556;&#30340;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.14090</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#21033;&#29992;&#26263;&#29289;&#36136;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter. (arXiv:2303.14090v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#23558;&#29702;&#35770;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#24182;&#32467;&#21512;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#37325;&#23376;&#25955;&#23556;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#21512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23558;&#32479;&#35745;&#27169;&#24335;&#19982;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20854;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#24050;&#30693;&#20851;&#31995;&#26469;&#20016;&#23500;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#20197;&#38480;&#21046;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#27700;&#21160;&#21147;&#23398;&#27169;&#25311;&#26159;&#29616;&#20195;&#23431;&#23449;&#23398;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#32780;&#25152;&#38656;&#30340;&#35745;&#31639;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24555;&#36895;&#27169;&#25311;&#26263;&#29289;&#36136;&#38656;&#35201;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25104;&#20026;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;;&#22312;&#36825;&#37324;&#65292;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#21457;&#29616;&#30340;&#25955;&#23556;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#21644;&#29289;&#29702;&#32422;&#26463;&#65292;&#23558;&#20851;&#20110;&#37325;&#23376;&#36716;&#21270;&#25928;&#29575;&#30340;&#29702;&#35770;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#22522;&#20110;&#32467;&#26524;&#22270;&#20687;&#20013;&#21160;&#21147;&#23398;&#21151;&#29575;&#35889;&#20013;&#30340;&#35823;&#24046;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#37327;&#21270;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have emerged as a coherent framework for building predictive models that combine statistical patterns with domain knowledge. The underlying notion is to enrich the optimization loss function with known relationships to constrain the space of possible solutions. Hydrodynamic simulations are a core constituent of modern cosmology, while the required computations are both expensive and time-consuming. At the same time, the comparatively fast simulation of dark matter requires fewer resources, which has led to the emergence of machine learning algorithms for baryon inpainting as an active area of research; here, recreating the scatter found in hydrodynamic simulations is an ongoing challenge. This paper presents the first application of physics-informed neural networks to baryon inpainting by combining advances in neural network architectures with physical constraints, injecting theory on baryon conversion efficiency into the model loss function. We also in
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;DyGFormer&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#24211;DyGLib&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;DyGFormer&#36890;&#36807;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#21644;&#20998;&#22359;&#25216;&#26415;&#23454;&#29616;&#26356;&#38271;&#26399;&#21382;&#21490;&#30340;&#39640;&#25928;&#25512;&#29702;&#65292;&#22312;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13047</link><description>&lt;p&gt;
&#21521;&#26356;&#22909;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#36808;&#36827;&#65306;&#26032;&#30340;&#26550;&#26500;&#21644;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13047
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;DyGFormer&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#24211;DyGLib&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;DyGFormer&#36890;&#36807;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#21644;&#20998;&#22359;&#25216;&#26415;&#23454;&#29616;&#26356;&#38271;&#26399;&#21382;&#21490;&#30340;&#39640;&#25928;&#25512;&#29702;&#65292;&#22312;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DyGFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;&#65292;&#20165;&#20174;&#33410;&#28857;&#21382;&#21490;&#30340;&#31532;&#19968;&#36339;&#20132;&#20114;&#24207;&#21015;&#20013;&#23398;&#20064;&#12290;DyGFormer&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#35745;&#65306;&#19968;&#31181;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#65292;&#25506;&#32034;&#28304;&#33410;&#28857;&#21644;&#30446;&#26631;&#33410;&#28857;&#22522;&#20110;&#23427;&#20204;&#30340;&#24207;&#21015;&#30340;&#30456;&#20851;&#24615;&#65307;&#19968;&#31181;&#20998;&#22359;&#25216;&#26415;&#65292;&#23558;&#27599;&#20010;&#24207;&#21015;&#20998;&#25104;&#22810;&#20010;&#22359;&#24182;&#23558;&#20854;&#39304;&#36865;&#32473;Transformer&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#21463;&#30410;&#20110;&#26356;&#38271;&#26399;&#30340;&#21382;&#21490;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;DyGLib&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#65292;&#20855;&#26377;&#26631;&#20934;&#30340;&#35757;&#32451;&#31649;&#36947;&#12289;&#21487;&#25193;&#23637;&#30340;&#32534;&#30721;&#25509;&#21475;&#21644;&#32508;&#21512;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;13&#20010;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#36827;&#34892;&#25512;&#23548;/&#24402;&#32435;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#21644;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65306;DyGFormer&#22312;mo&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo
&lt;/p&gt;</description></item><item><title>EDGI&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#22788;&#29702;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20869;&#22312;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.12410</link><description>&lt;p&gt;
EDGI: &#20869;&#22312;&#23545;&#31216;&#24615;&#35268;&#21010;&#30340;&#31561;&#21464;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
EDGI: Equivariant Diffusion for Planning with Embodied Agents. (arXiv:2303.12410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12410
&lt;/p&gt;
&lt;p&gt;
EDGI&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#22788;&#29702;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20869;&#22312;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#23545;&#31216;&#24615;&#26159;&#26102;&#31354;&#21644;&#25490;&#21015;&#19978;&#30340;&#65292;&#22823;&#22810;&#25968;&#35745;&#21010;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#36825;&#31181;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#23548;&#33268;&#37319;&#26679;&#25928;&#29575;&#20302;&#21644;&#27867;&#21270;&#33021;&#21147;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#23545;&#31216;&#24615;&#35268;&#21010;&#30340;&#31561;&#21464;&#25193;&#25955;&#31639;&#27861;(EDGI), &#21487;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25903;&#25345;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group $\mathrm{SE(3)}$, the discrete-time translation group $\mathbb{Z}$, and the object permutation group $\mathrm{S}_n$. EDGI follows the Diffuser framework (Janner et al. 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new $\mathrm{SE(3)} \times \mathbb{Z} \times \mathrm{S}_n$-equivariant diffusion model that supports multiple representations. We integrate this model i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#36827;&#34892;&#38544;&#31169;&#25439;&#22833;&#23457;&#35745;&#65292;&#19988;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2302.03098</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#36827;&#34892;&#38544;&#31169;&#25439;&#22833;&#23457;&#35745;&#65292;&#19988;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;ially private&#65288;DP&#65289;&#31639;&#27861;&#30340;&#38544;&#31169;&#20272;&#35745;&#25216;&#26415;&#21487;&#29992;&#20110;&#19982;&#20998;&#26512;&#19978;&#30028;&#36827;&#34892;&#27604;&#36739;&#65292;&#25110;&#22312;&#24050;&#30693;&#20998;&#26512;&#19978;&#30028;&#19981;&#32039;&#30340;&#24773;&#20917;&#19979;&#23454;&#39564;&#27979;&#37327;&#38544;&#31169;&#25439;&#22833;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#38544;&#31169;&#23457;&#35745;&#25216;&#26415;&#36890;&#24120;&#23545;&#23545;&#25163;&#20570;&#20986;&#24378;&#28872;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#20013;&#38388;&#27169;&#22411;&#36845;&#20195;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65289;&#65292;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#22810;&#27425;&#65288;&#36890;&#24120;&#25968;&#37327;&#32423;&#20026;&#25968;&#21315;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#22823;&#35268;&#27169;&#37096;&#32626;&#27492;&#31867;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#38656;&#35201;&#25968;&#22825;&#25110;&#25968;&#21608;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#27425;&#8221;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#36816;&#34892;&#26399;&#38388;&#39640;&#25928;&#22320;&#23457;&#35745;&#25110;&#20272;&#35745;&#27169;&#22411;&#30340;&#38544;&#31169;&#25439;&#22833;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#31561;&#35774;&#32622;&#20013;&#20351;&#29992;&#30340;&#19968;&#33324;DP&#31639;&#27861;&#65292;&#24182;&#30001;&#23454;&#39564;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20854;&#25552;&#20379;&#30340;&#20934;&#30830;&#38544;&#31169;&#25439;&#22833;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks and model architectures, and require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel "one-shot" approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the mod
&lt;/p&gt;</description></item><item><title>"IC3: Image Captioning by Committee Consensus"&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#29983;&#25104;&#22270;&#20687;&#23383;&#24149;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#39640;&#23618;&#32454;&#33410;&#65292;&#20248;&#20110;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.01328</link><description>&lt;p&gt;
IC3&#65306;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
IC3: Image Captioning by Committee Consensus. (arXiv:2302.01328v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01328
&lt;/p&gt;
&lt;p&gt;
"IC3: Image Captioning by Committee Consensus"&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#29983;&#25104;&#22270;&#20687;&#23383;&#24149;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#39640;&#23618;&#32454;&#33410;&#65292;&#20248;&#20110;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20320;&#35831;&#19968;&#20010;&#20154;&#25551;&#36848;&#19968;&#24133;&#22270;&#20687;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#29992;&#19968;&#21315;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;&#12290;&#20256;&#32479;&#19978;&#65292;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#29983;&#25104;&#19968;&#20010;&#8220;&#26368;&#20339;&#8221;&#65288;&#19982;&#21442;&#32771;&#26368;&#30456;&#20284;&#65289;&#30340;&#22270;&#20687;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#40723;&#21169;&#29983;&#25104;&#8220;&#20449;&#24687;&#36139;&#20047;&#8221;&#30340;&#23383;&#24149;&#65292;&#24182;&#19988;&#21482;&#20851;&#27880;&#21487;&#33021;&#32454;&#33410;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#32780;&#24573;&#30053;&#20102;&#22330;&#26223;&#20013;&#20854;&#20182;&#21487;&#33021;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;"&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;"&#65288;IC3&#65289;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#21040;&#39640;&#23618;&#32454;&#33410;&#30340;&#21333;&#20010;&#23383;&#24149;&#12290;&#20154;&#31867;&#35780;&#20215;IC3&#29983;&#25104;&#30340;&#23383;&#24149;&#33267;&#23569;&#19982;&#22522;&#20934;SOTA&#27169;&#22411;&#19968;&#26679;&#26377;&#24110;&#21161;&#30340;&#24773;&#20917;&#21344;&#20102;&#19977;&#20998;&#20043;&#20108;&#20197;&#19978;&#65292;&#24182;&#19988;IC3&#21487;&#20197;&#23558;SOTA&#33258;&#21160;&#21484;&#22238;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;84%&#65292;&#32988;&#36807;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#30456;&#27604;&#20110;SOTA&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#20195;&#30721;&#21487;&#36890;&#36807;https://davidmchan&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to generate a single "best" (most like a reference) image caption. Unfortunately, doing so encourages captions that are "informationally impoverished," and focus on only a subset of the possible details, while ignoring other potentially useful information in the scene. In this work, we introduce a simple, yet novel, method: "Image Captioning by Committee Consensus" (IC3), designed to generate a single caption that captures high-level details from several annotator viewpoints. Humans rate captions produced by IC3 at least as helpful as baseline SOTA models more than two thirds of the time, and IC3 can improve the performance of SOTA automated recall systems by up to 84%, outperforming single human-generated reference captions, and indicating significant improvements over SOTA approaches for visual description. Code is available at https://davidmchan.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.12321</link><description>&lt;p&gt;
&#31070;&#32463;&#20851;&#31995;&#22270;&#65306;&#35782;&#21035;&#26631;&#31614;&#22122;&#38899;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#21644;&#28165;&#29702;&#25968;&#25454;&#26159;&#26500;&#24314;&#20581;&#22766;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23384;&#22312;&#22797;&#26434;&#38382;&#39064;&#65292;&#22914;&#26631;&#31614;&#38169;&#35823;&#12289;&#27424;&#34920;&#31034;&#21644;&#24322;&#24120;&#20540;&#65292;&#22240;&#27492;&#22312;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#36825;&#19968;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20851;&#31995;&#22270;&#32467;&#26500;&#26469;&#26816;&#27979;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25552;&#20379;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#28857;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20316;&#20026;&#20132;&#20114;&#24335;&#35786;&#26029;&#25968;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26631;&#31614;&#38169;&#35823;&#21644;&#31163;&#32676;&#20540;/&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#38408;&#20540;&#26679;&#24335;&#31070;&#32463;&#20803;&#30340;&#20020;&#30028;&#27493;&#38271;&#12290;</title><link>http://arxiv.org/abs/2212.07469</link><description>&lt;p&gt;
&#36890;&#36807;"&#31283;&#23450;&#36793;&#32536;"&#23398;&#20064;&#38408;&#20540;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Learning threshold neurons via the "edge of stability". (arXiv:2212.07469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#38408;&#20540;&#26679;&#24335;&#31070;&#32463;&#20803;&#30340;&#20020;&#30028;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20998;&#26512;&#36890;&#24120;&#22522;&#20110;&#26497;&#23567;&#23398;&#20064;&#29575;&#30340;&#19981;&#29616;&#23454;&#20551;&#35774;&#12290;&#19982;&#23454;&#38469;&#26234;&#24935;&#21644;&#32463;&#39564;&#30740;&#31350;&#30456;&#21453;&#65292;&#20363;&#22914;J. Cohen&#31561;&#20154;&#30340;&#24037;&#20316;&#65288;ICLR 2021&#65289;&#65292;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26032;&#29616;&#35937;&#65288;"&#31283;&#23450;&#36793;&#32536;"&#25110;"&#19981;&#31283;&#23450;&#25910;&#25947;"&#65289;&#65292;&#20197;&#21450;&#22823;&#23398;&#20064;&#29575;&#20307;&#21046;&#19979;&#30340;&#28508;&#22312;&#27867;&#21270;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#21518;&#19968;&#31181;&#25928;&#24212;&#20173;&#28982;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#36808;&#20986;&#20102;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#30495;&#27491;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#27493;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#23574;&#38160;&#30340;&#38454;&#36291;&#36716;&#21464;&#65292;&#24403;&#27493;&#38271;&#23567;&#20110;&#27492;&#20540;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#21040;"&#38408;&#20540;&#26679;&#24335;"&#31070;&#32463;&#20803;&#65288;&#21363;&#20855;&#26377;&#38750;&#38646;&#31532;&#19968;&#23618;&#20559;&#32622;&#30340;&#31070;&#32463;&#20803;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing analyses of neural network training often operate under the unrealistic assumption of an extremely small learning rate. This lies in stark contrast to practical wisdom and empirical studies, such as the work of J. Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the "edge of stability" or "unstable convergence") and potential benefits for generalization in the large learning rate regime. Despite a flurry of recent works on this topic, however, the latter effect is still poorly understood. In this paper, we take a step towards understanding genuinely non-convex training dynamics with large learning rates by performing a detailed analysis of gradient descent for simplified models of two-layer neural networks. For these models, we provably establish the edge of stability phenomenon and discover a sharp phase transition for the step size below which the neural network fails to learn "threshold-like" neurons (i.e., neurons with a non-zero first-layer bias). This elu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.06348</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#33041;&#20449;&#21495;&#25581;&#31034;&#20154;&#31867;&#35821;&#35328;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#65288;&#22914;&#33041;&#30005;&#22270;&#65289;&#21644;&#20154;&#31867;&#35821;&#35328;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20108;&#32773;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33041;&#30005;&#22270;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#24615;&#12290;&#22312;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#65288;Multimodal Transformer Alignment Model&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35266;&#23519;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#21327;&#35843;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#20851;&#31995;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#21644;Wasserstein&#36317;&#31163;&#65292;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#36716;&#25442;&#29305;&#24449;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#22312;ZuCo&#21644;K-EmoCon&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#20351;K-EmoCon&#25968;&#25454;&#38598;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;1.7&#65285;&#65292;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;9.3&#65285;&#65292;&#22312;&#20851;&#31995;&#26816;&#27979;&#26041;&#38754;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;7.4&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22269;&#38469;&#19978;&#26368;&#22823;&#30340;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
&lt;/p&gt;</description></item><item><title>PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.07940</link><description>&lt;p&gt;
PROFHIT: &#38754;&#21521;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#40065;&#26834;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07940
&lt;/p&gt;
&lt;p&gt;
PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24615;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#31181;&#65292;&#20854;&#30446;&#26631;&#26159;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#20998;&#23618;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#19978;&#20063;&#24341;&#20837;&#20102;&#20998;&#23618;&#20851;&#31995;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#40664;&#40664;&#22320;&#20551;&#35774;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#20998;&#23618;&#20851;&#31995;&#19968;&#33268;&#65292;&#24182;&#19988;&#19981;&#36866;&#24212;&#26174;&#31034;&#19982;&#27492;&#20551;&#35774;&#20559;&#31163;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHIT&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#24615;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;PROFHIT&#37319;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gaps and propose PROFHIT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHIT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#21028;&#21035;&#21040;&#26680;&#29983;&#25104;&#32593;&#32476;&#30340;&#23450;&#26631;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Discriminative to Kernel Generative Networks for Calibrated Inference. (arXiv:2201.13001v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#19982;&#29983;&#25104;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#26234;&#33021;&#30340;&#30740;&#31350;&#20013;&#37117;&#26377;&#20854;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20108;&#32773;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#26680;&#29983;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#27169;&#22411;&#35270;&#20026;&#24191;&#20041;&#30340;&#21010;&#20998;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#30001;&#35757;&#32451;&#25968;&#25454;&#26500;&#25104;&#30340;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#65292;&#26469;&#33719;&#24471;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#21487;&#20197;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#36825;&#31867;&#20989;&#25968;&#25152;&#38656;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#30028;&#38480;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#32780;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#34987;&#35748;&#20026;&#26159;&#22266;&#23450;&#24120;&#25968;&#12290;</title><link>http://arxiv.org/abs/2111.08117</link><description>&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#32467;&#26500;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural networks with linear threshold activations: structure and algorithms. (arXiv:2111.08117v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#21487;&#20197;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#36825;&#31867;&#20989;&#25968;&#25152;&#38656;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#30028;&#38480;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#32780;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#34987;&#35748;&#20026;&#26159;&#22266;&#23450;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#20102;&#21487;&#20197;&#30001;&#36825;&#26679;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#29992;2&#20010;&#38544;&#34255;&#23618;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#21487;&#34920;&#31034;&#20989;&#25968;&#26082;&#26159;&#24517;&#35201;&#30340;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;&#36825;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#32771;&#34385;&#21040;&#26368;&#36817;&#20351;&#29992;&#20854;&#20182;&#27969;&#34892;&#28608;&#27963;&#20989;&#25968;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#21487;&#34920;&#31034;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#35813;&#31867;&#20013;&#20219;&#20309;&#20989;&#25968;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#31934;&#30830;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#38382;&#39064;&#65292;&#20197;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;&#22914;&#26524;&#23558;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#35270;&#20026;&#22266;&#23450;&#24120;&#25968;&#65292;&#21017;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#26159;&#22810;&#39033;&#24335;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#21487;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we present new results on neural networks with linear threshold activation functions. We precisely characterize the class of functions that are representable by such neural networks and show that 2 hidden layers are necessary and sufficient to represent any function representable in the class. This is a surprising result in the light of recent exact representability investigations for neural networks using other popular activation functions like rectified linear units (ReLU). We also give precise bounds on the sizes of the neural networks required to represent any function in the class. Finally, we design an algorithm to solve the empirical risk minimization (ERM) problem to global optimality for these neural networks with a fixed architecture. The algorithm's running time is polynomial in the size of the data sample, if the input dimension and the size of the network architecture are considered fixed constants. The algorithm is unique in the sense that it works for any
&lt;/p&gt;</description></item></channel></rss>