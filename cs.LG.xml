<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#26469;&#36817;&#20284;&#35745;&#31639;&#20999;&#29255;&#21326;&#29791;&#26031;&#22374;&#36317;&#31163;&#12290;&#19982;&#33945;&#29305;&#21345;&#32599;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#29702;&#35770;&#24615;&#36136;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01493</link><description>&lt;p&gt;
&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#30340;&#20999;&#29255;&#21326;&#29791;&#26031;&#22374;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01493
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#26469;&#36817;&#20284;&#35745;&#31639;&#20999;&#29255;&#21326;&#29791;&#26031;&#22374;&#36317;&#31163;&#12290;&#19982;&#33945;&#29305;&#21345;&#32599;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;&#21326;&#29791;&#26031;&#22374;&#65288;SW&#65289;&#36317;&#31163;&#26159;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#21326;&#29791;&#26031;&#22374;&#36317;&#31163;&#30340;&#24179;&#22343;&#20540;&#65292;&#32467;&#26524;&#20026;&#30456;&#20851;&#30340;&#19968;&#32500;&#25237;&#24433;&#30340;&#21326;&#29791;&#26031;&#22374;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;SW&#36317;&#31163;&#21487;&#20197;&#20889;&#25104;&#23545;&#29699;&#38754;&#19978;&#22343;&#21248;&#27979;&#24230;&#30340;&#31215;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26694;&#26550;&#26469;&#35745;&#31639;SW&#36317;&#31163;&#12290;&#29699;&#35856;&#20989;&#25968;&#26159;&#29699;&#38754;&#19978;&#30340;&#22810;&#39033;&#24335;&#65292;&#23427;&#20204;&#26500;&#25104;&#20102;&#29699;&#38754;&#19978;&#21487;&#31215;&#20989;&#25968;&#38598;&#21512;&#30340;&#27491;&#20132;&#22522;&#12290;&#23558;&#36825;&#20004;&#20010;&#20107;&#23454;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#31216;&#20026;&#29699;&#35856;&#25511;&#21046;&#21464;&#37327;&#65288;SHCV&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#29699;&#35856;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#36817;&#20284;&#35745;&#31639;SW&#36317;&#31163;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#20363;&#22914;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#19968;&#23450;&#24418;&#24335;&#30340;&#32447;&#24615;&#20381;&#36182;&#26102;&#65292;&#28151;&#21512;&#39640;&#26031;&#27979;&#24230;&#30340;&#26080;&#35823;&#24046;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#33945;&#29305;&#21345;&#32599;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sliced-Wasserstein (SW) distance between probability measures is defined as the average of the Wasserstein distances resulting for the associated one-dimensional projections. As a consequence, the SW distance can be written as an integral with respect to the uniform measure on the sphere and the Monte Carlo framework can be employed for calculating the SW distance. Spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere. Putting these two facts together, a new Monte Carlo method, hereby referred to as Spherical Harmonics Control Variates (SHCV), is proposed for approximating the SW distance using spherical harmonics as control variates. The resulting approach is shown to have good theoretical properties, e.g., a no-error property for Gaussian measures under a certain form of linear dependency between the variables. Moreover, an improved rate of convergence, compared to Monte Carlo, is established for g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09499</link><description>&lt;p&gt;
&#20351;&#29992;Q&#23398;&#20064;&#30340;&#22902;&#29275;&#20859;&#27542;&#22330;&#30005;&#27744;&#31649;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22902;&#29275;&#20859;&#27542;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#65292;&#26159;&#20892;&#19994;&#20013;&#19968;&#20010;&#33021;&#28304;&#23494;&#38598;&#22411;&#30340;&#37096;&#38376;&#12290;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#38598;&#25104;&#21040;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#26377;&#25928;&#30340;&#30005;&#27744;&#31649;&#29702;&#23545;&#20110;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#33267;&#20851;&#37325;&#35201;&#12290;&#31649;&#29702;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#30001;&#20110;&#30005;&#33021;&#28040;&#32791;&#30340;&#27874;&#21160;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#38388;&#27463;&#24615;&#20197;&#21450;&#33021;&#28304;&#20215;&#26684;&#30340;&#27874;&#21160;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26377;&#28508;&#21147;&#26174;&#33879;&#25913;&#21892;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21033;&#29992;&#65292;&#28982;&#32780;&#22312;&#36825;&#19968;&#29305;&#23450;&#39046;&#22495;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20197;&#29233;&#23572;&#20848;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#20854;&#20197;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#20026;&#26680;&#24515;&#30340;2030&#24180;&#33021;&#28304;&#25112;&#30053;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23433;&#25490;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09499v1 Announce Type: cross  Abstract: Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in 
&lt;/p&gt;</description></item><item><title>3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03954</link><description>&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
3D Diffusion Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03954
&lt;/p&gt;
&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20026;&#25945;&#25480;&#26426;&#22120;&#20154;&#28789;&#24039;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#23398;&#20064;&#22797;&#26434;&#32780;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#25216;&#33021;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#28436;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#34701;&#20837;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#30340;&#26032;&#39062;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#25193;&#25955;&#31574;&#30053;&#26159;&#19968;&#31867;&#26377;&#26465;&#20214;&#30340;&#21160;&#20316;&#29983;&#25104;&#27169;&#22411;&#12290;DP3&#30340;&#26680;&#24515;&#35774;&#35745;&#26159;&#21033;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;3D&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#26159;&#20174;&#31232;&#30095;&#28857;&#20113;&#20013;&#25552;&#21462;&#20986;&#26469;&#30340;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#28857;&#32534;&#30721;&#22120;&#12290;&#22312;&#25105;&#20204;&#28085;&#30422;&#20102;72&#20010;&#20223;&#30495;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;DP3&#20165;&#38656;&#35201;10&#20010;&#28436;&#31034;&#23601;&#21487;&#20197;&#25104;&#21151;&#22788;&#29702;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;55.3%&#12290;&#22312;4&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;DP3&#34920;&#29616;&#20986;&#20102;&#39640;&#25104;&#21151;&#29575;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#27599;&#39033;&#20219;&#21153;&#20165;&#38656;40&#27425;&#28436;&#31034;&#21363;&#21487;&#25104;&#21151;&#29575;&#20026;85%&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03954v1 Announce Type: cross  Abstract: Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in di
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#38024;&#23545;&#21453;&#23398;&#20064;&#29615;&#22659;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23545;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.01218</link><description>&lt;p&gt;
&#31895;&#31961;&#21453;&#23398;&#20064;&#38656;&#35201;&#26356;&#21152;&#35880;&#24910;&#30340;&#35780;&#20272;&#20197;&#36991;&#20813;&#34394;&#20551;&#38544;&#31169;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01218
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#38024;&#23545;&#21453;&#23398;&#20064;&#29615;&#22659;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23545;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#20351;&#24471;&#24320;&#21457;&#21453;&#23398;&#20064;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#21024;&#38500;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20174;&#30452;&#35273;&#19978;&#35762;&#65292;&#19968;&#26086;&#27169;&#22411;&#23436;&#25104;&#21453;&#23398;&#20064;&#65292;&#19982;&#35813;&#27169;&#22411;&#20132;&#20114;&#30340;&#23545;&#25163;&#23601;&#19981;&#24212;&#20877;&#33021;&#22815;&#21028;&#26029;&#21453;&#23398;&#20064;&#30340;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#22312;&#38544;&#31169;&#39046;&#22495;&#65292;&#36825;&#34987;&#31216;&#20026;&#25104;&#21592;&#25512;&#26029;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#23545;&#21453;&#23398;&#20064;&#35774;&#32622;&#30340;&#35843;&#25972;&#65288;&#23548;&#33268;&#23427;&#20204;&#30340;&#8220;U-MIA&#8221;&#23545;&#24212;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23558;&#20854;&#20998;&#20026;&#8220;&#20154;&#21475;U-MIA&#8221;&#65292;&#20854;&#20013;&#21516;&#19968;&#25915;&#20987;&#32773;&#36866;&#29992;&#20110;&#25152;&#26377;&#31034;&#20363;&#65292;&#21644;&#8220;&#27599;&#20010;&#31034;&#20363;U-MIA&#8221;&#65292;&#20854;&#20013;&#20026;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21518;&#19968;&#31867;&#21035;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#20026;&#27599;&#20010;&#23454;&#20363;&#23450;&#21046;&#20854;&#25104;&#21592;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01218v1 Announce Type: new  Abstract: The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each ex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17472</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#31995;&#20132;&#20114;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fraud Detection with Binding Global and Local Relational Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#25972;&#20307;&#35270;&#35282;&#20013;&#32534;&#30721;&#33410;&#28857;&#20132;&#20114;&#21644;&#32858;&#21512;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#20855;&#26377;&#20986;&#33394;&#24207;&#21015;&#32534;&#30721;&#33021;&#21147;&#30340;Transformer&#32593;&#32476;&#22312;&#25991;&#29486;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#21644;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#21482;&#32534;&#30721;&#25972;&#20010;&#22270;&#30340;&#19968;&#20010;&#35270;&#35282;&#65292;&#32780;GNN&#32534;&#30721;&#20840;&#23616;&#29305;&#24449;&#65292;Transformer&#32593;&#32476;&#32534;&#30721;&#23616;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#20351;&#29992;&#21333;&#29420;&#32593;&#32476;&#32534;&#30721;&#24322;&#26500;&#22270;&#30340;&#20840;&#23616;&#20132;&#20114;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Relation-Aware GNN with transFormer&#65288;RAGFormer&#65289;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21516;&#26102;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#20013;&#12290;&#36825;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32593;&#32476;&#24212;&#29992;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;GAGA&#27169;&#22359;&#65292;&#20854;&#20013;&#27599;&#20010;Transformer&#23618;&#21518;&#38754;&#37117;&#36319;&#30528;&#19968;&#20010;&#36328;&#20851;&#31995;&#32858;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17472v1 Announce Type: cross  Abstract: Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation lay
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14035</link><description>&lt;p&gt;
&#22996;&#21592;&#20250;&#30340;&#26234;&#24935;&#65306;&#20174;&#22522;&#30784;&#27169;&#22411;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#30340;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14035
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#29305;&#23450;&#24212;&#29992;&#65292;&#20174;&#19994;&#32773;&#20204;&#19968;&#30452;&#22312;&#24320;&#21457;&#19987;&#38376;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;&#20026;&#20102;&#20139;&#21463;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#22909;&#22788;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#36335;&#24452;&#26159;&#23558;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#65292;&#21518;&#32773;&#36890;&#24120;&#26356;&#26377;&#25928;&#22320;&#25552;&#20379;&#26381;&#21153;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#25216;&#26415;&#21487;&#20197;&#22312;&#36825;&#37324;&#24212;&#29992;&#65292;&#20854;&#20013;&#24212;&#29992;&#27169;&#22411;&#23398;&#20250;&#27169;&#20223;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#23481;&#37327;&#19978;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#20248;&#21270;&#12290;&#27169;&#22411;&#29305;&#24449;&#19978;&#30340;&#36825;&#20123;&#24046;&#24322;&#23548;&#33268;&#20102;&#33976;&#39311;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21019;&#24314;&#19968;&#20010;&#25945;&#23398;&#22996;&#21592;&#20250;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;&#21644;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14035v1 Announce Type: cross  Abstract: Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22312;&#32447;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#21644;&#35299;&#20915;&#22238;&#24402;&#20013;&#24322;&#26041;&#24046;&#24615;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#21516;&#26102;&#39044;&#27979;&#36793;&#30028;&#65292;&#24182;&#33021;&#22815;&#21487;&#38752;&#22320;&#35206;&#30422;&#26032;&#38543;&#26426;&#36712;&#36857;&#30340;&#25972;&#20010;&#36335;&#24452;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#31934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#32780;&#19988;&#24448;&#24448;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09623</link><description>&lt;p&gt;
&#22810;&#20803;&#36712;&#36857;&#30340;&#31526;&#21512;&#24615;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformalized Adaptive Forecasting of Heterogeneous Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22312;&#32447;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#21644;&#35299;&#20915;&#22238;&#24402;&#20013;&#24322;&#26041;&#24046;&#24615;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#21516;&#26102;&#39044;&#27979;&#36793;&#30028;&#65292;&#24182;&#33021;&#22815;&#21487;&#38752;&#22320;&#35206;&#30422;&#26032;&#38543;&#26426;&#36712;&#36857;&#30340;&#25972;&#20010;&#36335;&#24452;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#31934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#32780;&#19988;&#24448;&#24448;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21516;&#26102;&#39044;&#27979;&#36793;&#30028;&#65292;&#20197;&#20855;&#26377;&#36275;&#22815;&#39640;&#30340;&#27010;&#29575;&#35206;&#30422;&#26032;&#38543;&#26426;&#36712;&#36857;&#30340;&#25972;&#20010;&#36335;&#24452;&#12290;&#37492;&#20110;&#22312;&#36816;&#21160;&#35268;&#21010;&#24212;&#29992;&#20013;&#38656;&#35201;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20854;&#20013;&#19981;&#21516;&#29289;&#20307;&#30340;&#34892;&#20026;&#21487;&#33021;&#26356;&#25110;&#26356;&#23569;&#21487;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#21333;&#20010;&#21644;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#20197;&#21450;&#35299;&#20915;&#22238;&#24402;&#20013;&#30340;&#24322;&#26041;&#24046;&#24615;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#34701;&#21512;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#26082;&#26377;&#21407;&#21017;&#24615;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#21448;&#26377;&#25928;&#65292;&#36890;&#24120;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09623v1 Announce Type: cross  Abstract: This paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability. Prompted by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression. This solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#26368;&#22823;&#29109;&#28304;&#20998;&#24067;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#20445;&#30041;&#19981;&#30830;&#23450;&#24615;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#28304;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20999;&#29255;-&#29926;&#30707;&#22374;&#26031;&#22374;&#36317;&#31163;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#25311;&#36827;&#34892;&#34913;&#37327;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#30340;&#27169;&#25311;&#22120;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#26356;&#39640;&#29109;&#30340;&#28304;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07808</link><description>&lt;p&gt;
Sourcerer: &#22522;&#20110;&#26679;&#26412;&#30340;&#26368;&#22823;&#29109;&#28304;&#20998;&#24067;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#26368;&#22823;&#29109;&#28304;&#20998;&#24067;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#20445;&#30041;&#19981;&#30830;&#23450;&#24615;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#28304;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20999;&#29255;-&#29926;&#30707;&#22374;&#26031;&#22374;&#36317;&#31163;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#25311;&#36827;&#34892;&#34913;&#37327;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#30340;&#27169;&#25311;&#22120;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#26356;&#39640;&#29109;&#30340;&#28304;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#24314;&#27169;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#20272;&#35745;&#19982;&#35266;&#27979;&#25968;&#25454;&#38598;&#19968;&#33268;&#30340;&#21442;&#25968;&#20998;&#24067;&#65292;&#34987;&#31216;&#20026;&#28304;&#20998;&#24067;&#20272;&#35745;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#26159;&#19981;&#36866;&#23450;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#19981;&#21516;&#30340;&#28304;&#20998;&#24067;&#21487;&#33021;&#20135;&#29983;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;&#20026;&#20102;&#22312;&#20247;&#22810;&#21516;&#26679;&#26377;&#25928;&#30340;&#28304;&#20013;&#20570;&#20986;&#26377;&#21407;&#21017;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21363;&#20248;&#20808;&#20445;&#30041;&#23613;&#21487;&#33021;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#22522;&#20110;&#26679;&#26412;&#65292;&#21033;&#29992;&#20999;&#29255;-&#29926;&#30707;&#22374;&#26031;&#22374;&#36317;&#31163;&#26469;&#34913;&#37327;&#25968;&#25454;&#38598;&#19982;&#27169;&#25311;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#26356;&#39640;&#29109;&#30340;&#28304;&#20998;&#24067;&#65292;&#32780;&#19981;&#29306;&#29298;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25512;&#26029;&#28304;&#20998;&#24067;...
&lt;/p&gt;
&lt;p&gt;
Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation. This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent simulations. To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations - and thus suitable for simulators with intractable likelihoods. We benchmark our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the simulations. Finally, to demonstrate the utility of our approach, we infer source distri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;&#65292;&#36890;&#36807;&#35266;&#23519;&#23454;&#35777;&#21457;&#29616;&#65292;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#65292;&#22797;&#21512;&#20219;&#21153;&#20013;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#38543;&#27169;&#22411;&#21464;&#22823;&#32780;&#22686;&#38271;&#65292;&#20445;&#25345;&#36164;&#28304;&#27604;&#20363;&#19981;&#21464;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#35813;&#36164;&#28304;&#27169;&#22411;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.05164</link><description>&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Resource Model For Neural Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;&#65292;&#36890;&#36807;&#35266;&#23519;&#23454;&#35777;&#21457;&#29616;&#65292;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#65292;&#22797;&#21512;&#20219;&#21153;&#20013;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#38543;&#27169;&#22411;&#21464;&#22823;&#32780;&#22686;&#38271;&#65292;&#20445;&#25345;&#36164;&#28304;&#27604;&#20363;&#19981;&#21464;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#35813;&#36164;&#28304;&#27169;&#22411;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#24459;&#25551;&#36848;&#20102;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#27169;&#22411;&#24615;&#33021;&#22914;&#20309;&#25552;&#39640;&#12290;&#21463;&#23454;&#35777;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#32553;&#25918;&#30340;&#36164;&#28304;&#27169;&#22411;&#12290;&#19968;&#20010;&#20219;&#21153;&#36890;&#24120;&#26159;&#22797;&#21512;&#20219;&#21153;&#65292;&#21487;&#20197;&#20998;&#35299;&#20026;&#35768;&#22810;&#23376;&#20219;&#21153;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#31454;&#20105;&#36164;&#28304;&#65288;&#20197;&#20998;&#37197;&#32473;&#23376;&#20219;&#21153;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#26469;&#34913;&#37327;&#65289;&#12290;&#22312;&#29609;&#20855;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65306;&#65288;1&#65289;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20854;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#12290;&#65288;2&#65289;&#24403;&#22797;&#21512;&#20219;&#21153;&#20013;&#23384;&#22312;&#22810;&#20010;&#23376;&#20219;&#21153;&#26102;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#22823;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#22343;&#21248;&#22686;&#38271;&#65292;&#20445;&#25345;&#33719;&#24471;&#36164;&#28304;&#30340;&#27604;&#20363;&#19981;&#21464;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#21457;&#29616;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#19968;&#33324;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;arXiv:2203.15556&#20013;&#25253;&#21578;&#30340;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#25105;&#20204;&#30456;&#20449;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#36164;&#28304;&#27010;&#24565;&#23558;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural scaling laws characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural scaling. A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural 
&lt;/p&gt;</description></item><item><title>BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04291</link><description>&lt;p&gt;
BiLLM: &#25512;&#21160;LLMs&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04291
&lt;/p&gt;
&lt;p&gt;
BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#36890;&#29992;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#24456;&#22823;&#30340;&#38656;&#27714;&#12290;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20108;&#20540;&#21270;&#21487;&#20197;&#23558;&#27169;&#22411;&#26435;&#37325;&#26497;&#22823;&#22320;&#20943;&#23569;&#21040;&#20165;1&#20301;&#65292;&#38477;&#20302;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#25216;&#26415;&#22312;&#36229;&#20302;&#20301;&#23485;&#19979;&#26080;&#27861;&#20445;&#25345;LLM&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BiLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLM&#23450;&#21046;&#30340;&#24320;&#21019;&#24615;&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#12290;&#22522;&#20110;LLMs&#30340;&#26435;&#37325;&#20998;&#24067;&#65292;BiLLM&#39318;&#20808;&#35782;&#21035;&#21644;&#32467;&#26500;&#36873;&#25321;&#37325;&#35201;&#30340;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#20108;&#20540;&#21270;&#27531;&#24046;&#36924;&#36817;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#21387;&#32553;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#38750;&#37325;&#35201;&#26435;&#37325;&#30340;&#38047;&#24418;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#20998;&#21106;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#22320;&#23558;&#23427;&#20204;&#20998;&#32452;&#21644;&#20108;&#20540;&#21270;&#12290;BiLLM&#39318;&#27425;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
&lt;/p&gt;</description></item><item><title>&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02746</link><description>&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#36275;&#20197;&#24212;&#23545;
&lt;/p&gt;
&lt;p&gt;
Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02746
&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#20351;&#29992;&#26631;&#20934; Gaussian &#36807;&#31243;&#65288;GP&#65289;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#21363;&#26631;&#20934; BO&#65292;&#22312;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#36825;&#31181;&#35266;&#24565;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110; Gaussian &#36807;&#31243;&#22312;&#21327;&#26041;&#24046;&#24314;&#27169;&#21644;&#20989;&#25968;&#20272;&#35745;&#20013;&#23545;&#39640;&#32500;&#36755;&#20837;&#30340;&#22256;&#38590;&#12290;&#34429;&#28982;&#36825;&#20123;&#25285;&#24551;&#30475;&#36215;&#26469;&#21512;&#29702;&#65292;&#20294;&#32570;&#20047;&#25903;&#25345;&#36825;&#31181;&#35266;&#28857;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#22238;&#24402;&#36827;&#34892;&#39640;&#32500;&#20248;&#21270;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26631;&#20934; GP &#30340;&#34920;&#29616;&#22987;&#32456;&#20301;&#20110;&#26368;&#20339;&#33539;&#22260;&#20869;&#65292;&#24448;&#24448;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#29616;&#26377; BO &#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#21051;&#26495;&#21360;&#35937;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934; GP &#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#39640;&#32500;&#30446;&#26631;&#20989;&#25968;&#30340;&#33021;&#21147;&#24378;&#22823;&#30340;&#20195;&#29702;&#12290;&#22312;&#27809;&#26377;&#24378;&#32467;&#26500;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#36827;&#34892; BO &#21487;&#20197;&#33719;&#24471;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#65292;&#32469;&#36807;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.02551</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#22120;&#19982;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;
&lt;/p&gt;
&lt;p&gt;
Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#65292;&#32469;&#36807;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#29616;&#20195;&#31574;&#30053;&#24448;&#24448;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#65292;&#20854;&#29305;&#28857;&#26159;&#40657;&#30418;&#24615;&#36136;&#22797;&#26434;&#65292;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#21487;&#33021;&#22312;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26080;&#38556;&#30861;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36712;&#36857;&#35268;&#21010;&#22120;&#19982;&#26032;&#39062;&#30340;&#33258;&#21160;&#35843;&#35856;&#20302;&#32423;&#21644;&#20851;&#33410;&#32423;&#25511;&#21046;&#31574;&#30053;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#24182;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#38454;&#27573;&#12290;&#36825;&#31181;&#26041;&#27861;&#32469;&#36807;&#20102;&#19982;&#35745;&#31639;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#26080;&#27169;&#22411;DRL&#20195;&#29702;&#22312;&#20851;&#33410;&#32423;&#25512;&#29702;&#20219;&#21153;&#31354;&#38388;&#20013;&#36827;&#34892;&#36895;&#24230;&#38480;&#21046;&#21644;&#26080;&#38556;&#30861;&#36816;&#21160;&#35268;&#21010;&#65292;&#28982;&#21518;&#23558;&#35813;&#35268;&#21010;&#36755;&#20837;&#21040;&#31283;&#20581;&#30340;&#23376;&#31995;&#32479;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#20013;&#65292;&#20135;&#29983;&#25152;&#38656;&#30340;&#25197;&#30697;&#65292;&#32780;&#26460;&#40515;&#25628;&#32034;&#20248;&#21270;&#65288;CSO&#65289;&#31639;&#27861;&#22686;&#24378;&#20102;&#25511;&#21046;&#22686;&#30410;&#20197;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety. To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low- and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment. This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks. First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning. This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65307;&#36890;&#36807;MAGBIG&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#37325;&#35201;&#24046;&#24322;&#65307;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#28040;&#38500;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.16092</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25918;&#22823;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#19988;&#20462;&#27491;&#24037;&#31243;&#21487;&#33021;&#26080;&#27861;&#24110;&#21161;&#24744;
&lt;/p&gt;
&lt;p&gt;
Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16092
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65307;&#36890;&#36807;MAGBIG&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#37325;&#35201;&#24046;&#24322;&#65307;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#28040;&#38500;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#24182;&#22240;&#27492;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#36890;&#36807;&#25913;&#21892;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#26356;&#22810;&#30340;&#31038;&#32676;&#29616;&#22312;&#21487;&#20197;&#35775;&#38382;&#36825;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#23558;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#19982;&#21333;&#35821;&#27169;&#22411;&#19968;&#26679;&#21463;&#21040;(&#24615;&#21035;)&#20559;&#35265;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#33258;&#28982;&#26399;&#26395;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#25552;&#20379;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#20294;&#20107;&#23454;&#24182;&#38750;&#22914;&#27492;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#27809;&#26377;&#24615;&#21035;&#20559;&#35265;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#30340;&#26032;&#22522;&#20934;MAGBIG&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;T2I&#27169;&#22411;&#26159;&#21542;&#36890;&#36807;MAGBIG&#25918;&#22823;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;&#25552;&#31034;&#35831;&#27714;&#29305;&#23450;&#32844;&#19994;&#25110;&#29305;&#36136;&#30340;&#20154;&#20687;&#22270;&#20687;(&#20351;&#29992;&#24418;&#23481;&#35789;)&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#34920;&#26126;&#27169;&#22411;&#20559;&#31163;&#20102;&#35268;&#33539;&#30340;&#20551;&#35774;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this kind of technology. Yet, as we will show, multilingual models suffer similarly from (gender) biases as monolingual models. Furthermore, the natural expectation is that these models will provide similar results across languages, but this is not the case and there are important differences between languages. Thus, we propose a novel benchmark MAGBIG intending to foster research in multilingual models without gender bias. We investigate whether multilingual T2I models magnify gender bias with MAGBIG. To this end, we use multilingual prompts requesting portrait images of persons of a certain occupation or trait (using adjectives). Our results show not only that models deviate from the normative assumption th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKSeed&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#21644;&#26377;&#38480;&#30340;&#38543;&#26426;&#31181;&#23376;&#38598;&#21512;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25104;&#26412;&#36739;&#20302;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#21487;&#20197;&#36827;&#34892;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.06353</link><description>&lt;p&gt;
&#20351;&#29992;&#36890;&#20449;&#25104;&#26412;&#20302;&#20110;18&#21315;&#23383;&#33410;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKSeed&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#21644;&#26377;&#38480;&#30340;&#38543;&#26426;&#31181;&#23376;&#38598;&#21512;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25104;&#26412;&#36739;&#20302;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#21487;&#20197;&#36827;&#34892;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38656;&#35201;&#36890;&#36807;&#32454;&#21270;&#35843;&#25972;&#26469;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#19981;&#29306;&#29298;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#32456;&#31471;&#35774;&#22791;&#19978;&#20016;&#23500;&#30340;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#32454;&#21270;&#35843;&#25972;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;LLM&#32852;&#37030;&#32454;&#21270;&#35843;&#25972;&#26041;&#27861;&#20381;&#36182;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#32454;&#21270;&#35843;&#25972;&#25216;&#26415;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#20840;&#21442;&#25968;&#35843;&#25972;&#21487;&#33021;&#36798;&#21040;&#30340;&#24615;&#33021;&#39640;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;LLM&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;FedKSeed&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#31181;&#23376;&#30340;&#26377;&#38480;&#38598;&#21512;&#36827;&#34892;&#38646;&#38454;&#20248;&#21270;&#12290;&#23427;&#26174;&#33879;&#38477;&#20302;&#20102;&#26381;&#21153;&#22120;&#21644;&#32456;&#31471;&#20043;&#38388;&#30340;&#20256;&#36755;&#35201;&#27714;&#65292;&#20165;&#38656;&#20256;&#36755;&#20960;&#20010;&#38543;&#26426;&#31181;&#23376;&#21644;&#26631;&#37327;&#26799;&#24230;&#65292;&#20165;&#21344;&#29992;&#20960;&#21315;&#23383;&#33410;&#30340;&#31354;&#38388;&#65292;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#33021;&#22815;&#36827;&#34892;&#20159;&#32423;LLM&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#27010;&#29575;&#24046;&#24322;&#21270;&#31181;&#23376;&#37319;&#26679;&#65292;&#20248;&#20808;&#32771;&#34385;&#19968;&#20123;&#31181;&#23376;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioriti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#31435;&#21306;&#21035;&#26469;&#24378;&#35843;&#37325;&#35201;&#21306;&#22495;, &#24182;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#23454;&#35777;&#35843;&#26597;&#34920;&#26126;&#36825;&#20123;&#21306;&#22495;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14434</link><description>&lt;p&gt;
&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transforming gradient-based techniques into interpretable methods. (arXiv:2401.14434v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#31435;&#21306;&#21035;&#26469;&#24378;&#35843;&#37325;&#35201;&#21306;&#22495;, &#24182;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#23454;&#35777;&#35843;&#26597;&#34920;&#26126;&#36825;&#20123;&#21306;&#22495;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;xAI&#25216;&#26415;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36890;&#24120;&#22312;&#35299;&#37322;&#19978;&#38754;&#20020;&#25361;&#25112;&#12290;&#22270;&#20687;&#25552;&#21462;&#30340;&#20687;&#32032;&#31561;&#36755;&#20837;&#29305;&#24449;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#24341;&#21457;&#20102;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#12290;&#38598;&#25104;&#26799;&#24230;&#65288;IG&#65289;&#31561;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#35299;&#37322;&#36716;&#21270;&#20026;&#22270;&#20687;&#26102;&#24120;&#24120;&#20135;&#29983;&#22823;&#37327;&#22122;&#38899;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26799;&#24230;&#20154;&#24037;&#20998;&#31163;&#65288;GAD&#65289;&#20316;&#20026;&#26799;&#24230;&#22522;&#20110;&#25216;&#26415;&#30340;&#25903;&#25345;&#26694;&#26550;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#24314;&#31435;&#31867;&#21035;&#20043;&#38388;&#30340;&#21306;&#21035;&#26469;&#24378;&#35843;&#26377;&#24433;&#21709;&#21147;&#30340;&#21306;&#22495;&#12290;GAD&#30340;&#26680;&#24515;&#26159;&#22312;&#21487;&#35270;&#21270;&#36807;&#31243;&#20013;&#38480;&#21046;&#20998;&#26512;&#33539;&#22260;&#65292;&#20174;&#32780;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#36890;&#36807;&#23545;&#34987;&#36974;&#25377;&#22270;&#20687;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#30830;&#23450;&#30340;&#21306;&#22495;&#30830;&#23454;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explication of Convolutional Neural Networks (CNN) through xAI techniques often poses challenges in interpretation. The inherent complexity of input features, notably pixels extracted from images, engenders complex correlations. Gradient-based methodologies, exemplified by Integrated Gradients (IG), effectively demonstrate the significance of these features. Nevertheless, the conversion of these explanations into images frequently yields considerable noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a supportive framework for gradient-based techniques. Its primary objective is to accentuate influential regions by establishing distinctions between classes. The essence of GAD is to limit the scope of analysis during visualization and, consequently reduce image noise. Empirical investigations involving occluded images have demonstrated that the identified regions through this methodology indeed play a pivotal role in facilitating class differentiation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12533</link><description>&lt;p&gt;
&#26377;&#25928;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;k&#20013;&#24515;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#20026;&#22522;&#30784;&#30340;&#32858;&#31867;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;k&#20013;&#24515;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#32972;&#26223;&#30693;&#35782;&#24314;&#27169;&#20026;&#24517;&#36830;&#65288;ML&#65289;&#21644;&#19981;&#36830;&#65288;CL&#65289;&#32422;&#26463;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21253;&#25324;k&#20013;&#24515;&#22312;&#20869;&#30340;&#32858;&#31867;&#38382;&#39064;&#26412;&#36136;&#19978;&#37117;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#21463;&#32422;&#26463;&#21464;&#20307;&#34987;&#35748;&#20026;&#21463;&#21040;&#26356;&#20005;&#37325;&#30340;&#36817;&#20284;&#21644;&#35745;&#31639;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#21453;&#25903;&#37197;&#38598;&#65292;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#25972;&#25968;&#24179;&#38754;&#21644;LP&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#32422;&#26463;k&#20013;&#24515;&#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#31454;&#20105;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#36817;&#20284;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#25277;&#35937;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20363;&#31243;&#20197;&#21450;&#21033;&#29992;&#23569;&#37327;&#30340;nun</title><link>http://arxiv.org/abs/2401.01974</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#27491;&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65306;&#20197;LLMs&#20026;&#31243;&#24207;&#21592;
&lt;/p&gt;
&lt;p&gt;
Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#25277;&#35937;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20363;&#31243;&#20197;&#21450;&#21033;&#29992;&#23569;&#37327;&#30340;nun
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25512;&#29702;&#20027;&#35201;&#37319;&#29992;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#25512;&#29702;&#12289;&#27867;&#21270;&#12289;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#25512;&#29702;&#20197;&#21450;&#35745;&#25968;&#26041;&#38754;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#25511;&#21046;&#22120;&#36827;&#34892;&#35270;&#35273;&#25512;&#29702;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35843;&#24230;&#19968;&#32452;(&#35270;&#35273;)&#24037;&#20855;&#26469;&#35299;&#20915;&#23376;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#12289;&#35270;&#35273; grounding &#21644;&#35270;&#39057;&#30340;&#26102;&#38388;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#24403;&#21069;&#24418;&#24335;&#19979;&#20005;&#37325;&#20381;&#36182;&#20110;&#22312;&#25552;&#31034;&#20013;&#38024;&#23545;&#20855;&#20307;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#20154;&#24037;&#35774;&#35745;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#36825;&#38656;&#35201;&#39640;&#25216;&#33021;&#31243;&#24207;&#21592;&#25237;&#20837;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#30340;&#20363;&#31243;&#65292;&#24182;&#21033;&#29992;&#23569;&#37327;&#30340;nu
&lt;/p&gt;
&lt;p&gt;
Visual reasoning is dominated by end-to-end neural networks scaled to billions of model parameters and training examples. However, even the largest models struggle with compositional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting. Visual reasoning with large language models (LLMs) as controllers can, in principle, address these limitations by decomposing the task and solving subtasks by orchestrating a set of (visual) tools. Recently, these models achieved great performance on tasks such as compositional visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in their current form, these models heavily rely on human engineering of in-context examples in the prompt, which are often dataset- and task-specific and require significant labor by highly skilled programmers. In this work, we present a framework that mitigates these issues by introducing spatially and temporally abstract routines and by leveraging a small nu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;</title><link>http://arxiv.org/abs/2312.05134</link><description>&lt;p&gt;
&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;k&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#65292;&#24050;&#25104;&#20026;&#36866;&#24212;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#22810;&#32452;&#21512;&#20316;&#31561;&#38656;&#27714;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;MDL&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20063;&#31216;&#20026;&#25353;&#38656;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#38024;&#23545;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#25509;&#36817;&#20110;&#65288;d+k&#65289;/&#949;^2&#65288;&#22312;&#26576;&#20123;&#23545;&#25968;&#22240;&#23376;&#20013;&#65289;&#65292;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19979;&#30028;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24605;&#24819;&#21644;&#29702;&#35770;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.15047</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;-&#65288;&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#65289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta- (out-of-context) learning in neural networks. (arXiv:2310.15047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Brown&#31561;&#20154;&#65288;2020&#65289;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#23454;&#39564;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20351;LLMs&#26356;&#23481;&#26131;&#8220;&#20869;&#21270;&#8221;&#25991;&#26412;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#35813;&#25991;&#26412;&#24191;&#27867;&#36866;&#29992;&#65288;&#20363;&#22914;&#30495;&#23454;&#38472;&#36848;&#25110;&#26435;&#23041;&#26469;&#28304;&#30340;&#25991;&#26412;&#65289;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#21512;&#25104;&#35745;&#31639;&#26426;&#35270;&#35273;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20551;&#35774;&#65292;&#35299;&#37322;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#30340;&#20986;&#29616;&#65306;&#19968;&#31181;&#26159;&#20381;&#36182;&#20110;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#21478;&#19968;&#31181;&#26159;&#26263;&#31034;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#30340;&#38544;&#21547;&#26799;&#24230;&#23545;&#40784;&#20559;&#24046;&#21487;&#33021;&#36127;&#36131;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#33021;&#24847;&#21619;&#30528;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#24605;&#32771;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/krasheni&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheni
&lt;/p&gt;</description></item><item><title>MINDE&#26159;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#25991;&#29486;&#20013;&#30340;&#20027;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09031</link><description>&lt;p&gt;
MINDE: &#20114;&#20449;&#24687;&#31070;&#32463;&#25193;&#25955;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
MINDE: Mutual Information Neural Diffusion Estimation. (arXiv:2310.09031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09031
&lt;/p&gt;
&lt;p&gt;
MINDE&#26159;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#25991;&#29486;&#20013;&#30340;&#20027;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Girsanov&#23450;&#29702;&#30340;&#21407;&#21019;&#35299;&#37322;&#65292;&#20801;&#35768;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20272;&#35745;&#20004;&#20010;&#23494;&#24230;&#20989;&#25968;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#65292;&#35813;&#20272;&#35745;&#26159;&#23427;&#20204;&#24471;&#20998;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#22815;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#30340;&#29109;&#12290;&#20511;&#21161;&#36825;&#26679;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27979;&#37327;MI&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#26041;&#21521;&#23637;&#24320;&#65306;&#19968;&#20010;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#36807;&#31243;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26469;&#33258;&#20110;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#21508;&#31181;&#21464;&#20307;&#36827;&#34892;&#24443;&#24213;&#30340;&#23454;&#39564;&#21327;&#35758;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#25991;&#29486;&#20013;&#30340;&#20027;&#35201;&#26367;&#20195;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20102;MI&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
In this work we present a new method for the estimation of Mutual Information (MI) between random variables. Our approach is based on an original interpretation of the Girsanov theorem, which allows us to use score-based diffusion models to estimate the Kullback Leibler divergence between two densities as a difference between their score functions. As a by-product, our method also enables the estimation of the entropy of random variables. Armed with such building blocks, we present a general recipe to measure MI, which unfolds in two directions: one uses conditional diffusion process, whereas the other uses joint diffusion processes that allow simultaneous modelling of two random variables. Our results, which derive from a thorough experimental protocol over all the variants of our approach, indicate that our method is more accurate than the main alternatives from the literature, especially for challenging distributions. Furthermore, our methods pass MI self-consistency tests, includin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#20855;&#26377;&#26080;&#38480;&#26041;&#24046;&#30340;&#19981;&#24688;&#24403;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#26469;&#23450;&#20041;&#38745;&#27490;&#20294;&#19981;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#29305;&#27530;&#30340;&#19981;&#24688;&#24403;&#26680;&#20989;&#25968;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.02877</link><description>&lt;p&gt;
&#26080;&#22343;&#20540;&#22238;&#24402;&#65306;&#19981;&#24688;&#24403;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21644;&#19981;&#24688;&#24403;&#26680;
&lt;/p&gt;
&lt;p&gt;
Stationarity without mean reversion: Improper Gaussian process regression and improper kernels. (arXiv:2310.02877v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#20855;&#26377;&#26080;&#38480;&#26041;&#24046;&#30340;&#19981;&#24688;&#24403;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#26469;&#23450;&#20041;&#38745;&#27490;&#20294;&#19981;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#29305;&#27530;&#30340;&#19981;&#24688;&#24403;&#26680;&#20989;&#25968;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24050;&#32463;&#24191;&#27867;&#27969;&#34892;&#12290;GP&#22238;&#24402;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#38745;&#27490;&#21327;&#26041;&#24046;&#20989;&#25968;&#26159;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#65288;&#38750;&#21608;&#26399;&#24615;&#30340;&#65289;&#38745;&#27490;&#21327;&#26041;&#24046;&#20989;&#25968;&#24635;&#26159;&#22343;&#20540;&#22238;&#24402;&#30340;&#65292;&#22240;&#27492;&#22312;&#24212;&#29992;&#20110;&#19981;&#36890;&#36807;&#21040;&#22266;&#23450;&#20840;&#23616;&#22343;&#20540;&#20540;&#30340;&#25968;&#25454;&#26102;&#21487;&#33021;&#34920;&#29616;&#20986;&#30149;&#24577;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#20855;&#26377;&#26080;&#38480;&#26041;&#24046;&#30340;&#19981;&#24688;&#24403;GP&#20808;&#39564;&#26469;&#23450;&#20041;&#38745;&#27490;&#20294;&#19981;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#26159;&#21487;&#33021;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22823;&#31867;&#21482;&#33021;&#22312;&#36825;&#31181;&#19981;&#24688;&#24403;&#30340;&#33539;&#22260;&#20869;&#23450;&#20041;&#30340;&#19981;&#24688;&#24403;&#26680;&#20989;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24179;&#28369;&#34892;&#36208;&#26680;&#65292;&#23427;&#20135;&#29983;&#26080;&#38480;&#24179;&#28369;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#19968;&#31867;&#19981;&#24688;&#24403;&#30340;Matern&#26680;&#65292;&#23427;&#21487;&#20197;&#34987;&#23450;&#20041;&#20026;&#20219;&#24847;&#25972;&#25968;j&#20493;&#21487;&#24494;&#12290;&#25152;&#24471;&#21040;&#30340;&#21518;&#39564;&#20998;&#24067;&#21487;&#20197;&#29992;&#35299;&#26512;&#30340;&#26041;&#24335;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GP) regression has gained substantial popularity in machine learning applications. The behavior of a GP regression depends on the choice of covariance function. Stationary covariance functions are favorite in machine learning applications. However, (non-periodic) stationary covariance functions are always mean reverting and can therefore exhibit pathological behavior when applied to data that does not relax to a fixed global mean value. In this paper, we show that it is possible to use improper GP prior with infinite variance to define processes that are stationary but not mean reverting. To this aim, we introduce a large class of improper kernels that can only be defined in this improper regime. Specifically, we introduce the Smooth Walk kernel, which produces infinitely smooth samples, and a family of improper Mat\'ern kernels, which can be defined to be $j$-times differentiable for any integer $j$. The resulting posterior distributions can be computed analyticall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#36941;&#21382;&#25968;&#25454;&#24207;&#21015;&#19978;&#35757;&#32451;&#26102;&#30340;&#26680;&#26497;&#38480;&#65292;&#21033;&#29992;&#25968;&#23398;&#26041;&#27861;&#23545;&#20854;&#28176;&#36817;&#29305;&#24615;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#35777;&#26126;&#20102;RNN&#25910;&#25947;&#21040;&#19982;&#38543;&#26426;&#20195;&#25968;&#26041;&#31243;&#30340;&#19981;&#21160;&#28857;&#32806;&#21512;&#30340;&#26080;&#31351;&#32500;ODE&#30340;&#35299;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#25913;&#36827;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.14555</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#36941;&#21382;&#25968;&#25454;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;&#26680;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data Sequences. (arXiv:2308.14555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#36941;&#21382;&#25968;&#25454;&#24207;&#21015;&#19978;&#35757;&#32451;&#26102;&#30340;&#26680;&#26497;&#38480;&#65292;&#21033;&#29992;&#25968;&#23398;&#26041;&#27861;&#23545;&#20854;&#28176;&#36817;&#29305;&#24615;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#35777;&#26126;&#20102;RNN&#25910;&#25947;&#21040;&#19982;&#38543;&#26426;&#20195;&#25968;&#26041;&#31243;&#30340;&#19981;&#21160;&#28857;&#32806;&#21512;&#30340;&#26080;&#31351;&#32500;ODE&#30340;&#35299;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#25913;&#36827;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#25968;&#23398;&#26041;&#27861;&#26469;&#25551;&#36848;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#28176;&#36817;&#29305;&#24615;&#65292;&#20854;&#20013;&#38544;&#34255;&#21333;&#20803;&#30340;&#25968;&#37327;&#12289;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#12289;&#38544;&#34255;&#29366;&#24577;&#30340;&#26356;&#26032;&#21644;&#35757;&#32451;&#27493;&#39588;&#21516;&#26102;&#36235;&#20110;&#26080;&#31351;&#22823;&#12290;&#23545;&#20110;&#20855;&#26377;&#31616;&#21270;&#26435;&#37325;&#30697;&#38453;&#30340;RNN&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RNN&#25910;&#25947;&#21040;&#19982;&#38543;&#26426;&#20195;&#25968;&#26041;&#31243;&#30340;&#19981;&#21160;&#28857;&#32806;&#21512;&#30340;&#26080;&#31351;&#32500;ODE&#30340;&#35299;&#12290;&#20998;&#26512;&#38656;&#35201;&#35299;&#20915;RNN&#25152;&#29305;&#26377;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;&#22312;&#20856;&#22411;&#30340;&#22343;&#22330;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#31163;&#25955;&#30340;&#26356;&#26032;&#37327;&#20026;$\mathcal{O}(\frac{1}{N})$&#65292;&#26356;&#26032;&#30340;&#27425;&#25968;&#20026;$\mathcal{O}(N)$&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#21487;&#20197;&#34920;&#31034;&#20026;&#36866;&#24403;ODE/PDE&#30340;Euler&#36924;&#36817;&#65292;&#24403;$N \rightarrow \infty$&#26102;&#25910;&#25947;&#21040;&#35813;ODE/PDE&#12290;&#28982;&#32780;&#65292;RNN&#30340;&#38544;&#34255;&#23618;&#26356;&#26032;&#20026;$\mathcal{O}(1)$&#12290;&#22240;&#27492;&#65292;RNN&#19981;&#33021;&#34920;&#31034;&#20026;ODE/PDE&#30340;&#31163;&#25955;&#21270;&#21644;&#26631;&#20934;&#22343;&#22330;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical methods are developed to characterize the asymptotics of recurrent neural networks (RNN) as the number of hidden units, data samples in the sequence, hidden state updates, and training steps simultaneously grow to infinity. In the case of an RNN with a simplified weight matrix, we prove the convergence of the RNN to the solution of an infinite-dimensional ODE coupled with the fixed point of a random algebraic equation. The analysis requires addressing several challenges which are unique to RNNs. In typical mean-field applications (e.g., feedforward neural networks), discrete updates are of magnitude $\mathcal{O}(\frac{1}{N})$ and the number of updates is $\mathcal{O}(N)$. Therefore, the system can be represented as an Euler approximation of an appropriate ODE/PDE, which it will converge to as $N \rightarrow \infty$. However, the RNN hidden layer updates are $\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization of an ODE/PDE and standard mean-field tec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#65292;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#27604;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31616;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12874</link><description>&lt;p&gt;
&#31616;&#26131;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#31616;&#21333;&#33258;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#65292;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#27604;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31616;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#29992;&#20110;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#31216;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#12290;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#20165;&#20351;&#29992;&#26597;&#35810;&#21644;&#38190;&#30340;&#20869;&#31215;&#65292;&#22240;&#27492;&#35777;&#26126;&#20102;&#20026;&#20102;&#33719;&#21462;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#25152;&#38656;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#65292;&#24182;&#19981;&#38656;&#35201;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#12290;&#36890;&#36807;&#22312;softmax&#27880;&#24847;&#21147;&#24471;&#20998;&#19978;&#23454;&#26045;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#33258;&#27880;&#24847;&#21147;&#22312;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#24352;&#25104;&#31354;&#38388;&#20013;&#21387;&#32553;&#20102;&#26469;&#33258;&#26597;&#35810;&#21644;&#38190;&#30340;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31616;&#26131;&#27880;&#24847;&#21147;&#26041;&#27861;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#23637;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#21644;&#26356;&#23569;&#22797;&#26434;&#24615;&#30340;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26102;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#27604;&#33258;&#27880;&#24847;&#26426;&#21046;&#25110;&#24191;&#27867;&#20351;&#29992;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention. Due to the fact that self attention only makes usage of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through implementing singular-value decomposition (SVD) on the softmax attention score, we further observe that the self attention compresses contribution from both queries and keys in the spanned space of the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than the self attention or the widely-used long short-ter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.11267</link><description>&lt;p&gt;
&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#21644;&#23545;&#25239;&#24615;&#31574;&#30053;&#26799;&#24230;&#22312;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RCMDP&#65289;&#26159;&#19968;&#20010;&#26368;&#36817;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#24314;&#27169;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#22312;&#36716;&#31227;&#21160;&#24577;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27169;&#25311;RCMDPs&#38656;&#35201;&#22522;&#20110;&#27599;&#20010;&#29366;&#24577;&#30340;&#20540;&#20272;&#35745;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#36825;&#31181;&#26041;&#27861;&#20043;&#21069;&#22312;&#40065;&#26834;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;RCPG&#65289;&#20013;&#20351;&#29992;&#36807;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#32780;&#19981;&#26159;&#20540;&#25110;&#32422;&#26463;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#20174;&#32780;&#20462;&#25913;RCPG&#12290;&#23545;&#25239;&#24615;RCPG&#20063;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#20294;&#26159;&#23558;&#20854;&#20316;&#20026;&#23545;&#25239;&#31574;&#30053;&#30452;&#25509;&#21644;&#22686;&#37327;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#25552;&#31034;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;&#30740;&#31350;&#12290;&#36890;&#36807;&#27979;&#37327;&#37326;&#22806;&#36234;&#29425;&#25552;&#31034;&#30340;&#21807;&#19968;&#29305;&#24449;&#21644;&#20027;&#35201;&#25915;&#20987;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#36234;&#29425;&#25552;&#31034;&#36234;&#26469;&#36234;&#22810;&#22320;&#20174;&#20844;&#20849;&#24179;&#21488;&#36716;&#31227;&#21040;&#31169;&#20154;&#24179;&#21488;&#65292;&#32473;LLM&#20379;&#24212;&#21830;&#22312;&#20027;&#21160;&#26816;&#27979;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.03825</link><description>&lt;p&gt;
&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#37326;&#22806;&#36234;&#29425;&#25552;&#31034;&#30340;&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. (arXiv:2308.03825v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#25552;&#31034;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;&#30740;&#31350;&#12290;&#36890;&#36807;&#27979;&#37327;&#37326;&#22806;&#36234;&#29425;&#25552;&#31034;&#30340;&#21807;&#19968;&#29305;&#24449;&#21644;&#20027;&#35201;&#25915;&#20987;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#36234;&#29425;&#25552;&#31034;&#36234;&#26469;&#36234;&#22810;&#22320;&#20174;&#20844;&#20849;&#24179;&#21488;&#36716;&#31227;&#21040;&#31169;&#20154;&#24179;&#21488;&#65292;&#32473;LLM&#20379;&#24212;&#21830;&#22312;&#20027;&#21160;&#26816;&#27979;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28389;&#29992;&#24050;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;LLM&#20379;&#24212;&#21830;&#30340;&#37325;&#35270;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20123;&#21162;&#21147;&#24050;&#32463;&#34987;&#20570;&#20986;&#26469;&#65292;&#20351;LLM&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21363;&#36234;&#29425;&#25552;&#31034;&#65292;&#24050;&#32463;&#20986;&#29616;&#24182;&#19981;&#26029;&#28436;&#21464;&#20197;&#32469;&#36807;&#20445;&#38556;&#24182;&#24341;&#21457;LLM&#20013;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#37326;&#22806;&#36234;&#29425;&#25552;&#31034;&#36827;&#34892;&#20102;&#27979;&#37327;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;6,387&#20010;&#22312;&#20845;&#20010;&#26376;&#20869;&#20174;&#22235;&#20010;&#24179;&#21488;&#19978;&#33719;&#24471;&#30340;&#25552;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#22522;&#20110;&#22270;&#30340;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36234;&#29425;&#25552;&#31034;&#30340;&#29420;&#29305;&#29305;&#24449;&#21450;&#20854;&#20027;&#35201;&#25915;&#20987;&#31574;&#30053;&#65292;&#22914;&#25552;&#31034;&#27880;&#20837;&#21644;&#26435;&#38480;&#25552;&#21319;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#36234;&#29425;&#25552;&#31034;&#36234;&#26469;&#36234;&#22810;&#22320;&#20174;&#20844;&#20849;&#24179;&#21488;&#36716;&#31227;&#21040;&#31169;&#20154;&#24179;&#21488;&#65292;&#32473;LLM&#20379;&#24212;&#21830;&#22312;&#20027;&#21160;&#26816;&#27979;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#36234;&#29425;&#25552;&#31034;&#21487;&#33021;&#36896;&#25104;&#30340;&#21361;&#23475;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;
&lt;/p&gt;
&lt;p&gt;
The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2307.11127</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#23454;&#29616;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#19979;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Control Methods by Density Matching under Implicit Endogeneitiy. (arXiv:2307.11127v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65288;SCMs&#65289;&#24050;&#25104;&#20026;&#27604;&#36739;&#26696;&#20363;&#30740;&#31350;&#20013;&#22240;&#26524;&#25512;&#26029;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;SCMs&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#35266;&#27979;&#32467;&#26524;&#30340;&#21152;&#26435;&#21644;&#26469;&#20272;&#35745;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#21512;&#25104;&#23545;&#29031;&#65288;SC&#65289;&#30340;&#20934;&#30830;&#24615;&#23545;&#20110;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;SC&#26435;&#37325;&#30340;&#20272;&#35745;&#25104;&#20026;&#20102;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#29616;&#26377;&#30340;SCMs&#23384;&#22312;&#19968;&#20010;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#65292;&#21363;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#19982;&#21453;&#20107;&#23454;&#32467;&#26524;&#27169;&#22411;&#20013;&#30340;&#35823;&#24046;&#39033;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#20250;&#23545;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#20135;&#29983;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#21305;&#37197;&#30340;&#26032;&#22411;SCM&#65292;&#20551;&#35774;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#21487;&#20197;&#29992;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#36817;&#20284;&#65288;&#21363;&#28151;&#21512;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matchi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37117;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.08929</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#21442;&#25968;&#21270;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On-the-fly machine learning for parametrization of the effective Hamiltonian. (arXiv:2307.08929v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37117;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#34987;&#24191;&#27867;&#29992;&#20110;&#39044;&#27979;&#21644;&#27169;&#25311;&#38081;&#30005;&#21644;&#24347;&#35947;&#38081;&#30005;&#20307;&#30340;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#22797;&#26434;&#65292;&#24456;&#38590;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;/&#25110;&#22797;&#26434;&#32452;&#20998;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#21442;&#25968;&#21270;&#26159;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23436;&#25104;&#30340;&#65292;&#27599;&#19968;&#27493;&#39044;&#27979;&#33021;&#37327;&#12289;&#21147;&#21644;&#24212;&#21147;&#20197;&#21450;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#19981;&#30830;&#23450;&#24615;&#36739;&#22823;&#26102;&#65292;&#25191;&#34892;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20197;&#37325;&#26032;&#35757;&#32451;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#35745;&#31639;&#20219;&#20309;&#25152;&#32771;&#34385;&#31995;&#32479;&#30340;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#21644;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#20197;BaTiO3&#21644;Pb(Sc,Ta)O3&#20026;&#20363;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The first-principles-based effective Hamiltonian is widely used to predict and simulate the properties of ferroelectrics and relaxor ferroelectrics. However, the parametrization method of the effective Hamiltonian is complicated and hardly can resolve the systems with complex interactions and/or complex components. Here, we developed an on-the-fly machine learning approach to parametrize the effective Hamiltonian based on Bayesian linear regression. The parametrization is completed in molecular dynamics simulations, with the energy, forces and stress predicted at each step along with their uncertainties. First-principles calculations are executed when the uncertainties are large to retrain the parameters. This approach provides a universal and automatic way to compute the effective Hamiltonian parameters for any considered systems including complex systems which previous methods can not handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness of this approach compa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20999;&#32447;&#27880;&#24847;&#24494;&#35843;&#26041;&#27861;&#65288;TAFT&#65289;&#65292;&#36890;&#36807;&#32447;&#24615;&#21270;&#21464;&#21387;&#22120;&#30340;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#21407;&#22987;&#38750;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#32452;&#21512;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#26426;&#22120;&#21435;&#38500;&#21644;&#24046;&#20998;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08122</link><description>&lt;p&gt;
&#20999;&#32447;&#21464;&#25442;&#22120;&#29992;&#20110;&#32452;&#21512;&#12289;&#38544;&#31169;&#21644;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Tangent Transformers for Composition, Privacy and Removal. (arXiv:2307.08122v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08122
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20999;&#32447;&#27880;&#24847;&#24494;&#35843;&#26041;&#27861;&#65288;TAFT&#65289;&#65292;&#36890;&#36807;&#32447;&#24615;&#21270;&#21464;&#21387;&#22120;&#30340;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#21407;&#22987;&#38750;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#32452;&#21512;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#26426;&#22120;&#21435;&#38500;&#21644;&#24046;&#20998;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20999;&#32447;&#20851;&#27880;&#24494;&#35843;&#65288;TAFT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#28857;&#21608;&#22260;&#35745;&#31639;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#26469;&#33719;&#24471;&#32447;&#24615;&#21270;&#21464;&#21387;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#32447;&#24615;&#21270;&#24471;&#21040;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;-&#21521;&#37327;&#31215;&#21487;&#20197;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#39640;&#25928;&#35745;&#31639;&#65292;&#23558;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#38477;&#20302;&#21040;&#19982;&#21407;&#22987;&#38750;&#32447;&#24615;&#27169;&#22411;&#30456;&#21516;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#36890;&#36807;TAFT&#36827;&#34892;&#24494;&#35843;&#30340;&#32467;&#26524;&#20999;&#32447;&#21464;&#25442;&#22120;&#21487;&#20197;&#19982;&#23545;&#21407;&#22987;&#38750;&#32447;&#24615;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#30456;&#24403;&#12290;&#30001;&#20110;&#20999;&#32447;&#21464;&#25442;&#22120;&#23545;&#20110;&#26032;&#30340;&#26435;&#20540;&#26159;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#32467;&#26524;&#24494;&#35843;&#25439;&#22833;&#26159;&#20984;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#20110;&#38750;&#32447;&#24615;&#24494;&#35843;&#65292;TAFT&#22312;&#27169;&#22411;&#32452;&#21512;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#26426;&#22120;&#21435;&#38500;&#21644;&#24046;&#20998;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#23436;&#20840;&#22312;&#32447;&#30340;IDS&#65292;&#26080;&#38656;&#31163;&#32447;&#23398;&#20064;&#25110;&#20154;&#24037;&#24178;&#39044;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#27969;&#37327;&#30340;&#26102;&#21464;&#29305;&#24615;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#36991;&#20813;&#20102;&#25968;&#25454;&#26631;&#35760;&#20013;&#30340;&#20154;&#20026;&#35823;&#24046;&#65292;&#20855;&#26377;&#20934;&#30830;&#29575;&#39640;&#12289;&#25104;&#26412;&#20302;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.13030</link><description>&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;&#20013;&#22522;&#20110;&#22312;&#32447;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things. (arXiv:2306.13030v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#23436;&#20840;&#22312;&#32447;&#30340;IDS&#65292;&#26080;&#38656;&#31163;&#32447;&#23398;&#20064;&#25110;&#20154;&#24037;&#24178;&#39044;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#27969;&#37327;&#30340;&#26102;&#21464;&#29305;&#24615;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#36991;&#20813;&#20102;&#25968;&#25454;&#26631;&#35760;&#20013;&#30340;&#20154;&#20026;&#35823;&#24046;&#65292;&#20855;&#26377;&#20934;&#30830;&#29575;&#39640;&#12289;&#25104;&#26412;&#20302;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#30417;&#30563;&#20837;&#20405;&#26816;&#27979;&#65288;SSID&#65289;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#23436;&#20840;&#22312;&#32447;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#25110;&#31163;&#32447;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20165;&#22522;&#20110;IDS&#26412;&#36523;&#30340;&#20915;&#31574;&#21644;&#22312;&#32447;&#20272;&#35745;&#30340;&#32479;&#35745;&#21487;&#20449;&#24230;&#65292;&#20351;&#29992;&#33258;&#32852;&#24819;&#28145;&#24230;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#26631;&#35760;&#20256;&#20837;&#30340;&#27969;&#37327;&#25968;&#25454;&#21253;&#12290;SSID&#26694;&#26550;&#20351;IDS&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#32593;&#32476;&#27969;&#37327;&#30340;&#26102;&#21464;&#29305;&#24615;&#65292;&#24182;&#28040;&#38500;&#20102;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#25968;&#25454;&#26631;&#35760;&#20013;&#30340;&#20154;&#20026;&#35823;&#24046;&#65292;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#21644;&#25968;&#25454;&#25910;&#38598;&#30340;&#20154;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#19982;&#30693;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20316;&#20026;&#20934;&#30830;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IDS&#65292;&#35813;SSID&#26694;&#26550;&#38750;&#24120;&#26377;&#29992;&#19988;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Machine Learning (ML) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning. The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness. The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection. This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection. The approach is experimentally evaluated on public datasets and compared with well-known ML models, showing that this SSID framework is very useful and advantageous as an accurate and online learning ML-based IDS for IoT systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#23398;&#20064;&#26576;&#20123;&#20809;&#28369;&#20989;&#25968;&#31867;&#65292;&#20855;&#26377;&#36866;&#24403;&#26435;&#37325;&#38480;&#21046;&#25110;&#27491;&#21017;&#21270;&#30340;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#20316;&#32773;&#36890;&#36807;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25104;&#21151;&#35777;&#26126;&#22312;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#26159;&#26368;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;&#21516;&#26102;&#20316;&#32773;&#36824;&#24471;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;Rademacher&#22797;&#26434;&#24230;&#30340;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.08321</link><description>&lt;p&gt;
&#21033;&#29992;&#36807;&#21442;&#25968;&#21270;&#30340;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Nonparametric regression using over-parameterized shallow ReLU neural networks. (arXiv:2306.08321v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#23398;&#20064;&#26576;&#20123;&#20809;&#28369;&#20989;&#25968;&#31867;&#65292;&#20855;&#26377;&#36866;&#24403;&#26435;&#37325;&#38480;&#21046;&#25110;&#27491;&#21017;&#21270;&#30340;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#20316;&#32773;&#36890;&#36807;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25104;&#21151;&#35777;&#26126;&#22312;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#26159;&#26368;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;&#21516;&#26102;&#20316;&#32773;&#36824;&#24471;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;Rademacher&#22797;&#26434;&#24230;&#30340;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26435;&#37325;&#24471;&#21040;&#21512;&#36866;&#30340;&#38480;&#21046;&#25110;&#27491;&#21017;&#21270;&#65292;&#37027;&#20040;&#21487;&#20197;&#35777;&#26126;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#26576;&#20123;&#20809;&#28369;&#20989;&#25968;&#31867;&#30340;&#23398;&#20064;&#26368;&#23567;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65288;&#26368;&#22810;&#23545;&#25968;&#22240;&#25968;&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26469;&#20272;&#35745;&#26410;&#30693;&#30340;$d$&#21464;&#37327;&#20989;&#25968;&#12290;&#20551;&#35774;&#22238;&#24402;&#20989;&#25968;&#26159;&#20174;&#20855;&#26377;&#20809;&#28369;&#24230;$\alpha &lt; (d+3)/2$&#30340;Holder&#31354;&#38388;&#25110;&#23545;&#24212;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#21270;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#65292;&#21518;&#32773;&#21487;&#20197;&#35270;&#20026;&#26080;&#38480;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20855;&#26377;&#26435;&#37325;&#26576;&#20123;&#33539;&#25968;&#32422;&#26463;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#26159;&#26368;&#23567;&#21270;&#26368;&#20248;&#30340;&#65292;&#22914;&#26524;&#32593;&#32476;&#23485;&#24230;&#36275;&#22815;&#22823;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;Rademacher&#22797;&#26434;&#24230;&#26080;&#20851;&#30340;&#19978;&#30028;&#65292;&#36825;&#21487;&#33021;&#26159;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized. Specifically, we consider the nonparametric regression of estimating an unknown $d$-variate function by using shallow ReLU neural networks. It is assumed that the regression function is from the H\"older space with smoothness $\alpha&lt;(d+3)/2$ or a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide neural network. In this setting, we prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal, if the network width is sufficiently large. As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09651</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25351;&#23548;&#26377;&#21161;&#20110;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#33021;&#21147;&#36229;&#32676;&#30340;&#25945;&#24072;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#35753;&#23398;&#29983;&#27700;&#24179;&#24471;&#21040;&#25552;&#21319;&#65292;&#36825;&#20984;&#26174;&#20102;&#24403;&#21069;&#25945;&#24072;&#22521;&#35757;&#23454;&#36341;&#21644;&#26377;&#25928;&#30693;&#35782;&#20256;&#25480;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#25945;&#24072;&#22521;&#35757;&#36807;&#31243;&#30340;&#25351;&#23548;&#25928;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#33976;&#39311;&#25928;&#24212;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#23545;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#22909;&#25945;&#24072;&#24456;&#37325;&#35201;&#65288;LGTM&#65289;&#30340;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#23558;&#33976;&#39311;&#25928;&#24212;&#32435;&#20837;&#25945;&#24072;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#21487;&#33021;&#25552;&#21319;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;LGTM&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03568</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#35270;&#39057;&#35328;&#35821;&#24773;&#24863;&#35782;&#21035;&#30340;&#30690;&#37327;&#37327;&#21270;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A vector quantized masked autoencoder for audiovisual speech emotion recognition. (arXiv:2305.03568v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#38754;&#30417;&#30563;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#38899;&#35270;&#39057;&#35328;&#35821;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#65292;&#24050;&#25104;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65288;VQ-MAE-AV&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#21407;&#22987;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#22788;&#29702;&#30340;&#22810;&#27169;&#24577;MAEs&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22522;&#20110;&#20004;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;VoxCeleb2&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#38899;&#35270;&#39057;SER&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fully-supervised models have been shown to be effective for audiovisual speech emotion recognition (SER), the limited availability of labeled data remains a major challenge in the field. To address this issue, self-supervised learning approaches, such as masked autoencoders (MAEs), have gained popularity as potential solutions. In this paper, we propose the VQ-MAE-AV model, a vector quantized MAE specifically designed for audiovisual speech self-supervised representation learning. Unlike existing multimodal MAEs that rely on the processing of the raw audiovisual speech data, the proposed method employs a self-supervised paradigm based on discrete audio and visual speech representations learned by two pre-trained vector quantized variational autoencoders. Experimental results show that the proposed approach, which is pre-trained on the VoxCeleb2 database and fine-tuned on standard emotional audiovisual speech datasets, outperforms the state-of-the-art audiovisual SER methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2304.03427</link><description>&lt;p&gt;
&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#25991;&#23398;&#32773;&#22312;&#30740;&#31350;&#21382;&#21490;&#12289;&#23447;&#25945;&#21644;&#31038;&#20250;&#25919;&#27835;&#32467;&#26500;&#31561;&#26041;&#38754;&#32463;&#24120;&#20381;&#36182;&#20110;&#21476;&#20195;&#25163;&#31295;&#12290;&#34429;&#28982;OCR&#25216;&#26415;&#21487;&#20197;&#23558;&#36825;&#20123;&#23453;&#36149;&#25163;&#31295;&#25968;&#23383;&#21270;&#65292;&#20294;&#22810;&#25968;&#25163;&#31295;&#22240;&#30952;&#25439;&#32780;&#36807;&#26102;&#65292;OCR&#31243;&#24207;&#27809;&#21150;&#27861;&#35782;&#21035;&#32763;&#39029;&#30340;&#34394;&#28129;&#25110;&#27745;&#28173;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#20998;&#20026;&#22235;&#20010;&#37096;&#20998;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#21407;&#22987;&#34255;&#25991;&#30005;&#23376;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#29305;&#24449;&#24037;&#31243;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#20004;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#26694;&#8212;&#8212;&#19968;&#32452;&#21305;&#37197;&#30340;&#29609;&#20855;&#25968;&#25454;&#21644;&#19968;&#32452;&#21305;&#37197;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;Transformer&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#26469;&#25191;&#34892;&#25340;&#20889;&#26657;&#27491;&#20219;&#21153;&#12290;&#26681;&#25454;&#25439;&#22833;&#21644;&#23383;&#31526;&#38169;&#35823;&#29575;&#65292;&#25105;&#20204;&#30340;Transformer + &#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#27604;&#20854;&#20182;&#24120;&#29992;&#30340;&#25340;&#20889;&#26657;&#27491;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholars in the humanities rely heavily on ancient manuscripts to study history, religion, and socio-political structures in the past. Many efforts have been devoted to digitizing these precious manuscripts using OCR technology, but most manuscripts were blemished over the centuries so that an Optical Character Recognition (OCR) program cannot be expected to capture faded graphs and stains on pages. This work presents a neural spelling correction model built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy output. This paper is divided into four sections: dataset, model architecture, training and analysis. First, we feature-engineered our raw Tibetan etext corpus into two sets of structured data frames -- a set of paired toy data and a set of paired real data. Then, we implemented a Confidence Score mechanism into the Transformer architecture to perform spelling correction tasks. According to the Loss and Character Error Rate, our Transformer + Confidence score mechani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#65292;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#22312;Lipschitz&#24120;&#25968;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#20013;&#35782;&#21035;&#20986;&#20102;&#26126;&#26174;&#30340;&#21452;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.10886</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#30340;&#19968;&#20123;&#22522;&#26412;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions. (arXiv:2302.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#65292;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#22312;Lipschitz&#24120;&#25968;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#20013;&#35782;&#21035;&#20986;&#20102;&#26126;&#26174;&#30340;&#21452;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#36830;&#32493;&#24615;&#26159;&#20219;&#20309;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#31616;&#21333;&#20294;&#20851;&#38190;&#30340;&#21151;&#33021;&#24615;&#36136;&#65292;&#23427;&#22788;&#20110;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#32791;&#23613;&#26368;&#31616;&#21333;&#21644;&#26368;&#19968;&#33324;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#30340;&#26497;&#38480;&#65292;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65288;&#21363;&#65292;&#20307;&#31995;&#32467;&#26500;&#12289;&#25439;&#22833;&#12289;&#20248;&#21270;&#22120;&#12289;&#26631;&#31614;&#22122;&#38899;&#31561;&#65289;&#65292;&#34429;&#28982;&#36825;&#19968;&#36873;&#25321;&#20027;&#35201;&#26159;&#21463;&#35745;&#31639;&#38590;&#24230;&#32467;&#26524;&#30340;&#39537;&#21160;&#65292;&#20294;&#23427;&#20063;&#38750;&#24120;&#20016;&#23500;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20960;&#20010;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#36824;&#34917;&#20805;&#20102;&#36866;&#24403;&#30340;&#29702;&#35770;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lipschitz continuity is a simple yet crucial functional property of any predictive model for it lies at the core of the model's robustness, generalisation, as well as adversarial vulnerability. Our aim is to thoroughly investigate and characterise the Lipschitz behaviour of the functions realised by neural networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, losses, optimisers, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. Although motivated primarily by computational hardness results, this choice nevertheless turns out to be rather resourceful and sheds light on several fundamental and intriguing traits of the Lipschitz continuity of neural network functions, which we also supplement with suitable theoretical arguments. As a highlight of this investigation, we identify a striking double descent trend in both upper and lower bounds to the Lipschitz constant with in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#30340;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#26469;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20998;&#31867;&#22120;&#22312;&#26410;&#30693;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#19978;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01792</link><description>&lt;p&gt;
&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification by sparse additive models. (arXiv:2212.01792v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01792
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#30340;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#26469;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20998;&#31867;&#22120;&#22312;&#26410;&#30693;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#19978;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#20110;&#20998;&#31867;&#30340;&#38750;&#21442;&#25968;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#65288;SpAM&#65289;&#12290;SpAM&#20998;&#31867;&#22120;&#30340;&#35774;&#35745;&#22522;&#20110;&#26368;&#23567;&#21270;logistic&#25439;&#22833;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#23637;&#24320;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#26356;&#19968;&#33324;&#30340;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#65288;&#20363;&#22914;&#65292;&#20613;&#37324;&#21494;&#25110;&#23567;&#27874;&#65289;&#12290;&#25152;&#24471;&#30340;&#20998;&#31867;&#22120;&#23545;&#26410;&#30693;&#30340;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#20855;&#26377;&#22266;&#26377;&#30340;&#33258;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#31232;&#30095;&#32452;&#21463;&#38480;&#29305;&#24449;&#20540;&#26465;&#20214;&#19979;&#65292;&#31232;&#30095;&#32452;Lasso&#20998;&#31867;&#22120;&#22312;&#25972;&#20010;&#35299;&#26512;&#12289;Sobolev&#21644;Besov&#31867;&#33539;&#22260;&#20869;&#20960;&#20046;&#26159;&#26368;&#23567;&#21270;&#26497;&#23567;&#65288;&#21152;&#19978;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#32780;&#31232;&#30095;&#32452;Slope&#20998;&#31867;&#22120;&#22312;&#31232;&#30095;&#21644;&#36866;&#24230;&#31264;&#23494;&#35774;&#23450;&#19979;&#36798;&#21040;&#20102;&#30830;&#20999;&#30340;&#26368;&#23567;&#21270;&#26497;&#23567;&#38454;&#25968;&#65288;&#19981;&#21547;&#39069;&#22806;&#30340;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#35813;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#22312;&#23454;&#38469;&#25968;&#25454;&#20363;&#23376;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider (nonparametric) sparse additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso and more general sparse group Slope-type penalties on the coefficients of univariate components' expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifiers are inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition the sparse group Lasso classifier is nearly-minimax (up to log-factors) within the entire range of analytic, Sobolev and Besov classes while the sparse group Slope classifier achieves the exact minimax order (without the extra log-factors) for sparse and moderately dense setups. The performance of the proposed classifier is illustrated on the real-data example.
&lt;/p&gt;</description></item><item><title>&#26497;&#21270;&#32534;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#33021;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#38656;&#25554;&#34917;&#65292;&#35753;&#20915;&#31574;&#26641;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.01905</link><description>&lt;p&gt;
&#26497;&#21270;&#32534;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#20540;&#20998;&#31867;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Polar Encoding: A Simple Baseline Approach for Classification with Missing Values. (arXiv:2210.01905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01905
&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#32534;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#33021;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#38656;&#25554;&#34917;&#65292;&#35753;&#20915;&#31574;&#26641;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26497;&#21270;&#32534;&#30721;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20998;&#31867;&#21644;&#25968;&#20540;&#22411;$[0,1]$&#20540;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#24456;&#22909;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#31639;&#27861;&#37197;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#65292;&#38750;&#24120;&#31616;&#21333;&#26131;&#29992;&#24182;&#19988;&#24615;&#33021;&#33391;&#22909;&#12290;&#19982;&#29616;&#26377;&#30340;&#32570;&#22833;&#25351;&#31034;&#26041;&#27861;&#19981;&#21516;&#65292;&#26497;&#21270;&#32534;&#30721;&#19981;&#38656;&#35201;&#25554;&#34917;&#65292;&#30830;&#20445;&#32570;&#22833;&#20540;&#19982;&#38750;&#32570;&#22833;&#20540;&#31561;&#36317;&#31163;&#65292;&#35753;&#20915;&#31574;&#26641;&#31639;&#27861;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#20998;&#21106;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#8220;&#23646;&#24615;&#20013;&#21253;&#21547;&#32570;&#22833;&#24615;&#8221;&#65288;MIA&#65289;&#30340;&#23454;&#38469;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20998;&#31867;&#21644;$[0,1]$&#20540;&#23646;&#24615;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#21333;&#19968;&#23646;&#24615;&#31867;&#22411;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23545;&#24212;&#20110;&#32463;&#20856;&#30340;&#37325;&#24515;&#22352;&#26631;&#27010;&#24565;&#65292;&#36825;&#25552;&#20379;&#20102;&#26497;&#21270;&#32534;&#30721;&#30340;&#27169;&#31946;&#21270;&#24418;&#24335;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose polar encoding, a representation of categorical and numerical $[0,1]$-valued attributes with missing values to be used in a classification context. We argue that this is a good baseline approach, because it can be used with any classification algorithm, preserves missingness information, is very simple to apply and offers good performance. In particular, unlike the existing missing-indicator approach, it does not require imputation, ensures that missing values are equidistant from non-missing values, and lets decision tree algorithms choose how to split missing values, thereby providing a practical realisation of the "missingness incorporated in attributes" (MIA) proposal. Furthermore, we show that categorical and $[0,1]$-valued attributes can be viewed as special cases of a single attribute type, corresponding to the classical concept of barycentric coordinates, and that this offers a natural interpretation of polar encoding as a fuzzified form of one-hot encoding. With an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#26377;&#25928;&#22320;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#25490;&#21517;&#20449;&#24687;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20805;&#20998;&#21033;&#29992;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#36824;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35299;&#20915;&#20102;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2109.03459</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#20110;&#25490;&#21517;&#33976;&#39311;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#26377;&#25928;&#22320;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#25490;&#21517;&#20449;&#24687;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20805;&#20998;&#21033;&#29992;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#36824;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35299;&#20915;&#20102;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#35757;&#32451;&#20805;&#20998;&#30340;&#22823;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#23567;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#32780;&#35328;&#65292;&#23427;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26368;&#36817;&#65292;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#65288;RRD&#65289;&#34920;&#26126;&#65292;&#22312;&#25512;&#33616;&#21015;&#34920;&#20013;&#33976;&#39311;&#25490;&#21517;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;1&#65289;&#23427;&#26410;&#20805;&#20998;&#21033;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#20351;&#24471;&#35757;&#32451;&#25928;&#29575;&#19981;&#39640;&#65307;2&#65289;&#23427;&#21482;&#33976;&#39311;&#29992;&#25143;&#20391;&#30340;&#25490;&#21517;&#20449;&#24687;&#65292;&#22312;&#31232;&#30095;&#30340;&#38544;&#24335;&#21453;&#39304;&#19979;&#25552;&#20379;&#30340;&#35270;&#35282;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#21363;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20915;&#23450;&#35201;&#33976;&#39311;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list significantly improves the performance. However, the method still has limitations in that 1) it does not fully utilize the prediction errors of the student model, which makes the training not fully efficient, and 2) it only distills the user-side ranking information, which provides an insufficient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more efficient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be disti
&lt;/p&gt;</description></item></channel></rss>