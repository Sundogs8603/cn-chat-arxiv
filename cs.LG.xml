<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;Primal-Attention&#65292;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#25237;&#24433;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19798</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#34920;&#36798;&#30340;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation. (arXiv:2305.19798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;Primal-Attention&#65292;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#25237;&#24433;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19968;&#31995;&#21015;&#24037;&#20316;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35270;&#20026;&#26680;&#26426;&#22120;&#65292;&#20197;&#27492;&#26469;&#29702;&#35299;&#21644;&#25913;&#36827;Transformers&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#23545;&#31216;&#26680;&#32780;&#19981;&#36866;&#29992;&#20110;&#19981;&#23545;&#31216;&#30340;&#33258;&#27880;&#24847;&#21147;&#65292;&#23548;&#33268;&#20102;&#29702;&#35770;&#21644;&#23454;&#38469;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;KSVD&#65289;&#26469;&#34920;&#36798;&#21644;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;&#36890;&#36807;&#19981;&#23545;&#31216;KSVD&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#65306;i&#65289;&#33258;&#27880;&#24847;&#21147;&#30340;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#34920;&#36798;&#65292;&#20854;&#20013;&#20248;&#21270;&#30446;&#26631;&#34987;&#36716;&#21270;&#20026;&#26368;&#22823;&#21270;&#27880;&#24847;&#21147;&#36755;&#20986;&#20013;&#30340;&#25237;&#24433;&#26041;&#24046;&#65307;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;-Primal-Attention&#65292;&#36890;&#36807;KSVD&#30340;&#21407;&#22987;&#34920;&#36798;&#24335;&#36991;&#20813;&#20102;&#22312;&#23545;&#20598;&#20013;&#26174;&#24335;&#35745;&#31639;&#26680;&#30697;&#38453;&#30340;&#38382;&#39064;&#65307;iii&#65289;&#36890;&#36807;KKT&#26465;&#20214;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Primal-Attention&#30340;&#29366;&#24577;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#19982;&#20043;&#21069;&#30340;&#23545;&#20598;&#31639;&#27861;&#20855;&#26377;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26144;&#23556;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#25968;&#25454;&#65292;&#24182;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19779</link><description>&lt;p&gt;
&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#20197;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#65306;&#20197;&#32943;&#23612;&#20122;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26144;&#23556;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#25968;&#25454;&#65292;&#24182;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#30142;&#30149;&#26144;&#23556;&#26159;&#20844;&#20849;&#21355;&#29983;&#21644;&#30142;&#30149;&#30417;&#27979;&#20013;&#22522;&#26412;&#30340;&#25919;&#31574;&#20449;&#24687;&#24037;&#20855;&#65292;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#24403;&#22788;&#29702;&#21306;&#22495;&#25968;&#25454;&#65292;&#22914;&#34892;&#25919;&#21306;&#21010;&#21333;&#20301;&#65288;&#20363;&#22914;&#21439;&#25110;&#30465;&#65289;&#30340;&#32858;&#21512;&#25968;&#25454;&#26102;&#65292;&#24120;&#29992;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#21306;&#22495;&#21333;&#20803;&#30340;&#30456;&#37051;&#32467;&#26500;&#20197;&#32771;&#34385;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#30142;&#30149;&#30417;&#27979;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#38543;&#26102;&#38388;&#36319;&#36394;&#30142;&#30149;&#32467;&#26524;&#65292;&#20294;&#22312;&#21361;&#26426;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#25919;&#27835;&#21464;&#21270;&#23548;&#33268;&#34892;&#25919;&#36793;&#30028;&#26356;&#25913;&#65289;&#65292;&#36825;&#23558;&#24102;&#26469;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#23454;&#29992;&#21644;&#26131;&#20110;&#23454;&#26045;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20381;&#36182;&#20110;&#32452;&#21512;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#21644;&#20840;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#29616;&#26377;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE) &#24037;&#20316;&#19978;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#32858;&#21512;VAE(aggVAE)&#20307;&#31995;&#32467;&#26500;&#21487;&#29992;&#20110;&#22312;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#22788;&#29702;&#25968;&#25454;&#65292;&#20197;&#26144;&#23556;&#32943;&#23612;&#20122;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36830;&#32493;&#30340;&#26041;&#24335;&#32771;&#34385;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30456;&#37051;&#24615;&#20551;&#35774;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based disease mapping remains a fundamental policy-informing tool in public health and disease surveillance with hierarchical Bayesian models being the current state-of-the-art approach. When working with areal data, e.g. aggregates at the administrative unit level such as district or province, routinely used models rely on the adjacency structure of areal units to account for spatial correlations. The goal of disease surveillance systems is to track disease outcomes over time, but this provides challenging in situations of crises, such as political changes, leading to changes of administrative boundaries. Kenya is an example of such country. Moreover, adjacency-based approach ignores the continuous nature of spatial processes and cannot solve the change-of-support problem, i.e. when administrative boundaries change. We present a novel, practical, and easy to implement solution relying on a methodology combining deep generative modelling and fully Bayesian inference. We build on 
&lt;/p&gt;</description></item><item><title>J-UNIWARD &#26159;&#19968;&#31181;&#23558;&#31192;&#23494;&#20449;&#24687;&#38544;&#34255;&#22312;JPEG&#22270;&#20687;&#20013;&#30340;&#38544;&#20889;&#26041;&#27861;&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#20854;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010; off-by-one &#38169;&#35823;&#65292;&#20351;&#19968;&#20123;&#22270;&#20687;&#22359;&#34987;&#39640;&#20272;&#65292;&#21478;&#19968;&#20123;&#34987;&#20302;&#20272;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#29992;&#20110;&#26816;&#27979;&#27492;&#31181;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.19776</link><description>&lt;p&gt;
J-UNIWARD&#20013;&#30340;&#19968;&#20010;&#23454;&#29616;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Off-By-One Implementation Error in J-UNIWARD. (arXiv:2305.19776v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19776
&lt;/p&gt;
&lt;p&gt;
J-UNIWARD &#26159;&#19968;&#31181;&#23558;&#31192;&#23494;&#20449;&#24687;&#38544;&#34255;&#22312;JPEG&#22270;&#20687;&#20013;&#30340;&#38544;&#20889;&#26041;&#27861;&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#20854;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010; off-by-one &#38169;&#35823;&#65292;&#20351;&#19968;&#20123;&#22270;&#20687;&#22359;&#34987;&#39640;&#20272;&#65292;&#21478;&#19968;&#20123;&#34987;&#20302;&#20272;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#29992;&#20110;&#26816;&#27979;&#27492;&#31181;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
J-UNIWARD&#26159;&#19968;&#31181;&#23558;&#31192;&#23494;&#20449;&#24687;&#38544;&#34255;&#22312;JPEG&#30422;&#26495;&#22270;&#20687;&#20013;&#30340;&#27969;&#34892;&#38544;&#20889;&#26415;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;J-UNIWARD&#26088;&#22312;&#23884;&#20837;&#21040;&#32441;&#29702;&#22270;&#20687;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#30340;&#21464;&#21270;&#38590;&#20197;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;J-UNIWARD&#39318;&#20808;&#20026;&#27599;&#20010;DCT&#31995;&#25968;&#20998;&#37197;&#19968;&#20010;&#23884;&#20837;&#25104;&#26412;&#65292;&#35813;&#25104;&#26412;&#22522;&#20110;&#22270;&#20687;&#30340;&#23567;&#27874;&#27531;&#24046;&#35745;&#31639;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#32534;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23884;&#20837;&#25152;&#38656;&#30340;&#26377;&#25928;&#36733;&#33655;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;&#26356;&#25913;&#19968;&#20010;DCT&#31995;&#25968;&#20250;&#24433;&#21709;23x23&#20010;&#23567;&#27874;&#31995;&#25968;&#31383;&#21475;&#12290;&#20026;&#20102;&#21152;&#36895;&#25104;&#26412;&#22270;&#30340;&#35745;&#31639;&#65292;&#21407;&#22987;&#23454;&#29616;&#39044;&#20808;&#35745;&#31639;&#23567;&#27874;&#27531;&#24046;&#65292;&#28982;&#21518;&#23545;&#20110;&#27599;&#20010;&#26356;&#25913;&#30340;DCT&#31995;&#25968;&#65292;&#32771;&#34385;&#19968;&#20010;23x23&#30340;&#23567;&#27874;&#27531;&#24046;&#31383;&#21475;&#12290;&#28982;&#32780;&#65292;&#35813;&#23454;&#29616;&#38169;&#35823;&#22320;&#23558;&#31383;&#21475;&#20559;&#31227;&#20102;&#19968;&#20010;&#20687;&#32032;&#21040;&#21491;&#19979;&#26041;&#12290;&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20010;off-by-one&#38169;&#35823;&#23545;&#29983;&#25104;&#30340;&#25104;&#26412;&#22270;&#30340;&#24433;&#21709;&#12290;&#19968;&#20123;&#22270;&#20687;&#22359;&#34987;&#39640;&#20272;&#65292;&#32780;&#20854;&#20182;&#22270;&#20687;&#22359;&#21017;&#34987;&#20302;&#20272;&#65292;&#20294;&#24046;&#24322;&#30456;&#23545;&#36739;&#23567;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35828;&#26126;&#22914;&#20309;&#26816;&#27979;&#20351;&#29992;&#24102;&#26377;&#20559;&#31227;&#38169;&#35823;&#30340;J-UNIWARD&#38544;&#34255;&#30340;&#38544;&#20889;&#26415;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
J-UNIWARD is a popular steganography method for hiding secret messages in JPEG cover images. As a content-adaptive method, J-UNIWARD aims to embed into textured image regions where changes are difficult to detect. To this end, J-UNIWARD first assigns to each DCT coefficient an embedding cost calculated based on the image's Wavelet residual, and then uses a coding method that minimizes the cost while embedding the desired payload. Changing one DCT coefficient affects a 23x23 window of Wavelet coefficients. To speed up the costmap computation, the original implementation pre-computes the Wavelet residual and then considers per changed DCT coefficient a 23x23 window of the Wavelet residual. However, the implementation accesses a window accidentally shifted by one pixel to the bottom right. In this report, we evaluate the effect of this off-by-one error on the resulting costmaps. Some image blocks are over-priced while other image blocks are under-priced, but the difference is relatively s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#23567;&#30340;&#31070;&#32463;&#32467;&#26500;&#12289;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#39033;&#30340;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#20462;&#25913;&#31574;&#30053;&#21644;&#23545;&#22270;&#20687;&#36827;&#34892;&#20302;&#31209;&#34920;&#31034;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#25552;&#39640;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#27169;&#31946;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22270;&#20687;&#21435;&#27169;&#31946;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19774</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#23376;&#35299;&#20915;&#25104;&#20687;&#21453;&#38382;&#39064;&#26102;&#23384;&#22312;&#30340;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
Ambiguity in solving imaging inverse problems with deep learning based operators. (arXiv:2305.19774v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#23567;&#30340;&#31070;&#32463;&#32467;&#26500;&#12289;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#39033;&#30340;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#20462;&#25913;&#31574;&#30053;&#21644;&#23545;&#22270;&#20687;&#36827;&#34892;&#20302;&#31209;&#34920;&#31034;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#25552;&#39640;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#27169;&#31946;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22270;&#20687;&#21435;&#27169;&#31946;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20316;&#22270;&#20687;&#21435;&#27169;&#31946;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#38750;&#24120;&#31934;&#30830;&#22320;&#24674;&#22797;&#22270;&#20687;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#22270;&#20687;&#21435;&#27169;&#31946;&#22312;&#25968;&#23398;&#19978;&#34987;&#24314;&#27169;&#20026;&#21453;&#38382;&#39064;&#65292;&#24403;&#22122;&#22768;&#24433;&#21709;&#25968;&#25454;&#26102;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#38590;&#20197;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#21435;&#27169;&#31946;&#26102;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#20854;&#23545;&#22122;&#22768;&#21644;&#20854;&#20182;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#24182;&#20135;&#29983;&#36739;&#24046;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#24403;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26102;&#65292;&#32593;&#32476;&#19981;&#19968;&#23450;&#32771;&#34385;&#24213;&#23618;&#25104;&#20687;&#38382;&#39064;&#30340;&#25968;&#23383;&#21270;&#24418;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23558;&#22270;&#20687;&#21435;&#27169;&#31946;&#30340;&#31283;&#23450;&#24615;&#25552;&#39640;&#32780;&#19981;&#33267;&#20110;&#22833;&#21435;&#22826;&#22810;&#20934;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#38750;&#24120;&#23567;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#36825;&#23558;&#20943;&#23569;&#35757;&#32451;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#21516;&#26102;&#28385;&#36275;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#19981;&#20250;&#26497;&#22823;&#22320;&#25918;&#22823;&#35745;&#31639;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#39033;&#20197;&#31283;&#23450;&#35757;&#32451;&#24182;&#38477;&#20302;&#23545;&#25200;&#21160;&#30340;&#25935;&#24863;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#20302;&#31209;&#34920;&#31034;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#20026;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#25552;&#20379;&#26356;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#36215;&#28857;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#27169;&#31946;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large convolutional neural networks have been widely used as tools for image deblurring, because of their ability in restoring images very precisely. It is well known that image deblurring is mathematically modeled as an ill-posed inverse problem and its solution is difficult to approximate when noise affects the data. Really, one limitation of neural networks for deblurring is their sensitivity to noise and other perturbations, which can lead to instability and produce poor reconstructions. In addition, networks do not necessarily take into account the numerical formulation of the underlying imaging problem, when trained end-to-end. In this paper, we propose some strategies to improve stability without losing to much accuracy to deblur images with deep-learning based methods. First, we suggest a very small neural architecture, which reduces the execution time for training, satisfying a green AI need, and does not extremely amplify noise in the computed image. Second, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#27604;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.19770</link><description>&lt;p&gt;
&#8220;&#36136;&#37327;&#36827;/&#36136;&#37327;&#20986;&#65306;&#35780;&#20272;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#30340;&#25968;&#25454;&#36136;&#37327;&#8221;
&lt;/p&gt;
&lt;p&gt;
Quality In / Quality Out: Assessing Data quality in an Anomaly Detection Benchmark. (arXiv:2305.19770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#27604;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#25110;&#33258;&#21160;&#39550;&#39542;&#32593;&#32476;&#34987;&#26399;&#26395;&#25104;&#20026;&#26410;&#26469;&#20114;&#32852;&#32593;&#20013;&#26497;&#23500;&#25361;&#25112;&#21644;&#38656;&#27714;&#30340;&#26032;&#22411;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22788;&#29702;&#22797;&#26434;&#24615;&#30340;&#20851;&#38190;&#22312;&#20110;&#36890;&#36807;&#26368;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#25191;&#34892;&#32593;&#32476;&#20248;&#21270;&#21644;&#25925;&#38556;&#24674;&#22797;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#31038;&#21306;&#20381;&#36182;&#20110;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25216;&#26415;&#30340;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#22909;&#22351;&#21462;&#20915;&#20110;&#23427;&#25152;&#25311;&#21512;&#30340;&#25968;&#25454;&#12290;&#20026;&#30740;&#31350;&#30446;&#30340;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65288;&#23545;&#30740;&#31350;&#32467;&#26524;&#21644;&#26041;&#21521;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65289;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#40664;&#35748;&#20855;&#26377;&#33391;&#22909;&#36136;&#37327;&#30340;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#21516;&#19968;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;UGR'16&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20110;&#27969;&#37327;&#30340;&#23454;&#26102;&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#30456;&#23545;&#36739;&#23567;&#30340;&#20462;&#25913;&#65292;&#27604;&#25152;&#32771;&#34385;&#30340;&#20855;&#20307;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26356;&#26174;&#33879;&#22320;&#24433;&#21709;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#26681;&#26412;&#21407;&#22240;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous or self-driving networks are expected to provide a solution to the myriad of extremely demanding new applications in the Future Internet. The key to handle complexity is to perform tasks like network optimization and failure recovery with minimal human supervision. For this purpose, the community relies on the development of new Machine Learning (ML) models and techniques. However, ML can only be as good as the data it is fitted with. Datasets provided to the community as benchmarks for research purposes, which have a relevant impact in research findings and directions, are often assumed to be of good quality by default. In this paper, we show that relatively minor modifications on the same benchmark dataset (UGR'16, a flow-based real-traffic dataset for anomaly detection) cause significantly more impact on model performance than the specific ML technique considered. To understand this finding, we contribute a methodology to investigate the root causes for those differences,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#38899;&#39057;&#38382;&#31572;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#21442;&#32771;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;Clotho-AQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26377;&#26126;&#26174;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.19769</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38899;&#39057;&#38382;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Methods For Audio Question Answering. (arXiv:2305.19769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#38899;&#39057;&#38382;&#31572;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#21442;&#32771;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;Clotho-AQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26377;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#38382;&#31572;(AQA)&#26159;&#23545;&#20110;&#38899;&#39057;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#38382;&#39064;&#26102;&#65292;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#38899;&#39057;&#38382;&#31572;&#20219;&#21153;&#12290;&#33258;&#25105;&#27880;&#24847;&#21147;&#23618;&#21487;&#20197;&#25552;&#21462;&#24378;&#22823;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;&#20132;&#21449;&#27880;&#24847;&#21147;&#23558;&#19982;&#25991;&#26412;&#29305;&#24449;&#30456;&#20851;&#30340;&#38899;&#39057;&#29305;&#24449;&#26144;&#23556;&#21040;&#31572;&#26696;&#20013;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#27169;&#22411;&#37117;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;Clotho-AQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#29992;&#20110;&#20108;&#36827;&#21046;&#26159;/&#21542;&#38382;&#39064;&#21644;&#21333;&#35789;&#22238;&#31572;&#38382;&#39064;&#12290;&#32467;&#26524;&#28165;&#26970;&#22320;&#26174;&#31034;&#20986;&#30456;&#23545;&#20110;&#21407;&#22987;&#35770;&#25991;&#20013;&#30340;&#21442;&#32771;&#26041;&#27861;&#25913;&#36827;&#12290;&#22312;&#26159;/&#21542;&#20108;&#36827;&#21046;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#21442;&#32771;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20174;62.7&#65285;&#25552;&#39640;&#21040;&#20102;68.3&#65285;&#12290;&#23545;&#20110;&#21333;&#35789;&#31572;&#26696;&#22810;&#31867;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20998;&#21035;&#20135;&#29983;&#20102;57.9&#65285;&#21644;99.8&#65285;&#30340;top-1&#21644;top-5&#20934;&#30830;&#29575;&#65292;&#30456;&#23545;&#20110;&#21442;&#32771;&#27169;&#22411;&#30340;54.2&#65285;&#21644;93.7&#65285;&#26377;&#20102;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio question answering (AQA) is the task of producing natural language answers when a system is provided with audio and natural language questions. In this paper, we propose neural network architectures based on self-attention and cross-attention for the AQA task. The self-attention layers extract powerful audio and textual representations. The cross-attention maps audio features that are relevant to the textual features to produce answers. All our models are trained on the recently proposed Clotho-AQA dataset for both binary yes/no questions and single-word answer questions. Our results clearly show improvement over the reference method reported in the original paper. On the yes/no binary classification task, our proposed model achieves an accuracy of 68.3% compared to 62.7% in the reference model. For the single-word answers multiclass classifier, our model produces a top-1 and top-5 accuracy of 57.9% and 99.8% compared to 54.2% and 93.7% in the reference model respectively. We fur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;TDA&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#65292;&#20174;&#20013;&#21457;&#29616;&#20010;&#21035;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#24120;&#34987;&#22122;&#22768;&#25513;&#30422;&#65292;TDA&#21482;&#33021;&#29992;&#20110;&#35299;&#37322;&#23545;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#31283;&#23450;&#12289;&#29420;&#31435;&#20110;&#20854;&#20182;&#22122;&#22768;&#22240;&#32032;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.19765</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Perspective On Training Data Attribution. (arXiv:2305.19765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;TDA&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#65292;&#20174;&#20013;&#21457;&#29616;&#20010;&#21035;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#24120;&#34987;&#22122;&#22768;&#25513;&#30422;&#65292;TDA&#21482;&#33021;&#29992;&#20110;&#35299;&#37322;&#23545;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#31283;&#23450;&#12289;&#29420;&#31435;&#20110;&#20854;&#20182;&#22122;&#22768;&#22240;&#32032;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#65288;TDA&#65289;&#25216;&#26415;&#21487;&#25214;&#20986;&#24433;&#21709;&#27169;&#22411;&#23545;&#25152;&#20851;&#27880;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#39044;&#27979;&#30340;&#37325;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20272;&#35745;&#20943;&#23569;&#25110;&#22686;&#21152;&#29305;&#23450;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;TDA&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#65292;&#23558;&#23398;&#20064;&#30340;&#27169;&#22411;&#35270;&#20026;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;TDA&#20272;&#35745;&#20316;&#20026;&#38543;&#26426;&#21464;&#37327;&#12290;&#20174;&#36825;&#20010;&#26032;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#21457;&#29616;&#20010;&#21035;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#24120;&#20250;&#34987;&#27169;&#22411;&#21021;&#22987;&#21270;&#21644;SGD&#25209;&#27425;&#32452;&#21512;&#20135;&#29983;&#30340;&#22122;&#22768;&#25513;&#30422;&#12290;&#22522;&#20110;&#36825;&#20010;&#21457;&#29616;&#65292;&#25105;&#20204;&#35748;&#20026;TDA&#21482;&#33021;&#21487;&#38752;&#22320;&#29992;&#20110;&#35299;&#37322;&#19968;&#20123;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#26159;&#31283;&#23450;&#30340;&#65292;&#29420;&#31435;&#20110;&#20854;&#20182;&#22122;&#22768;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#29420;&#31435;&#20110;&#22122;&#22768;&#30340;&#35757;&#32451;-&#27979;&#35797;&#25968;&#25454;&#26159;&#24456;&#32597;&#35265;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training data attribution (TDA) techniques find influential training data for the model's prediction on the test data of interest. They approximate the impact of down- or up-weighting a particular training sample. While conceptually useful, they are hardly applicable in practice, particularly because of their sensitivity to different model initialisation. In this paper, we introduce a Bayesian perspective on the TDA task, where the learned model is treated as a Bayesian posterior and the TDA estimates as random variables. From this novel viewpoint, we observe that the influence of an individual training sample is often overshadowed by the noise stemming from model initialisation and SGD batch composition. Based on this observation, we argue that TDA can only be reliably used for explaining model predictions that are consistently influenced by certain training data, independent of other noise factors. Our experiments demonstrate the rarity of such noise-independent training-test data pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402; Metropolis-Hastings &#21629;&#21517;&#28216;&#25103; (RMHNG) &#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#31526;&#21495;&#20986;&#29616;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19761</link><description>&lt;p&gt;
&#36882;&#24402; Metropolis-Hastings &#21629;&#21517;&#28216;&#25103;&#65306;&#22522;&#20110;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#31526;&#21495;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models. (arXiv:2305.19761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402; Metropolis-Hastings &#21629;&#21517;&#28216;&#25103; (RMHNG) &#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#31526;&#21495;&#20986;&#29616;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#29702;&#20154;&#32676;&#20307;&#20013;&#30740;&#31350;&#31526;&#21495;&#30340;&#20986;&#29616;&#21644;&#32039;&#24613;&#36890;&#20449;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#27169;&#22411;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#21442;&#19982;&#21508;&#31181;&#35821;&#35328;&#28216;&#25103;&#12290;&#20854;&#20013;&#65292;Metropolis-Hastings &#21629;&#21517;&#28216;&#25103; (MHNG) &#20855;&#26377;&#19968;&#20010;&#26174;&#33879;&#30340;&#25968;&#23398;&#23646;&#24615;&#65306;&#36890;&#36807; MHNG &#30340;&#31526;&#21495;&#20986;&#29616;&#34987;&#35777;&#26126;&#26159;&#20998;&#25955;&#24335;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#26159;&#20195;&#29702;&#20154;&#20849;&#20139;&#30340;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#25552;&#20986;&#30340; MHNG &#20165;&#22312;&#20004;&#20010;&#20195;&#29702;&#20154;&#22330;&#26223;&#20013;&#20351;&#29992;&#12290;&#26412;&#25991;&#23558; MHNG &#25193;&#23637;&#21040; N &#20195;&#29702;&#20154;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#65306;(1) &#25105;&#20204;&#23558;&#36882;&#24402; Metropolis-Hastings &#21629;&#21517;&#28216;&#25103; (RMHNG) &#25552;&#20986;&#20026; MHNG &#30340; N &#20195;&#29702;&#20154;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126; RMHNG &#26159;&#19968;&#31181;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110; MHNG&#65292;&#29992;&#20110;&#20195;&#29702;&#20154;&#20849;&#20139;&#30340;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65307;(2) &#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102; RMHNG &#30340;&#24615;&#33021;&#23454;&#35777;&#35780;&#20272;&#65292;&#20351;&#22810;&#20010;&#20195;&#29702;&#21487;&#20197;&#24320;&#21457;&#21644;&#20849;&#20139;&#31526;&#21495;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558; RMHNG &#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the studies on symbol emergence and emergent communication in a population of agents, a computational model was employed in which agents participate in various language games. Among these, the Metropolis-Hastings naming game (MHNG) possesses a notable mathematical property: symbol emergence through MHNG is proven to be a decentralized Bayesian inference of representations shared by the agents. However, the previously proposed MHNG is limited to a two-agent scenario. This paper extends MHNG to an N-agent scenario. The main contributions of this paper are twofold: (1) we propose the recursive Metropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and demonstrate that RMHNG is an approximate Bayesian inference method for the posterior distribution over a latent variable shared by agents, similar to MHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and real image data, enabling multiple agents to develop and share a symbol system. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#19968;&#31181;&#21517;&#20026;&#8220;&#38567;&#36947;&#8221;&#30340;&#29616;&#35937;&#65292;&#23427;&#22312;&#32593;&#32476;&#30340;&#35757;&#32451;&#26089;&#26399;&#23601;&#20986;&#29616;&#65292;&#24182;&#19988;&#23545;&#26368;&#32456;&#30340;&#25968;&#25454;&#34920;&#31034;&#36215;&#21040;&#20102;&#21387;&#32553;&#20316;&#29992;&#12290;&#20854;&#20013;&#65292;&#21021;&#22987;&#23618;&#26500;&#24314;&#20102;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#24418;&#24335;&#65292;&#32780;&#38543;&#21518;&#30340;&#23618;&#21387;&#32553;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#24182;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#38567;&#36947;&#20250;&#21066;&#24369;&#32593;&#32476;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19753</link><description>&lt;p&gt;
&#38567;&#36947;&#25928;&#24212;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25968;&#25454;&#34920;&#31034;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
The Tunnel Effect: Building Data Representations in Deep Neural Networks. (arXiv:2305.19753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#19968;&#31181;&#21517;&#20026;&#8220;&#38567;&#36947;&#8221;&#30340;&#29616;&#35937;&#65292;&#23427;&#22312;&#32593;&#32476;&#30340;&#35757;&#32451;&#26089;&#26399;&#23601;&#20986;&#29616;&#65292;&#24182;&#19988;&#23545;&#26368;&#32456;&#30340;&#25968;&#25454;&#34920;&#31034;&#36215;&#21040;&#20102;&#21387;&#32553;&#20316;&#29992;&#12290;&#20854;&#20013;&#65292;&#21021;&#22987;&#23618;&#26500;&#24314;&#20102;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#24418;&#24335;&#65292;&#32780;&#38543;&#21518;&#30340;&#23618;&#21387;&#32553;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#24182;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#38567;&#36947;&#20250;&#21066;&#24369;&#32593;&#32476;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#38395;&#21517;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#26356;&#28145;&#30340;&#32593;&#32476;&#38544;&#21547;&#30528;&#23545;&#26356;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#29992;&#20110;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#32593;&#32476;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#37096;&#20998;&#65292;&#23427;&#20204;&#23545;&#26368;&#32456;&#25968;&#25454;&#34920;&#31034;&#30340;&#24418;&#25104;&#36215;&#30528;&#19981;&#21516;&#30340;&#20316;&#29992;&#65306;&#26368;&#21021;&#30340;&#23618;&#26500;&#24314;&#20102;&#32447;&#24615;&#21487;&#20998;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#32780;&#38543;&#21518;&#30340;&#23618;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#38567;&#36947;&#8221;&#65289;&#21017;&#21387;&#32553;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#65292;&#24182;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#19981;&#22823;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#38567;&#36947;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20250;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26089;&#26399;&#20986;&#29616;&#65292;&#38567;&#36947;&#30340;&#28145;&#24230;&#21462;&#20915;&#20110;&#32593;&#32476;&#23481;&#37327;&#19982;&#20219;&#21153;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#38567;&#36947;&#20250;&#21066;&#24369;&#32593;&#32476;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely known for their remarkable effectiveness across various tasks, with the consensus that deeper networks implicitly learn more complex data representations. This paper shows that sufficiently deep networks trained for supervised image classification split into two distinct parts that contribute to the resulting data representations differently. The initial layers create linearly-separable representations, while the subsequent layers, which we refer to as \textit{the tunnel}, compress these representations and have a minimal impact on the overall performance. We explore the tunnel's behavior through comprehensive empirical studies, highlighting that it emerges early in the training process. Its depth depends on the relation between the network's capacity and task complexity. Furthermore, we show that the tunnel degrades out-of-distribution generalization and discuss its implications for continual learning.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21487;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;&#30340;&#21021;&#22987;&#20998;&#24067;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#36716;&#31227;&#27010;&#29575;&#29575;&#65292;&#21516;&#26102;&#22312;&#20808;&#39564;&#36807;&#31243;&#30340;&#26102;&#38388;&#26080;&#20851;&#29575;&#19978;&#20063;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19744</link><description>&lt;p&gt;
&#31070;&#32463;&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Markov Jump Processes. (arXiv:2305.19744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19744
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21487;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;&#30340;&#21021;&#22987;&#20998;&#24067;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#36716;&#31227;&#27010;&#29575;&#29575;&#65292;&#21516;&#26102;&#22312;&#20808;&#39564;&#36807;&#31243;&#30340;&#26102;&#38388;&#26080;&#20851;&#29575;&#19978;&#20063;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;&#26159;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#25512;&#26029;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#25110;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#36827;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#21487;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#35266;&#27979;&#25968;&#25454;&#30340;&#31070;&#32463;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#65292;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;&#30340;&#21021;&#22987;&#20998;&#24067;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#36716;&#31227;&#27010;&#29575;&#29575;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20808;&#39564;&#36807;&#31243;&#30340;&#26102;&#38388;&#26080;&#20851;&#29575;&#21017;&#20687;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#19968;&#26679;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#26159;&#20174;&#30495;&#23454;&#30340;&#39532;&#23572;&#21487;&#22827;&#36339;&#36291;&#36807;&#31243;&#12289;&#23454;&#39564;&#24615;&#24320;&#20851;&#31163;&#23376;&#36890;&#36947;&#25968;&#25454;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#37319;&#26679;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov jump processes are continuous-time stochastic processes with a wide range of applications in both natural and social sciences. Despite their widespread use, inference in these models is highly non-trivial and typically proceeds via either Monte Carlo or expectation-maximization methods. In this work we introduce an alternative, variational inference algorithm for Markov jump processes which relies on neural ordinary differential equations, and is trainable via back-propagation. Our methodology learns neural, continuous-time representations of the observed data, that are used to approximate the initial distribution and time-dependent transition probability rates of the posterior Markov jump process. The time-independent rates of the prior process are in contrast trained akin to generative adversarial networks. We test our approach on synthetic data sampled from ground-truth Markov jump processes, experimental switching ion channel data and molecular dynamics simulations. Source c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#26032;&#39062;&#21487;&#38752;&#30340;&#33073;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#65292;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#30340;&#37325;&#21472;&#26377;&#38480;&#21306;&#22495;&#65292;&#28982;&#21518;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.19742</link><description>&lt;p&gt;
&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#21487;&#38752;&#33073;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reliable Off-Policy Learning for Dosage Combinations. (arXiv:2305.19742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#26032;&#39062;&#21487;&#38752;&#30340;&#33073;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#65292;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#30340;&#37325;&#21472;&#26377;&#38480;&#21306;&#22495;&#65292;&#28982;&#21518;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#23398;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#30284;&#30151;&#27835;&#30103;&#25110;&#21361;&#37325;&#25252;&#29702;&#65292;&#36890;&#24120;&#24517;&#39035;&#23545;&#21058;&#37327;&#32452;&#21512;&#36827;&#34892;&#36873;&#25321;&#65292;&#21363;&#22810;&#31181;&#36830;&#32493;&#27835;&#30103;&#12290;&#29616;&#26377;&#30340;&#36825;&#39033;&#20219;&#21153;&#30340;&#24037;&#20316;&#24050;&#32463;&#29420;&#31435;&#22320;&#24314;&#27169;&#20102;&#22810;&#31181;&#27835;&#30103;&#30340;&#25928;&#26524;&#65292;&#32780;&#20272;&#35745;&#32852;&#21512;&#25928;&#26524;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#38754;&#20020;&#30528;&#38750;&#24179;&#20961;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#21487;&#38752;&#33073;&#26426;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#20989;&#25968;&#65292;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#30456;&#20851;&#21058;&#37327;&#30340;&#32852;&#21512;&#25928;&#24212;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#37327;&#20272;&#35745;&#24191;&#20041;&#20542;&#21521;&#24471;&#20998;&#65292;&#20197;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#37325;&#21472;&#26377;&#38480;&#30340;&#21306;&#22495;&#12290;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#30830;&#20445;&#21487;&#38752;&#22320;&#20272;&#35745;&#31574;&#30053;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments independently, while estimating the joint effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy val
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#22270;&#20449;&#21495;&#20998;&#24067;&#31354;&#38388;&#20013;&#23884;&#20837;&#22270;&#26469;&#23450;&#20041;&#22270;&#30340;&#24179;&#22343;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;Wasserstein&#24230;&#37327;&#34913;&#37327;&#22270;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19738</link><description>&lt;p&gt;
&#22270;&#30340;Bures-Wasserstein&#24179;&#22343;&#20540;
&lt;/p&gt;
&lt;p&gt;
Bures-Wasserstein Means of Graphs. (arXiv:2305.19738v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#22270;&#20449;&#21495;&#20998;&#24067;&#31354;&#38388;&#20013;&#23884;&#20837;&#22270;&#26469;&#23450;&#20041;&#22270;&#30340;&#24179;&#22343;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;Wasserstein&#24230;&#37327;&#34913;&#37327;&#22270;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#65292;&#25214;&#21040;&#37319;&#26679;&#25968;&#25454;&#30340;&#24179;&#22343;&#20540;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#26679;&#26412;&#20026;&#22270;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#23450;&#20041;&#24179;&#22343;&#20540;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#22270;&#20449;&#21495;&#20998;&#24067;&#31354;&#38388;&#20013;&#23884;&#20837;&#22270;&#26469;&#23450;&#20041;&#22270;&#30340;&#24179;&#22343;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;Wasserstein&#24230;&#37327;&#34913;&#37327;&#22270;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#22312;&#27492;&#23884;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#24179;&#22343;&#20540;&#65292;&#25105;&#20204;&#21487;&#20197;&#24674;&#22797;&#20445;&#30041;&#32467;&#26500;&#20449;&#24687;&#30340;&#24179;&#22343;&#22270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26032;&#30340;&#22270;&#24179;&#22343;&#20540;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#26469;&#35745;&#31639;&#23427;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#32467;&#26500;&#21270;&#22270;&#30340;k-means&#32858;&#31867;&#12289;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#20998;&#31867;&#20197;&#21450;&#22810;&#23618;&#22270;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;p&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the mean of sampled data is a fundamental task in machine learning and statistics. However, in cases where the data samples are graph objects, defining a mean is an inherently difficult task. We propose a novel framework for defining a graph mean via embeddings in the space of smooth graph signal distributions, where graph similarity can be measured using the Wasserstein metric. By finding a mean in this embedding space, we can recover a mean graph that preserves structural information. We establish the existence and uniqueness of the novel graph mean, and provide an iterative algorithm for computing it. To highlight the potential of our framework as a valuable tool for practical applications in machine learning, it is evaluated on various tasks, including k-means clustering of structured graphs, classification of functional brain networks, and semi-supervised node classification in multi-layer graphs. Our experimental results demonstrate that our approach achieves consistent p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#38169;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;APPRAISER&#65292;&#23427;&#37319;&#29992;&#36817;&#20284;&#35823;&#24046;&#36827;&#34892;DNN&#25925;&#38556;&#23481;&#24525;&#24615;&#20998;&#26512;&#12290;APPRAISER&#25552;&#20379;&#20102;&#25104;&#21315;&#19978;&#19975;&#20493;&#30340;&#35780;&#20272;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20998;&#26512;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19733</link><description>&lt;p&gt;
APPRAISER: &#21033;&#29992;&#36817;&#20284;&#35823;&#24046;&#36827;&#34892;DNN&#25925;&#38556;&#23481;&#24525;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
APPRAISER: DNN Fault Resilience Analysis Employing Approximation Errors. (arXiv:2305.19733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#38169;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;APPRAISER&#65292;&#23427;&#37319;&#29992;&#36817;&#20284;&#35823;&#24046;&#36827;&#34892;DNN&#25925;&#38556;&#23481;&#24525;&#24615;&#20998;&#26512;&#12290;APPRAISER&#25552;&#20379;&#20102;&#25104;&#21315;&#19978;&#19975;&#20493;&#30340;&#35780;&#20272;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20998;&#26512;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#26032;&#30340;&#21487;&#38752;&#24615;&#20851;&#27880;&#12290;&#23454;&#38469;&#19978;&#65292;&#36890;&#36807;&#30828;&#20214;&#20223;&#30495;&#36827;&#34892;&#25925;&#38556;&#27880;&#20837;&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;DNN&#26550;&#26500;&#23481;&#38169;&#24615;&#30340;&#26377;&#25928;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#26089;&#26399;&#35774;&#35745;&#38454;&#27573;&#24050;&#32463;&#20943;&#36731;&#20102;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#22312;&#26102;&#38388;&#12289;&#35774;&#35745;&#21644;&#25511;&#21046;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#38169;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;APPRAISER&#65292;&#23427;&#23558;&#20989;&#25968;&#36817;&#20284;&#29992;&#20110;&#38750;&#20256;&#32479;&#29992;&#36884;&#65292;&#24182;&#21033;&#29992;&#36817;&#20284;&#35745;&#31639;&#35823;&#24046;&#12290;&#36890;&#36807;&#22312;&#23481;&#38169;&#35780;&#20272;&#39046;&#22495;&#37319;&#29992;&#36825;&#20010;&#27010;&#24565;&#65292;APPRAISER&#25552;&#20379;&#20102;&#25104;&#21315;&#19978;&#19975;&#20493;&#30340;&#35780;&#20272;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20998;&#26512;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;APPRAISER&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the extensive exploitation of Deep Neural Networks (DNNs) in safety-critical applications raises new reliability concerns. In practice, methods for fault injection by emulation in hardware are efficient and widely used to study the resilience of DNN architectures for mitigating reliability issues already at the early design stages. However, the state-of-the-art methods for fault injection by emulation incur a spectrum of time-, design- and control-complexity problems. To overcome these issues, a novel resiliency assessment method called APPRAISER is proposed that applies functional approximation for a non-conventional purpose and employs approximate computing errors for its interest. By adopting this concept in the resiliency assessment domain, APPRAISER provides thousands of times speed-up in the assessment process, while keeping high accuracy of the analysis. In this paper, APPRAISER is validated by comparing it with state-of-the-art approaches for fault injection by emulat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#27969;&#24418;&#30340;&#26354;&#29575;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23618;&#38388;&#20855;&#26377;&#29305;&#24449;&#26354;&#29575;&#21078;&#38754;&#65292;&#26354;&#29575;&#24046;&#24322;&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#65292;&#19988;mixup&#31561;&#24120;&#35265;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#20135;&#29983;&#26356;&#24179;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.19730</link><description>&lt;p&gt;
&#28508;&#22312;&#22270;&#20687;&#27969;&#24418;&#30340;&#25968;&#25454;&#34920;&#31034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data Representations' Study of Latent Image Manifolds. (arXiv:2305.19730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#27969;&#24418;&#30340;&#26354;&#29575;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23618;&#38388;&#20855;&#26377;&#29305;&#24449;&#26354;&#29575;&#21078;&#38754;&#65292;&#26354;&#29575;&#24046;&#24322;&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#65292;&#19988;mixup&#31561;&#24120;&#35265;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#20135;&#29983;&#26356;&#24179;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#20869;&#22312;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#27969;&#24418;&#30340;&#26354;&#29575;&#65292;&#21363;&#22312;&#20854;&#20027;&#26041;&#21521;&#19978;&#30340;&#27969;&#24418;&#20559;&#31163;&#24179;&#22374;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23618;&#38388;&#20855;&#26377;&#29305;&#24449;&#26354;&#29575;&#21078;&#38754;&#65306;&#19968;&#20010;&#21021;&#22987;&#24613;&#21095;&#22686;&#21152;&#65292;&#25509;&#30528;&#26159;&#38271;&#26102;&#38388;&#30340;&#24179;&#21488;&#26399;&#65292;&#28982;&#21518;&#26159;&#21478;&#19968;&#20010;&#22686;&#21152;&#12290;&#30456;&#21453;&#65292;&#22312;&#26410;&#32463;&#35757;&#32451;&#30340;&#32593;&#32476;&#20013;&#19981;&#20986;&#29616;&#36825;&#31181;&#34892;&#20026;&#65292;&#20854;&#20013;&#26354;&#29575;&#21464;&#24179;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#26368;&#21518;&#20004;&#23618;&#20043;&#38388;&#30340;&#26354;&#29575;&#24046;&#24322;&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#28508;&#22312;&#32534;&#30721;&#30340;&#20869;&#22312;&#32500;&#24230;&#24182;&#38750;&#24517;&#28982;&#34920;&#24449;&#26354;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;mixup&#31561;&#24120;&#35265;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#22312;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#26102;&#20135;&#29983;&#26356;&#24179;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been demonstrated to achieve phenomenal success in many domains, and yet their inner mechanisms are not well understood. In this paper, we investigate the curvature of image manifolds, i.e., the manifold deviation from being flat in its principal directions. We find that state-of-the-art trained convolutional neural networks for image classification have a characteristic curvature profile along layers: an initial steep increase, followed by a long phase of a plateau, and followed by another increase. In contrast, this behavior does not appear in untrained networks in which the curvature flattens. We also show that the curvature gap between the last two layers has a strong correlation with the generalization capability of the network. Moreover, we find that the intrinsic dimension of latent codes is not necessarily indicative of curvature. Finally, we observe that common regularization methods such as mixup yield flatter representations when compared to other m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#34701;&#21512;&#20302;&#31209;&#36817;&#20284;&#21644;&#38750;&#24179;&#34913;&#20844;&#24335;&#30340;O(n)&#20248;&#21270;&#36755;&#36816;&#27714;&#35299;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19727</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#20302;&#31209;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unbalanced Low-rank Optimal Transport Solvers. (arXiv:2305.19727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#34701;&#21512;&#20302;&#31209;&#36817;&#20284;&#21644;&#38750;&#24179;&#34913;&#20844;&#24335;&#30340;O(n)&#20248;&#21270;&#36755;&#36816;&#27714;&#35299;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#36755;&#36816;&#26041;&#27861;&#38271;&#26399;&#20197;&#26469;&#37117;&#21463;&#21040;&#20102;&#20004;&#20010;&#26174;&#30528;&#38480;&#21046;&#30340;&#24433;&#21709;&#65306;&#31532;&#19968;&#65292;&#26631;&#20934;&#26679;&#26412;&#27714;&#35299;&#22120;&#65288;&#24403;&#29992;&#20110;n&#20010;&#26679;&#26412;&#25209;&#27425;&#26102;&#65289;&#30340;O(n^3)&#35745;&#31639;&#25104;&#26412;&#26159;&#31105;&#27490;&#30340;&#65307;&#31532;&#20108;&#65292;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#20351;&#24471;OT&#35299;&#31639;&#22120;&#22312;&#23454;&#36341;&#20013;&#22826;&#36807;&#20005;&#26684;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#21305;&#37197;&#26469;&#33258;&#20004;&#31181;&#24230;&#37327;&#30340;&#25152;&#26377;&#28857;&#65292;&#20854;&#36755;&#20986;&#21487;&#33021;&#20250;&#21463;&#21040;&#24322;&#24120;&#20540;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#21512;&#24182;&#36215;&#26469;&#65292;&#23454;&#29616;&#39318;&#20010;&#25454;&#25105;&#20204;&#25152;&#30693;&#34701;&#21512;&#20102;&#20302;&#31209;&#36817;&#20284;&#21644;&#38750;&#24179;&#34913;&#20844;&#24335;&#30340;O(n)&#20248;&#21270;&#36755;&#36816;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27714;&#35299;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22914;&#20309;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36895;&#24230;&#65292;&#20801;&#35768;&#26356;&#36731;&#26494;&#22320;&#24182;&#20837;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#24322;&#24120;&#20540;&#26356;&#26377;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations. First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive. Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers. A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods: While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further. On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation. The goal of this paper is to merge these two strains, to achieve th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32452;&#21512;&#20551;&#35774;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#34920;&#31034;&#19981;&#21516;&#29305;&#24449;&#38598;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;LEGATO&#20998;&#23618;&#22270;&#33258;&#32534;&#30721;&#22120;&#26469;&#25429;&#33719;&#23616;&#37096;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.19726</link><description>&lt;p&gt;
&#26080;&#32452;&#21512;&#20551;&#35774;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Representations without Compositional Assumptions. (arXiv:2305.19726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32452;&#21512;&#20551;&#35774;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#34920;&#31034;&#19981;&#21516;&#29305;&#24449;&#38598;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;LEGATO&#20998;&#23618;&#22270;&#33258;&#32534;&#30721;&#22120;&#26469;&#25429;&#33719;&#23616;&#37096;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21253;&#21547;&#30001;&#19981;&#21516;&#30340;&#27979;&#37327;&#26469;&#28304;&#29983;&#25104;&#30340;&#22810;&#20010;&#35270;&#22270;&#30340;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#22810;&#35270;&#22270;&#26694;&#26550;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#20294;&#21463;&#21040;&#39044;&#23450;&#20041;&#30340;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#35748;&#20026;&#29305;&#24449;&#38598;&#20849;&#20139;&#30456;&#21516;&#30340;&#20449;&#24687;&#65292;&#34920;&#31034;&#24212;&#35813;&#23398;&#20064;&#20840;&#23616;&#20849;&#20139;&#30340;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#36866;&#29992;&#20110;&#22797;&#26434;&#29305;&#24449;&#38598;&#20043;&#38388;&#23384;&#22312;&#30340;&#20381;&#36182;&#20851;&#31995;&#30340;&#30495;&#23454;&#19990;&#30028;&#34920;&#26684;&#25968;&#25454;&#65292;&#36825;&#23548;&#33268;&#20102;&#36739;&#38590;&#23398;&#20064;&#30340;&#23616;&#37096;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#38598;&#34920;&#31034;&#20026;&#22270;&#33410;&#28857;&#65292;&#23558;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20316;&#20026;&#21487;&#23398;&#20064;&#30340;&#36793;&#26469;&#23398;&#20064;&#29305;&#24449;&#38598;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#23618;&#22270;&#33258;&#32534;&#30721;&#22120;LEGATO&#65292;&#23398;&#20064;&#19968;&#20010;&#36739;&#23567;&#30340;&#28508;&#22312;&#22270;&#26469;&#21160;&#24577;&#22320;&#25972;&#21512;&#22810;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19987;&#38376;&#25429;&#33719;&#23616;&#37096;&#20449;&#24687;&#30340;&#28508;&#22312;&#22270;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses unsupervised representation learning on tabular data containing multiple views generated by distinct sources of measurement. Traditional methods, which tackle this problem using the multi-view framework, are constrained by predefined assumptions that assume feature sets share the same information and representations should learn globally shared factors. However, this assumption is not always valid for real-world tabular datasets with complex dependencies between feature sets, resulting in localized information that is harder to learn. To overcome this limitation, we propose a data-driven approach that learns feature set dependencies by representing feature sets as graph nodes and their relationships as learnable edges. Furthermore, we introduce LEGATO, a novel hierarchical graph autoencoder that learns a smaller, latent graph to aggregate information from multiple views dynamically. This approach results in latent graph components that specialize in capturing local
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19718</link><description>&lt;p&gt;
&#31895;&#31961;&#38598;&#19979;&#19968;&#31181;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#12290;&#23558;&#20004;&#32773;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#23398;&#20064;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#19987;&#23478;&#21487;&#20197;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#26631;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#36825;&#20010;&#25805;&#20316;&#24456;&#26114;&#36149;&#12290;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#32467;&#21512;&#22312;&#22788;&#29702;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#21322;&#30417;&#30563;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#20197;&#21450;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#21644;&#29983;&#25104;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#31895;&#31961;&#38598;&#29702;&#35770;&#26159;&#35299;&#20915;&#20449;&#24687;&#31995;&#32479;&#20013;&#30693;&#35782;&#22788;&#29702;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#31961;&#38598;&#19979;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65288;RS-ABL&#65289;&#12290;&#36890;&#36807;&#23558;&#35268;&#21017;&#30340;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#36716;&#21270;&#20026;&#20449;&#24687;&#34920;&#65292;&#21033;&#29992;&#31895;&#31961;&#38598;&#29702;&#35770;&#26469;&#35299;&#20915;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#21644;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#35268;&#21017;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#36127;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#35268;&#21017;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25913;&#36830;&#26041;&#27861;&#26159;&#21542;&#26377;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2305.19717</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25913;&#36830;&#26159;&#21542;&#30495;&#27491;&#26377;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Rewiring Actually Helpful in Graph Neural Networks?. (arXiv:2305.19717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25913;&#36830;&#26041;&#27861;&#26159;&#21542;&#26377;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#28040;&#24687;&#20256;&#36882;&#27493;&#39588;&#26469;&#35745;&#31639;&#33410;&#28857;&#34920;&#31034;&#65292;&#36825;&#20123;&#27493;&#39588;&#21253;&#25324;&#33410;&#28857;&#29305;&#24449;&#30340;&#26412;&#22320;&#32858;&#21512;&#12290;&#20294;&#26159;&#65292;&#28145;&#23618;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#33410;&#28857;&#20043;&#38388;&#26356;&#38271;&#36317;&#31163;&#30340;&#20132;&#20114;&#30340;&#38382;&#39064;&#21463;&#21040;&#20102;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#24433;&#21709;&#12290;&#32780;&#21518;&#32773;&#24402;&#22240;&#20110;&#25351;&#23548;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#25299;&#25169;&#65292;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#23545;&#21253;&#21547;&#22312;&#36828;&#31243;&#33410;&#28857;&#19978;&#30340;&#20449;&#24687;&#19981;&#25935;&#24863;&#12290;&#35768;&#22810;&#25913;&#36830;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#25110;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#24230;&#21387;&#32553;&#19982;&#20854;&#20182;&#19982;&#27169;&#22411;&#35757;&#32451;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#65288;&#22914;&#28040;&#22833;&#30340;&#26799;&#24230;&#65289;&#30456;&#32806;&#21512;&#65292;&#25152;&#20197;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#22909;&#22788;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#38656;&#35201;&#35757;&#32451;&#21363;&#21487;&#35745;&#31639;&#33410;&#28857;&#21644;&#22270;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks compute node representations by performing multiple message-passing steps that consist in local aggregations of node features. Having deep models that can leverage longer-range interactions between nodes is hindered by the issues of over-smoothing and over-squashing. In particular, the latter is attributed to the graph topology which guides the message-passing, causing a node representation to become insensitive to information contained at distant nodes. Many graph rewiring methods have been proposed to remedy or mitigate this problem. However, properly evaluating the benefits of these methods is made difficult by the coupling of over-squashing with other issues strictly related to model training, such as vanishing gradients. Therefore, we propose an evaluation setting based on message-passing models that do not require training to compute node and graph representations. We perform a systematic experimental comparison on real-world node and graph classification ta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19706</link><description>&lt;p&gt;
&#21487;&#20998;&#30446;&#26631;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65306;&#25512;&#21160;&#21160;&#24577;&#35268;&#21010;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30340;&#20840;&#23616;&#20248;&#21270;&#22312;&#20934;&#30830;&#24615;&#65292;&#22823;&#23567;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23558;&#23376;&#26641;&#20316;&#20026;&#29420;&#31435;&#30340;&#23376;&#38382;&#39064;&#35299;&#20915;&#26469;&#21033;&#29992;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#20998;&#21035;&#20248;&#21270;&#23376;&#26641;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#20998;&#31163;&#32422;&#26463;&#21644;&#30446;&#26631;&#20219;&#24847;&#32452;&#21512;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#20063;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#23494;&#24230;&#22330;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#30456;&#23545;&#35823;&#24046;&#21463;&#22806;&#37096;&#22240;&#32032;&#24433;&#21709;&#36739;&#23567;&#65292;&#19988;&#35757;&#32451;&#22522;&#30784;&#20107;&#23454;&#24182;&#38750;&#20934;&#30830;&#24615;&#30340;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2305.19698</link><description>&lt;p&gt;
&#31070;&#32463;&#23494;&#24230;&#22330;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigation of the Robustness of Neural Density Fields. (arXiv:2305.19698v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#23494;&#24230;&#22330;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#30456;&#23545;&#35823;&#24046;&#21463;&#22806;&#37096;&#22240;&#32032;&#24433;&#21709;&#36739;&#23567;&#65292;&#19988;&#35757;&#32451;&#22522;&#30784;&#20107;&#23454;&#24182;&#38750;&#20934;&#30830;&#24615;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24314;&#27169;&#23494;&#24230;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#23494;&#24230;&#22330;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25551;&#36848;&#22825;&#20307;&#30340;&#23494;&#24230;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24418;&#29366;&#24314;&#27169;&#65292;&#36825;&#22312;&#36817;&#36317;&#31163;&#35774;&#35745;&#36712;&#36857;&#26102;&#20855;&#26377;&#26497;&#22823;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#23494;&#24230;&#22330;&#21450;&#20854;&#30456;&#23545;&#35823;&#24046;&#22312;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22122;&#22768;&#25110;&#32422;&#26463;&#26465;&#20214;&#65289;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#26576;&#20123;&#36317;&#31163;&#19979;&#30340;&#26368;&#22823;&#21487;&#29992;&#37325;&#21147;&#20449;&#21495;&#24378;&#24230;&#25152;&#31034;&#30340;433 Eros&#21644;67P/Churyumov-Gerasimenko&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#35757;&#32451;&#22312;&#22810;&#38754;&#20307;&#21644;mascon&#22522;&#30784;&#19978;&#30340;&#27169;&#22411;&#34920;&#29616;&#30456;&#20284;&#65292;&#36825;&#34920;&#26126;&#22522;&#30784;&#20107;&#23454;&#24182;&#19981;&#26159;&#20934;&#30830;&#24615;&#30340;&#29942;&#39048;&#12290;&#20856;&#22411;&#25506;&#27979;&#20219;&#21153;&#20013;&#22826;&#38451;&#36752;&#23556;&#21387;&#30340;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#30456;&#23545;&#35823;&#24046;&#19982;&#26080;&#22122;&#22768;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#38480;&#21046;&#25506;&#38024;&#31934;&#24230;&#30340;&#24433;&#21709;&#20250;&#23545;&#27169;&#22411;&#36896;&#25104;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in modeling density distributions, so-called neural density fields, can accurately describe the density distribution of celestial bodies without, e.g., requiring a shape model - properties of great advantage when designing trajectories close to these bodies. Previous work introduced this approach, but several open questions remained. This work investigates neural density fields and their relative errors in the context of robustness to external factors like noise or constraints during training, like the maximal available gravity signal strength due to a certain distance exemplified for 433 Eros and 67P/Churyumov-Gerasimenko. It is found that both models trained on a polyhedral and mascon ground truth perform similarly, indicating that the ground truth is not the accuracy bottleneck. The impact of solar radiation pressure on a typical probe affects training neglectable, with the relative error being of the same magnitude as without noise. However, limiting the precision o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#20449;&#36947;&#34928;&#33853;&#26679;&#26412;&#30340;OFDM&#23376;&#24102;&#20449;&#36947;&#39044;&#27979;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#36873;&#25321;&#24615;&#34928;&#33853;&#20013;&#26410;&#26469;&#20449;&#36947;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.19696</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;OFDM&#23376;&#24102;&#20449;&#36947;&#39044;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
An Efficient Machine Learning-based Channel Prediction Technique for OFDM Sub-Bands. (arXiv:2305.19696v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19696
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#20449;&#36947;&#34928;&#33853;&#26679;&#26412;&#30340;OFDM&#23376;&#24102;&#20449;&#36947;&#39044;&#27979;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#36873;&#25321;&#24615;&#34928;&#33853;&#20013;&#26410;&#26469;&#20449;&#36947;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#33719;&#21462;&#23545;&#20110;&#25552;&#39640;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26080;&#32447;&#29615;&#22659;&#30340;&#26102;&#21464;&#24615;&#21644;&#39057;&#29575;&#36873;&#25321;&#24615;&#30340;&#22797;&#26434;&#24615;&#65292;&#33719;&#24471;&#31934;&#30830;&#30340;CSI&#65288;&#21487;&#20197;&#36890;&#36807;&#20449;&#36947;&#20272;&#35745;&#25110;&#20449;&#36947;&#39044;&#27979;&#23454;&#29616;&#65289;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#23376;&#24102;&#20013;&#36827;&#34892;&#20449;&#36947;&#39044;&#27979;&#30340;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#35757;&#32451;&#29992;&#20110;&#20272;&#35745;&#36873;&#25321;&#24615;&#34928;&#33853;&#20013;&#26410;&#26469;&#20449;&#36947;&#34892;&#20026;&#30340;&#20449;&#36947;&#34928;&#33853;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The acquisition of accurate channel state information (CSI) is of utmost importance since it provides performance improvement of wireless communication systems. However, acquiring accurate CSI, which can be done through channel estimation or channel prediction, is an intricate task due to the complexity of the time-varying and frequency selectivity of the wireless environment. To this end, we propose an efficient machine learning (ML)-based technique for channel prediction in orthogonal frequency-division multiplexing (OFDM) sub-bands. The novelty of the proposed approach lies in the training of channel fading samples used to estimate future channel behaviour in selective fading.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#19982;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#30456;&#20851;&#24615;&#21457;&#29616;&#34394;&#20551;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20960;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19695</link><description>&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#27169;&#22411;&#21644;PMIME&#24230;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal discovery for time series with constraint-based model and PMIME measure. (arXiv:2305.19695v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#19982;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#30456;&#20851;&#24615;&#21457;&#29616;&#34394;&#20551;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20960;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#23450;&#20041;&#20102;&#22240;&#26524;&#21644;&#25928;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#65292;&#36825;&#20010;&#27010;&#24565;&#20801;&#35768;&#32771;&#34385;&#26102;&#38388;&#28382;&#21518;&#26469;&#34920;&#24449;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#36825;&#20123;&#29616;&#35937;&#22312;&#21307;&#23398;&#19978;&#20998;&#26512;&#33647;&#29289;&#25928;&#24212;&#12289;&#22312;&#21046;&#36896;&#19994;&#20013;&#26816;&#27979;&#22797;&#26434;&#31995;&#32479;&#20013;&#24322;&#24120;&#24773;&#20917;&#30340;&#21407;&#22240;&#25110;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#29305;&#21035;&#37325;&#35201;......&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#20165;&#36890;&#36807;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#12290;&#20294;&#26159;&#65292;&#30456;&#20851;&#24615;&#21487;&#33021;&#23548;&#33268;&#34394;&#20551;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#19982;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#24230;&#37327;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#20801;&#35768;&#25512;&#26029;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20851;&#31995;&#24182;&#26500;&#24314;&#28508;&#22312;&#30340;&#22240;&#26524;&#22270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causality defines the relationship between cause and effect. In multivariate time series field, this notion allows to characterize the links between several time series considering temporal lags. These phenomena are particularly important in medicine to analyze the effect of a drug for example, in manufacturing to detect the causes of an anomaly in a complex system or in social sciences... Most of the time, studying these complex systems is made through correlation only. But correlation can lead to spurious relationships. To circumvent this problem, we present in this paper a novel approach for discovering causality in time series data that combines a causal discovery algorithm with an information theoretic-based measure. Hence the proposed method allows inferring both linear and non-linear relationships and building the underlying causal graph. We evaluate the performance of our approach on several simulated data sets, showing promising results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20195;&#29702;&#20998;&#31867;&#25439;&#22833;&#30340;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#30340;&#23398;&#20064;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19694</link><description>&lt;p&gt;
&#21033;&#29992;&#20195;&#29702;&#20998;&#31867;&#25439;&#22833;&#30340;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hypothesis Transfer Learning with Surrogate Classification Losses. (arXiv:2305.19694v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20195;&#29702;&#20998;&#31867;&#25439;&#22833;&#30340;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#30340;&#23398;&#20064;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#65288;HTL&#65289;&#36890;&#36807;&#20801;&#35768;&#20808;&#21069;&#20219;&#21153;&#65288;&#21363;&#28304;&#20219;&#21153;&#65289;&#21521;&#19968;&#20010;&#26032;&#20219;&#21153;&#65288;&#30446;&#26631;&#20219;&#21153;&#65289;&#36716;&#31227;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#65292;&#19982;&#39046;&#22495;&#33258;&#36866;&#24212;&#30456;&#23545;&#24212;&#12290;&#20107;&#23454;&#19978;&#65292;HTL&#20165;&#20381;&#36182;&#20110;&#20174;&#28304;&#25968;&#25454;&#23398;&#20064;&#21040;&#30340;&#20551;&#35774;&#65292;&#20813;&#38500;&#20102;&#22823;&#37327;&#25968;&#25454;&#23384;&#20648;&#30340;&#38556;&#30861;&#65292;&#24182;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#23454;&#38469;&#21033;&#30410;&#12290;&#22240;&#27492;&#65292;HTL&#23545;&#20110;&#20381;&#36182;&#20110;&#22823;&#25968;&#25454;&#30340;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#26377;&#21033;&#12290;&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#30740;&#31350;HTL&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#20108;&#20998;&#31867;&#24773;&#20917;&#19979;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#27604;&#20197;&#21069;&#26356;&#32039;&#23494;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#21487;&#20197;&#23454;&#38469;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothesis transfer learning (HTL) contrasts domain adaptation by allowing for a previous task leverage, named the source, into a new one, the target, without requiring access to the source data. Indeed, HTL relies only on a hypothesis learnt from such source data, relieving the hurdle of expansive data storage and providing great practical benefits. Hence, HTL is highly beneficial for real-world applications relying on big data. The analysis of such a method from a theoretical perspective faces multiple challenges, particularly in classification tasks. This paper deals with this problem by studying the learning theory of HTL through algorithmic stability, an attractive theoretical framework for machine learning algorithms analysis. In particular, we are interested in the statistical behaviour of the regularized empirical risk minimizers in the case of binary classification. Our stability analysis provides learning guarantees under mild assumptions. Consequently, we derive several comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#29616;&#35937;&#65292;&#36825;&#23558;&#20854;&#29983;&#25104;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65292;&#25552;&#20986;&#20102;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19693</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;
&lt;/p&gt;
&lt;p&gt;
Spontaneous symmetry breaking in generative diffusion models. (arXiv:2305.19693v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#29616;&#35937;&#65292;&#36825;&#23558;&#20854;&#29983;&#25104;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65292;&#25552;&#20986;&#20102;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#36817;&#26399;&#25104;&#20026;&#20102;&#29983;&#25104;&#39640;&#32500;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#65292;&#23558;&#29983;&#25104;&#24335;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65306;1&#65289;&#20013;&#24515;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#32447;&#24615;&#31283;&#24577;&#21160;&#21147;&#23398;&#65292;2&#65289;&#26397;&#21521;&#25968;&#25454;&#27969;&#24418;&#30340;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#12290;&#36825;&#20004;&#31181;&#8220;&#30456;&#8221;&#30001;&#20013;&#24515;&#22266;&#23450;&#28857;&#31283;&#23450;&#24615;&#21464;&#21270;&#25152;&#20998;&#38548;&#65292;&#32780;&#19981;&#31283;&#23450;&#31383;&#21475;&#36127;&#36131;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20934;&#30830;&#22320;&#27169;&#25311;&#26089;&#26399;&#21160;&#21147;&#23398;&#24182;&#19981;&#20250;&#23545;&#26368;&#32456;&#29983;&#25104;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#26089;&#26399;&#28072;&#33853;&#20250;&#22238;&#21040;&#20013;&#24515;&#22266;&#23450;&#28857;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#24555;&#36895;&#21462;&#26679;&#22120;&#19978;&#23454;&#29616;&#20102;&#38271;&#36798;3&#20493;&#30340;FID&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#24322;&#27493;&#22810;&#20154;&#36172;&#21338;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#24773;&#20917;&#65292;&#25512;&#20986;&#20102;Cautious Greedy&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#24120;&#25968;&#36951;&#25022;&#65292;&#21516;&#26102;UCB&#31639;&#27861;&#30340;&#33258;&#28982;&#25193;&#23637;&#23637;&#29616;&#20986;&#20102; $\mathcal{O}(\sqrt{T\log(T)})$ &#26497;&#23567;&#21270;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2305.19691</link><description>&lt;p&gt;
&#24322;&#27493;&#22810;&#20154;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#24120;&#25968;&#25110;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Constant or logarithmic regret in asynchronous multiplayer bandits. (arXiv:2305.19691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24322;&#27493;&#22810;&#20154;&#36172;&#21338;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#24773;&#20917;&#65292;&#25512;&#20986;&#20102;Cautious Greedy&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#24120;&#25968;&#36951;&#25022;&#65292;&#21516;&#26102;UCB&#31639;&#27861;&#30340;&#33258;&#28982;&#25193;&#23637;&#23637;&#29616;&#20986;&#20102; $\mathcal{O}(\sqrt{T\log(T)})$ &#26497;&#23567;&#21270;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#22810;&#20154;&#36172;&#21338;&#38382;&#39064;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#34429;&#28982;&#25991;&#29486;&#22823;&#22810;&#32771;&#34385;&#21516;&#27493;&#29609;&#23478;&#65292;&#20294;&#26080;&#32447;&#30005;&#32593;&#32476;&#65288;&#20363;&#22914;&#29289;&#32852;&#32593;&#65289; tend to have asynchronous devices&#12290;&#36825;&#24341;&#21457;&#20102;&#26356;&#21152;&#22256;&#38590;&#30340;&#24322;&#27493;&#22810;&#20154;&#36172;&#21338;&#38382;&#39064;&#65292;&#39318;&#20808;&#29992;&#25506;&#32034;&#28982;&#21518;&#25215;&#35834;&#65288;ETC&#65289;&#31639;&#27861;&#35299;&#20915;&#65288;&#35831;&#21442;&#35265; Dakdouk&#65292;2022&#65289;&#65292;&#36951;&#25022;&#19978;&#38480;&#20026; $\mathcal{O}(T^{\frac{2}{3}})$&#12290;&#29978;&#33267;&#22312;&#32771;&#34385;&#20998;&#25955;&#21270;&#20043;&#21069;&#65292;&#29702;&#35299;&#38598;&#20013;&#24335;&#24773;&#20917;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#30693;&#36947;&#26159;&#21542;&#21487;&#33021;&#24471;&#21040;&#23567;&#20110; $\Omega(T^\frac{2}{3})$ &#30340;&#36951;&#25022;&#12290; &#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#20316;&#20986;&#20102;&#32943;&#23450;&#22238;&#31572;&#65292;&#22240;&#20026;UCB&#30340;&#33258;&#28982;&#25193;&#23637;&#23637;&#29616;&#20986;&#20102; $\mathcal{O}(\sqrt{T\log(T)})$ &#26497;&#23567;&#21270;&#36951;&#25022;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21483;&#20570;&#8220;&#35880;&#24910;&#36138;&#23146;&#8221;&#30340;&#38598;&#20013;&#24335;&#31639;&#27861;&#65292;&#22914;&#26524;&#26368;&#20248;&#31574;&#30053;&#33267;&#23569;&#23558;&#19968;&#20010;&#29609;&#23478;&#25351;&#23450;&#22312;&#27599;&#20010;&#27494;&#22120;&#19978;&#65288;&#34987;&#35777;&#26126;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#65289;&#65292;&#21017;&#21487;&#20197;&#20135;&#29983;&#24120;&#25968;&#20445;&#35777;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiplayer bandits have recently been extensively studied because of their application to cognitive radio networks.  While the literature mostly considers synchronous players, radio networks (e.g. for IoT) tend to have asynchronous devices. This motivates the harder, asynchronous multiplayer bandits problem, which was first tackled with an explore-then-commit (ETC) algorithm (see Dakdouk, 2022), with a regret upper-bound in $\mathcal{O}(T^{\frac{2}{3}})$. Before even considering decentralization, understanding the centralized case was still a challenge as it was unknown whether getting a regret smaller than $\Omega(T^{\frac{2}{3}})$ was possible.  We answer positively this question, as a natural extension of UCB exhibits a $\mathcal{O}(\sqrt{T\log(T)})$ minimax regret.  More importantly, we introduce Cautious Greedy, a centralized algorithm that yields constant instance-dependent regret if the optimal policy assigns at least one player on each arm (a situation that is proved to occur 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.19685</link><description>&lt;p&gt;
&#28145;&#24230;&#38543;&#26426;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep Stochastic Mechanics. (arXiv:2305.19685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21463;&#38543;&#26426;&#21147;&#23398;&#21644;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#20174;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#20013;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#28508;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26356;&#39640;&#30340;&#32500;&#24230;&#19978;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#32467;&#26524;&#20855;&#26377;&#19982;&#32500;&#25968;&#25968;&#37327;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#29992;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schr\"odinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Metropolis-Hastings&#32806;&#21512;&#21644;&#23616;&#37096;&#27169;&#24577;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#20013;&#30340;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#38382;&#39064;&#65292;&#20351;&#24471;DBMs&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#20854;&#20182;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19684</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#27169;&#24577;&#21021;&#22987;&#21270;&#21644;&#26080;&#20559;&#24046;&#23545;&#27604;&#25955;&#24230;&#23454;&#29616;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization. (arXiv:2305.19684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Metropolis-Hastings&#32806;&#21512;&#21644;&#23616;&#37096;&#27169;&#24577;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#20013;&#30340;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#38382;&#39064;&#65292;&#20351;&#24471;DBMs&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#20854;&#20182;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;DBMs&#65289;&#20013;&#30340;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#33719;&#21462;&#26080;&#20559;&#20272;&#35745;&#37327;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;Gibbs&#37319;&#26679;&#30340;&#26368;&#22823;&#32806;&#21512;&#65292;&#20294;&#24403;&#29366;&#24577;&#26159;&#39640;&#32500;&#26102;&#65292;&#20854;&#25910;&#25947;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Metropolis-Hastings&#30340;&#32806;&#21512;&#65292;&#24182;&#22260;&#32469;&#30446;&#26631;&#20998;&#24067;&#30340;&#23616;&#37096;&#27169;&#24577;&#21021;&#22987;&#21270;&#29366;&#24577;&#12290;&#30001;&#20110;MH&#20542;&#21521;&#20110;&#25298;&#32477;&#25552;&#26696;&#65292;&#36825;&#31181;&#32806;&#21512;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#22312;&#19968;&#27493;&#20869;&#25910;&#25947;&#65292;&#22240;&#27492;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#36138;&#24515;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;DBMs&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#23454;&#29992;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DBMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#31639;&#27861;&#20351;DBMs&#33021;&#22815;&#23637;&#29616;&#20986;&#19982;&#20854;&#20182;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#22312;MNIST&#19978;&#36798;&#21040;&#20102;10.33&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of biased gradient estimation in deep Boltzmann machines (DBMs). The existing method to obtain an unbiased estimator uses a maximal coupling based on a Gibbs sampler, but when the state is high-dimensional, it takes a long time to converge. In this study, we propose to use a coupling based on the Metropolis-Hastings (MH) and to initialize the state around a local mode of the target distribution. Because of the propensity of MH to reject proposals, the coupling tends to converge in only one step with a high probability, leading to high efficiency. We find that our method allows DBMs to be trained in an end-to-end fashion without greedy pretraining. We also propose some practical techniques to further improve the performance of DBMs. We empirically demonstrate that our training algorithm enables DBMs to show comparable generative performance to other deep generative models, achieving the FID score of 10.33 for MNIST.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#32773;&#23558;&#24179;&#28369;&#20851;&#27880;&#26426;&#21046;&#24341;&#20837;&#26368;&#20808;&#36827;&#30340;Trajectron++&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#24182;&#27604;&#36739;&#20854;&#24615;&#33021;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#23558;&#20154;&#31867;&#30340;&#35748;&#30693;&#22240;&#32032;&#32435;&#20837;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.19678</link><description>&lt;p&gt;
Smooth-Trajectron++: &#23558;&#24179;&#28369;&#20851;&#27880;&#26426;&#21046;&#24341;&#20837;Trajectron++&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Smooth-Trajectron++: Augmenting the Trajectron++ behaviour prediction model with smooth attention. (arXiv:2305.19678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#32773;&#23558;&#24179;&#28369;&#20851;&#27880;&#26426;&#21046;&#24341;&#20837;&#26368;&#20808;&#36827;&#30340;Trajectron++&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#24182;&#27604;&#36739;&#20854;&#24615;&#33021;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#23558;&#20154;&#31867;&#30340;&#35748;&#30693;&#22240;&#32032;&#32435;&#20837;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#23545;&#20110;&#39044;&#27979;&#20854;&#26410;&#26469;&#36712;&#36857;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24320;&#21457;&#25552;&#20379;&#23433;&#20840;&#21487;&#38752;&#30340;&#35268;&#21010;&#31995;&#32479;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;Trajectron++&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;&#20854;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#21152;&#20837;&#24179;&#28369;&#39033;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#35813;&#20851;&#27880;&#26426;&#21046;&#27169;&#20223;&#20102;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25152;&#21551;&#21457;&#30340;&#20154;&#31867;&#27880;&#24847;&#21147;&#65292;&#34920;&#26126;&#20154;&#30340;&#27880;&#24847;&#21147;&#22312;&#20999;&#25442;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23558;&#20154;&#31867;&#35748;&#30693;&#27934;&#23519;&#21147;&#32435;&#20837;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding traffic participants' behaviour is crucial for predicting their future trajectories, aiding in developing safe and reliable planning systems for autonomous vehicles. Integrating cognitive processes and machine learning models has shown promise in other domains but is lacking in the trajectory forecasting of multiple traffic agents in large-scale autonomous driving datasets. This work investigates the state-of-the-art trajectory forecasting model Trajectron++ which we enhance by incorporating a smoothing term in its attention module. This attention mechanism mimics human attention inspired by cognitive science research indicating limits to attention switching. We evaluate the performance of the resulting Smooth-Trajectron++ model and compare it to the original model on various benchmarks, revealing the potential of incorporating insights from human cognition into trajectory prediction models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#8220;&#27867;&#21270;&#28216;&#25103;&#8221;&#30340;&#26694;&#26550;&#65292;&#23558;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#29616;&#21644;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#38480;&#32852;&#31995;&#20102;&#36215;&#26469;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#26631;&#20934;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19674</link><description>&lt;p&gt;
&#22312;&#32447;&#21040;PAC&#30340;&#36716;&#25442;: &#36890;&#36807;&#36951;&#25022;&#20998;&#26512;&#24471;&#20986;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Online-to-PAC Conversions: Generalization Bounds via Regret Analysis. (arXiv:2305.19674v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#8220;&#27867;&#21270;&#28216;&#25103;&#8221;&#30340;&#26694;&#26550;&#65292;&#23558;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#29616;&#21644;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#38480;&#32852;&#31995;&#20102;&#36215;&#26469;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#26631;&#20934;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#25512;&#23548;&#20986;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#31216;&#20026;&#8220;&#27867;&#21270;&#28216;&#25103;&#8221;&#65292;&#20854;&#20013;&#22312;&#32447;&#23398;&#20064;&#22120;&#35797;&#22270;&#19982;&#22266;&#23450;&#30340;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#31454;&#20105;&#65292;&#39044;&#27979;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#28857;&#35757;&#32451;&#38598;&#19978;&#30340;&#27867;&#21270;&#38388;&#38553;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#36825;&#20010;&#28216;&#25103;&#20013;&#23384;&#22312;&#26377;&#30028;&#36951;&#25022;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#19982;&#32479;&#35745;&#23398;&#20064;&#35774;&#32622;&#20043;&#38388;&#30340;&#32852;&#31995;&#26469;&#24314;&#31435;&#36825;&#31181;&#20851;&#32852;&#65292;&#36825;&#24847;&#21619;&#30528;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#38169;&#35823;&#23384;&#22312;&#19968;&#20010;&#30028;&#38480;&#65292;&#30452;&#21040;&#19982;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#26080;&#20851;&#30340;&#38789;&#27987;&#24230;&#39033;&#12290;&#36825;&#31181;&#25216;&#26415;&#20801;&#35768;&#25105;&#20204;&#24674;&#22797;&#20960;&#20010;&#26631;&#20934;&#30340;&#27867;&#21270;&#38480;&#21046;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#30340;PAC-Bayesian&#20445;&#35777;&#21644;&#20449;&#24687;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new framework for deriving bounds on the generalization bound of statistical learning algorithms from the perspective of online learning. Specifically, we construct an online learning game called the "generalization game", where an online learner is trying to compete with a fixed statistical learning algorithm in predicting the sequence of generalization gaps on a training set of i.i.d. data points. We establish a connection between the online and statistical learning setting by showing that the existence of an online learning algorithm with bounded regret in this game implies a bound on the generalization error of the statistical learning algorithm, up to a martingale concentration term that is independent of the complexity of the statistical learning method. This technique allows us to recover several standard generalization bounds including a range of PAC-Bayesian and information-theoretic guarantees, as well as generalizations thereof.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#35757;&#32451;&#26377;&#20559;&#21644;&#26080;&#20559;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#35299;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#21644;Focal Loss&#21551;&#21457;&#30340;&#26041;&#27861;&#21435;&#20559;&#24046;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#24615;&#33021;&#26356;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#31354;&#38388;&#30340;&#21487;&#35270;&#21270;&#20559;&#24046;&#26041;&#26696;&#65292;&#21487;&#20197;&#24110;&#21161;&#23454;&#36341;&#32773;&#35748;&#35782;&#34394;&#20551;&#30456;&#20851;&#30340;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2305.19671</link><description>&lt;p&gt;
&#20449;&#21495;&#27604;&#20559;&#24046;&#26356;&#38590;&#23398;&#20064;&#65306;&#20351;&#29992;Focal Loss&#36827;&#34892;&#21435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Signal Is Harder To Learn Than Bias: Debiasing with Focal Loss. (arXiv:2305.19671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19671
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#35757;&#32451;&#26377;&#20559;&#21644;&#26080;&#20559;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#35299;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#21644;Focal Loss&#21551;&#21457;&#30340;&#26041;&#27861;&#21435;&#20559;&#24046;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#24615;&#33021;&#26356;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#31354;&#38388;&#30340;&#21487;&#35270;&#21270;&#20559;&#24046;&#26041;&#26696;&#65292;&#21487;&#20197;&#24110;&#21161;&#23454;&#36341;&#32773;&#35748;&#35782;&#34394;&#20551;&#30456;&#20851;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#30456;&#20851;&#24615;&#26080;&#22788;&#19981;&#22312;&#12290; &#23613;&#31649;&#20154;&#31867;&#24120;&#24120;&#19981;&#20250;&#23519;&#35273;&#23427;&#20204;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#20197;&#23398;&#20064;&#19981;&#38656;&#35201;&#30340;&#20851;&#32852;&#32780;&#20986;&#20102;&#21517;&#65292;&#20063;&#23601;&#26159;&#25152;&#35859;&#30340;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#22522;&#30784;&#20915;&#31574;&#35268;&#21017;&#12290; &#22240;&#27492;&#65292;&#23454;&#36341;&#32773;&#32463;&#24120;&#19981;&#30693;&#36947;&#20998;&#31867;&#22120;&#30340;&#26377;&#20559;&#20915;&#31574;&#12290; &#22522;&#20110;&#34394;&#20551;&#20851;&#32852;&#30340;&#36825;&#31181;&#26377;&#20559;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Signal is Harder&#65288;SiH&#65289;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21463;Focal Loss&#21551;&#21457;&#30340;&#26032;&#22411;&#20998;&#35299;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#21516;&#26102;&#35757;&#32451;&#26377;&#20559;&#21644;&#26080;&#20559;&#20998;&#31867;&#22120;&#12290; &#20351;&#29992;&#26080;&#20559;&#20998;&#31867;&#22120;&#65292;SiH&#21305;&#37197;&#25110;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#24046;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290; &#20026;&#20102;&#25552;&#39640;&#25105;&#20204;&#25216;&#26415;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#25200;&#21160;&#26041;&#26696;&#26469;&#21487;&#35270;&#21270;&#20559;&#24046;&#65292;&#36825;&#26377;&#21161;&#20110;&#23454;&#36341;&#32773;&#35748;&#35782;&#21040;&#34394;&#20551;&#30456;&#20851;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations are everywhere. While humans often do not perceive them, neural networks are notorious for learning unwanted associations, also known as biases, instead of the underlying decision rule. As a result, practitioners are often unaware of the biased decision-making of their classifiers. Such a biased model based on spurious correlations might not generalize to unobserved data, leading to unintended, adverse consequences. We propose Signal is Harder (SiH), a variational-autoencoder-based method that simultaneously trains a biased and unbiased classifier using a novel, disentangling reweighting scheme inspired by the focal loss. Using the unbiased classifier, SiH matches or improves upon the performance of state-of-the-art debiasing methods. To improve the interpretability of our technique, we propose a perturbation scheme in the latent space for visualizing the bias that helps practitioners become aware of the sources of spurious correlations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;Vandermonde&#31070;&#32463;&#31639;&#23376;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#28857;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19663</link><description>&lt;p&gt;
Vandermonde&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Vandermonde Neural Operators. (arXiv:2305.19663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;Vandermonde&#31070;&#32463;&#31639;&#23376;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#28857;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#24050;&#25104;&#20026;&#38750;&#24120;&#21463;&#27426;&#36814;&#30340;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#25805;&#20316;&#31526;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;PDE&#20013;&#20986;&#29616;&#30340;&#25805;&#20316;&#31526;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;FNO&#20381;&#36182;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#20197;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#65292;&#25152;&#20197;&#35813;&#20307;&#31995;&#32467;&#26500;&#21487;&#33021;&#20165;&#38480;&#20110;&#31515;&#21345;&#23572;&#32593;&#26684;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;FNO&#25512;&#24191;&#21040;&#22788;&#29702;&#20998;&#24067;&#22312;&#38750;&#22343;&#21248;&#28857;&#20998;&#24067;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31216;&#20026;Vandermonde&#31070;&#32463;&#36816;&#31639;&#31526;&#65288;VNO&#65289;&#65292;&#21033;&#29992;Vandermonde&#32467;&#26500;&#30697;&#38453;&#26469;&#39640;&#25928;&#22320;&#35745;&#31639;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#21363;&#20351;&#22312;&#20219;&#24847;&#20998;&#24067;&#30340;&#28857;&#19978;&#20063;&#21487;&#20197;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;VNO&#21487;&#20197;&#27604;FNO&#24555;&#24471;&#22810;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25913;&#36827;&#20102;&#21487;&#27604;&#30340;&#38750;&#22343;&#21248;&#26041;&#27861;&#65288;&#22914;Geo-FNO&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operators (FNOs) have emerged as very popular machine learning architectures for learning operators, particularly those arising in PDEs. However, as FNOs rely on the fast Fourier transform for computational efficiency, the architecture can be limited to input data on equispaced Cartesian grids. Here, we generalize FNOs to handle input data on non-equispaced point distributions. Our proposed model, termed as Vandermonde Neural Operator (VNO), utilizes Vandermonde-structured matrices to efficiently compute forward and inverse Fourier transforms, even on arbitrarily distributed points. We present numerical experiments to demonstrate that VNOs can be significantly faster than FNOs, while retaining comparable accuracy, and improve upon accuracy of comparable non-equispaced methods such as the Geo-FNO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#24182;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#65292;&#20063;&#32473;&#20986;&#20102;&#19968;&#20123;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#26356;&#39640;&#30340;$k-$WL&#21464;&#20307;&#21644;&#20998;&#35010;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.19659</link><description>&lt;p&gt;
&#21033;&#29992;&#23616;&#37096;&#21270;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressivity of Graph Neural Networks using Localization. (arXiv:2305.19659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#24182;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#65292;&#20063;&#32473;&#20986;&#20102;&#19968;&#20123;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#26356;&#39640;&#30340;$k-$WL&#21464;&#20307;&#21644;&#20998;&#35010;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#26088;&#22312;&#22686;&#21152;&#34920;&#36798;&#33021;&#21147;&#24182;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#65292;&#24182;&#20026;&#20219;&#24847;$k$&#32473;&#20986;$k-$WL&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;Local $k-$WL&#30340;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#27604;$k-$WL&#26356;&#20855;&#34920;&#29616;&#21147;&#65292;&#24182;&#19988;&#33267;&#22810;&#19982;$(k+1)-$WL&#19968;&#26679;&#20855;&#26377;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20123;&#27169;&#24335;&#30340;&#29305;&#24449;&#65292;&#22914;&#26524;&#20004;&#20010;&#22270;&#26159;Local $k-$WL&#31561;&#20215;&#30340;&#65292;&#21017;&#23427;&#20204;&#30340;&#23376;&#22270;&#21644;&#35825;&#23548;&#23376;&#22270;&#30340;&#35745;&#25968;&#26159;&#19981;&#21464;&#30340;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;$k-$WL&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#23618;$k-$WL&#21644;&#36882;&#24402;$k-$WL&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#27604;&#22312;&#25972;&#20010;&#22270;&#19978;&#24212;&#29992;$k-$WL&#26356;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35010;&#25216;&#26415;&#65292;&#20351;&#29992;$1-$WL&#21363;&#21487;&#20445;&#35777;&#25152;&#26377;&#22823;&#23567;&#19981;&#36229;&#36807;4&#30340;&#35825;&#23548;&#23376;&#22270;&#30340;&#20934;&#30830;&#35745;&#25968;&#12290;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;$k&gt;1$&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#36824;&#23558;Local $k-$WL&#30340;&#34920;&#29616;&#21147;&#19982;&#20854;&#20182;GNN&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose localized versions of Weisfeiler-Leman (WL) algorithms in an effort to both increase the expressivity, as well as decrease the computational overhead. We focus on the specific problem of subgraph counting and give localized versions of $k-$WL for any $k$. We analyze the power of Local $k-$WL and prove that it is more expressive than $k-$WL and at most as expressive as $(k+1)-$WL. We give a characterization of patterns whose count as a subgraph and induced subgraph are invariant if two graphs are Local $k-$WL equivalent. We also introduce two variants of $k-$WL: Layer $k-$WL and recursive $k-$WL. These methods are more time and space efficient than applying $k-$WL on the whole graph. We also propose a fragmentation technique that guarantees the exact count of all induced subgraphs of size at most 4 using just $1-$WL. The same idea can be extended further for larger patterns using $k&gt;1$. We also compare the expressive power of Local $k-$WL with other GNN hierarc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#24182;&#22522;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#24471;&#20986;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.19640</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Estimates for Pairwise Learning with Deep ReLU Networks. (arXiv:2305.19640v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#24182;&#22522;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#24471;&#20986;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#23545;&#23398;&#20064;&#25351;&#30340;&#26159;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#32771;&#34385;&#19968;&#23545;&#26679;&#26412;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#24182;&#20272;&#35745;&#20102;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#12290;&#23545;&#20110;&#28385;&#36275;&#26576;&#20123;&#28201;&#21644;&#26465;&#20214;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#65292;&#24314;&#31435;&#20102;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#20854;&#35823;&#24046;&#20272;&#35745;&#30340;&#38454;&#25968;&#20026;O&#65288;&#65288;Vlog&#65288;n&#65289;/ n&#65289;1 /&#65288;2-&#946;&#65289;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30340;&#20960;&#20046;&#26368;&#20248;&#30028;&#38480;&#65292;&#22312;&#30495;&#23454;&#30340;&#39044;&#27979;&#22120;&#28385;&#36275;&#26576;&#20123;&#20809;&#28369;&#24615;&#27491;&#21017;&#24615;&#26102;&#65292;&#26368;&#20248;&#30028;&#38480;&#36798;&#21040;&#20102;&#26368;&#23567;&#21270;&#30028;&#38480;&#65292;&#24046;&#36317;&#20165;&#20026;&#23545;&#25968;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise learning refers to learning tasks where a loss takes a pair of samples into consideration. In this paper, we study pairwise learning with deep ReLU networks and estimate the excess generalization error. For a general loss satisfying some mild conditions, a sharp bound for the estimation error of order $O((V\log(n) /n)^{1/(2-\beta)})$ is established. In particular, with the pairwise least squares loss, we derive a nearly optimal bound of the excess generalization error which achieves the minimax lower bound up to a logrithmic term when the true predictor satisfies some smoothness regularities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24322;&#26500;m-health&#25968;&#25454;&#30340;&#26089;&#26399;&#21644;&#21487;&#35299;&#37322;&#30340;&#33829;&#20859;&#19981;&#33391;&#39118;&#38505;&#26816;&#27979;&#30340;&#26032;&#22411;AI&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22411;&#35780;&#20272;&#21457;&#29616;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#26799;&#24230;&#19978;&#21319;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#23588;&#20854;&#26159;&#22312;&#32435;&#20837;&#36523;&#20307;&#32452;&#25104;&#35780;&#20272;&#25968;&#25454;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.19636</link><description>&lt;p&gt;
&#20174;&#31227;&#21160;&#20581;&#24247;&#21644;&#20020;&#24202;&#25968;&#25454;&#20013;&#39044;&#27979;&#33829;&#20859;&#19981;&#33391;&#39118;&#38505;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Malnutrition Risk Prediction from m-Health and Clinical Data. (arXiv:2305.19636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24322;&#26500;m-health&#25968;&#25454;&#30340;&#26089;&#26399;&#21644;&#21487;&#35299;&#37322;&#30340;&#33829;&#20859;&#19981;&#33391;&#39118;&#38505;&#26816;&#27979;&#30340;&#26032;&#22411;AI&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22411;&#35780;&#20272;&#21457;&#29616;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#26799;&#24230;&#19978;&#21319;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#23588;&#20854;&#26159;&#22312;&#32435;&#20837;&#36523;&#20307;&#32452;&#25104;&#35780;&#20272;&#25968;&#25454;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33829;&#20859;&#19981;&#33391;&#26159;&#32769;&#24180;&#20154;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20005;&#37325;&#20581;&#24247;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#20303;&#38498;&#25110;&#26426;&#26500;&#21270;&#30340;&#20010;&#20307;&#20013;&#26356;&#21152;&#26222;&#36941;&#12290;&#20934;&#30830;&#21644;&#26089;&#26399;&#30340;&#39118;&#38505;&#26816;&#27979;&#23545;&#20110;&#33829;&#20859;&#19981;&#33391;&#30340;&#31649;&#29702;&#21644;&#39044;&#38450;&#33267;&#20851;&#37325;&#35201;&#12290;&#37197;&#22791;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;m-health&#26381;&#21153;&#21487;&#20197;&#22312;&#26356;&#33258;&#21160;&#21270;&#65292;&#23458;&#35266;&#21644;&#25345;&#32493;&#30340;&#30417;&#27979;&#21644;&#35780;&#20272;&#26041;&#38754;&#24102;&#26469;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#21487;&#20197;&#20351;AI&#20915;&#31574;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#20219;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24322;&#26500;m-health&#25968;&#25454;&#30340;&#26089;&#26399;&#21644;&#21487;&#35299;&#37322;&#30340;&#33829;&#20859;&#19981;&#33391;&#39118;&#38505;&#26816;&#27979;&#30340;&#26032;&#22411;AI&#26694;&#26550;&#12290;&#25105;&#20204;&#25191;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#35780;&#20272;&#65292;&#21253;&#25324;&#29420;&#31435;&#20010;&#20307;&#21644;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#19988;&#33719;&#24471;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#26799;&#24230;&#19978;&#21319;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#23588;&#20854;&#26159;&#22312;&#32435;&#20837;&#36523;&#20307;&#32452;&#25104;&#35780;&#20272;&#25968;&#25454;&#26102;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#20960;&#31181;&#22522;&#20934;XAI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malnutrition is a serious and prevalent health problem in the older population, and especially in hospitalised or institutionalised subjects. Accurate and early risk detection is essential for malnutrition management and prevention. M-health services empowered with Artificial Intelligence (AI) may lead to important improvements in terms of a more automatic, objective, and continuous monitoring and assessment. Moreover, the latest Explainable AI (XAI) methodologies may make AI decisions interpretable and trustworthy for end users. This paper presents a novel AI framework for early and explainable malnutrition risk detection based on heterogeneous m-health data. We performed an extensive model evaluation including both subject-independent and personalised predictions, and the obtained results indicate Random Forest (RF) and Gradient Boosting as the best performing classifiers, especially when incorporating body composition assessment data. We also investigated several benchmark XAI metho
&lt;/p&gt;</description></item><item><title>Point-GCC&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;-&#39068;&#33394;&#23545;&#27604;&#36827;&#34892;&#36890;&#29992;&#33258;&#30417;&#30563;&#19977;&#32500;&#22330;&#26223;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20998;&#23618;&#30417;&#30563;&#21644;&#26550;&#26500;&#26080;&#20851;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.19623</link><description>&lt;p&gt;
Point-GCC: &#22522;&#20110;&#20960;&#20309;-&#39068;&#33394;&#23545;&#27604;&#30340;&#36890;&#29992;&#33258;&#30417;&#30563;&#19977;&#32500;&#22330;&#26223;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast. (arXiv:2305.19623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19623
&lt;/p&gt;
&lt;p&gt;
Point-GCC&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;-&#39068;&#33394;&#23545;&#27604;&#36827;&#34892;&#36890;&#29992;&#33258;&#30417;&#30563;&#19977;&#32500;&#22330;&#26223;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20998;&#23618;&#30417;&#30563;&#21644;&#26550;&#26500;&#26080;&#20851;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#25552;&#20379;&#30340;&#20960;&#20309;&#21644;&#39068;&#33394;&#20449;&#24687;&#23545;&#20110;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#37117;&#38750;&#24120;&#37325;&#35201;&#65292;&#28982;&#32780;&#29616;&#26377;&#26041;&#27861;&#22312;&#21306;&#20998;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#32570;&#20047;&#31934;&#32454;&#35774;&#35745;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28857;&#20113;&#20449;&#24687;&#20851;&#31995;&#30340;&#19977;&#32500;&#33258;&#30417;&#30563;&#33539;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; Siamese &#32593;&#32476;&#23545;&#40784;&#20960;&#20309;&#21644;&#39068;&#33394;&#20449;&#24687;&#30340;&#36890;&#29992;&#19977;&#32500;&#22330;&#26223;&#39044;&#35757;&#32451;&#26694;&#26550; Point-GCC&#12290;&#20026;&#20102;&#29031;&#39038;&#23454;&#38469;&#24212;&#29992;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#65288;i&#65289;&#22522;&#20110;&#26032;&#39062;&#30340;&#28145;&#24230;&#32858;&#31867;&#27169;&#22359;&#30340;&#28857;&#32423;&#23545;&#27604;&#21644;&#37325;&#24314;&#21644;&#29289;&#20307;&#32423;&#23545;&#27604;&#30340;&#20998;&#23618;&#30417;&#30563;&#65292;&#26469;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65307;&#65288;ii&#65289;&#26550;&#26500;&#26080;&#20851;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#27169;&#22411;&#12290;&#30001;&#20110;&#19982;&#19979;&#28216;&#20219;&#21153;&#30456;&#20851;&#32852;&#30340;&#29289;&#20307;&#32423;&#34920;&#31034;&#65292;Point-GCC &#21487;&#20197;&#30452;&#25509;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometry and color information provided by the point clouds are both crucial for 3D scene understanding. Two pieces of information characterize the different aspects of point clouds, but existing methods lack an elaborate design for the discrimination and relevance. Hence we explore a 3D self-supervised paradigm that can better utilize the relations of point cloud information. Specifically, we propose a universal 3D scene pre-training framework via Geometry-Color Contrast (Point-GCC), which aligns geometry and color information using a Siamese network. To take care of actual application tasks, we design (i) hierarchical supervision with point-level contrast and reconstruct and object-level contrast based on the novel deep clustering module to close the gap between pre-training and downstream tasks; (ii) architecture-agnostic backbone to adapt for various downstream models. Benefiting from the object-level representation associated with downstream tasks, Point-GCC can directly evaluate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#27969;&#24418;&#20132;&#25442;Mixup(MSMix)&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#30340;&#29305;&#23450;&#23618;&#37096;&#20998;&#26367;&#25442;&#38544;&#34255;&#29305;&#24449;&#26469;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#26679;&#26412;&#20013;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#24182;&#22312;&#19977;&#20010;&#20013;&#25991;&#24847;&#22270;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;MSMix&#22312;&#23436;&#25972;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#37197;&#32622;&#19979;&#22343;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19617</link><description>&lt;p&gt;
MSMix: &#22522;&#20110;&#25554;&#20540;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#27969;&#24418;&#20132;&#25442;Mixup
&lt;/p&gt;
&lt;p&gt;
MSMix:An Interpolation-Based Text Data Augmentation Method Manifold Swap Mixup. (arXiv:2305.19617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#27969;&#24418;&#20132;&#25442;Mixup(MSMix)&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#30340;&#29305;&#23450;&#23618;&#37096;&#20998;&#26367;&#25442;&#38544;&#34255;&#29305;&#24449;&#26469;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#26679;&#26412;&#20013;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#24182;&#22312;&#19977;&#20010;&#20013;&#25991;&#24847;&#22270;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;MSMix&#22312;&#23436;&#25972;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#37197;&#32622;&#19979;&#22343;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#32780;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25554;&#20540;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;MSMix(&#27969;&#24418;&#20132;&#25442;Mixup)&#12290;&#35813;&#26041;&#27861;&#23558;&#20004;&#20010;&#19981;&#21516;&#30340;&#26679;&#26412;&#36755;&#20837;&#21516;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#29305;&#23450;&#30340;&#23618;&#65292;&#37096;&#20998;&#26367;&#25442;&#35813;&#23618;&#19968;&#20010;&#26679;&#26412;&#30340;&#38544;&#34255;&#29305;&#24449;&#20026;&#21478;&#19968;&#20010;&#26679;&#26412;&#30340;&#23545;&#24212;&#29305;&#24449;&#12290;&#28151;&#21512;&#21518;&#30340;&#38544;&#34255;&#29305;&#24449;&#36755;&#20837;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32593;&#32476;&#30340;&#20854;&#20313;&#37096;&#20998;&#12290;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#20013;&#25991;&#24847;&#22270;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;MSMix&#26041;&#27861;&#22312;&#23436;&#25972;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#37197;&#32622;&#19979;&#22343;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve the problem of poor performance of deep neural network models due to insufficient data, a simple yet effective interpolation-based data augmentation method is proposed: MSMix (Manifold Swap Mixup). This method feeds two different samples to the same deep neural network model, and then randomly select a specific layer and partially replace hidden features at that layer of one of the samples by the counterpart of the other. The mixed hidden features are fed to the model and go through the rest of the network. Two different selection strategies are also proposed to obtain richer hidden representation. Experiments are conducted on three Chinese intention recognition datasets, and the results show that the MSMix method achieves better results than other methods in both full-sample and small-sample configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#27169;&#26495;&#30340;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65288;UART&#65289;&#65292;&#20174;&#19981;&#21516;&#35270;&#35282;&#25972;&#21512;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38899;&#39057;-&#39057;&#35889;&#22270;-&#25991;&#26412;&#19977;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36171;&#20104;UART&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#22768;&#23398;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19612</link><description>&lt;p&gt;
&#25299;&#23637;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#20449;&#24687;&#35270;&#35282;&#30340;&#25991;&#26412;&#27169;&#26495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Underwater-Art: Expanding Information Perspectives With Text Templates For Underwater Acoustic Target Recognition. (arXiv:2305.19612v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#27169;&#26495;&#30340;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65288;UART&#65289;&#65292;&#20174;&#19981;&#21516;&#35270;&#35282;&#25972;&#21512;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38899;&#39057;-&#39057;&#35889;&#22270;-&#25991;&#26412;&#19977;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36171;&#20104;UART&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#22768;&#23398;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26159;&#19968;&#39033;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#22768;&#28304;&#29305;&#24449;&#21644;&#22768;&#27874;&#20256;&#25773;&#27169;&#24335;&#32780;&#21464;&#24471;&#21313;&#20998;&#22256;&#38590;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35782;&#21035;&#27169;&#22411;&#65292;&#21463;&#38480;&#20110;&#25968;&#25454;&#37327;&#19981;&#36275;&#21644;&#29421;&#31364;&#20449;&#24687;&#35270;&#35282;&#65292;&#22312;&#23454;&#38469;&#27700;&#19979;&#22330;&#26223;&#20013;&#20284;&#20046;&#36828;&#26410;&#36798;&#21040;&#28385;&#24847;&#31243;&#24230;&#12290;&#23613;&#31649;&#27700;&#19979;&#22768;&#23398;&#20449;&#21495;&#21463;&#36317;&#31163;&#12289;&#27700;&#28145;&#25110;&#20854;&#20182;&#22240;&#32032;&#30340;&#20005;&#37325;&#24433;&#21709;&#65292;&#20294;&#30456;&#20851;&#20449;&#24687;&#30340;&#27880;&#37322;&#24448;&#24448;&#26159;&#19981;&#22343;&#21248;&#30340;&#12289;&#19981;&#23436;&#25972;&#30340;&#12289;&#38590;&#20197;&#20351;&#29992;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#27169;&#26495;&#30340;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65288;UART&#65289;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#30456;&#20851;&#20449;&#24687;&#25972;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#26469;&#35774;&#35745;&#27169;&#26495;&#12290;UART&#37319;&#29992;&#38899;&#39057;-&#39057;&#35889;&#22270;-&#25991;&#26412;&#19977;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36171;&#20104;UART&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#22768;&#23398;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UART&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#27700;&#19979;&#22768;&#23398;&#25968;&#25454;&#38598;&#19978;&#35782;&#21035;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underwater acoustic target recognition is an intractable task due to the complex acoustic source characteristics and sound propagation patterns. Limited by insufficient data and narrow information perspective, recognition models based on deep learning seem far from satisfactory in practical underwater scenarios. Although underwater acoustic signals are severely influenced by distance, channel depth, or other factors, annotations of relevant information are often non-uniform, incomplete, and hard to use. In our work, we propose to implement Underwater Acoustic Recognition based on Templates made up of rich relevant information (hereinafter called "UART"). We design templates to integrate relevant information from different perspectives into descriptive natural language. UART adopts an audio-spectrogram-text tri-modal contrastive learning framework, which endows UART with the ability to guide the learning of acoustic representations by descriptive natural language. Our experiments reveal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;MUSER&#65292;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#19988;&#21482;&#38656;&#35201;&#20351;&#29992;0.056%&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19602</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#38899;&#20048;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Music Sequence Representation from Text Supervision. (arXiv:2305.19602v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;MUSER&#65292;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#19988;&#21482;&#38656;&#35201;&#20351;&#29992;0.056%&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#34920;&#31034;&#23398;&#20064;&#22240;&#20854;&#22312;&#25968;&#23383;&#20449;&#21495;&#24207;&#21015;&#20013;&#21253;&#21547;&#22797;&#26434;&#30340;&#19982;&#20154;&#31867;&#30456;&#20851;&#30340;&#27010;&#24565;&#32780;&#38395;&#21517;&#22256;&#38590;&#12290;&#20026;&#20102;&#20174;&#26377;&#26631;&#31614;&#30340;&#38899;&#39057;&#20013;&#25366;&#25496;&#26356;&#22909;&#30340;&#38899;&#20048;&#24207;&#21015;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;MUSER&#12290;MUSER&#37319;&#29992;&#38899;&#39057;-&#39057;&#35889;-&#25991;&#26412;&#19977;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25991;&#26412;&#36755;&#20837;&#21487;&#20197;&#26159;&#20219;&#20309;&#24418;&#24335;&#30340;&#20803;&#25968;&#25454;&#65292;&#20511;&#21161;&#25991;&#26412;&#27169;&#26495;&#36827;&#34892;&#24110;&#21161;&#65292;&#32780;&#39057;&#35889;&#26159;&#20174;&#38899;&#39057;&#24207;&#21015;&#20013;&#27966;&#29983;&#20986;&#26469;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20986;&#65292;&#19982;&#30446;&#21069;&#30340;&#25968;&#25454;&#23494;&#38598;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;MUSER&#21487;&#20197;&#26356;&#28789;&#27963;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#19988;&#21482;&#38656;&#35201;&#20351;&#29992;0.056%&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music representation learning is notoriously difficult for its complex human-related concepts contained in the sequence of numerical signals. To excavate better MUsic SEquence Representation from labeled audio, we propose a novel text-supervision pre-training method, namely MUSER. MUSER adopts an audio-spectrum-text tri-modal contrastive learning framework, where the text input could be any form of meta-data with the help of text templates while the spectrum is derived from an audio sequence. Our experiments reveal that MUSER could be more flexibly adapted to downstream tasks compared with the current data-hungry pre-training method, and it only requires 0.056% of pre-training data to achieve the state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19600</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#19979;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Heterogeneous Data via Adaptive Self-Distillation. (arXiv:2305.19600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20351;&#24471;&#23458;&#25143;&#26426;&#21487;&#20197;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#20219;&#20309;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#20174;&#32780;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#21457;&#29616;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35266;&#23519;&#21040;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#22343;&#21248;&#24615;&#65288;&#20363;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65289;&#12290;&#22312;&#36825;&#31181;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#20250;&#20986;&#29616;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#38382;&#39064;&#65292;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#25910;&#25947;&#21040;&#20854;&#33258;&#24049;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#22312;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#23458;&#25143;&#31471;&#25110;&#26381;&#21153;&#22120;&#20195;&#30721;&#36827;&#34892;&#20219;&#20309;&#26356;&#25913;&#65292;&#22240;&#27492;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#37096;&#32626;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid data distributions across clients, FL suffers from the 'client-drift' problem where every client converges to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to the client's training data based on: (1) the closeness of the local model's predictions with that of the global model and (2) the client's label distribution. The proposed regularization can be easily integrated atop exis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;UGNN&#65292; &#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#35299;&#20915;&#20102;&#31867;&#21035;&#20559;&#31227;&#65292;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19598</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Semi-supervised Universal Graph Classification. (arXiv:2305.19598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;UGNN&#65292; &#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#35299;&#20915;&#20102;&#31867;&#21035;&#20559;&#31227;&#65292;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20102;&#22270;&#20998;&#31867;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UGNN&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20197;&#20174;&#23376;&#22270;&#35282;&#24230;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#35299;&#20915;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#30340;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#26631;&#31614;&#25968;&#25454;&#21644;&#21487;&#33021;&#30340;&#31867;&#21035;&#20559;&#31227;&#12290;UGNN&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#26469;&#35299;&#20915;&#31867;&#21035;&#20559;&#31227;&#65292;&#20351;&#20854;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have pushed state-of-the-arts in graph classifications recently. Typically, these methods are studied within the context of supervised end-to-end training, which necessities copious task-specific labels. However, in real-world circumstances, labeled data could be limited, and there could be a massive corpus of unlabeled data, even from unknown classes as a complementary. Towards this end, we study the problem of semi-supervised universal graph classification, which not only identifies graph samples which do not belong to known classes, but also classifies the remaining samples into their respective classes. This problem is challenging due to a severe lack of labels and potential class shifts. In this paper, we propose a novel graph neural network framework named UGNN, which makes the best of unlabeled data from the subgraph perspective. To tackle class shifts, we estimate the certainty of unlabeled graphs using multiple subgraphs, which facilities the discovery of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#19978;&#23545;&#20110;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#21457;&#29616;QNN&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19593</link><description>&lt;p&gt;
&#20351;&#29992;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#25506;&#31350;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26131;&#21463;&#25915;&#20987;&#28431;&#27934;: &#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring the Vulnerabilities of Machine Learning and Quantum Machine Learning to Adversarial Attacks using a Malware Dataset: A Comparative Analysis. (arXiv:2305.19593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#19978;&#23545;&#20110;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#21457;&#29616;QNN&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#21508;&#39046;&#22495;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#23433;&#20840;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;&#36825;&#20123;&#31995;&#32479;&#26102;&#65292;&#20854;&#26131;&#21463;&#25915;&#20987;&#30340;&#29305;&#24615;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25506;&#31350;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21517;&#20026;ClaMP&#30340;&#36719;&#20214;&#20379;&#24212;&#38142;&#25915;&#20987;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#20026;QNN&#21644;NN&#24320;&#21457;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;Pennylane&#36827;&#34892;&#37327;&#23376;&#23454;&#29616;&#65292;&#20351;&#29992;TensorFlow&#21644;Keras&#36827;&#34892;&#20256;&#32479;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#24341;&#20837;&#38543;&#26426;&#22122;&#22768;&#26469;&#21019;&#24314;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#25351;&#26631;&#35780;&#20272;&#20854;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#24403;&#35757;&#32451;&#20351;&#29992;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#22312;&#25105;&#20204;&#30340;&#27604;&#36739;&#20998;&#26512;&#20013;&#21457;&#29616;&#65292;&#19982;&#20256;&#32479;NN&#30456;&#27604;&#65292;QNN&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning fields of machine learning (ML) and quantum machine learning (QML) have shown remarkable potential in tackling complex problems across various domains. However, their susceptibility to adversarial attacks raises concerns when deploying these systems in security sensitive applications. In this study, we present a comparative analysis of the vulnerability of ML and QML models, specifically conventional neural networks (NN) and quantum neural networks (QNN), to adversarial attacks using a malware dataset. We utilize a software supply chain attack dataset known as ClaMP and develop two distinct models for QNN and NN, employing Pennylane for quantum implementations and TensorFlow and Keras for traditional implementations. Our methodology involves crafting adversarial samples by introducing random noise to a small portion of the dataset and evaluating the impact on the models performance using accuracy, precision, recall, and F1 score metrics. Based on our observations, both M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#20803;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30740;&#31350;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.19591</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#65306;&#36817;&#26399;&#36827;&#23637;&#19982;&#26032;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities. (arXiv:2305.19591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#20803;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30740;&#31350;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#22312;&#32531;&#35299;&#20840;&#29699;&#24615;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20854;&#36127;&#38754;&#24433;&#21709;&#21253;&#25324;&#39069;&#22806;&#26053;&#34892;&#26102;&#38388;&#30340;&#25439;&#22833;&#21644;&#29123;&#26009;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;&#23558;&#26032;&#20852;&#25216;&#26415;&#34701;&#20837;&#20132;&#36890;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20132;&#36890;&#39044;&#27979;&#65292;&#24182;&#24102;&#26469;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#20102;&#35299;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#65292;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#21464;&#37327;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#36817;&#26399;&#36827;&#23637;&#21644;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#65292;&#36825;&#26159;&#30001;&#20110;&#36817;&#24180;&#26469;&#36825;&#31867;&#26041;&#27861;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#25104;&#21151;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction plays a crucial role in alleviating traffic congestion which represents a critical problem globally, resulting in negative consequences such as lost hours of additional travel time and increased fuel consumption. Integrating emerging technologies into transportation systems provides opportunities for improving traffic prediction significantly and brings about new research problems. In order to lay the foundation for understanding the open research challenges in traffic prediction, this survey aims to provide a comprehensive overview of traffic prediction methodologies. Specifically, we focus on the recent advances and emerging research opportunities in Artificial Intelligence (AI)-based traffic prediction methods, due to their recent success and potential in traffic prediction, with an emphasis on multivariate traffic time series modeling. We first provide a list and explanation of the various data types and resources used in the literature. Next, the essential data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24102;&#24314;&#35758;&#30340;&#20027;&#21160;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#24314;&#35758;&#20013;&#21463;&#30410;&#65292;&#21363;&#20351;&#24314;&#35758;&#26159;&#20219;&#24847;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19588</link><description>&lt;p&gt;
&#24102;&#24314;&#35758;&#30340;&#20027;&#21160;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active causal structure learning with advice. (arXiv:2305.19588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24102;&#24314;&#35758;&#30340;&#20027;&#21160;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#24314;&#35758;&#20013;&#21463;&#30410;&#65292;&#21363;&#20351;&#24314;&#35758;&#26159;&#20219;&#24847;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#24314;&#35758;&#30340;&#20027;&#21160;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#20856;&#22411;&#30340;&#30740;&#31350;&#20013;&#65292;&#23398;&#20064;&#31639;&#27861;&#38024;&#23545;&#35266;&#27979;&#20998;&#24067;&#33719;&#24471;&#26412;&#36136;&#22270;&#65292;&#24182;&#34987;&#35201;&#27714;&#22312;&#26368;&#23567;&#21270;&#24178;&#39044;&#27425;&#25968;&#30340;&#21516;&#26102;&#24674;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG) $G^*$&#12290;&#22312;&#25105;&#20204;&#30340;&#38382;&#39064;&#35774;&#23450;&#20013;&#65292;&#38500;&#20102;&#20851;&#20110; $G^*$&#30340;&#24517;&#35201;&#20449;&#24687;&#22806;&#65292;&#20363;&#22914;&#19968;&#20010;&#22768;&#31216;&#26159; $G^*$&#30340;DAG $G$&#65292;&#25105;&#20204;&#36824;&#20250;&#39069;&#22806;&#33719;&#24471;&#20851;&#20110; $G^*$&#30340;&#20391;&#38754;&#20449;&#24687;&#12290;&#25105;&#20204;&#24819;&#30693;&#36947;&#65292;&#24403;&#24314;&#35758;&#25509;&#36817;&#27491;&#30830;&#26102;&#65292;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#20174;&#24314;&#35758;&#20013;&#21463;&#30410;&#65292;&#21516;&#26102;&#21363;&#20351;&#24314;&#35758;&#26159;&#20219;&#24847;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;&#20851;&#20110;&#24102;&#39044;&#27979;&#31639;&#27861;&#30340;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#39046;&#22495;&#30456;&#21516;&#12290;&#24403;&#24314;&#35758;&#26159;&#26377;&#21521;&#26080;&#29615;&#22270;$G$&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#26469;&#24674;&#22797; $G^*$&#65292;&#20854;&#24178;&#39044;&#25104;&#26412;&#26368;&#22810;&#20026;&#39564;&#35777;$G^*$&#30340;&#25104;&#26412;&#30340;$O(max\{1, \log \psi\})$&#20493;&#12290;&#36825;&#37324;&#65292;$\psi$&#26159;$G$&#21644;$G^*$&#20043;&#38388;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#23427;&#34987;&#19978;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of active causal structure learning with advice. In the typical well-studied setting, the learning algorithm is given the essential graph for the observational distribution and is asked to recover the underlying causal directed acyclic graph (DAG) $G^*$ while minimizing the number of interventions made. In our setting, we are additionally given side information about $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the learning algorithm can benefit from the advice when it is close to being correct, while still having worst-case guarantees even when the advice is arbitrarily bad. Our work is in the same space as the growing body of research on algorithms with predictions. When the advice is a DAG $G$, we design an adaptive search algorithm to recover $G^*$ whose intervention cost is at most $O(\max\{1, \log \psi\})$ times the cost for verifying $G^*$; here, $\psi$ is a distance measure between $G$ and $G^*$ that is upper bounded by the numb
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;meta-learning&#26694;&#26550;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22823;&#23567;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.19587</link><description>&lt;p&gt;
&#38754;&#21521;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#20840;&#36890;&#29992;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Omni-generalizable Neural Methods for Vehicle Routing Problems. (arXiv:2305.19587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;meta-learning&#26694;&#26550;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22823;&#23567;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36991;&#20813;&#20102;&#23545;&#25163;&#24037;&#35268;&#21017;&#30340;&#20381;&#36182;&#65292;&#23398;&#20064;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#22266;&#23450;&#22823;&#23567;&#21644;&#33410;&#28857;&#20998;&#24067;&#30340;&#21516;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#22240;&#27492;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#35813;&#22330;&#26223;&#32771;&#34385;&#20102;VRP&#22312;&#22823;&#23567;&#21644;&#20998;&#24067;&#26041;&#38754;&#30340;&#19968;&#33324;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#19979;&#23545;&#21021;&#22987;&#21270;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#20943;&#23569;&#35757;&#32451;&#24320;&#38144;&#12290;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#30340;&#21512;&#25104;&#21644;&#22522;&#20934;&#23454;&#20363;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/RoyalSkye/Omni-VRP&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.
&lt;/p&gt;</description></item><item><title>LAIT&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#27573;&#32534;&#30721;&#23454;&#29616;&#36328;&#27573;&#27880;&#24847;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;Layer-Adjustable Interactions&#25216;&#26415;&#23454;&#29616;&#20998;&#27573;&#32534;&#30721;&#21644;&#36880;&#23618;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#24182;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.19585</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35843;&#23618;&#20132;&#20114;&#30340;Transformer&#39640;&#25928;&#22810;&#27573;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction. (arXiv:2305.19585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19585
&lt;/p&gt;
&lt;p&gt;
LAIT&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#27573;&#32534;&#30721;&#23454;&#29616;&#36328;&#27573;&#27880;&#24847;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;Layer-Adjustable Interactions&#25216;&#26415;&#23454;&#29616;&#20998;&#27573;&#32534;&#30721;&#21644;&#36880;&#23618;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#24182;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32534;&#30721;&#22120;&#36890;&#36807;&#23545;&#21508;&#20010;&#20196;&#29260;&#30340;&#27880;&#24847;&#21147;&#36827;&#34892;&#32534;&#30721;&#65292;&#20351;&#20854;&#19978;&#19979;&#25991;&#24471;&#20197;&#24314;&#31435;&#65292;&#20294;&#23545;&#20110;&#38271;&#25991;&#26412;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LAIT&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27573;&#32534;&#30721;&#23454;&#29616;&#36328;&#27573;&#27880;&#24847;&#21147;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;Layer-Adjustable Interactions&#25216;&#26415;&#23454;&#29616;&#20998;&#27573;&#32534;&#30721;&#21644;&#36880;&#23618;&#20132;&#20114;&#65292;&#19981;&#20165;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;Transformers&#27169;&#22411;&#65292;&#32780;&#19988;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer encoders contextualize token representations by attending to all other tokens at each layer, leading to quadratic increase in compute effort with the input length. In practice, however, the input text of many NLP tasks can be seen as a sequence of related segments (e.g., the sequence of sentences within a passage, or the hypothesis and premise in NLI). While attending across these segments is highly beneficial for many tasks, we hypothesize that this interaction can be delayed until later encoding stages.  To this end, we introduce Layer-Adjustable Interactions in Transformers (LAIT). Within LAIT, segmented inputs are first encoded independently, and then jointly. This partial two-tower architecture bridges the gap between a Dual Encoder's ability to pre-compute representations for segments and a fully self-attentive Transformer's capacity to model cross-segment attention. The LAIT framework effectively leverages existing pretrained Transformers and converts them into the h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#23454;&#29616;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;&#21333;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#30340;OICA&#38382;&#39064;&#24471;&#21040;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#21487;&#27979;&#35797;&#30340;&#21333;&#28508;&#22312;&#25104;&#20998;&#26465;&#20214;&#65292;&#36890;&#36807;&#36845;&#20195;&#21024;&#38500;&#20849;&#20139;&#35782;&#21035;&#30340;&#28508;&#22312;&#25104;&#20998;&#25104;&#21151;&#25193;&#23637;&#20102;&#32467;&#26524;&#21040;&#22810;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.19582</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#38454;&#32047;&#31215;&#37327;&#30340;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery with Latent Confounders Based on Higher-Order Cumulants. (arXiv:2305.19582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#23454;&#29616;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;&#21333;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#30340;OICA&#38382;&#39064;&#24471;&#21040;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#21487;&#27979;&#35797;&#30340;&#21333;&#28508;&#22312;&#25104;&#20998;&#26465;&#20214;&#65292;&#36890;&#36807;&#36845;&#20195;&#21024;&#38500;&#20849;&#20139;&#35782;&#21035;&#30340;&#28508;&#22312;&#25104;&#20998;&#25104;&#21151;&#25193;&#23637;&#20102;&#32467;&#26524;&#21040;&#22810;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#26576;&#20123;&#36807;&#23436;&#22791;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#65288;OICA&#65289;&#26041;&#27861;&#22312;&#26576;&#20123;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#21457;&#29616;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#28151;&#21512;&#36807;&#31243;&#36981;&#24490;&#21333;&#28508;&#22312;&#20998;&#37327;&#32467;&#26500;&#65289;&#65292;&#23384;&#22312;OICA&#30340;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#37492;&#20110;OICA&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#19982;&#21333;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#30456;&#23545;&#24212;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#20272;&#35745;&#28151;&#21512;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#21487;&#27979;&#35797;&#30340;&#21333;&#28508;&#22312;&#25104;&#20998;&#26465;&#20214;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#24182;&#30830;&#23450;&#22240;&#26524;&#39034;&#24207;&#12290;&#36890;&#36807;&#36845;&#20195;&#21024;&#38500;&#20849;&#20139;&#35782;&#21035;&#30340;&#28508;&#22312;&#25104;&#20998;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#21333;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#22810;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery with latent confounders is an important but challenging task in many scientific areas. Despite the success of some overcomplete independent component analysis (OICA) based methods in certain domains, they are computationally expensive and can easily get stuck into local optima. We notice that interestingly, by making use of higher-order cumulants, there exists a closed-form solution to OICA in specific cases, e.g., when the mixing procedure follows the One-Latent-Component structure. In light of the power of the closed-form solution to OICA corresponding to the One-Latent-Component structure, we formulate a way to estimate the mixing matrix using the higher-order cumulants, and further propose the testable One-Latent-Component condition to identify the latent variables and determine causal orders. By iteratively removing the share identified latent components, we successfully extend the results on the One-Latent-Component structure to the Multi-Latent-Component structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Hadamard&#21442;&#25968;&#21270;&#19979;&#31574;&#30053;&#26799;&#24230;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#20855;&#26377;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#21644;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.19575</link><description>&lt;p&gt;
&#20851;&#20110;Hadamard&#21442;&#25968;&#21270;&#19979;&#31574;&#30053;&#26799;&#24230;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;.
&lt;/p&gt;
&lt;p&gt;
On the Linear Convergence of Policy Gradient under Hadamard Parameterization. (arXiv:2305.19575v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hadamard&#21442;&#25968;&#21270;&#19979;&#31574;&#30053;&#26799;&#24230;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#20855;&#26377;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#21644;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#24335;&#35774;&#32622;&#19979;Hadamard&#21442;&#25968;&#21270;&#19979;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#31639;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#38169;&#35823;&#22312;&#25152;&#26377;&#36845;&#20195;&#20013;&#20197;$O(\frac{1}{k})$&#30340;&#36895;&#29575;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;$k_0$&#27425;&#36845;&#20195;&#20043;&#21518;&#20855;&#26377;&#26356;&#24555;&#30340;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$k_0$&#26159;&#20165;&#20381;&#36182;&#20110;MDP&#38382;&#39064;&#21644;&#27493;&#38271;&#30340;&#24120;&#25968;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35813;&#31639;&#27861;&#26174;&#31034;&#20102;&#19968;&#20010;&#36739;&#24369;&#24120;&#25968;&#30340;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of deterministic policy gradient under the Hadamard parametrization is studied in the tabular setting and the global linear convergence of the algorithm is established. To this end, we first show that the error decreases at an $O(\frac{1}{k})$ rate for all the iterations. Based on this result, we further show that the algorithm has a faster local linear convergence rate after $k_0$ iterations, where $k_0$ is a constant that only depends on the MDP problem and the step size. Overall, the algorithm displays a linear convergence rate for all the iterations with a loose constant than that for the local linear convergence rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#26631;&#31614;&#31227;&#20301;&#38382;&#39064;&#65292;&#22312;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#20445;&#35777;&#20102;&#26368;&#20248;&#21160;&#24577;&#36951;&#25022;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19570</link><description>&lt;p&gt;
&#22312;&#32447;&#26631;&#31614;&#31227;&#20301;&#65306;&#26368;&#20248;&#21160;&#24577;&#36951;&#25022;&#30456;&#36935;&#23454;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms. (arXiv:2305.19570v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#26631;&#31614;&#31227;&#20301;&#38382;&#39064;&#65292;&#22312;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#20445;&#35777;&#20102;&#26368;&#20248;&#21160;&#24577;&#36951;&#25022;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#22312;&#32447;&#26631;&#31614;&#31227;&#20301;&#65292;&#20854;&#20013;&#31867;&#36793;&#38469; $Q(y)$ &#21464;&#21270;&#65292;&#20294;&#31867;&#26465;&#20214; $Q(x|y)$ &#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36866;&#24212;&#19968;&#20010;&#20174;&#26576;&#20123;&#31163;&#32447;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#30340;&#23398;&#20064;&#22120;&#65292;&#20197;&#36866;&#24212;&#32473;&#23450;&#26410;&#26631;&#35760;&#22312;&#32447;&#25968;&#25454;&#19979;&#30340;&#21464;&#21270;&#26631;&#31614;&#20998;&#24067;&#12290;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#24517;&#39035;&#23398;&#20064;&#20998;&#31867;&#22120;&#24182;&#36866;&#24212;&#21482;&#32473;&#23450;&#26377;&#26631;&#35760;&#22312;&#32447;&#25968;&#25454;&#30340;&#21160;&#24577;&#28436;&#21270;&#31867;&#36793;&#38469;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#31639;&#27861;&#65292;&#23558;&#36866;&#24212;&#38382;&#39064;&#20943;&#23569;&#21040;&#22312;&#32447;&#22238;&#24402;&#24182;&#20445;&#35777;&#26080;&#20808;&#39564;&#30693;&#35782;&#19979;&#30340;&#26368;&#20248;&#21160;&#24577;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#24341;&#23548;&#22312;&#32447;&#22238;&#24402;&#39044;&#27979;&#22120;&#20272;&#35745;&#65292;&#20197;&#36319;&#36394;&#28418;&#31227;&#27604;&#20363;&#12290;&#22312;&#20247;&#22810;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22312;&#32447;&#26631;&#31614;&#31227;&#20301;&#22330;&#26223;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#36890;&#24120;&#23454;&#29616;1-3&#65285;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on supervised and unsupervised online label shift, where the class marginals $Q(y)$ varies but the class-conditionals $Q(x|y)$ remain invariant. In the unsupervised setting, our goal is to adapt a learner, trained on some offline labeled data, to changing label distributions given unlabeled online data. In the supervised setting, we must both learn a classifier and adapt to the dynamically evolving class marginals given only labeled online data. We develop novel algorithms that reduce the adaptation problem to online regression and guarantee optimal dynamic regret without any prior knowledge of the extent of drift in the label distribution. Our solution is based on bootstrapping the estimates of \emph{online regression oracles} that track the drifting proportions. Experiments across numerous simulated and real-world online label shift scenarios demonstrate the superior performance of our proposed approaches, often achieving 1-3\% improvement in accuracy while being s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#27880;&#25968;&#25454;&#65292;&#20351;&#29992;&#25513;&#30721;&#27169;&#22359;&#30772;&#22351;&#35821;&#38899;&#36755;&#20837;&#65292;&#24212;&#29992;k&#22343;&#20540;&#32858;&#31867;&#33719;&#24471;&#26631;&#35760;&#24207;&#21015;&#24182;&#20351;&#29992;&#35780;&#20998;&#27169;&#22359;&#27979;&#37327;&#38169;&#35823;&#24674;&#22797;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#22238;&#24402;&#22522;&#32447;&#30456;&#24403;&#30340;&#24615;&#33021;&#20197;&#21450;&#20248;&#20110;&#38750;&#22238;&#24402;&#22522;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19563</link><description>&lt;p&gt;
&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Automatic Pronunciation Assessment. (arXiv:2305.19563v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#27880;&#25968;&#25454;&#65292;&#20351;&#29992;&#25513;&#30721;&#27169;&#22359;&#30772;&#22351;&#35821;&#38899;&#36755;&#20837;&#65292;&#24212;&#29992;k&#22343;&#20540;&#32858;&#31867;&#33719;&#24471;&#26631;&#35760;&#24207;&#21015;&#24182;&#20351;&#29992;&#35780;&#20998;&#27169;&#22359;&#27979;&#37327;&#38169;&#35823;&#24674;&#22797;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#22238;&#24402;&#22522;&#32447;&#30456;&#24403;&#30340;&#24615;&#33021;&#20197;&#21450;&#20248;&#20110;&#38750;&#22238;&#24402;&#22522;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#23545;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#35821;&#35328;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#24102;&#27880;&#37322;&#30340;&#35821;&#38899;&#25991;&#26412;&#25968;&#25454;&#26469;&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#25110;&#20381;&#36182;&#20110;&#24102;&#20998;&#25968;&#30340;&#35821;&#38899;&#25968;&#25454;&#26469;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;HuBERT&#30340;&#20840;&#26032;&#38646;&#26679;&#26412;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#35821;&#38899;&#36755;&#20837;&#36827;&#34892;&#32534;&#30721;&#24182;&#36890;&#36807;&#25513;&#30721;&#27169;&#22359;&#36827;&#34892;&#30772;&#22351;&#12290;&#28982;&#21518;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#24182;&#24212;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20197;&#33719;&#24471;&#26631;&#35760;&#24207;&#21015;&#12290;&#26368;&#21518;&#65292;&#35774;&#35745;&#20102;&#35780;&#20998;&#27169;&#22359;&#26469;&#27979;&#37327;&#38169;&#35823;&#24674;&#22797;&#26631;&#35760;&#30340;&#25968;&#37327;&#12290;&#22312;speechocean762&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#65288;PCC&#65289;&#26041;&#38754;&#19982;&#30417;&#30563;&#22238;&#24402;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#24182;&#19988;&#22312;&#38750;&#22238;&#24402;&#22522;&#32447;&#26041;&#27861;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25513;&#30721;&#31574;&#30053;&#23545;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Pronunciation Assessment (APA) is vital for computer-assisted language learning. Prior methods rely on annotated speech-text data to train Automatic Speech Recognition (ASR) models or speech-score data to train regression models. In this work, we propose a novel zero-shot APA method based on the pre-trained acoustic model, HuBERT. Our method involves encoding speech input and corrupting them via a masking module. We then employ the Transformer encoder and apply k-means clustering to obtain token sequences. Finally, a scoring module is designed to measure the number of wrongly recovered tokens. Experimental results on speechocean762 demonstrate that the proposed method achieves comparable performance to supervised regression baselines and outperforms non-regression baselines in terms of Pearson Correlation Coefficient (PCC). Additionally, we analyze how masking strategies affect the performance of APA.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#21644;&#26494;&#24347;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36825;&#23545;&#20110;RL&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#30340;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;&#20855;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.19562</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#29616;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#21644;&#26494;&#24347;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36825;&#23545;&#20110;RL&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#30340;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;&#20855;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#21487;&#22797;&#29616;&#24615;&#20316;&#20026;&#31639;&#27861;&#23646;&#24615;&#36827;&#34892;&#20102;&#25968;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#30340;&#24102;&#25240;&#25187;&#34920;&#26684;MDP&#30340;&#22522;&#26412;&#35774;&#32622;&#12290;&#21463;Impagliazzo&#31561;&#20154; [2022]&#30340;&#21551;&#21457;&#65292;&#22914;&#26524;&#22312;&#20869;&#37096;&#38543;&#26426;&#24615;&#30456;&#21516;&#26102;&#65292;RL&#31639;&#27861;&#22312;&#20174;&#29983;&#25104;&#22120;&#25277;&#21462;&#30340;&#20004;&#20010;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#26679;&#26412;&#19978;&#25191;&#34892;&#20004;&#27425;&#24182;&#36755;&#20986;&#23436;&#20840;&#30456;&#21516;&#30340;&#31574;&#30053;&#65292;&#21017;&#34920;&#31034;&#35813;RL&#31639;&#27861;&#26159;&#21487;&#22797;&#21046;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#26377;&#25928;&#30340;$\rho$-&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#29992;&#20110;$(\varepsilon,\delta)$-&#26368;&#20248;&#31574;&#30053;&#20272;&#35745;&#65292;&#20854;&#26679;&#26412;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026; $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$&#65292;&#20854;&#20013;$N$&#26159;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#25968;&#37327;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#23376;&#31867;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; $ \Omega\left(\frac {N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right) $ &#38454;&#30340;&#19979;&#38480;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Kalavasis&#31561;&#20154;[2019]&#25552;&#20986;&#30340;&#21487;&#22797;&#21046;&#24615;&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#20854;&#20013;&#20165;&#35201;&#27714;&#31639;&#27861;&#30340;&#36755;&#20986;&#25509;&#36817;&#22797;&#21046;&#31639;&#27861;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#20854;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026; $\widetilde O\left(\frac{N^5\cdot\log(1/\delta)}{(1-\gamma)^9\cdot\varepsilon^4\cdot\rho^2}\right)$&#65292;&#29992;&#20110;$(\varepsilon,\delta)$&#24847;&#20041;&#19979;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#36825;&#27604;&#20808;&#21069;&#19982;&#30456;&#20851;&#38382;&#39064;&#30340;&#30028;&#38480;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;RL&#31639;&#27861;&#35774;&#35745;&#21644;&#21487;&#37325;&#22797;&#24615;&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\rho$-replicable algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and time complexity $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#39044;&#23450;&#21464;&#25442;&#32676;&#19979;&#23398;&#20064;&#19981;&#21464;&#30340;&#23383;&#20856;&#38382;&#39064;&#12290;&#21033;&#29992;&#38750;&#38463;&#36125;&#23572;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#26377;&#25928;&#22320;&#29702;&#35299;&#20026;&#26576;&#20123;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.19557</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#34920;&#31034;&#23398;&#20064;&#23545;&#31216;&#19979;&#30340;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dictionary Learning under Symmetries via Group Representations. (arXiv:2305.19557v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#39044;&#23450;&#21464;&#25442;&#32676;&#19979;&#23398;&#20064;&#19981;&#21464;&#30340;&#23383;&#20856;&#38382;&#39064;&#12290;&#21033;&#29992;&#38750;&#38463;&#36125;&#23572;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#26377;&#25928;&#22320;&#29702;&#35299;&#20026;&#26576;&#20123;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21512;&#36866;&#30340;&#21464;&#25442;&#65292;&#20197;&#20415;&#36890;&#36807;&#31034;&#20363;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#23450;&#30340;&#21464;&#25442;&#32676;&#19979;&#23398;&#20064;&#19981;&#21464;&#30340;&#23383;&#20856;&#38382;&#39064;&#12290;&#33258;&#28982;&#30340;&#24212;&#29992;&#39046;&#22495;&#21253;&#25324;&#20919;&#20923;&#30005;&#38236;&#12289;&#22810;&#30446;&#26631;&#36319;&#36394;&#12289;&#21516;&#27493;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;&#25105;&#20204;&#29305;&#21035;&#20174;&#25968;&#23398;&#34920;&#31034;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38750;&#38463;&#36125;&#23572;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#25105;&#20204;&#20026;&#31526;&#21512;&#36825;&#20123;&#19981;&#21464;&#24615;&#30340;&#23383;&#20856;&#23398;&#20064;&#25552;&#20379;&#20102;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#33258;&#28982;&#30028;&#20013;&#30340;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#33258;&#28982;&#34987;&#24314;&#27169;&#20026;&#26080;&#38480;&#32500;&#24230;&#30340;&#38382;&#39064;&#65292;&#19982;&#30456;&#20851;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#36825;&#24517;&#28982;&#26159;&#26377;&#38480;&#32500;&#24230;&#30340;&#38382;&#39064;&#65292;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#26377;&#25928;&#22320;&#29702;&#35299;&#20026;&#26576;&#20123;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dictionary learning problem can be viewed as a data-driven process to learn a suitable transformation so that data is sparsely represented directly from example data. In this paper, we examine the problem of learning a dictionary that is invariant under a pre-specified group of transformations. Natural settings include Cryo-EM, multi-object tracking, synchronization, pose estimation, etc. We specifically study this problem under the lens of mathematical representation theory. Leveraging the power of non-abelian Fourier analysis for functions over compact groups, we prescribe an algorithmic recipe for learning dictionaries that obey such invariances. We relate the dictionary learning problem in the physical domain, which is naturally modelled as being infinite dimensional, with the associated computational problem, which is necessarily finite dimensional. We establish that the dictionary learning problem can be effectively understood as an optimization instance over certain matrix o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19555</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#20316;&#20026;&#25277;&#35937;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#21151;&#30340;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#65292;LLMs&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#25110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36824;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#20063;&#19981;&#30830;&#23450;&#12290;&#25277;&#35937;&#25512;&#29702;&#26159;&#35748;&#30693;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#25214;&#21040;&#21644;&#24212;&#29992;&#19968;&#33324;&#27169;&#24335;&#12290;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#20197;&#25581;&#31034;&#23427;&#20204;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#22312;&#23616;&#38480;&#24615;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30446;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#25506;&#31350;&#20102;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;2023&#24180;&#23569;&#26679;&#26412;&#24320;&#25918;&#24335;&#20652;&#21270;&#21058;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#36827;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20652;&#21270;&#34920;&#38754;&#19978;&#39044;&#27979;&#20652;&#21270;&#21453;&#24212;&#65292;&#29305;&#21035;&#20851;&#27880;&#21452;&#21407;&#23376;&#20652;&#21270;&#21058;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.19545</link><description>&lt;p&gt;
&#20652;&#21270;&#39311;&#20998;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#24320;&#25918;&#24335;&#20652;&#21270;&#21058;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Catalysis distillation neural network for the few shot open catalyst challenge. (arXiv:2305.19545v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;2023&#24180;&#23569;&#26679;&#26412;&#24320;&#25918;&#24335;&#20652;&#21270;&#21058;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#36827;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20652;&#21270;&#34920;&#38754;&#19978;&#39044;&#27979;&#20652;&#21270;&#21453;&#24212;&#65292;&#29305;&#21035;&#20851;&#27880;&#21452;&#21407;&#23376;&#20652;&#21270;&#21058;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#31185;&#23398;&#30340;&#25972;&#21512;&#20351;&#24471;&#35745;&#31639;&#21270;&#23398;&#26041;&#27861;&#22312;&#35774;&#35745;&#21644;&#21457;&#29616;&#26032;&#22411;&#20652;&#21270;&#21058;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30005;&#20652;&#21270;&#21453;&#24212;&#21644;&#24320;&#23637;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20652;&#21270;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#20381;&#28982;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#36229;&#36807;BERT&#30340;&#26368;&#36817;&#25104;&#21151;&#31361;&#26174;&#20102;&#30740;&#31350;&#20013;&#26377;&#38480;&#30340;&#25968;&#25454;&#12289;&#39640;&#26114;&#30340;&#35745;&#31639;&#12289;&#26102;&#38388;&#38480;&#21046;&#21644;&#32467;&#26500;&#27963;&#24615;&#20851;&#31995;&#22788;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20652;&#21270;&#30340;&#23569;&#26679;&#26412;&#25216;&#26415;&#30340;&#24320;&#21457;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#26080;&#35770;&#29616;&#22312;&#21644;&#26410;&#26469;&#30340;&#38656;&#27714;&#26159;&#20160;&#20040;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;&#23569;&#26679;&#26412;&#24320;&#25918;&#24335;&#20652;&#21270;&#21058;&#25361;&#25112;&#36187;&#65292;&#35813;&#36187;&#20107;&#26088;&#22312;&#25512;&#36827;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#20652;&#21270;&#34920;&#38754;&#19978;&#30340;&#20652;&#21270;&#21453;&#24212;&#65292;&#29305;&#21035;&#20851;&#27880;&#21452;&#21407;&#23376;&#20652;&#21270;&#21058;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of artificial intelligence and science has resulted in substantial progress in computational chemistry methods for the design and discovery of novel catalysts. Nonetheless, the challenges of electrocatalytic reactions and developing a large-scale language model in catalysis persist, and the recent success of ChatGPT's (Chat Generative Pre-trained Transformer) few-shot methods surpassing BERT (Bidirectional Encoder Representation from Transformers) underscores the importance of addressing limited data, expensive computations, time constraints and structure-activity relationship in research. Hence, the development of few-shot techniques for catalysis is critical and essential, regardless of present and future requirements. This paper introduces the Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the application of machine learning technology for predicting catalytic reactions on catalytic surfaces, with a specific focus on dual-atom catalysts in hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20272;&#35745;&#22810;&#20809;&#35889;&#22270;&#20687;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#19979;&#29031;&#26126;&#35889;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19538</link><description>&lt;p&gt;
&#33258;&#21160;&#20809;&#35889;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Automatic Illumination Spectrum Recovery. (arXiv:2305.19538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20272;&#35745;&#22810;&#20809;&#35889;&#22270;&#20687;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#19979;&#29031;&#26126;&#35889;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26469;&#20272;&#35745;&#22810;&#20809;&#35889;&#22270;&#20687;&#22312;&#21508;&#31181;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#29031;&#26126;&#35889;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;IllumNet&#12290;&#20351;&#29992;Specim IQ&#30456;&#26426;&#25293;&#25668;&#21508;&#31181;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#22270;&#20687;&#65292;&#21253;&#25324;&#23460;&#20869;&#22806;&#12290;&#23460;&#22806;&#22270;&#20687;&#22312;&#26228;&#22825;&#12289;&#38452;&#22825;&#21644;&#19981;&#21516;&#26102;&#38388;&#25293;&#25668;&#12290;&#23460;&#20869;&#22270;&#20687;&#20351;&#29992;&#21348;&#32032;&#21644;LED&#20809;&#28304;&#20197;&#21450;&#28151;&#21512;&#20809;&#28304;&#65292;&#20027;&#35201;&#26159;&#21348;&#32032;&#25110;LED&#21644;&#33639;&#20809;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;ResNet18&#32593;&#32476;&#65292;&#20294;&#23558;2D&#20869;&#26680;&#25913;&#20026;3D&#20869;&#26680;&#20197;&#36866;&#24212;&#25968;&#25454;&#30340;&#20809;&#35889;&#29305;&#24615;&#12290;&#38500;&#20102;&#24456;&#22909;&#22320;&#36866;&#24212;&#23454;&#38469;&#29031;&#26126;&#35889;&#22806;&#65292;&#39044;&#27979;&#30340;&#29031;&#26126;&#35889;&#36824;&#24212;&#24179;&#28369;&#65292;&#36825;&#26159;&#36890;&#36807;&#31435;&#26041;&#24179;&#28369;&#26679;&#26465;&#35823;&#24046;&#25104;&#26412;&#20989;&#25968;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#20934;&#30830;&#30340;&#29031;&#26126;&#35889;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a deep learning network to estimate the illumination spectrum of hyperspectral images under various lighting conditions. To this end, a dataset, IllumNet, was created. Images were captured using a Specim IQ camera under various illumination conditions, both indoor and outdoor. Outdoor images were captured in sunny, overcast, and shady conditions and at different times of the day. For indoor images, halogen and LED light sources were used, as well as mixed light sources, mainly halogen or LED and fluorescent. The ResNet18 network was employed in this study, but with the 2D kernel changed to a 3D kernel to suit the spectral nature of the data. As well as fitting the actual illumination spectrum well, the predicted illumination spectrum should also be smooth, and this is achieved by the cubic smoothing spline error cost function. Experimental results indicate that the trained model can infer an accurate estimate of the illumination spectrum.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;</title><link>http://arxiv.org/abs/2305.19535</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32447;&#23398;&#20064;&#30340;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-rank extended Kalman filtering for online learning of neural networks from streaming data. (arXiv:2305.19535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21487;&#33021;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#20013;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;EKF&#65289;&#65292;&#20294;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#30340;&#21518;&#39564;&#31934;&#24230;&#30697;&#38453;&#20998;&#35299;&#65292;&#20854;&#27599;&#27493;&#30340;&#25104;&#26412;&#19982;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#19982;&#22522;&#20110;&#38543;&#26426;&#21464;&#20998;&#25512;&#29702;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#30830;&#23450;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27493;&#38271;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#23548;&#33268;&#26356;&#24555;&#65288;&#26356;&#39640;&#25928;&#65289;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#29992;&#20316;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#30340;&#19968;&#37096;&#20998;&#26102;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient online approximate Bayesian inference algorithm for estimating the parameters of a nonlinear function from a potentially non-stationary data stream. The method is based on the extended Kalman filter (EKF), but uses a novel low-rank plus diagonal decomposition of the posterior precision matrix, which gives a cost per step which is linear in the number of model parameters. In contrast to methods based on stochastic variational inference, our method is fully deterministic, and does not require step-size tuning. We show experimentally that this results in much faster (more sample efficient) learning, which results in more rapid adaptation to changing distributions, and faster accumulation of reward when used as part of a contextual bandit algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;HRR&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#37325;&#26032;&#26500;&#24314;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#25509;&#36817;&#20110;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.19534</link><description>&lt;p&gt;
&#29992;&#20840;&#24687;&#32422;&#21270;&#34920;&#31034;&#37325;&#26032;&#24314;&#27169;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Recasting Self-Attention with Holographic Reduced Representations. (arXiv:2305.19534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;HRR&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#37325;&#26032;&#26500;&#24314;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#25509;&#36817;&#20110;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#27880;&#24847;&#21147;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#38750;&#24120;&#38271;&#30340;&#39046;&#22495;&#20013;&#65292;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(T^2)$&#30340;&#20869;&#23384;&#21644;$\mathcal{O}(T^2 \cdot H)$&#30340;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#20250;&#20351;&#24471;&#20351;&#29992;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#19981;&#21487;&#34892;&#12290;&#21463;&#24778;&#29289;&#26816;&#27979;&#20013;$T \geq 100,000$&#30340;&#24207;&#21015;&#38271;&#24230;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#25318;&#36335;&#34382;&#30340;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20840;&#24687;&#32422;&#21270;&#34920;&#31034;&#65288;HRR&#65289;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#37325;&#26032;&#26500;&#24314;&#33258;&#27880;&#24847;&#21147;&#12290;&#36825;&#26679;&#25105;&#20204;&#25191;&#34892;&#30456;&#21516;&#30340;&#39640;&#32423;&#31574;&#30053;&#65292;&#21363;&#26631;&#20934;&#33258; &#27880;&#24847;&#21147;&#30340;&#26597;&#35810;&#21305;&#37197;&#38053;&#21273;&#65292;&#36820;&#22238;&#27599;&#20010;&#38190;&#30340;&#20540;&#30340;&#21152;&#26435;&#21709;&#24212;&#12290;&#36890;&#36807;&#23454;&#29616;&#8220;Hrrformer&#8221;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20123;&#22909;&#22788;&#65292;&#21253;&#25324;$\mathcal{O}(T H \log H)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12289;$\mathcal{O}(T H)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#25910;&#25947;&#20110;$10\times$&#26356;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;Hrrformer&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24230;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21040;&#28145;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $\mathcal{O}(T^2)$ memory and $\mathcal{O}(T^2 H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \geq 100,000$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including $\mathcal{O}(T H \log H)$ time complexity, $\mathcal{O}(T H)$ space complexity, and convergence in $10\times$ fewer epochs. Nevertheless, the Hrrformer achieves near state-of-the-art accuracy on LRA benchmarks and we are able to learn wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#22312;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19529</link><description>&lt;p&gt;
&#24102;&#26377;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Meta Reinforcement Learning with In-Distribution Online Adaptation. (arXiv:2305.19529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#22312;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#30340;&#34892;&#20026;&#31574;&#30053;(&#20363;&#22914;&#65292;&#23545;&#27599;&#20010;&#20010;&#20307;&#20219;&#21153;&#36827;&#34892;RL&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;)&#26469;&#25910;&#38598;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24635;&#26159;&#38656;&#35201;&#39069;&#22806;&#30340;&#20449;&#24687;&#36827;&#34892;&#24555;&#36895;&#35843;&#25972;&#65292;&#20363;&#22914;&#27979;&#35797;&#20219;&#21153;&#30340;&#31163;&#32447;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#22320;&#34920;&#24449;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#29420;&#29305;&#25361;&#25112;&#65306;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#36866;&#24212;&#20043;&#38388;&#30340;&#36716;&#25442;-&#22870;&#21169;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#26469;&#33258;&#20998;&#24067;&#20043;&#22806;&#30340;&#36866;&#24212;&#24773;&#20917;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#20998;&#24067;&#20869;&#30340;&#24773;&#20917;&#36827;&#34892;&#22312;&#32447;&#36866;&#24212;&#21487;&#20197;&#30830;&#20445;&#36866;&#24212;&#24615;&#33021;&#20445;&#35777;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#26694;&#26550;&#65292;&#31216;&#20026;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#65292;&#23427;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDAQ&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#22312;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent offline meta-reinforcement learning (meta-RL) methods typically utilize task-dependent behavior policies (e.g., training RL agents on each individual task) to collect a multi-task dataset. However, these methods always require extra information for fast adaptation, such as offline context for testing tasks. To address this problem, we first formally characterize a unique challenge in offline meta-RL: transition-reward distribution shift between offline datasets and online adaptation. Our theory finds that out-of-distribution adaptation episodes may lead to unreliable policy evaluation and that online adaptation with in-distribution episodes can ensure adaptation performance guarantee. Based on these theoretical insights, we propose a novel adaptation framework, called In-Distribution online Adaptation with uncertainty Quantification (IDAQ), which generates in-distribution context using a given uncertainty quantification and performs effective task belief inference to address new
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse Invariant Detector&#65288;SID&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20445;&#23432;&#24459;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#30340;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#65292;&#24182;&#19988;&#24050;&#21457;&#29616;&#30340;&#20445;&#23432;&#24459;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19525</link><description>&lt;p&gt;
&#21457;&#29616;&#26032;&#30340;&#21487;&#35299;&#37322;&#20445;&#23432;&#24459;&#20316;&#20026;&#31232;&#30095;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Discovering New Interpretable Conservation Laws as Sparse Invariants. (arXiv:2305.19525v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19525
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse Invariant Detector&#65288;SID&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20445;&#23432;&#24459;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#30340;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#65292;&#24182;&#19988;&#24050;&#21457;&#29616;&#30340;&#20445;&#23432;&#24459;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32473;&#23450;&#21160;&#21147;&#31995;&#32479;&#30340;&#20445;&#23432;&#24459;&#26159;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#35774;&#32622;&#65288;&#24050;&#30693;&#24494;&#20998;&#26041;&#31243;&#21644;&#22522;&#20989;&#25968;&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sparse Invariant Detector&#65288;SID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#33258;&#21160;&#21457;&#29616;&#20445;&#23432;&#24459;&#30340;&#31639;&#27861;&#12290;&#20854;&#31639;&#27861;&#31616;&#21333;&#24615;&#30830;&#20445;&#20102;&#24050;&#21457;&#29616;&#20445;&#23432;&#25968;&#37327;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SID&#33021;&#22815;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#12290;&#22312;&#27969;&#20307;&#21147;&#23398;&#21644;&#22823;&#27668;&#21270;&#23398;&#30340;&#20004;&#20010;&#20363;&#23376;&#20013;&#65292;SID&#20998;&#21035;&#21457;&#29616;&#20102;14&#20010;&#21644;3&#20010;&#23432;&#24658;&#37327;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#19987;&#23478;&#20808;&#21069;&#21482;&#30693;&#36947;12&#20010;&#21644;2&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering conservation laws for a given dynamical system is important but challenging. In a theorist setup (differential equations and basis functions are both known), we propose the Sparse Invariant Detector (SID), an algorithm that auto-discovers conservation laws from differential equations. Its algorithmic simplicity allows robustness and interpretability of the discovered conserved quantities. We show that SID is able to rediscover known and even discover new conservation laws in a variety of systems. For two examples in fluid mechanics and atmospheric chemistry, SID discovers 14 and 3 conserved quantities, respectively, where only 12 and 2 were previously known to domain experts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28176;&#36827;&#24335;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26041;&#27861;&#65288;IRS&#65289;&#65292;&#21487;&#36890;&#36807;&#37325;&#29992;&#21407;&#22987;&#24179;&#28369;&#27169;&#22411;&#30340;&#35748;&#35777;&#20445;&#35777;&#26469;&#35748;&#35777;&#36817;&#20284;&#27169;&#22411;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#35748;&#35777;&#20462;&#25913;DNN&#30340;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19521</link><description>&lt;p&gt;
&#28176;&#36827;&#24335;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental Randomized Smoothing Certification. (arXiv:2305.19521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28176;&#36827;&#24335;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26041;&#27861;&#65288;IRS&#65289;&#65292;&#21487;&#36890;&#36807;&#37325;&#29992;&#21407;&#22987;&#24179;&#28369;&#27169;&#22411;&#30340;&#35748;&#35777;&#20445;&#35777;&#26469;&#35748;&#35777;&#36817;&#20284;&#27169;&#22411;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#35748;&#35777;&#20462;&#25913;DNN&#30340;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;DNN&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#25277;&#26679;&#26469;&#35777;&#26126;&#20854;&#40065;&#26834;&#24615;&#65292;&#20294;&#35745;&#31639;&#20195;&#20215;&#36739;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#20351;&#29992;&#22823;&#37327;&#26679;&#26412;&#36827;&#34892;&#35777;&#26126;&#26102;&#12290;&#27492;&#22806;&#65292;&#24403;&#20462;&#25913;&#24179;&#28369;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#37327;&#21270;&#25110;&#20462;&#21098;&#65289;&#26102;&#65292;&#35748;&#35777;&#20445;&#35777;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#20462;&#25913;&#30340;DNN&#65292;&#24182;&#19988;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35748;&#35777;&#21487;&#33021;&#20195;&#20215;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#28176;&#36827;&#24335;&#40065;&#26834;&#24615;&#35748;&#35777;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65288;IRS&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#24179;&#28369;&#27169;&#22411;&#30340;&#35748;&#35777;&#20445;&#35777;&#65292;&#20197;&#21033;&#29992;&#24456;&#23569;&#30340;&#26679;&#26412;&#35748;&#35777;&#36817;&#20284;&#27169;&#22411;&#12290;IRS&#26174;&#33879;&#38477;&#20302;&#20102;&#35748;&#35777;&#20462;&#25913;DNN&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;IRS&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive.  We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#26816;&#32034;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#26500;&#36896;&#24182;&#21033;&#29992;&#20266;&#24178;&#20928;&#26631;&#31614;&#65292;&#20197;&#20943;&#23567;&#22024;&#26434;&#26631;&#31614;&#23545;&#20998;&#31867;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.19518</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#26816;&#32034;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels. (arXiv:2305.19518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#26816;&#32034;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#26500;&#36896;&#24182;&#21033;&#29992;&#20266;&#24178;&#20928;&#26631;&#31614;&#65292;&#20197;&#20943;&#23567;&#22024;&#26434;&#26631;&#31614;&#23545;&#20998;&#31867;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#20174;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#35838;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#27604;&#36739;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#21463;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#21028;&#26029;&#38543;&#26426;&#30340;&#26631;&#31614;&#12290;&#20316;&#32773;&#21033;&#29992;&#36825;&#19968;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26631;&#31614;&#26816;&#32034;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#26469;&#26377;&#25928;&#30340;&#26500;&#36896;&#21644;&#21033;&#29992;&#20266;&#24178;&#20928;&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23567;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is an important and long-standing problem in machine learning for real applications. One of the main research lines focuses on learning a label corrector to purify potential noisy labels. However, these methods typically rely on strict assumptions and are limited to certain types of label noise. In this paper, we reformulate the label-noise problem from a generative-model perspective, $\textit{i.e.}$, labels are generated by gradually refining an initial random guess. This new perspective immediately enables existing powerful diffusion models to seamlessly learn the stochastic generative process. Once the generative uncertainty is modeled, we can perform classification inference using maximum likelihood estimation of labels. To mitigate the impact of noisy labels, we propose the $\textbf{L}$abel-$\textbf{R}$etrieval-$\textbf{A}$ugmented (LRA) diffusion model, which leverages neighbor consistency to effectively construct pseudo-clean labels for diffusion train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.19512</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#24335;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#22312;&#21487;&#25511;&#21046;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35797;&#22270;&#23558;&#36825;&#31181;&#21487;&#25511;&#24615;&#36816;&#29992;&#21040;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#65288;&#22914;&#39044;&#35757;&#32451;&#26435;&#37325;&#65289;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#24615;&#33021;&#21644;&#21487;&#25511;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;StylePTB&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35780;&#20272;&#20219;&#21153;&#30456;&#27604;&#65292;StylePTB&#20013;&#30340;&#20219;&#21153;&#38656;&#35201;&#23545;&#36755;&#20986;&#25991;&#26412;&#36827;&#34892;&#26356;&#21152;&#31934;&#32454;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;StylePTB&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20010;&#21035;&#21644;&#32452;&#21512;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;StylePTB&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#30340;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#22312;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#35777;&#26126;&#20102;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.19510</link><description>&lt;p&gt;
&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#20855;&#26377;&#26377;&#21033;&#30340;&#25439;&#22833;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape. (arXiv:2305.19510v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#22312;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#35777;&#26126;&#20102;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#65292;&#20108;&#23618;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#20351;&#29992;&#20102;&#21442;&#25968;&#26144;&#23556;&#30340;Jacobian&#30697;&#38453;&#30340;&#31209;&#26469;&#20272;&#35745;&#23616;&#37096;&#21644;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#30340;&#32500;&#24230;&#12290;&#20351;&#29992;&#38543;&#26426;&#20108;&#36827;&#21046;&#30697;&#38453;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#30340;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#22823;&#22810;&#25968;&#21306;&#22495;&#20855;&#26377;&#23436;&#25972;&#30340;&#31209;&#25110;&#32570;&#20047;&#31209;&#65292;&#20197;&#23454;&#39564;&#30340;&#26041;&#24335;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#36825;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parameterization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization.
&lt;/p&gt;</description></item><item><title>M3ICRO&#26159;&#19968;&#31181;&#22522;&#20110;&#23450;&#21046;MOMMI&#22120;&#20214;&#30340;&#26426;&#22120;&#23398;&#20064;&#20809;&#23376;&#24352;&#37327;&#26680;&#24515;&#65292;&#20855;&#26377;&#36229;&#39640;&#33021;&#25928;&#12289;&#32039;&#20945;&#22411;&#35774;&#35745;&#21644;ML for optics&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#22270;&#20687;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#31181;ML&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.19505</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#32534;&#31243;&#22810;&#25805;&#20316;&#22810;&#27169;&#24178;&#28041;&#30340;&#26426;&#22120;&#23398;&#20064;&#20809;&#23376;&#24352;&#37327;&#26680;&#24515;&#8212;&#8212;M3ICRO
&lt;/p&gt;
&lt;p&gt;
M3ICRO: Machine Learning-Enabled Compact Photonic Tensor Core based on PRogrammable Multi-Operand Multimode Interference. (arXiv:2305.19505v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19505
&lt;/p&gt;
&lt;p&gt;
M3ICRO&#26159;&#19968;&#31181;&#22522;&#20110;&#23450;&#21046;MOMMI&#22120;&#20214;&#30340;&#26426;&#22120;&#23398;&#20064;&#20809;&#23376;&#24352;&#37327;&#26680;&#24515;&#65292;&#20855;&#26377;&#36229;&#39640;&#33021;&#25928;&#12289;&#32039;&#20945;&#22411;&#35774;&#35745;&#21644;ML for optics&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#22270;&#20687;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#31181;ML&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23376;&#35745;&#31639;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#20855;&#26377;&#36229;&#24555;&#36895;&#24230;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21644;&#39640;&#33021;&#25928;&#31561;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#26631;&#20934;&#20809;&#23398;&#20803;&#20214;&#30340;&#20809;&#23376;&#24352;&#37327;&#26680;&#24515;&#65288;PTC&#65289;&#35774;&#35745;&#30001;&#20110;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#21344;&#29992;&#38754;&#31215;&#32780;&#38480;&#21046;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#35745;&#31639;&#23494;&#24230;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#27454;&#21033;&#29992;&#23450;&#21046;&#21487;&#32534;&#31243;&#22810;&#25805;&#20316;&#22810;&#27169;&#24178;&#28041;&#65288;MOMMI&#65289;&#22120;&#20214;&#30340;&#36229;&#32039;&#20945;&#22411;PTC&#65292;&#21517;&#20026;M3ICRO&#12290;&#21487;&#32534;&#31243;&#30340;MOMMI&#21033;&#29992;&#20809;&#30340;&#22266;&#26377;&#20256;&#25773;&#21407;&#29702;&#65292;&#25552;&#20379;&#21333;&#35774;&#22791;&#21487;&#32534;&#31243;&#30697;&#38453;&#21333;&#20803;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#35745;&#31639;&#33539;&#24335;&#20013;&#27599;&#20010;&#35774;&#22791;&#19968;&#20010;&#20056;&#31215;&#32047;&#21152;&#65288;MAC&#65289;&#25805;&#20316;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#24120;&#35268;&#20248;&#21270;&#25216;&#26415;&#23545;&#23450;&#21046;&#22120;&#20214;&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#27169;&#25311;&#65292;&#25105;&#20204;&#20351;&#29992;ML for optics&#26469;&#39044;&#27979;&#22120;&#20214;&#34892;&#20026;&#24182;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#20248;&#21270;&#27969;&#31243;&#12290;&#25105;&#20204;&#20840;&#38754;&#30740;&#31350;&#20102;M3ICRO&#30340;&#21487;&#37325;&#26500;&#24615;&#21644;&#30697;&#38453;&#34920;&#29616;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#35774;&#35745;&#30340;&#33021;&#25928;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21152;&#36895;&#22270;&#20687;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photonic computing shows promise for transformative advancements in machine learning (ML) acceleration, offering ultra-fast speed, massive parallelism, and high energy efficiency. However, current photonic tensor core (PTC) designs based on standard optical components hinder scalability and compute density due to their large spatial footprint. To address this, we propose an ultra-compact PTC using customized programmable multi-operand multimode interference (MOMMI) devices, named M3ICRO. The programmable MOMMI leverages the intrinsic light propagation principle, providing a single-device programmable matrix unit beyond the conventional computing paradigm of one multiply-accumulate (MAC) operation per device. To overcome the optimization difficulty of customized devices that often requires time-consuming simulation, we apply ML for optics to predict the device behavior and enable a differentiable optimization flow. We thoroughly investigate the reconfigurability and matrix expressivity 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#22270;&#29109;&#26368;&#23567;&#21270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#35299;&#20915;&#39044;&#27979;&#35823;&#24046;&#12289;&#35757;&#32451;&#36164;&#28304;&#21644;&#25512;&#29702;&#24310;&#36831;&#31561;&#19977;&#20010;&#38382;&#39064;&#12290;&#20854;&#37319;&#29992;&#20174;&#22823;&#37327;&#26410;&#20998;&#31867;&#33410;&#28857;&#20013;&#30340;&#19968;&#36339;&#32858;&#21512;&#36827;&#34892;&#39044;&#27979;&#65292;&#20351;&#24471;&#20854;&#39044;&#27979;&#31934;&#24230;&#19982;&#20855;&#26377;&#20004;&#20010;&#25110;&#26356;&#22810;&#36339;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#30456;&#24403;&#12290;&#32780;&#19988;&#23427;&#36824;&#25903;&#25345;&#38543;&#26426;&#35757;&#32451;&#21644;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#31561;&#21152;&#36895;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.19502</link><description>&lt;p&gt;
&#22270;&#29109;&#26368;&#23567;&#21270;&#29992;&#20110;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Graph Entropy Minimization for Semi-supervised Node Classification. (arXiv:2305.19502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#22270;&#29109;&#26368;&#23567;&#21270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#35299;&#20915;&#39044;&#27979;&#35823;&#24046;&#12289;&#35757;&#32451;&#36164;&#28304;&#21644;&#25512;&#29702;&#24310;&#36831;&#31561;&#19977;&#20010;&#38382;&#39064;&#12290;&#20854;&#37319;&#29992;&#20174;&#22823;&#37327;&#26410;&#20998;&#31867;&#33410;&#28857;&#20013;&#30340;&#19968;&#36339;&#32858;&#21512;&#36827;&#34892;&#39044;&#27979;&#65292;&#20351;&#24471;&#20854;&#39044;&#27979;&#31934;&#24230;&#19982;&#20855;&#26377;&#20004;&#20010;&#25110;&#26356;&#22810;&#36339;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#30456;&#24403;&#12290;&#32780;&#19988;&#23427;&#36824;&#25903;&#25345;&#38543;&#26426;&#35757;&#32451;&#21644;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#31561;&#21152;&#36895;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#39046;&#22495;&#38656;&#35201;&#32508;&#21512;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#12289;&#35757;&#32451;&#36164;&#28304;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#32780;&#33410;&#28857;&#20998;&#31867;&#22120;&#23454;&#29616;&#36825;&#20123;&#26041;&#38754;&#24448;&#24448;&#21482;&#20851;&#27880;&#20854;&#20013;&#19968;&#20004;&#20010;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#22949;&#21327;&#30340;&#26041;&#38754;&#25104;&#20026;&#26368;&#30701;&#30340;&#30701;&#26495;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#24037;&#19994;&#32423;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#29109;&#26368;&#23567;&#21270;&#65288;GEM&#65289;&#65292;&#21487;&#21516;&#26102;&#35299;&#20915;&#36825;&#19977;&#20010;&#38382;&#39064;&#12290;GEM&#20174;&#22823;&#37327;&#26410;&#20998;&#31867;&#33410;&#28857;&#20013;&#21463;&#30410;&#20110;&#20854;&#19968;&#36339;&#32858;&#21512;&#65292;&#20351;&#20854;&#39044;&#27979;&#31934;&#24230;&#19982;&#20855;&#26377;&#20004;&#20010;&#25110;&#26356;&#22810;&#36339;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#30456;&#24403;&#12290;&#23427;&#21487;&#20197;&#34987;&#20998;&#35299;&#20026;&#25903;&#25345;&#26368;&#23567;&#36793;&#32536;&#29420;&#31435;&#26679;&#26412;&#30340;&#38543;&#26426;&#35757;&#32451;&#65292;&#23454;&#29616;&#26497;&#24555;&#30340;&#37319;&#26679;&#21644;&#33410;&#30465;&#31354;&#38388;&#30340;&#35757;&#32451;&#12290;&#34429;&#28982;&#23427;&#30340;&#19968;&#36339;&#32858;&#21512;&#22312;&#25512;&#29702;&#20013;&#27604;&#28145;&#24230;GNN&#24555;&#65292;&#20294;&#36890;&#36807;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#21040;&#26497;&#33268;&#65292;&#20174;&#32780;&#24471;&#21040;&#38750;&#36339;&#33410;&#28857;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classifiers are required to comprehensively reduce prediction errors, training resources, and inference latency in the industry. However, most graph neural networks (GNN) concentrate only on one or two of them. The compromised aspects thus are the shortest boards on the bucket, hindering their practical deployments for industrial-level tasks. This work proposes a novel semi-supervised learning method termed Graph Entropy Minimization (GEM) to resolve the three issues simultaneously. GEM benefits its one-hop aggregation from massive uncategorized nodes, making its prediction accuracy comparable to GNNs with two or more hops message passing. It can be decomposed to support stochastic training with mini-batches of independent edge samples, achieving extremely fast sampling and space-saving training. While its one-hop aggregation is faster in inference than deep GNNs, GEM can be further accelerated to an extreme by deriving a non-hop classifier via online knowledge distillation. Thus,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#21035;&#34913;&#37327;&#20869;&#37096;&#30456;&#20851;&#32467;&#26500;&#30340;&#24046;&#24322;&#21644;&#36793;&#32536;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292; significantly improves the transfer learning performance.</title><link>http://arxiv.org/abs/2305.19499</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20851;&#24615;&#27491;&#21017;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#28145;&#20837;&#25506;&#31350;&#39046;&#22495;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Deep into The Domain Shift: Transfer Learning through Dependence Regularization. (arXiv:2305.19499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#21035;&#34913;&#37327;&#20869;&#37096;&#30456;&#20851;&#32467;&#26500;&#30340;&#24046;&#24322;&#21644;&#36793;&#32536;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292; significantly improves the transfer learning performance.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36890;&#36807;&#35268;&#33539;&#21270;&#28304;&#22495;&#65288;&#26631;&#35760;&#65289;&#20013;&#30340;&#29305;&#24449;&#21644;&#30446;&#26631;&#22495;&#65288;&#26410;&#26631;&#35760;&#65289;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#25972;&#20307;&#20998;&#24067;&#24046;&#24322;&#26469;&#33719;&#24471;&#21487;&#36716;&#31227;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#21306;&#20998;&#39046;&#22495;&#24046;&#24322;&#26159;&#26469;&#33258;&#36793;&#32536;&#20998;&#24067;&#36824;&#26159;&#30456;&#20851;&#32467;&#26500;&#12290;&#22312;&#35768;&#22810;&#19994;&#21153;&#21644;&#37329;&#34701;&#24212;&#29992;&#20013;&#65292;&#26631;&#35760;&#20989;&#25968;&#36890;&#24120;&#23545;&#19982;&#36793;&#32536;&#20998;&#24067;&#21464;&#21270;&#21644;&#30456;&#20851;&#32467;&#26500;&#21464;&#21270;&#30340;&#25935;&#24863;&#31243;&#24230;&#19981;&#21516;&#12290;&#20165;&#20165;&#27979;&#37327;&#25972;&#20307;&#20998;&#24067;&#24046;&#24322;&#22312;&#33719;&#24471;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#19981;&#22815;&#20855;&#26377;&#21028;&#21035;&#21147;&#12290;&#27809;&#26377;&#24517;&#35201;&#30340;&#32467;&#26500;&#20998;&#36776;&#29575;&#65292;&#23398;&#21040;&#30340;&#36716;&#31227;&#25928;&#26524;&#23601;&#20250;&#19981;&#22815;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#21035;&#34913;&#37327;&#20869;&#37096;&#30456;&#20851;&#32467;&#26500;&#30340;&#24046;&#24322;&#21644;&#36793;&#32536;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#20248;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#23545;&#26435;&#37325;&#65292;&#26032;&#30340;&#35268;&#33539;&#21270;&#31574;&#30053;&#22823;&#22823;&#25918;&#26494;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20005;&#26684;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36716;&#31227;&#23398;&#20064;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20851;&#24615;&#27491;&#21017;&#21270;&#65288;DR&#65289;&#26041;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#26032;&#30340;&#30456;&#20851;&#24615;&#25439;&#22833;&#21644;&#19968;&#20010;&#34701;&#21512;&#21464;&#37327;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#19981;&#20551;&#35774;&#20219;&#20309;&#29305;&#23450;&#30340;&#20998;&#24067;&#24418;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#28145;&#24230;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DR &#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical Domain Adaptation methods acquire transferability by regularizing the overall distributional discrepancies between features in the source domain (labeled) and features in the target domain (unlabeled). They often do not differentiate whether the domain differences come from the marginals or the dependence structures. In many business and financial applications, the labeling function usually has different sensitivities to the changes in the marginals versus changes in the dependence structures. Measuring the overall distributional differences will not be discriminative enough in acquiring transferability. Without the needed structural resolution, the learned transfer is less optimal. This paper proposes a new domain adaptation approach in which one can measure the differences in the internal dependence structure separately from those in the marginals. By optimizing the relative weights among them, the new regularization strategy greatly relaxes the rigidness of the existing ap
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24191;&#20041;&#22343;&#34913;&#8221;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#20197;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22914;&#26524;&#27809;&#26377;&#32431;&#32435;&#20160;&#22343;&#34913;&#65292;&#21017;&#19968;&#21517;&#29609;&#23478;&#21487;&#20197;&#20174;&#19981;&#21516;&#31574;&#30053;&#20013;&#21463;&#30410;&#65292;&#32467;&#26524;&#25429;&#33719;&#20102;Stackelberg&#22343;&#34913;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.19496</link><description>&lt;p&gt;
&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#26159;&#21542;&#23545;&#23398;&#20064;&#32773;&#26377;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Learning in Games Good for the Learners?. (arXiv:2305.19496v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19496
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24191;&#20041;&#22343;&#34913;&#8221;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#20197;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22914;&#26524;&#27809;&#26377;&#32431;&#32435;&#20160;&#22343;&#34913;&#65292;&#21017;&#19968;&#21517;&#29609;&#23478;&#21487;&#20197;&#20174;&#19981;&#21516;&#31574;&#30053;&#20013;&#21463;&#30410;&#65292;&#32467;&#26524;&#25429;&#33719;&#20102;Stackelberg&#22343;&#34913;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19982;&#22870;&#21169;&#21644;&#21518;&#24724;&#22312;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#37325;&#22797;&#29609;&#28216;&#25103;&#30456;&#20851;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#20801;&#35768;&#19981;&#23545;&#31216;&#30340;&#21518;&#24724;&#32422;&#26463;&#65292;&#24182;&#20026;&#27599;&#20010;&#20195;&#29702;&#21644;&#19968;&#23545;&#21518;&#24724;&#32422;&#26463;&#27966;&#29983;&#21487;&#34892;&#20540;&#30340;&#22810;&#38754;&#20307;&#12290;&#20316;&#20026;&#26680;&#24515;&#26696;&#20363;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#26041;&#26159;&#31105;&#27490;&#20132;&#25442;&#30340;&#65292;&#21478;&#19968;&#26041;&#30340;&#21518;&#24724;&#27809;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#23427;&#25429;&#33719;&#20102;&#19982;Stackelberg&#22343;&#34913;&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#21487;&#21305;&#37197;&#26368;&#20248;&#20540;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#22823;&#31867;&#28216;&#25103;&#65292;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#65292;&#19968;&#21517;&#29609;&#23478;&#21487;&#20197;&#36890;&#36807;&#20174;&#31105;&#27490;&#20132;&#25442;&#30340;&#21518;&#24724;&#31639;&#27861;&#20013;&#20559;&#31163;&#65292;&#26174;&#33879;&#22686;&#21152;&#33258;&#24049;&#30340;&#25928;&#29992;&#65288;&#23454;&#38469;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#27809;&#26377;&#32431;&#32435;&#20160;&#22343;&#34913;&#30340;&#28216;&#25103;&#37117;&#26159;&#36825;&#31181;&#24418;&#24335;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay between two agents. To facilitate this, we introduce a notion of {\it generalized equilibrium} which allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and pair of regret constraints, where we show that any such equilibrium is reachable by a pair of algorithms which maintain their regret guarantees against arbitrary opponents. As a central example, we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that this captures an extension of {\it Stackelberg} equilibria with a matching optimal value, and that there exists a wide class of games where a player can significantly increase their utility by deviating from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider tradeo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#38544;&#31169;&#20445;&#38556;&#30340;&#33258;&#36866;&#24212;FDR&#25511;&#21046;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;p&#20540;&#36716;&#25442;&#26041;&#27861;&#21644;&#38236;&#20687;&#21093;&#31163;&#31639;&#27861;&#65292;&#21487;&#22312;&#29992;&#25143;&#25351;&#23450;&#30340;&#27700;&#24179;&#945;&#19979;&#30830;&#20999;&#22320;&#25511;&#21046;&#32463;&#20856;&#30340;FDR&#25351;&#26631;&#65292;&#34920;&#29616;&#26356;&#22909;&#19988;&#21487;&#20943;&#23567;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.19482</link><description>&lt;p&gt;
&#24102;&#38544;&#31169;&#20445;&#38556;&#30340;&#33258;&#36866;&#24212;FDR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Adaptive False Discovery Rate Control with Privacy Guarantee. (arXiv:2305.19482v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#38544;&#31169;&#20445;&#38556;&#30340;&#33258;&#36866;&#24212;FDR&#25511;&#21046;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;p&#20540;&#36716;&#25442;&#26041;&#27861;&#21644;&#38236;&#20687;&#21093;&#31163;&#31639;&#27861;&#65292;&#21487;&#22312;&#29992;&#25143;&#25351;&#23450;&#30340;&#27700;&#24179;&#945;&#19979;&#30830;&#20999;&#22320;&#25511;&#21046;&#32463;&#20856;&#30340;FDR&#25351;&#26631;&#65292;&#34920;&#29616;&#26356;&#22909;&#19988;&#21487;&#20943;&#23567;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#22810;&#37325;&#26816;&#39564;&#31243;&#24207;&#21487;&#22312;&#20445;&#35777;&#20551;&#38451;&#24615;&#29575;&#30340;&#21516;&#26102;&#20445;&#25252;&#29992;&#20110;&#20551;&#35774;&#26816;&#39564;&#30340;&#20010;&#20307;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#36866;&#24212;FDR&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#25351;&#23450;&#30340;&#27700;&#24179;&#945;&#19979;&#30830;&#20999;&#22320;&#25511;&#21046;&#32463;&#20856;&#30340;FDR&#25351;&#26631;&#65292;&#24182;&#25552;&#20379;&#38544;&#31169;&#20445;&#38556;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#27934;&#35265;&#65306;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;p&#20540;&#36716;&#25442;&#26041;&#27861;&#65292;&#26082;&#20445;&#25252;&#38544;&#31169;&#65292;&#21516;&#26102;&#21448;&#20445;&#25345;&#20102;&#38236;&#20687;&#20445;&#23432;&#29305;&#24615;&#65307;2&#65289;&#19968;&#31181;&#38236;&#20687;&#21093;&#31163;&#31639;&#27861;&#65292;&#20801;&#35768;&#26500;&#24314;&#36807;&#28388;&#22120;&#65292;&#24182;&#24212;&#29992;&#26368;&#20248;&#20572;&#27490;&#25216;&#26415;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DP-AdaPT&#30456;&#27604;&#29616;&#26377;&#30340;&#24046;&#20998;&#38544;&#31169;FDR&#25511;&#21046;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#38750;&#38544;&#31169;&#30340;AdaPT&#30456;&#27604;&#65292;&#23427;&#20250;&#20135;&#29983;&#19968;&#20123;&#31934;&#24230;&#25439;&#22833;&#65292;&#20294;&#26174;&#33879;&#22320;&#20943;&#23567;&#20102;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private multiple testing procedures can protect the information of individuals used in hypothesis tests while guaranteeing a small fraction of false discoveries. In this paper, we propose a differentially private adaptive FDR control method that can control the classic FDR metric exactly at a user-specified level $\alpha$ with privacy guarantee, which is a non-trivial improvement compared to the differentially private Benjamini-Hochberg method proposed in Dwork et al. (2021). Our analysis is based on two key insights: 1) a novel p-value transformation that preserves both privacy and the mirror conservative property, and 2) a mirror peeling algorithm that allows the construction of the filtration and application of the optimal stopping technique. Numerical studies demonstrate that the proposed DP-AdaPT performs better compared to the existing differentially private FDR control methods. Compared to the non-private AdaPT, it incurs a small accuracy loss but significantly re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19476</link><description>&lt;p&gt;
&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#25506;&#32034;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#26159;&#36890;&#36807;&#40723;&#21169;&#23545;&#35775;&#38382;&#29366;&#24577;&#31354;&#38388;&#30340;&#22343;&#21248;&#35206;&#30422;&#26469;&#26368;&#22823;&#21270;&#24050;&#35775;&#38382;&#29366;&#24577;&#20998;&#24067;&#30340;&#29109;&#65292;&#21363;&#29366;&#24577;&#29109;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26377;&#20219;&#21153;&#22870;&#21169;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#24448;&#24448;&#38590;&#20197;&#24212;&#23545;&#65292;&#20854;&#20013;&#20195;&#29702;&#36235;&#21521;&#20110;&#35775;&#38382;&#39640;&#20215;&#20540;&#29366;&#24577;&#20197;&#21033;&#29992;&#20219;&#21153;&#22870;&#21169;&#12290;&#36825;&#20010;&#20559;&#22909;&#20250;&#23548;&#33268;&#39640;&#20215;&#20540;&#29366;&#24577;&#21644;&#20302;&#20215;&#20540;&#29366;&#24577;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#24403;&#20998;&#24067;&#21464;&#24471;&#26356;&#21152;&#22343;&#21248;&#26102;&#65292;&#29366;&#24577;&#29109;&#20250;&#22686;&#21152;&#65292;&#20174;&#32780;&#20559;&#21521;&#20110;&#25506;&#32034;&#20302;&#20215;&#20540;&#21306;&#22495;&#12290;&#24403;&#39640;&#20215;&#20540;&#29366;&#24577;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#20998;&#24067;&#29421;&#31364;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#36827;&#19968;&#27493;&#24694;&#21270;&#65292;&#20351;&#24471;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#65292;&#23427;&#20998;&#21035;&#20272;&#35745;&#27599;&#20010;&#29366;&#24577;&#20215;&#20540;&#20272;&#35745;&#26465;&#20214;&#19979;&#30340;&#29366;&#24577;&#29109;&#65292;&#28982;&#21518;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#21152;&#26435;&#21644;&#12290;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#37327;&#21270;&#20102;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#21306;&#22495;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20174;&#32780;&#20351;&#20854;&#23545;&#19981;&#24179;&#34913;&#38382;&#39064;&#26356;&#21152;&#20581;&#22766;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20851;&#27880;&#20844;&#24179;&#32858;&#31867;&#38382;&#39064;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#27010;&#24565;&#65292;&#25552;&#20986;&#19968;&#31181;&#21516;&#26102;&#28385;&#36275;&#19981;&#21516;&#20844;&#24179;&#35201;&#27714;&#30340;&#24555;&#36895;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19475</link><description>&lt;p&gt;
&#21452;&#37325;&#32422;&#26463;&#20844;&#24179;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Doubly Constrained Fair Clustering. (arXiv:2305.19475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20844;&#24179;&#32858;&#31867;&#38382;&#39064;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#27010;&#24565;&#65292;&#25552;&#20986;&#19968;&#31181;&#21516;&#26102;&#28385;&#36275;&#19981;&#21516;&#20844;&#24179;&#35201;&#27714;&#30340;&#24555;&#36895;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#20844;&#24179;&#32858;&#31867;&#21560;&#24341;&#20102;&#26174;&#30528;&#30340;&#20851;&#27880;&#65292;&#23548;&#33268;&#20986;&#29616;&#20102;&#22823;&#37327;&#19981;&#21516;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#34429;&#28982;&#36825;&#20123;&#27010;&#24565;&#26159;&#26377;&#20805;&#20998;&#29702;&#25454;&#30340;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#34987;&#32771;&#34385;&#21644;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#20010;&#20844;&#24179;&#35201;&#27714;&#34987;&#29420;&#31435;&#22320;&#32771;&#34385;&#32780;&#19981;&#32771;&#34385;&#20854;&#20182;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#20844;&#24179;&#32858;&#31867;&#20013;&#29702;&#35299;&#19981;&#21516;&#20844;&#24179;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26397;&#21521;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32858;&#31867;&#20013;&#26368;&#31361;&#20986;&#30340;&#20004;&#31181;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#27010;&#24565;:(1)&#38598;&#22242;&#20844;&#27491;(GF)&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#24212;&#35813;&#26377;&#25509;&#36817;&#20154;&#21475;&#27700;&#24179;&#30340;&#20195;&#34920;;(2)&#20013;&#24515;&#36873;&#25321;&#20013;&#30340;&#22810;&#26679;&#24615;(DS)&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25152;&#36873;&#25321;&#30340;&#20013;&#24515;&#24212;&#35813;&#25509;&#36817;&#27599;&#20010;&#32676;&#20307;&#30340;&#20154;&#21475;&#27700;&#24179;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#20010;&#24658;&#23450;&#30340;&#36817;&#20284;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#24555;&#36895;&#30340;&#31639;&#27861;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#36825;&#20004;&#20010;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable attention which fair clustering has received in the last few years has resulted in a significant number of different notions of fairness. Despite the fact that these notions are well-justified, they are often motivated and studied in a disjoint manner where one fairness desideratum is considered exclusively in isolation from the others. This leaves the understanding of the relations between different fairness notions as an important open problem in fair clustering. In this paper, we take the first step in this direction. Specifically, we consider the two most prominent demographic representation fairness notions in clustering: (1) Group Fairness (GF), where the different demographic groups are supposed to have close to population-level representation in each cluster and (2) Diversity in Center Selection (DS), where the selected centers are supposed to have close to population-level representation of each group. We show that given a constant approximation algorithm for on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22522;&#20110;&#23545;&#25968;&#20985;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#65292;&#20351;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#39640;&#32500;&#19979;&#25277;&#26679;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19473</link><description>&lt;p&gt;
&#23545;&#25968;&#20985;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38142;
&lt;/p&gt;
&lt;p&gt;
Chain of Log-Concave Markov Chains. (arXiv:2305.19473v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22522;&#20110;&#23545;&#25968;&#20985;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#65292;&#20351;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#39640;&#32500;&#19979;&#25277;&#26679;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26159;&#19968;&#31181;&#20174;&#26410;&#26631;&#20934;&#21270;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#36890;&#29992;&#31639;&#27861;&#31867;&#12290;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;MCMC&#38754;&#20020;&#20004;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65306;(i)&#24863;&#20852;&#36259;&#30340;&#20998;&#24067;&#22312;&#30001;&#23567;&#27010;&#29575;&#22359;&#38548;&#24320;&#30340;&#21306;&#22495;&#20013;&#38598;&#20013;;(ii)&#23545;&#25968;&#20985;&#24615;&#30340;&#23567;&#27010;&#29575;&#22359;&#26412;&#36523;&#36890;&#24120;&#23384;&#22312;&#30149;&#24577;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37319;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#35770;&#23494;&#24230;&#20989;&#25968;&#30340;&#26368;&#23567;&#20551;&#35774;&#26159;&#20160;&#20040;&#65292;&#20174;&#23494;&#24230;&#20989;&#25968;&#20013;&#37319;&#26679;&#24635;&#26159;&#21487;&#20197;&#20998;&#35299;&#20026;&#36890;&#36807;&#31561;&#22122;&#22768;&#27979;&#37327;&#30340;&#32047;&#31215;&#65292;&#20174;&#23545;&#25968;&#20985;&#24615;&#26465;&#20214;&#23494;&#24230;&#20013;&#37319;&#26679;&#30340;&#24207;&#21015;&#12290;&#35813;&#26500;&#36896;&#36319;&#36394;&#20102;&#26679;&#26412;&#21382;&#21490;&#65292;&#22240;&#27492;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#32780;&#35328;&#26159;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#65292;&#20294;&#21382;&#21490;&#20165;&#20197;&#32463;&#39564;&#22343;&#20540;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#20869;&#23384;&#21360;&#36857;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#25512;&#24191;&#20102;&#27493;&#34892;&#36339;&#36291;&#37319;&#26679;&#65288;1&#65289;&#12290;"&#36208;"&#38454;&#27573;&#21464;&#25104;&#20102;&#23545;&#25968;&#20985;&#38142;&#30340;(&#38750;&#39532;&#23572;&#21487;&#22827;)&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov chain Monte Carlo (MCMC) is a class of general-purpose algorithms for sampling from unnormalized densities. There are two well-known problems facing MCMC in high dimensions: (i) The distributions of interest are concentrated in pockets separated by large regions with small probability mass, and (ii) The log-concave pockets themselves are typically ill-conditioned. We introduce a framework to tackle these problems using isotropic Gaussian smoothing. We prove one can always decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling from log-concave conditional densities via accumulation of noisy measurements with equal noise levels. This construction keeps track of a history of samples, making it non-Markovian as a whole, but the history only shows up in the form of an empirical mean, making the memory footprint minimal. Our sampling algorithm generalizes walk-jump sampling [1]. The "walk" phase becomes a (non-Markovian) chain of log-co
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19470</link><description>&lt;p&gt;
&#29992;Johnson-Lindenstrauss&#30697;&#38453;&#36827;&#34892;&#26631;&#31614;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Johnson-Lindenstrauss&#30697;&#38453;&#65288;JLMs&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#26497;&#31471;&#22810;&#20803;&#20998;&#31867;&#26694;&#26550;&#12290;&#21033;&#29992;JLM&#30340;&#21015;&#26469;&#23884;&#20837;&#26631;&#31614;&#65292;&#23558;&#19968;&#20010;C&#31867;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;$\cO(\log C)$&#36755;&#20986;&#32500;&#24230;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#36229;&#37327;&#39118;&#38505;&#38480;&#21046;&#65292;&#38416;&#26126;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;Massart&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#38477;&#32500;&#30340;&#24809;&#32602;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19466</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#23545; Transformer &#27169;&#22411;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based &#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#65292;&#38271;&#24230;&#25512;&#24191;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#23427;&#26159;&#25351;&#20174;&#23567;&#30340;&#35757;&#32451;&#25991;&#26412;&#33539;&#22260;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#21457;&#29616;&#26159;&#24433;&#21709;&#38271;&#24230;&#25512;&#24191;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#19981;&#21516;&#30340; PE &#26041;&#26696;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#25512;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#20116;&#31181;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;&#21253;&#25324;&#32477;&#23545;&#20301;&#32622;&#23884;&#20837;&#12289;T5 &#30340;&#30456;&#23545; PE&#12289;ALiBi&#12289;Rotary &#21644;&#26080;&#20301;&#32622;&#32534;&#30721;&#65289;&#30340;&#35299;&#30721;&#22120; Transformer &#30340;&#38271;&#24230;&#25512;&#24191;&#33021;&#21147;&#65292;&#23545;&#25512;&#29702;&#21644;&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#22914; ALiBi&#12289;Rotary &#21644; APE&#65292;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#26080; PE &#30340; Transformer &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26174;&#24335; PE &#26041;&#27861;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#23454;&#38469;&#19978;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#26377;&#25928; Transformer &#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#38899;&#35270;&#39057;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#23450;&#20301;&#12289;&#20998;&#31163;&#21644;&#35782;&#21035;&#65292;&#21253;&#25324;&#20849;&#20139;&#30340;&#38899;&#35270;&#39057;&#32534;&#30721;&#22120;&#21644;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#23616;&#37096;&#38899;&#35270;&#39057;&#23545;&#24212;&#25439;&#22833;&#12289;&#28151;&#21512;&#20998;&#31163;&#26694;&#26550;&#21644;&#24378;&#21270;&#35270;&#35273;&#35782;&#21035;&#33021;&#21147;&#26469;&#20248;&#21270;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19458</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#38899;&#35270;&#39057;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20301;&#12289;&#20998;&#31163;&#21644;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition. (arXiv:2305.19458v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#38899;&#35270;&#39057;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#23450;&#20301;&#12289;&#20998;&#31163;&#21644;&#35782;&#21035;&#65292;&#21253;&#25324;&#20849;&#20139;&#30340;&#38899;&#35270;&#39057;&#32534;&#30721;&#22120;&#21644;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#23616;&#37096;&#38899;&#35270;&#39057;&#23545;&#24212;&#25439;&#22833;&#12289;&#28151;&#21512;&#20998;&#31163;&#26694;&#26550;&#21644;&#24378;&#21270;&#35270;&#35273;&#35782;&#21035;&#33021;&#21147;&#26469;&#20248;&#21270;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#20998;&#31163;&#22768;&#28304;&#23545;&#20110;&#20219;&#20309;&#38899;&#35270;&#39057;&#24863;&#30693;&#20219;&#21153;&#26469;&#35828;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#20123;&#33021;&#21147;&#20998;&#21035;&#34987;&#35299;&#20915;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#21333;&#29420;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28304;&#23450;&#20301;&#12289;&#20998;&#31163;&#21644;&#35782;&#21035;&#20855;&#26377;&#30456;&#20114;&#20851;&#32852;&#30340;&#24615;&#36136;&#65292;&#29420;&#31435;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#25429;&#25417;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#38899;&#35270;&#39057;&#23398;&#20064;&#26694;&#26550;(&#31216;&#20026;OneAVM)&#65292;&#23427;&#38598;&#25104;&#20102;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#29992;&#20110;&#32852;&#21512;&#23450;&#20301;&#12289;&#20998;&#31163;&#21644;&#35782;&#21035;&#12290;OneAVM&#21253;&#25324;&#19968;&#20010;&#20849;&#20139;&#30340;&#38899;&#35270;&#39057;&#32534;&#30721;&#22120;&#21644;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#35299;&#30721;&#22120;&#65292;&#20351;&#29992;&#19977;&#20010;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#31532;&#19968;&#20010;&#30446;&#26631;&#36890;&#36807;&#23616;&#37096;&#38899;&#35270;&#39057;&#23545;&#24212;&#25439;&#22833;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#35273;&#34920;&#31034;&#12290;&#31532;&#20108;&#20010;&#30446;&#26631;&#21033;&#29992;&#20256;&#32479;&#30340;&#28151;&#21512;&#20998;&#31163;&#26694;&#26550;&#26469;&#35299;&#20915;&#35270;&#35273;&#28304;&#20998;&#31163;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#31532;&#19977;&#20010;&#30446;&#26631;&#24378;&#21270;&#35270;&#35273;&#35782;&#21035;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#21327;&#21516;&#36827;&#34892;&#23450;&#20301;&#12289;&#20998;&#31163;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to accurately recognize, localize and separate sound sources is fundamental to any audio-visual perception task. Historically, these abilities were tackled separately, with several methods developed independently for each task. However, given the interconnected nature of source localization, separation, and recognition, independent models are likely to yield suboptimal performance as they fail to capture the interdependence between these tasks. To address this problem, we propose a unified audio-visual learning framework (dubbed OneAVM) that integrates audio and visual cues for joint localization, separation, and recognition. OneAVM comprises a shared audio-visual encoder and task-specific decoders trained with three objectives. The first objective aligns audio and visual representations through a localized audio-visual correspondence loss. The second tackles visual source separation using a traditional mix-and-separate framework. Finally, the third objective reinforces vis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Channel-aware dynamic sparse (Chase)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#25805;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#22312;&#36890;&#29992;&#30828;&#20214;&#19978;&#21152;&#36895;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19454</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#26159;&#36890;&#36947;&#32423;&#31232;&#30095;&#30340;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Channel-aware dynamic sparse (Chase)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#25805;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#22312;&#36890;&#29992;&#30828;&#20214;&#19978;&#21152;&#36895;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#30001;&#20110;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#20013;&#20855;&#26377;&#35825;&#20154;&#30340;&#33410;&#30465;&#33021;&#21147;&#32780;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;(DST)&#20316;&#20026;&#19968;&#31181;&#39046;&#20808;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#36798;&#21040;&#19982;&#23494;&#38598;&#23545;&#24212;&#29289;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#39640;&#31232;&#30095;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DST&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#34920;&#26126;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#22312;&#39640;&#24230;&#19981;&#35268;&#21017;&#30340;&#31232;&#30095;&#27169;&#24335;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#19978;&#65292;&#36825;&#22312;&#24120;&#35265;&#30828;&#20214;&#19978;&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#25903;&#25345;&#12290;&#36825;&#31181;&#38480;&#21046;&#38459;&#30861;&#20102;DST&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#36947;&#24863;&#30693;&#21160;&#24577;&#31232;&#30095;&#65288;Chase&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#38750;&#32467;&#26500;&#21270;&#21160;&#24577;&#31232;&#30095;&#30340;&#24615;&#33021;&#36716;&#25442;&#20026;&#36866;&#21512;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#30340;&#25805;&#20316;&#12290;&#25152;&#24471;&#21040;&#30340;&#23567;&#22411;&#31232;&#30095;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;&#36890;&#29992;&#30828;&#20214;&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#19987;&#29992;&#30340;&#31232;&#30095;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Chase&#21487;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#23454;&#29616;&#39640;&#36890;&#36947;&#32423;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#65292;&#24182;&#26174;&#30528;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19452</link><description>&lt;p&gt;
&#26356;&#22823;&#12289;&#26356;&#22909;&#12289;&#26356;&#24555;&#65306;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#30340;&#20154;&#31867;&#32423;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19452
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;BBF&#30340;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23454;&#29616;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;BBF&#20381;&#38752;&#31070;&#32463;&#32593;&#32476;&#30340;&#20215;&#20540;&#20272;&#35745;&#25193;&#23637;&#20197;&#21450;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#65292;&#22312;&#36981;&#24490;&#26679;&#26412;&#39640;&#25928;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26356;&#26032;ALE&#19978;&#26679;&#26412;&#39640;&#25928;&#30340;RL&#30740;&#31350;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#22312;https://github.com/google-research/google-research/tree/master/bigger_better_faster&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OWA&#31639;&#23376;&#30340;&#27169;&#31946;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#36845;&#20195;&#21152;&#26435;&#31574;&#30053;&#24212;&#23545;&#31867;&#21035;&#32423;&#21035;&#22122;&#22768;&#26465;&#20214;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#20256;&#32479;&#30340;&#24120;&#29992;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.19443</link><description>&lt;p&gt;
OWAdapt&#65306;&#20351;&#29992;OWA&#31639;&#23376;&#30340;&#28145;&#24230;&#23398;&#20064;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
OWAdapt: An adaptive loss function for deep learning using OWA operators. (arXiv:2305.19443v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OWA&#31639;&#23376;&#30340;&#27169;&#31946;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#36845;&#20195;&#21152;&#26435;&#31574;&#30053;&#24212;&#23545;&#31867;&#21035;&#32423;&#21035;&#22122;&#22768;&#26465;&#20214;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#20256;&#32479;&#30340;&#24120;&#29992;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#21319;&#20998;&#31867;&#20219;&#21153;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26377;&#25928;&#24212;&#23545;&#31867;&#21035;&#32423;&#21035;&#22122;&#22768;&#26465;&#20214;&#65292;&#21253;&#25324;&#31867;&#21035;&#19981;&#24179;&#34913;&#36825;&#19968;&#38590;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#32858;&#21512;&#31639;&#23376;&#65292;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#33021;&#21147;&#25552;&#39640;&#20102;&#20998;&#31867;&#31934;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#20110;&#25439;&#22833;&#20989;&#25968;&#20869;&#31867;&#21035;&#32423;&#21035;&#32452;&#20214;&#30340;&#36845;&#20195;&#21152;&#26435;&#65292;&#37325;&#28857;&#20851;&#27880;&#37027;&#20123;&#23384;&#22312;&#36739;&#22823;&#35823;&#24046;&#30340;&#32452;&#20214;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#65288;OWA&#65289;&#31639;&#23376;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20108;&#20803;&#21644;&#22810;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#26631;&#20934;&#20132;&#21449;&#29109;&#25110;&#32858;&#28966;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19982;OWA&#31639;&#23376;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a fuzzy adaptive loss function for enhancing deep learning performance in classification tasks. Specifically, we redefine the cross-entropy loss to effectively address class-level noise conditions, including the challenging problem of class imbalance. Our approach introduces aggregation operators, leveraging the power of fuzzy logic to improve classification accuracy. The rationale behind our proposed method lies in the iterative up-weighting of class-level components within the loss function, focusing on those with larger errors. To achieve this, we employ the ordered weighted average (OWA) operator and combine it with an adaptive scheme for gradient-based learning. Through extensive experimentation, our method outperforms other commonly used loss functions, such as the standard cross-entropy or focal loss, across various binary and multiclass classification tasks. Furthermore, we explore the influence of hyperparameters associated with the OWA operators and 
&lt;/p&gt;</description></item><item><title>SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.19442</link><description>&lt;p&gt;
SimFBO&#65306;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#21452;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19442
&lt;/p&gt;
&lt;p&gt;
SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#12289;&#24494;&#35843;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#39046;&#22495;&#20013;&#23884;&#22871;&#20248;&#21270;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32852;&#37030;&#21452;&#23618;&#20248;&#21270;&#65288;FBO&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FBO&#31639;&#27861;&#24448;&#24448;&#28041;&#21450;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#24182;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#22810;&#20010;&#23376;&#24490;&#29615;&#65292;&#27599;&#20010;&#23376;&#24490;&#29615;&#21253;&#21547;&#22810;&#20010;&#36890;&#20449;&#36718;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimFBO&#30340;&#31616;&#21333;&#28789;&#27963;&#30340;FBO&#26694;&#26550;&#65292;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#23376;&#24490;&#29615;&#65292;&#24182;&#21253;&#25324;&#19968;&#31181;&#24191;&#20041;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#21644;&#26356;&#26032;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#31995;&#32479;&#32423;&#24322;&#26500;&#40065;&#26834;FBO&#65288;ShroFBO&#65289;&#20316;&#20026;SimFBO&#30340;&#21464;&#20307;&#65292;&#20854;&#23545;&#26412;&#22320;&#35745;&#31639;&#30340;&#24322;&#26500;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#21644;&#26080;&#26367;&#25442;&#30340;&#23458;&#25143;&#31471;&#37319;&#26679;&#19979;&#65292;SimFBO&#21644;ShroFBO&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#21152;&#36895;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#20803;&#23398;&#20064;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26641;&#29366;&#24352;&#37327;&#32593;&#32476;&#30340;CP&#31209;&#32422;&#26463;&#21644;&#24352;&#37327;&#20002;&#24323;&#65292;&#26469;&#26500;&#24314;&#20302;&#31209;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#26102;&#23578;-MNIST&#22270;&#20687;&#20998;&#31867;&#20013;&#23637;&#31034;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19440</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#24352;&#37327;&#32593;&#32476;&#12289;CP&#31209;&#32422;&#26463;&#21644;&#24352;&#37327;&#20002;&#24323;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning with tree tensor networks, CP rank constraints, and tensor dropout. (arXiv:2305.19440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26641;&#29366;&#24352;&#37327;&#32593;&#32476;&#30340;CP&#31209;&#32422;&#26463;&#21644;&#24352;&#37327;&#20002;&#24323;&#65292;&#26469;&#26500;&#24314;&#20302;&#31209;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#26102;&#23578;-MNIST&#22270;&#20687;&#20998;&#31867;&#20013;&#23637;&#31034;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#38477;&#20302;&#33258;&#30001;&#24230;&#26469;&#36817;&#20284;&#34920;&#31034;$N$&#38454;&#24352;&#37327;&#65292;&#24182;&#26500;&#25104;&#19968;&#31995;&#21015;&#21387;&#32553;&#30340;&#23567;&#24352;&#37327;&#32593;&#32476;&#12290;&#22312;[arXiv:2205.15296]&#25991;&#31456;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#21487;&#20197;&#36890;&#36807;&#23545;&#24352;&#37327;&#32593;&#32476;&#20013;&#30340;&#24352;&#37327;&#30340;CP&#31209;&#38468;&#21152;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#26088;&#22312;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#22522;&#20110;&#26641;&#29366;&#24352;&#37327;&#32593;&#32476;(TTN)&#30340;CP&#31209;&#32422;&#26463;&#21644;&#24352;&#37327;&#20002;&#24323;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26102;&#23578;-MNIST&#22270;&#20687;&#20998;&#31867;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#24403;&#20998;&#25903;&#31995;&#25968;$b=4$&#26102;&#65292;&#20302;&#31209;TTN&#20998;&#31867;&#22120;&#36798;&#21040;&#20102;&#27979;&#35797;&#38598;&#20934;&#30830;&#29575;90.3\%&#65292;&#21516;&#26102;&#25317;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#32447;&#24615;&#20803;&#32032;&#26500;&#25104;&#30340;&#24352;&#37327;&#32593;&#32476;&#20998;&#31867;&#22120;&#36991;&#20813;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;CP&#31209;&#32422;&#26463;&#36824;&#26377;&#20854;&#20182;&#20248;&#28857;&#65306;&#21487;&#20197;&#20943;&#23569;&#21644;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks approximate order-$N$ tensors with a reduced number of degrees of freedom that is only polynomial in $N$ and arranged as a network of partially contracted smaller tensors. As suggested in [arXiv:2205.15296] in the context of quantum many-body physics, computation costs can be further substantially reduced by imposing constraints on the canonical polyadic (CP) rank of the tensors in such networks. Here we demonstrate how tree tensor networks (TTN) with CP rank constraints and tensor dropout can be used in machine learning. The approach is found to outperform other tensor-network based methods in Fashion-MNIST image classification. A low-rank TTN classifier with branching ratio $b=4$ reaches test set accuracy 90.3\% with low computation costs. Consisting of mostly linear elements, tensor network classifiers avoid the vanishing gradient problem of deep neural networks. The CP rank constraints have additional advantages: The number of parameters can be decreased and tuned m
&lt;/p&gt;</description></item><item><title>AdANNS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#30456;&#20284;&#24230;&#35745;&#31639;&#36234;&#25509;&#36817;&#30340;&#25968;&#25454;&#28857;&#23558;&#20351;&#29992;&#26356;&#20302;&#23481;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#35745;&#31639;&#65292;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.19435</link><description>&lt;p&gt;
AdANNS: &#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AdANNS: A Framework for Adaptive Semantic Search. (arXiv:2305.19435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19435
&lt;/p&gt;
&lt;p&gt;
AdANNS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#30456;&#20284;&#24230;&#35745;&#31639;&#36234;&#25509;&#36817;&#30340;&#25968;&#25454;&#28857;&#23558;&#20351;&#29992;&#26356;&#20302;&#23481;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#35745;&#31639;&#65292;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#35268;&#27169;&#30340;&#25628;&#32034;&#31995;&#32479;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#23884;&#20837;&#19968;&#20010;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#28982;&#21518;&#23558;&#20854;&#36830;&#25509;&#21040;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#31649;&#36947;&#20013;&#26469;&#26816;&#32034;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#25429;&#25417;&#23614;&#37096;&#26597;&#35810;&#21644;&#25968;&#25454;&#28857;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36890;&#24120;&#26159;&#21018;&#24615;&#30340;&#12289;&#39640;&#32500;&#30340;&#21521;&#37327;&#65292;&#36890;&#24120;&#22312;&#25972;&#20010;ANNS&#31649;&#36947;&#20013;&#19968;&#25104;&#19981;&#21464;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#26816;&#32034;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#19982;&#20854;&#20351;&#29992;&#21018;&#24615;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;ANNS&#30340;&#19981;&#21516;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#21363;&#21487;&#20197;&#36827;&#34892;&#26356;&#21152;&#36817;&#20284;&#35745;&#31639;&#30340;ANNS&#38454;&#27573;&#24212;&#35813;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#28857;&#30340;&#20302;&#23481;&#37327;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AdANNS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;ANNS&#35774;&#35745;&#26694;&#26550;&#65292;&#26126;&#30830;&#21033;&#29992;Matryoshka&#34920;&#31034;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;AdANNS&#30340;&#26032;&#22411;&#20851;&#38190;ANNS&#26500;&#24314;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#30340;&#20998;&#31867;&#12290;&#20256;&#32479;&#26041;&#27861;&#20250;&#21152;&#21095;&#27495;&#35270;&#12290;&#26412;&#25991;&#35777;&#26126;&#20174;&#25554;&#34917;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20250;&#24694;&#21270;&#32452;&#20844;&#24179;&#24615;&#21644;&#24179;&#22343;&#20934;&#30830;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20844;&#24179;&#24178;&#39044;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22788;&#29702;&#25152;&#26377;&#21487;&#33021;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.19429</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#19979;&#30340;&#20844;&#24179;&#24615;&#24178;&#39044;&#25514;&#26045;&#30340;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Fairness Interventions to Missing Values. (arXiv:2305.19429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#30340;&#20998;&#31867;&#12290;&#20256;&#32479;&#26041;&#27861;&#20250;&#21152;&#21095;&#27495;&#35270;&#12290;&#26412;&#25991;&#35777;&#26126;&#20174;&#25554;&#34917;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20250;&#24694;&#21270;&#32452;&#20844;&#24179;&#24615;&#21644;&#24179;&#22343;&#20934;&#30830;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20844;&#24179;&#24178;&#39044;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22788;&#29702;&#25152;&#26377;&#21487;&#33021;&#30340;&#32570;&#22833;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#20013;&#25968;&#25454;&#30340;&#32570;&#22833;&#20540;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#25552;&#20986;&#20102;&#26174;&#33879;&#32780;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#19981;&#21516;&#30340;&#26063;&#32676;&#21487;&#33021;&#19981;&#20250;&#21516;&#31561;&#22320;&#21463;&#21040;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#32780;&#22788;&#29702;&#32570;&#22833;&#20540;&#30340;&#26631;&#20934;&#31243;&#24207;&#65292;&#21363;&#20808;&#23545;&#25968;&#25454;&#36827;&#34892;&#25554;&#34917;&#65292;&#28982;&#21518;&#20351;&#29992;&#25554;&#34917;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#8220;&#25554;&#34917;&#20877;&#20998;&#31867;&#8221;&#65292;&#20250;&#21152;&#21095;&#27495;&#35270;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#32570;&#22833;&#20540;&#22914;&#20309;&#24433;&#21709;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#20174;&#25554;&#34917;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20250;&#26174;&#33879;&#24694;&#21270;&#21487;&#20197;&#23454;&#29616;&#30340;&#32452;&#20844;&#24179;&#24615;&#21644;&#24179;&#22343;&#20934;&#30830;&#24615;&#30340;&#20540;&#12290;&#36825;&#26159;&#22240;&#20026;&#25554;&#34917;&#25968;&#25454;&#20250;&#23548;&#33268;&#25968;&#25454;&#32570;&#22833;&#27169;&#24335;&#30340;&#20002;&#22833;&#65292;&#25968;&#25454;&#32570;&#22833;&#27169;&#24335;&#36890;&#24120;&#20250;&#20256;&#36798;&#26377;&#20851;&#39044;&#27979;&#26631;&#31614;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#20540;&#30340;&#20844;&#24179;&#20998;&#31867;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22788;&#29702;&#25152;&#26377;&#21487;&#33021;&#30340;&#32570;&#22833;&#27169;&#24335;&#65292;&#24182;&#20445;&#30041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing values in real-world data pose a significant and unique challenge to algorithmic fairness. Different demographic groups may be unequally affected by missing data, and the standard procedure for handling missing values where first data is imputed, then the imputed data is used for classification -- a procedure referred to as "impute-then-classify" -- can exacerbate discrimination. In this paper, we analyze how missing values affect algorithmic fairness. We first prove that training a classifier from imputed data can significantly worsen the achievable values of group fairness and average accuracy. This is because imputing data results in the loss of the missing pattern of the data, which often conveys information about the predictive label. We present scalable and adaptive algorithms for fair classification with missing values. These algorithms can be combined with any preexisting fairness-intervention algorithm to handle all possible missing patterns while preserving informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#19982;&#30456;&#20851;&#24037;&#20316;&#26377;&#20851;&#30340;&#32972;&#26223;&#34920;&#31034;&#27861;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21644;SHAP&#26041;&#27861;&#35780;&#20272;&#20102;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#22312;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#20013;&#30340;&#36129;&#29486;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25551;&#36848;&#19982;&#22522;&#30784;&#35774;&#26045;&#32593;&#32476;&#20851;&#31995;&#30340;&#29305;&#24449;&#23545;&#39044;&#27979;&#26377;&#26174;&#30528;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.19428</link><description>&lt;p&gt;
&#35780;&#20272;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#22312;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating geospatial context information for travel mode detection. (arXiv:2305.19428v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#19982;&#30456;&#20851;&#24037;&#20316;&#26377;&#20851;&#30340;&#32972;&#26223;&#34920;&#31034;&#27861;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21644;SHAP&#26041;&#27861;&#35780;&#20272;&#20102;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#22312;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#20013;&#30340;&#36129;&#29486;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25551;&#36848;&#19982;&#22522;&#30784;&#35774;&#26045;&#32593;&#32476;&#20851;&#31995;&#30340;&#29305;&#24449;&#23545;&#39044;&#27979;&#26377;&#26174;&#30528;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20840;&#29699;&#23450;&#20301;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#36712;&#36857;&#26816;&#27979;&#20986;&#34892;&#26041;&#24335;&#23545;&#20102;&#35299;&#20010;&#20154;&#20986;&#34892;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#23454;&#29616;&#21487;&#25345;&#32493;&#20132;&#36890;&#31995;&#32479;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#34429;&#28982;&#30740;&#31350;&#24050;&#32463;&#35748;&#35782;&#21040;&#23558;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#32435;&#20837;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#22909;&#22788;&#65292;&#20294;&#24456;&#23569;&#26377;&#25991;&#31456;&#24635;&#32467;&#20102;&#32972;&#26223;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#32972;&#26223;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#22952;&#30861;&#20102;&#39640;&#25928;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#19982;&#30456;&#20851;&#24037;&#20316;&#26377;&#20851;&#30340;&#32972;&#26223;&#34920;&#31034;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#31649;&#36947;&#65292;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21644;SHapley Additive exPlanation&#65288;SHAP&#65289;&#26041;&#27861;&#35780;&#20272;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#22312;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#20013;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;GNSS&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25551;&#36848;&#19982;&#22522;&#30784;&#35774;&#26045;&#32593;&#32476;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#21040;&#38081;&#36335;&#25110;&#36947;&#36335;&#32593;&#32476;&#30340;&#36317;&#31163;&#65292;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#26377;&#26174;&#30528;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting travel modes from global navigation satellite system (GNSS) trajectories is essential for understanding individual travel behaviour and a prerequisite for achieving sustainable transport systems. While studies have acknowledged the benefits of incorporating geospatial context information into travel mode detection models, few have summarized context modelling approaches and analyzed the significance of these context features, hindering the development of an efficient model. Here, we identify context representations from related work and propose an analytical pipeline to assess the contribution of geospatial context information for travel mode detection based on a random forest model and the SHapley Additive exPlanation (SHAP) method. Through experiments on a large-scale GNSS tracking dataset, we report that features describing relationships with infrastructure networks, such as the distance to the railway or road network, significantly contribute to the model's prediction. Mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Scoped Negation NLI (ScoNe-NLI)&#22522;&#20934;&#27979;&#35797;&#20197;&#35780;&#20272;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#21542;&#23450;&#25512;&#29702;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36827;&#34892;&#35768;&#22810;&#27425;&#24494;&#35843;&#21518;&#65292;RoBERTa&#21644;DeBERTa&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;ScoNe-NLI&#12290;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#25552;&#31034;&#31574;&#30053;&#37117;&#26080;&#27861;&#25104;&#21151;&#65292;&#20294;&#22312;&#23884;&#20837;&#21542;&#23450;&#25512;&#29702;&#30340;&#30701;&#25925;&#20107;&#30340;&#21477;&#23376;&#23436;&#25104;&#27979;&#35797;&#20013;&#65292;InstructGPT&#26159;&#25104;&#21151;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.19426</link><description>&lt;p&gt;
ScoNe: &#29992;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21542;&#23450;&#25512;&#29702;&#34920;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning. (arXiv:2305.19426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Scoped Negation NLI (ScoNe-NLI)&#22522;&#20934;&#27979;&#35797;&#20197;&#35780;&#20272;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#21542;&#23450;&#25512;&#29702;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36827;&#34892;&#35768;&#22810;&#27425;&#24494;&#35843;&#21518;&#65292;RoBERTa&#21644;DeBERTa&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;ScoNe-NLI&#12290;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#25552;&#31034;&#31574;&#30053;&#37117;&#26080;&#27861;&#25104;&#21151;&#65292;&#20294;&#22312;&#23884;&#20837;&#21542;&#23450;&#25512;&#29702;&#30340;&#30701;&#25925;&#20107;&#30340;&#21477;&#23376;&#23436;&#25104;&#27979;&#35797;&#20013;&#65292;InstructGPT&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#22522;&#20934;&#27979;&#35797;&#35797;&#22270;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#21542;&#23450;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#21463;&#25511;&#30340;&#31034;&#20363;&#33539;&#20363;&#65292;&#26080;&#27861;&#25512;&#26029;&#27169;&#22411;&#26159;&#21542;&#24050;&#32463;&#23398;&#20250;&#20102;&#21542;&#23450;&#35821;&#32032;&#30340;&#35821;&#20041;&#20316;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#20998;&#26512;&#19978;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scoped Negation NLI (ScoNe-NLI)&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20845;&#20010;&#23545;&#27604;&#31034;&#20363;&#32452;&#25104;&#30340;&#38598;&#21512;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#36798;&#20004;&#20010;&#21542;&#23450;&#35821;&#32032;&#65292;&#20854;&#20013;&#38646;&#20010;&#12289;&#19968;&#20010;&#25110;&#20004;&#20010;&#21542;&#23450;&#35821;&#32032;&#24433;&#21709;NLI&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;ScoNe-NLI&#35780;&#20272;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;RoBERTa&#21644;DeBERTa&#27169;&#22411;&#22312;&#36827;&#34892;&#35768;&#22810;&#27425;&#24494;&#35843;&#20043;&#21518;&#21487;&#20197;&#35299;&#20915;ScoNe-NLI&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;InstructGPT&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#25552;&#31034;&#31574;&#30053;&#37117;&#19981;&#25104;&#21151;&#65292;&#21253;&#25324;&#37027;&#20123;&#20351;&#29992;&#36880;&#27493;&#25512;&#29702;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23558;ScoNe&#25193;&#23637;&#20026;ScoNe-NLG&#65292;&#36825;&#26159;&#19968;&#20010;&#23884;&#20837;&#20102;&#21542;&#23450;&#25512;&#29702;&#30340;&#30701;&#25925;&#20107;&#30340;&#21477;&#23376;&#23436;&#25104;&#27979;&#35797;&#38598;&#12290;&#22312;&#36825;&#37324;&#65292;InstructGPT&#26159;&#25104;&#21151;&#30340;&#65292;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37327;&#21270;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26368;&#21518;&#19968;&#23618;&#30340;&#38646;&#31354;&#38388;&#20998;&#26512;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#24182;&#20445;&#35777;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19424</link><description>&lt;p&gt;
&#37327;&#21270;&#36807;&#25311;&#21512;&#65306;&#36890;&#36807;&#38646;&#31354;&#38388;&#20998;&#26512;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Quantifying Overfitting: Evaluating Neural Network Performance through Analysis of Null Space. (arXiv:2305.19424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37327;&#21270;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26368;&#21518;&#19968;&#23618;&#30340;&#38646;&#31354;&#38388;&#20998;&#26512;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#24182;&#20445;&#35777;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33509;&#36807;&#25311;&#21512;/&#35757;&#32451;&#65292;&#21017;&#26356;&#23481;&#26131;&#21463;&#21040;&#30693;&#35782;&#27844;&#28431;&#30340;&#23041;&#32961;&#65292;&#20174;&#32780;&#23545;&#38544;&#31169;&#26500;&#25104;&#39118;&#38505;&#12290;&#26412;&#25991;&#38024;&#23545;&#22914;&#20309;&#22312;&#19981;&#30693;&#36947;&#27169;&#22411;&#30340;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#36807;&#25311;&#21512;&#25110;&#36807;&#35757;&#32451;&#32780;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#26368;&#21518;&#19968;&#23618;&#30340;&#38646;&#31354;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#37327;&#21270;&#36807;&#25311;&#21512;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#30693;&#36947;&#37027;&#20123;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#26041;&#27861;&#30340;&#38544;&#31169;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models that are overfitted/overtrained are more vulnerable to knowledge leakage, which poses a risk to privacy. Suppose we download or receive a model from a third-party collaborator without knowing its training accuracy. How can we determine if it has been overfitted or overtrained on its training data? It's possible that the model was intentionally over-trained to make it vulnerable during testing. While an overfitted or overtrained model may perform well on testing data and even some generalization tests, we can't be sure it's not over-fitted. Conducting a comprehensive generalization test is also expensive. The goal of this paper is to address these issues and ensure the privacy and generalization of our method using only testing data. To achieve this, we analyze the null space in the last layer of neural networks, which enables us to quantify overfitting without access to training data or knowledge of the accuracy of those data. We evaluated our approach on variou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#36710;&#22330;&#26223;&#25968;&#25454;&#38598;&#21644;&#30693;&#35782;&#34920;&#31034;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19421</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#36229;&#36710;&#22330;&#26223;&#25968;&#25454;&#21644;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Data and Knowledge for Overtaking Scenarios in Autonomous Driving. (arXiv:2305.19421v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#36710;&#22330;&#26223;&#25968;&#25454;&#38598;&#21644;&#30693;&#35782;&#34920;&#31034;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#27969;&#34892;&#30340;&#30740;&#31350;&#26041;&#21521;&#20043;&#19968;&#12290;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#24863;&#30693;&#12289;&#20915;&#31574;&#21046;&#23450;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#20219;&#21153;&#65292;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#37117;&#38656;&#35201;&#36710;&#36742;&#25910;&#38598;&#21608;&#22260;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#20570;&#20986;&#33391;&#22909;&#30340;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;&#36229;&#36710;&#22330;&#26223;&#30340;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#36229;&#36710;&#21160;&#20316;&#30340;&#30693;&#35782;&#34920;&#31034;&#27169;&#22411;&#65292;&#38500;&#20102;&#29992;&#20110;&#24863;&#30693;&#21644;&#20915;&#31574;&#21046;&#23450;&#20219;&#21153;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#21644;&#25511;&#21046;&#31639;&#27861;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving has become one of the most popular research topics within Artificial Intelligence. An autonomous vehicle is understood as a system that combines perception, decision-making, planning, and control. All of those tasks require that the vehicle collects surrounding data in order to make a good decision and action. In particular, the overtaking maneuver is one of the most critical actions of driving. The process involves lane changes, acceleration and deceleration actions, and estimation of the speed and distance of the vehicle in front or in the lane in which it is moving. Despite the amount of work available in the literature, just a few handle overtaking maneuvers and, because overtaking can be risky, no real-world dataset is available. This work contributes in this area by presenting a new synthetic dataset whose focus is the overtaking maneuver. We start by performing a thorough review of the state of the art in autonomous driving and then explore the main datasets f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;In-Context Learning&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#26469;&#38544;&#24335;&#22320;&#23454;&#29616;ICL&#20272;&#35745;&#37327;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;ICL&#24615;&#33021;&#65292;&#24314;&#31435;&#21518;&#24724;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#36817;&#20284;&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.19420</link><description>&lt;p&gt;
In-Context Learning&#23398;&#20064;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#23398;&#20064;&#65311;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#21442;&#25968;&#21270;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization. (arXiv:2305.19420v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;In-Context Learning&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#26469;&#38544;&#24335;&#22320;&#23454;&#29616;ICL&#20272;&#35745;&#37327;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;ICL&#24615;&#33021;&#65292;&#24314;&#31435;&#21518;&#24724;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#36817;&#20284;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22238;&#31572;&#20960;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#23545;In-Context Learning&#65288;ICL&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65306;(a)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#26159;&#20160;&#20040;&#31867;&#22411;&#30340;ICL&#20272;&#35745;&#37327;&#65311;(b)&#36866;&#21512;&#35780;&#20272;ICL&#30340;&#24615;&#33021;&#24230;&#37327;&#26159;&#20160;&#20040;&#65292;&#24182;&#19988;&#38169;&#35823;&#29575;&#26159;&#22810;&#23569;&#65311;(c)Transformer&#26550;&#26500;&#22914;&#20309;&#23454;&#29616;ICL&#65311;&#20026;&#20102;&#22238;&#31572;(a)&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#36125;&#21494;&#26031;&#35266;&#28857;&#65292;&#24182;&#35777;&#26126;ICL&#38544;&#21547;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#36817;&#20284;&#21442;&#25968;&#21270;&#12290;(b)&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#20998;&#26512;ICL&#24615;&#33021;&#65292;&#24314;&#31435;&#19968;&#20010;&#21518;&#24724;&#30028;&#38480; $\mathcal{O}(1/T)$&#65292;&#20854;&#20013;$T$&#26159;ICL&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#12290;(c)&#38500;&#20102;&#22312;&#20851;&#27880;&#20013;&#32534;&#30721;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;&#28041;&#21450;&#26399;&#38388;&#65292;&#23398;&#20064;&#27169;&#22411;&#21644;&#21517;&#20041;&#27169;&#22411;&#20043;&#38388;&#30340;&#24635;&#21464;&#20998;&#36317;&#31163;&#34987;&#19968;&#20010;&#36817;&#20284;&#35823;&#24046;&#21644;&#19968;&#20010;&#27867;&#21270;&#35823;&#24046;&#20043;&#21644;&#25152;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned within language models? (b) What are suitable performance metrics to evaluate ICL accurately and what are the error rates? (c) How does the transformer architecture enable ICL? To answer (a), we take a Bayesian view and demonstrate that ICL implicitly implements the Bayesian model averaging algorithm. This Bayesian model averaging algorithm is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a regret bound $\mathcal{O}(1/T)$, where $T$ is the ICL input sequence length. To address (c), in addition to the encoded Bayesian model averaging algorithm in attention, we show that during pertaining, the total variation distance between the learned model and the nominal model is bounded by a sum of an approximation error and a genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KrAD&#30340;&#26032;&#30340;Kronecker&#20998;&#35299;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#20013;&#20108;&#38454;&#20248;&#21270;&#22120;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#35201;&#27714;&#12290;&#36890;&#36807;KrADagrad&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;64&#20301;&#31934;&#24230;&#35201;&#27714;&#65292;&#24182;&#22312;32&#20301;&#31934;&#24230;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19416</link><description>&lt;p&gt;
KrADagrad&#65306;Kronecker&#36817;&#20284;-&#20027;&#23548;&#26799;&#24230;&#39044;&#22788;&#29702;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned Stochastic Optimization. (arXiv:2305.19416v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KrAD&#30340;&#26032;&#30340;Kronecker&#20998;&#35299;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#20013;&#20108;&#38454;&#20248;&#21270;&#22120;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#35201;&#27714;&#12290;&#36890;&#36807;KrADagrad&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;64&#20301;&#31934;&#24230;&#35201;&#27714;&#65292;&#24182;&#22312;32&#20301;&#31934;&#24230;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#38454;&#38543;&#26426;&#20248;&#21270;&#22120;&#20801;&#35768;&#21442;&#25968;&#26356;&#26032;&#27493;&#38271;&#21644;&#26041;&#21521;&#36866;&#24212;&#25439;&#22833;&#26354;&#29575;&#65292;&#20294;&#20256;&#32479;&#19978;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#32780;&#35328;&#38656;&#35201;&#22826;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#65292;Shampoo [Gupta et al.&#65292;2018]&#24341;&#20837;&#20102;Kronecker&#20998;&#35299;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#20123;&#35201;&#27714;&#65306; &#23427;&#29992;&#20110;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;[Anil et al.&#65292;2020]&#24182;&#19988;&#22312;&#29983;&#20135;&#20013;[Anil et al.&#65292;2022]&#12290; &#20294;&#26159;&#65292;&#23427;&#38656;&#35201;&#27714;&#35299;&#30149;&#24577;&#30697;&#38453;&#30340;&#36870;&#30697;&#38453;&#26681;&#12290;&#36825;&#38656;&#35201;64&#20301;&#31934;&#24230;&#65292;&#20250;&#20135;&#29983;&#24378;&#30828;&#20214;&#38480;&#21046;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#21363;Kronecker&#36817;&#20284;-&#20027;&#23548;&#65288;KrAD&#65289;&#12290; &#20351;&#29992;KrAD&#65292;&#25105;&#20204;&#26356;&#26032;&#19968;&#20010;&#30697;&#38453;&#65292;&#30452;&#25509;&#36817;&#20284;&#36870;&#32463;&#39564;Fisher&#30697;&#38453;&#65288;&#31867;&#20284;&#20110;&#23436;&#25972;&#30697;&#38453;AdaGrad&#65289;&#65292;&#36991;&#20813;&#27714;&#36870;&#30697;&#38453;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;64&#20301;&#31934;&#24230;&#12290;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;KrADagrad$^\star$&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#19982;Shampoo&#30456;&#20284;&#24182;&#20855;&#26377;&#30456;&#21516;&#30340;&#21518;&#24724;&#20540;&#12290;&#22312;32&#20301;&#31934;&#24230;&#19979;&#65292;&#21512;&#25104;&#30340;&#30149;&#24577;&#23454;&#39564;&#34920;&#29616;&#20248;&#20110;Shampoo&#65292;&#21516;&#26102;&#22312;64&#20301;&#31934;&#24230;&#19979;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20026;KrADagrad&#30340;&#25910;&#25947;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Second order stochastic optimizers allow parameter update step size and direction to adapt to loss curvature, but have traditionally required too much memory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018] introduced a Kronecker factored preconditioner to reduce these requirements: it is used for large deep models [Anil et al., 2020] and in production [Anil et al., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices. This requires 64-bit precision, imposing strong hardware constraints. In this paper, we propose a novel factorization, Kronecker Approximation-Domination (KrAD). Using KrAD, we update a matrix that directly approximates the inverse empirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and hence 64-bit precision. We then propose KrADagrad$^\star$, with similar computational costs to Shampoo and the same regret. Synthetic ill-conditioned experiments show improved performance over Shampoo for 32-bit precision, while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;Jarzynski&#24179;&#31561;&#24335;&#21644;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#32469;&#36807;&#26631;&#20934;&#23545;&#27604;&#25955;&#24230;&#31639;&#27861;&#20013;&#30340;&#19981;&#21487;&#25511;&#36924;&#36817;&#35823;&#24046;&#65292;&#26377;&#25928;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19414</link><description>&lt;p&gt;
&#21033;&#29992;Jarzynski&#24179;&#31561;&#24335;&#39640;&#25928;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Energy-Based Models Using Jarzynski Equality. (arXiv:2305.19414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;Jarzynski&#24179;&#31561;&#24335;&#21644;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#32469;&#36807;&#26631;&#20934;&#23545;&#27604;&#25955;&#24230;&#31639;&#27861;&#20013;&#30340;&#19981;&#21487;&#25511;&#36924;&#36817;&#35823;&#24046;&#65292;&#26377;&#25928;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411;&#26159;&#21463;&#32479;&#35745;&#29289;&#29702;&#21551;&#21457;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#20998;&#24067;&#30456;&#23545;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#20132;&#21449;&#29109;&#26159;&#34913;&#37327;&#23427;&#20204;&#24615;&#33021;&#30340;&#26368;&#20339;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#25361;&#25112;&#37325;&#37325;&#65292;&#22240;&#20026;&#23427;&#23545;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#35745;&#31639;&#38656;&#35201;&#23545;&#27169;&#22411;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Jarzynski&#31561;&#24335;&#30340;&#38750;&#24179;&#34913;&#28909;&#21147;&#23398;&#32467;&#26524;&#65292;&#32467;&#21512;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#35745;&#31639;&#65292;&#36991;&#20813;&#20351;&#29992;&#26631;&#20934;&#23545;&#27604;&#25955;&#24230;&#31639;&#27861;&#25152;&#20135;&#29983;&#30340;&#19981;&#21487;&#25511;&#36924;&#36817;&#35823;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26410;&#35843;&#25972;Langevin&#31639;&#27861;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;Walker&#37117;&#20250;&#33719;&#24471;&#19968;&#20010;&#26435;&#37325;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#20219;&#20309;&#27493;&#39588;&#26102;&#20272;&#35745;&#20132;&#21449;&#29109;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#35268;&#36991;&#30001;&#37319;&#26679;&#20559;&#24046;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models (EBMs) are generative models inspired by statistical physics with a wide range of applications in unsupervised learning. Their performance is best measured by the cross-entropy (CE) of the model distribution relative to the data distribution. Using the CE as the objective for training is however challenging because the computation of its gradient with respect to the model parameters requires sampling the model distribution. Here we show how results for nonequilibrium thermodynamics based on Jarzynski equality together with tools from sequential Monte-Carlo sampling can be used to perform this computation efficiently and avoid the uncontrolled approximations made using the standard contrastive divergence algorithm. Specifically, we introduce a modification of the unadjusted Langevin algorithm (ULA) in which each walker acquires a weight that enables the estimation of the gradient of the cross-entropy at any step during GD, thereby bypassing sampling biases induced by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;FRAMM&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#20020;&#24202;&#35797;&#39564;&#36873;&#22336;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#32570;&#22833;&#21644;&#20248;&#21270;&#25307;&#21215;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#30495;&#23454;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FRAMM&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#35797;&#39564;&#36873;&#22336;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#25307;&#21215;&#29575;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19407</link><description>&lt;p&gt;
FRAMM&#65306;&#38024;&#23545;&#20020;&#24202;&#35797;&#39564;&#25307;&#21215;&#19981;&#36275;&#30340;&#20844;&#24179;&#25490;&#21517;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection. (arXiv:2305.19407v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;FRAMM&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#20020;&#24202;&#35797;&#39564;&#36873;&#22336;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#32570;&#22833;&#21644;&#20248;&#21270;&#25307;&#21215;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#30495;&#23454;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FRAMM&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#35797;&#39564;&#36873;&#22336;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#25307;&#21215;&#29575;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#20570;&#20986;&#26469;&#65292;&#20294;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23569;&#25968;&#27665;&#26063;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21066;&#24369;&#20102;&#23569;&#25968;&#27665;&#26063;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;&#26412;&#25991;&#38024;&#23545;&#35797;&#39564;&#36873;&#22336;&#20219;&#21153;&#25552;&#20986;FRAMM&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#35797;&#39564;&#36873;&#22336;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35299;&#20915;&#24433;&#21709;&#20844;&#24179;&#35797;&#39564;&#36873;&#22336;&#30340;&#20004;&#20010;&#29616;&#23454;&#25361;&#25112;&#65306;&#35768;&#22810;&#28508;&#22312;&#35797;&#39564;&#22330;&#22320;&#30340;&#25968;&#25454;&#27169;&#24335;&#32463;&#24120;&#19981;&#23436;&#25972;&#65292;&#32780;&#19988;&#35797;&#39564;&#36873;&#22336;&#38656;&#35201;&#21516;&#26102;&#20248;&#21270;&#25307;&#21215;&#21644;&#22810;&#26679;&#24615;&#65292;&#22240;&#20026;&#38382;&#39064;&#24517;&#28982;&#26159;&#20108;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21807;&#19968;&#21487;&#33021;&#36890;&#36807;&#38480;&#21046;&#25307;&#21215;&#25968;&#37327;&#26469;&#22686;&#21152;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#22833;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;FRAMM&#20855;&#26377;&#19968;&#20010;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25513;&#30721;&#20132;&#21449;&#20851;&#27880;&#26426;&#21046;&#65292;&#21487;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#65292;&#26080;&#38656;&#36827;&#34892;&#25968;&#25454;&#22635;&#20805;&#21644;&#23436;&#25972;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#22788;&#29702;&#20248;&#21270;&#25307;&#21215;&#21644;&#22810;&#26679;&#24615;&#30340;&#38656;&#27714;&#65292;FRAMM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#24230;&#37327;&#21644;&#21487;&#35843;&#33410;&#30340;&#38480;&#21046;&#25307;&#21215;&#26426;&#21046;&#65292;&#20197;&#36827;&#34892;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30495;&#23454;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#38598;&#19978;&#65292;FRAMM&#33021;&#22815;&#26377;&#25928;&#23454;&#29616;&#20844;&#24179;&#30340;&#35797;&#39564;&#36873;&#22336;&#65292;&#24182;&#22312;&#19981;&#29306;&#29298;&#20837;&#36873;&#29575;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite many efforts to address the disparities, the underrepresentation of gender, racial, and ethnic minorities in clinical trials remains a problem and undermines the efficacy of treatments on minorities. This paper focuses on the trial site selection task and proposes FRAMM, a deep reinforcement learning framework for fair trial site selection. We focus on addressing two real-world challenges that affect fair trial sites selection: the data modalities are often not complete for many potential trial sites, and the site selection needs to simultaneously optimize for both enrollment and diversity since the problem is necessarily a trade-off between the two with the only possible way to increase diversity post-selection being through limiting enrollment via caps. To address the missing data challenge, FRAMM has a modality encoder with a masked cross-attention mechanism for handling missing data, bypassing data imputation and the need for complete data in training. To handle the need fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20998;&#27495;&#24863;&#30693;&#8221;&#30340;&#21452;&#27969;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#38024;&#23545;&#19981;&#26029;&#28436;&#21270;&#30340;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#21464;&#21270;&#12289;&#26410;&#35265;&#36807;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#32570;&#22833;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19404</link><description>&lt;p&gt;
&#33041;&#37096;&#32959;&#30244;MRI&#24322;&#36136;&#32467;&#26500;&#20998;&#21106;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI. (arXiv:2305.19404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20998;&#27495;&#24863;&#30693;&#8221;&#30340;&#21452;&#27969;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#38024;&#23545;&#19981;&#26029;&#28436;&#21270;&#30340;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#21464;&#21270;&#12289;&#26410;&#35265;&#36807;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#32570;&#22833;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21333;&#28304;&#22495;&#35757;&#32451;&#30340;&#38745;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#35299;&#21078;&#32467;&#26500;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#38745;&#24577;&#27169;&#22411;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#21464;&#24046;&#65292;&#38656;&#35201;&#36866;&#24403;&#22320;&#26356;&#26032;&#27169;&#22411;&#12290;&#22312;&#22686;&#37327;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#26356;&#26032;&#22909;&#30340;&#38745;&#24577;&#27169;&#22411;&#65292;&#36319;&#38543;&#19981;&#26029;&#28436;&#21270;&#30340;&#30446;&#26631;&#22495;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26469;&#33258;&#19981;&#21516;&#31449;&#28857;&#30340;&#39069;&#22806;&#25439;&#20260;&#25110;&#24863;&#20852;&#36259;&#32467;&#26500;&#65289;&#65292;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65306;&#20998;&#24067;&#30340;&#21464;&#21270;&#12289;&#22312;&#21021;&#22987;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#20854;&#20182;&#32467;&#26500;&#20197;&#21450;&#28304;&#22495;&#20013;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#36880;&#27493;&#28436;&#21270;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#35757;&#32451;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#19981;&#26029;&#22686;&#21152;&#30340;&#35299;&#21078;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20998;&#27495;&#24863;&#30693;&#8221;&#30340;&#21452;&#27969;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models for segmenting various anatomical structures have achieved great success via a static DL model that is trained in a single source domain. Yet, the static DL model is likely to perform poorly in a continually evolving environment, requiring appropriate model updates. In an incremental learning setting, we would expect that well-trained static models are updated, following continually evolving target domain data -- e.g., additional lesions or structures of interest -- collected from different sites, without catastrophic forgetting. This, however, poses challenges, due to distribution shifts, additional structures not seen during the initial model training, and the absence of training data in a source domain. To address these challenges, in this work, we seek to progressively evolve an ``off-the-shelf" trained segmentation model to diverse datasets with additional anatomical categories in a unified manner. Specifically, we first propose a divergence-aware dual-fl
&lt;/p&gt;</description></item><item><title>DyGen&#26159;&#19968;&#20010;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19395</link><description>&lt;p&gt;
DyGen: &#36890;&#36807;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#24314;&#27169;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling. (arXiv:2305.19395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19395
&lt;/p&gt;
&lt;p&gt;
DyGen&#26159;&#19968;&#20010;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#19981;&#27491;&#30830;&#25110;&#24050;&#25439;&#22351;&#30340;&#26631;&#31614;&#65292;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#26631;&#31614;&#22122;&#22768;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#20351;&#29992;&#38745;&#24577;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#21435;&#22122;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21463;&#38480;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#26041;&#38754;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#26377;&#20559;&#30340;&#25110;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DyGen&#30340;&#21160;&#24577;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#26469;&#25913;&#21892;&#22122;&#22768;&#26631;&#31614;&#39044;&#27979;&#12290;DyGen&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#26694;&#26550;&#20174;&#22122;&#22768;&#26631;&#31614;&#21644;&#35757;&#32451;&#21160;&#24577;&#20013;&#25512;&#26029;&#30495;&#23454;&#26631;&#31614;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#12290;&#22312;&#23384;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#26631;&#31614;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;DyGen&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is a challenge that arises in many real-world applications where training data can contain incorrect or corrupted labels. When fine-tuning language models with noisy labels, models can easily overfit the label noise, leading to decreased performance. Most existing methods for learning from noisy labels use static input features for denoising, but these methods are limited by the information they can provide on true label distributions and can result in biased or incorrect predictions. In this work, we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic patterns in the embedding space during the fine-tuning process of language models to improve noisy label predictions. DyGen uses the variational auto-encoding framework to infer the posterior distributions of true labels from noisy labels and training dynamics. Additionally, a co-regularization mechanism is used to minimize the impact of potentially noisy labels and priors. DyGen demonstr
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31361;&#35302;&#26435;&#37325;&#20998;&#24067;&#21462;&#20915;&#20110;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;&#65292;&#36827;&#32780;&#34920;&#26126;&#23454;&#39564;&#35266;&#27979;&#21040;&#30340;&#23545;&#25968;&#27491;&#24577;&#26435;&#37325;&#20998;&#24067;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#19981;&#19968;&#33268;&#65292;&#21487;&#33021;&#35828;&#26126;&#22823;&#33041;&#20013;&#20351;&#29992;&#30340;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2305.19394</link><description>&lt;p&gt;
&#31361;&#35302;&#26435;&#37325;&#20998;&#24067;&#21462;&#20915;&#20110;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
Synaptic Weight Distributions Depend on the Geometry of Plasticity. (arXiv:2305.19394v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19394
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31361;&#35302;&#26435;&#37325;&#20998;&#24067;&#21462;&#20915;&#20110;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;&#65292;&#36827;&#32780;&#34920;&#26126;&#23454;&#39564;&#35266;&#27979;&#21040;&#30340;&#23545;&#25968;&#27491;&#24577;&#26435;&#37325;&#20998;&#24067;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#19981;&#19968;&#33268;&#65292;&#21487;&#33021;&#35828;&#26126;&#22823;&#33041;&#20013;&#20351;&#29992;&#30340;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22823;&#22810;&#25968;&#23398;&#20064;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#26799;&#24230;&#19979;&#38477;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#65292;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#25991;&#29486;&#21033;&#29992;&#36825;&#20123;&#24605;&#24819;&#30740;&#31350;&#31361;&#35302;&#21487;&#22609;&#24615;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#37096;&#20998;&#27492;&#31867;&#30740;&#31350;&#24573;&#30053;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#20551;&#35774;&#65306;&#31361;&#35302;&#21464;&#21270;&#30340;&#36317;&#31163;&#36873;&#25321;&#65288;&#21363;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;&#65289;&#12290;&#26799;&#24230;&#19979;&#38477;&#20551;&#23450;&#36317;&#31163;&#20026;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#65292;&#20294;&#35768;&#22810;&#20854;&#20182;&#36317;&#31163;&#20063;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#29983;&#29289;&#23398;&#19981;&#19968;&#23450;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#24418;&#24577;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#25552;&#20379;&#30340;&#29702;&#35770;&#24037;&#20855;&#34920;&#26126;&#65292;&#26080;&#35770;&#26368;&#23567;&#21270;&#30340;&#25439;&#22833;&#20026;&#20309;&#65292;&#31361;&#35302;&#26435;&#37325;&#30340;&#20998;&#24067;&#37117;&#21462;&#20915;&#20110;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#20960;&#20309;&#24418;&#24577;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22823;&#33041;&#21306;&#22495;&#20013;&#21457;&#29616;&#30340;&#23454;&#39564;&#35266;&#27979;&#21040;&#30340;&#23545;&#25968;&#27491;&#24577;&#26435;&#37325;&#20998;&#24067;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;&#21363;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#24418;&#24577;&#65289;&#19981;&#19968;&#33268;&#65292;&#32780;&#26159;&#19982;&#38750;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most learning algorithms in machine learning rely on gradient descent to adjust model parameters, and a growing literature in computational neuroscience leverages these ideas to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes (i.e. the geometry of synaptic plasticity). Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that, regardless of the loss being minimized, the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with non-Euclidean distances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#36923;&#36753;DCC&#25439;&#22833;&#20989;&#25968;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22240;&#23376;&#20998;&#26512;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#29992;&#20197;&#25269;&#24481;&#22024;&#26434;&#30340;&#27880;&#37322;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19391</link><description>&lt;p&gt;
&#24102;&#26377;&#19981;&#23436;&#25972;&#22122;&#22768;&#37197;&#23545;&#27880;&#37322;&#30340;&#28145;&#24230;&#32858;&#31867;&#65306;&#19968;&#31181;&#20960;&#20309;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Clustering with Incomplete Noisy Pairwise Annotations: A Geometric Regularization Approach. (arXiv:2305.19391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#36923;&#36753;DCC&#25439;&#22833;&#20989;&#25968;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22240;&#23376;&#20998;&#26512;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#29992;&#20197;&#25269;&#24481;&#22024;&#26434;&#30340;&#27880;&#37322;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#27880;&#37322;&#30456;&#20284;&#24615;&#23545;&#38480;&#21046;&#32858;&#31867;&#30340;&#32467;&#21512;&#65292;&#21363;&#28145;&#24230;&#38480;&#21046;&#32858;&#31867;&#65288;DCC&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#23558;&#24369;&#30417;&#30563;&#32435;&#20837;&#22823;&#35268;&#27169;&#25968;&#25454;&#32858;&#31867;&#26159;&#26377;&#25928;&#30340;&#65306;&#23569;&#20110;1&#65285;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#27880;&#37322;&#36890;&#24120;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#32463;&#39564;&#24615;&#25104;&#21151;&#22806;&#65292;&#23545;DCC&#32570;&#20047;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;DCC&#33539;&#20363;&#23545;&#27880;&#37322;&#22122;&#22768;&#25935;&#24863;&#65292;&#20294;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#22024;&#26434;DCC&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;&#26368;&#36817;&#20986;&#29616;&#30340;DCC&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#34920;&#24449;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36923;&#36753;DCC&#25439;&#22833;&#30830;&#20445;&#22312;&#21512;&#29702;&#26465;&#20214;&#19979;&#25968;&#25454;&#25104;&#21592;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#21487;&#33021;&#20026;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22240;&#23376;&#20998;&#26512;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25269;&#24481;&#22024;&#26434;&#30340;&#27880;&#37322;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent integration of deep learning and pairwise similarity annotation-based constrained clustering -- i.e., $\textit{deep constrained clustering}$ (DCC) -- has proven effective for incorporating weak supervision into massive data clustering: Less than 1% of pair similarity annotations can often substantially enhance the clustering accuracy. However, beyond empirical successes, there is a lack of understanding of DCC. In addition, many DCC paradigms are sensitive to annotation noise, but performance-guaranteed noisy DCC methods have been largely elusive. This work first takes a deep look into a recently emerged logistic loss function of DCC, and characterizes its theoretical properties. Our result shows that the logistic DCC loss ensures the identifiability of data membership under reasonable conditions, which may shed light on its effectiveness in practice. Building upon this understanding, a new loss function based on geometric factor analysis is proposed to fend against noisy an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36328;&#20027;&#20307;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#24120;&#35268;&#12289;&#28145;&#24230;&#21644;&#21487;&#20998;&#31163;&#21367;&#31215;&#23618;&#65292;&#36798;&#21040;&#20102;73.04&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19379</link><description>&lt;p&gt;
EEG&#20449;&#21495;&#20013;&#26102;&#31354;&#29305;&#24449;&#29992;&#20110;&#36328;&#20027;&#20307;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Inter Subject Emotion Recognition Using Spatio-Temporal Features From EEG Signal. (arXiv:2305.19379v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36328;&#20027;&#20307;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#24120;&#35268;&#12289;&#28145;&#24230;&#21644;&#21487;&#20998;&#31163;&#21367;&#31215;&#23618;&#65292;&#36798;&#21040;&#20102;73.04&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#20027;&#20307;/&#20010;&#20307;&#26080;&#20851;&#24773;&#24863;&#35782;&#21035;&#19968;&#30452;&#26159;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#38590;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#21487;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#36328;&#20027;&#20307;&#30340;&#24773;&#24863;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#33879;&#21517;&#30340;EEGNet&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#24120;&#29992;&#20110;&#19982;EEG&#30456;&#20851;&#30340;BCI&#20013;&#12290;&#20316;&#32773;&#37319;&#29992;&#20102;DENS&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24773;&#24863;&#20107;&#20214;&#30340;&#31934;&#30830;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#24120;&#35268;&#12289;&#28145;&#24230;&#21644;&#21487;&#20998;&#31163;&#21367;&#31215;&#23618;&#65292;&#21487;&#23545;&#24773;&#24863;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#23398;&#20064;EEG&#36890;&#36947;&#30340;&#31354;&#38388;&#29305;&#24449;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;EEG&#20449;&#21495;&#21464;&#24322;&#30340;&#26102;&#38388;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#23545;&#24773;&#24863;&#35780;&#20998;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27169;&#22411;&#36798;&#21040;&#20102;73.04&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inter-subject or subject-independent emotion recognition has been a challenging task in affective computing. This work is about an easy-to-implement emotion recognition model that classifies emotions from EEG signals subject independently. It is based on the famous EEGNet architecture, which is used in EEG-related BCIs. We used the Dataset on Emotion using Naturalistic Stimuli (DENS) dataset. The dataset contains the Emotional Events -- the precise information of the emotion timings that participants felt. The model is a combination of regular, depthwise and separable convolution layers of CNN to classify the emotions. The model has the capacity to learn the spatial features of the EEG channels and the temporal features of the EEG signals variability with time. The model is evaluated for the valence space ratings. The model achieved an accuracy of 73.04%.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#24816;&#24615;&#35757;&#32451;&#26399;&#38388;&#65292;&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#21487;&#22312;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#20998;&#31163;&#26102;&#23454;&#29616;&#36125;&#21494;&#26031;&#26368;&#20248;&#27979;&#35797;&#35823;&#24046;&#12290;&#36890;&#36807;&#25554;&#20540;&#24179;&#28369;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19377</link><description>&lt;p&gt;
&#24816;&#24615;&#35757;&#32451;&#19979;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Benign Overfitting in Deep Neural Networks under Lazy Training. (arXiv:2305.19377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#24816;&#24615;&#35757;&#32451;&#26399;&#38388;&#65292;&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#21487;&#22312;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#20998;&#31163;&#26102;&#23454;&#29616;&#36125;&#21494;&#26031;&#26368;&#20248;&#27979;&#35797;&#35823;&#24046;&#12290;&#36890;&#36807;&#25554;&#20540;&#24179;&#28369;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#20998;&#31163;&#26102;&#65292;&#37319;&#29992;&#24816;&#24615;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#20351;DNN&#20998;&#31867;&#23454;&#29616;&#36125;&#21494;&#26031;&#26368;&#20248;&#27979;&#35797;&#35823;&#24046;&#65292;&#21516;&#26102;&#33719;&#24471;&#65288;&#20960;&#20046;&#65289;&#38646;&#35757;&#32451;&#35823;&#24046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32479;&#19968;&#20102;&#36229;&#21442;&#25968;&#21270;&#12289;&#33391;&#24615;&#36807;&#25311;&#21512;&#21644;DNN Lipschitz&#24120;&#25968;&#36825;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25554;&#20540;&#24179;&#28369;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#27861;&#36827;&#34892;&#25554;&#20540;&#24179;&#28369;&#30340;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;&#27867;&#21270;&#35823;&#24046;&#25910;&#25947;&#21040;&#20165;&#21462;&#20915;&#20110;&#26631;&#31614;&#22122;&#22768;&#21644;&#21021;&#22987;&#21270;&#22122;&#22768;&#30340;&#24120;&#25968;&#38454;&#65292;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25552;&#20379;&#20102;&#38750;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#19979;&#24402;&#19968;&#21270;&#36793;&#32536;&#30340;&#20005;&#26684;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on over-parameterized deep neural networks (DNNs) with ReLU activation functions and proves that when the data distribution is well-separated, DNNs can achieve Bayes-optimal test error for classification while obtaining (nearly) zero-training error under the lazy training regime. For this purpose, we unify three interrelated concepts of overparameterization, benign overfitting, and the Lipschitz constant of DNNs. Our results indicate that interpolating with smoother functions leads to better generalization. Furthermore, we investigate the special case where interpolating smooth ground-truth functions is performed by DNNs under the Neural Tangent Kernel (NTK) regime for generalization. Our result demonstrates that the generalization error converges to a constant order that only depends on label noise and initialization noise, which theoretically verifies benign overfitting. Our analysis provides a tight lower bound on the normalized margin under non-smooth activation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;LOPO&#38382;&#39064;&#65292;&#36890;&#36807;&#35843;&#25972;&#36317;&#31163;&#21152;&#26435;&#21644;&#24341;&#36827;&#22522;&#20110;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#27604;RF + clust&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.19375</link><description>&lt;p&gt;
LOPO&#24615;&#33021;&#39044;&#27979;&#30340;RF + clust&#28789;&#25935;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sensitivity Analysis of RF+clust for Leave-one-problem-out Performance Prediction. (arXiv:2305.19375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;LOPO&#38382;&#39064;&#65292;&#36890;&#36807;&#35843;&#25972;&#36317;&#31163;&#21152;&#26435;&#21644;&#24341;&#36827;&#22522;&#20110;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#27604;RF + clust&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Leave-one-problem-out&#65288;LOPO&#65289;&#24615;&#33021;&#39044;&#27979;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23558;&#31639;&#27861;&#30340;&#24615;&#33021;&#20174;&#19968;&#32452;&#35757;&#32451;&#38382;&#39064;&#25512;&#24191;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#38382;&#39064;&#19978;&#12290;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;LOPO&#20063;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26356;&#31616;&#21333;&#30340;leave-one-instance-out&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#36890;&#24120;&#26410;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;LOPO&#35774;&#32622;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;LOPO&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#21152;&#26435;&#31639;&#27861;&#24615;&#33021;&#30340;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#24615;&#33021;&#22238;&#24402;&#27169;&#22411;&#23545;&#26631;&#20934;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#36825;&#20123;&#31639;&#27861;&#24615;&#33021;&#34987;&#35748;&#20026;&#19982;&#27979;&#35797;&#38382;&#39064;&#30456;&#20284;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#22312;&#36825;&#20010;RF + clust&#26041;&#27861;&#20013;&#65292;&#26435;&#37325;&#26159;&#26681;&#25454;&#26576;&#20123;&#29305;&#24449;&#31354;&#38388;&#20013;&#38382;&#39064;&#30340;&#36317;&#31163;&#25104;&#27604;&#20363;&#36873;&#25321;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#25972;&#22522;&#20110;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#23545;&#24615;&#33021;&#22238;&#24402;&#30340;&#36317;&#31163;&#21152;&#26435;&#26469;&#25193;&#23637;RF + clust&#26041;&#27861;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#19981;&#20877;&#32771;&#34385;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20313;&#24358;&#36317;&#31163;&#65292;&#32780;&#26159;&#32771;&#34385;&#21152;&#26435;&#20313;&#24358;&#36317;&#31163;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#22522;&#20110;RF&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24230;&#37327;&#33258;&#21160;&#35745;&#31639;&#30340;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;23&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#27604;&#36739;RF + clust&#19982;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23545;&#26032;&#38382;&#39064;&#30340;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#27604;RF + clust&#26174;&#30528;&#26356;&#20248;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#38382;&#39064;&#30340;&#25968;&#37327;&#36739;&#23569;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leave-one-problem-out (LOPO) performance prediction requires machine learning (ML) models to extrapolate algorithms' performance from a set of training problems to a previously unseen problem. LOPO is a very challenging task even for state-of-the-art approaches. Models that work well in the easier leave-one-instance-out scenario often fail to generalize well to the LOPO setting. To address the LOPO problem, recent work suggested enriching standard random forest (RF) performance regression models with a weighted average of algorithms' performance on training problems that are considered similar to a test problem. More precisely, in this RF+clust approach, the weights are chosen proportionally to the distances of the problems in some feature space. Here in this work, we extend the RF+clust approach by adjusting the distance-based weights with the importance of the features for performance regression. That is, instead of considering cosine distance in the feature space, we consider a weig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#21033;&#29992;&#32452;&#21512;&#24615;&#36827;&#34892;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31243;&#24207;&#24402;&#32435;&#27169;&#22411;&#26469;&#29983;&#25104;&#20505;&#36873;&#35270;&#35273;&#22270;&#24418;&#65292;&#21457;&#29616;&#20154;&#31867;&#21644;&#27169;&#22411;&#37117;&#21487;&#20197;&#36827;&#34892;&#22810;&#26679;&#30340;&#32452;&#21512;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.19374</link><description>&lt;p&gt;
&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#20013;&#30340;&#32452;&#21512;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Compositional diversity in visual concept learning. (arXiv:2305.19374v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#21033;&#29992;&#32452;&#21512;&#24615;&#36827;&#34892;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31243;&#24207;&#24402;&#32435;&#27169;&#22411;&#26469;&#29983;&#25104;&#20505;&#36873;&#35270;&#35273;&#22270;&#24418;&#65292;&#21457;&#29616;&#20154;&#31867;&#21644;&#27169;&#22411;&#37117;&#21487;&#20197;&#36827;&#34892;&#22810;&#26679;&#30340;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#29702;&#35299;&#29087;&#24713;&#37096;&#20214;&#22914;&#20309;&#32452;&#21512;&#22312;&#19968;&#36215;&#24418;&#25104;&#26032;&#39062;&#23545;&#35937;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#27969;&#34892;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#38590;&#20197;&#36827;&#34892;&#30456;&#21516;&#31867;&#22411;&#30340;&#25512;&#29702;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#22914;&#20154;&#31867;&#20855;&#26377;&#28789;&#27963;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#20123;&#20154;&#31867;&#29305;&#26377;&#30340;&#33021;&#21147;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35270;&#35273;&#32452;&#21512;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#20154;&#31867;&#22914;&#20309;&#23545;&#20855;&#26377;&#20016;&#23500;&#20851;&#31995;&#32467;&#26500;&#30340;&#8220;&#22806;&#26143;&#20154;&#22270;&#24418;&#8221;&#36827;&#34892;&#20998;&#31867;&#21644;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#31243;&#24207;&#24402;&#32435;&#27169;&#22411;&#65292;&#25628;&#32034;&#29983;&#25104;&#20505;&#36873;&#35270;&#35273;&#22270;&#24418;&#30340;&#26368;&#20339;&#31243;&#24207;&#65292;&#21033;&#29992;&#21253;&#21547;&#19981;&#21516;&#32452;&#21512;&#26426;&#21046;&#21644;&#25277;&#35937;&#30340;&#22823;&#22411;&#31243;&#24207;&#31354;&#38388;&#12290;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#21644;&#31243;&#24207;&#24402;&#32435;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#35768;&#22810;&#26377;&#24847;&#20041;&#30340;&#32452;&#21512;&#27867;&#21270;&#65292;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#39564;&#25968;&#25454;&#30340;&#24378;&#26377;&#21147;&#35299;&#37322;&#20197;&#21450;&#26174;&#31034;&#20154;&#31867;&#24335;&#32452;&#21512;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans leverage compositionality to efficiently learn new concepts, understanding how familiar parts can combine together to form novel objects. In contrast, popular computer vision models struggle to make the same types of inferences, requiring more data and generalizing less flexibly than people do. Here, we study these distinctively human abilities across a range of different types of visual composition, examining how people classify and generate ``alien figures'' with rich relational structure. We also develop a Bayesian program induction model which searches for the best programs for generating the candidate visual figures, utilizing a large program space containing different compositional mechanisms and abstractions. In few shot classification tasks, we find that people and the program induction model can make a range of meaningful compositional generalizations, with the model providing a strong account of the experimental data as well as interpretable parameters that reveal huma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#23545;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#65292;&#26088;&#22312;&#20174;&#20013;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#30340;&#20020;&#24202;&#34920;&#22411;&#24182;&#39044;&#27979;&#30149;&#20154;&#20303;&#38498;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.19373</link><description>&lt;p&gt;
&#36890;&#36807;&#25366;&#25496;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#20027;&#39064;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#34920;&#22411;&#24182;&#39044;&#27979;&#20303;&#38498;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure. (arXiv:2305.19373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#23545;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#65292;&#26088;&#22312;&#20174;&#20013;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#30340;&#20020;&#24202;&#34920;&#22411;&#24182;&#39044;&#27979;&#30149;&#20154;&#20303;&#38498;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#26159;&#19968;&#31181;&#32508;&#21512;&#24449;&#65292;&#24403;&#24515;&#33039;&#26080;&#27861;&#27893;&#34880;&#21644;&#36755;&#36865;&#27687;&#27668;&#20197;&#25903;&#25345;&#20307;&#20869;&#20854;&#20182;&#22120;&#23448;&#26102;&#20986;&#29616;&#12290;&#35782;&#21035;&#25509;&#21463;&#24515;&#21147;&#34928;&#31469;&#27835;&#30103;&#30340;&#30149;&#20154;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#30340;&#28508;&#22312;&#20027;&#39064;&#65292;&#21487;&#20197;&#25581;&#31034;&#19982;&#24515;&#21147;&#34928;&#31469;&#30456;&#20851;&#30340;&#20020;&#24202;&#34920;&#22411;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#30340;&#29305;&#24449;&#23545;&#30149;&#20154;&#36827;&#34892;&#20998;&#32452;&#65292;&#36825;&#20063;&#26377;&#21161;&#20110;&#39044;&#27979;&#30149;&#20154;&#30340;&#20303;&#38498;&#26102;&#38388;&#12290;&#30001;&#20110;&#36825;&#20123;&#20020;&#24202;&#34920;&#22411;&#36890;&#24120;&#20855;&#26377;&#27010;&#29575;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#19988;&#20043;&#21069;&#27809;&#26377;&#20851;&#20110;&#20351;&#29992;&#27010;&#29575;&#26694;&#26550;&#22312;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#20020;&#24202;&#31508;&#35760;&#20013;&#35782;&#21035;&#34920;&#22411;&#21644;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#39044;&#27979;&#36825;&#20123;&#24739;&#32773;&#30340;&#20303;&#38498;&#26102;&#38388;&#30340;&#30740;&#31350;&#65292;&#22240;&#27492;&#25105;&#20204;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#8212;&#8212;&#20027;&#39064;&#24314;&#27169;&#65292;&#23545;&#20234;&#21033;&#35834;&#20234;&#22823;&#23398;&#21307;&#38498;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a syndrome which occurs when the heart is not able to pump blood and oxygen to support other organs in the body. Identifying the underlying themes in the diagnostic codes and procedure reports of patients admitted for heart failure could reveal the clinical phenotypes associated with heart failure and to group patients based on their similar characteristics which could also help in predicting patient outcomes like length of stay. These clinical phenotypes usually have a probabilistic latent structure and hence, as there has been no previous work on identifying phenotypes in clinical notes of heart failure patients using a probabilistic framework and to predict length of stay of these patients using data-driven artificial intelligence-based methods, we apply natural language processing technique, topic modeling, to identify the themes present in diagnostic codes and in procedure reports of 1,200 patients admitted for heart failure at the University of Illinois Hospital 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.19370</link><description>&lt;p&gt;
&#22823;&#22411;&#38271;&#24207;&#21015;&#27169;&#22411;&#30340;&#22359;&#32423;&#24182;&#34892;Transformer
&lt;/p&gt;
&lt;p&gt;
Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#22312;&#21508;&#31181;AI&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#22823;&#22411;&#21069;&#39304;&#32593;&#32476;&#25152;&#38656;&#30340;&#20869;&#23384;&#23481;&#37327;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#28041;&#21450;&#22810;&#20010;&#38271;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#22359;&#32423;&#24182;&#34892;Transformer&#65288;BPT&#65289;&#65292;&#23427;&#21033;&#29992;&#22359;&#32423;&#35745;&#31639;&#33258;&#25105;&#27880;&#24847;&#21644;&#21069;&#39304;&#32593;&#32476;&#34701;&#21512;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#30340;&#21516;&#26102;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;BPT&#20351;&#35757;&#32451;&#24207;&#21015;&#30340;&#38271;&#24230;&#27604;&#21407;&#22987;&#30340;Transformer&#38271;32&#20493;&#65292;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#38271;2&#21040;4&#20493;&#12290;&#23545;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;BPT&#22312;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#19968;&#29983;&#25104;&#27969;&#32593;&#32476;&#20013;&#32852;&#21512;&#24314;&#27169;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38750;&#31163;&#25955;&#26679;&#26412;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#23616;&#37096;&#27010;&#29575;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19366</link><description>&lt;p&gt;
&#21333;&#19968;&#29983;&#25104;&#27969;&#32593;&#32476;&#20013;&#30340;&#22270;&#32467;&#26500;&#19982;&#21442;&#25968;&#30340;&#32852;&#21512;&#36125;&#21494;&#26031;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network. (arXiv:2305.19366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#19968;&#29983;&#25104;&#27969;&#32593;&#32476;&#20013;&#32852;&#21512;&#24314;&#27169;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38750;&#31163;&#25955;&#26679;&#26412;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#23616;&#37096;&#27010;&#29575;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#26159;&#19968;&#31867;&#23545;&#31163;&#25955;&#21644;&#32467;&#26500;&#21270;&#26679;&#26412;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#26029;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#36793;&#32536;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#22312;&#38750;&#31163;&#25955;&#26679;&#26412;&#31354;&#38388;&#19978;&#23558;&#27492;&#26694;&#26550;&#25193;&#23637;&#21040;&#32852;&#21512;&#21518;&#39564;&#20998;&#24067;&#30340;&#24314;&#27169;&#65292;&#19981;&#20165;&#21253;&#25324;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#36824;&#32771;&#34385;&#20102;&#20854;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title>&#22312;&#26426;&#26800;&#39046;&#22495;&#23454;&#29616;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#21033;&#29992;&#22768;&#23376;&#20803;&#32467;&#26500;&#23454;&#29616;&#22810;&#21151;&#33021;&#26426;&#26800;-&#26234;&#33021;&#65307;&#36890;&#36807;&#29289;&#29702;&#20648;&#24211;&#35745;&#31639;&#26694;&#26550;&#65292;&#22312;&#26426;&#26800;&#39046;&#22495;&#30452;&#25509;&#23454;&#29616;&#35745;&#31639;&#33021;&#21147;&#21644;&#21508;&#31181;&#26234;&#33021;&#20803;&#32032;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25972;&#21512;&#19981;&#21516;&#26234;&#33021;&#20803;&#32032;&#30340;&#31995;&#32479;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.19354</link><description>&lt;p&gt;
&#21033;&#29992;&#22768;&#23376;&#20803;&#32467;&#26500;&#23454;&#29616;&#22810;&#21151;&#33021;&#26426;&#26800;-&#26234;&#33021;&#30340;&#21457;&#29616;&#19982;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Uncovering multifunctional mechano-intelligence in and through phononic metastructures harnessing physical reservoir computing. (arXiv:2305.19354v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19354
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#26800;&#39046;&#22495;&#23454;&#29616;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#21033;&#29992;&#22768;&#23376;&#20803;&#32467;&#26500;&#23454;&#29616;&#22810;&#21151;&#33021;&#26426;&#26800;-&#26234;&#33021;&#65307;&#36890;&#36807;&#29289;&#29702;&#20648;&#24211;&#35745;&#31639;&#26694;&#26550;&#65292;&#22312;&#26426;&#26800;&#39046;&#22495;&#30452;&#25509;&#23454;&#29616;&#35745;&#31639;&#33021;&#21147;&#21644;&#21508;&#31181;&#26234;&#33021;&#20803;&#32032;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25972;&#21512;&#19981;&#21516;&#26234;&#33021;&#20803;&#32032;&#30340;&#31995;&#32479;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#19979;&#19968;&#20195;&#33258;&#36866;&#24212;&#32467;&#26500;&#21644;&#26448;&#26009;&#22312;&#26426;&#26800;&#39046;&#22495;&#25317;&#26377;&#26356;&#22810;&#20869;&#32622;&#26234;&#33021;&#65288;MI&#65289;&#38656;&#27714;&#24378;&#28872;&#12290;&#20197;&#21069;&#30340;MI&#23581;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#35774;&#35745;&#21644;&#26696;&#20363;&#30740;&#31350;&#19978;&#65292;&#20197;&#23454;&#29616;&#26377;&#38480;&#30340;MI&#26041;&#38754;&#65292;&#24182;&#19988;&#32570;&#20047;&#22312;&#26377;&#25928;&#21644;&#39640;&#25928;&#26041;&#24335;&#19979;&#26500;&#24314;&#21644;&#25972;&#21512;&#19981;&#21516;&#26234;&#33021;&#20803;&#32032;&#30340;&#31995;&#32479;&#22522;&#30784;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#23454;&#29616;&#32508;&#21512;&#22810;&#21151;&#33021;MI&#25152;&#38656;&#22522;&#30784;&#30340;&#29289;&#29702;&#20648;&#24211;&#35745;&#31639;&#65288;PRC&#65289;&#26694;&#26550;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#30452;&#25509;&#22312;&#26426;&#26800;&#39046;&#22495;&#20013;&#21516;&#26102;&#20307;&#29616;&#35745;&#31639;&#33021;&#21147;&#21644;&#21508;&#31181;&#26234;&#33021;&#20803;&#32032;&#65292;&#21363;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#21629;&#20196;&#65292;&#20174;&#20256;&#32479;&#20381;&#38752;&#22806;&#21152;&#25968;&#23383;&#35745;&#31639;&#26426;&#21644;&#22823;&#35268;&#27169;&#30005;&#23376;&#23454;&#29616;&#26234;&#33021;&#30340;&#33258;&#36866;&#24212;&#32467;&#26500;&#20013;&#21457;&#23637;&#12290;&#20316;&#20026;&#31034;&#20363;&#24179;&#21488;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#27454;
&lt;/p&gt;
&lt;p&gt;
The recent advances in autonomous systems have prompted a strong demand for the next generation of adaptive structures and materials to possess more built-in intelligence in their mechanical domain, the so-called mechano-intelligence (MI). Previous MI attempts mainly focused on specific designs and case studies to realize limited aspects of MI, and there is a lack of a systematic foundation in constructing and integrating the different elements of intelligence in an effective and efficient manner. Here, we propose a new approach to create the needed foundation in realizing integrated multifunctional MI via a physical reservoir computing (PRC) framework. That is, to concurrently embody computing power and the various elements of intelligence, namely perception, decision-making, and commanding, directly in the mechanical domain, advancing from conventional adaptive structures that rely solely on add-on digital computers and massive electronics to achieve intelligence. As an exemplar plat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#20984;&#36125;&#21494;&#26031;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19350</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#38750;&#20984;&#36125;&#21494;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte Carlo. (arXiv:2305.19350v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19350
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#20984;&#36125;&#21494;&#26031;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#21462;&#20915;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#35757;&#32451;&#65292;&#36825;&#28041;&#21450;&#21040;&#38750;&#20984;&#20248;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24402;&#32467;&#20026;&#38750;&#20984;&#36125;&#21494;&#26031;&#23398;&#20064;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#24182;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of artificial intelligence (AI) hinges on the efficient training of modern deep neural networks (DNNs) for non-convex optimization and uncertainty quantification, which boils down to a non-convex Bayesian learning problem. A standard tool to handle the problem is Langevin Monte Carlo, which proposes to approximate the posterior distribution with theoretical guarantees. In this thesis, we start with the replica exchange Langevin Monte Carlo (also known as parallel tempering), which proposes appropriate swaps between exploration and exploitation to achieve accelerations. However, the na\"ive extension of swaps to big data problems leads to a large bias, and bias-corrected swaps are required. Such a mechanism leads to few effective swaps and insignificant accelerations. To alleviate this issue, we first propose a control variates method to reduce the variance of noisy energy estimators and show a potential to accelerate the exponential convergence. We also present the population-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#32422;&#26463;&#38598;&#24773;&#20917;&#19979;&#30340;&#26354;&#32447;&#31354;&#38388;&#22312;&#32447;&#27979;&#22320;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#33719;&#24471;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19349</link><description>&lt;p&gt;
&#20851;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#26080;&#25237;&#24433;&#22312;&#32447;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Riemannian Projection-free Online Learning. (arXiv:2305.19349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#32422;&#26463;&#38598;&#24773;&#20917;&#19979;&#30340;&#26354;&#32447;&#31354;&#38388;&#22312;&#32447;&#27979;&#22320;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#33719;&#24471;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#24433;&#25805;&#20316;&#26159;&#35768;&#22810;&#20248;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;[OGD]&#65289;&#20013;&#24378;&#21046;&#32422;&#26463;&#21644;&#23454;&#29616;&#26368;&#20248;&#36951;&#25022;&#36793;&#30028;&#25152;&#24517;&#38656;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#39640;&#32500;&#35774;&#32622;&#25110;&#20855;&#26377;&#30149;&#24577;&#32422;&#26463;&#38598;&#26102;&#65292;&#23427;&#20250;&#21463;&#21040;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#12290;&#26080;&#25237;&#24433;&#31639;&#27861;&#36890;&#36807;&#29992;&#26356;&#26377;&#25928;&#30340;&#20248;&#21270;&#23376;&#31243;&#24207;&#21462;&#20195;&#25237;&#24433;&#39044;&#27979;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22312;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#24320;&#21457;&#65292;&#24182;&#19988;&#34429;&#28982;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#65292;&#20294;&#22312;&#23581;&#35797;&#21033;&#29992;&#26080;&#25237;&#24433;&#24037;&#20855;&#26041;&#38754;&#22522;&#26412;&#19978;&#27809;&#26377;&#24037;&#20316;&#12290;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#26159;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#38750;&#24179;&#20961;&#30340;&#20223;&#23556;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#26354;&#32447;&#31354;&#38388;&#19978;&#36827;&#34892;&#22312;&#32447;&#27979;&#22320;&#20984;&#20248;&#21270;&#65292;&#20197;&#33719;&#24471;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#65306;&#24403;&#25105;&#20204;&#35775;&#38382;&#65288;a&#65289;&#26102;
&lt;/p&gt;
&lt;p&gt;
The projection operation is a critical component in a wide range of optimization algorithms, such as online gradient descent (OGD), for enforcing constraints and achieving optimal regret bounds. However, it suffers from computational complexity limitations in high-dimensional settings or when dealing with ill-conditioned constraint sets. Projection-free algorithms address this issue by replacing the projection oracle with more efficient optimization subroutines. But to date, these methods have been developed primarily in the Euclidean setting, and while there has been growing interest in optimization on Riemannian manifolds, there has been essentially no work in trying to utilize projection-free tools here. An apparent issue is that non-trivial affine functions are generally non-convex in such domains. In this paper, we present methods for obtaining sub-linear regret guarantees in online geodesically convex optimization on curved spaces for two scenarios: when we have access to (a) a s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#12289;&#32463;&#27982;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#30315;&#30187;&#26816;&#27979;&#31995;&#32479;&#65292;&#22522;&#20110;&#31616;&#21333;&#30340;&#23454;&#26102;kNN&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#22312;&#19981;&#21040;&#22235;&#31186;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23450;&#21046;&#21644;&#36866;&#24212;&#20010;&#20154;&#29992;&#25143;&#65292;&#24182;&#20855;&#26377;94.5%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.19347</link><description>&lt;p&gt;
&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#65306;&#35299;&#21078;&#19982;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy Seizure Detection: Anatomy and Analysis. (arXiv:2305.19347v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#12289;&#32463;&#27982;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#30315;&#30187;&#26816;&#27979;&#31995;&#32479;&#65292;&#22522;&#20110;&#31616;&#21333;&#30340;&#23454;&#26102;kNN&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#22312;&#19981;&#21040;&#22235;&#31186;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23450;&#21046;&#21644;&#36866;&#24212;&#20010;&#20154;&#29992;&#25143;&#65292;&#24182;&#20855;&#26377;94.5%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#36319;&#36394;&#31995;&#32479;&#23545;&#20110;&#30417;&#27979;&#21644;&#35780;&#20272;&#30315;&#30187;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#30315;&#30187;&#25252;&#29702;&#20013;&#20351;&#29992;&#30340;&#25252;&#29702;&#35760;&#24405;&#21487;&#33021;&#20250;&#38169;&#36807;&#30315;&#30187;&#21457;&#20316;&#12290;&#21487;&#31359;&#25140;&#30340;&#30417;&#27979;&#35774;&#22791;&#21487;&#33021;&#26356;&#23481;&#26131;&#34987;&#32784;&#21463;&#65292;&#24182;&#19988;&#26356;&#36866;&#21512;&#38271;&#26399;&#36827;&#34892;&#12290;&#35768;&#22810;&#25216;&#26415;&#21644;&#26041;&#27861;&#34987;&#25552;&#20986;&#29992;&#20110;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#65307;&#28982;&#32780;&#65292;&#22312;&#20445;&#25345;&#26816;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#31616;&#21333;&#24615;&#21644;&#36153;&#29992;&#37117;&#26159;&#26085;&#24120;&#20351;&#29992;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#23454;&#26102;k-&#26368;&#36817;&#37051;&#65288;kNN&#65289;&#26426;&#22120;&#23398;&#20064;&#30340;&#36890;&#29992;&#12289;&#32463;&#27982;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#30315;&#30187;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#21487;&#22312;&#19981;&#21040;&#22235;&#31186;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23450;&#21046;&#21644;&#36866;&#24212;&#20010;&#20154;&#29992;&#25143;&#65307;&#35813;&#31995;&#32479;&#32463;&#36807;&#20102;500&#20010;&#34987;&#35797;&#39564;&#32773;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#65292;&#25277;&#26679;&#39057;&#29575;&#20026;178 Hz&#65292;&#20854;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;94.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A seizure tracking system is crucial for monitoring and evaluating epilepsy treatments. Caretaker seizure diaries are used in epilepsy care today, but clinical seizure monitoring may miss seizures. Monitoring devices that can be worn may be better tolerated and more suitable for long-term ambulatory use. Many techniques and methods are proposed for seizure detection; However, simplicity and affordability are key concepts for daily use while preserving the accuracy of the detection. In this study, we propose a versal, affordable noninvasive based on a simple real-time k-Nearest-Neighbors (kNN) machine learning that can be customized and adapted to individual users in less than four (4) seconds of training time; the system was verified and validated using 500 subjects, with seizure detection data sampled at 178 Hz, the operated with a mean accuracy of (94.5%).
&lt;/p&gt;</description></item><item><title>HiGen&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#29983;&#25104;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#24418;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22810;&#39033;&#24335;&#20998;&#24067;&#26469;&#29983;&#25104;&#20855;&#26377;&#25972;&#25968;&#20540;&#30340;&#23376;&#22270;&#30340;&#36793;&#26435;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#24418;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19337</link><description>&lt;p&gt;
HiGen&#65306;&#23618;&#27425;&#22270;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HiGen: Hierarchical Graph Generative Networks. (arXiv:2305.19337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19337
&lt;/p&gt;
&lt;p&gt;
HiGen&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#29983;&#25104;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#24418;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22810;&#39033;&#24335;&#20998;&#24067;&#26469;&#29983;&#25104;&#20855;&#26377;&#25972;&#25968;&#20540;&#30340;&#23376;&#22270;&#30340;&#36793;&#26435;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#24418;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#34920;&#29616;&#20986;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#36890;&#24120;&#34987;&#29616;&#26377;&#30340;&#22270;&#24418;&#29983;&#25104;&#26041;&#27861;&#25152;&#24573;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#29983;&#25104;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#24418;&#30340;&#23618;&#27425;&#32467;&#26500;&#24182;&#25104;&#21151;&#22320;&#29983;&#25104;&#22270;&#24418;&#23376;&#32467;&#26500;&#12290;&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#65292;&#35813;&#27169;&#22411;&#24182;&#34892;&#29983;&#25104;&#31038;&#21306;&#65292;&#20351;&#29992;&#29420;&#31435;&#30340;&#27169;&#22411;&#39044;&#27979;&#31038;&#21306;&#20043;&#38388;&#30340;&#36328;&#36793;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#20351;&#29983;&#25104;&#30340;&#22270;&#24418;&#32593;&#32476;&#39640;&#24230;&#21487;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#22810;&#39033;&#24335;&#20998;&#24067;&#24314;&#27169;&#23618;&#27425;&#22270;&#24418;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#38024;&#23545;&#27492;&#20998;&#24067;&#25512;&#23548;&#20102;&#36882;&#24402;&#20998;&#35299;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#25972;&#25968;&#20540;&#30340;&#23376;&#22270;&#30340;&#36793;&#26435;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#24418;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using a separate model. This modular approach results in a highly scalable graph generative network. Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution, enabling us to generate sub-graphs with integer-valued edge weights in an autoregressive approach. Empirical studies demonstrate that the proposed generative model can effectively capture both local and global properties of graphs and achieves state-of-the-art performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21518;&#32622;&#20559;&#24046;&#32531;&#35299;&#65288;PBM&#65289;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#20013;&#24615;&#25991;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#30340;&#22270;&#20687;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19329</link><description>&lt;p&gt;
&#32531;&#35299;&#27979;&#35797;&#26102;&#38388;&#20559;&#24046;&#65292;&#23454;&#29616;&#20844;&#24179;&#30340;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Mitigating Test-Time Bias for Fair Image Retrieval. (arXiv:2305.19329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21518;&#32622;&#20559;&#24046;&#32531;&#35299;&#65288;PBM&#65289;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#20013;&#24615;&#25991;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#30340;&#22270;&#20687;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#20013;&#24615;&#25991;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65288;&#27809;&#26377;&#26126;&#30830;&#30340;&#24615;&#21035;&#25110;&#31181;&#26063;&#20869;&#28085;&#65289;&#29983;&#25104;&#20844;&#24179;&#21644;&#26080;&#20559;&#35265;&#30340;&#22270;&#20687;&#26816;&#32034;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#30340;&#25928;&#29992;&#65288;&#24615;&#33021;&#65289;&#30340;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#26088;&#22312;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#26597;&#35810;&#30340;&#23398;&#20064;&#34920;&#31034;&#19982;&#24615;&#21035;&#21644;&#31181;&#26063;&#29305;&#24449;&#20998;&#31163;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20943;&#36731;&#27979;&#35797;&#38598;&#20013;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#25152;&#38656;&#30340;&#24179;&#31561;&#34920;&#31034;&#32467;&#26524;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#21518;&#32622;&#20559;&#24046;&#32531;&#35299;&#65288;PBM&#65289;&#65292;&#26469;&#21518;&#22788;&#29702;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#22270;&#20687;&#25628;&#32034;&#25968;&#25454;&#38598;Occupation 1&#21644;2&#65292;&#20197;&#21450;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;MS-COCO&#21644;Flickr30k&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#26816;&#32034;&#32467;&#26524;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of generating fair and unbiased image retrieval results given neutral textual queries (with no explicit gender or race connotations), while maintaining the utility (performance) of the underlying vision-language (VL) model. Previous methods aim to disentangle learned representations of images and text queries from gender and racial characteristics. However, we show these are inadequate at alleviating bias for the desired equal representation result, as there usually exists test-time bias in the target retrieval set. So motivated, we introduce a straightforward technique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from the pre-trained vision-language model. We evaluate our algorithm on real-world image search datasets, Occupation 1 and 2, as well as two large-scale image-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest bias, compared with various existing bias-mitigation methods, in text-based image retrieval result whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#29992;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20197;&#36817;32&#20493;&#30340;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19306</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#24471;&#19968;&#27604;&#29305;&#30340;&#24046;&#24322;&#24615;&#65306;&#24403;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#36935;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#29992;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20197;&#36817;32&#20493;&#30340;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20294;&#23545;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#36861;&#27714;&#38656;&#35201;&#22823;&#30340;&#38544;&#34255;&#32500;&#24230;&#26469;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#12289;&#26377;&#21306;&#21035;&#24615;&#30340;&#20840;&#31934;&#24230;&#34920;&#31034;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#35745;&#31639;&#12289;&#23384;&#20648;&#21644;&#33021;&#28304;&#28040;&#32791;&#36127;&#25285;&#65288;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#22823;&#22810;&#34987;&#24573;&#30053;&#65289;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21363;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#36827;&#34892;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#21033;&#29992;&#31232;&#30095;&#21644;&#20108;&#20803;&#29305;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#23398;&#20064;&#22270;&#30340;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#30340;&#26032;&#22411;GCL&#26694;&#26550;&#65292;&#24179;&#34913;&#20102;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#35777;&#26126;SpikeGCL&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#19982;&#20854;&#20840;&#31934;&#24230;&#23545;&#24212;&#29289;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#36817;32&#20493;&#65292;SpikeGCL&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#38899;&#20048;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#19981;&#21516;&#31639;&#27861;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#20998;&#31867;&#22120;&#23558;&#38899;&#20048;&#31867;&#22411;&#20998;&#20026;&#21476;&#20856;&#38899;&#20048;&#21644;&#37329;&#23646;&#20048;&#12290;</title><link>http://arxiv.org/abs/2305.19304</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Audio classification using ML methods. (arXiv:2305.19304v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#38899;&#20048;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#19981;&#21516;&#31639;&#27861;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#20998;&#31867;&#22120;&#23558;&#38899;&#20048;&#31867;&#22411;&#20998;&#20026;&#21476;&#20856;&#38899;&#20048;&#21644;&#37329;&#23646;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#38899;&#20048;&#31867;&#22411;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#20195;&#30721;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#38899;&#39057;&#25991;&#20214;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#23558;&#23427;&#20204;&#20998;&#20026;&#20004;&#20010;&#31867;&#22411;&#65292;&#21363;&#21476;&#20856;&#38899;&#20048;&#21644;&#37329;&#23646;&#20048;&#12290;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#20351;&#29992;&#19981;&#21516;&#26680;&#65288;&#32447;&#24615;&#12289;sigmoid&#12289;rbf&#21644;poly&#65289;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;K&#36817;&#37051;&#20998;&#31867;&#22120;&#12289;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#12289;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#21644;&#39640;&#26031;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning systems have achieved outstanding performance in different domains. In this paper machine learning methods have been applied to classification task to classify music genre. The code shows how to extract features from audio files and classify them using supervised learning into 2 genres namely classical and metal. Algorithms used are LogisticRegression, SVC using different kernals (linear, sigmoid, rbf and poly), KNeighborsClassifier , RandomForestClassifier, DecisionTreeClassifier and GaussianNB.
&lt;/p&gt;</description></item><item><title>MAGNet&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#29983;&#25104;&#25216;&#26415;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20195;&#34920;&#36229;&#20986;&#24050;&#30693;motif&#38598;&#20043;&#22806;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19303</link><description>&lt;p&gt;
MAGNet&#65306;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
MAGNet: Motif-Agnostic Generation of Molecules from Shapes. (arXiv:2305.19303v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19303
&lt;/p&gt;
&lt;p&gt;
MAGNet&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#29983;&#25104;&#25216;&#26415;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20195;&#34920;&#36229;&#20986;&#24050;&#30693;motif&#38598;&#20043;&#22806;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20998;&#23376;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#26497;&#26377;&#28508;&#21147;&#65292;&#21487;&#20026;&#33647;&#29289;&#21457;&#29616;&#25552;&#20379;&#20415;&#21033;&#12290;&#29616;&#26377;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#23558;&#20998;&#23376;&#20998;&#35299;&#20026;&#24120;&#35265;&#30340;&#23376;&#32467;&#26500;&#65288;motifs&#65289;&#65292;&#20043;&#21518;&#29983;&#25104;&#26032;&#30340;&#21270;&#21512;&#29289;&#12290;&#23613;&#31649;motif&#30340;&#34920;&#31034;&#27861;&#26497;&#22823;&#22320;&#24110;&#21161;&#23398;&#20064;&#20998;&#23376;&#20998;&#24067;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#20195;&#34920;&#36229;&#20986;&#24050;&#30693;motif&#38598;&#20043;&#22806;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;MAGNet&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#37197;&#21407;&#23376;&#21644;&#38190;&#31867;&#22411;&#20043;&#21069;&#65292;&#29983;&#25104;&#25277;&#35937;&#30340;&#24418;&#29366;&#65292;&#20197;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#24182;&#22686;&#21152;&#23545;&#25968;&#25454;&#38598;&#30340;&#28789;&#27963;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#20998;&#23376;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#24182;&#20419;&#36827;&#20102;&#21407;&#23376;&#21644;&#38190;&#30340;&#36866;&#24403;&#20998;&#37197;&#12290;&#34429;&#28982;&#23558;&#25277;&#35937;&#24418;&#29366;&#24341;&#20837;&#20998;&#24067;&#23398;&#20064;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;MAGNet&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from in silico predictions. Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods struggle to represent substructures beyond their known motif set. To alleviate this issue and increase flexibility across datasets, we propose MAGNet, a graph-based model that generates abstract shapes before allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that accounts for the molecules' global context and facilitates learning adequate assignments of atoms and bonds onto shapes. While the abstraction to shapes introduces greater complexity for distribution learning, we show the competitive performance of MAGNet on standard benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23398;&#20064;&#35270;&#39057;&#21387;&#32553;&#26102;&#65292;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#20110;&#37325;&#24314;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36873;&#25321; PLF-JD &#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#36328;&#24103;&#26102;&#24207;&#30456;&#20851;&#24615;&#65292;&#20294;&#21516;&#26102;&#20250;&#24102;&#26469;&#26356;&#22823;&#30340;&#22833;&#30495;&#24809;&#32602;&#21644;&#26356;&#38590;&#20197;&#32416;&#27491;&#26089;&#26399;&#36755;&#20986;&#24103;&#20013;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.19301</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#35270;&#39057;&#21387;&#32553;&#20013;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
On the Choice of Perception Loss Function for Learned Video Compression. (arXiv:2305.19301v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23398;&#20064;&#35270;&#39057;&#21387;&#32553;&#26102;&#65292;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#20110;&#37325;&#24314;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36873;&#25321; PLF-JD &#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#36328;&#24103;&#26102;&#24207;&#30456;&#20851;&#24615;&#65292;&#20294;&#21516;&#26102;&#20250;&#24102;&#26469;&#26356;&#22823;&#30340;&#22833;&#30495;&#24809;&#32602;&#21644;&#26356;&#38590;&#20197;&#32416;&#27491;&#26089;&#26399;&#36755;&#20986;&#24103;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#24310;&#36831;&#12289;&#39034;&#24207;&#35270;&#39057;&#21387;&#32553;&#19979;&#21463;&#21040;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#22833;&#30495;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65288;&#20197;&#23454;&#29616;&#30495;&#23454;&#24863;&#20026;&#30446;&#26631;&#65289;&#30340;&#36755;&#20986;&#35774;&#35745;&#20102;&#30740;&#31350;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65288;PLFs&#65289;&#65292;&#20998;&#21035;&#26159;PLF-JD&#21644;PLF-FMD&#12290;&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#26512;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102; PLF &#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#37325;&#24314;&#25928;&#26524;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#27604;&#29305;&#29575;&#30340;&#24773;&#20917;&#19979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#20445;&#30041;&#36328;&#24103;&#26102;&#24207;&#30456;&#20851;&#24615;&#26041;&#38754;&#65292;&#22522;&#20110;PLF-JD &#30340;&#37325;&#24314;&#25928;&#26524;&#20250;&#26356;&#22909;&#65292;&#20294;&#19982; PLF-FMD &#30456;&#27604;&#65292;&#20063;&#20250;&#24102;&#26469;&#26356;&#22823;&#30340;&#22833;&#30495;&#24809;&#32602;&#65292;&#24182;&#20351;&#20854;&#26356;&#38590;&#20197;&#24674;&#22797;&#26089;&#26399;&#36755;&#20986;&#24103;&#20013;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study causal, low-latency, sequential video compression when the output is subjected to both a mean squared-error (MSE) distortion loss as well as a perception loss to target realism. Motivated by prior approaches, we consider two different perception loss functions (PLFs). The first, PLF-JD, considers the joint distribution (JD) of all the video frames up to the current one, while the second metric, PLF-FMD, considers the framewise marginal distributions (FMD) between the source and reconstruction. Using information theoretic analysis and deep-learning based experiments, we demonstrate that the choice of PLF can have a significant effect on the reconstruction, especially at low-bit rates. In particular, while the reconstruction based on PLF-JD can better preserve the temporal correlation across frames, it also imposes a significant penalty in distortion compared to PLF-FMD and further makes it more difficult to recover from errors made in the earlier output frames. Although the cho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#22312;&#20225;&#19994;&#32423;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#35814;&#32454;&#35299;&#37322;&#20102;MLOps&#24037;&#20316;&#27969;&#31243;&#12289;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#19981;&#21516;&#25104;&#29087;&#24230;&#27700;&#24179;&#20197;&#21450;&#21508;&#31181;&#24213;&#23618;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#29992;&#19968;&#20010;&#29289;&#20307;&#26816;&#27979;&#26381;&#21153;&#30340;&#20225;&#19994;&#32423;MLOps&#39033;&#30446;&#30340;&#35814;&#32454;&#31034;&#20363;&#26469;&#35299;&#37322;&#25216;&#26415;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.19298</link><description>&lt;p&gt;
MLOps&#65306;&#20225;&#19994;&#32423;&#26426;&#22120;&#23398;&#20064;&#36808;&#36827;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
MLOps: A Step Forward to Enterprise Machine Learning. (arXiv:2305.19298v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#22312;&#20225;&#19994;&#32423;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#35814;&#32454;&#35299;&#37322;&#20102;MLOps&#24037;&#20316;&#27969;&#31243;&#12289;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#19981;&#21516;&#25104;&#29087;&#24230;&#27700;&#24179;&#20197;&#21450;&#21508;&#31181;&#24213;&#23618;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#29992;&#19968;&#20010;&#29289;&#20307;&#26816;&#27979;&#26381;&#21153;&#30340;&#20225;&#19994;&#32423;MLOps&#39033;&#30446;&#30340;&#35814;&#32454;&#31034;&#20363;&#26469;&#35299;&#37322;&#25216;&#26415;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#27491;&#25104;&#20026;&#24076;&#26395;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#30410;&#30340;&#20225;&#19994;&#38750;&#24120;&#20851;&#38190;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#30740;&#31350;&#35814;&#32454;&#22238;&#39038;&#20102;MLOps&#21450;&#20854;&#22909;&#22788;&#12289;&#22256;&#38590;&#12289;&#36827;&#21270;&#20197;&#21450;&#37325;&#35201;&#30340;&#24213;&#23618;&#25216;&#26415;&#65292;&#22914;MLOps&#26694;&#26550;&#12289;Docker&#12289;GitHub&#25805;&#20316;&#21644;Kubernetes&#12290;&#35814;&#32454;&#35299;&#37322;&#20102;MLOps&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#27169;&#22411;&#35774;&#35745;&#12289;&#37096;&#32626;&#21644;&#25805;&#20316;&#65292;&#20197;&#21450;&#20026;&#27169;&#22411;&#21644;&#25968;&#25454;&#25506;&#32034;&#21644;&#37096;&#32626;&#25152;&#24517;&#38656;&#30340;&#21508;&#31181;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#35813;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#20351;&#29992;&#21508;&#31181;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#19981;&#21516;&#25104;&#29087;&#24230;&#27700;&#24179;&#26469;&#31471;&#21040;&#31471;&#29983;&#20135;ML&#39033;&#30446;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#20302;&#20026;&#26080;&#33258;&#21160;&#21270;&#65292;&#26368;&#39640;&#20026;&#20855;&#26377;&#23436;&#25972;&#30340;CI/CD&#21644;CT&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#29289;&#20307;&#26816;&#27979;&#26381;&#21153;&#30340;&#20225;&#19994;&#32423;MLOps&#39033;&#30446;&#30340;&#35814;&#32454;&#31034;&#20363;&#65292;&#29992;&#20110;&#35299;&#37322;&#25216;&#26415;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;TensorFlow Object Detection API&#35757;&#32451;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#20102;Flask Web Framework&#22312;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#25176;&#31649;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Operations (MLOps) is becoming a highly crucial part of businesses looking to capitalize on the benefits of AI and ML models. This research presents a detailed review of MLOps, its benefits, difficulties, evolutions, and important underlying technologies such as MLOps frameworks, Docker, GitHub actions, and Kubernetes. The MLOps workflow, which includes model design, deployment, and operations, is explained in detail along with the various tools necessary for both model and data exploration and deployment. This article also puts light on the end-to-end production of ML projects using various maturity levels of automated pipelines, with the least at no automation at all and the highest with complete CI/CD and CT capabilities. Furthermore, a detailed example of an enterprise-level MLOps project for an object detection service is used to explain the workflow of the technology in a real-world scenario. For this purpose, a web application hosting a pre-trained model from Te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; PNKA&#65292;&#19968;&#31181;&#21487;&#20197;&#37327;&#21270;&#21333;&#20010;&#36755;&#20837;&#22312;&#20004;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20840;&#23616;&#30456;&#20284;&#24615;&#24230;&#37327;&#19981;&#33021;&#23616;&#37096;&#35843;&#26597;&#34920;&#31034;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.19294</link><description>&lt;p&gt;
&#20010;&#21035;&#28857;&#34920;&#31034;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pointwise Representational Similarity. (arXiv:2305.19294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; PNKA&#65292;&#19968;&#31181;&#21487;&#20197;&#37327;&#21270;&#21333;&#20010;&#36755;&#20837;&#22312;&#20004;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20840;&#23616;&#30456;&#20284;&#24615;&#24230;&#37327;&#19981;&#33021;&#23616;&#37096;&#35843;&#26597;&#34920;&#31034;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#36182;&#24615;&#36234;&#26469;&#36234;&#22823;&#65292;&#21457;&#23637;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#25152;&#23398;&#34920;&#31034;&#26041;&#24335;&#30340;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34920;&#24449;&#30456;&#20284;&#24615;&#24230;&#37327;&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#24120;&#29992;&#24037;&#20855;&#65292;&#29992;&#20110;&#26816;&#26597;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24230;&#37327;&#21482;&#33021;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#23545;&#30456;&#20284;&#24615;&#36827;&#34892;&#27719;&#24635;&#20272;&#35745;&#65292;&#21363;&#22312;N&#20010;&#36755;&#20837;&#31034;&#20363;&#30340;&#19968;&#32452;&#34920;&#31034;&#20013;&#36827;&#34892;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#24230;&#37327;&#19981;&#36866;&#21512;&#20110;&#22312;&#23616;&#37096;&#23618;&#38754;&#19978;&#35843;&#26597;&#34920;&#31034;&#65292;&#21363;&#21333;&#20010;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#38656;&#35201;&#23616;&#37096;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#20197;&#20102;&#35299;&#21738;&#20123;&#21333;&#20010;&#36755;&#20837;&#34920;&#31034;&#21463;&#21040;&#20102;&#27169;&#22411;&#35757;&#32451;&#24178;&#39044;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#26356;&#20844;&#24179;&#21644;&#26080;&#20559;&#65289;&#25110;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#24182;&#25552;&#20986;&#20102;PNKA&#65288;Pointwise Normalized Kernel Alignment&#65289;&#65292;&#23427;&#23545;&#27604;&#24230;&#37327;&#20102;&#22312;&#20004;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#19968;&#20010;&#20010;&#21035;&#36755;&#20837;&#30340;&#34920;&#31034;&#30456;&#20284;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing reliance on deep neural networks, it is important to develop ways to better understand their learned representations. Representation similarity measures have emerged as a popular tool for examining learned representations However, existing measures only provide aggregate estimates of similarity at a global level, i.e. over a set of representations for N input examples. As such, these measures are not well-suited for investigating representations at a local level, i.e. representations of a single input example. Local similarity measures are needed, for instance, to understand which individual input representations are affected by training interventions to models (e.g. to be more fair and unbiased) or are at greater risk of being misclassified. In this work, we fill in this gap and propose Pointwise Normalized Kernel Alignment (PNKA), a measure that quantifies how similarly an individual input is represented in two representation spaces. Intuitively, PNKA compares the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20256;&#32479;&#26041;&#27861;&#38543;&#26426;&#26862;&#26519;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#23558;&#30697;&#38453;&#20998;&#35299;&#12289;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#29305;&#23450;&#30340;&#27169;&#22411;&#26435;&#37325;&#21333;&#29420;&#25110;&#38598;&#20307;&#21152;&#20837;GCNN&#21487;&#20197;&#25552;&#39640;&#20854;&#25972;&#20307;&#24615;&#33021;&#65292;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;GCNN&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.19292</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38543;&#26426;&#26862;&#26519;&#65306;&#27604;&#36739;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21464;&#20307;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Revisiting Random Forests in a Comparative Evaluation of Graph Convolutional Neural Network Variants for Traffic Prediction. (arXiv:2305.19292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20256;&#32479;&#26041;&#27861;&#38543;&#26426;&#26862;&#26519;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#23558;&#30697;&#38453;&#20998;&#35299;&#12289;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#29305;&#23450;&#30340;&#27169;&#22411;&#26435;&#37325;&#21333;&#29420;&#25110;&#38598;&#20307;&#21152;&#20837;GCNN&#21487;&#20197;&#25552;&#39640;&#20854;&#25972;&#20307;&#24615;&#33021;&#65292;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;GCNN&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26102;&#31354;&#39044;&#27979;&#20219;&#21153;&#12290;&#30446;&#21069;&#65292;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNN&#65289;&#24050;&#25104;&#20026;&#20132;&#36890;&#39044;&#27979;&#39046;&#22495;&#20027;&#27969;&#27169;&#22411;&#65292;&#22240;&#20854;&#25797;&#38271;&#25552;&#21462;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#23545;&#25104;&#21151;&#30340;GCNN&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#32452;&#25104;&#37096;&#20998;&#30340;&#20998;&#31867;&#24182;&#20998;&#26512;&#30697;&#38453;&#20998;&#35299;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26435;&#37325;&#20849;&#20139;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#21464;&#20307;&#19982;&#20256;&#32479;&#22238;&#24402;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20262;&#22810;&#20004;&#20010;&#22320;&#21306;&#30340;&#27169;&#25311;&#25968;&#25454;&#20197;&#21450;&#36873;&#23450;&#30340;&#21152;&#24030;&#39640;&#36895;&#20844;&#36335;&#30340;&#30495;&#23454;&#19990;&#30028;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#30697;&#38453;&#20998;&#35299;&#12289;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#29305;&#23450;&#30340;&#27169;&#22411;&#26435;&#37325;&#21333;&#29420;&#25110;&#38598;&#20307;&#21152;&#20837;GCNN&#20013;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#20173;&#20855;&#26377;&#19968;&#23450;&#30340;&#31454;&#20105;&#21147;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#65292;GCNN&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a spatiotemporal predictive task that plays an essential role in intelligent transportation systems. Today, graph convolutional neural networks (GCNNs) have become the prevailing models in the traffic prediction literature since they excel at extracting spatial correlations. In this work, we classify the components of successful GCNN prediction models and analyze the effects of matrix factorization, attention mechanism, and weight sharing on their performance. Furthermore, we compare these variations against random forests, a traditional regression method that predates GCNNs by over 15 years. We evaluated these methods using simulated data of two regions in Toronto as well as real-world sensor data from selected California highways. We found that incorporating matrix factorization, attention, and location-specific model weights either individually or collectively into GCNNs can result in a better overall performance. Moreover, although random forest regression is 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#21608;&#30028;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#19982;&#27169;&#25311;&#36710;&#36742;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#21516;&#36136;&#27969;&#37327;&#20248;&#21270;&#65292;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.19291</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21608;&#30028;&#25511;&#21046;: &#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#23454;&#29616;&#21516;&#36136;&#27969;&#37327;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Perimeter Control Using Deep Reinforcement Learning: A Model-free Approach towards Homogeneous Flow Rate Optimization. (arXiv:2305.19291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#21608;&#30028;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#19982;&#27169;&#25311;&#36710;&#36742;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#21516;&#36136;&#27969;&#37327;&#20248;&#21270;&#65292;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21608;&#30028;&#25511;&#21046;&#36890;&#36807;&#25511;&#21046;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#30340;&#20132;&#36890;&#36716;&#31227;&#27969;&#37327;&#65292;&#20197;&#20445;&#35777;&#20854;&#20132;&#36890;&#23494;&#24230;&#20302;&#20110;&#20020;&#30028;&#20540;&#65292;&#20174;&#32780;&#20445;&#25345;&#21463;&#20445;&#25252;&#21306;&#22495;&#20869;&#20132;&#36890;&#39640;&#25928;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#32593;&#32476;&#20256;&#36755;&#27169;&#22411;&#65288;NTMs&#65289;&#21644;&#23439;&#35266;&#22522;&#26412;&#22270;&#65288;MFDs&#65289;&#30340;&#27169;&#22411;&#22522;&#26041;&#27861;&#21644;&#26080;&#27169;&#22411;&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#20445;&#35777;&#65292;&#20294;&#23427;&#20204;&#22825;&#29983;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#20559;&#24046;&#21644;&#19981;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#27809;&#26377;&#19968;&#39033;&#30740;&#31350;&#22312;&#24494;&#35266;&#27169;&#25311;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#21516;&#36136;&#27969;&#37327;&#20248;&#21270;&#65292;&#24494;&#35266;&#27169;&#25311;&#32771;&#34385;&#20102;&#31354;&#38388;&#29305;&#24449;&#12289;&#36710;&#36742;&#32423;&#21035;&#20449;&#24687;&#21644;&#35745;&#37327;&#23454;&#29616;&#65292;&#36825;&#20123;&#22312;&#23439;&#35266;&#27169;&#25311;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26080;&#27169;&#22411;&#21608;&#30028;&#25511;&#21046;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#19982;&#27169;&#25311;&#36710;&#36742;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#22914;&#20309;&#20248;&#21270;&#20449;&#21495;&#26102;&#24207;&#20197;&#25511;&#21046;&#21463;&#20445;&#25252;&#21306;&#22495;&#30340;&#27969;&#20837;&#21644;&#27969;&#20986;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;DRL&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#20132;&#36890;&#24773;&#20917;&#19979;&#22343;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perimeter control maintains high traffic efficiency within protected regions by controlling transfer flows among regions to ensure that their traffic densities are below critical values. Existing approaches can be categorized as either model-based or model-free, depending on whether they rely on network transmission models (NTMs) and macroscopic fundamental diagrams (MFDs). Although model-based approaches are more data efficient and have performance guarantees, they are inherently prone to model bias and inaccuracy. For example, NTMs often become imprecise for a large number of protected regions, and MFDs can exhibit scatter and hysteresis that are not captured in existing model-based works. Moreover, no existing studies have employed reinforcement learning for homogeneous flow rate optimization in microscopic simulation, where spatial characteristics, vehicle-level information, and metering realizations -- often overlooked in macroscopic simulations -- are taken into account. To circu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#27169;&#22411;&#20010;&#24615;&#21270;&#26041;&#27861;Global Layers (GL)&#65292;&#35813;&#26041;&#27861;&#26159;&#30446;&#21069;&#21807;&#19968;&#19968;&#31181;&#33021;&#22815;&#25903;&#25345;&#23458;&#25143;&#31471;&#19987;&#26377;&#29305;&#24449;&#21644;&#31867;&#21035;&#30340;FL&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#23454;&#39564;&#20013;&#65292;GL&#30340;&#24615;&#33021;&#20248;&#20110;&#32852;&#37030;&#24179;&#22343;&#21644;&#20165;&#26412;&#22320;&#35757;&#32451;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#26377;&#20123;&#23458;&#25143;&#31471;&#30340;&#24615;&#33021;&#27604;&#20182;&#20204;&#30340;&#38598;&#20013;&#24335;&#22522;&#32447;&#36824;&#35201;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19290</link><description>&lt;p&gt;
&#20840;&#23616;&#23618;&#65306;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#34920;&#26684;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Global Layers: Non-IID Tabular Federated Learning. (arXiv:2305.19290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#27169;&#22411;&#20010;&#24615;&#21270;&#26041;&#27861;Global Layers (GL)&#65292;&#35813;&#26041;&#27861;&#26159;&#30446;&#21069;&#21807;&#19968;&#19968;&#31181;&#33021;&#22815;&#25903;&#25345;&#23458;&#25143;&#31471;&#19987;&#26377;&#29305;&#24449;&#21644;&#31867;&#21035;&#30340;FL&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#23454;&#39564;&#20013;&#65292;GL&#30340;&#24615;&#33021;&#20248;&#20110;&#32852;&#37030;&#24179;&#22343;&#21644;&#20165;&#26412;&#22320;&#35757;&#32451;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#26377;&#20123;&#23458;&#25143;&#31471;&#30340;&#24615;&#33021;&#27604;&#20182;&#20204;&#30340;&#38598;&#20013;&#24335;&#22522;&#32447;&#36824;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#20173;&#28982;&#26159;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;Global Layers&#65288;GL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#27169;&#22411;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20855;&#26377;&#32852;&#21512;&#20998;&#24067; $P(X,Y)$ &#36716;&#21464;&#21644;&#28151;&#21512;&#36755;&#20837;/&#36755;&#20986;&#31354;&#38388; $X \times Y$ &#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#40065;&#26834;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GL&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#25903;&#25345;&#23458;&#25143;&#31471;&#19987;&#26377;&#29305;&#24449;&#21644;&#31867;&#21035;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#29616;&#26377;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#33258;&#28982;&#22320;&#23545;&#34920;&#26684;FL&#36827;&#34892;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#23454;&#39564;&#65306;i&#65289;&#23558;UCI Covertype&#20998;&#20026;4&#20010;&#20855;&#26377;&#8220;&#37326;&#22806;&#21306;&#22495;&#8221;&#29305;&#24449;&#30340;&#23458;&#25143;&#31471;&#65292;&#20197;&#21450;ii&#65289;&#23558;UCI Heart Disease&#12289;SAHeart&#12289;UCI Heart Failure&#20998;&#21035;&#20316;&#20026;&#23458;&#25143;&#31471;&#12290;&#22312;&#20840;&#21592;&#21442;&#19982;&#35774;&#32622;&#30340;&#23454;&#39564;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;GL&#30340;&#24615;&#33021;&#20248;&#20110;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#21644;&#20165;&#26412;&#22320;&#35757;&#32451;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#26377;&#20123;&#23458;&#25143;&#31471;&#30340;&#24615;&#33021;&#27604;&#20182;&#20204;&#30340;&#38598;&#20013;&#24335;&#22522;&#32447;&#36824;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity between clients remains a key challenge in Federated Learning (FL), particularly in the case of tabular data. This work presents Global Layers (GL), a novel partial model personalization method robust in the presence of joint distribution $P(X,Y)$ shift and mixed input/output spaces $X \times Y$ across clients. To the best of our knowledge, GL is the first method capable of supporting both client-exclusive features and classes. We introduce two new benchmark experiments for tabular FL naturally partitioned from existing real world datasets: i) UCI Covertype split into 4 clients by "wilderness area" feature, and ii) UCI Heart Disease, SAHeart, UCI Heart Failure, each as clients. Empirical results in these experiments in the full-participant setting show that GL achieves better outcomes than Federated Averaging (FedAvg) and local-only training, with some clients even performing better than their centralized baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23545;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19280</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Large language models improve Alzheimer's disease diagnosis using multi-modality data. (arXiv:2305.19280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23545;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35786;&#26029;&#20687;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#36825;&#26679;&#30340;&#33392;&#38590;&#30149;&#30151;&#26102;&#65292;&#24433;&#20687;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;&#38750;&#24433;&#20687;&#24739;&#32773;&#25968;&#25454;&#65288;&#20363;&#22914;&#24739;&#32773;&#20449;&#24687;&#12289;&#36951;&#20256;&#25968;&#25454;&#12289;&#33647;&#29289;&#20449;&#24687;&#12289;&#35748;&#30693;&#21644;&#35760;&#24518;&#27979;&#35797;&#65289;&#22312;&#35786;&#26029;&#20013;&#20063;&#36215;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25366;&#25496;&#36825;&#20123;&#20449;&#24687;&#30340;&#33021;&#21147;&#21463;&#38480;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#27169;&#22411;&#21482;&#33021;&#20351;&#29992;&#22810;&#27169;&#24577;&#24433;&#20687;&#25968;&#25454;&#65292;&#32780;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#38750;&#24433;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#38750;&#24120;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#20351;&#29992;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In diagnosing challenging conditions such as Alzheimer's disease (AD), imaging is an important reference. Non-imaging patient data such as patient information, genetic data, medication information, cognitive and memory tests also play a very important role in diagnosis. Effect. However, limited by the ability of artificial intelligence models to mine such information, most of the existing models only use multi-modal image data, and cannot make full use of non-image data. We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38750;&#21512;&#20316;&#25968;&#25454;&#39537;&#21160;&#28216;&#25103;&#65292;&#20854;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30830;&#23450;&#26377;&#25928;&#30340;&#26448;&#26009;&#23450;&#24459;&#65292;&#31616;&#21270;&#23454;&#38469;&#23454;&#26045;&#65292;&#24182;&#19988;&#19982;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#27809;&#26377;&#20551;&#35774;&#21644;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.19279</link><description>&lt;p&gt;
&#35745;&#31639;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Games in Computational Mechanics. (arXiv:2305.19279v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38750;&#21512;&#20316;&#25968;&#25454;&#39537;&#21160;&#28216;&#25103;&#65292;&#20854;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30830;&#23450;&#26377;&#25928;&#30340;&#26448;&#26009;&#23450;&#24459;&#65292;&#31616;&#21270;&#23454;&#38469;&#23454;&#26045;&#65292;&#24182;&#19988;&#19982;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#27809;&#26377;&#20551;&#35774;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#21338;&#24328;&#35770;&#26469;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#30340;&#23454;&#20307;&#21147;&#23398;&#26041;&#27861;&#65292;&#20854;&#20013;&#24212;&#21147;&#21644;&#24212;&#21464;&#29609;&#23478;&#36861;&#27714;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#24212;&#21147;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#23558;&#20854;&#19982;&#26448;&#26009;&#25968;&#25454;&#38598;&#30340;&#24046;&#24322;&#26368;&#23567;&#21270;&#65292;&#32780;&#24212;&#21464;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#26426;&#26800;&#29366;&#24577;&#30340;&#21487;&#20801;&#35768;&#24615;&#65292;&#21363;&#30456;&#23481;&#24615;&#21644;&#24179;&#34913;&#24615;&#12290;&#19982;&#36807;&#21435;&#25552;&#20986;&#30340;&#21512;&#20316;&#25968;&#25454;&#39537;&#21160;&#28216;&#25103;&#19981;&#21516;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26032;&#22411;&#30340;&#38750;&#21512;&#20316;&#25968;&#25454;&#39537;&#21160;&#28216;&#25103;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30830;&#23450;&#26377;&#25928;&#30340;&#26448;&#26009;&#23450;&#24459;&#65292;&#24182;&#31616;&#21270;&#20026;&#20256;&#32479;&#20301;&#31227;&#36793;&#30028;&#20540;&#38382;&#39064;&#65292;&#36825;&#26377;&#21161;&#20110;&#23454;&#38469;&#23454;&#26045;&#12290;&#25552;&#20986;&#30340;&#38750;&#21512;&#20316;&#25968;&#25454;&#39537;&#21160;&#28216;&#25103;&#19982;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#27809;&#26377;&#20551;&#35774;&#21644;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#26377;&#25928;&#30340;&#26448;&#26009;&#23450;&#24459;&#26159;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#65292;&#32780;&#26080;&#38656;&#22238;&#24402;&#21040;&#31070;&#32463;&#32593;&#32476;&#31561;&#21442;&#25968;&#21270;&#20989;&#25968;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We resort to game theory in order to formulate Data-Driven methods for solid mechanics in which stress and strain players pursue different objectives. The objective of the stress player is to minimize the discrepancy to a material data set, whereas the objective of the strain player is to ensure the admissibility of the mechanical state, in the sense of compatibility and equilibrium. We show that, unlike the cooperative Data-Driven games proposed in the past, the new non-cooperative Data-Driven games identify an effective material law from the data and reduce to conventional displacement boundary-value problems, which facilitates their practical implementation. However, unlike supervised machine learning methods, the proposed non-cooperative Data-Driven games are unsupervised, ansatz-free and parameter-free. In particular, the effective material law is learned from the data directly, without recourse to regression to a parameterized class of functions such as neural networks. We presen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#24179;&#22343;&#20540;&#20248;&#21270;&#20219;&#21153;&#30446;&#26631;&#65292;&#20174;&#38750;&#32447;&#24615;&#32806;&#21512;&#20013;&#33258;&#21457;&#20986;&#29616;&#30340;&#26080;&#30417;&#30563;&#21327;&#26041;&#24046;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.19265</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#20852;&#21327;&#26041;&#24046;&#36827;&#34892;&#27010;&#29575;&#35745;&#31639;&#65306;&#36208;&#21521;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Computation with Emerging Covariance: Towards Efficient Uncertainty Quantification. (arXiv:2305.19265v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#24179;&#22343;&#20540;&#20248;&#21270;&#20219;&#21153;&#30446;&#26631;&#65292;&#20174;&#38750;&#32447;&#24615;&#32806;&#21512;&#20013;&#33258;&#21457;&#20986;&#29616;&#30340;&#26080;&#30417;&#30563;&#21327;&#26041;&#24046;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#40065;&#26834;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#24378;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#36890;&#36807;&#27010;&#29575;&#35270;&#35282;&#37327;&#21270;&#21644;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#27169;&#20223;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27010;&#29575;&#35745;&#31639;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25130;&#26029;&#27010;&#29575;&#34920;&#31034;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#21363;&#24179;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#32593;&#32476;&#30340;&#30830;&#23450;&#24615;&#26367;&#20195;&#21697;&#26469;&#23454;&#20363;&#21270;&#35813;&#26694;&#26550;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#31616;&#21333;&#28608;&#27963;&#30340;&#32452;&#21512;&#23398;&#20064;&#22797;&#26434;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#23553;&#35013;&#20102;&#24179;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30340;&#38750;&#32447;&#24615;&#32806;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#24179;&#22343;&#20540;&#21463;&#21040;&#30417;&#30563;&#20197;&#20248;&#21270;&#20219;&#21153;&#30446;&#26631;&#26102;&#65292;&#20174;&#20854;&#19982;&#21327;&#26041;&#24046;&#30340;&#38750;&#32447;&#24615;&#32806;&#21512;&#20013;&#33258;&#21457;&#20986;&#29616;&#30340;&#26080;&#30417;&#30563;&#21327;&#26041;&#24046;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building robust, interpretable, and secure artificial intelligence system requires some degree of quantifying and representing uncertainty via a probabilistic perspective, as it allows to mimic human cognitive abilities. However, probabilistic computation presents significant challenges due to its inherent complexity. In this paper, we develop an efficient and interpretable probabilistic computation framework by truncating the probabilistic representation up to its first two moments, i.e., mean and covariance. We instantiate the framework by training a deterministic surrogate of a stochastic network that learns the complex probabilistic representation via combinations of simple activations, encapsulating the non-linearities coupling of the mean and covariance. We show that when the mean is supervised for optimizing the task objective, the unsupervised covariance spontaneously emerging from the non-linear coupling with the mean faithfully captures the uncertainty associated with model p
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476;&#27169;&#22359;&#65292;&#21487;&#20351;&#33292;&#37096;&#36229;&#22768;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26080;&#22768;&#35821;&#38899;&#25509;&#21475;&#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#19981;&#21516;&#30340;&#29992;&#25143;&#21644;&#20250;&#35805;&#65292;&#19988;&#33021;&#26174;&#33879;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19130</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476;&#30340;&#33292;&#37096;&#36229;&#22768;&#26080;&#22768;&#35821;&#38899;&#25509;&#21475;&#30340;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks. (arXiv:2305.19130v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19130
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476;&#27169;&#22359;&#65292;&#21487;&#20351;&#33292;&#37096;&#36229;&#22768;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26080;&#22768;&#35821;&#38899;&#25509;&#21475;&#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#19981;&#21516;&#30340;&#29992;&#25143;&#21644;&#20250;&#35805;&#65292;&#19988;&#33021;&#26174;&#33879;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#20174;&#21457;&#38899;&#36816;&#21160;&#25968;&#25454;&#20013;&#21512;&#25104;&#21487;&#25026;&#30340;&#35821;&#38899;&#65292;&#36825;&#20123;&#25104;&#26524;&#26469;&#33258;&#20110;&#26080;&#22768;&#35821;&#38899;&#25509;&#21475;&#65288;SSI&#65289;&#12290;&#28982;&#32780;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#24448;&#24448;&#29305;&#23450;&#20110;&#26576;&#19968;&#35828;&#35805;&#20154;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#36827;&#34892;&#24555;&#36895;&#20999;&#25442;&#21464;&#24471;&#40635;&#28902;&#12290;&#21363;&#20351;&#26159;&#21516;&#19968;&#20010;&#35762;&#32773;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#36827;&#34892;&#27169;&#22411;&#24212;&#29992;&#25928;&#26524;&#20063;&#36739;&#24046;&#12290;&#20026;&#20102;&#24110;&#21161;&#33292;&#37096;&#36229;&#22768;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340; SSI &#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#19981;&#21516;&#30340;&#29992;&#25143;&#21644;&#20250;&#35805;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#28145;&#24230;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476; (STN) &#27169;&#22359;&#65292;&#33021;&#22815;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#12290;&#34429;&#28982; STN &#21482;&#21344;&#32593;&#32476;&#30340;&#32422; 10%&#65292;&#20294;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36866;&#24212; STN &#27169;&#22359;&#23601;&#21487;&#20197;&#23558;&#22343;&#26041;&#35823;&#24046;&#24179;&#22343;&#20943;&#23569; 88%&#65292;&#32780;&#19981;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#32593;&#32476;&#12290;&#24403;&#23558;&#32593;&#32476;&#36866;&#24212;&#21040;&#21516;&#19968;&#35762;&#32773;&#30340;&#19981;&#21516;&#35760;&#24405;&#20250;&#35805;&#26102;&#65292;&#25913;&#36827;&#25928;&#26524;&#26356;&#22823;&#65288;&#22823;&#32422; 92%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thanks to the latest deep learning algorithms, silent speech interfaces (SSI) are now able to synthesize intelligible speech from articulatory movement data under certain conditions. However, the resulting models are rather speaker-specific, making a quick switch between users troublesome. Even for the same speaker, these models perform poorly cross-session, i.e. after dismounting and re-mounting the recording equipment. To aid quick speaker and session adaptation of ultrasound tongue imaging-based SSI models, we extend our deep networks with a spatial transformer network (STN) module, capable of performing an affine transformation on the input images. Although the STN part takes up only about 10% of the network, our experiments show that adapting just the STN module might allow to reduce MSE by 88% on the average, compared to retraining the whole network. The improvement is even larger (around 92%) when adapting the network to different recording sessions from the same speaker.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32435;&#31859;&#20809;&#23376;&#23398;&#22120;&#20214;&#21453;&#21521;&#35774;&#35745;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#36827;&#34892;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#37325;&#22797;&#30340;&#31185;&#23398;&#35774;&#35745;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#31665;IDToolkit&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#27169;&#25311;&#21644;&#20248;&#21270;&#22120;&#27169;&#22359;&#20197;&#21450;&#21518;&#22788;&#29702;&#32467;&#26524;&#30340;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#19982;&#22522;&#20934;&#26041;&#27861;&#27604;&#36739;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18978</link><description>&lt;p&gt;
IDToolkit: &#29992;&#20110;&#32435;&#31859;&#20809;&#23376;&#23398;&#21453;&#21521;&#35774;&#35745;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#21644;&#24320;&#21457;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design Algorithms in Nanophotonics. (arXiv:2305.18978v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32435;&#31859;&#20809;&#23376;&#23398;&#22120;&#20214;&#21453;&#21521;&#35774;&#35745;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#36827;&#34892;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#37325;&#22797;&#30340;&#31185;&#23398;&#35774;&#35745;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#31665;IDToolkit&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#27169;&#25311;&#21644;&#20248;&#21270;&#22120;&#27169;&#22359;&#20197;&#21450;&#21518;&#22788;&#29702;&#32467;&#26524;&#30340;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#19982;&#22522;&#20934;&#26041;&#27861;&#27604;&#36739;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24110;&#21161;&#20154;&#31867;&#36827;&#34892;&#31185;&#23398;&#35774;&#35745;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21457;&#29616;&#26032;&#33647;&#29289;&#12289;&#35774;&#35745;&#26032;&#26448;&#26009;&#21644;&#21270;&#21512;&#29289;&#31561;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#35774;&#35745;&#36890;&#24120;&#38656;&#35201;&#29087;&#24713;&#39046;&#22495;&#30693;&#35782;&#30340;&#19987;&#19994;&#25216;&#33021;&#65292;&#36825;&#20123;&#23545;&#20110;AI&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#24182;&#19981;&#29087;&#24713;&#12290;&#27492;&#22806;&#65292;&#31185;&#23398;&#30740;&#31350;&#38656;&#35201;&#19987;&#19994;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#25216;&#33021;&#12290;&#36825;&#20123;&#38556;&#30861;&#38459;&#30861;&#20102;AI&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#20026;&#36808;&#21521;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#31185;&#23398;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32435;&#31859;&#20809;&#23376;&#23398;&#22120;&#20214;&#21453;&#21521;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#22312;&#35745;&#31639;&#19978;&#21644;&#20934;&#30830;&#22320;&#39564;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#32435;&#31859;&#20809;&#23376;&#23398;&#35774;&#35745;&#38382;&#39064;&#65292;&#20998;&#21035;&#26159;&#36752;&#23556;&#20919;&#21364;&#22120;&#65292;&#36866;&#29992;&#20110;&#28909;&#20809;&#20239;&#36873;&#25321;&#24615;&#21457;&#23556;&#20307;&#20197;&#21450;&#32467;&#26500;&#33394;&#28388;&#20809;&#22120;&#65292;&#23427;&#20204;&#22312;&#35774;&#35745;&#21442;&#25968;&#31354;&#38388;&#12289;&#22797;&#26434;&#24230;&#21644;&#29289;&#29702;&#24615;&#36136;&#19978;&#37117;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IDToolkit&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#19982;&#22522;&#20934;&#26041;&#27861;&#27604;&#36739;&#20854;&#31639;&#27861;&#30340;&#31616;&#21333;&#25509;&#21475;&#12290;IDToolkit&#21253;&#21547;&#20102;&#21453;&#21521;&#35774;&#35745;&#25152;&#38656;&#30340;&#20960;&#20010;&#27169;&#22359;&#65292;&#20363;&#22914;&#27169;&#25311;&#27169;&#22359;&#21644;&#20248;&#21270;&#22120;&#27169;&#22359;&#65292;&#20197;&#21450;&#21518;&#22788;&#29702;&#32467;&#26524;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#24076;&#26395;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#24037;&#20855;&#21253;&#23558;&#21152;&#36895;&#32435;&#31859;&#20809;&#23376;&#23398;&#31185;&#23398;&#35774;&#35745;&#26032;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiding humans with scientific designs is one of the most exciting of artificial intelligence (AI) and machine learning (ML), due to their potential for the discovery of new drugs, design of new materials and chemical compounds, etc. However, scientific design typically requires complex domain knowledge that is not familiar to AI researchers. Further, scientific studies involve professional skills to perform experiments and evaluations. These obstacles prevent AI researchers from developing specialized methods for scientific designs. To take a step towards easy-to-understand and reproducible research of scientific design, we propose a benchmark for the inverse design of nanophotonic devices, which can be verified computationally and accurately. Specifically, we implemented three different nanophotonic design problems, namely a radiative cooler, a selective emitter for thermophotovoltaics, and structural color filters, all of which are different in design parameter spaces, complexity, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#21367;&#31215;&#33945;&#26085;&#26144;&#23556;&#24402;&#19968;&#21270; (CMMN)&#65292;&#29992;&#20110;&#20449;&#21495;&#20247;&#22810;&#20294;&#21464;&#24322;&#24615;&#36739;&#22823;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#65292;&#33021;&#33258;&#36866;&#24212;&#35843;&#25972;&#24133;&#24230;&#12289;&#28388;&#27874;&#22120;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#21644; barycenters &#23454;&#29616;&#20010;&#20307;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#19988;&#26174;&#33879;&#25552;&#21319;&#20449;&#21495;&#20998;&#31867;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18831</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#33945;&#26085;&#26144;&#23556;&#24402;&#19968;&#21270;&#30340;&#29983;&#29289;&#20449;&#21495;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Convolutional Monge Mapping Normalization for learning on biosignals. (arXiv:2305.18831v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#21367;&#31215;&#33945;&#26085;&#26144;&#23556;&#24402;&#19968;&#21270; (CMMN)&#65292;&#29992;&#20110;&#20449;&#21495;&#20247;&#22810;&#20294;&#21464;&#24322;&#24615;&#36739;&#22823;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#65292;&#33021;&#33258;&#36866;&#24212;&#35843;&#25972;&#24133;&#24230;&#12289;&#28388;&#27874;&#22120;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#21644; barycenters &#23454;&#29616;&#20010;&#20307;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#19988;&#26174;&#33879;&#25552;&#21319;&#20449;&#21495;&#20998;&#31867;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20449;&#21495;&#19982;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#33041;&#30005;&#22270; (EEG) &#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#25968;&#25454;&#22312;&#21463;&#35797;&#32773;&#12289;&#20250;&#35805;&#21644;&#30828;&#20214;&#35774;&#22791;&#19978;&#30340;&#21464;&#24322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#21367;&#31215;&#33945;&#26085;&#26144;&#23556;&#24402;&#19968;&#21270; (CMMN)&#65292;&#20854;&#26680;&#24515;&#26159;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#20272;&#35745;Wasserstein barycenter&#65292;&#36890;&#36807;&#28388;&#27874;&#20449;&#21495;&#20197;&#36866;&#24212;&#20854;&#21151;&#29575;&#35889;&#23494;&#24230; (PSD)&#12290;CMMN &#22522;&#20110;&#26032;&#30340;&#38381;&#24335;&#35299;&#65292;&#25552;&#20379;&#20102;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#21644; barycenters&#65292;&#24182;&#25552;&#20379;&#20102;&#20010;&#20307;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#30561;&#30496; EEG &#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;CMMN &#22312;&#36866;&#24212;&#21463;&#35797;&#32773;&#12289;&#20250;&#35805;&#29978;&#33267;&#22312;&#20351;&#29992;&#19981;&#21516;&#30828;&#20214;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#65292;&#37117;&#33021;&#24102;&#26469;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#19988;&#20854;&#24615;&#33021;&#25552;&#21319;&#19982;&#25968;&#20540;&#23494;&#38598;&#30340;&#22495;&#36866;&#24212; (DA) &#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization (CMMN), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. CMMN relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that CMMN leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18766</link><description>&lt;p&gt;
HiFA: &#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#21450;&#20854;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;3D&#27169;&#22411;&#65292;&#33258;&#21160;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#22312;&#25552;&#21319;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#25552;&#20379;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#30340;2D&#28210;&#26579;&#24471;&#20998;&#24182;&#29992;&#20110;&#20248;&#21270;NeRFs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;3D&#20960;&#20309;&#30340;&#26377;&#38480;&#29702;&#35299;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36935;&#21040;&#22810;&#20010;&#35270;&#35282;&#19978;&#30340;&#20266;&#24433;&#21644;&#19981;&#19968;&#33268;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#20808;&#39564;&#37325;&#26032;&#21046;&#23450;&#20248;&#21270;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#37322;&#25918;&#20102;&#25193;&#25955;&#20808;&#39564;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#65292;&#25105;&#20204;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#65292;&#24182;&#35268;&#33539;&#21270;NeRF&#30340;&#23494;&#24230;&#22330;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18758</link><description>&lt;p&gt;
&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#27599;&#31867;&#20855;&#26377;&#36275;&#22815;&#26631;&#35760;&#33410;&#28857;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#19981;&#26159;&#25152;&#26377;&#31867;&#37117;&#26377;&#35768;&#22810;&#26631;&#35760;&#33410;&#28857;&#65292;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#20998;&#31867;&#26032;&#31867;&#21035;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#35760;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;GNN&#38656;&#35201;&#33021;&#22815;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#33410;&#28857;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#31216;&#20026;Few-shot&#33410;&#28857;&#20998;&#31867;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#21095;&#38598;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;Few-shot&#33410;&#28857;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#20165;&#26377;&#22810;&#26679;&#30340;&#35757;&#32451;&#20803;&#20219;&#21153;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;Few-shot&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TEG&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#26356;&#24191;&#27867;&#30340;&#26680;&#20989;&#25968;&#65292;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#21487;&#21046;&#23450;&#20986;&#26377;&#25928;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18755</link><description>&lt;p&gt;
&#36890;&#29992;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#38477;&#32500;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction for General KDE Mode Finding. (arXiv:2305.18755v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#26356;&#24191;&#27867;&#30340;&#26680;&#20989;&#25968;&#65292;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#21487;&#21046;&#23450;&#20986;&#26377;&#25928;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#23547;&#25214;&#39640;&#32500;&#27010;&#29575;&#20998;&#24067;&#30340;&#27169;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#27169;&#20272;&#35745;&#32467;&#26524;&#36827;&#34892;&#20102;&#36890;&#29992;&#25193;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#26680;&#20989;&#25968;&#65292;&#21253;&#25324;&#27969;&#34892;&#30340;&#36923;&#36753;&#12289;Sigmoid&#21644;&#24191;&#20041;&#39640;&#26031;&#26680;&#12290;&#36890;&#36807;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#65292;&#35813;&#31639;&#27861;&#21487;&#21046;&#23450;&#20986;&#26377;&#25928;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the mode of a high dimensional probability distribution $D$ is a fundamental algorithmic problem in statistics and data analysis. There has been particular interest in efficient methods for solving the problem when $D$ is represented as a mixture model or kernel density estimate, although few algorithmic results with worst-case approximation and runtime guarantees are known.  In this work, we significantly generalize a result of (LeeLiMusco:2021) on mode approximation for Gaussian mixture models. We develop randomized dimensionality reduction methods for mixtures involving a broader class of kernels, including the popular logistic, sigmoid, and generalized Gaussian kernels. As in Lee et al.'s work, our dimensionality reduction results yield quasi-polynomial algorithms for mode finding with multiplicative accuracy $(1-\epsilon)$ for any $\epsilon &gt; 0$. Moreover, when combined with gradient descent, they yield efficient practical heuristics for the problem.  In addition to our po
&lt;/p&gt;</description></item><item><title>NUNO &#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#25968;&#25454;&#30340;&#39640;&#25928;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#19977;&#32500; PDE &#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18694</link><description>&lt;p&gt;
NUNO: &#29992;&#20110;&#23398;&#20064;&#38750;&#22343;&#21248;&#25968;&#25454; Parametric PDEs &#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NUNO: A General Framework for Learning Parametric PDEs with Non-Uniform Data. (arXiv:2305.18694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18694
&lt;/p&gt;
&lt;p&gt;
NUNO &#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#25968;&#25454;&#30340;&#39640;&#25928;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#19977;&#32500; PDE &#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20989;&#25968;&#31354;&#38388;&#26144;&#23556;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#24403;&#38754;&#20020;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20998;&#24067;&#38750;&#24120;&#19981;&#22343;&#21248;&#65292;&#20351;&#29992;&#22522;&#20110;&#32593;&#26684;&#30340;&#25216;&#26415;&#65288;&#22914;FFT&#65289;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Non-Uniform Neural Operator (NUNO)&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#25968;&#25454;&#30340;&#39640;&#25928;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;K-D&#26641;&#30340;&#22495;&#20998;&#35299;&#65292;&#25105;&#20204;&#23558;&#38750;&#22343;&#21248;&#25968;&#25454;&#36716;&#25442;&#25104;&#22343;&#21248;&#32593;&#26684;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#25511;&#21046;&#25554;&#20540;&#35823;&#24046;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#20174;&#38750;&#22343;&#21248;&#25968;&#25454;&#23398;&#20064;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#24182;&#34892;&#12290;&#25105;&#20204;&#22312;&#20108;&#32500;&#24377;&#24615;&#12289;(2+1)D&#27827;&#36947;&#27969;&#21644;&#19977;&#32500;&#22810;&#29289;&#29702; heatsink &#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#28041;&#21450;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#19977;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;&#26368;&#22810;60&#65285;&#65292;&#24182;&#23558;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;2&#20493;&#33267;30&#20493;&#12290;&#20195;&#30721;&#29616;&#22312;&#22312;https://github.com &#19978;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural operator has emerged as a powerful tool in learning mappings between function spaces in PDEs. However, when faced with real-world physical data, which are often highly non-uniformly distributed, it is challenging to use mesh-based techniques such as the FFT. To address this, we introduce the Non-Uniform Neural Operator (NUNO), a comprehensive framework designed for efficient operator learning with non-uniform data. Leveraging a K-D tree-based domain decomposition, we transform non-uniform data into uniform grids while effectively controlling interpolation error, thereby paralleling the speed and accuracy of learning from non-uniform data. We conduct extensive experiments on 2D elasticity, (2+1)D channel flow, and a 3D multi-physics heatsink, which, to our knowledge, marks a novel exploration into 3D PDE problems with complex geometries. Our framework has reduced error rates by up to 60% and enhanced training speeds by 2x to 30x. The code is now available at https://github.co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25910;&#25947;&#20998;&#26512;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#28508;&#22312;&#30340;&#25238;&#21160;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#36827;&#19968;&#27493;&#32467;&#21512;&#22810;&#20010;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.18594</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#20316;&#23398;&#20064;&#30340;&#20998;&#26512;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Analytic End-to-End Deep Learning Algorithm based on Collaborative Learning. (arXiv:2305.18594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25910;&#25947;&#20998;&#26512;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#28508;&#22312;&#30340;&#25238;&#21160;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#36827;&#19968;&#27493;&#32467;&#21512;&#22810;&#20010;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#25511;&#21046;&#24212;&#29992;&#20013;&#65292;&#31995;&#32479;&#30340;&#29702;&#35770;&#20998;&#26512;&#23545;&#30830;&#20445;&#31283;&#23450;&#24615;&#25110;&#25910;&#25947;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21487;&#38752;&#30340;&#36816;&#34892;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#31995;&#32479;&#29702;&#35299;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#40657;&#30418;&#26041;&#27861;&#65292;&#26356;&#22810;&#22320;&#20851;&#27880;&#32463;&#39564;&#24615;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25910;&#25947;&#20998;&#26512;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#28508;&#22312;&#30340;&#25238;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#19981;&#26131;&#23548;&#33268;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20004;&#23618;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#36827;&#19968;&#27493;&#32467;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most control applications, theoretical analysis of the systems is crucial in ensuring stability or convergence, so as to ensure safe and reliable operations and also to gain a better understanding of the systems for further developments. However, most current deep learning methods are black-box approaches that are more focused on empirical studies. Recently, some results have been obtained for convergence analysis of end-to end deep learning based on non-smooth ReLU activation functions, which may result in chattering for control tasks. This paper presents a convergence analysis for end-to-end deep learning of fully connected neural networks (FNN) with smooth activation functions. The proposed method therefore avoids any potential chattering problem, and it also does not easily lead to gradient vanishing problems. The proposed End-to-End algorithm trains multiple two-layer fully connected networks concurrently and collaborative learning can be used to further combine their strengths
&lt;/p&gt;</description></item><item><title>Trompt&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20854;&#20013;&#20998;&#31163;&#20102;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#35843;&#25972;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#33021;&#22815;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18446</link><description>&lt;p&gt;
Trompt&#65306;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#26356;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Trompt: Towards a Better Deep Neural Network for Tabular Data. (arXiv:2305.18446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18446
&lt;/p&gt;
&lt;p&gt;
Trompt&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20854;&#20013;&#20998;&#31163;&#20102;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#35843;&#25972;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#33021;&#22815;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#21487;&#35859;&#26159;&#21508;&#31181;&#23454;&#38469;&#39046;&#22495;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#32467;&#26500;&#20043;&#19968;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#30005;&#23376;&#21830;&#21153;&#12290;&#20869;&#22312;&#30340;&#24322;&#36136;&#24615;&#20801;&#35768;&#34920;&#26684;&#25968;&#25454;&#23384;&#20648;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#26368;&#36817;&#21457;&#24067;&#30340;&#34920;&#26684;&#22522;&#20934;&#65292;&#25105;&#20204;&#21487;&#20197;&#30475;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#26641;&#29366;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Trompt&#65292;&#23427;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#19968;&#32452;&#27169;&#22411;&#22806;&#30340;&#25552;&#31034;&#26469;&#35843;&#25972;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20462;&#25913;&#27169;&#22411;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;Trompt&#23558;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#31574;&#30053;&#20998;&#20026;&#20004;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#31867;&#20284;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#27880;&#37325;&#23398;&#20064;&#34920;&#26684;&#30340;&#20869;&#22312;&#20449;&#24687;&#12290;&#31532;&#20108;&#37096;&#20998;&#31867;&#20284;&#20110;&#25552;&#31034;&#65292;&#27880;&#37325;&#23398;&#20064;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290; Trompt&#22312;&#19978;&#36848;&#22522;&#20934;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is arguably one of the most commonly used data structures in various practical domains, including finance, healthcare and e-commerce. The inherent heterogeneity allows tabular data to store rich information. However, based on a recently published tabular benchmark, we can see deep neural networks still fall behind tree-based models on tabular datasets. In this paper, we propose Trompt--which stands for Tabular Prompt--a novel architecture inspired by prompt learning of language models. The essence of prompt learning is to adjust a large pre-trained model through a set of prompts outside the model without directly modifying the model. Based on this idea, Trompt separates the learning strategy of tabular data into two parts. The first part, analogous to pre-trained models, focus on learning the intrinsic information of a table. The second part, analogous to prompts, focus on learning the variations among samples. Trompt is evaluated with the benchmark mentioned above. The ex
&lt;/p&gt;</description></item><item><title>Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18405</link><description>&lt;p&gt;
Dink-Net: &#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18405
&lt;/p&gt;
&lt;p&gt;
Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#24418;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#32452;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Dink-Net&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21306;&#20998;&#24102;&#22686;&#24378;&#30340;&#36319;&#19981;&#24102;&#22686;&#24378;&#30340;&#33410;&#28857;&#65292;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#24418;&#24335;&#12290;&#21516;&#26102;&#65292;&#23558;&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#26041;&#24335;&#26368;&#23567;&#21270;&#25552;&#20986;&#30340;&#38598;&#32676;&#33192;&#32960;&#25439;&#22833;&#21644;&#38598;&#32676;&#25910;&#32553;&#25439;&#22833;&#65292;&#20248;&#21270;&#32858;&#31867;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#20004;&#20010;&#27493;&#39588;&#32479;&#19968;&#20026;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24341;&#23548;&#32593;&#32476;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;Dink-Net&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#65292;&#22240;&#20026;&#35774;&#35745;&#30340;&#33192;&#32960;&#25910;&#32553;&#25805;&#20316;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Dink-Net&#22312;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#22270;&#24418;&#30340;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#22270;&#32858;&#31867;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23558;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#20316;&#20026;&#33258;&#36866;&#24212;&#20449;&#20219;&#21306;&#22495;&#65288;TR&#65289;&#31639;&#27861;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#27604;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#26356;&#24555;&#22320;&#36867;&#33073;&#38797;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.18188</link><description>&lt;p&gt;
&#23558;&#39044;&#27979;&#32534;&#30721;&#29702;&#35299;&#20026;&#33258;&#36866;&#24212;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Predictive Coding as an Adaptive Trust-Region Method. (arXiv:2305.18188v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18188
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23558;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#20316;&#20026;&#33258;&#36866;&#24212;&#20449;&#20219;&#21306;&#22495;&#65288;TR&#65289;&#31639;&#27861;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#27604;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#26356;&#24555;&#22320;&#36867;&#33073;&#38797;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#26159;&#19968;&#31181;&#31867;&#33041;&#26412;&#22320;&#23398;&#20064;&#31639;&#27861;&#65292;&#26368;&#36817;&#34987;&#25552;&#20986;&#22312;&#29983;&#29289;&#30456;&#20851;&#22330;&#26223;&#20013;&#25552;&#20379;&#27604;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#26356;&#22909;&#30340;&#20248;&#21183;&#12290;&#34429;&#28982;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23637;&#31034;PC&#22914;&#20309;&#22312;&#21508;&#31181;&#26497;&#38480;&#20013;&#36924;&#36817;BP&#65292;&#20294;&#8220;&#33258;&#28982;&#8221;&#30340;PC&#30340;&#28508;&#22312;&#20248;&#21183;&#23578;&#19981;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;PC&#24320;&#21457;&#20026;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#30340;&#33258;&#36866;&#24212;&#20449;&#20219;&#21306;&#22495;&#65288;TR&#65289;&#31639;&#27861;&#30340;&#29702;&#35770;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive coding (PC) is a brain-inspired local learning algorithm that has recently been suggested to provide advantages over backpropagation (BP) in biologically relevant scenarios. While theoretical work has mainly focused on showing how PC can approximate BP in various limits, the putative benefits of "natural" PC are less understood. Here we develop a theory of PC as an adaptive trust-region (TR) algorithm that uses second-order information. We show that the learning dynamics of PC can be interpreted as interpolating between BP's loss gradient direction and a TR direction found by the PC inference dynamics. Our theory suggests that PC should escape saddle points faster than BP, a prediction which we prove in a shallow linear model and support with experiments on deeper networks. This work lays a foundation for understanding PC in deep and wide networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.17537</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#22270;&#35760;&#24518;&#24314;&#27169;&#21160;&#24577;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#22914;&#23621;&#23460;&#31561;&#65292;&#23547;&#25214;&#29289;&#21697;&#30340;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#38656;&#35201;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#39044;&#27979;&#29289;&#21697;&#20301;&#32622;&#26469;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#38142;&#36335;&#39044;&#27979;&#38382;&#39064;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#22270;&#34920;&#36798;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;&#25151;&#38388;&#21644;&#29289;&#21697;&#26159;&#33410;&#28857;&#65292;&#22312;&#36793;&#32536;&#20013;&#32534;&#30721;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#65292;&#20195;&#29702;&#20154;&#20165;&#30693;&#36947;&#26356;&#25913;&#22270;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#23545;&#20110;&#29616;&#26377;&#30340;&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#34920;&#31034; - &#22330;&#26223;&#22270;&#35760;&#24518;&#65288;SGM&#65289; - &#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#20154;&#30340;&#32047;&#31215;&#35266;&#23519;&#38598;&#21512;&#65292;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20174;SGM&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#12290;&#25105;&#20204;&#22312;&#21160;&#24577;&#25151;&#23627;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23427;&#25353;&#29031;&#35821;&#20041;&#27169;&#24335;&#21019;&#24314;&#19981;&#21516;&#30340;&#21160;&#24577;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;</title><link>http://arxiv.org/abs/2305.17493</link><description>&lt;p&gt;
&#36882;&#24402;&#30340;&#35781;&#21650;&#65306;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#35753;&#27169;&#22411;&#24536;&#35760;
&lt;/p&gt;
&lt;p&gt;
The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17493
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#20174;&#25551;&#36848;&#24615;&#25991;&#26412;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;GPT-2&#12289;GPT-3(.5)&#21644;GPT-4&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#24778;&#20154;&#12290;ChatGPT&#23558;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#22823;&#20247;&#35270;&#37326;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#19981;&#21487;&#36991;&#20813;&#24182;&#23558;&#24443;&#24213;&#25913;&#21464;&#22312;&#32447;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20010;&#29983;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26410;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#20107;&#24773;&#12290;&#24403;LLMs&#21344;&#25454;&#20102;&#22312;&#32447;&#35821;&#35328;&#30340;&#22823;&#37096;&#20998;&#26102;&#65292;GPT-{n}&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20250;&#23548;&#33268;&#25152;&#24471;&#27169;&#22411;&#20013;&#19981;&#21487;&#36870;&#32570;&#38519;&#65292;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#21457;&#29983;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;LLMs&#20013;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#35937;&#32972;&#21518;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23601;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17401</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#29305;&#23450;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#21160;&#20174;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#25991;&#26723;&#20013;&#25552;&#21462;&#29305;&#23450;&#27169;&#24335;&#12290;&#24403;&#21069;&#30340;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#65288;RB&#65289;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#38656;&#35201;&#32534;&#20889;&#22797;&#26434;&#25490;&#29256;&#35770;&#25991;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23545;&#25991;&#31456;&#20013;&#22797;&#26434;&#20869;&#23481;&#31867;&#22411;&#36827;&#34892;&#27880;&#37322;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23481;&#26131;&#35782;&#21035;&#30340;&#27169;&#24335;&#34987;&#38169;&#35823;&#25552;&#21462;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#20998;&#26512;&#25351;&#23450;&#33879;&#20316;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24067;&#23616;&#21644;&#25490;&#29256;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.17026</link><description>&lt;p&gt;
&#35770;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#23545;&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Transformer&#27169;&#22411;&#30340;&#29702;&#35770;&#25991;&#29486;&#65292;&#24182;&#34920;&#26126;&#20165;&#20351;&#29992;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#30340;&#35299;&#30721;&#22120;Transformer&#32467;&#26500;&#65292;&#22312;&#21512;&#29702;&#20551;&#35774;&#19979;&#20855;&#22791;&#22270;&#28789;&#23436;&#22791;&#24615;&#12290;&#20174;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#22270;&#28789;&#23436;&#22791;&#24615;&#25104;&#31435;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HUB&#30340;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#20248;&#21270;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#20248;&#21270;&#22120;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16823</link><description>&lt;p&gt;
HUB: &#29992;&#25345;&#32493;&#25552;&#31034;&#35843;&#25972;&#24341;&#23548;&#23398;&#20064;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
HUB: Guiding Learned Optimizers with Continuous Prompt Tuning. (arXiv:2305.16823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HUB&#30340;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#20248;&#21270;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#20248;&#21270;&#22120;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22120;&#26159;&#20803;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20854;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#32593;&#32476;&#26550;&#26500;&#26102;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;HUB&#65289;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#20013;&#30828;&#25552;&#31034;&#35843;&#25972;&#21644;&#32467;&#26524;&#36873;&#25321;&#25216;&#26415;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#23558;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#20316;&#20026;&#25105;&#20204;&#28151;&#21512;&#26041;&#27861;&#30340;&#31532;&#20108;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#31283;&#23450;&#35757;&#32451;&#30340;&#21516;&#26102;&#20445;&#30041;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned optimizers are a crucial component of meta-learning. Recent advancements in scalable learned optimizers have demonstrated their superior performance over hand-designed optimizers in various tasks. However, certain characteristics of these models, such as an unstable learning curve, limited ability to handle unseen tasks and network architectures, difficult-to-control behaviours, and poor performance in fine-tuning tasks impede their widespread adoption. To tackle the issue of generalization in scalable learned optimizers, we propose a hybrid-update-based (HUB) optimization strategy inspired by recent advancements in hard prompt tuning and result selection techniques used in large language and vision models. This approach can be easily applied to any task that involves hand-designed or learned optimizer. By incorporating hand-designed optimizers as the second component in our hybrid approach, we are able to retain the benefits of learned optimizers while stabilizing the training
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.16653</link><description>&lt;p&gt;
AdaPlanner:&#33258;&#36866;&#24212;&#35268;&#21010;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#12290; &#65288;arXiv&#65306;2305.16653v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16653
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36138;&#23146;&#22320;&#37319;&#21462;&#34892;&#21160;&#32780;&#27809;&#26377;&#35745;&#21010;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19981;&#21487;&#36866;&#24212;&#29615;&#22659;&#21453;&#39304;&#30340;&#38745;&#24577;&#35745;&#21010;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#35745;&#21010;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;LLM&#20195;&#29702;&#30340;&#39034;&#24207;&#20915;&#31574;&#24615;&#33021;&#20250;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#26041;&#27861;AdaPlanner&#65292;&#23427;&#20801;&#35768;LLM&#20195;&#29702;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;&#22312;AdaPlanner&#20013;&#65292;LLM&#20195;&#29702;&#36890;&#36807;&#35745;&#21010;&#20869;&#21644;&#35745;&#21010;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#35745;&#21010;&#12290;&#20026;&#20102;&#20943;&#36731;&#24187;&#35273;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#65292;&#20419;&#36827;&#20102;&#36328;&#21508;&#31181;&#20219;&#21153;&#65292;&#29615;&#22659;&#21644;&#20195;&#29702;&#33021;&#21147;&#30340;&#35745;&#21010;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#65292;&#21033;&#29992;&#25104;&#21151;&#30340;&#35745;&#21010;&#20316;&#20026;&#23569;&#37327;&#31034;&#20363;&#65292;&#20351;&#35745;&#21010;&#26356;&#20855;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36870;&#21521;&#24037;&#31243;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#34920;&#31034;&#65292;&#21457;&#29616;SSL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#12290;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.15614</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reverse Engineering Self-Supervised Learning. (arXiv:2305.15614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36870;&#21521;&#24037;&#31243;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#34920;&#31034;&#65292;&#21457;&#29616;SSL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#12290;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#20294;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#21450;&#20854;&#22522;&#30784;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;SSL&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#21253;&#25324;&#22810;&#31181;&#27169;&#22411;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;SSL&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#20010;&#26377;&#36259;&#26041;&#38754;&#65306;&#23427;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#26159;&#30001;SSL&#30446;&#26631;&#30340;&#27491;&#21017;&#21270;&#39033;&#39537;&#21160;&#30340;&#12290;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#19981;&#20165;&#22686;&#24378;&#20102;&#19979;&#28216;&#20998;&#31867;&#65292;&#32780;&#19988;&#21387;&#32553;&#20102;&#25968;&#25454;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#31867;&#21035;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#34920;&#31034;&#19982;&#21508;&#31181;&#23618;&#27425;&#30340;&#35821;&#20041;&#31867;&#21035;&#23545;&#40784;&#65292;&#24182;&#19988;&#36825;&#31181;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provid
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20449;&#24687;&#34701;&#21512;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26102;&#24207;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#23553;&#35013;&#22120;&#65292;&#21487;&#29992;&#20110;&#23545;&#26102;&#24207;&#25968;&#25454;&#36827;&#34892;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21516;&#26102;&#36890;&#36807;&#20449;&#24687;&#34701;&#21512;&#21644;&#26102;&#24207;&#24863;&#30693;&#30340;&#36755;&#20837;&#36136;&#37327;&#29305;&#24449;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14872</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20449;&#24687;&#34701;&#21512;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26102;&#24207;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#23553;&#35013;&#22120;
&lt;/p&gt;
&lt;p&gt;
Timeseries-aware Uncertainty Wrappers for Uncertainty Quantification of Information-Fusion-Enhanced AI Models based on Machine Learning. (arXiv:2305.14872v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20449;&#24687;&#34701;&#21512;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26102;&#24207;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#23553;&#35013;&#22120;&#65292;&#21487;&#29992;&#20110;&#23545;&#26102;&#24207;&#25968;&#25454;&#36827;&#34892;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21516;&#26102;&#36890;&#36807;&#20449;&#24687;&#34701;&#21512;&#21644;&#26102;&#24207;&#24863;&#30693;&#30340;&#36755;&#20837;&#36136;&#37327;&#29305;&#24449;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#32452;&#20214;&#22312;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#31995;&#32479;&#26550;&#26500;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#24863;&#30693;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27169;&#22411;&#32467;&#26524;&#36890;&#24120;&#19981;&#36275;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24207;&#24863;&#30693;&#30340;&#19981;&#30830;&#23450;&#24615;&#23553;&#35013;&#22120;&#65292;&#29992;&#20110;&#23545;&#26102;&#24207;&#25968;&#25454;&#36827;&#34892;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19981;&#30830;&#23450;&#24615;&#23553;&#35013;&#22120;&#19982;&#27169;&#22411;&#30340;&#36830;&#32493;&#39044;&#27979;&#20449;&#24687;&#34701;&#21512;&#30456;&#32467;&#21512;&#12290;&#19981;&#30830;&#23450;&#24615;&#23553;&#35013;&#22120;&#30340;&#24212;&#29992;&#20197;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20026;&#20363;&#36827;&#34892;&#28436;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20449;&#24687;&#34701;&#21512;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#26102;&#24207;&#24863;&#30693;&#30340;&#36755;&#20837;&#36136;&#37327;&#29305;&#24449;&#36824;&#21487;&#20197;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of Artificial Intelligence (AI) components in cyber-physical systems is becoming more common, the need for reliable system architectures arises. While data-driven models excel at perception tasks, model outcomes are usually not dependable enough for safety-critical applications. In this work,we present a timeseries-aware uncertainty wrapper for dependable uncertainty estimates on timeseries data. The uncertainty wrapper is applied in combination with information fusion over successive model predictions in time. The application of the uncertainty wrapper is demonstrated with a traffic sign recognition use case. We show that it is possible to increase model accuracy through information fusion and additionally increase the quality of uncertainty estimates through timeseries-aware input quality features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#35821;&#38899;&#33258;&#30417;&#30563;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#26469;&#20998;&#26512;&#29983;&#29289;&#22768;&#23398;&#20449;&#21495;&#30340;&#20132;&#21449;&#21487;&#36801;&#31227;&#24615;&#65292;&#22312;&#29416;&#29492;&#21483;&#22768;&#35782;&#21035;&#21644;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#26410;&#26469;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.14035</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31070;&#32463;&#34920;&#31034;&#26159;&#21542;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#20154;&#31867;&#35821;&#38899;&#21518;&#21306;&#20998;&#21160;&#29289;&#21628;&#21483;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?. (arXiv:2305.14035v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#35821;&#38899;&#33258;&#30417;&#30563;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#26469;&#20998;&#26512;&#29983;&#29289;&#22768;&#23398;&#20449;&#21495;&#30340;&#20132;&#21449;&#21487;&#36801;&#31227;&#24615;&#65292;&#22312;&#29416;&#29492;&#21483;&#22768;&#35782;&#21035;&#21644;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#26410;&#26469;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20165;&#20351;&#29992;&#32473;&#23450;&#20449;&#21495;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#29420;&#31435;&#20110;&#22768;&#23398;&#39046;&#22495;&#65292;&#23558;&#20449;&#21495;&#36716;&#25442;&#20026;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#36825;&#24847;&#21619;&#30528;&#36825;&#20123;&#34920;&#31034;&#30340;&#25928;&#29992;&#19981;&#20165;&#23616;&#38480;&#20110;&#20154;&#31867;&#35821;&#38899;&#24314;&#27169;&#12290;&#26412;&#25991;&#22522;&#20110;&#27492;&#25506;&#35752;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#20154;&#31867;&#35821;&#38899;&#33258;&#30417;&#30563;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#26469;&#20998;&#26512;&#29983;&#29289;&#22768;&#23398;&#20449;&#21495;&#30340;&#20132;&#21449;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;11&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#23545;&#21483;&#22768;&#36827;&#34892;&#36776;&#21035;&#21644;&#26816;&#27979;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#23884;&#20837;&#31354;&#38388;&#21253;&#21547;&#26377;&#24847;&#20041;&#30340;&#21628;&#21483;&#32773;&#20449;&#24687;&#65292;&#21487;&#25104;&#21151;&#21306;&#20998;&#19981;&#21516;&#29416;&#29492;&#21483;&#30340;&#20010;&#20307;&#36523;&#20221;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#36825;&#34920;&#26126;&#22312;&#29983;&#29289;&#22768;&#23398;&#39046;&#22495;&#65292;&#39044;&#20808;&#35757;&#32451;&#20110;&#20154;&#31867;&#35821;&#38899;&#30340;&#34920;&#31034;&#33021;&#22815;&#34987;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) models use only the intrinsic structure of a given signal, independent of its acoustic domain, to extract essential information from the input to an embedding space. This implies that the utility of such representations is not limited to modeling human speech alone. Building on this understanding, this paper explores the cross-transferability of SSL neural representations learned from human speech to analyze bio-acoustic signals. We conduct a caller discrimination analysis and a caller detection study on Marmoset vocalizations using eleven SSL models pre-trained with various pretext tasks. The results show that the embedding spaces carry meaningful caller information and can successfully distinguish the individual identities of Marmoset callers without fine-tuning. This demonstrates that representations pre-trained on human speech can be effectively applied to the bio-acoustics domain, providing valuable insights for future investigations in this field.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;KGT5-context&#65292;&#36890;&#36807;&#21152;&#20837;&#26597;&#35810;&#23454;&#20307;&#30340;&#30452;&#25509;&#37051;&#23621;&#20449;&#24687;&#23454;&#29616;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#30340;&#39640;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13059</link><description>&lt;p&gt;
&#21451;&#22909;&#30340;&#37051;&#23621;&#65306;&#35821;&#22659;&#21270;&#24207;&#21015;&#21040;&#24207;&#21015;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction. (arXiv:2305.13059v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13059
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;KGT5-context&#65292;&#36890;&#36807;&#21152;&#20837;&#26597;&#35810;&#23454;&#20307;&#30340;&#30452;&#25509;&#37051;&#23621;&#20449;&#24687;&#23454;&#29616;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#30340;&#39640;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; KGT5-context&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#38142;&#25509;&#39044;&#27979;&#65288;LP&#65289;&#30340;&#31616;&#21333;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#26368;&#36817;&#30340;LP&#27169;&#22411;KGT5&#30340;&#22522;&#30784;&#19978;&#25299;&#23637;&#65292;KGT5&#21033;&#29992;&#20102;KG&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#27169;&#22411;&#35268;&#27169;&#23567;&#19988;&#21487;&#25193;&#23637;&#65292;&#20294;&#20026;&#20102;&#36798;&#21040;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;KGT5&#20381;&#36182;&#20110;&#19982;&#20043;&#37197;&#21512;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26412;&#36523;&#38750;&#24120;&#22823;&#19988;&#20351;&#29992;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#31687;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21152;&#20837;&#35821;&#22659;&#20449;&#24687;&#65292;&#21363;&#20851;&#20110;&#26597;&#35810;&#23454;&#20307;&#30340;&#30452;&#25509;&#37051;&#23621;&#20449;&#24687;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#29420;&#31435;KGE&#27169;&#22411;&#30340;&#38656;&#27714;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;KGT5-context&#27169;&#22411;&#31616;&#21333;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose KGT5-context, a simple sequence-to-sequence model for link prediction (LP) in knowledge graphs (KG). Our work expands on KGT5, a recent LP model that exploits textual features of the KG, has small model size, and is scalable. To reach good predictive performance, however, KGT5 relies on an ensemble with a knowledge graph embedding model, which itself is excessively large and costly to use. In this short paper, we show empirically that adding contextual information - i.e., information about the direct neighborhood of the query entity - alleviates the need for a separate KGE model to obtain good performance. The resulting KGT5-context model is simple, reduces model size significantly, and obtains state-of-the-art performance in our experimental study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388; &#33021;&#22815;&#39640;&#25928;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#8220;Web-Available Image Search (WAIS)&#8221;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.12102</link><description>&lt;p&gt;
&#32479;&#19968;&#23884;&#20837;&#65306;&#38754;&#21521; Web &#35268;&#27169; ML &#31995;&#32479;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems. (arXiv:2305.12102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388; &#33021;&#22815;&#39640;&#25928;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#8220;Web-Available Image Search (WAIS)&#8221;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#23545;&#20110; Web &#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#27599;&#20010;&#29305;&#24449;&#20540;&#34920;&#31034;&#20026;&#19968;&#20010; d &#32500;&#23884;&#20837;&#65292;&#24341;&#20837;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#65292;&#32780;&#36825;&#20123;&#29305;&#24449;&#30340;&#22522;&#25968;&#38750;&#24120;&#39640;&#12290;&#36825;&#20010;&#29942;&#39048;&#23548;&#33268;&#20102;&#22791;&#36873;&#23884;&#20837;&#31639;&#27861;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#65292;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#20043;&#38388;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#22797;&#29992;&#30340;&#23884;&#20837;&#21487;&#20197;&#20998;&#35299;&#20026;&#27599;&#20010;&#32452;&#25104;&#29305;&#24449;&#30340;&#32452;&#20214;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#29992;&#30340;&#23884;&#20837;&#22312;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Web-Available Image Search (WAIS)&#8221;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272; Web &#35268;&#27169;&#19979;&#30340;&#26032;&#23884;&#20837;&#31639;&#27861;&#12290;&#25105;&#20204;&#36992;&#35831;&#31038;&#21306;&#36890;&#36807;&#25552;&#20986;&#21487;&#20197;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#23558;&#25968;&#30334;&#19975;&#24352;&#22270;&#20687;&#23884;&#20837;&#21644;&#20998;&#31867;&#21040;&#25104;&#21315;&#19978;&#19975;&#20010;&#31867;&#21035;&#30340;&#26032;&#27169;&#22411;&#26469;&#36129;&#29486; WAIS &#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality feature embeddings efficiently and effectively is critical for the performance of web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on the order of millions to billions of tokens. The standard approach is to represent each feature value as a d-dimensional embedding, introducing hundreds of billions of parameters for extremely high-cardinality features. This bottleneck has led to substantial progress in alternative embedding algorithms. Many of these methods, however, make the assumption that each feature uses an independent embedding table. This work introduces a simple yet highly effective framework, Feature Multiplexing, where one single representation space is used across many different categorical features. Our theoretical and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each constituent feature, allowing models to distinguish between features. We show that multip
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32534;&#30721;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#65292;&#35774;&#35745;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20445;&#25345;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10631</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism. (arXiv:2305.10631v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32534;&#30721;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#65292;&#35774;&#35745;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20445;&#25345;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;U-Net&#22312;&#20998;&#21106;&#30452;&#32928;&#30284;&#27835;&#30103;&#26399;&#38388;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#26102;&#65292;&#30001;&#20110;&#22810;&#27425;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#23548;&#33268;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#38169;&#20301;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;MRI&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#35774;&#35745;&#20102;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#22312;&#32534;&#30721;&#20013;&#20351;&#29992;&#20102;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#12290;2&#65289;&#35774;&#35745;&#20102;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20445;&#25345;U-Net&#30340;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;&#23545;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#24635;&#20043;&#65292;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21487;&#20197;&#20943;&#23569;&#35821;&#20041;&#24046;&#36317;&#65292;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#20351;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to solve the semantic gap and misalignment issue between encoding and decoding because of multiple convolutional and pooling operations in U-Net when segmenting subabdominal MRI images during rectal cancer treatment. A MRI Image Segmentation is proposed based on a multi-scale feature pyramid network and dual attention mechanism. Our innovation is the design of two modules: 1) a dilated convolution and multi-scale feature pyramid network are used in the encoding to avoid the semantic gap. 2) a dual attention mechanism is designed to maintain spatial information of U-Net and reduce misalignment. Experiments on a subabdominal MRI image dataset show the proposed method achieves better performance than others methods. In conclusion, a multi-scale feature pyramid network can reduce the semantic gap, and the dual attention mechanism can make an alignment of features between encoding and decoding.
&lt;/p&gt;</description></item><item><title>ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10424</link><description>&lt;p&gt;
ZeroFlow: &#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#24555;&#36895;&#38646;&#26631;&#31614;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10424
&lt;/p&gt;
&lt;p&gt;
ZeroFlow&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#26041;&#27861;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#36827;&#34892;&#23454;&#26102;&#22330;&#26223;&#27969;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#20272;&#35745;&#26159;&#25551;&#36848;&#36830;&#32493;&#28857;&#20113;&#20043;&#38388;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#30340;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#38656;&#35201;&#25968;&#21313;&#31186;&#30340;&#26102;&#38388;&#65292;&#20351;&#20854;&#26080;&#27861;&#20316;&#20026;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65289;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20803;&#20351;&#29992;&#12290;&#21069;&#21521;&#20256;&#36882;&#26041;&#27861;&#30456;&#23545;&#24555;&#36895;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#28857;&#20113;&#30340;&#36816;&#34892;&#26102;&#38388;&#22312;&#25968;&#21313;&#33267;&#25968;&#30334;&#27627;&#31186;&#20043;&#38388;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33976;&#39311;&#26694;&#26550; Scene Flow via Distillation&#65292;&#20351;&#29992;&#26080;&#26631;&#31614;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#20197;&#30417;&#30563;&#21069;&#21521;&#20256;&#36882;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340; ZeroFlow&#65292;&#20351;&#29992;&#38646;&#20154;&#24037;&#26631;&#31614;&#65292;&#22312;&#22823;&#35268;&#27169;&#28857;&#20113;&#19978;&#23454;&#26102;&#29983;&#25104;&#22330;&#26223;&#27969;&#20272;&#35745;&#32467;&#26524;&#65292;&#21516;&#26102;&#36136;&#37327;&#31454;&#20105;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102; ZeroFlow
&lt;/p&gt;
&lt;p&gt;
Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07247</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#30340;Schr\"odinger bridge&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Schr\"odinger bridge&#38382;&#39064;&#65288;SBP&#65289;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#30340;&#25237;&#24433;&#26159;&#21807;&#19968;&#21487;&#29992;&#30340;&#65292;&#20854;&#25910;&#25947;&#24615;&#36824;&#19981;&#26159;&#21313;&#20998;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;SBP&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#23637;&#31034;&#20102;&#20248;&#21270;&#20256;&#36755;&#25104;&#26412;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Schr\"odinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schr\"odinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time ser
&lt;/p&gt;</description></item><item><title>ImageBind&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#21487;&#20197;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#32452;&#21512;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05665</link><description>&lt;p&gt;
ImageBind:&#19968;&#20010;&#20849;&#21516;&#23884;&#20837;&#31354;&#38388;&#32465;&#23450;&#25152;&#26377;&#27169;&#24577;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05665
&lt;/p&gt;
&lt;p&gt;
ImageBind&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#21487;&#20197;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#32452;&#21512;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ImageBind&#65292;&#36825;&#26159;&#19968;&#31181;&#36328;&#36234;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#28145;&#24230;&#12289;&#28909;&#20256;&#24863;&#21644;IMU&#25968;&#25454;&#30340;&#20845;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25152;&#26377;&#37197;&#23545;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#36275;&#20197;&#23558;&#36825;&#20123;&#27169;&#24577;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;ImageBind&#21487;&#20197;&#21033;&#29992;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23427;&#20204;&#19982;&#22270;&#20687;&#30340;&#33258;&#28982;&#37197;&#23545;&#65292;&#23558;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#26032;&#22411;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#29992;&#31639;&#26415;&#32452;&#21512;&#27169;&#24577;&#12289;&#36328;&#27169;&#24577;&#26816;&#27979;&#21644;&#29983;&#25104;&#12290;&#26032;&#22411;&#24212;&#29992;&#38543;&#30528;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24378;&#24230;&#32780;&#19981;&#26029;&#25913;&#36827;&#65292;&#25105;&#20204;&#22312;&#36328;&#27169;&#24577;&#30340;&#38646;&#26679;&#26412;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#25104;&#32489;&#65292;&#36229;&#36807;&#20102;&#19987;&#23478;&#30417;&#30563;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24378;&#30340;&#20960;&#20309;&#35782;&#21035;&#32467;&#26524;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;ImageBind&#25104;&#20026;&#20102;&#35780;&#20272;&#35270;&#35273;&#27169;&#24577;&#32852;&#21512;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.04111</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#24314;&#27169;&#19979;&#30340;&#39640;&#25928;&#21644;&#24230;&#25968;&#24341;&#23548;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23567;&#22270;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#29983;&#25104;&#21253;&#21547;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22270;&#24182;&#28385;&#36275;&#22270;&#32479;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#38543;&#26426;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#26368;&#32456;&#33719;&#24471;&#19968;&#24352;&#31354;&#30333;&#22270;&#12290;EDGE&#20165;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#12290;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#26356;&#23569;&#22320;&#36827;&#34892;&#36793;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;EDGE&#26126;&#30830;&#22320;&#20801;&#35768;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;&#23427;&#36824;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02995</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02995
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#23545;&#20110;&#21487;&#38752;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#35748;&#20026;&#35757;&#32451;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#21644;&#26032;&#25968;&#25454;&#22806;&#37096;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#36817;&#20046;&#23436;&#32654;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#26102;&#26399;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#65292;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#20026;&#24494;&#22937;&#65292;&#24182;&#19988;&#22312;&#19978;&#21319;&#38454;&#27573;&#23384;&#22312;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#65288;&#25243;&#29289;&#32447;&#19978;&#21319;&#26354;&#32447;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#65292;&#20108;&#26159;&#23558;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#31561;&#35270;&#35273;&#30740;&#31350;&#21551;&#21457;&#30340;&#25216;&#26415;&#24341;&#20837;&#21040;SWF&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02164</link><description>&lt;p&gt;
&#26080;&#21442;&#25968;&#26465;&#20214;&#21644;&#23616;&#37096;&#36830;&#25509;&#20999;&#29255;Wasserstein&#27969;&#37327;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows. (arXiv:2305.02164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#65292;&#20108;&#26159;&#23558;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#31561;&#35270;&#35273;&#30740;&#31350;&#21551;&#21457;&#30340;&#25216;&#26415;&#24341;&#20837;&#21040;SWF&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;Wasserstein&#27969;&#65288;SWF&#65289;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#29983;&#25104;&#24314;&#27169;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#21457;&#29983;&#29983;&#25104;&#36136;&#37327;&#30340;&#20122;&#20248;&#24615;&#21644;&#32570;&#20047;&#26465;&#20214;&#24314;&#27169;&#33021;&#21147;&#32780;&#26410;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#20570;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#19968;&#20010;&#24841;&#24742;&#30340;&#35266;&#23519;&#65288;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65289;&#65292;&#32852;&#21512;&#20998;&#24067;&#30340;SWF&#19982;&#26465;&#20214;&#20998;&#24067;&#30340;SWF&#30456;&#31526;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#20999;&#29255;Wasserstein&#27969;&#65288;CSWF&#65289;&#65292;&#36825;&#26159;SWF&#30340;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#21487;&#23454;&#29616;&#38750;&#21442;&#25968;&#26465;&#20214;&#24314;&#27169;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#24403;&#30340;&#22270;&#20687;&#24402;&#32435;&#20559;&#32622;&#21040;SWF&#20013;&#65292;&#29992;&#20004;&#20010;&#25216;&#26415;&#21463;&#21040;&#35270;&#35273;&#30740;&#31350;&#20013;&#30340;&#23616;&#37096;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22270;&#20687;&#24314;&#27169;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#36890;&#36807;&#20840;&#37096;&#25913;&#36827;&#65292;&#22312;&#36827;&#34892;&#32431;&#38750;&#21442;&#25968;&#24314;&#27169;&#30340;&#21516;&#26102;&#65292;&#22312;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#35768;&#22810;&#28145;&#24230;&#21442;&#25968;&#21270;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametric generative modeling but has not been widely adopted due to its suboptimal generative quality and lack of conditional modeling capabilities. In this work, we make two major contributions to bridging this gap. First, based on a pleasant observation that (under certain conditions) the SWF of joint distributions coincides with those of conditional distributions, we propose Conditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension of SWF that enables nonparametric conditional modeling. Second, we introduce appropriate inductive biases of images into SWF with two techniques inspired by local connectivity and multiscale representation in vision research, which greatly improve the efficiency and quality of modeling images. With all the improvements, we achieve generative performance comparable with many deep parametric generative models on both conditional and unconditional tasks in a purely nonparametric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Moccasin&#30340;&#26032;&#22411;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#12290;</title><link>http://arxiv.org/abs/2304.14463</link><description>&lt;p&gt;
Moccasin&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Moccasin: Efficient Tensor Rematerialization for Neural Networks. (arXiv:2304.14463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Moccasin&#30340;&#26032;&#22411;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#36739;&#20302;&#30340;&#20869;&#23384;&#26159;&#37096;&#32626;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26102;&#32463;&#24120;&#36935;&#21040;&#30340;&#26368;&#22823;&#38480;&#21046;&#22240;&#32032;&#20043;&#19968;&#12290;&#24352;&#37327;&#37325;&#31639;&#26159;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#29702;&#25152;&#38656;&#39640;&#20869;&#23384;&#38656;&#27714;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#31216;&#20026;Moccasin&#65292;&#20854;&#20013;&#21482;&#26377;$O(n)$&#20010;&#25972;&#25968;&#21464;&#37327;&#65292;$n$&#26159;&#35745;&#31639;&#22270;&#20013;&#33410;&#28857;&#30340;&#25968;&#37327;&#12290;&#36825;&#30456;&#23545;&#20110;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20855;&#26377;$O(n^2)$&#24067;&#23572;&#21464;&#37327;&#30340;&#20844;&#24335;&#25552;&#20986;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#20540;&#30740;&#31350;&#32467;&#26524;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#27604;&#26368;&#36817;&#30340;&#24037;&#20316;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment and training of neural networks on edge computing devices pose many challenges. The low memory nature of edge devices is often one of the biggest limiting factors encountered in the deployment of large neural network models. Tensor rematerialization or recompute is a way to address high memory requirements for neural network training and inference. In this paper we consider the problem of execution time minimization of compute graphs subject to a memory budget. In particular, we develop a new constraint programming formulation called \textsc{Moccasin} with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute graph. This is a significant improvement over the works in the recent literature that propose formulations with $O(n^2)$ Boolean variables. We present numerical studies that show that our approach is up to an order of magnitude faster than recent work especially for large-scale graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22788;&#29702;&#19981;&#23436;&#25972;&#35266;&#27979;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22238;&#24402;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#31614;&#20540;&#30001;&#20110;&#19981;&#23436;&#25972;&#35266;&#27979;&#23548;&#33268;&#23398;&#20064;&#32467;&#26524;&#20559;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13415</link><description>&lt;p&gt;
&#21253;&#21547;&#19981;&#23436;&#25972;&#35266;&#27979;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regression with Sensor Data Containing Incomplete Observations. (arXiv:2304.13415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22788;&#29702;&#19981;&#23436;&#25972;&#35266;&#27979;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22238;&#24402;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#31614;&#20540;&#30001;&#20110;&#19981;&#23436;&#25972;&#35266;&#27979;&#23548;&#33268;&#23398;&#20064;&#32467;&#26524;&#20559;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20986;&#26631;&#31614;&#20540;&#26159;&#24863;&#24212;&#29616;&#35937;&#24133;&#24230;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26631;&#31614;&#20540;&#20302;&#21487;&#33021;&#24847;&#21619;&#30528;&#29616;&#35937;&#30340;&#23454;&#38469;&#24133;&#24230;&#20302;&#25110;&#20256;&#24863;&#22120;&#20570;&#20986;&#20102;&#19981;&#23436;&#25972;&#30340;&#35266;&#27979;&#12290;&#36825;&#23548;&#33268;&#26631;&#31614;&#20540;&#20559;&#20302;&#65292;&#23398;&#20064;&#32467;&#26524;&#20063;&#20559;&#20302;&#65292;&#22240;&#20026;&#26631;&#31614;&#20540;&#21487;&#33021;&#30001;&#20110;&#19981;&#23436;&#25972;&#30340;&#35266;&#27979;&#32780;&#20559;&#20302;&#65292;&#21363;&#20351;&#29616;&#35937;&#30340;&#23454;&#38469;&#24133;&#24230;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#26174;&#24335;&#22320;&#27169;&#25311;&#20102;&#24102;&#26377;&#36127;&#20540;&#30340;&#19981;&#23545;&#31216;&#22122;&#22768;&#30340;&#19981;&#23436;&#25972;&#35266;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#26080;&#20559;&#30340;&#65292;&#23601;&#20687;&#20174;&#19981;&#21253;&#21547;&#19981;&#23436;&#25972;&#35266;&#27979;&#30340;&#26410;&#25439;&#22351;&#25968;&#25454;&#23398;&#20064;&#19968;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a regression problem in which output label values are the results of sensing the magnitude of a phenomenon. A low value of such labels can mean either that the actual magnitude of the phenomenon was low or that the sensor made an incomplete observation. This leads to a bias toward lower values in labels and its resultant learning because labels may have lower values due to incomplete observations, even if the actual magnitude of the phenomenon was high. Moreover, because an incomplete observation does not provide any tags indicating incompleteness, we cannot eliminate or impute them. To address this issue, we propose a learning algorithm that explicitly models incomplete observations corrupted with an asymmetric noise that always has a negative value. We show that our algorithm is unbiased as if it were learned from uncorrupted data that does not involve incomplete observations. We demonstrate the advantages of our algorithm through numerical experiments.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#23545;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33324;&#35206;&#30422;&#26465;&#20214;&#65292;&#24182;&#21457;&#29616;&#26356;&#22810;&#30340;&#35206;&#30422;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#34920;&#29616;&#65292;&#21516;&#26102;&#38416;&#26126;&#33391;&#22909;&#30340;&#35206;&#30422;&#26465;&#20214;&#20173;&#28982;&#26377;&#30410;&#20110;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.12886</link><description>&lt;p&gt;
&#38024;&#23545;&#20989;&#25968;&#36924;&#36817;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#33324;&#35206;&#30422;&#26465;&#20214;&#30340;&#21487;&#35777;&#26126;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Provable benefits of general coverage conditions in efficient online RL with function approximation. (arXiv:2304.12886v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12886
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#23545;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33324;&#35206;&#30422;&#26465;&#20214;&#65292;&#24182;&#21457;&#29616;&#26356;&#22810;&#30340;&#35206;&#30422;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#34920;&#29616;&#65292;&#21516;&#26102;&#38416;&#26126;&#33391;&#22909;&#30340;&#35206;&#30422;&#26465;&#20214;&#20173;&#28982;&#26377;&#30410;&#20110;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19982;&#20854;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#26631;&#20934;&#32467;&#26500;&#20551;&#35774;&#65292;&#20351;&#29992;&#26576;&#31181;&#35206;&#30422;&#26465;&#20214;&#65288;&#28304;&#33258;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65289;&#36275;&#20197;&#30830;&#20445;&#26679;&#26412;&#26377;&#25928;&#20445;&#35777;&#65288;Xie&#31561;&#20154;&#65292;2023&#65289;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#20010;&#26032;&#26041;&#21521;&#65292;&#25366;&#25496;&#26356;&#22810;&#21487;&#33021;&#21644;&#26356;&#26222;&#36941;&#30340;&#35206;&#30422;&#26465;&#20214;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#39640;&#25928;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#21644;&#29992;&#36884;&#12290;&#25105;&#20204;&#37492;&#23450;&#20102;&#26356;&#22810;&#27010;&#24565;&#65292;&#21253;&#25324;$L^p$&#21151;&#33021;&#38598;&#20013;&#24230;&#12289;&#23494;&#24230;&#27604;&#23454;&#29616;&#24615;&#20197;&#21450;&#37096;&#20998;/&#20840;&#35206;&#30422;&#26465;&#20214;&#30340;&#26435;&#34913;&#65292;&#36825;&#20123;&#27010;&#24565;&#20063;&#26377;&#30410;&#20110;&#23454;&#29616;&#26679;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#21033;&#29992;&#25506;&#32034;&#24615;&#30340;&#31163;&#32447;&#25968;&#25454;&#65292;&#22312;&#25105;&#20204;&#30340;&#35206;&#30422;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#20026;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#32479;&#35745;&#21644;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;MDP&#32467;&#26500;&#24050;&#32463;&#32473;&#20986;&#65292;&#20363;&#22914;&#32447;&#24615;MDP&#65292;&#25105;&#20204;&#20063;&#38416;&#26126;&#20102;&#33391;&#22909;&#30340;&#35206;&#30422;&#26465;&#20214;&#20173;&#28982;&#26377;&#30410;&#20110;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online reinforcement learning (RL), instead of employing standard structural assumptions on Markov decision processes (MDPs), using a certain coverage condition (original from offline RL) is enough to ensure sample-efficient guarantees (Xie et al. 2023). In this work, we focus on this new direction by digging more possible and general coverage conditions, and study the potential and the utility of them in efficient online RL. We identify more concepts, including the $L^p$ variant of concentrability, the density ratio realizability, and trade-off on the partial/rest coverage condition, that can be also beneficial to sample-efficient online RL, achieving improved regret bound. Furthermore, if exploratory offline data are used, under our coverage conditions, both statistically and computationally efficient guarantees can be achieved for online RL. Besides, even though the MDP structure is given, e.g., linear MDP, we elucidate that, good coverage conditions are still beneficial to obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#26816;&#27979;&#21644;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03916</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#26102;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#26816;&#27979;&#21644;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#23475;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#25110;&#23548;&#33268;&#27169;&#22411;&#22522;&#20110;&#38169;&#35823;&#21407;&#22240;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#26159;&#23454;&#38469;&#37096;&#32626;&#38754;&#20020;&#30340;&#20027;&#35201;&#40065;&#26834;&#24615;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#26399;&#38388;&#32531;&#35299;&#36825;&#20123;&#30456;&#20851;&#24615;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#20999;&#23454;&#38469;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#20154;&#26469;&#35828;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#26399;&#38388;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#38024;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#27169;&#24577;&#26469;&#26816;&#27979;&#24182;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#36890;&#36807;&#34920;&#36798;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;CLIP&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#21644;&#28145;&#20837;&#30340;&#21487;&#35270;&#21270;&#26174;&#31034;&#65292;&#36825;&#31181;&#20171;&#20837;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#65292;&#32780;&#19981;&#23384;&#22312;&#38169;&#35823;&#23646;&#24615;&#65292;&#24182;&#23558;&#27169;&#22411;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21160;&#21147;&#21644;&#36895;&#24230;&#26469;&#20026;&#33258;&#34892;&#36710;&#27604;&#36187;&#20013;&#30340;&#27599;&#20010;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.03778</link><description>&lt;p&gt;
&#38024;&#23545;Jumbo-Visma&#38431;&#21345;&#36335;&#37324;&#39044;&#27979;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Conformal Regression in Calorie Prediction for Team Jumbo-Visma. (arXiv:2304.03778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21160;&#21147;&#21644;&#36895;&#24230;&#26469;&#20026;&#33258;&#34892;&#36710;&#27604;&#36187;&#20013;&#30340;&#27599;&#20010;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
UCI WorldTour&#36187;&#20107;&#26159;&#30007;&#23376;&#31934;&#33521;&#20844;&#36335;&#33258;&#34892;&#36710;&#27604;&#36187;&#30340;&#39030;&#32423;&#36187;&#20107;&#65292;&#23545;&#39569;&#34892;&#32773;&#30340;&#20307;&#33021;&#21644;&#32784;&#21147;&#36827;&#34892;&#32771;&#39564;&#12290;Jumbo-Visma&#38431;&#30340;&#25945;&#32451;&#20204;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#36127;&#36131;&#39044;&#27979;&#27599;&#20010;&#27604;&#36187;&#26085;&#21382;&#20013;&#33655;&#20848;&#38431;&#30340;&#27599;&#20301;&#39569;&#25163;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#20197;&#30830;&#20445;&#39569;&#25163;&#22312;&#27604;&#36187;&#36807;&#31243;&#20013;&#26377;&#36275;&#22815;&#30340;&#33021;&#37327;&#21644;&#36164;&#28304;&#26469;&#20445;&#25345;&#39640;&#27700;&#24179;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26356;&#26377;&#25928;&#30340;&#39044;&#27979;&#39569;&#34892;&#27604;&#36187;&#33021;&#37327;&#38656;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#36895;&#24230;&#21644;&#21160;&#21147;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#30340;&#27599;&#20010;&#20010;&#20307;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
UCI WorldTour races, the premier men's elite road cycling tour, are grueling events that put riders' physical fitness and endurance to the test. The coaches of Team Jumbo-Visma have long been responsible for predicting the energy needs of each rider of the Dutch team for every race on the calendar. Those must be estimated to ensure riders have the energy and resources necessary to maintain a high level of performance throughout a race. This task, however, is both time-consuming and challenging, as it requires precise estimates of race speed and power output. Traditionally, the approach to predicting energy needs has relied on coaches' judgement and experience, but this method has its limitations and often leads to inaccurate predictions. In this paper, we propose a new, more effective approach to predicting energy needs for cycling races. By predicting the speed and power with regression models, we provide the coaches with calorie needs estimate for each individual rider per stage inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17649</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;
&lt;/p&gt;
&lt;p&gt;
Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21407;&#26412;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#30340;&#20013;&#31561;&#22823;&#23567;&#33521;&#25991;GPT&#27169;&#22411;&#65292;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#34987;&#31934;&#32454;&#35843;&#25972;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#35757;&#32451;&#21644;&#23454;&#29616;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#22870;&#21169;&#27169;&#22411;&#65289;&#65292;&#20197;&#35780;&#20998;&#24182;&#30830;&#23450;&#31572;&#26696;&#26159;&#21542;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#35813;&#32452;&#20214;&#26377;&#21161;&#20110;&#25913;&#36827;&#31995;&#32479;&#22238;&#31572;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#12290; BLEU&#21644;perplexity&#31561;&#25968;&#23383;&#24230;&#37327;&#26631;&#20934;&#34987;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20351;&#29992;&#20154;&#31867;&#21028;&#26029;&#26469;&#27604;&#36739;&#35299;&#30721;&#25216;&#26415;&#19982;&#20854;&#20182;&#25216;&#26415;&#12290;&#26368;&#32456;&#65292;&#32467;&#26524;&#25903;&#25345;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#29983;&#25104;&#22238;&#31572;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26435;&#37325;&#26497;&#24615;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#24605;&#36335;&#65306;&#21457;&#32946;&#36807;&#31243;&#20250;&#21021;&#22987;&#21270;&#26377;&#20248;&#21183;&#26497;&#24615;&#37197;&#32622;&#30340;&#33258;&#28982;&#26234;&#33021;&#65292;&#24403;&#33258;&#28982;&#26234;&#33021;&#25104;&#38271;&#21644;&#23398;&#20064;&#26102;&#65292;&#31361;&#35302;&#30340;&#22823;&#23567;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#26497;&#24615;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#65292;&#22914;&#26524;&#26435;&#37325;&#26497;&#24615;&#34987;&#36866;&#24403;&#22320;&#35774;&#32622;&#22312;&#20808;&#65292;&#37027;&#20040;&#32593;&#32476;&#23398;&#20064;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25968;&#25454;&#23558;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#22686;&#21152;&#23398;&#20064;&#21644;&#36716;&#31227;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17589</link><description>&lt;p&gt;
&#26497;&#24615;&#26159;&#24744;&#23398;&#20064;&#21644;&#24555;&#36895;&#20256;&#36882;&#25152;&#38656;&#30340;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
Polarity is all you need to learn and transfer faster. (arXiv:2303.17589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26435;&#37325;&#26497;&#24615;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#24605;&#36335;&#65306;&#21457;&#32946;&#36807;&#31243;&#20250;&#21021;&#22987;&#21270;&#26377;&#20248;&#21183;&#26497;&#24615;&#37197;&#32622;&#30340;&#33258;&#28982;&#26234;&#33021;&#65292;&#24403;&#33258;&#28982;&#26234;&#33021;&#25104;&#38271;&#21644;&#23398;&#20064;&#26102;&#65292;&#31361;&#35302;&#30340;&#22823;&#23567;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#26497;&#24615;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#65292;&#22914;&#26524;&#26435;&#37325;&#26497;&#24615;&#34987;&#36866;&#24403;&#22320;&#35774;&#32622;&#22312;&#20808;&#65292;&#37027;&#20040;&#32593;&#32476;&#23398;&#20064;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25968;&#25454;&#23558;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#22686;&#21152;&#23398;&#20064;&#21644;&#36716;&#31227;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#26234;&#33021;&#22312;&#21160;&#24577;&#19990;&#30028;&#20013;&#33537;&#22766;&#25104;&#38271;&#8212;&#8212;&#23427;&#20204;&#21487;&#20197;&#24456;&#24555;&#22320;&#23398;&#20064;&#65292;&#26377;&#26102;&#20165;&#38656;&#35201;&#23569;&#37327;&#26679;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#35745;&#31639;&#33021;&#21147;&#25165;&#33021;&#23398;&#20064;&#12290;&#20160;&#20040;&#35774;&#35745;&#21407;&#21017;&#20351;&#24471;&#33258;&#28982;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#23384;&#22312;&#22914;&#27492;&#26126;&#26174;&#30340;&#24046;&#24322;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20174;&#26435;&#37325;&#26497;&#24615;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#24605;&#36335;&#65306;&#21457;&#32946;&#36807;&#31243;&#20250;&#21021;&#22987;&#21270;&#26377;&#20248;&#21183;&#26497;&#24615;&#37197;&#32622;&#30340;&#33258;&#28982;&#26234;&#33021;&#65292;&#24403;&#33258;&#28982;&#26234;&#33021;&#25104;&#38271;&#21644;&#23398;&#20064;&#26102;&#65292;&#31361;&#35302;&#30340;&#22823;&#23567;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#26497;&#24615;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#26435;&#37325;&#26497;&#24615;&#34987;&#36866;&#24403;&#22320;&#35774;&#32622;&#22312;&#20808;&#65292;&#37027;&#20040;&#32593;&#32476;&#23398;&#20064;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25968;&#25454;&#23558;&#20250;&#20943;&#23569;&#12290;&#25105;&#20204;&#36824;&#26126;&#30830;&#35828;&#26126;&#20102;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20808;&#39564;&#35774;&#32622;&#26435;&#37325;&#26497;&#24615;&#20250;&#23545;&#32593;&#32476;&#20135;&#29983;&#19981;&#21033;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20174;&#23398;&#20064;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35282;&#24230;&#38416;&#36848;&#20102;&#26435;&#37325;&#26497;&#24615;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural intelligences (NIs) thrive in a dynamic world - they learn quickly, sometimes with only a few samples. In contrast, Artificial intelligences (AIs) typically learn with prohibitive amount of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we propose an angle from weight polarity: development processes initialize NIs with advantageous polarity configurations; as NIs grow and learn, synapse magnitudes update yet polarities are largely kept unchanged. We demonstrate with simulation and image classification tasks that if weight polarities are adequately set $\textit{a priori}$, then networks learn with less time and data. We also explicitly illustrate situations in which $\textit{a priori}$ setting the weight polarities is disadvantageous for networks. Our work illustrates the value of weight polarities from the perspective of statistical and computational efficiency during learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#23398;&#20064;&#38750;&#25311;&#21512;&#20998;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20854;&#26435;&#37325;&#20998;&#37197;&#26356;&#22810;&#30340;&#37325;&#35201;&#24615;&#32473;&#27531;&#24046;&#19982;&#27979;&#35797;&#28857;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31526;&#21512;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.12695</link><description>&lt;p&gt;
&#38750;&#25311;&#21512;&#20998;&#25968;&#37325;&#26032;&#26435;&#37325;&#23454;&#29616;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Conformal Prediction by Reweighting Nonconformity Score. (arXiv:2303.12695v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#23398;&#20064;&#38750;&#25311;&#21512;&#20998;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20854;&#26435;&#37325;&#20998;&#37197;&#26356;&#22810;&#30340;&#37325;&#35201;&#24615;&#32473;&#27531;&#24046;&#19982;&#27979;&#35797;&#28857;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31526;&#21512;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#25104;&#21151;&#65292;&#20294;&#30001;&#19968;&#33268;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#32473;&#20986;&#30340;&#39044;&#27979;&#21306;&#38388;&#65288;PI&#65289;&#21487;&#33021;&#26080;&#27861;&#21453;&#26144;&#32473;&#23450;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110;CP&#26041;&#27861;&#23545;&#25152;&#26377;&#27979;&#35797;&#28857;&#20351;&#29992;&#24120;&#25968;&#20462;&#27491;&#65292;&#26080;&#35270;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#30830;&#20445;&#35206;&#30422;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#65288;QRF&#65289;&#26469;&#23398;&#20064;&#38750;&#25311;&#21512;&#20998;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;QRF&#30340;&#26435;&#37325;&#23558;&#26356;&#22810;&#30340;&#37325;&#35201;&#24615;&#20998;&#37197;&#32473;&#27531;&#24046;&#19982;&#27979;&#35797;&#28857;&#30456;&#20284;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#30340;PI&#38271;&#24230;&#26356;&#31526;&#21512;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;QRF&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#25552;&#20379;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#21010;&#20998;&#65292;&#36890;&#36807;&#32452;&#21512;&#19968;&#33268;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25913;&#36827;PI&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20139;&#26377;&#22522;&#20110;&#26679;&#26412;&#21644;&#22522;&#20110;&#35757;&#32451;&#26465;&#20214;&#30340;&#26080;&#20551;&#35774;&#26377;&#38480;&#35206;&#30422;&#29575;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#20063;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Despite attractive theoretical guarantees and practical successes, Predictive Interval (PI) given by Conformal Prediction (CP) may not reflect the uncertainty of a given model. This limitation arises from CP methods using a constant correction for all test points, disregarding their individual uncertainties, to ensure coverage properties. To address this issue, we propose using a Quantile Regression Forest (QRF) to learn the distribution of nonconformity scores and utilizing the QRF's weights to assign more importance to samples with residuals similar to the test point. This approach results in PI lengths that are more aligned with the model's uncertainty. In addition, the weights learnt by the QRF provide a partition of the features space, allowing for more efficient computations and improved adaptiveness of the PI through groupwise conformalization. Our approach enjoys an assumption-free finite sample marginal and training-conditional coverage, and under suitable assumptions, it also
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21333;&#20301;&#32553;&#25918;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#31616;&#21270;&#20302;&#31934;&#24230;&#25968;&#23383;&#26684;&#24335;&#30340;&#25805;&#20316;&#24182;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#21021;&#22987;&#21270;&#26102;&#23558;&#25152;&#26377;&#26435;&#37325;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#26799;&#24230;&#30340;&#21333;&#20301;&#24046;&#21464;&#20026;1&#26469;&#35299;&#20915;&#20302;&#31934;&#24230;&#35757;&#32451;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#20063;&#27809;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2303.11257</link><description>&lt;p&gt;
&#21333;&#20301;&#32553;&#25918;&#65306;&#24320;&#31665;&#21363;&#29992;&#30340;&#20302;&#31934;&#24230;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unit Scaling: Out-of-the-Box Low-Precision Training. (arXiv:2303.11257v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21333;&#20301;&#32553;&#25918;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#31616;&#21270;&#20302;&#31934;&#24230;&#25968;&#23383;&#26684;&#24335;&#30340;&#25805;&#20316;&#24182;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#21021;&#22987;&#21270;&#26102;&#23558;&#25152;&#26377;&#26435;&#37325;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#26799;&#24230;&#30340;&#21333;&#20301;&#24046;&#21464;&#20026;1&#26469;&#35299;&#20915;&#20302;&#31934;&#24230;&#35757;&#32451;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#20063;&#27809;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21333;&#20301;&#32553;&#25918;&#8221;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#23383;&#26684;&#24335;&#30340;&#25805;&#20316;&#12290;&#22312;FP16&#25110;FP8&#26684;&#24335;&#19979;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#30340;&#33539;&#22260;&#36827;&#34892;&#35757;&#32451;&#12290;&#21333;&#20301;&#32553;&#25918;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#22411;&#25968;&#23383;&#30340;&#21407;&#21017;&#26041;&#27861;&#65292;&#23547;&#27714;&#22312;&#21021;&#22987;&#21270;&#26102;&#25152;&#26377;&#26435;&#37325;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#26799;&#24230;&#30340;&#21333;&#20301;&#26041;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#19981;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#27604;&#20363;&#65292;&#20063;&#27809;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#20301;&#32553;&#25918;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20248;&#21270;&#22120;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21487;&#20197;&#34987;&#35843;&#25972;&#20026;&#21333;&#20301;&#32553;&#25918;&#65292;&#20363;&#22914;&#20351;&#29992;FP16&#23545;BERT-Large&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;FP8&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present unit scaling, a paradigm for designing deep learning models that simplifies the use of low-precision number formats. Training in FP16 or the recently proposed FP8 formats offers substantial efficiency gains, but can lack sufficient range for out-of-the-box training. Unit scaling addresses this by introducing a principled approach to model numerics: seeking unit variance of all weights, activations and gradients at initialisation. Unlike alternative methods, this approach neither requires multiple training runs to find a suitable scale nor has significant computational overhead. We demonstrate the efficacy of unit scaling across a range of models and optimisers. We further show that existing models can be adapted to be unit-scaled, training BERT-Large in FP16 and then FP8 with no degradation in accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#22270;&#30340;&#35282;&#24230;&#23457;&#26597;&#20102;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#25552;&#31034;&#20989;&#25968;&#19982;&#22270;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#35774;&#35745;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#32452;&#32455;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#25551;&#36848;&#20102;&#24212;&#29992;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.07275</link><description>&lt;p&gt;
&#22270;&#24418;&#25552;&#31034;&#26041;&#27861;&#32508;&#36848;&#65306;&#25216;&#26415;&#65292;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges. (arXiv:2303.07275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22270;&#30340;&#35282;&#24230;&#23457;&#26597;&#20102;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#25552;&#31034;&#20989;&#25968;&#19982;&#22270;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#35774;&#35745;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#32452;&#32455;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#25551;&#36848;&#20102;&#24212;&#29992;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#65292;&#39044;&#27979;&#35757;&#32451;&#8221;&#33539;&#20363;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#26222;&#36866;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#19968;&#20010;&#25552;&#31034;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#27169;&#26495;&#24212;&#29992;&#20110;&#36755;&#20837;&#26679;&#26412;&#65292;&#28155;&#21152;&#25351;&#31034;&#32972;&#26223;&#24182;&#23558;&#30446;&#26631;&#20219;&#21153;&#37325;&#26500;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#35774;&#35745;&#25552;&#31034;&#21487;&#33021;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35299;&#20915;&#65292;&#22240;&#20026;&#22270;&#24418;&#20316;&#20026;&#26174;&#24335;&#22320;&#24314;&#27169;&#23454;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#12290;&#22312;&#36825;&#20010;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;&#22270;&#30340;&#35282;&#24230;&#23457;&#26597;&#25552;&#31034;&#26041;&#27861;&#65292;&#20854;&#20013;&#25552;&#31034;&#20989;&#25968;&#20351;&#29992;&#22270;&#30693;&#35782;&#36827;&#34892;&#25193;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22270;&#24418;&#25552;&#31034;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#32452;&#32455;&#20102;&#35774;&#35745;&#22270;&#24418;&#25552;&#31034;&#20989;&#25968;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#23558;&#24357;&#21512;&#22270;&#24418;&#21644;&#25552;&#31034;&#35774;&#35745;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent "pre-train, prompt, predict training" paradigm has gained popularity as a way to learn generalizable models with limited labeled data. The approach involves using a pre-trained model and a prompting function that applies a template to input samples, adding indicative context and reformulating target tasks as the pre-training task. However, the design of prompts could be a challenging and time-consuming process in complex tasks. The limitation can be addressed by using graph data, as graphs serve as structured knowledge repositories by explicitly modeling the interaction between entities. In this survey, we review prompting methods from the graph perspective, where prompting functions are augmented with graph knowledge. In particular, we introduce the basic concepts of graph prompt learning, organize the existing work of designing graph prompting functions, and describe their applications and future challenges. This survey will bridge the gap between graphs and prompt design 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#39640;&#36136;&#37327;ImageNet&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#21442;&#25968;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#25552;&#39640;&#22810;&#31181;ImageNet&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#26102;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#36798;&#21040;&#31454;&#20105;&#21147;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04143</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;Transformer&#24212;&#29992;&#21040;&#22810;&#31181;ImageNet&#27169;&#22411;&#30340;&#21442;&#25968;&#39044;&#27979;&#20013;&#36827;&#34892;&#25193;&#23637;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?. (arXiv:2303.04143v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#39640;&#36136;&#37327;ImageNet&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#21442;&#25968;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#25552;&#39640;&#22810;&#31181;ImageNet&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#26102;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#36798;&#21040;&#31454;&#20105;&#21147;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#36825;&#21482;&#33021;&#30001;&#19968;&#20123;&#25317;&#26377;&#20805;&#36275;&#36164;&#28304;&#30340;&#31038;&#21306;&#23454;&#29616;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#19968;&#20010;&#38596;&#24515;&#21187;&#21187;&#30340;&#30446;&#26631;&#65306;&#27665;&#20027;&#21270;&#39044;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#36136;&#37327;ImageNet&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#21442;&#25968;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;PyTorch&#20013;&#21487;&#29992;&#30340;&#21508;&#31181;ImageNet&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#26102;&#65292;&#20351;&#29992;&#39044;&#27979;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20063;&#20250;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#36798;&#21040;&#31454;&#20105;&#21147;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. We aim at an ambitious goal of democratizing pretraining. Towards that goal, we train and release a single neural network that can predict high quality ImageNet parameters of other neural networks. By using predicted parameters for initialization we are able to boost training of diverse ImageNet models available in PyTorch. When transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22810;&#20998;&#36776;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#36882;&#24402;&#22320;&#29983;&#25104;&#22810;&#20010;&#23618;&#27425;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#24182;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#30001;&#31895;&#21040;&#32454;&#22320;&#29983;&#25104;&#22270;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03293</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#23618;&#22810;&#20998;&#36776;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Hierarchical Multi-Resolution Graph Generative Models. (arXiv:2303.03293v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22810;&#20998;&#36776;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#36882;&#24402;&#22320;&#29983;&#25104;&#22810;&#20010;&#23618;&#27425;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#24182;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#30001;&#31895;&#21040;&#32454;&#22320;&#29983;&#25104;&#22270;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22823;&#37096;&#20998;&#30340;&#22270;&#37117;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#29983;&#25104;&#20173;&#28982;&#27809;&#26377;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20197;&#22810;&#20010;&#20998;&#36776;&#29575;&#36882;&#24402;&#22320;&#29983;&#25104;&#31038;&#21306;&#32467;&#26500;&#65292;&#29983;&#25104;&#30340;&#32467;&#26500;&#22312;&#27599;&#20010;&#23618;&#27425;&#32467;&#26500;&#19978;&#65292;&#37117;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#22270;&#30340;&#29983;&#25104;&#34987;&#35774;&#35745;&#20026;&#19968;&#31995;&#21015;&#30001;&#31895;&#21040;&#32454;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20801;&#35768;&#25152;&#26377;&#23376;&#32467;&#26500;&#30340;&#24182;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22270;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#29983;&#25104;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real world domains, most graphs naturally exhibit a hierarchical structure. However, data-driven graph generation is yet to effectively capture such structures. To address this, we propose a novel approach that recursively generates community structures at multiple resolutions, with the generated structures conforming to training data distribution at each level of the hierarchy. The graphs generation is designed as a sequence of coarse-to-fine generative models allowing for parallel generation of all sub-structures, resulting in a high degree of scalability. Our method demonstrates generative performance improvement on multiple graph datasets.
&lt;/p&gt;</description></item><item><title>DeepMAD&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#23398;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#20013;&#35774;&#35745;&#20986;&#39640;&#24615;&#33021;&#30340;CNN&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;CNN&#32593;&#32476;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02165</link><description>&lt;p&gt;
DeepMAD: &#22522;&#20110;&#25968;&#23398;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network. (arXiv:2303.02165v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02165
&lt;/p&gt;
&lt;p&gt;
DeepMAD&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#23398;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#20013;&#35774;&#35745;&#20986;&#39640;&#24615;&#33021;&#30340;CNN&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;CNN&#32593;&#32476;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#20854;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#21047;&#26032;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;, &#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20123;&#26368;&#36817;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#21453;&#20987;&#24615;&#30740;&#31350;&#65292;&#34920;&#26126;&#24403;&#20180;&#32454;&#35843;&#25972;&#26102;&#65292;&#32431;CNN&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;ViT&#27169;&#22411;&#19968;&#26679;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#39640;&#24615;&#33021;&#30340;CNN&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#38656;&#35201;&#23545;&#32593;&#32476;&#35774;&#35745;&#20855;&#26377;&#38750;&#24179;&#20961;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#26550;&#26500;&#35774;&#35745;&#65288;DeepMAD&#65289;, &#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#35774;&#35745;&#39640;&#24615;&#33021;&#30340;CNN&#27169;&#22411;&#12290;&#22312;DeepMAD&#20013;&#65292;CNN&#32593;&#32476;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#65292;&#20854;&#34920;&#29616;&#21147;&#21644;&#25928;&#26524;&#21487;&#20197;&#36890;&#36807;&#32467;&#26500;&#21442;&#25968;&#36827;&#34892;&#20998;&#26512;&#21644;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32422;&#26463;&#30340;&#25968;&#23398;&#35268;&#21010;&#65288;MP&#65289;&#38382;&#39064;&#26469;&#20248;&#21270;&#36825;&#20123;&#32467;&#26500;&#21442;&#25968;&#12290;MP&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#29616;&#25104;&#30340;MP&#27714;&#35299;&#22120;&#22312;CPU&#19978;&#36731;&#26494;&#35299;&#20915;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#35760;&#24518;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advances in Vision Transformer (ViT) refresh the state-of-the-art performances in various vision tasks, overshadowing the conventional CNN-based models. This ignites a few recent striking-back research in the CNN world showing that pure CNN models can achieve as good performance as ViT models when carefully tuned. While encouraging, designing such high-performance CNN models is challenging, requiring non-trivial prior knowledge of network design. To this end, a novel framework termed Mathematical Architecture Design for Deep CNN (DeepMAD) is proposed to design high-performance CNN models in a principled way. In DeepMAD, a CNN network is modeled as an information processing system whose expressiveness and effectiveness can be analytically formulated by their structural parameters. Then a constrained mathematical programming (MP) problem is proposed to optimize these structural parameters. The MP problem can be easily solved by off-the-shelf MP solvers on CPUs with a small memo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;dropout&#19981;&#20165;&#21487;&#20197;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#65292;&#36824;&#21487;&#20197;&#32531;&#35299;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#21021;&#26399;&#37319;&#29992;early dropout&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#23567;&#25209;&#27425;&#26799;&#24230;&#30340;&#26041;&#21521;&#24046;&#24322;&#65292;&#32531;&#35299;SGD&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.01500</link><description>&lt;p&gt;
&#12298;Dropout Reduces Underfitting&#12299;
&lt;/p&gt;
&lt;p&gt;
Dropout Reduces Underfitting. (arXiv:2303.01500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;dropout&#19981;&#20165;&#21487;&#20197;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#65292;&#36824;&#21487;&#20197;&#32531;&#35299;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#21021;&#26399;&#37319;&#29992;early dropout&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#23567;&#25209;&#27425;&#26799;&#24230;&#30340;&#26041;&#21521;&#24046;&#24322;&#65292;&#32531;&#35299;SGD&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;Dropout Reduces Underfitting&#12299;&#26159;&#19968;&#31687;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;Dropout&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;Dropout&#21487;&#20197;&#22312;&#35757;&#32451;&#21021;&#26399;&#38450;&#27490;&#27424;&#25311;&#21512;&#30340;&#25928;&#26524;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#21457;&#29616;Dropout&#21487;&#20197;&#20943;&#23569;&#23567;&#25209;&#27425;&#26799;&#24230;&#30340;&#26041;&#21521;&#24046;&#24322;&#65292;&#24182;&#26377;&#21161;&#20110;&#23558;&#23567;&#25209;&#27425;&#26799;&#24230;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#26799;&#24230;&#23545;&#40784;&#65292;&#20174;&#32780;&#32531;&#35299;SGD&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20943;&#23569;&#27599;&#20010;&#25209;&#27425;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#27424;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#26696;&#8212;&#8212;early dropout: &#22312;&#35757;&#32451;&#30340;&#21021;&#22987;&#38454;&#27573;&#24212;&#29992;dropout&#65292;&#24182;&#22312;&#35757;&#32451;&#21518;&#20851;&#38381;dropout&#12290;&#30456;&#27604;&#20110;&#27809;&#26377;dropout&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;early dropout&#30340;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#20302;&#30340;&#26368;&#32456;&#35757;&#32451;&#25439;&#22833;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#36807;&#25311;&#21512;&#27169;&#22411;&#30340;&#23545;&#31216;&#25216;&#26415;&#8212;&#8212;late dropout&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#19968;&#27493;&#29983;&#25104;&#19988;&#25903;&#25345;&#38646;&#26679;&#26412;&#32534;&#36753;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#23558;&#22122;&#22768;&#26144;&#23556;&#21040;&#25968;&#25454;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#25903;&#25345;&#24555;&#36895;&#30340;&#19968;&#27493;&#29983;&#25104;&#65292;&#19988;&#20173;&#28982;&#25903;&#25345;&#22810;&#27493;&#25277;&#26679;&#20197;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.01469</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#27493;&#29983;&#25104;&#19988;&#25903;&#25345;&#38646;&#26679;&#26412;&#32534;&#36753;&#8212;&#8212;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Consistency Models. (arXiv:2303.01469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#19968;&#27493;&#29983;&#25104;&#19988;&#25903;&#25345;&#38646;&#26679;&#26412;&#32534;&#36753;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#23558;&#22122;&#22768;&#26144;&#23556;&#21040;&#25968;&#25454;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#25903;&#25345;&#24555;&#36895;&#30340;&#19968;&#27493;&#29983;&#25104;&#65292;&#19988;&#20173;&#28982;&#25903;&#25345;&#22810;&#27493;&#25277;&#26679;&#20197;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36845;&#20195;&#25277;&#26679;&#36807;&#31243;&#65292;&#23548;&#33268;&#29983;&#25104;&#36895;&#24230;&#32531;&#24930;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#26063;&#36890;&#36807;&#30452;&#25509;&#23558;&#22122;&#22768;&#26144;&#23556;&#21040;&#25968;&#25454;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#26032;&#27169;&#22411;&#12290;&#23427;&#20204;&#36890;&#36807;&#35774;&#35745;&#25903;&#25345;&#24555;&#36895;&#30340;&#19968;&#27493;&#29983;&#25104;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#22810;&#27493;&#25277;&#26679;&#26469;&#20197;&#35745;&#31639;&#25442;&#21462;&#26679;&#26412;&#36136;&#37327;&#12290;&#23427;&#20204;&#36824;&#25903;&#25345;&#38646;&#26679;&#26412;&#25968;&#25454;&#32534;&#36753;&#65292;&#22914;&#22270;&#20687;&#20462;&#22797;&#12289;&#19978;&#33394;&#21644;&#36229;&#20998;&#36776;&#29575;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#35757;&#32451;&#36825;&#20123;&#20219;&#21153;&#12290;&#19968;&#33268;&#24615;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33976;&#39311;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#35757;&#32451;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#29420;&#31435;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;DNN&#30340;&#20056;&#27861;&#36974;&#32617;&#25193;&#23637;&#21040;&#28145;&#24230;&#23376;&#24102;&#28388;&#27874;&#20197;&#22312;&#26102;&#39057;&#22495;&#20013;&#23454;&#29616;&#35821;&#38899;&#24674;&#22797;&#30340;&#26041;&#26696;&#65292;&#21487;&#36890;&#29992;&#20110;&#25552;&#20379;&#22312;&#26102;&#39057;&#22495;&#20013;&#30340;&#36974;&#32617;&#30340;&#20219;&#20309;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#20056;&#27861;&#36974;&#32617;&#21435;&#28151;&#21709;&#24615;&#33021;&#26356;&#22909;&#65292;&#22312;&#21435;&#22122;&#24615;&#33021;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.00529</link><description>&lt;p&gt;
&#23558;&#22522;&#20110;DNN&#30340;&#20056;&#27861;&#36974;&#32617;&#25193;&#23637;&#21040;&#28145;&#24230;&#23376;&#24102;&#28388;&#27874;&#20197;&#25913;&#21892;&#21435;&#28151;&#21709;
&lt;/p&gt;
&lt;p&gt;
Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation. (arXiv:2303.00529v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00529
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;DNN&#30340;&#20056;&#27861;&#36974;&#32617;&#25193;&#23637;&#21040;&#28145;&#24230;&#23376;&#24102;&#28388;&#27874;&#20197;&#22312;&#26102;&#39057;&#22495;&#20013;&#23454;&#29616;&#35821;&#38899;&#24674;&#22797;&#30340;&#26041;&#26696;&#65292;&#21487;&#36890;&#29992;&#20110;&#25552;&#20379;&#22312;&#26102;&#39057;&#22495;&#20013;&#30340;&#36974;&#32617;&#30340;&#20219;&#20309;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#20056;&#27861;&#36974;&#32617;&#21435;&#28151;&#21709;&#24615;&#33021;&#26356;&#22909;&#65292;&#22312;&#21435;&#22122;&#24615;&#33021;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20056;&#27861;&#36974;&#32617;&#25193;&#23637;&#21040;&#28145;&#24230;&#23376;&#24102;&#28388;&#27874;&#20013;&#65292;&#20197;&#22312;&#26102;&#39057;&#22495;&#20013;&#23454;&#29616;&#35821;&#38899;&#24674;&#22797;&#12290;&#32467;&#26524;&#26041;&#27861;&#21487;&#36890;&#29992;&#20110;&#25552;&#20379;&#22312;&#26102;&#39057;&#22495;&#20013;&#30340;&#36974;&#32617;&#30340;&#20219;&#20309;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21482;&#38656;&#35201;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#32467;&#26524;&#28145;&#24230;&#23376;&#24102;&#28388;&#27874;&#26041;&#26696;&#20248;&#20110;&#20056;&#27861;&#36974;&#32617;&#21435;&#28151;&#21709;&#65292;&#32780;&#20960;&#20046;&#19981;&#24433;&#21709;&#21435;&#22122;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#22240;&#20026;&#26102;&#39057;&#22495;&#20013;&#30340;&#28145;&#24230;&#23376;&#24102;&#28388;&#27874;&#36866;&#29992;&#20110;&#28151;&#21709;&#28040;&#38500;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#23376;&#24102;&#36817;&#20284;&#65292;&#32780;&#20056;&#27861;&#36974;&#32617;&#21017;&#23545;&#20110;&#19968;&#33324;&#29992;&#20110;&#21435;&#22122;&#30340;&#31364;&#24102;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a scheme for extending deep neural network-based multiplicative maskers to deep subband filters for speech restoration in the time-frequency domain. The resulting method can be generically applied to any deep neural network providing masks in the time-frequency domain, while requiring only few more trainable parameters and a computational overhead that is negligible for state-of-the-art neural networks. We demonstrate that the resulting deep subband filtering scheme outperforms multiplicative masking for dereverberation, while leaving the denoising performance virtually the same. We argue that this is because deep subband filtering in the time-frequency domain fits the subband approximation often assumed in the dereverberation literature, whereas multiplicative masking corresponds to the narrowband approximation generally employed for denoising.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#36816;&#31639;&#31526;Transformer&#8212;&#8212;GNOT&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24322;&#26500;&#24402;&#19968;&#21270;&#27880;&#24847;&#21147;&#23618;&#21644;&#24341;&#20837;&#20960;&#20309;&#38376;&#25511;&#26426;&#21046;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#35299;&#20915;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.14376</link><description>&lt;p&gt;
GNOT: &#19968;&#31181;&#29992;&#20110;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#36890;&#29992;&#31070;&#32463;&#36816;&#31639;&#31526;Transformer
&lt;/p&gt;
&lt;p&gt;
GNOT: A General Neural Operator Transformer for Operator Learning. (arXiv:2302.14376v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#36816;&#31639;&#31526;Transformer&#8212;&#8212;GNOT&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24322;&#26500;&#24402;&#19968;&#21270;&#27880;&#24847;&#21147;&#23618;&#21644;&#24341;&#20837;&#20960;&#20309;&#38376;&#25511;&#26426;&#21046;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#35299;&#20915;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#31639;&#23376;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#19981;&#35268;&#21017;&#30340;&#32593;&#26684;&#12289;&#22810;&#20010;&#36755;&#20837;&#20989;&#25968;&#21644;&#35299;&#20915;PDE&#35299;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNOT&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24322;&#26500;&#24402;&#19968;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39640;&#24230;&#28789;&#27963;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#20989;&#25968;&#21644;&#19981;&#35268;&#21017;&#32593;&#26684;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20960;&#20309;&#38376;&#25511;&#26426;&#21046;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#36719;&#22495;&#20998;&#35299;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#27169;&#22411;&#23481;&#37327;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#36328;&#26742;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#32447;&#24615;&#32972;&#26223;&#19979;&#30340;&#19978;&#19979;&#25991;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#20462;&#22797;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#20445;&#25252;&#22833;&#25928;&#12289;&#22240;&#22122;&#22768;&#35745;&#31639;&#38169;&#35823;&#32780;&#23548;&#33268;&#30340;&#19981;&#27491;&#30830;&#30340;&#36951;&#25022;&#30028;&#21644;&#19981;&#21487;&#20449;&#30340;&#36890;&#20449;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13945</link><description>&lt;p&gt;
&#20851;&#20110;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#32447;&#24615;&#32972;&#26223;&#19979;&#30340;&#19978;&#19979;&#25991;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Differentially Private Federated Linear Contextual Bandits. (arXiv:2302.13945v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#36328;&#26742;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#32447;&#24615;&#32972;&#26223;&#19979;&#30340;&#19978;&#19979;&#25991;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#20462;&#22797;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#20445;&#25252;&#22833;&#25928;&#12289;&#22240;&#22122;&#22768;&#35745;&#31639;&#38169;&#35823;&#32780;&#23548;&#33268;&#30340;&#19981;&#27491;&#30830;&#30340;&#36951;&#25022;&#30028;&#21644;&#19981;&#21487;&#20449;&#30340;&#36890;&#20449;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36328;&#26742;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#32447;&#24615;&#32972;&#26223;&#19979;&#30340;&#19978;&#19979;&#25991;&#21305;&#37197;&#38382;&#39064;&#65292;&#22312;&#19981;&#25439;&#23475;&#27599;&#20010;&#29992;&#25143;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#20010;&#27785;&#31215;&#29289;&#65288;&#20195;&#29702;&#65289;&#19982;&#26412;&#22320;&#29992;&#25143;&#20114;&#21160;&#65292;&#24182;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20197;&#23454;&#29616;&#21327;&#20316;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#25216;&#26415;&#20013;&#30340;&#19977;&#20010;&#38382;&#39064;&#65306;(i)&#22768;&#26126;&#30340;&#38544;&#31169;&#20445;&#25252;&#22833;&#25928;&#21644;(ii)&#30001;&#20110;&#22122;&#22768;&#35745;&#31639;&#38169;&#35823;&#32780;&#23548;&#33268;&#30340;&#19981;&#27491;&#30830;&#30340;&#36951;&#25022;&#30028;&#65292;&#20197;&#21450;(iii)&#19981;&#21487;&#20449;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#20010;&#20004;&#27493;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;LCB&#31639;&#27861;&#21644;&#28789;&#27963;&#30340;&#38544;&#31169;&#21327;&#35758;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#32852;&#37030;LCB&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#26742;&#32423;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#19979;&#24314;&#31435;&#20102;&#38544;&#31169;&#21644;&#36951;&#25022;&#20445;&#35777;&#65292;&#20462;&#22797;&#20102;&#29616;&#26377;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36951;&#25022;&#34920;&#29616;&#65292;&#25105;&#20204;&#25509;&#19979;&#26469;&#32771;&#34385;&#20102;&#24046;&#20998;privacy&#30340;&#27927;&#29260;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos (agents) interact with the local users and communicate via a central server to realize collaboration while without sacrificing each user's privacy. We identify three issues in the state-of-the-art: (i) failure of claimed privacy protection and (ii) incorrect regret bound due to noise miscalculation and (iii) ungrounded communication cost. To resolve these issues, we take a two-step principled approach. First, we design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, leveraging the proposed framework, we study federated LCBs under two different privacy constraints. We first establish privacy and regret guarantees under silo-level local differential privacy, which fix the issues present in state-of-the-art algorithm. To further improve the regret performance, we next consider shuffle model of differential p
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#39046;&#22495;&#36866;&#24212;&#30340;&#20915;&#31574;&#26641;(DADT)&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30446;&#26631;&#20154;&#32676;&#20013;&#30340;&#31934;&#30830;&#24615;&#65292;&#36991;&#20813;&#20559;&#35265;&#21644;&#38477;&#20302;&#26576;&#20123;&#20154;&#32676;&#22312;&#30446;&#26631;&#20154;&#32676;&#20013;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.13846</link><description>&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#20915;&#31574;&#26641;&#65306;&#23545;&#31934;&#30830;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Decision Trees: Implications for Accuracy and Fairness. (arXiv:2302.13846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13846
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#39046;&#22495;&#36866;&#24212;&#30340;&#20915;&#31574;&#26641;(DADT)&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30446;&#26631;&#20154;&#32676;&#20013;&#30340;&#31934;&#30830;&#24615;&#65292;&#36991;&#20813;&#20559;&#35265;&#21644;&#38477;&#20302;&#26576;&#20123;&#20154;&#32676;&#22312;&#30446;&#26631;&#20154;&#32676;&#20013;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#24050;&#30693;&#19968;&#20010;&#38382;&#39064;&#26159;&#35813;&#27169;&#22411;&#25152;&#37096;&#32626;&#30340;&#30446;&#26631;&#20154;&#32676;&#21487;&#33021;&#19982;&#35757;&#32451;&#27169;&#22411;&#26102;&#20351;&#29992;&#30340;&#28304;&#20154;&#32676;&#24182;&#19981;&#30456;&#21516;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#39118;&#38505;&#20043;&#19968;&#26159;&#65292;&#38543;&#30528;&#20154;&#32676;&#30340;&#21464;&#21270;&#65292;&#26576;&#20123;&#20154;&#32676;&#23558;&#34987;&#35813;&#27169;&#22411;&#20302;&#20272;&#65292;&#25110;&#32773;&#22312;&#30446;&#26631;&#20154;&#32676;&#20013;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#65292;&#21363;&#20351;&#20182;&#20204;&#22312;&#30446;&#26631;&#20154;&#32676;&#20013;&#21464;&#24471;&#26356;&#20855;&#20195;&#34920;&#24615;&#12290;&#39046;&#22495;&#36866;&#24212;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#30446;&#26631;&#20154;&#32676;&#19981;&#23384;&#22312;&#26631;&#31614;&#25968;&#25454;&#65292;&#20294;&#26576;&#20123;&#26377;&#20851;&#30446;&#26631;&#20998;&#24067;&#30340;&#20449;&#24687;&#23384;&#22312;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#36866;&#24212;&#30340;&#20915;&#31574;&#26641;(DADT)&#65292;&#20026;&#39046;&#22495;&#36866;&#24212;&#25991;&#29486;&#20570;&#20986;&#36129;&#29486;&#12290;&#37492;&#20110;&#20915;&#31574;&#26641;&#22240;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#30456;&#23545;&#20110;&#20854;&#20182;&#26356;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20915;&#31574;&#26641;&#19978;&#12290;&#36890;&#36807;DADT&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#25968;-free &#30340;&#21160;&#24577; SGD &#27493;&#38271;&#20844;&#24335;&#65292;&#31216;&#20026;&#26799;&#24230;&#36317;&#31163;&#20844;&#24335;&#65288;DoG&#65289;&#65292; &#23427;&#27809;&#26377;&#8220;&#23398;&#20064;&#29575;&#8221;&#21442;&#25968;&#65292;&#20294;&#26159;&#22312;&#23616;&#37096;&#26377;&#30028;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#20013;&#25317;&#26377;&#24378;&#22823;&#30340;&#26080;&#21442;&#25968;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19982;&#26377;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340; SGD &#30456;&#24403;&#25509;&#36817;&#12290;</title><link>http://arxiv.org/abs/2302.12022</link><description>&lt;p&gt;
DoG&#26159;SGD&#26368;&#22909;&#30340;&#26379;&#21451;&#65306;&#19968;&#20010;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#30340;&#21160;&#24577;&#27493;&#38271;&#22823;&#23567;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule. (arXiv:2302.12022v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12022
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#25968;-free &#30340;&#21160;&#24577; SGD &#27493;&#38271;&#20844;&#24335;&#65292;&#31216;&#20026;&#26799;&#24230;&#36317;&#31163;&#20844;&#24335;&#65288;DoG&#65289;&#65292; &#23427;&#27809;&#26377;&#8220;&#23398;&#20064;&#29575;&#8221;&#21442;&#25968;&#65292;&#20294;&#26159;&#22312;&#23616;&#37096;&#26377;&#30028;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#20013;&#25317;&#26377;&#24378;&#22823;&#30340;&#26080;&#21442;&#25968;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19982;&#26377;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340; SGD &#30456;&#24403;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#30340;&#21160;&#24577;SGD&#27493;&#38271;&#20844;&#24335;&#65292;&#31216;&#20026;&#26799;&#24230;&#36317;&#31163;&#20844;&#24335;&#65288;DoG&#65289;&#12290;DoG&#27493;&#38271;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#32463;&#39564;&#37327;&#65288;&#21021;&#22987;&#28857;&#36317;&#31163;&#21644;&#26799;&#24230;&#33539;&#25968;&#65289;&#65292;&#24182;&#19988;&#27809;&#26377;&#8220;&#23398;&#20064;&#29575;&#8221;&#21442;&#25968;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DoG&#20844;&#24335;&#30340;&#19968;&#20010;&#30053;&#24494;&#21464;&#21270;&#21487;&#20197;&#20445;&#35777;&#20855;&#26377;&#24378;&#22823;&#30340;&#26080;&#21442;&#25968;&#25910;&#25947;&#24615;&#65292;&#20551;&#23450;&#21482;&#26377;&#23616;&#37096;&#26377;&#30028;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24191;&#27867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#26174;&#31034;DoG&#30340;&#24615;&#33021;&#25509;&#36817;&#20855;&#26377;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#30340;SGD&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#21464;&#37327;&#30340;DoG&#21464;&#20307;&#65292;&#36890;&#24120;&#20248;&#20110;&#35843;&#25972;&#30340;SGD&#65292;&#24182;&#25509;&#36817;&#35843;&#25972;&#30340;Adam&#30340;&#24615;&#33021;&#12290;PyTorch&#23454;&#29616;&#21487;&#22312;https://github.com/formll/dog&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no ``learning rate'' parameter. Theoretically, we show that a slight variation of the DoG formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only \emph{locally bounded} stochastic gradients. Empirically, we consider a broad range of vision and language transfer learning tasks, and show that DoG's performance is close to that of SGD with tuned learning rate. We also propose a per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance of tuned Adam. A PyTorch implementation is available at https://github.com/formll/dog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11996</link><description>&lt;p&gt;
K-SHAP: &#19968;&#31181;&#29992;&#20110;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#23427;&#20204;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#25105;&#20204;&#35299;&#37322;&#23427;&#20204;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#19968;&#31181;&#29305;&#23450;&#30340;&#24773;&#20917;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#37027;&#23601;&#26159;&#26234;&#33021;&#20307;&#36523;&#20221;&#20445;&#25345;&#21311;&#21517;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#26159;&#19987;&#26377;&#30340;&#65292;&#20165;&#20844;&#24320;&#22810;&#20010;&#24066;&#22330;&#21442;&#19982;&#32773;&#20132;&#20114;&#32780;&#20135;&#29983;&#30340;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#34892;&#21160;&#24207;&#21015;&#19981;&#21487;&#35266;&#27979;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;K-SHAP&#65292;&#23427;&#23398;&#20064;&#26681;&#25454;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;(IL)&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;w...
&lt;/p&gt;
&lt;p&gt;
Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IB-RAR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#38750;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#36807;&#28388;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10896</link><description>&lt;p&gt;
IB-RAR&#65306;&#20449;&#24687;&#29942;&#39048;&#20316;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness. (arXiv:2302.10896v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IB-RAR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#38750;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#36807;&#28388;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;IB-RAR&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#38750;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#20351;&#29992;IB&#29702;&#35770;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#26500;&#24314;&#27491;&#21017;&#21270;&#22120;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20013;&#38388;&#34920;&#31034;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#36807;&#28388;&#25481;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#20351;&#29992;IB&#35757;&#32451;&#30340;&#32593;&#32476;&#25552;&#20379;&#26131;&#20110;&#21306;&#20998;&#30340;MI&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#23545;&#25239;&#24615;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#26032;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#19978;&#25552;&#20379;&#22987;&#32456;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38024;&#23545;VGG16&#32593;&#32476;&#36827;&#34892;&#19977;&#27425;&#23545;&#25239;&#24615;&#35757;&#32451;&#22522;&#20934;&#21644;CIFAR-10&#25968;&#25454;&#38598;&#30340;&#20116;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#26696;&#20013;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;3.07&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20026;&#26080;&#38450;&#24481;&#26041;&#27861;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#20165;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#22312;&#26080;&#20154;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#32570;&#22833;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method, IB-RAR, which uses Information Bottleneck (IB) to strengthen adversarial robustness for both adversarial training and non-adversarial-trained methods. We first use the IB theory to build regularizers as learning objectives in the loss function. Then, we filter out unnecessary features of intermediate representation according to their mutual information (MI) with labels, as the network trained with IB provides easily distinguishable MI for its features. Experimental results show that our method can be naturally combined with adversarial training and provides consistently better accuracy on new adversarial examples. Our method improves the accuracy by an average of 3.07% against five adversarial attacks for the VGG16 network, trained with three adversarial training benchmarks and the CIFAR-10 dataset. In addition, our method also provides good robustness for undefended methods, such as training with cross-entropy loss only. Finally, in the absenc
&lt;/p&gt;</description></item><item><title>MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10494</link><description>&lt;p&gt;
MaskedKD&#65306;&#20351;&#29992;&#36974;&#34109;&#22270;&#20687;&#30340;&#39640;&#25928;Vision Transformer&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MaskedKD: Efficient Distillation of Vision Transformers with Masked Images. (arXiv:2302.10494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10494
&lt;/p&gt;
&lt;p&gt;
MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23545;&#20110;&#35757;&#32451;&#36731;&#37327;&#32423;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20250;&#22312;&#35757;&#32451;&#25104;&#26412;&#20013;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#20026;&#35813;&#26041;&#27861;&#38656;&#35201;&#22312;&#35757;&#32451;&#26679;&#26412;&#19978;&#33719;&#21462;&#25945;&#24072;&#30417;&#30563;&#12290;&#24403;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;Vision Transformer&#65288;ViTs&#65289;&#31561;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#31181;&#38468;&#21152;&#25104;&#26412;&#8212;&#8212;&#33976;&#39311;&#25104;&#26412;&#8212;&#8212;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MaskedKD&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#33976;&#39311;ViTs&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MaskedKD&#36890;&#36807;&#36974;&#34109;&#19968;&#37096;&#20998;&#36755;&#20837;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;&#22270;&#20687;&#22359;&#20196;&#25945;&#24072;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#20943;&#23569;&#65292;&#22240;&#27492;&#21487;&#20197;&#36339;&#36807;&#22788;&#29702;&#36825;&#20123;&#22359;&#25152;&#38656;&#30340;&#35745;&#31639;&#12290;&#25152;&#36873;&#30340;&#36974;&#32617;&#20301;&#32622;&#26088;&#22312;&#38450;&#27490;&#23631;&#34109;&#23398;&#29983;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#30340;&#22270;&#20687;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#35813;&#36974;&#32617;&#36873;&#25321;&#26426;&#21046;&#22522;&#20110;&#23398;&#29983;&#27169;&#22411;&#30340;&#26576;&#20123;&#27880;&#24847;&#21147;&#20998;&#25968;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31163;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#20013;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.09795</link><description>&lt;p&gt;
&#35270;&#35273;&#34920;&#24449;&#20013;&#39118;&#26684;&#19982;&#20869;&#23481;&#30340;&#31616;&#21333;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Simple Disentanglement of Style and Content in Visual Representations. (arXiv:2302.09795v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31163;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#20013;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#24449;&#65292;&#21363;&#20998;&#31163;&#30340;&#34920;&#24449;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#25104;&#21151;&#65292;&#20294;&#35201;&#24212;&#29992;&#20110;&#20687;ImageNet&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#25968;&#25454;&#38598;&#21017;&#24456;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#20013;&#20998;&#31163;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#20013;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#29305;&#24449;&#27010;&#29575;&#22320;&#24314;&#27169;&#20026;&#28508;&#22312;&#20869;&#23481;&#21644;&#39118;&#26684;&#22240;&#32032;&#30340;&#32447;&#24615;&#32508;&#21512;&#65292;&#24182;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31163;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#21487;&#38752;&#22320;&#20998;&#31163;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22788;&#29702;&#21518;&#30340;&#29305;&#24449;&#22312;&#26679;&#24335;&#21464;&#21270;&#25110;&#19982;&#26679;&#24335;&#30456;&#20851;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning visual representations with interpretable features, i.e., disentangled representations, remains a challenging problem. Existing methods demonstrate some success but are hard to apply to large-scale vision datasets like ImageNet. In this work, we propose a simple post-processing framework to disentangle content and style in learned representations from pre-trained vision models. We model the pre-trained features probabilistically as linearly entangled combinations of the latent content and style factors and develop a simple disentanglement algorithm based on the probabilistic model. We show that the method provably disentangles content and style features and verify its efficacy empirically. Our post-processed features yield significant domain generalization performance improvements when the distribution shift occurs due to style changes or style-related spurious correlations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09195</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#26131;&#20110;&#23398;&#20064;&#30340;&#26679;&#26412;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#35782;&#21035;&#23545;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#26368;&#26377;&#29992;&#30340;&#31034;&#20363;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;SSL&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;SSL&#30340;&#20215;&#20540;&#22914;&#20309;&#37327;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#65292;&#23545;&#27604;SSL&#20013;&#23545;&#23398;&#20064;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#31034;&#20363;&#26159;&#20855;&#26377;&#26368;&#30456;&#20284;&#25968;&#25454;&#22686;&#24378;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;SSL&#30340;&#24191;&#20041;&#24615;&#33021;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;SSL&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#23376;&#38598;&#26159;&#23545;&#30417;&#30563;&#23398;&#20064;&#20570;&#20986;&#26368;&#23567;&#36129;&#29486;&#30340;&#23376;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23376;&#38598;&#22312;CIFAR100&#12289;CIFAR&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#23376;&#38598;3%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#20844;&#24179;&#24615;&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.08973</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#20013;&#30340;&#24179;&#31561;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Equality in Machine Learning Security Defenses. (arXiv:2302.08973v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#20844;&#24179;&#24615;&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#31038;&#21306;&#24050;&#32463;&#21457;&#23637;&#20102;&#35768;&#22810;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#20294;&#36825;&#20010;&#31038;&#21306;&#20013;&#40092;&#26377;&#30740;&#31350;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#20026;&#35841;&#25552;&#20379;&#20445;&#25252;&#21602;&#65311;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#34987;&#19981;&#21516;&#30340;&#23376;&#32676;&#20307;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#23454;&#35777;&#32467;&#26524;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35768;&#22810;&#26041;&#27861;&#21487;&#33021;&#20250;&#30452;&#25509;&#36896;&#25104;&#20260;&#23475;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26377;&#20559;&#28431;&#27934;&#21644;&#26377;&#20559;&#25490;&#26021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24378;&#21270;&#35757;&#32451;&#27169;&#22411;&#12289;&#22522;&#20110;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#21644;&#25298;&#32477;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#22312;&#23433;&#20840;&#39044;&#31639;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#36866;&#21512;&#20110;&#27979;&#37327;&#38450;&#24481;&#30340;&#24179;&#31561;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#20195;&#38450;&#24481;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#24179;&#31561;&#24615;&#24615;&#33021;&#30340;&#34913;&#37327;&#20215;&#20540;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#21644;&#26041;&#27861;&#33021;&#22815;&#40723;&#21169;&#21644;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#21644;&#38450;&#24481;&#39046;&#22495;&#30340;&#20844;&#24179;&#24615;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning security community has developed myriad defenses for evasion attacks over the past decade. An understudied question in that community is: for whom do these defenses defend? In this work, we consider some common approaches to defending learned systems and whether those approaches may offer unexpected performance inequities when used by different sub-populations. We outline simple parity metrics and a framework for analysis that can begin to answer this question through empirical results of the fairness implications of machine learning security methods. Many methods have been proposed that can cause direct harm, which we describe as biased vulnerability and biased rejection. Our framework and metric can be applied to robustly trained models, preprocessing-based methods, and rejection methods to capture behavior over security budgets. We identify a realistic dataset with a reasonable computational cost suitable for measuring the equality of defenses. Through a case st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#36710;&#36742;&#25511;&#21046;&#22120;&#65292;&#21487;&#36890;&#36807;&#23398;&#20064;&#20462;&#27491;&#32463;&#20856;&#25511;&#21046;&#22120;&#20197;&#23454;&#29616;&#36187;&#36710;&#30340;&#36335;&#24452;&#36319;&#36394;&#65292;&#24182;&#22312;&#21313;&#20108;&#20010;&#30495;&#23454;&#36187;&#36947;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.07035</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36187;&#36710;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#36710;&#36742;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Residual Policy Learning for Vehicle Control of Autonomous Racing Cars. (arXiv:2302.07035v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#36710;&#36742;&#25511;&#21046;&#22120;&#65292;&#21487;&#36890;&#36807;&#23398;&#20064;&#20462;&#27491;&#32463;&#20856;&#25511;&#21046;&#22120;&#20197;&#23454;&#29616;&#36187;&#36710;&#30340;&#36335;&#24452;&#36319;&#36394;&#65292;&#24182;&#22312;&#21313;&#20108;&#20010;&#30495;&#23454;&#36187;&#36947;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36187;&#36710;&#25511;&#21046;&#22120;&#30340;&#24320;&#21457;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36187;&#36710;&#36816;&#34892;&#22312;&#20854;&#29289;&#29702;&#39550;&#39542;&#26497;&#38480;&#19978;&#12290;&#21463;&#21040;&#23545;&#34920;&#29616;&#30340;&#38656;&#27714;&#30340;&#25512;&#21160;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#24320;&#22987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#36187;&#36710;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#25215;&#35834;&#36890;&#36807;&#23558;&#20256;&#32479;&#25511;&#21046;&#22120;&#19982;&#23398;&#20064;&#30340;&#27531;&#24046;&#25511;&#21046;&#22120;&#30456;&#32467;&#21512;&#26469;&#32531;&#35299;&#27492;&#32570;&#28857;&#65292;&#27531;&#20313;&#25511;&#21046;&#22120;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20854;&#39640;&#24230;&#36866;&#24212;&#24615;&#65292;&#19982;&#20256;&#32479;&#25511;&#21046;&#22120;&#30340;&#31283;&#23450;&#34892;&#20026;&#24182;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#39550;&#39542;&#36187;&#36710;&#30340;&#27531;&#24046;&#36710;&#36742;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#23398;&#20064;&#20462;&#27491;&#32463;&#20856;&#25511;&#21046;&#22120;&#20197;&#23454;&#29616;&#36187;&#36947;&#32447;&#30340;&#36335;&#24452;&#36319;&#36394;&#12290;&#22312;&#24191;&#27867;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#34987;&#35780;&#20272;&#65292;&#24182;&#24212;&#29992;&#20110;F1TENTH&#33258;&#20027;&#39550;&#39542;&#36187;&#36710;&#30340;&#20223;&#30495;&#36710;&#36742;&#19978;&#12290;&#23545;&#21313;&#20108;&#20010;&#22797;&#21046;&#30340;&#30495;&#23454;&#36187;&#36947;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#25928;&#26524;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of vehicle controllers for autonomous racing is challenging because racing cars operate at their physical driving limit. Prompted by the demand for improved performance, autonomous racing research has seen the proliferation of machine learning-based controllers. While these approaches show competitive performance, their practical applicability is often limited. Residual policy learning promises to mitigate this drawback by combining classical controllers with learned residual controllers. The critical advantage of residual controllers is their high adaptability parallel to the classical controller's stable behavior. We propose a residual vehicle controller for autonomous racing cars that learns to amend a classical controller for the path-following of racing lines. In an extensive study, performance gains of our approach are evaluated for a simulated car of the F1TENTH autonomous racing series. The evaluation for twelve replicated real-world racetracks shows that the re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#32858;&#21512;&#35268;&#21017;-&#32852;&#37030;&#26799;&#24230;&#25237;&#24433;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21152;&#26435;&#26041;&#26696;&#65292;&#29992;&#20110;&#26368;&#20248;&#22320;&#32467;&#21512;&#28304;&#21644;&#30446;&#26631;&#26799;&#24230;&#65292;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#39046;&#22495;&#36716;&#31227;&#26102;&#24120;&#35265;&#30340;&#25216;&#26415;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.05049</link><description>&lt;p&gt;
&#32852;&#37030;&#33258;&#21152;&#26435;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Federated Auto-weighted Domain Adaptation. (arXiv:2302.05049v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#32858;&#21512;&#35268;&#21017;-&#32852;&#37030;&#26799;&#24230;&#25237;&#24433;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21152;&#26435;&#26041;&#26696;&#65292;&#29992;&#20110;&#26368;&#20248;&#22320;&#32467;&#21512;&#28304;&#21644;&#30446;&#26631;&#26799;&#24230;&#65292;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#39046;&#22495;&#36716;&#31227;&#26102;&#24120;&#35265;&#30340;&#25216;&#26415;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;FDA&#65289;&#26159;&#25551;&#36848;&#22810;&#20010;&#28304;&#23458;&#25143;&#31471;&#21327;&#20316;&#25913;&#21892;&#30446;&#26631;&#23458;&#25143;&#31471;&#24615;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#26377;&#38480;&#12290;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#65292;&#21152;&#19978;&#30446;&#26631;&#39046;&#22495;&#30340;&#31232;&#30095;&#25968;&#25454;&#65292;&#20351;&#24471;FDA&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#24120;&#35265;&#30340;&#25216;&#26415;&#65288;&#22914;FedAvg&#21644;&#24494;&#35843;&#65289;&#22312;&#23384;&#22312;&#26174;&#33879;&#39046;&#22495;&#36716;&#31227;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#26102;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#34920;&#24449;FDA&#35774;&#32622;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20998;&#26512;&#32858;&#21512;&#35268;&#21017;&#24615;&#33021;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;FDA&#30340;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#35268;&#21017;&#65292;&#31216;&#20026;&#32852;&#37030;&#26799;&#24230;&#25237;&#24433;&#65288;$\texttt{FedGP}$&#65289;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#26399;&#38388;&#32858;&#21512;&#28304;&#26799;&#24230;&#21644;&#30446;&#26631;&#26799;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#24471;&#24320;&#21457;&#19968;&#20010;&#33258;&#21160;&#21152;&#26435;&#26041;&#26696;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20010;&#26041;&#26696;&#33021;&#22815;&#26368;&#20248;&#22320;&#32467;&#21512;&#28304;&#21644;&#30446;&#26631;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Domain Adaptation (FDA) describes the federated learning setting where a set of source clients work collaboratively to improve the performance of a target client where limited data is available. The domain shift between the source and target domains, coupled with sparse data in the target domain, makes FDA a challenging problem, e.g., common techniques such as FedAvg and fine-tuning, often fail with the presence of significant domain shift and data scarcity. To comprehensively understand the problem, we introduce metrics that characterize the FDA setting and put forth a theoretical framework for analyzing the performance of aggregation rules. We also propose a novel aggregation rule for FDA, Federated Gradient Projection ($\texttt{FedGP}$), used to aggregate the source gradients and target gradient during training. Importantly, our framework enables the development of an $\textit{auto-weighting scheme}$ that optimally combines the source and target gradients. This scheme impr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04831</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21327;&#21516;&#21512;&#20316;&#23398;&#20064;&#26694;&#26550;&#30340;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38646;&#26679;&#26412;&#21327;&#35843;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#21327;&#35843;&#19968;&#31995;&#21015;&#30475;&#19981;&#35265;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#35797;&#22270;&#36890;&#36807;&#20248;&#21270;&#31181;&#32676;&#20013;&#30340;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#21892;&#31574;&#30053;&#25110;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25439;&#22833;&#21644;&#19982;&#31181;&#32676;&#20013;&#26576;&#20123;&#31574;&#30053;&#26080;&#27861;&#21512;&#20316;&#65292;&#21363;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;&#65288;COLE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#21327;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20197;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26126;&#30830;&#20102;&#26694;&#26550;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#21338;&#24328;&#35770;&#21644;&#22270;&#35770;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#31639;&#27861;&#30340;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20811;&#26381;&#23398;&#20064;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#22522;&#20110;&#20256;&#36755;&#26144;&#23556;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#27969;&#30340;&#25552;&#35758;&#21487;&#20197;&#22788;&#29702;&#22810;&#23792;&#30446;&#26631;&#65292;&#22312;&#39640;&#32500;&#24230;&#21644;&#35757;&#32451;&#19981;&#33391;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20381;&#36182;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2302.04763</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#36817;&#20284;&#20256;&#36755;&#26144;&#23556;&#36827;&#34892;&#25277;&#26679;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Sampling with Approximate Transport Maps. (arXiv:2302.04763v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#22522;&#20110;&#20256;&#36755;&#26144;&#23556;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#27969;&#30340;&#25552;&#35758;&#21487;&#20197;&#22788;&#29702;&#22810;&#23792;&#30446;&#26631;&#65292;&#22312;&#39640;&#32500;&#24230;&#21644;&#35757;&#32451;&#19981;&#33391;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20381;&#36182;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20998;&#24067;&#36716;&#21270;&#20026;&#26131;&#20110;&#22788;&#29702;&#30340;&#20998;&#24067;&#65292;&#20256;&#36755;&#26144;&#23556;&#21487;&#20197;&#31616;&#21270;&#20855;&#26377;&#38750;&#24179;&#20961;&#20960;&#20309;&#32467;&#26500;&#30340;&#20998;&#24067;&#30340;&#25277;&#26679;&#12290;&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20256;&#32479;&#27969;&#65288;NF&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#19981;&#26029;&#25552;&#39640;&#12290;NF&#22686;&#24378;&#37319;&#26679;&#22120;&#26368;&#36817;&#25552;&#20986;&#20102;&#23558;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#19982;&#65288;i&#65289;&#26469;&#33258;&#27969;&#30340;&#25552;&#35758;&#32472;&#21046;&#25110;&#65288;ii&#65289;&#22522;&#20110;&#27969;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#21040;&#30340;&#20256;&#36755;&#30340;&#36136;&#37327;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#38416;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24471;&#20986;&#32467;&#35770;&#65306;&#30452;&#21040;&#20013;&#31561;&#32500;&#24230;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#20351;&#29992;&#22522;&#20110;&#27969;&#30340;&#25552;&#35758;&#22788;&#29702;&#22810;&#23792;&#30446;&#26631;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#39640;&#32500;&#24230;&#21644;&#35757;&#32451;&#19981;&#33391;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#36182;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24335;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#20854;&#20182;&#26041;&#38754;&#26356;&#20026;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transport maps can ease the sampling of distributions with non-trivial geometries by transforming them into distributions that are easier to handle. The potential of this approach has risen with the development of Normalizing Flows (NF) which are maps parameterized with deep neural networks trained to push a reference distribution towards a target. NF-enhanced samplers recently proposed blend (Markov chain) Monte Carlo methods with either (i) proposal draws from the flow or (ii) a flow-based reparametrization. In both cases, the quality of the learned transport conditions performance. The present work clarifies for the first time the relative strengths and weaknesses of these two approaches. Our study concludes that multimodal targets can be reliably handled with flow-based proposals up to moderately high dimensions. In contrast, methods relying on reparametrization struggle with multimodality but are more robust otherwise in high-dimensional settings and under poor training. To furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22270;&#23398;&#20064;&#36712;&#36947;&#23884;&#20837;&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#27867;&#21270;&#31070;&#32463;&#27874;&#20989;&#25968;&#21040;&#19981;&#21516;&#30340;&#20998;&#23376;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#23567;&#19968;&#33268;&#30340;&#27874;&#20989;&#25968;Ansatz&#65292;&#21517;&#20026;&#8220;&#20998;&#23376;&#36712;&#36947;&#32593;&#32476;&#8221;&#65292;&#21487;&#20197;&#32852;&#21512;&#27714;&#35299;&#19981;&#21516;&#20998;&#23376;&#30340;Schr&#246;dinger&#26041;&#31243;&#65292;&#25552;&#39640;&#20102;&#27714;&#35299;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.04168</link><description>&lt;p&gt;
&#27867;&#21270;&#31070;&#32463;&#27874;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Generalizing Neural Wave Functions. (arXiv:2302.04168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22270;&#23398;&#20064;&#36712;&#36947;&#23884;&#20837;&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#27867;&#21270;&#31070;&#32463;&#27874;&#20989;&#25968;&#21040;&#19981;&#21516;&#30340;&#20998;&#23376;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#23567;&#19968;&#33268;&#30340;&#27874;&#20989;&#25968;Ansatz&#65292;&#21517;&#20026;&#8220;&#20998;&#23376;&#36712;&#36947;&#32593;&#32476;&#8221;&#65292;&#21487;&#20197;&#32852;&#21512;&#27714;&#35299;&#19981;&#21516;&#20998;&#23376;&#30340;Schr&#246;dinger&#26041;&#31243;&#65292;&#25552;&#39640;&#20102;&#27714;&#35299;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27874;&#20989;&#25968;&#22312;&#24314;&#27169;&#20174;&#22836;&#31639;&#22522;&#24577;&#21183;&#33021;&#26354;&#38754;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#29366;&#24577;-of-the-art&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#21482;&#33021;&#35299;&#20915;&#30456;&#21516;&#21407;&#23376;&#32452;&#30340;&#19981;&#21516;&#31354;&#38388;&#25490;&#21015;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22270;&#23398;&#20064;&#36712;&#36947;&#23884;&#20837;(Globe)&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#27874;&#20989;&#25968;&#36866;&#24212;&#20110;&#19981;&#21516;&#30340;&#20998;&#23376;&#12290;Globe&#36890;&#36807;&#36830;&#25509;&#20998;&#23376;&#36712;&#36947;&#21644;&#20849;&#20215;&#38190;&#20197;&#23454;&#29616;&#31354;&#38388;&#20449;&#24687;&#20256;&#36882;&#65292;&#23398;&#20064;&#23616;&#37096;&#30005;&#23376;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#22312;&#20998;&#23376;&#20043;&#38388;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#23567;&#19968;&#33268;&#30340;&#27874;&#20989;&#25968;Ansatz&#65292;&#20998;&#23376;&#36712;&#36947;&#32593;&#32476;(Moon)&#65292;&#19987;&#38376;&#29992;&#20110;&#32852;&#21512;&#27714;&#35299;&#19981;&#21516;&#20998;&#23376;&#30340;Schr&#246;dinger&#26041;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Moon&#22312;4.5&#20493;&#30340;&#36845;&#20195;&#27493;&#25968;&#20869;&#21363;&#21487;&#25910;&#25947;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#20284;&#30340;&#31934;&#24230;&#65292;&#25110;&#22312;&#30456;&#21516;&#30340;&#26102;&#38388;&#20869;&#32473;&#20986;&#26356;&#20302;&#30340;&#33021;&#37327;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;Moon&#30340;&#33021;&#37327;&#20272;&#31639;&#20855;&#26377;&#21487;&#21152;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural network-based wave functions have achieved state-of-the-art accuracies in modeling ab-initio ground-state potential energy surface. However, these networks can only solve different spatial arrangements of the same set of atoms. To overcome this limitation, we present Graph-learned orbital embeddings (Globe), a neural network-based reparametrization method that can adapt neural wave functions to different molecules. Globe learns representations of local electronic structures that generalize across molecules via spatial message passing by connecting molecular orbitals to covalent bonds. Further, we propose a size-consistent wave function Ansatz, the Molecular orbital network (Moon), tailored to jointly solve Schr\"odinger equations of different molecules. In our experiments, we find Moon converging in 4.5 times fewer steps to similar accuracy as previous methods or to lower energies given the same time. Further, our analysis shows that Moon's energy estimate scales additive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;&#26041;&#27861;&#33021;&#22815;&#24212;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26368;&#23569;&#20294;&#37325;&#35201;&#30340;&#26356;&#25913;&#65292;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#20102;&#23454;&#36341;&#20013;&#21487;&#24212;&#29992;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2302.02948</link><description>&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Reinforcement Learning with Offline Data. (arXiv:2302.02948v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;&#26041;&#27861;&#33021;&#22815;&#24212;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26368;&#23569;&#20294;&#37325;&#35201;&#30340;&#26356;&#25913;&#65292;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#20102;&#23454;&#36341;&#20013;&#21487;&#24212;&#29992;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26679;&#26412;&#25928;&#29575;&#21644;&#25506;&#32034;&#20173;&#28982;&#26159;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#26041;&#27861;&#26159;&#21253;&#25324;&#31163;&#32447;&#25968;&#25454;&#65292;&#22914;&#26469;&#33258;&#20154;&#31867;&#19987;&#23478;&#25110;&#27425;&#20248;&#25506;&#32034;&#31574;&#30053;&#30340;&#20808;&#21069;&#36712;&#36857;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24191;&#27867;&#30340;&#20462;&#25913;&#21644;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#26469;&#30830;&#20445;&#26377;&#25928;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24819;&#38382;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#31616;&#21333;&#22320;&#24212;&#29992;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;&#26041;&#27861;&#26469;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#38656;&#35201;&#23545;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#19968;&#20123;&#26368;&#23569;&#20294;&#37325;&#35201;&#30340;&#26356;&#25913;&#12290;&#25105;&#20204;&#24191;&#27867;&#22320;&#27979;&#35797;&#20102;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24471;&#20986;&#20102;&#19968;&#32452;&#23454;&#36341;&#32773;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#30340;&#24314;&#35758;&#65292;&#26080;&#35770;&#20854;&#25968;&#25454;&#21253;&#25324;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#25110;&#22823;&#37327;&#27425;&#20248;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; CHiLS&#65292;&#23427;&#21033;&#29992;&#20998;&#23618;&#26631;&#31614;&#38598;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#65292;&#36890;&#36807;&#20135;&#29983;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#31867;&#21035;&#21517;&#31216;&#65292;&#22312;&#33719;&#24471;&#26356;&#31934;&#32454;&#30340;&#20998;&#31867;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20351;&#29992;&#26356;&#23569;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.02551</link><description>&lt;p&gt;
CHiLS&#65306;&#20855;&#26377;&#20998;&#23618;&#26631;&#31614;&#38598;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets. (arXiv:2302.02551v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; CHiLS&#65292;&#23427;&#21033;&#29992;&#20998;&#23618;&#26631;&#31614;&#38598;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#65292;&#36890;&#36807;&#20135;&#29983;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#31867;&#21035;&#21517;&#31216;&#65292;&#22312;&#33719;&#24471;&#26356;&#31934;&#32454;&#30340;&#20998;&#31867;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20351;&#29992;&#26356;&#23569;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#36890;&#36807;&#33021;&#22815;&#22522;&#20110;&#23427;&#20204;&#65288;&#33258;&#28982;&#35821;&#35328;&#65289;&#30340;&#21517;&#31216;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#23884;&#20837;&#21521;&#37327;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30528;&#37325;&#20110;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#36890;&#36807;&#21253;&#21547;&#23569;&#37327;&#26631;&#35760;&#30340;&#19979;&#28216;&#25968;&#25454;&#65288;&#36890;&#36807;&#24494;&#35843;&#65289;&#26469;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#22312;&#25913;&#36827;&#31867;&#21035;&#21517;&#31216;&#30340;&#20016;&#23500;&#24615;&#26041;&#38754;&#21364;&#40092;&#26377;&#30740;&#31350;&#65292;&#36825;&#22312;&#31867;&#21035;&#26631;&#31614;&#34987;&#31895;&#30053;&#23450;&#20041;&#24182;&#19988;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#26377;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20998;&#23618;&#26631;&#31614;&#38598;&#65288;CHiLS&#65289;&#30340;&#20998;&#31867;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#20855;&#26377;&#38544;&#21547;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26367;&#20195;&#31574;&#30053;&#12290;CHiLS&#20998;&#19977;&#27493;&#36827;&#34892;&#65306;&#65288;i&#65289;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#19968;&#32452;&#23376;&#31867;&#65292;&#20351;&#29992;&#29616;&#26377;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#25110;&#36890;&#36807;&#26597;&#35810;GPT-3&#23454;&#29616;&#65307;&#65288;ii&#65289;&#25191;&#34892;&#26631;&#20934;&#30340;&#38646;&#26679;&#26412;CLIP&#36807;&#31243;&#65292;&#23601;&#20687;&#36825;&#20123;&#23376;&#31867;&#21035;&#26159;&#24863;&#20852;&#36259;&#30340;&#26631;&#31614;&#37027;&#26679;&#36827;&#34892;&#65307;&#65288;iii&#65289;&#23558;&#39044;&#27979;&#30340;&#31867;&#21035;&#23884;&#20837;&#26144;&#23556;&#22238;&#21407;&#22987;&#31867;&#21035;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#22312;&#26399;&#26395;&#30340;&#31890;&#24230;&#27700;&#24179;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#29992;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#23618;&#27425;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;CHiLS&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#38656;&#35201;&#26356;&#23569;&#30340;&#19979;&#28216;&#26631;&#31614;&#30340;&#21516;&#26102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#37325;&#26500;&#20986;&#38543;&#26426;&#36974;&#34109;&#35270;&#28857;&#30340;&#20687;&#32032;&#65292;&#24182;&#23398;&#20064;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#35270;&#22270;&#25511;&#21046;&#21644;&#21333;&#35270;&#22270;&#25511;&#21046;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#35757;&#32451;&#20986;&#20855;&#26377;&#24378;&#35270;&#28857;&#38543;&#26426;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.02408</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#36974;&#34109;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Multi-View Masked World Models for Visual Robotic Manipulation. (arXiv:2302.02408v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#37325;&#26500;&#20986;&#38543;&#26426;&#36974;&#34109;&#35270;&#28857;&#30340;&#20687;&#32032;&#65292;&#24182;&#23398;&#20064;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#35270;&#22270;&#25511;&#21046;&#21644;&#21333;&#35270;&#22270;&#25511;&#21046;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#35757;&#32451;&#20986;&#20855;&#26377;&#24378;&#35270;&#28857;&#38543;&#26426;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#30740;&#31350;&#21644;&#24212;&#29992;&#36890;&#24120;&#20351;&#29992;&#22810;&#20010;&#25668;&#20687;&#22836;&#25110;&#35270;&#35282;&#26469;&#26356;&#22909;&#22320;&#24863;&#30693;&#19990;&#30028;&#12290;&#37027;&#20040;&#25105;&#20204;&#22914;&#20309;&#21033;&#29992;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#21602;&#65311;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#22810;&#35270;&#22270;&#25968;&#25454;&#23398;&#20064;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#22810;&#35270;&#22270;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#37325;&#26500;&#20986;&#38543;&#26426;&#36974;&#34109;&#35270;&#28857;&#30340;&#20687;&#32032;&#65292;&#28982;&#21518;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22810;&#35270;&#22270;&#25511;&#21046;&#12289;&#20026;&#34920;&#31034;&#23398;&#20064;&#20351;&#29992;&#36741;&#21161;&#25668;&#20687;&#22836;&#30340;&#21333;&#35270;&#22270;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#20010;&#38543;&#26426;&#35270;&#28857;&#35757;&#32451;&#30340;&#22810;&#35270;&#22270;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#35757;&#32451;&#20855;&#26377;&#24378;&#35270;&#28857;&#38543;&#26426;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#23558;&#35813;&#31574;&#30053;&#36716;&#31227;&#21040;&#35299;&#20915;&#23454;&#38469;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#32780;&#26080;&#38656;&#30456;&#26426;&#26657;&#20934;&#21644;&#36866;&#24212;&#31243;&#24207;&#12290;&#35270;&#39057;&#28436;&#31034;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#26597;&#30475;&#65306;https:/ / 2302.02408v2 [cs.RO] UPDATED
&lt;/p&gt;
&lt;p&gt;
Visual robotic manipulation research and applications often use multiple cameras, or views, to better perceive the world. How else can we utilize the richness of multi-view data? In this paper, we investigate how to learn good representations with multi-view data and utilize them for visual robotic manipulation. Specifically, we train a multi-view masked autoencoder which reconstructs pixels of randomly masked viewpoints and then learn a world model operating on the representations from the autoencoder. We demonstrate the effectiveness of our method in a range of scenarios, including multi-view control and single-view control with auxiliary cameras for representation learning. We also show that the multi-view masked autoencoder trained with multiple randomized viewpoints enables training a policy with strong viewpoint randomization and transferring the policy to solve real-robot tasks without camera calibration and an adaptation procedure. Video demonstrations are available at: https:/
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;&#65292;&#21517;&#20026;&#21442;&#19982;&#20852;&#22859;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#24178;&#39044;&#29983;&#25104;&#36807;&#31243;&#20197;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#20449;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#35821;&#20041;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22833;&#28789;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2301.13826</link><description>&lt;p&gt;
&#21442;&#19982;&#20852;&#22859;&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. (arXiv:2301.13826v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;&#65292;&#21517;&#20026;&#21442;&#19982;&#20852;&#22859;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#24178;&#39044;&#29983;&#25104;&#36807;&#31243;&#20197;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#20449;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#35821;&#20041;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22833;&#28789;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#31181;&#26080;&#19982;&#20262;&#27604;&#30340;&#36890;&#36807;&#30446;&#26631;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#25351;&#23548;&#29983;&#25104;&#22810;&#31181;&#22810;&#26679;&#21644;&#23500;&#26377;&#21019;&#36896;&#24615;&#30340;&#24418;&#35937;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#20855;&#26377;&#38761;&#21629;&#24615;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#20173;&#21487;&#33021;&#22312;&#29983;&#25104;&#23436;&#20840;&#20256;&#36798;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#35821;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#22833;&#36133;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20844;&#24320;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#24573;&#35270;&#30340;&#23384;&#22312;&#65292;&#21363;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#36824;&#26410;&#33021;&#23558;&#23646;&#24615;&#65288;&#20363;&#22914;&#39068;&#33394;&#65289;&#27491;&#30830;&#32465;&#23450;&#21040;&#20854;&#30456;&#24212;&#30340;&#20027;&#39064;&#19978;&#12290;&#20026;&#20102;&#24110;&#21161;&#20943;&#36731;&#36825;&#20123;&#22833;&#36133;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20135;&#29983;&#24335;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#23547;&#27714;&#24178;&#39044;&#29983;&#25104;&#36807;&#31243;&#20197;&#25913;&#21892;&#25152;&#29983;&#25104;&#22270;&#20687;&#30340;&#20449;&#23454;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340; GSN &#20844;&#24335;&#65292;&#34987;&#31216;&#20026;&#21442;&#19982;&#20852;&#22859;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#25913;&#36827;&#36328;&#27880;&#24847;&#21147;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention un
&lt;/p&gt;</description></item><item><title>UPop&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.13741</link><description>&lt;p&gt;
UPop&#65306;&#29992;&#20110;&#21387;&#32553;&#35270;&#35273;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13741
&lt;/p&gt;
&lt;p&gt;
UPop&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#21644;&#28176;&#36827;&#24335;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20854;&#20013;&#35270;&#35273;&#21644;&#35821;&#35328;&#26159;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#20004;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#37325;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;Transformer&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#27169;&#22411;&#21387;&#32553;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21387;&#32553;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;&#35821;&#35328;Transformer&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UPop&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;Transformer&#21387;&#32553;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;1&#65289;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#22312;&#36830;&#32493;&#20248;&#21270;&#31354;&#38388;&#20013;&#32479;&#19968;&#25628;&#32034;&#22810;&#27169;&#24577;&#23376;&#32593;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#21387;&#32553;&#27169;&#24577;&#21644;&#32467;&#26500;&#20043;&#38388;&#33258;&#21160;&#20998;&#37197;&#21098;&#26525;&#27604;&#29575;&#65307;2&#65289;&#28176;&#36827;&#24335;&#25628;&#32034;&#21644;&#24494;&#35843;&#23376;&#32593;&#65292;&#20174;&#32780;&#20445;&#25345;&#25628;&#32034;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#25910;&#25947;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \textit{e}.\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive \textbf{P}runing (\textbf{\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model archit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MESH&#65292;&#19968;&#20010;&#20132;&#21449;&#20851;&#27880;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#38145; Slot Attention &#23545;&#20110;&#21160;&#24577;&#25968;&#37327;&#29289;&#20307;&#35270;&#39057;&#24314;&#27169;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.13197</link><description>&lt;p&gt;
&#20462;&#25913;&#26368;&#20248;&#20256;&#36755;&#25104;&#26412;&#20197;&#35299;&#38145; Slot Attention &#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking Slot Attention by Changing Optimal Transport Costs. (arXiv:2301.13197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MESH&#65292;&#19968;&#20010;&#20132;&#21449;&#20851;&#27880;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#38145; Slot Attention &#23545;&#20110;&#21160;&#24577;&#25968;&#37327;&#29289;&#20307;&#35270;&#39057;&#24314;&#27169;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Slot Attention &#26159;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#36827;&#34892;&#29289;&#20307;&#20013;&#24515;&#24314;&#27169;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#38598;&#21512;&#31561;&#21464;&#24615;&#38480;&#21046;&#20102;&#22788;&#29702;&#21160;&#24577;&#25968;&#37327;&#29289;&#20307;&#35270;&#39057;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#25171;&#30772;&#24182;&#21015;&#20851;&#31995;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#20316;&#32773;&#39318;&#20808;&#24314;&#31435;&#20102; Slot Attention &#19982;&#26368;&#20248;&#20256;&#36755;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#20010;&#26032;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102; MESH&#65288;Minimize Entropy of Sinkhorn&#65289;&#65306;&#19968;&#20010;&#20132;&#21449;&#20851;&#27880;&#27169;&#22359;&#65292;&#23427;&#23558;&#26410;&#35268;&#33539;&#21270;&#30340;&#26368;&#20248;&#20256;&#36755;&#30340;&#25171;&#30772;&#24182;&#21015;&#20851;&#31995;&#30340;&#29305;&#24615;&#19982;&#35268;&#33539;&#21270;&#26368;&#20248;&#20256;&#36755;&#30340;&#36895;&#24230;&#32467;&#21512;&#36215;&#26469;&#12290;&#20316;&#32773;&#20351;&#29992; MESH &#35780;&#20272;&#20102; Slot Attention &#24182;&#22312;&#22810;&#20010;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#29616;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#19982; Slot Attention &#30456;&#27604;&#26377;&#26174;&#30528;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Slot attention is a powerful method for object-centric modeling in images and videos. However, its set-equivariance limits its ability to handle videos with a dynamic number of objects because it cannot break ties. To overcome this limitation, we first establish a connection between slot attention and optimal transport. Based on this new perspective we propose MESH (Minimize Entropy of Sinkhorn): a cross-attention module that combines the tiebreaking properties of unregularized optimal transport with the speed of regularized optimal transport. We evaluate slot attention using MESH on multiple object-centric learning benchmarks and find significant improvements over slot attention in every setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HPRL&#30340;&#20998;&#23618;&#32534;&#31243;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31243;&#24207;&#32452;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#22312;&#35780;&#20272;&#20505;&#36873;&#26041;&#26696;&#26102;&#21487;&#20197;&#20934;&#30830;&#22870;&#21169;&#21644;&#24809;&#32602;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2301.12950</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31243;&#24207;&#32452;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#20998;&#23618;&#32534;&#31243;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs. (arXiv:2301.12950v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HPRL&#30340;&#20998;&#23618;&#32534;&#31243;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31243;&#24207;&#32452;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#22312;&#35780;&#20272;&#20505;&#36873;&#26041;&#26696;&#26102;&#21487;&#20197;&#20934;&#30830;&#22870;&#21169;&#21644;&#24809;&#32602;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Trivedi&#31561;&#20154;(2021)&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;(LEAPS)&#65292;&#26088;&#22312;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#22330;&#26223;&#30340;&#21152;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#12290;&#26041;&#27861;&#20808;&#23398;&#20064;&#19968;&#20010;&#31243;&#24207;&#23884;&#20837;&#31354;&#38388;&#65292;&#20197;&#36830;&#32493;&#21442;&#25968;&#21270;&#26469;&#33258;&#39044;&#29983;&#25104;&#30340;&#31243;&#24207;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#21270;&#31243;&#24207;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#20219;&#21153;&#26102;&#22312;&#23398;&#20064;&#30340;&#31243;&#24207;&#23884;&#20837;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#20219;&#21153;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pre-generated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite the encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#32452;&#21512;&#21333;&#20010;&#22266;&#23450;&#22823;&#23567;RELU&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#24778;&#20154;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36924;&#36817;&#20855;&#26377;$1-$Lipschitz&#36830;&#32493;&#24615;&#21644;&#19968;&#33324;&#36830;&#32493;&#24615;&#30340;&#20989;&#25968;&#26102;&#12290;</title><link>http://arxiv.org/abs/2301.12353</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#20010;&#22266;&#23450;&#22823;&#23567;RELU&#32593;&#32476;&#30340;&#32452;&#21512;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On Enhancing Expressive Power via Compositions of Single Fixed-Size ReLU Network. (arXiv:2301.12353v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#32452;&#21512;&#21333;&#20010;&#22266;&#23450;&#22823;&#23567;RELU&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#24778;&#20154;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36924;&#36817;&#20855;&#26377;$1-$Lipschitz&#36830;&#32493;&#24615;&#21644;&#19968;&#33324;&#36830;&#32493;&#24615;&#30340;&#20989;&#25968;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20989;&#25968;&#32452;&#21512;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37325;&#22797;&#32452;&#21512;&#21333;&#20010;&#22266;&#23450;&#22823;&#23567;RELU&#32593;&#32476;&#30340;&#24778;&#20154;&#34920;&#36798;&#33021;&#21147;&#65292;&#23613;&#31649;&#21333;&#20010;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#27492;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;$[0,1]^d$&#19978;&#30340;&#19968;&#33324;&#36830;&#32493;&#20989;&#25968;&#65292;&#20854;&#36924;&#36817;&#35823;&#24046;&#30001;&#36830;&#32493;&#24615;&#27169;&#37327;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the expressive power of deep neural networks through the framework of function compositions. We demonstrate that the repeated compositions of a single fixed-size ReLU network exhibit surprising expressive power, despite the limited expressive capabilities of the individual network itself. Specifically, we prove by construction that $\mathcal{L}_2\circ \boldsymbol{g}^{\circ r}\circ \boldsymbol{\mathcal{L}}_1$ can approximate $1$-Lipschitz continuous functions on $[0,1]^d$ with an error $\mathcal{O}(r^{-1/d})$, where $\boldsymbol{g}$ is realized by a fixed-size ReLU network, $\boldsymbol{\mathcal{L}}_1$ and $\mathcal{L}_2$ are two affine linear maps matching the dimensions, and $\boldsymbol{g}^{\circ r}$ denotes the $r$-times composition of $\boldsymbol{g}$. Furthermore, we extend such a result to generic continuous functions on $[0,1]^d$ with the approximation error characterized by the modulus of continuity. Our results reveal that a continuous-depth network generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;INT4&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;W4A4&#32534;&#30721;&#22120;&#25512;&#26029;&#31649;&#36947;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;&#20351;&#29992;W4A4&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#22312;&#24310;&#36831;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2301.12017</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#27169;&#22411;&#30340;INT4&#37327;&#21270;&#65306;&#24310;&#36831;&#36895;&#24230;&#25552;&#21319;&#12289;&#21487;&#32452;&#21512;&#24615;&#21644;&#25925;&#38556;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. (arXiv:2301.12017v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;INT4&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;W4A4&#32534;&#30721;&#22120;&#25512;&#26029;&#31649;&#36947;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;&#20351;&#29992;W4A4&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#22312;&#24310;&#36831;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;Transformer&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#25552;&#39640;&#20854;&#37096;&#32626;&#25928;&#29575;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#20102;INT8&#37327;&#21270;&#22312;&#20943;&#23569;&#20869;&#23384;&#25104;&#26412;&#21644;&#24310;&#36831;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;INT4&#65288;&#21487;&#20197;&#20351;&#30828;&#20214;&#23792;&#20540;&#21534;&#21520;&#37327;&#22686;&#21152;&#19968;&#20493;&#65289;&#26469;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#24310;&#36831;&#25913;&#36827;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;INT4&#26435;&#37325;&#21644;&#28608;&#27963;&#65288;W4A4&#65289;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23545;&#20110;&#20165;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;W4A4&#37327;&#21270;&#24341;&#20837;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#20294;&#23545;&#20110;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#32780;&#35328;&#65292;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#20351;&#29992;W4A4&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#24230;&#20248;&#21270;&#30340;&#31471;&#21040;&#31471;W4A4&#32534;&#30721;&#22120;&#25512;&#26029;&#31649;&#36947;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;INT4&#31649;&#36947;&#22312;&#38754;&#21521;&#24310;&#36831;&#30340;&#22330;&#26223;&#19979;&#30340;&#36895;&#24230;&#21487;&#20197;&#25552;&#39640;8.5&#20493;&#65292;&#22312;&#20854;&#20182;&#22330;&#26223;&#19979;&#26368;&#22810;&#21487;&#20197;&#25552;&#39640;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\times$ faster for latency-oriented scenarios and up to $3\times$ for t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11716</link><description>&lt;p&gt;
&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;(ST)&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#26080;&#38656;&#25913;&#21464;ST&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#34920;&#26126;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;(CTC)&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26469;&#20943;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#36890;&#36807;&#19982;&#26356;&#24120;&#35265;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22987;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#26368;&#32456;ST&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#36825;&#31181;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#39044;&#35757;&#32451;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#27809;&#26377;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#22343;&#33021;&#22815;&#25552;&#20379;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gap between speech and text modalities is a major challenge in speech-to-text translation (ST). Different methods have been proposed to reduce this gap, but most of them require architectural changes in ST training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the ST model. First, we show that the connectionist temporal classification (CTC) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with CTC consistently achieves better final ST accuracy. Nevertheless, CTC is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining CTC and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wassers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#22270;&#20687;&#24674;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22343;&#20540;&#22238;&#24402;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23558;&#19968;&#20010;&#39640;&#36136;&#37327;&#22270;&#20687;&#36716;&#25442;&#20026;&#38477;&#32423;&#22270;&#20687;&#24182;&#21152;&#20837;&#39640;&#26031;&#22122;&#22768;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#30446;&#26631;&#26469;&#23398;&#20064;&#36870;&#36716;&#36712;&#36857;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#29305;&#23450;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#20302;&#36136;&#37327;&#22270;&#20687;&#30340;&#21407;&#22987;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2301.11699</link><description>&lt;p&gt;
&#20351;&#29992;&#22343;&#20540;&#22238;&#24402;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Image Restoration with Mean-Reverting Stochastic Differential Equations. (arXiv:2301.11699v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#22270;&#20687;&#24674;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22343;&#20540;&#22238;&#24402;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23558;&#19968;&#20010;&#39640;&#36136;&#37327;&#22270;&#20687;&#36716;&#25442;&#20026;&#38477;&#32423;&#22270;&#20687;&#24182;&#21152;&#20837;&#39640;&#26031;&#22122;&#22768;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#30446;&#26631;&#26469;&#23398;&#20064;&#36870;&#36716;&#36712;&#36857;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#29305;&#23450;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#20302;&#36136;&#37327;&#22270;&#20687;&#30340;&#21407;&#22987;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#22270;&#20687;&#24674;&#22797;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26041;&#27861;&#12290;&#20027;&#35201;&#26500;&#36896;&#26159;&#20351;&#29992;&#19968;&#31181;&#22343;&#20540;&#22238;&#24402;SDE&#23558;&#39640;&#36136;&#37327;&#22270;&#20687;&#36716;&#25442;&#20026;&#22343;&#20540;&#29366;&#24577;&#19979;&#30340;&#38477;&#32423;&#22270;&#20687;&#24182;&#21152;&#20837;&#22266;&#23450;&#39640;&#26031;&#22122;&#22768;&#12290;&#28982;&#21518;&#36890;&#36807;&#27169;&#25311;&#30456;&#24212;&#30340;&#36870;&#36716;&#26102;&#38388;SDE&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#29305;&#23450;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#20302;&#36136;&#37327;&#22270;&#20687;&#30340;&#21407;&#22987;&#29366;&#24577;&#12290;&#20851;&#38190;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#22343;&#20540;&#22238;&#24402;SDE&#26377;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35745;&#31639;&#20986;&#30495;&#23454;&#30340;&#26102;&#38388;&#20381;&#36182;&#24471;&#20998;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#22823;&#20284;&#28982;&#30446;&#26631;&#26469;&#23398;&#20064;&#19968;&#20010;&#26368;&#20248;&#30340;&#36870;&#36716;&#36712;&#36857;&#65292;&#20174;&#32780;&#31283;&#23450;&#35757;&#32451;&#24182;&#25913;&#36827;&#24674;&#22797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a stochastic differential equation (SDE) approach for general-purpose image restoration. The key construction consists in a mean-reverting SDE that transforms a high-quality image into a degraded counterpart as a mean state with fixed Gaussian noise. Then, by simulating the corresponding reverse-time SDE, we are able to restore the origin of the low-quality image without relying on any task-specific prior knowledge. Crucially, the proposed mean-reverting SDE has a closed-form solution, allowing us to compute the ground truth time-dependent score and learn it with a neural network. Moreover, we propose a maximum likelihood objective to learn an optimal reverse trajectory that stabilizes the training and improves the restoration results. The experiments show that our proposed method achieves highly competitive performance in quantitative comparisons on image deraining, deblurring, and denoising, setting a new state-of-the-art on two deraining datasets. Finally, the ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#31232;&#30095;&#35266;&#27979;&#20132;&#20114;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#35299;&#30340;&#23398;&#20064;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;CDE&#65289;&#65292;&#21033;&#29992;&#31614;&#21517;&#29702;&#35770;&#23558;&#38750;&#32447;&#24615;&#38382;&#39064;&#36716;&#21270;&#20026;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#20381;&#36182;&#20110;&#20010;&#20307;&#29305;&#23450;&#37319;&#26679;&#26041;&#26696;&#30340;&#39044;&#27979;&#35823;&#24046;&#30340;oracle&#30028;&#38480;&#12290;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#22238;&#25910;&#23436;&#25972;&#26102;&#38388;&#24207;&#21015;&#65292;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2301.11647</link><description>&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#35266;&#27979;&#20132;&#20114;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning the Dynamics of Sparsely Observed Interacting Systems. (arXiv:2301.11647v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#31232;&#30095;&#35266;&#27979;&#20132;&#20114;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#35299;&#30340;&#23398;&#20064;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;CDE&#65289;&#65292;&#21033;&#29992;&#31614;&#21517;&#29702;&#35770;&#23558;&#38750;&#32447;&#24615;&#38382;&#39064;&#36716;&#21270;&#20026;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#20381;&#36182;&#20110;&#20010;&#20307;&#29305;&#23450;&#37319;&#26679;&#26041;&#26696;&#30340;&#39044;&#27979;&#35823;&#24046;&#30340;oracle&#30028;&#38480;&#12290;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#22238;&#25910;&#23436;&#25972;&#26102;&#38388;&#24207;&#21015;&#65292;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#26410;&#30693;&#30340;&#38750;&#21442;&#25968;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#35813;&#31995;&#32479;&#23558;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#21644;&#29305;&#24449;&#26102;&#38388;&#24207;&#21015;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#24449;&#26102;&#38388;&#24207;&#21015;&#22312;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#30340;&#32593;&#26684;&#19978;&#27979;&#37327;&#65292;&#32780;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#20123;&#28857;&#12290;&#23398;&#20064;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#21160;&#24577;&#23558;&#29305;&#24449;&#26102;&#38388;&#24207;&#21015;&#30340;&#21069;&#20960;&#20010;&#26102;&#38388;&#28857;&#26469;&#39044;&#27979;&#30446;&#26631;&#30340;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#20316;&#20026;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;CDE&#65289;&#35299;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#31614;&#21517;&#29702;&#35770;&#30340;&#20016;&#23500;&#29702;&#35770;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#36825;&#20010;&#38750;&#32447;&#24615;&#38382;&#39064;&#36716;&#21270;&#20026;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#39044;&#27979;&#35823;&#24046;&#30340;oracle&#30028;&#38480;&#65292;&#20854;&#20855;&#26377;&#26126;&#30830;&#30340;&#20381;&#36182;&#20110;&#20010;&#20307;&#29305;&#23450;&#37319;&#26679;&#26041;&#26696;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#27169;&#25311;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22238;&#25910;&#23436;&#25972;&#26102;&#38388;&#24207;&#21015;&#26102;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;&#25105;&#20204;&#26368;&#21518;&#23637;&#31034;&#20102;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#27969;&#34892;&#30149;&#23398;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of learning the dynamics of an unknown non-parametric system linking a target and a feature time series. The feature time series is measured on a sparse and irregular grid, while we have access to only a few points of the target time series. Once learned, we can use these dynamics to predict values of the target from the previous values of the feature time series. We frame this task as learning the solution map of a controlled differential equation (CDE). By leveraging the rich theory of signatures, we are able to cast this non-linear problem as a high-dimensional linear regression. We provide an oracle bound on the prediction error which exhibits explicit dependencies on the individual-specific sampling schemes. Our theoretical results are illustrated by simulations which show that our method outperforms existing algorithms for recovering the full time series while being computationally cheap. We conclude by demonstrating its potential on real-world epidemiologi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#22686;&#24378;&#21644;&#22870;&#21169;&#24809;&#32602;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26032;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#20445;&#35777;&#23433;&#20840;&#24615;&#21644;&#30446;&#26631;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.11592</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#22686;&#24378;&#21644;&#22870;&#21169;&#24809;&#32602;&#35299;&#20915;&#22797;&#26434;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Richly Constrained Reinforcement Learning through State Augmentation and Reward Penalties. (arXiv:2301.11592v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#22686;&#24378;&#21644;&#22870;&#21169;&#24809;&#32602;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26032;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#20445;&#35777;&#23433;&#20840;&#24615;&#21644;&#30446;&#26631;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#39044;&#26399;&#25104;&#26412;&#32422;&#26463;&#26469;&#25191;&#34892;&#31574;&#30053;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#20854;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22788;&#29702;&#20351;&#29992;&#31574;&#30053;&#31215;&#32047;&#30340;&#39044;&#26399;&#25104;&#26412;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#27493;&#39588;&#20013;&#30340;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#23558;&#25972;&#20010;&#31574;&#30053;&#30340;&#25104;&#26412;&#32422;&#26463;&#36716;&#25442;&#20026;&#26412;&#22320;&#20915;&#31574;&#65288;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#65289;&#32422;&#26463;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#23458;&#35266;&#19978;&#25552;&#20379;&#20102;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#21487;&#33021;&#22312;&#25104;&#26412;&#26041;&#38754;&#36807;&#20110;&#28608;&#36827;&#25110;&#36807;&#20110;&#20445;&#23432;&#12290;&#36825;&#26159;&#30001;&#20110;&#22312;&#26412;&#22320;&#25104;&#26412;&#32422;&#26463;&#20013;&#20351;&#29992;&#20102;&#8220;&#26410;&#26469;&#8221;&#25110;&#8220;&#21518;&#21521;&#8221;&#25104;&#26412;&#30340;&#20272;&#35745;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#30340;&#26080;&#32422;&#26463;RL&#20844;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#24378;&#29366;&#24577;&#31354;&#38388;&#21644;&#22870;&#21169;&#24809;&#32602;&#12290;&#36825;&#31181;&#30452;&#35266;&#30340;&#20844;&#24335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#36259;&#30340;&#29702;&#35770;&#23646;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20026;&#26377;&#25928;&#35299;&#20915;&#32422;&#26463;RL&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#33539;&#20363;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#23454;&#39564;&#32467;&#26524;&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#24615;&#21644;&#30446;&#26631;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning has been employed to enforce safety constraints on policy through the use of expected cost constraints. The key challenge is in handling expected cost accumulated using the policy and not just in a single step. Existing methods have developed innovative ways of converting this cost constraint over entire policy to constraints over local decisions (at each time step). While such approaches have provided good solutions with regards to objective, they can either be overly aggressive or conservative with respect to costs. This is owing to use of estimates for "future" or "backward" costs in local cost constraints.  To that end, we provide an equivalent unconstrained formulation to constrained RL that has an augmented state space and reward penalties. This intuitive formulation is general and has interesting theoretical properties. More importantly, this provides a new paradigm for solving constrained RL problems effectively. As we show in our experimental
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;3D-aware&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#20110;&#20687;&#32032;&#30340;&#20197;&#21450;&#26368;&#26032;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11520</link><description>&lt;p&gt;
SNeRL: &#35821;&#20041;&#24863;&#30693;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning. (arXiv:2301.11520v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;3D-aware&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#20110;&#20687;&#32032;&#30340;&#20197;&#21450;&#26368;&#26032;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#24456;&#38590;&#26377;&#25928;&#22320;&#34701;&#21512;&#20154;&#31867;&#30452;&#35266;&#30340;3D&#29615;&#22659;&#29702;&#35299;&#65292;&#22240;&#27492;&#32463;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21367;&#31215;&#32534;&#30721;&#22120;&#21644;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#26469;&#20174;&#22810;&#35270;&#35282;&#22270;&#20687;&#20013;&#23398;&#20064;3D&#24863;&#30693;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;NeRF&#20013;&#24341;&#20837;&#20102;3D&#35821;&#20041;&#21644;&#33976;&#39311;&#29305;&#24449;&#22330;&#65292;&#24182;&#19982;RGB&#36752;&#23556;&#22330;&#24182;&#34892;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#21644;&#23545;&#35937;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#12290;SNeRL&#22312;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#20165;&#20248;&#20110;&#20197;&#24448;&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#36824;&#20248;&#20110;&#26368;&#36817;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36712;&#36857;&#24863;&#30693;&#30340;&#36164;&#26684;&#36861;&#36394;&#22810;&#27493;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#21516;&#26102;&#34920;&#36798;&#27599;&#20010;&#20915;&#31574;&#21644;&#36712;&#36857;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#34987;&#23436;&#20840;&#35009;&#21098;&#30340;&#36164;&#26684;&#36861;&#36394;&#26080;&#27861;&#36870;&#36716;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11321</link><description>&lt;p&gt;
&#36712;&#36857;&#24863;&#30693;&#30340;&#36164;&#26684;&#36861;&#36394;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning. (arXiv:2301.11321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36712;&#36857;&#24863;&#30693;&#30340;&#36164;&#26684;&#36861;&#36394;&#22810;&#27493;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#21516;&#26102;&#34920;&#36798;&#27599;&#20010;&#20915;&#31574;&#21644;&#36712;&#36857;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#34987;&#23436;&#20840;&#35009;&#21098;&#30340;&#36164;&#26684;&#36861;&#36394;&#26080;&#27861;&#36870;&#36716;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#22810;&#27493;&#36820;&#22238;&#30340;&#38750;&#25919;&#31574;&#23398;&#20064;&#23545;&#20110;&#33410;&#32422;&#26679;&#26412;&#30340;&#24378;&#21270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25269;&#28040;&#20559;&#24046;&#30340;&#21516;&#26102;&#19981;&#21152;&#21095;&#26041;&#24046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#38750;&#25919;&#31574;&#20559;&#24046;&#26159;&#36890;&#36807;&#36164;&#26684;&#36861;&#36394;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20462;&#27491;&#30340;&#65292;&#36164;&#26684;&#36861;&#36394;&#36890;&#36807;&#36890;&#21507;&#22240;&#23376;(Impotance Sampling)&#27604;&#20363;&#23545;&#36807;&#21435;&#30340;&#26102;&#38388;&#24046;&#20998;&#35823;&#24046;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#20197;&#32416;&#27491;&#12290;&#35768;&#22810;&#31163;&#32447;&#31639;&#27861;&#37117;&#20381;&#36182;&#36825;&#31181;&#26426;&#21046;&#65292;&#19981;&#21516;&#30340;&#26159;&#38024;&#23545;IS&#30340;&#32479;&#35745;&#20272;&#35745;&#26041;&#27861;&#25152;&#37319;&#29992;&#30340;&#8220;&#35009;&#21098;IS&#27604;&#20363;&#8221;&#21327;&#35758;&#30340;&#19981;&#21516;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#26086;&#36164;&#26684;&#36861;&#36394;&#34987;&#23436;&#20840;&#35009;&#21098;&#65292;&#20854;&#24433;&#21709;&#23601;&#26080;&#27861;&#36870;&#36716;&#12290;&#36825;&#24050;&#32463;&#23548;&#33268;&#20102;&#23558;&#22810;&#20010;&#36807;&#21435;&#32463;&#21382;&#21516;&#26102;&#32771;&#34385;&#22312;&#20869;&#30340;&#20449;&#29992;&#20998;&#37197;&#31574;&#30053;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#36712;&#36857;&#24863;&#30693;&#30340;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#20381;&#25454;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#21516;&#26102;&#34920;&#36798;&#27599;&#20010;&#20915;&#31574;&#21644;&#36712;&#36857;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#30340;&#25910;&#25947;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios to combat the variance of the IS estimator. Unfortunately, once a trace has been fully cut, the effect cannot be reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time. These trajectory-aware methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose a multistep operator that can express both per-decision and trajectory-aware methods. We prove convergence conditi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#29305;&#24449;&#30340;&#24674;&#22797;&#65292;&#24182;&#34920;&#26126;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.10956</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20165;&#20165;&#20174;&#22270;&#32467;&#26500;&#20013;&#24674;&#22797;&#20986;&#38544;&#34255;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure. (arXiv:2301.10956v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#29305;&#24449;&#30340;&#24674;&#22797;&#65292;&#24182;&#34920;&#26126;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#22788;&#29702;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#23427;&#22312;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20854;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#38416;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#35282;&#24230;&#30740;&#31350;&#20102;GNN&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#32467;&#26500;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21463;&#38544;&#34255;&#65288;&#25110;&#28508;&#22312;&#65289;&#33410;&#28857;&#29305;&#24449;&#25511;&#21046;&#30340;&#22270;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#20123;&#29305;&#24449;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#36825;&#31181;&#26694;&#26550;&#30340;&#20856;&#22411;&#31034;&#20363;&#26159;&#20174;&#38544;&#34255;&#29305;&#24449;&#26500;&#24314;&#30340;kNN&#22270;&#12290;&#22312;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;GNN&#21487;&#20197;&#20165;&#20174;&#36755;&#20837;&#22270;&#20013;&#24674;&#22797;&#20986;&#38544;&#34255;&#33410;&#28857;&#29305;&#24449;&#65292;&#21363;&#20351;&#25152;&#26377;&#33410;&#28857;&#29305;&#24449;&#65292;&#21253;&#25324;&#38544;&#34255;&#29305;&#24449;&#26412;&#36523;&#21644;&#20219;&#20309;&#38388;&#25509;&#25552;&#31034;&#37117;&#19981;&#21487;&#29992;&#12290;GNN&#21487;&#20197;&#36827;&#19968;&#27493;&#21033;&#29992;&#24674;&#22797;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#23436;&#20840;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#22270;&#32467;&#26500;&#33258;&#36523;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are popular models for graph learning problems. GNNs show strong empirical performance in many practical tasks. However, the theoretical properties have not been completely elucidated. In this paper, we investigate whether GNNs can exploit the graph structure from the perspective of the expressive power of GNNs. In our analysis, we consider graph generation processes that are controlled by hidden (or latent) node features, which contain all information about the graph structure. A typical example of this framework is kNN graphs constructed from the hidden features. In our main results, we show that GNNs can recover the hidden node features from the input graph alone, even when all node features, including the hidden features themselves and any indirect hints, are unavailable. GNNs can further use the recovered node features for downstream tasks. These results show that GNNs can fully exploit the graph structure by themselves, and in effect, GNNs can use bot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#38480;&#21046;&#22270;&#30340;&#24102;&#23485;&#26469;&#25552;&#39640;&#29616;&#26377;&#22270;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#21487;&#25193;&#23637;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#22312;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#25110;&#20943;&#23569;&#34920;&#36798;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#19982;&#29616;&#26377;&#22270;&#29983;&#25104;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2301.10857</link><description>&lt;p&gt;
&#38480;&#21046;&#22270;&#24102;&#23485;&#26469;&#25552;&#39640;&#22270;&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improving Graph Generation by Restricting Graph Bandwidth. (arXiv:2301.10857v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#38480;&#21046;&#22270;&#30340;&#24102;&#23485;&#26469;&#25552;&#39640;&#29616;&#26377;&#22270;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#21487;&#25193;&#23637;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#22312;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#25110;&#20943;&#23569;&#34920;&#36798;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#19982;&#29616;&#26377;&#22270;&#29983;&#25104;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#29983;&#25104;&#24314;&#27169;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#23398;&#20064;&#34920;&#24449;&#30495;&#23454;&#19990;&#30028;&#22270;&#24418;&#30340;&#22797;&#26434;&#22810;&#23610;&#24230;&#32467;&#26500;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#20043;&#19968;&#26159;&#23427;&#20204;&#30340;&#22823;&#36755;&#20986;&#31354;&#38388;&#65292;&#36825;&#38480;&#21046;&#20102;&#29983;&#25104;&#21487;&#25193;&#23637;&#24615;&#24182;&#38459;&#30861;&#20102;&#22522;&#30784;&#20998;&#24067;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26174;&#30528;&#20943;&#23569;&#20102;&#29616;&#26377;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#31354;&#38388;&#12290;&#20855;&#20307;&#22320;&#65292;&#20174;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#22270;&#24418;&#20855;&#26377;&#20302;&#22270;&#24102;&#23485;&#30340;&#35266;&#23519;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#38480;&#21046;&#22270;&#24102;&#23485;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36136;&#37327;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#26550;&#26500;&#22797;&#26434;&#24615;&#25110;&#20943;&#23567;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#20860;&#23481;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#22312;&#33258;&#22238;&#24402;&#21644;&#21333;&#27425;&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39564;&#35777;&#65292;&#21253;&#25324;&#20998;&#23376;&#32467;&#26500;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph generative modeling has proven capable of learning the distribution of complex, multi-scale structures characterizing real-world graphs. However, one of the main limitations of existing methods is their large output space, which limits generation scalability and hinders accurate modeling of the underlying distribution. To overcome these limitations, we propose a novel approach that significantly reduces the output space of existing graph generative models. Specifically, starting from the observation that many real-world graphs have low graph bandwidth, we restrict graph bandwidth during training and generation. Our strategy improves both generation scalability and quality without increasing architectural complexity or reducing expressiveness. Our approach is compatible with existing graph generative methods, and we describe its application to both autoregressive and one-shot models. We extensively validate our strategy on synthetic and real datasets, including molecular grap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#65292;&#20351;&#29992;&#38750;&#25345;&#32493;&#36816;&#34892;&#30340;EBM&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31934;&#30830;&#30340;&#21160;&#24577;&#36807;&#31243;&#23436;&#32654;&#22320;&#22797;&#21046;&#25968;&#25454;&#38598;&#30340;&#19968;&#32452;&#32479;&#35745;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#24179;&#34913;&#27979;&#24230;&#30340;&#27700;&#24179;&#65292;&#36825;&#20026;&#20351;&#29992;EBM&#20316;&#20026;&#25193;&#25955;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2301.09428</link><description>&lt;p&gt;
&#35299;&#37322;&#38750;&#25910;&#25947;&#37319;&#26679;&#23545;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;(Energy-Based Models, EBMs)&#35757;&#32451;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Explaining the effects of non-convergent sampling in the training of Energy-Based Models. (arXiv:2301.09428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#65292;&#20351;&#29992;&#38750;&#25345;&#32493;&#36816;&#34892;&#30340;EBM&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31934;&#30830;&#30340;&#21160;&#24577;&#36807;&#31243;&#23436;&#32654;&#22320;&#22797;&#21046;&#25968;&#25454;&#38598;&#30340;&#19968;&#32452;&#32479;&#35745;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#24179;&#34913;&#27979;&#24230;&#30340;&#27700;&#24179;&#65292;&#36825;&#20026;&#20351;&#29992;EBM&#20316;&#20026;&#25193;&#25955;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#20351;&#29992;&#38750;&#25910;&#25947;&#39532;&#23572;&#21487;&#22827;&#38142;(Markov chains)&#35757;&#32451;EBMs&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#30701;&#30340;&#38750;&#25345;&#32493;&#36816;&#34892;&#26469;&#20272;&#35745;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#30340;EBMs&#33021;&#22815;&#36890;&#36807;&#31934;&#30830;&#30340;&#21160;&#24577;&#36807;&#31243;&#23436;&#32654;&#22797;&#21046;&#25968;&#25454;&#30340;&#19968;&#32452;&#32479;&#35745;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#24179;&#34913;&#27979;&#24230;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#26368;&#36817;&#25552;&#20986;&#30340;&#20351;&#29992;&#36215;&#22987;&#20110;&#38543;&#26426;&#21021;&#24577;&#30340;&#30701;&#36305;&#20316;&#20026;EBMs&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#26377;&#25928;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20010;&#26681;&#25454;&#65292;&#20026;&#20351;&#29992;EBMs&#20316;&#20026;&#25193;&#25955;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#35299;&#37322;&#20102;&#36825;&#31181;&#25928;&#26524;&#22312;&#36890;&#29992;EBMs&#20013;&#30340;&#24773;&#20917;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#21487;&#35299;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#25551;&#36848;&#20102;&#35757;&#32451;&#21442;&#25968;&#20013;&#38750;&#25910;&#25947;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;ConvNet EBM&#21644;Boltzmann&#26426;&#22120;&#19978;&#36827;&#34892;&#20102;&#25968;&#23383;&#39044;&#27979;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we quantify the impact of using non-convergent Markov chains to train Energy-Based models (EBMs). In particular, we show analytically that EBMs trained with non-persistent short runs to estimate the gradient can perfectly reproduce a set of empirical statistics of the data, not at the level of the equilibrium measure, but through a precise dynamical process. Our results provide a first-principles explanation for the observations of recent works proposing the strategy of using short runs starting from random initial conditions as an efficient way to generate high-quality samples in EBMs, and lay the groundwork for using EBMs as diffusion models. After explaining this effect in generic EBMs, we analyze two solvable models in which the effect of the non-convergent sampling in the trained parameters can be described in detail. Finally, we test these predictions numerically on a ConvNet EBM and a Boltzmann machine.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35745;&#31639;&#39640;&#25928;&#19977;&#32500;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#28151;&#21512;&#20351;&#29992;CNN&#21644;dMLP&#27169;&#22359;&#12290;&#30456;&#27604;&#20110;&#24403;&#21069;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#36739;&#22823;&#30340;&#22270;&#20687;&#23610;&#23544;&#21644;GPU&#20869;&#23384;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.08868</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35745;&#31639;&#39640;&#25928;&#19977;&#32500;MRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP. (arXiv:2301.08868v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35745;&#31639;&#39640;&#25928;&#19977;&#32500;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#28151;&#21512;&#20351;&#29992;CNN&#21644;dMLP&#27169;&#22359;&#12290;&#30456;&#27604;&#20110;&#24403;&#21069;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#36739;&#22823;&#30340;&#22270;&#20687;&#23610;&#23544;&#21644;GPU&#20869;&#23384;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#20108;&#32500;MRI&#65292;&#19977;&#32500;MRI&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20307;&#31215;&#31354;&#38388;&#20998;&#36776;&#29575;&#21644;&#20449;&#22122;&#27604;&#12290;&#28982;&#32780;&#65292;&#37325;&#24314;&#19977;&#32500;MRI&#22270;&#20687;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#24102;&#26377;&#23567;&#20869;&#26680;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#20294;&#30001;&#20110;&#22270;&#20687;&#23610;&#23544;&#22823;&#21644;GPU&#20869;&#23384;&#38480;&#21046;&#65292;&#36825;&#24456;&#38590;&#25193;&#23637;&#21040;&#36275;&#22815;&#36866;&#24212;&#19977;&#32500;MRI&#37325;&#24314;&#30340;&#35268;&#27169;&#12290;&#27492;&#22806;&#65292;MRI&#37325;&#24314;&#26159;&#19968;&#20010;&#21453;&#21367;&#31215;&#38382;&#39064;&#65292;&#38656;&#35201;&#25429;&#25417;CNN&#38590;&#20197;&#25429;&#25417;&#30340;&#38271;&#36317;&#31163;&#20449;&#24687;&#12290; &#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21487;&#20197;&#27169;&#25311;&#36825;&#31181;&#38271;&#36317;&#31163;&#20449;&#24687;&#65292;&#20294;&#23427;&#38656;&#35201;&#22266;&#23450;&#30340;&#36755;&#20837;&#23610;&#23544;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Recon3DMLP&#65292;&#36825;&#26159;&#19968;&#20010;CNN&#27169;&#22359;&#21644;&#23567;&#20869;&#26680;&#29992;&#20110;&#20302;&#39057;&#37325;&#24314;&#65292;&#33258;&#36866;&#24212;MLP&#65288;dMLP&#65289;&#27169;&#22359;&#21644;&#22823;&#20869;&#26680;&#29992;&#20110;&#25552;&#21319;&#39640;&#39057;&#37325;&#24314;&#30340;&#28151;&#21512;&#20307;&#65292;&#29992;&#20110;&#19977;&#32500;MRI&#37325;&#24314;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#20102;&#22522;&#20110;MRI&#29289;&#29702;&#23398;&#30340;&#24490;&#29615;&#20301;&#31227;&#25805;&#20316;&#65292;&#20174;&#32780;...&#65288;&#21407;&#25991;&#26410;&#23436;&#65289;
&lt;/p&gt;
&lt;p&gt;
Compared with 2D MRI, 3D MRI provides superior volumetric spatial resolution and signal-to-noise ratio. However, it is more challenging to reconstruct 3D MRI images. Current methods are mainly based on convolutional neural networks (CNN) with small kernels, which are difficult to scale up to have sufficient fitting power for 3D MRI reconstruction due to the large image size and GPU memory constraint. Furthermore, MRI reconstruction is a deconvolution problem, which demands long-distance information that is difficult to capture by CNNs with small convolution kernels. The multi-layer perceptron (MLP) can model such long-distance information, but it requires a fixed input size. In this paper, we proposed Recon3DMLP, a hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction, for 3D MRI reconstruction. We further utilized the circular shift operation based on MRI physics such that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20351;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#23454;&#26102;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.01201</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#31995;&#32479;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Real-Time Semantic Segmentation on Embedded Systems. (arXiv:2301.01201v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20351;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#23454;&#26102;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#38656;&#35201;&#23454;&#26102;&#39044;&#27979;&#33021;&#21147;&#12290;&#23454;&#26102;&#24212;&#29992;&#30340;&#25361;&#25112;&#34987;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25152;&#21152;&#21095;&#12290;&#34429;&#28982;&#36825;&#20123;&#24179;&#21488;&#19978;&#23454;&#26102;&#26041;&#27861;&#30340;&#24320;&#21457;&#24471;&#21040;&#20102;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#36275;&#22815;&#22320;&#32771;&#34385;&#21040;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#28145;&#23618;&#29305;&#24449;&#25552;&#21462;&#19982;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#23454;&#26102;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful uncertainty on embedded hardware in real-time whilst maintaining predictive performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#36873;&#25321;&#20247;&#21253;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#26368;&#20196;&#20154;&#22256;&#24785;&#30340;&#31572;&#26696;&#21644;&#28151;&#28102;&#27010;&#29575;&#12290;&#22312;&#35813;&#27169;&#22411;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#25512;&#26029;&#31639;&#27861;&#26469;&#25512;&#26029;&#26368;&#26377;&#21487;&#33021;&#30340;&#31572;&#26696;&#21644;&#28151;&#28102;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.00006</link><description>&lt;p&gt;
&#22312;&#22810;&#39033;&#36873;&#25321;&#20247;&#21253;&#20013;&#24674;&#22797;&#21069;&#20004;&#20010;&#31572;&#26696;&#21644;&#28151;&#28102;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing. (arXiv:2301.00006v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#36873;&#25321;&#20247;&#21253;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#26368;&#20196;&#20154;&#22256;&#24785;&#30340;&#31572;&#26696;&#21644;&#28151;&#28102;&#27010;&#29575;&#12290;&#22312;&#35813;&#27169;&#22411;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#25512;&#26029;&#31639;&#27861;&#26469;&#25512;&#26029;&#26368;&#26377;&#21487;&#33021;&#30340;&#31572;&#26696;&#21644;&#28151;&#28102;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26631;&#35760;&#22823;&#37327;&#25968;&#25454;&#30340;&#26377;&#25928;&#24179;&#21488;&#65292;&#20855;&#26377;&#25104;&#26412;&#21644;&#26102;&#38388;&#25928;&#30410;&#12290;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20165;&#24674;&#22797;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#39033;&#36873;&#25321;&#20247;&#21253;&#20219;&#21153;&#65292;&#30446;&#26631;&#19981;&#20165;&#26159;&#24674;&#22797;&#30495;&#23454;&#26631;&#31614;&#65292;&#36824;&#21253;&#25324;&#26368;&#20196;&#20154;&#22256;&#24785;&#30340;&#31572;&#26696;&#21644;&#28151;&#28102;&#27010;&#29575;&#12290;&#26368;&#20196;&#20154;&#22256;&#24785;&#30340;&#31572;&#26696;&#25552;&#20379;&#20102;&#20851;&#20110;&#20219;&#21153;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#25581;&#31034;&#20102;&#38500;&#30495;&#23454;&#31572;&#26696;&#20197;&#22806;&#26368;&#21487;&#20449;&#30340;&#31572;&#26696;&#20197;&#21450;&#23427;&#30340;&#21487;&#20449;&#24230;&#12290;&#20026;&#20102;&#29702;&#35770;&#20998;&#26512;&#36825;&#26679;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#27599;&#20010;&#20219;&#21153;&#26377;&#20004;&#20010;&#26368;&#21487;&#20449;&#30340;&#31572;&#26696;&#65292;&#19982;&#20854;&#20182;&#36873;&#25321;&#26377;&#25152;&#19981;&#21516;&#12290;&#20219;&#21153;&#38590;&#24230;&#30001;&#21069;&#20004;&#20010;&#31572;&#26696;&#20043;&#38388;&#30340;&#28151;&#28102;&#27010;&#29575;&#37327;&#21270;&#65292;&#24037;&#20316;&#21487;&#38752;&#24615;&#30001;&#32473;&#20986;&#21069;&#20004;&#20010;&#31572;&#26696;&#30340;&#27010;&#29575;&#37327;&#21270;&#12290;&#22312;&#27492;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#25512;&#26029;&#31639;&#27861;&#26469;&#25512;&#26029;&#21069;&#20004;&#20010;&#31572;&#26696;&#21644;&#28151;&#28102;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowdsourcing has emerged as an effective platform for labeling large amounts of data in a cost- and time-efficient manner. Most previous work has focused on designing an efficient algorithm to recover only the ground-truth labels of the data. In this paper, we consider multi-choice crowdsourcing tasks with the goal of recovering not only the ground truth, but also the most confusing answer and the confusion probability. The most confusing answer provides useful information about the task by revealing the most plausible answer other than the ground truth and how plausible it is. To theoretically analyze such scenarios, we propose a model in which there are the top two plausible answers for each task, distinguished from the rest of the choices. Task difficulty is quantified by the probability of confusion between the top two, and worker reliability is quantified by the probability of giving an answer among the top two. Under this model, we propose a two-stage inference algorithm to infe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RepDIB&#30340;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#26500;&#24314;&#38544;&#29366;&#24577;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21435;&#38500;&#24863;&#30693;&#20449;&#24687;&#20013;&#30340;&#22122;&#22768;&#21644;&#26080;&#20851;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.13835</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in Deep RL via Discrete Information Bottleneck. (arXiv:2212.13835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RepDIB&#30340;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#26500;&#24314;&#38544;&#29366;&#24577;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21435;&#38500;&#24863;&#30693;&#20449;&#24687;&#20013;&#30340;&#22122;&#22768;&#21644;&#26080;&#20851;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#24863;&#30693;&#20449;&#24687;&#24102;&#26377;&#22122;&#22768;&#21644;&#26080;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#26500;&#24314;&#38544;&#29366;&#24577;&#30340;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;RepDIB&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#21464;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#32467;&#26500;&#21270;&#20998;&#35299;&#34920;&#31034;&#65292;&#24182;&#23558;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29942;&#39048;&#19982;&#29616;&#26377;&#33258;&#30417;&#30563;&#30446;&#26631;&#30456;&#32467;&#21512;&#12290;&#22312;&#22810;&#20010;&#22312;&#32447;&#21644;&#31163;&#32447;RL&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#30495;&#23454;&#26426;&#22120;&#20154;&#33218;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;RepDIB&#36827;&#34892;&#21387;&#32553;&#34920;&#24449;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several self-supervised representation learning methods have been proposed for reinforcement learning (RL) with rich observations. For real-world applications of RL, recovering underlying latent states is crucial, particularly when sensory inputs contain irrelevant and exogenous information. In this work, we study how information bottlenecks can be used to construct latent states efficiently in the presence of task-irrelevant information. We propose architectures that utilize variational and discrete information bottlenecks, coined as RepDIB, to learn structured factorized representations. Exploiting the expressiveness bought by factorized representations, we introduce a simple, yet effective, bottleneck that can be integrated with any existing self-supervised objective for RL. We demonstrate this across several online and offline RL benchmarks, along with a real robot arm task, where we find that compressed representations with RepDIB can lead to strong performance improvements, as th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#26426;&#21046;&#65292;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;&#20102;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.07677</link><description>&lt;p&gt;
Transformer&#27169;&#22411;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers learn in-context by gradient descent. (arXiv:2212.07677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#26426;&#21046;&#65292;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;&#20102;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#22823;&#22810;&#21482;&#20572;&#30041;&#22312;&#30452;&#35273;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#26435;&#37325;&#26500;&#36896;&#65292;&#35777;&#26126;&#20102;&#30001;&#21333;&#20010;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#23618;&#24341;&#21457;&#30340;&#25968;&#25454;&#36716;&#25442;&#19982;&#30001;&#20855;&#26377;&#22238;&#24402;&#25439;&#22833;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#33719;&#24471;&#30340;&#36716;&#25442;&#20855;&#26377;&#31561;&#20215;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20165;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;Transformer&#27169;&#22411;&#36827;&#34892;&#31616;&#21333;&#30340;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#36890;&#36807;GD&#20248;&#21270;&#24471;&#21040;&#30340;&#27169;&#22411;&#19982;&#27169;&#22411;&#26435;&#37325;&#21313;&#20998;&#30456;&#20284;&#65292;&#25110;&#32773;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;GD&#20248;&#21270;&#30340;&#26435;&#37325;&#19982;&#26500;&#36896;&#30340;&#26435;&#37325;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26159;&#22914;&#20309;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#12290;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06751</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#21152;&#36895;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#30340;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23454;&#36341;&#32773;&#36890;&#24120;&#38754;&#20020;&#22810;&#20010;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39640;&#25928;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#19979;&#65292;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;TPE&#30340;&#25910;&#36141;&#20989;&#25968;&#25193;&#23637;&#21040;&#20803;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30001;&#20219;&#21153;&#20043;&#38388;&#39030;&#32423;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#24230;&#23450;&#20041;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35299;&#20915;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;HPO&#22522;&#20934;&#19978;&#21152;&#36895;&#20102;MO-TPE&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#36194;&#24471;AutoML 2022&#26469;&#24471;&#21040;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#31283;&#23450;&#30340;&#32654;&#26415;&#23478;"&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;SEGA&#20197;&#21450;&#28508;&#22312;&#36941;&#21382;&#31561;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#23454;&#29616;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#65292;&#21487;&#20197;&#24494;&#22937;&#22320;&#32534;&#36753;&#22270;&#20687;&#12289;&#25913;&#21464;&#26500;&#22270;&#21644;&#39118;&#26684;&#65292;&#36798;&#21040;&#33402;&#26415;&#26500;&#24605;&#20248;&#21270;&#31561;&#30446;&#30340;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.06013</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#32654;&#26415;&#23478;&#65306;&#25197;&#26354;&#28459;&#27969;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#39550;&#39533;
&lt;/p&gt;
&lt;p&gt;
The Stable Artist: Steering Semantics in Diffusion Latent Space. (arXiv:2212.06013v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#31283;&#23450;&#30340;&#32654;&#26415;&#23478;"&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;SEGA&#20197;&#21450;&#28508;&#22312;&#36941;&#21382;&#31561;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#23454;&#29616;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#65292;&#21487;&#20197;&#24494;&#22937;&#22320;&#32534;&#36753;&#22270;&#20687;&#12289;&#25913;&#21464;&#26500;&#22270;&#21644;&#39118;&#26684;&#65292;&#36798;&#21040;&#33402;&#26415;&#26500;&#24605;&#20248;&#21270;&#31561;&#30446;&#30340;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#24778;&#20154;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#19968;&#27425;&#24615;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#30456;&#21453;&#65292;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#21253;&#25324;&#29992;&#25143;&#23545;&#36755;&#20837;&#36827;&#34892;&#22810;&#27425;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#20197;&#36845;&#20195;&#22320;&#38613;&#21051;&#20986;&#25152;&#24819;&#35937;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36755;&#20837;&#25552;&#31034;&#30340;&#24494;&#23567;&#26356;&#25913;&#36890;&#24120;&#20250;&#23548;&#33268;&#29983;&#25104;&#23436;&#20840;&#19981;&#21516;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#33402;&#26415;&#23478;&#30340;&#25511;&#21046;&#21463;&#38480;&#20110;&#20854;&#31890;&#24230;&#12290;&#20026;&#20102;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#32654;&#26415;&#23478;&#65292;&#36825;&#26159;&#19968;&#31181;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#21487;&#20351;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#26356;&#21152;&#23481;&#26131;&#12290;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#35821;&#20041;&#24341;&#23548;(SEGA)&#65292;&#23427;&#27839;&#30528;&#19981;&#21516;&#25968;&#37327;&#30340;&#35821;&#20041;&#26041;&#21521;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#12290;&#36825;&#20801;&#35768;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#22937;&#30340;&#32534;&#36753;&#12289;&#25913;&#21464;&#26500;&#22270;&#21644;&#39118;&#26684;&#65292;&#20197;&#21450;&#20248;&#21270;&#25972;&#20307;&#33402;&#26415;&#26500;&#24605;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#36941;&#21382;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#23545;&#22270;&#20687;&#29305;&#23450;&#21306;&#22495;&#30340;&#23616;&#37096;&#21644;&#26377;&#23548;&#21521;&#22320;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23545;&#25152;&#29983;&#25104;&#30340;&#22270;&#20687;&#21069;&#25152;&#26410;&#26377;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, text-conditioned generative diffusion models have recently gained a lot of attention for their impressive performance in generating high-fidelity images from text alone. However, achieving high-quality results is almost unfeasible in a one-shot fashion. On the contrary, text-guided image generation involves the user making many slight changes to inputs in order to iteratively carve out the envisioned image. However, slight changes to the input prompt often lead to entirely different images being generated, and thus the control of the artist is limited in its granularity. To provide flexibility, we present the Stable Artist, an image editing approach enabling fine-grained control of the image generation process. The main component is semantic guidance (SEGA) which steers the diffusion process along variable numbers of semantic directions. This allows for subtle edits to images, changes in composition and style, as well as optimization of the overall artistic conception. Furthermo
&lt;/p&gt;</description></item><item><title>Elixir &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#36816;&#34892;&#27169;&#22411;&#20998;&#26512;&#30340;&#33258;&#21160;&#21270;&#39640;&#25928;&#22823;&#27169;&#22411;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#23558;&#20869;&#23384;&#20351;&#29992;&#21368;&#36733;&#21040; CPU &#21644; NVMe &#23384;&#20648;&#22120;&#20013;&#65292;&#20805;&#20998;&#21457;&#25381;&#30828;&#20214;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.05339</link><description>&lt;p&gt;
Elixir: &#22312;&#23567;&#22411; GPU &#38598;&#32676;&#19978;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Elixir: Train a Large Language Model on a Small GPU Cluster. (arXiv:2212.05339v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05339
&lt;/p&gt;
&lt;p&gt;
Elixir &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#36816;&#34892;&#27169;&#22411;&#20998;&#26512;&#30340;&#33258;&#21160;&#21270;&#39640;&#25928;&#22823;&#27169;&#22411;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#23558;&#20869;&#23384;&#20351;&#29992;&#21368;&#36733;&#21040; CPU &#21644; NVMe &#23384;&#20648;&#22120;&#20013;&#65292;&#20805;&#20998;&#21457;&#25381;&#30828;&#20214;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#35268;&#27169;&#21069;&#25152;&#26410;&#26377;&#30340;&#22823;&#23567;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340; GPU&#12290;&#20026;&#20102;&#20943;&#23569; GPU &#20869;&#23384;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#20869;&#23384;&#20998;&#21306;&#21644;&#20869;&#23384;&#21368;&#36733;&#12290;&#36825;&#20123;&#26041;&#27861;&#28040;&#38500;&#20102;&#20869;&#23384;&#20887;&#20313;&#65292;&#24182;&#23558;&#20869;&#23384;&#20351;&#29992;&#21368;&#36733;&#21040; CPU &#21644; NVMe &#23384;&#20648;&#22120;&#20013;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#23567;&#22411; GPU &#38598;&#32676;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#37096;&#32626;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20250;&#23548;&#33268;&#27425;&#20248;&#25928;&#29575;&#12290;&#21482;&#26377;&#32463;&#39564;&#20016;&#23500;&#30340;&#19987;&#23478;&#25165;&#33021;&#36890;&#36807;&#20180;&#32454;&#35843;&#25972;&#20998;&#24067;&#24335;&#37197;&#32622;&#26469;&#20805;&#20998;&#21457;&#25381;&#30828;&#20214;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696; Elixir&#65292;&#23427;&#22522;&#20110;&#39044;&#36816;&#34892;&#27169;&#22411;&#20998;&#26512;&#33258;&#21160;&#21270;&#39640;&#25928;&#30340;&#22823;&#27169;&#22411;&#35757;&#32451;&#12290;Elixir &#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20998;&#21306;&#21644;&#21368;&#36733;&#25216;&#26415;&#30340;&#26368;&#20339;&#32452;&#21512;&#65292;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;Elixir &#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38646;&#26679;&#26412;&#35782;&#21035;&#21644;text2image&#27169;&#22411;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#21487;&#25193;&#23637;&#30340;&#22797;&#21046;&#31896;&#36148;&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#19981;&#21516;&#29289;&#20307;&#31867;&#21035;&#30340;&#22270;&#20687;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;</title><link>http://arxiv.org/abs/2212.03863</link><description>&lt;p&gt;
X-Paste&#65306;&#21033;&#29992;CLIP&#21644;StableDiffusion&#37325;&#26032;&#24605;&#32771;&#21487;&#25193;&#23637;&#30340;&#23454;&#20363;&#20998;&#21106;&#22797;&#21046;&#31896;&#36148;(arXiv:2212.03863v2 [cs.CV] &#20462;&#35746;&#29256;)
&lt;/p&gt;
&lt;p&gt;
X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion. (arXiv:2212.03863v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38646;&#26679;&#26412;&#35782;&#21035;&#21644;text2image&#27169;&#22411;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#21487;&#25193;&#23637;&#30340;&#22797;&#21046;&#31896;&#36148;&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#19981;&#21516;&#29289;&#20307;&#31867;&#21035;&#30340;&#22270;&#20687;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#21046;&#31896;&#36148;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;&#23545;&#35937;&#23454;&#20363;&#38543;&#26426;&#31896;&#36148;&#21040;&#26032;&#30340;&#32972;&#26223;&#22270;&#20687;&#20013;&#65292;&#21487;&#20197;&#20813;&#36153;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#26174;&#33879;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#32597;&#35265;&#30340;&#29289;&#20307;&#31867;&#21035;&#12290;&#26412;&#25991;&#21033;&#29992;&#26032;&#20986;&#29616;&#30340;&#38646;&#26679;&#26412;&#35782;&#21035;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#21644;text2image&#27169;&#22411;&#65288;&#20363;&#22914;StableDiffusion&#65289;&#30340;&#33021;&#21147;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#21487;&#25193;&#23637;&#30340;&#22797;&#21046;&#31896;&#36148;&#12290;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#20351;&#29992;text2image&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#25110;&#20351;&#29992;&#38646;&#26679;&#26412;&#35782;&#21035;&#27169;&#22411;&#36807;&#28388;&#22024;&#26434;&#29228;&#21462;&#30340;&#22270;&#20687;&#20197;&#33719;&#21462;&#19981;&#21516;&#29289;&#20307;&#31867;&#21035;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copy-Paste is a simple and effective data augmentation strategy for instance segmentation. By randomly pasting object instances onto new background images, it creates new training data for free and significantly boosts the segmentation performance, especially for rare object categories. Although diverse, high-quality object instances used in Copy-Paste result in more performance gain, previous works utilize object instances either from human-annotated instance segmentation datasets or rendered from 3D object models, and both approaches are too expensive to scale up to obtain good diversity. In this paper, we revisit Copy-Paste at scale with the power of newly emerged zero-shot recognition models (e.g., CLIP) and text2image models (e.g., StableDiffusion). We demonstrate for the first time that using a text2image model to generate images or zero-shot recognition model to filter noisily crawled images for different object categories is a feasible way to make Copy-Paste truly scalable. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#27987;&#24230;&#29616;&#35937;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#36870;&#24615;&#22312;&#27987;&#24230;&#29616;&#35937;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.03670</link><description>&lt;p&gt;
&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#30340;&#27987;&#24230;&#29616;&#35937;&#65306;&#19968;&#31181;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Concentration Phenomenon for Random Dynamical Systems: An Operator Theoretic Approach. (arXiv:2212.03670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#27987;&#24230;&#29616;&#35937;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#36870;&#24615;&#22312;&#27987;&#24230;&#29616;&#35937;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#65292;&#23545;&#20855;&#26377;&#19981;&#21464;&#30340;&#36951;&#20256;&#27979;&#24230;$\mu_{\pi}$&#30340;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#32473;&#23450;&#35266;&#27979;&#37327;&#8220;$r$&#8221;&#30340;&#27987;&#24230;&#29616;&#35937;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#21487;&#33021;&#20855;&#26377;&#25903;&#25345;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#23545;Markov&#36716;&#31227;&#31639;&#23376;$P$&#21644;&#30001;$e^{r}$&#23450;&#20041;&#30340;&#20056;&#27861;&#31639;&#23376;&#30340;&#32452;&#21512;&#30740;&#31350;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#27010;&#29575;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21487;&#35266;&#27979;/&#22870;&#21169;&#20989;&#25968;&#26159;&#26080;&#30028;&#30340;&#65292;&#20294;&#23545;&#20110;&#26576;&#20123;$q&gt;2$&#65292;$\|e^{r}\|_{q \rightarrow 2} \propto \exp\big(\mu_{\pi}(r) +\frac{2q}{q-2}\big) $&#65292;&#24182;&#19988;$P$&#20855;&#26377;&#19978;&#30830;&#30028;&#25511;&#21046;$\|P\|_{2 \rightarrow q }&lt; e^{\frac{1}{2}[\frac{1}{2}-\frac{1}{q}]}$&#65292;&#21017;&#21487;&#20197;&#24471;&#21040;&#23574;&#38160;&#30340;&#38750;&#28176;&#36827;&#27987;&#24230;&#30028;&#38480;&#12290;&#30001;&#20110;&#36755;&#36816;-&#29109;&#19981;&#31561;&#24335;&#65292;&#23545;&#20110;&#25152;&#26377;$q&gt;2$&#65292;&#35813;&#20056;&#27861;&#31639;&#23376;&#30340;&#19978;&#30028;&#24471;&#21040;&#20445;&#35777;&#12290;&#26412;&#25991;&#35770;&#35777;&#20102;&#21487;&#36870;&#24615;&#22312;&#27987;&#24230;&#29616;&#35937;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Via operator theoretic methods, we formalize the concentration phenomenon for a given observable `$r$' of a discrete time Markov chain with `$\mu_{\pi}$' as invariant ergodic measure, possibly having support on an unbounded state space. The main contribution of this paper is circumventing tedious probabilistic methods with a study of a composition of the Markov transition operator $P$ followed by a multiplication operator defined by $e^{r}$. It turns out that even if the observable/ reward function is unbounded, but for some for some $q&gt;2$, $\|e^{r}\|_{q \rightarrow 2} \propto \exp\big(\mu_{\pi}(r) +\frac{2q}{q-2}\big) $ and $P$ is hyperbounded with norm control $\|P\|_{2 \rightarrow q }&lt; e^{\frac{1}{2}[\frac{1}{2}-\frac{1}{q}]}$, sharp non-asymptotic concentration bounds follow. \emph{Transport-entropy} inequality ensures the aforementioned upper bound on multiplication operator for all $q&gt;2$. The role of \emph{reversibility} in concentration phenomenon is demystified. These results a
&lt;/p&gt;</description></item><item><title>Yggdrasil Decision Forests&#26159;&#19968;&#31181;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#26862;&#26519;&#24211;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#21644;&#29983;&#20135;&#24037;&#20316;&#25552;&#20379;&#25903;&#25345;&#12290;&#35813;&#24211;&#30340;&#35774;&#35745;&#21407;&#21017;&#20026;&#26131;&#29992;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#27169;&#22359;&#21270;&#21644;&#39640;&#23618;&#25277;&#35937;&#20197;&#21450;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24211;&#30340;&#38598;&#25104;&#65292;&#24182;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#20351;&#29992;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2212.02934</link><description>&lt;p&gt;
Yggdrasil Decision Forests&#65306;&#19968;&#31181;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#26862;&#26519;&#24211;
&lt;/p&gt;
&lt;p&gt;
Yggdrasil Decision Forests: A Fast and Extensible Decision Forests Library. (arXiv:2212.02934v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02934
&lt;/p&gt;
&lt;p&gt;
Yggdrasil Decision Forests&#26159;&#19968;&#31181;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#26862;&#26519;&#24211;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#21644;&#29983;&#20135;&#24037;&#20316;&#25552;&#20379;&#25903;&#25345;&#12290;&#35813;&#24211;&#30340;&#35774;&#35745;&#21407;&#21017;&#20026;&#26131;&#29992;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#27169;&#22359;&#21270;&#21644;&#39640;&#23618;&#25277;&#35937;&#20197;&#21450;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24211;&#30340;&#38598;&#25104;&#65292;&#24182;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#20351;&#29992;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yggdrasil Decision Forests&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#12289;&#26381;&#21153;&#21644;&#35299;&#37322;&#20915;&#31574;&#26862;&#26519;&#27169;&#22411;&#30340;&#24211;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#21644;&#29983;&#20135;&#24037;&#20316;&#25552;&#20379;&#25903;&#25345;&#65292;&#20351;&#29992;C++&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;C++&#21629;&#20196;&#34892;&#30028;&#38754;&#12289;Python&#65288;&#21517;&#31216;&#20026;TensorFlow Decision Forests&#65289;&#12289;JavaScript&#12289;Go&#21644;Google Sheets&#65288;&#21517;&#31216;&#20026;Simple ML for Sheets&#65289;&#31561;&#35821;&#35328;&#25509;&#21475;&#12290;&#35813;&#24211;&#33258;2018&#24180;&#20197;&#26469;&#25353;&#29031;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#24211;&#21644;&#26694;&#26550;&#30340;&#35774;&#35745;&#21407;&#21017;&#36827;&#34892;&#26377;&#26426;&#24320;&#21457;&#65306;&#26131;&#29992;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#27169;&#22359;&#21270;&#21644;&#39640;&#23618;&#25277;&#35937;&#20197;&#21450;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24211;&#30340;&#38598;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#36825;&#20123;&#21407;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#25351;&#23548;&#24211;&#30340;&#35774;&#35745;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#32452;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24211;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#23558;&#25105;&#20204;&#30340;&#24211;&#19982;&#30456;&#20851;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yggdrasil Decision Forests is a library for the training, serving and interpretation of decision forest models, targeted both at research and production work, implemented in C++, and available in C++, command line interface, Python (under the name TensorFlow Decision Forests), JavaScript, Go, and Google Sheets (under the name Simple ML for Sheets). The library has been developed organically since 2018 following a set of four design principles applicable to machine learning libraries and frameworks: simplicity of use, safety of use, modularity and high-level abstraction, and integration with other machine learning libraries. In this paper, we describe those principles in detail and present how they have been used to guide the design of the library. We then showcase the use of our library on a set of classical machine learning problems. Finally, we report a benchmark comparing our library to related solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#24102;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#20687;&#32032;&#20998;&#31867;&#22120;&#21644;&#26144;&#23556;&#32534;&#36753;&#25805;&#20316;&#24555;&#36895;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#24847;&#22270;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#32534;&#36753;&#32467;&#26524;&#36136;&#37327;&#21644;&#36895;&#24230;&#22343;&#20248;&#20110;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.02024</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models. (arXiv:2212.02024v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#24102;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#20687;&#32032;&#20998;&#31867;&#22120;&#21644;&#26144;&#23556;&#32534;&#36753;&#25805;&#20316;&#24555;&#36895;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#24847;&#22270;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#32534;&#36753;&#32467;&#26524;&#36136;&#37327;&#21644;&#36895;&#24230;&#22343;&#20248;&#20110;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#32454;&#31890;&#24230;&#30495;&#23454;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#22235;&#20010;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#24102;&#26377;&#20687;&#32032;&#32423;&#25351;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#24102;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#20687;&#32032;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#25512;&#26029;&#30446;&#26631;&#22270;&#20687;&#30340;&#20998;&#21106;&#26144;&#23556;&#12290;&#28982;&#21518;&#29992;&#25143;&#36890;&#36807;&#25805;&#20316;&#26144;&#23556;&#26469;&#25351;&#23548;&#32534;&#36753;&#25805;&#20316;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#20687;&#32032;&#32423;&#25351;&#23548;&#29983;&#25104;&#19982;&#29992;&#25143;&#24847;&#22270;&#30456;&#20851;&#32852;&#30340;&#24050;&#32534;&#36753;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#25351;&#23548;&#21644;&#20854;&#20182;&#25216;&#26415;&#30340;&#26377;&#25928;&#32452;&#21512;&#23454;&#29616;&#20102;&#39640;&#24230;&#21487;&#25511;&#30340;&#32534;&#36753;&#65292;&#24182;&#20445;&#30041;&#32534;&#36753;&#21306;&#22495;&#20043;&#22806;&#65292;&#20174;&#32780;&#28385;&#36275;&#25105;&#20204;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32534;&#36753;&#36136;&#37327;&#21644;&#36895;&#24230;&#26041;&#38754;&#22343;&#20248;&#20110;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is to develop fine-grained real-image editing methods suitable for real-world applications. In this paper, we first summarize four requirements for these methods and propose a novel diffusion-based image editing framework with pixel-wise guidance that satisfies these requirements. Specifically, we train pixel-classifiers with a few annotated data and then infer the segmentation map of a target image. Users then manipulate the map to instruct how the image will be edited. We utilize a pre-trained diffusion model to generate edited images aligned with the user's intention with pixel-wise guidance. The effective combination of proposed guidance and other techniques enables highly controllable editing with preserving the outside of the edited area, which results in meeting our requirements. The experimental results demonstrate that our proposal outperforms the GAN-based method for editing quality and speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;Pareto&#26368;&#20248;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#34920;&#36848;&#21644;&#23450;&#20041;&#20102;Pareto&#21518;&#24724;&#65292;&#25552;&#20986;&#20102;&#26032;&#31639;&#27861;&#65292;&#20998;&#26512;&#35777;&#26126;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#26368;&#20248;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#25509;&#36817;&#26368;&#20248;&#65292;&#24182;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#26426;&#21046;&#20174;&#36172;&#24466;&#25512;&#24191;&#21040;&#22810;&#30446;&#26631;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2212.00884</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;Pareto&#21518;&#24724;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Pareto Regret Analyses in Multi-objective Multi-armed Bandit. (arXiv:2212.00884v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;Pareto&#26368;&#20248;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#34920;&#36848;&#21644;&#23450;&#20041;&#20102;Pareto&#21518;&#24724;&#65292;&#25552;&#20986;&#20102;&#26032;&#31639;&#27861;&#65292;&#20998;&#26512;&#35777;&#26126;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#26368;&#20248;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#25509;&#36817;&#26368;&#20248;&#65292;&#24182;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#26426;&#21046;&#20174;&#36172;&#24466;&#25512;&#24191;&#21040;&#22810;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;Pareto&#26368;&#20248;&#24615;&#12290;&#36890;&#36807;&#25552;&#20986;&#23545;&#25239;&#24615;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#34920;&#36848;&#24182;&#23450;&#20041;&#20854;Pareto&#21518;&#24724;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#12290;&#36825;&#20123;&#21518;&#24724;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26631;&#37327;&#21270;&#20989;&#25968;&#65292;&#24182;&#21453;&#26144;&#20102;&#19982;&#26631;&#37327;&#21270;&#21518;&#24724;&#30456;&#27604;&#30340;Pareto&#26368;&#20248;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#26377;&#21644;&#26080;&#20808;&#39564;&#20449;&#24687;&#30340;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#29615;&#22659;&#19979;&#30340;&#26032;&#31639;&#27861;&#12290;&#36890;&#36807;&#25105;&#20204;&#23545;Pareto&#21518;&#24724;&#30340;&#19978;&#19979;&#30028;&#20998;&#26512;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#25509;&#36817;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#19979;&#30028;&#20998;&#26512;&#34920;&#26126;&#65292;&#26032;&#30340;&#21518;&#24724;&#19982;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;&#29616;&#26377;Pareto&#21518;&#24724;&#19968;&#33268;&#65292;&#24182;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#26426;&#21046;&#20174;&#36172;&#24466;&#25512;&#24191;&#21040;&#20102;&#22810;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Pareto optimality in multi-objective multi-armed bandit by providing a formulation of adversarial multi-objective multi-armed bandit and defining its Pareto regrets that can be applied to both stochastic and adversarial settings. The regrets do not rely on any scalarization functions and reflect Pareto optimality compared to scalarized regrets. We also present new algorithms assuming both with and without prior information of the multi-objective multi-armed bandit setting. The algorithms are shown optimal in adversarial settings and nearly optimal up to a logarithmic factor in stochastic settings simultaneously by our established upper bounds and lower bounds on Pareto regrets. Moreover, the lower bound analyses show that the new regrets are consistent with the existing Pareto regret for stochastic settings and extend an adversarial attack mechanism from bandit to the multi-objective one.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.16327</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#26080;&#38480;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#28857;&#12289;&#26080;&#38480;&#35745;&#31639;&#33021;&#21147;&#12289;&#19968;&#20010;&#26080;&#38480;&#22823;&#30340;&#23436;&#32654;&#35757;&#32451;&#31639;&#27861;&#12289;&#20197;&#21450;&#22312;&#39044;&#35774;&#20219;&#21153;&#19978;&#20445;&#35777;&#38646;&#27867;&#21270;&#35823;&#24046;&#65292;&#37027;&#20040;&#23427;&#21487;&#20197;&#29992;&#20110;&#19968;&#20999;&#21527;&#65311;&#20256;&#32479;&#30340;&#34920;&#31034;&#12289;&#20248;&#21270;&#25110;&#27867;&#21270;&#29702;&#35770;&#26080;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#25506;&#35752;&#30340;&#38382;&#39064;&#22312;&#36825;&#37324;&#37117;&#26159;&#19981;&#23384;&#22312;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#33539;&#30068;&#35770;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19977;&#20010;&#32467;&#26524;&#65292;&#31532;&#19968;&#20010;&#38480;&#21046;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#20165;&#24403;&#20219;&#21153;&#21487;&#34920;&#31034;&#26102;&#65292;&#27169;&#22411;&#25165;&#33021;&#29992;&#25552;&#31034;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65307;&#31532;&#20108;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#24494;&#35843;&#19981;&#21463;&#36825;&#20010;&#38480;&#21046;&#65292;&#22240;&#20026;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#65288;&#23545;&#31216;&#24615;&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#29702;&#35770;&#19978;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#31532;&#20108;&#20010;&#32467;&#26524;&#30340;&#19968;&#33324;&#21270;&#65292;&#34920;&#26126;&#22914;&#26524;&#20801;&#35768;&#24494;&#35843;&#24182;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#65292;&#21017;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#23567;&#33021;&#21147;&#20063;&#36275;&#20197;&#35299;&#20915;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23384;&#22312;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#65292;&#36825;&#19968;&#38382;&#39064;&#19982;&#22270;&#24418;&#26354;&#29575;&#30456;&#20851;&#65292;&#20316;&#32773;&#21033;&#29992;Ollivier-Ricci&#26354;&#29575;&#25552;&#20986;&#20102;Batch Ollivier-Ricci Flow&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.15779</link><description>&lt;p&gt;
&#20351;&#29992;Ollivier-Ricci&#26354;&#29575;&#37325;&#26032;&#23457;&#35270;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature. (arXiv:2211.15779v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15779
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23384;&#22312;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#65292;&#36825;&#19968;&#38382;&#39064;&#19982;&#22270;&#24418;&#26354;&#29575;&#30456;&#20851;&#65292;&#20316;&#32773;&#21033;&#29992;Ollivier-Ricci&#26354;&#29575;&#25552;&#20986;&#20102;Batch Ollivier-Ricci Flow&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#22825;&#29983;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#38480;&#21046;&#20102;GNN&#22312;&#36827;&#34892;&#36828;&#36317;&#31163;&#20449;&#24687;&#22788;&#29702;&#26102;&#23545;&#22797;&#26434;&#22270;&#24418;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#30340;&#25928;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23616;&#37096;&#22270;&#24418;&#31354;&#38388;&#21644;&#36825;&#20004;&#20010;&#38382;&#39064;&#20043;&#38388;&#30340;&#20851;&#38190;&#32852;&#31995;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20351;&#29992;Ollivier-Ricci&#26354;&#29575;&#22312;&#23616;&#37096;&#23610;&#24230;&#19978;&#30740;&#31350;&#23427;&#20204;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#36807;&#24230;&#24179;&#28369;&#19982;&#27491;&#22270;&#26354;&#29575;&#30456;&#32852;&#31995;&#65292;&#32780;&#36807;&#24230;&#21387;&#32553;&#21017;&#19982;&#36127;&#22270;&#26354;&#29575;&#30456;&#32852;&#31995;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Batch Ollivier-Ricci Flow&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#36830;&#31639;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) had been demonstrated to be inherently susceptible to the problems of over-smoothing and over-squashing. These issues prohibit the ability of GNNs to model complex graph interactions by limiting their effectiveness in taking into account distant information. Our study reveals the key connection between the local graph geometry and the occurrence of both of these issues, thereby providing a unified framework for studying them at a local scale using the Ollivier-Ricci curvature. Specifically, we demonstrate that over-smoothing is linked to positive graph curvature while over-squashing is linked to negative graph curvature. Based on our theory, we propose the Batch Ollivier-Ricci Flow, a novel rewiring algorithm capable of simultaneously addressing both over-smoothing and over-squashing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26631;&#31614;&#36755;&#20837;&#30340;GNN&#21644;&#38544;&#24335;GNN&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;IGNN&#20013;&#30340;&#38544;&#24335;&#24494;&#20998;&#26041;&#27861;&#65292;&#20351;&#24471;&#26631;&#31614;&#26080;&#38480;&#20256;&#25773;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2211.10629</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#32479;&#19968;&#26631;&#31614;&#36755;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Unifying Label-inputted Graph Neural Networks with Deep Equilibrium Models. (arXiv:2211.10629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26631;&#31614;&#36755;&#20837;&#30340;GNN&#21644;&#38544;&#24335;GNN&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;IGNN&#20013;&#30340;&#38544;&#24335;&#24494;&#20998;&#26041;&#27861;&#65292;&#20351;&#24471;&#26631;&#31614;&#26080;&#38480;&#20256;&#25773;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26041;&#38754;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#35768;&#22810;&#23376;&#35838;&#39064;&#65292;&#20363;&#22914;&#26631;&#31614;&#36755;&#20837;&#30340;GNN(LGNN)&#21644;&#38544;&#24335;GNN(IGNN)&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;LGNN&#35299;&#37322;&#20026;IGNN&#29702;&#35770;&#24182;&#23558;&#27969;&#34892;&#30340;LGNN&#24402;&#32422;&#20026;IGNN&#30340;&#24418;&#24335;&#26469;&#32479;&#19968;&#36825;&#20004;&#20010;&#23376;&#22495;&#12290;&#35813;&#32479;&#19968;&#31616;&#21270;&#20102;&#20004;&#20010;&#23376;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#24182;&#21551;&#21457;&#20102;&#26356;&#22810;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20171;&#32461;&#20102;IGNN&#30340;&#38544;&#24335;&#24494;&#20998;&#21040;LGNN&#20013;&#65292;&#20197;&#24120;&#25968;&#20869;&#23384;&#24494;&#20998;&#20854;&#26080;&#38480;&#33539;&#22260;&#30340;&#26631;&#31614;&#20256;&#25773;&#65292;&#20351;&#20256;&#25773;&#25104;&#20026;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Graph Neural Networks (GNN) in learning on non-Euclidean data arouses many subtopics, such as Label-inputted GNN (LGNN) and Implicit GNN (IGNN). LGNN, explicitly inputting supervising information (a.k.a. labels) in GNN, integrates label propagation to achieve superior performance, but with the dilemma between its propagating distance and adaptiveness. IGNN, outputting an equilibrium point by iterating its network infinite times, exploits information in the entire graph to capture long-range dependencies, but with its network constrained to guarantee the existence of the equilibrium. This work unifies the two subdomains by interpreting LGNN in the theory of IGNN and reducing prevailing LGNNs to the form of IGNN. The unification facilitates the exchange between the two subdomains and inspires more studies. Specifically, implicit differentiation of IGNN is introduced to LGNN to differentiate its infinite-range label propagation with constant memory, making the propagation b
&lt;/p&gt;</description></item><item><title>PAD-Net&#26159;&#19968;&#20010;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#23558;&#20887;&#20313;&#30340;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#32593;&#32476;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05528</link><description>&lt;p&gt;
PAD-Net&#65306;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PAD-Net: An Efficient Framework for Dynamic Networks. (arXiv:2211.05528v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05528
&lt;/p&gt;
&lt;p&gt;
PAD-Net&#26159;&#19968;&#20010;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#23558;&#20887;&#20313;&#30340;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#32593;&#32476;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32593;&#32476;&#65292;&#20363;&#22914;&#21160;&#24577;&#21367;&#31215;&#65288;DY-Conv&#65289;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#65292;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#29616;&#21160;&#24577;&#32593;&#32476;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#23558;&#32473;&#23450;&#30340;&#38745;&#24577;&#23618;&#36716;&#25442;&#20026;&#23436;&#20840;&#21160;&#24577;&#30340;&#23618;&#65292;&#20854;&#20013;&#25152;&#26377;&#21442;&#25968;&#37117;&#26159;&#21160;&#24577;&#30340;&#65288;&#33267;&#23569;&#22312;&#21333;&#20010;&#23618;&#20869;&#65289;&#24182;&#38543;&#36755;&#20837;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#23436;&#20840;&#21160;&#24577;&#30340;&#35774;&#32622;&#21487;&#33021;&#20250;&#23548;&#33268;&#20887;&#20313;&#21442;&#25968;&#21644;&#39640;&#37096;&#32626;&#25104;&#26412;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21160;&#24577;&#32593;&#32476;&#22312;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25361;&#25112;&#21160;&#24577;&#32593;&#32476;&#30340;&#22522;&#26412;&#24120;&#35782;&#65292;&#24182;&#25552;&#20986;&#37096;&#20998;&#21160;&#24577;&#32593;&#32476;&#65292;&#21363;PAD-Net&#65292;&#20197;&#23558;&#20887;&#20313;&#21160;&#24577;&#21442;&#25968;&#36716;&#25442;&#20026;&#38745;&#24577;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#36845;&#20195;&#27169;&#24335;&#20998;&#21306;&#26469;&#26377;&#25928;&#22320;&#20998;&#21306;&#21160;&#24577;&#21644;&#38745;&#24577;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#20840;&#38754;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CNN-LSTM&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#37325;&#24314;&#25152;&#26377;&#20195;&#29702;&#29366;&#24577;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#29305;&#23450;&#20195;&#29702;&#30340;&#29366;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26862;&#26519;&#28779;&#28798;&#27169;&#22411;&#20013;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#32452;&#32455;&#20247;&#26234;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.17289</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#37325;&#24314;&#30340;&#39044;&#27979;&#33258;&#32452;&#32455;&#20247;&#26234;&#31995;&#32479;&#30340;&#23616;&#37096;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Forecasting Local Behavior of Self-organizing Many-agent System without Reconstruction. (arXiv:2210.17289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CNN-LSTM&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#37325;&#24314;&#25152;&#26377;&#20195;&#29702;&#29366;&#24577;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#29305;&#23450;&#20195;&#29702;&#30340;&#29366;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26862;&#26519;&#28779;&#28798;&#27169;&#22411;&#20013;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#32452;&#32455;&#20247;&#26234;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#20195;&#29702;&#31995;&#32479;&#36890;&#24120;&#30001;&#23616;&#37096;&#23450;&#20041;&#30340;&#20195;&#29702;&#20132;&#20114;&#39537;&#21160;&#65292;&#21363;&#33258;&#32452;&#32455;&#12290;&#26412;&#25991;&#30340;&#39318;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#36825;&#31181;&#23616;&#37096;&#20132;&#20114;&#25193;&#25955;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#20195;&#29702;&#30340;&#26102;&#38388;&#12290;&#34429;&#28982;&#21487;&#20197;&#20351;&#29992;&#37325;&#24314;&#25152;&#26377;&#20195;&#29702;&#29366;&#24577;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#20294;&#21487;&#33021;&#20250;&#24102;&#26469;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;CNN-LSTM&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#24314;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#22823;&#22411;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#29305;&#23450;&#20195;&#29702;&#30340;&#29366;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;CNN&#32534;&#30721;&#22120;&#20197;&#20302;&#32500;&#24230;&#21521;&#37327;&#34920;&#31034;&#31995;&#32479;&#65292;LSTM&#27169;&#22359;&#23398;&#20064;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#20195;&#29702;&#21160;&#24577;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#20195;&#29702;&#29366;&#24577;&#30340;MLP&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#20197;&#26862;&#26519;&#28779;&#28798;&#27169;&#22411;&#20026;&#20363;&#65292;&#26088;&#22312;&#39044;&#27979;&#29305;&#23450;&#26641;&#20195;&#29702;&#20309;&#26102;&#24320;&#22987;&#29123;&#28903;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#37325;&#24314;&#22411;&#26041;&#27861;&#65288;&#22914;CNN-LSTM&#21644;ConvLSTM&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#39044;&#27979;&#23616;&#37096;&#20195;&#29702;&#30340;&#34892;&#20026;&#32780;&#26080;&#38656;&#37325;&#24314;&#25152;&#26377;&#20195;&#29702;&#29366;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#33258;&#32452;&#32455;&#20247;&#26234;&#31995;&#32479;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multi-agent systems are often driven by locally defined agent interactions, which is referred to as self-organization. Our primary objective is to determine when the propagation of such local interactions will reach a specific agent of interest. Although conventional approaches that reconstruct all agent states can be used, they may entail unnecessary computational costs. In this paper, we investigate a CNN-LSTM model to forecast the state of a particular agent in a large self-organizing multi-agent system without the reconstruction. The proposed model comprises a CNN encoder to represent the system in a low-dimensional vector, a LSTM module to learn agent dynamics in the vector space, and a MLP decoder to predict the future state of an agent. As an example, we consider a forest fire model where we aim to predict when a particular tree agent will start burning. We compare the proposed model with reconstruction-based approaches such as CNN-LSTM and ConvLSTM. The proposed model exh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20855;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#25191;&#34892;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2210.16402</link><description>&lt;p&gt;
GradSkip&#65306;&#20855;&#26377;&#26356;&#22909;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#36890;&#20449;&#21152;&#36895;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity. (arXiv:2210.16402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20855;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#25191;&#34892;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#36890;&#20449;&#20043;&#21069;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;&#26799;&#24230;&#31867;&#22411;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#30740;&#31350;&#20102;&#32422;&#21313;&#24180;&#65292;&#20294;&#26412;&#22320;&#35757;&#32451;&#30340;&#21152;&#36895;&#24615;&#36136;&#22312;&#29702;&#35770;&#19978;&#36824;&#26410;&#24471;&#21040;&#23436;&#20840;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;Mishchenko&#31561;&#20154;(2022 International Conference on Machine Learning)&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#35777;&#26126;&#20102;&#24403;&#26412;&#22320;&#35757;&#32451;&#24471;&#21040;&#27491;&#30830;&#25191;&#34892;&#26102;&#65292;&#20250;&#23548;&#33268;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#65292;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#36825;&#19968;&#28857;&#25104;&#31435;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#25968;&#25454;&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;ProxSkip&#35201;&#27714;&#25152;&#26377;&#23458;&#25143;&#31471;&#22312;&#27599;&#27425;&#36890;&#20449;&#36718;&#20013;&#25191;&#34892;&#30456;&#21516;&#25968;&#37327;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#12290;&#28789;&#24863;&#26469;&#33258;&#24120;&#35782;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#36890;&#36807;&#29468;&#27979;&#35748;&#20026;&#25317;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#24212;&#35813;&#33021;&#22815;&#29992;&#36739;&#23569;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#23601;&#33021;&#23436;&#25104;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
We study a class of distributed optimization algorithms that aim to alleviate high communication costs by allowing the clients to perform multiple local gradient-type training steps prior to communication. While methods of this type have been studied for about a decade, the empirically observed acceleration properties of local training eluded all attempts at theoretical understanding. In a recent breakthrough, Mishchenko et al. (ICML 2022) proved that local training, when properly executed, leads to provable communication acceleration, and this holds in the strongly convex regime without relying on any data similarity assumptions. However, their method ProxSkip requires all clients to take the same number of local training steps in each communication round. Inspired by a common sense intuition, we start our investigation by conjecturing that clients with ``less important'' data should be able to get away with fewer local training steps without this impacting the overall communication c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.13455</link><description>&lt;p&gt;
E-MCTS&#65306;&#36890;&#36807;&#35268;&#21010;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36864;&#28779;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#12289;&#24615;&#33021;&#26368;&#20248;&#31168;&#30340;&#35268;&#21010;&#26041;&#27861;&#20043;&#19968;&#12290;MCTS&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#28145;&#24230;&#25506;&#32034;&#21644;&#38754;&#23545;&#26410;&#30693;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#32531;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;MCTS&#20013;&#20256;&#25773;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20272;&#35745;&#20854;&#39044;&#27979;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25506;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;MCTS&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#21644;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36827;&#34892;&#20102;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#38750;&#35268;&#21010;&#30340;&#28145;&#24230;&#25506;&#32034;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340; Correlation Aware Pruner (CAP) &#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#39640;&#24230;&#20934;&#30830;&#30340;&#31232;&#30095;&#35270;&#35273;&#27169;&#22411;&#21098;&#26525;&#26041;&#38754;&#25512;&#21160;&#21387;&#32553;&#30028;&#38480;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#26377;&#25928;&#30340;&#32039;&#32553;&#24674;&#22797;&#36807;&#31243;&#23454;&#29616;&#21387;&#32553;&#21518;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.09223</link><description>&lt;p&gt;
CAP: &#38024;&#23545;&#39640;&#24230;&#20934;&#30830;&#30340;&#31232;&#30095;&#35270;&#35273;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#24863;&#30693;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models. (arXiv:2210.09223v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340; Correlation Aware Pruner (CAP) &#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#39640;&#24230;&#20934;&#30830;&#30340;&#31232;&#30095;&#35270;&#35273;&#27169;&#22411;&#21098;&#26525;&#26041;&#38754;&#25512;&#21160;&#21387;&#32553;&#30028;&#38480;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#26377;&#25928;&#30340;&#32039;&#32553;&#24674;&#22797;&#36807;&#31243;&#23454;&#29616;&#21387;&#32553;&#21518;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26550;&#26500;&#35774;&#35745;&#21644;&#35757;&#32451;&#27969;&#31243;&#26174;&#33879;&#25913;&#36827;&#30340;&#25512;&#21160;&#19979;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312; ImageNet &#31561;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20284;&#20046;&#26356;&#38590;&#20351;&#29992;&#26631;&#20934;&#30340;&#21387;&#32553;&#25216;&#26415;&#65288;&#20363;&#22914;&#21098;&#26525;&#25216;&#26415;&#65289;&#36827;&#34892;&#21387;&#32553;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20004;&#20010;&#25216;&#26415;&#36827;&#23637;&#30340; Correlation Aware Pruner (CAP) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#30340;&#21098;&#26525;&#22120;&#65292;&#21487;&#20197;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#31934;&#30830;&#12289;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#26435;&#37325;&#30456;&#20851;&#24615;&#65307;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#21518;&#24674;&#22797;&#30340;&#39640;&#25928;&#24494;&#35843;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#20010;&#29616;&#20195;&#35270;&#35273;&#27169;&#22411;&#65288;&#22914; Vision Transformers (ViT)&#12289;&#29616;&#20195; CNN &#21644; ViT-CNN &#28151;&#21512;&#27169;&#22411;&#65289;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by significant improvements in architectural design and training pipelines, computer vision has recently experienced dramatic progress in terms of accuracy on classic benchmarks such as ImageNet. These highly-accurate models are challenging to deploy, as they appear harder to compress using standard techniques such as pruning. We address this issue by introducing the Correlation Aware Pruner (CAP), a new unstructured pruning framework which significantly pushes the compressibility limits for state-of-the-art architectures. Our method is based on two technical advancements: a new theoretically-justified pruner, which can handle complex weight correlations accurately and efficiently during the pruning process itself, and an efficient finetuning procedure for post-compression recovery. We validate our approach via extensive experiments on several modern vision models such as Vision Transformers (ViT), modern CNNs, and ViT-CNN hybrids, showing for the first time that these can be pr
&lt;/p&gt;</description></item><item><title>RARR&#26159;&#19968;&#20010;&#21487;&#20197;&#23545;&#19981;&#30830;&#23450;&#20449;&#24687;&#36827;&#34892;&#30740;&#31350;&#21644;&#20462;&#35746;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2210.08726</link><description>&lt;p&gt;
RARR: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#21644;&#20462;&#27491;&#20854;&#36755;&#20986;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
RARR: Researching and Revising What Language Models Say, Using Language Models. (arXiv:2210.08726v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08726
&lt;/p&gt;
&lt;p&gt;
RARR&#26159;&#19968;&#20010;&#21487;&#20197;&#23545;&#19981;&#30830;&#23450;&#20449;&#24687;&#36827;&#34892;&#30740;&#31350;&#21644;&#20462;&#35746;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35832;&#22914;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#38382;&#31572;&#12289;&#25512;&#29702;&#21644;&#23545;&#35805;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#29983;&#25104;&#26080;&#25903;&#25345;&#25110;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#20219;&#20309;&#20869;&#32622;&#30340;&#24402;&#22240;&#22806;&#37096;&#35777;&#25454;&#30340;&#26426;&#21046;&#65292;&#29992;&#25143;&#24456;&#38590;&#30830;&#23450;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#38752;&#12290;&#20026;&#20102;&#22312;&#20445;&#30041;&#26368;&#26032;&#19968;&#20195;&#27169;&#22411;&#30340;&#25152;&#26377;&#24378;&#22823;&#20248;&#21183;&#30340;&#21516;&#26102;&#23454;&#29616;&#24402;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; RARR (&#20351;&#29992;&#30740;&#31350;&#21644;&#20462;&#35746;&#36827;&#34892;&#25913;&#36827;&#24402;&#22240;)&#31995;&#32479;&#65292;&#23427; 1) &#33258;&#21160;&#25214;&#21040;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182; 2) &#22312;&#23613;&#21487;&#33021;&#20445;&#30041;&#21407;&#22987;&#36755;&#20986;&#30340;&#21516;&#26102;&#65292;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;&#24403;&#24212;&#29992;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36755;&#20986;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;RARR&#22312;&#26174;&#33879;&#25552;&#39640;&#24402;&#22240;&#29575;&#30340;&#21516;&#26102;&#65292;&#27604;&#20197;&#21069;&#25506;&#32034;&#30340;&#32534;&#36753;&#27169;&#22411;&#26356;&#33021;&#20445;&#30041;&#21407;&#22987;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) now excel at many tasks such as few-shot learning, question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#22238;&#24402;&#21435;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#24191;&#27867;&#25512;&#24191;&#19988;&#23545;&#38544;&#31169;&#25915;&#20987;&#20855;&#26377;&#24378;&#38887;&#24615;&#30340;&#28145;&#24230;&#22238;&#24402;&#21435;&#23398;&#20064;&#26041;&#27861;Blindspot&#12290;</title><link>http://arxiv.org/abs/2210.08196</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#24402;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Regression Unlearning. (arXiv:2210.08196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#22238;&#24402;&#21435;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#24191;&#27867;&#25512;&#24191;&#19988;&#23545;&#38544;&#31169;&#25915;&#20987;&#20855;&#26377;&#24378;&#38887;&#24615;&#30340;&#28145;&#24230;&#22238;&#24402;&#21435;&#23398;&#20064;&#26041;&#27861;Blindspot&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#20445;&#25252;&#21644;&#38544;&#31169;&#27861;&#35268;&#30340;&#24341;&#20837;&#65292;&#20174;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20013;&#25353;&#38656;&#21024;&#38500;&#25968;&#25454;&#26469;&#28304;&#24050;&#32463;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#21435;&#23398;&#20064;&#22312;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#20174;ML&#27169;&#22411;&#20013;&#21024;&#38500;&#26576;&#20123;&#35757;&#32451;&#25968;&#25454;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22238;&#24402;&#38382;&#39064;&#30340;&#21435;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21435;&#23398;&#20064;&#12290;&#20998;&#31867;&#21644;&#31616;&#21333;&#32447;&#24615;&#22238;&#24402;&#30340;&#21435;&#23398;&#20064;&#24050;&#32463;&#24471;&#21040;&#20102;&#30456;&#24403;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#38382;&#39064;&#22312;&#29616;&#22312;&#20173;&#28982;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#24191;&#27867;&#25512;&#24191;&#19988;&#23545;&#38544;&#31169;&#25915;&#20987;&#20855;&#26377;&#24378;&#38887;&#24615;&#30340;&#28145;&#24230;&#22238;&#24402;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#39062;&#30340;&#26435;&#37325;&#20248;&#21270;&#36807;&#31243;&#30340;Blindspot&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#19968;&#31181;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#12289;&#37096;&#20998;&#26292;&#38706;&#20110;&#20445;&#30041;&#26679;&#26412;&#30340;&#27169;&#22411;&#21644;&#21407;&#22987;&#27169;&#22411;&#30340;&#21103;&#26412;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#21360;&#35760;&#26377;&#20851;&#35813;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks. We propose the Blindspot unlearning method which uses a novel weight optimization process. A randomly initialized model, partially exposed to the retain samples and a copy of the original model are used together to selectively imprint knowledge about t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2210.07890</link><description>&lt;p&gt;
&#20998;&#23618;&#31574;&#30053;&#28151;&#21512;&#20316;&#20026;&#21453;&#24212;&#24335;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Policy Blending as Inference for Reactive Robot Control. (arXiv:2210.07890v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#12289;&#23494;&#38598;&#21644;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#36816;&#21160;&#29983;&#25104;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#19968;&#26041;&#38754;&#65292;&#21453;&#24212;&#24335;&#31574;&#30053;&#20445;&#35777;&#20102;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#24555;&#36895;&#21709;&#24212;&#65292;&#20294;&#20197;&#27425;&#20248;&#30340;&#34892;&#20026;&#20316;&#20026;&#20195;&#20215;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#35268;&#21010;&#30340;&#36816;&#21160;&#29983;&#25104;&#25552;&#20379;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20294;&#39640;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#20250;&#38480;&#21046;&#25511;&#21046;&#39057;&#29575;&#65292;&#20174;&#32780;&#29306;&#29298;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#32467;&#21512;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#25512;&#29702;&#26041;&#27861;&#26469;&#27491;&#24335;&#21270;&#20998;&#23618;&#27169;&#22411;&#21644;&#38543;&#26426;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20026;&#38543;&#26426;&#21453;&#24212;&#24335;&#19987;&#23478;&#31574;&#30053;&#30340;&#21152;&#26435;&#20056;&#31215;&#65292;&#20854;&#20013;&#35268;&#21010;&#34987;&#29992;&#20110;&#33258;&#36866;&#24212;&#35745;&#31639;&#20219;&#21153;&#21608;&#26399;&#20869;&#30340;&#26368;&#20248;&#26435;&#37325;&#12290;&#36825;&#31181;&#38543;&#26426;&#20248;&#21270;&#36991;&#20813;&#20102;&#23616;&#37096;&#26368;&#20248;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#34892;&#30340;&#21453;&#24212;&#24335;&#35745;&#21010;&#65292;&#25214;&#21040;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion generation in cluttered, dense, and dynamic environments is a central topic in robotics, rendered as a multi-objective decision-making problem. Current approaches trade-off between safety and performance. On the one hand, reactive policies guarantee fast response to environmental changes at the risk of suboptimal behavior. On the other hand, planning-based motion generation provides feasible trajectories, but the high computational cost may limit the control frequency and thus safety. To combine the benefits of reactive policies and planning, we propose a hierarchical motion generation method. Moreover, we adopt probabilistic inference methods to formalize the hierarchical model and stochastic optimization. We realize this approach as a weighted product of stochastic, reactive expert policies, where planning is used to adaptively compute the optimal weights over the task horizon. This stochastic optimization avoids local optima and proposes feasible reactive plans that find path
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#26426;&#22120;&#20154;&#31574;&#30053;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25226;&#35745;&#21010;&#29983;&#25104;&#21644;&#35745;&#21010;&#25191;&#34892;&#20998;&#24320;&#65292;&#35299;&#20915;&#39046;&#22495;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#24314;&#31435;&#25277;&#35937;&#29615;&#22659;&#12289;&#29983;&#25104;&#25277;&#35937;&#36712;&#36857;&#21644;&#36890;&#36807;&#32763;&#35793;&#22120;&#35299;&#20915;&#21407;&#22987;&#20219;&#21153;&#19977;&#20010;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2210.07658</link><description>&lt;p&gt;
&#12298;&#19968;&#27425;&#24615;&#20219;&#21153;&#27867;&#21270;&#30340;&#25277;&#35937;&#21040;&#21487;&#25191;&#34892;&#36712;&#36857;&#32763;&#35793;&#12299;
&lt;/p&gt;
&lt;p&gt;
Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization. (arXiv:2210.07658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#26426;&#22120;&#20154;&#31574;&#30053;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25226;&#35745;&#21010;&#29983;&#25104;&#21644;&#35745;&#21010;&#25191;&#34892;&#20998;&#24320;&#65292;&#35299;&#20915;&#39046;&#22495;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#24314;&#31435;&#25277;&#35937;&#29615;&#22659;&#12289;&#29983;&#25104;&#25277;&#35937;&#36712;&#36857;&#21644;&#36890;&#36807;&#32763;&#35793;&#22120;&#35299;&#20915;&#21407;&#22987;&#20219;&#21153;&#19977;&#20010;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#29289;&#29702;&#29615;&#22659;&#20013;&#35757;&#32451;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#27604;&#22914;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21487;&#20197;&#27867;&#21270;&#21040;&#26410;&#30693;&#20219;&#21153;&#30340;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25286;&#20998;&#35745;&#21010;&#29983;&#25104;&#21644;&#35745;&#21010;&#25191;&#34892;&#26469;&#23454;&#29616;&#19968;&#27425;&#24615;&#20219;&#21153;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#19977;&#27493;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#65306;&#36890;&#36807;&#31616;&#21270;&#20960;&#20309;&#21644;&#29289;&#29702;&#23398;&#24314;&#31435;&#25104;&#23545;&#30340;&#25277;&#35937;&#29615;&#22659;&#65292;&#29983;&#25104;&#25277;&#35937;&#36712;&#36857;&#65292;&#28982;&#21518;&#36890;&#36807;&#25277;&#35937;&#21040;&#21487;&#25191;&#34892;&#36712;&#36857;&#32763;&#35793;&#22120;&#26469;&#35299;&#20915;&#21407;&#22987;&#20219;&#21153;&#12290;&#22312;&#25277;&#35937;&#29615;&#22659;&#20013;&#65292;&#21435;&#38500;&#20102;&#35832;&#22914;&#29289;&#29702;&#25805;&#32437;&#31561;&#22797;&#26434;&#21160;&#21147;&#23398;&#65292;&#20351;&#24471;&#25277;&#35937;&#36712;&#36857;&#26356;&#23481;&#26131;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#20837;&#20102;&#22823;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#22240;&#20026;&#25277;&#35937;&#36712;&#36857;&#32570;&#20047;&#20302;&#32423;&#32454;&#33410;&#24182;&#19988;&#27809;&#26377;&#19982;&#34987;&#25191;&#34892;&#30340;&#36712;&#36857;&#19968;&#19968;&#23545;&#24212;&#12290;&#31867;&#20284;&#20110;&#35821;&#35328;&#32763;&#35793;&#30340;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
Training long-horizon robotic policies in complex physical environments is essential for many applications, such as robotic manipulation. However, learning a policy that can generalize to unseen tasks is challenging. In this work, we propose to achieve one-shot task generalization by decoupling plan generation and plan execution. Specifically, our method solves complex long-horizon tasks in three steps: build a paired abstract environment by simplifying geometry and physics, generate abstract trajectories, and solve the original task by an abstract-to-executable trajectory translator. In the abstract environment, complex dynamics such as physical manipulation are removed, making abstract trajectories easier to generate. However, this introduces a large domain gap between abstract trajectories and the actual executed trajectories as abstract trajectories lack low-level details and are not aligned frame-to-frame with the executed trajectory. In a manner reminiscent of language translatio
&lt;/p&gt;</description></item><item><title>&#22806;&#37096;&#25968;&#25454;&#21487;&#33021;&#24341;&#20837;&#25915;&#20987;&#32773;&#31713;&#25913;&#30340;&#26377;&#27602;&#25968;&#25454;&#65292;&#20026;&#20102;&#25552;&#39640;&#27602;&#24615;&#38450;&#24481;&#24615;&#33021;&#65292;&#38656;&#35201;&#20934;&#30830;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#20986;&#24178;&#20928;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.06516</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#24773;&#20917;&#19979;&#31579;&#36873;&#20986;&#24178;&#20928;&#30340;&#25968;&#25454;&#23376;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?. (arXiv:2210.06516v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06516
&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#25968;&#25454;&#21487;&#33021;&#24341;&#20837;&#25915;&#20987;&#32773;&#31713;&#25913;&#30340;&#26377;&#27602;&#25968;&#25454;&#65292;&#20026;&#20102;&#25552;&#39640;&#27602;&#24615;&#38450;&#24481;&#24615;&#33021;&#65292;&#38656;&#35201;&#20934;&#30830;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#20986;&#24178;&#20928;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22806;&#37096;&#20379;&#24212;&#21830;&#12290;&#28982;&#32780;&#65292;&#21512;&#24182;&#22806;&#37096;&#25968;&#25454;&#20250;&#24102;&#26469;&#25968;&#25454;&#27745;&#26579;&#30340;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#20182;&#20204;&#30340;&#25968;&#25454;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#25928;&#29992;&#25110;&#23436;&#25972;&#24615;&#12290;&#22823;&#22810;&#25968;&#27602;&#21270;&#38450;&#24481;&#37117;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#19968;&#32452;&#24178;&#20928;&#30340;&#25968;&#25454;&#65288;&#25110;&#22522;&#30784;&#38598;&#65289;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#38544;&#34109;&#24615;&#27602;&#21270;&#25915;&#20987;&#30340;&#24555;&#36895;&#22686;&#38271;&#30740;&#31350;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#38450;&#24481;&#32773;&#30495;&#30340;&#33021;&#22815;&#22312;&#34987;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;&#19968;&#20010;&#24178;&#20928;&#30340;&#23376;&#38598;&#20197;&#25903;&#25345;&#38450;&#24481;&#21527;&#65311;&#26412;&#25991;&#20174;&#30740;&#31350;&#26377;&#27602;&#26679;&#26412;&#38169;&#35823;&#22320;&#28151;&#20837;&#22522;&#30784;&#38598;&#21518;&#23545;&#38450;&#24481;&#30340;&#24433;&#21709;&#24320;&#22987;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20116;&#31181;&#38450;&#24481;&#26041;&#27861;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#22312;&#22522;&#30784;&#38598;&#20013;&#27745;&#26579;&#28857;&#23569;&#20110;1&#65285;&#26102;&#24613;&#21095;&#19979;&#38477;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#38450;&#24481;&#30340;&#24615;&#33021;&#26041;&#38754;&#65292;&#31934;&#30830;&#22320;&#31579;&#36873;&#20986;&#19968;&#20010;&#22522;&#30784;&#38598;&#26159;&#20851;&#38190;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#31934;&#30830;&#30830;&#23450;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20197;&#22312;&#27745;&#26579;&#25968;&#25454;&#20013;&#37492;&#21035;&#19968;&#20010;&#24178;&#20928;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the volume of data needed to train modern machine learning models, external suppliers are increasingly used. However, incorporating external data poses data poisoning risks, wherein attackers manipulate their data to degrade model utility or integrity. Most poisoning defenses presume access to a set of clean data (or base set). While this assumption has been taken for granted, given the fast-growing research on stealthy poisoning attacks, a question arises: can defenders really identify a clean subset within a contaminated dataset to support defenses?  This paper starts by examining the impact of poisoned samples on defenses when they are mistakenly mixed into the base set. We analyze five defenses and find that their performance deteriorates dramatically with less than 1% poisoned points in the base set. These findings suggest that sifting out a base set with high precision is key to these defenses' performance. Motivated by these observations, we study how precise existing auto
&lt;/p&gt;</description></item><item><title>&#36739;&#20302;&#37319;&#26679;&#25928;&#29575;&#30340;NLP&#27169;&#22411;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#21487;&#33021;&#27604;&#36739;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#27169;&#22411;&#26356;&#20026;&#40065;&#26834;&#65292;&#34920;&#26126;&#36890;&#29992;&#30340;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#26041;&#27861;&#19981;&#22826;&#21487;&#33021;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.06456</link><description>&lt;p&gt;
&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;NLP&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Sample-Efficient NLP Models More Robust?. (arXiv:2210.06456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06456
&lt;/p&gt;
&lt;p&gt;
&#36739;&#20302;&#37319;&#26679;&#25928;&#29575;&#30340;NLP&#27169;&#22411;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#21487;&#33021;&#27604;&#36739;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#27169;&#22411;&#26356;&#20026;&#40065;&#26834;&#65292;&#34920;&#26126;&#36890;&#29992;&#30340;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#26041;&#27861;&#19981;&#22826;&#21487;&#33021;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#25277;&#21462;&#24335;&#38382;&#31572;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26356;&#23569;&#30340;&#20869;&#37096;&#25968;&#25454;&#19978;&#35757;&#32451;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#22806;&#37096;&#35780;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36235;&#21183;&#30340;&#26222;&#36866;&#24615;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#19977;&#20010;&#20219;&#21153;&#12289;&#19977;&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#24314;&#27169;&#24178;&#39044;&#65288;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#30340;&#36866;&#24212;&#26041;&#27861;&#21644;&#22312;&#26356;&#22810;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65289;&#21644;14&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#26679;&#26412;&#25928;&#29575;&#65288;&#36798;&#21040;&#32473;&#23450;ID&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65289;&#21644;&#40065;&#26834;&#24615;&#65288;&#27169;&#22411;&#22312;OOD&#35780;&#20272;&#20013;&#30340;&#34920;&#29616;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36739;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#20165;&#22312;&#26576;&#20123;&#24314;&#27169;&#24178;&#39044;&#21644;&#20219;&#21153;&#19978;&#19982;&#26356;&#22909;&#30340;&#24179;&#22343;OOD&#40065;&#26834;&#24615;&#30456;&#20851;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21017;&#19981;&#28982;&#12290;&#22312;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#65292;&#26679;&#26412;&#25928;&#29575;&#36739;&#20302;&#30340;&#27169;&#22411;&#29978;&#33267;&#26356;&#20026;&#20581;&#22766;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#36890;&#29992;&#26041;&#27861;&#19981;&#22826;&#21487;&#33021;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-of-distribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks, three broadly-applicable modeling interventions (increasing model size, using a different adaptation method, and pre-training on more data), and 14 diverse datasets to investigate the relationship between sample efficiency (amount of data needed to reach a given ID accuracy) and robustness (how models fare on OOD evaluation). We find that higher sample efficiency is only correlated with better average OOD robustness on some modeling interventions and tasks, but not others. On individual datasets, models with lower sample efficiency can even be more robust. These results suggest that general-purpose methods for improving sample efficiency are unlikely to yield universal OOD robustness impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#26816;&#32034;&#22686;&#24378;&#21151;&#33021;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.06345</link><description>&lt;p&gt;
&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Variational Open-Domain Question Answering. (arXiv:2210.06345v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#26816;&#32034;&#22686;&#24378;&#21151;&#33021;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#23545;&#23427;&#20204;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#37325;&#28857;&#25918;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#21644;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#12290;VOD&#30446;&#26631;&#26159;&#19968;&#31181;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#65292;&#36817;&#20284;&#20110;&#20219;&#21153;&#36793;&#32536;&#20284;&#28982;&#65292;&#24182;&#22312;&#19968;&#20010;&#36741;&#21161;&#37319;&#26679;&#20998;&#24067;&#65288;&#32531;&#23384;&#30340;&#26816;&#32034;&#22120;&#21644;/&#25110;&#36817;&#20284;&#21518;&#39564;&#65289;&#20013;&#36827;&#34892;&#26679;&#26412;&#25277;&#21462;&#35780;&#20272;&#12290;&#23427;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#26159;&#22312;&#23545;&#22823;&#37327;&#35821;&#26009;&#24211;&#23450;&#20041;&#30340;&#26816;&#32034;&#22120;&#20998;&#24067;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#38024;&#23545;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#30340;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;VOD&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;MedMCQA&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#36229;&#36807;&#20102;&#39046;&#22495;&#24494;&#35843;&#30340;Med-PaLM 5.3&#65285;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#21442;&#25968;&#23569;&#20102;2500&#20493;&#12290;&#25105;&#20204;&#30340;&#26816;&#32034;&#22686;&#24378;BioLinkBERT&#27169;&#22411;&#24471;&#20998;&#20026;62.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented models have proven to be effective in natural language processing tasks, yet there remains a lack of research on their optimization using variational inference. We introduce the Variational Open-Domain (VOD) framework for end-to-end training and evaluation of retrieval-augmented models, focusing on open-domain question answering and language modelling. The VOD objective, a self-normalized estimate of the R\'enyi variational bound, approximates the task marginal likelihood and is evaluated under samples drawn from an auxiliary sampling distribution (cached retriever and/or approximate posterior). It remains tractable, even for retriever distributions defined on large corpora. We demonstrate VOD's versatility by training reader-retriever BERT-sized models on multiple-choice medical exam questions. On the MedMCQA dataset, we outperform the domain-tuned Med-PaLM by +5.3% despite using 2.500$\times$ fewer parameters. Our retrieval-augmented BioLinkBERT model scored 62.9%
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#20351;&#31070;&#32463;ODE&#28385;&#36275;&#36755;&#20986;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#35777;&#20102;&#36755;&#20986;&#35268;&#33539;&#65292;&#19988;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#21463;&#38480;&#21442;&#25968;/&#36755;&#20837;&#36827;&#34892;&#25805;&#20316;&#65292;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21019;&#36896;&#20102;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04763</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;ODE&#30340;&#27491;&#21521;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Forward Invariance of Neural ODEs. (arXiv:2210.04763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#20351;&#31070;&#32463;ODE&#28385;&#36275;&#36755;&#20986;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#35777;&#20102;&#36755;&#20986;&#35268;&#33539;&#65292;&#19988;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#21463;&#38480;&#21442;&#25968;/&#36755;&#20837;&#36827;&#34892;&#25805;&#20316;&#65292;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21019;&#36896;&#20102;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#38598;&#20256;&#25773;&#26469;&#30830;&#20445;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#28385;&#36275;&#36755;&#20986;&#35268;&#33539;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#31867;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#23558;&#36755;&#20986;&#35268;&#33539;&#36716;&#25442;&#20026;&#23545;&#23398;&#20064;&#31995;&#32479;&#30340;&#21442;&#25968;&#21644;&#36755;&#20837;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290; &#36825;&#20010;&#35774;&#32622;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#25913;&#21464;&#21463;&#38480;&#21442;&#25968;/&#36755;&#20837;&#26469;&#23454;&#29616;&#36755;&#20986;&#35268;&#33539;&#20445;&#35777;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#25511;&#21046;&#30340;&#31070;&#32463;ODE&#30340;&#19981;&#21464;&#38598;&#20256;&#25773;&#19981;&#20165;&#20445;&#25345;&#20102;&#27010;&#25324;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#21551;&#29992;&#23545;&#31995;&#32479;&#21442;&#25968;/&#36755;&#20837;&#30340;&#22240;&#26524;&#25805;&#20316;&#21019;&#36896;&#20102;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290; &#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#29289;&#29702;&#21160;&#21147;&#23398;&#21644;&#20984;&#24615;&#30011;&#20687;&#65292;&#20197;&#21450;&#33258;&#20027;&#36710;&#36742;&#30340;&#23433;&#20840;&#36991;&#30896;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method to ensure neural ordinary differential equations (ODEs) satisfy output specifications by using invariance set propagation. Our approach uses a class of control barrier functions to transform output specifications into constraints on the parameters and inputs of the learning system. This setup allows us to achieve output specification guarantees simply by changing the constrained parameters/inputs both during training and inference. Moreover, we demonstrate that our invariance set propagation through data-controlled neural ODEs not only maintains generalization performance but also creates an additional degree of robustness by enabling causal manipulation of the system's parameters/inputs. We test our method on a series of representation learning tasks, including modeling physical dynamics and convexity portraits, as well as safe collision avoidance for autonomous vehicles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;&#22914;&#26524;&#38543;&#26426;&#25513;&#30721;&#30340;&#23485;&#24230;&#27604;&#31232;&#30095;&#24615;&#30340;&#20498;&#25968;&#30340;&#23545;&#25968;&#22240;&#23376;&#22823;&#65292;&#21017;&#23427;&#20204;&#21487;&#20197;&#36817;&#20284;&#20219;&#24847;&#30446;&#26631;&#32593;&#32476;&#65292;&#22240;&#27492;&#38543;&#26426;&#21098;&#26525;&#36275;&#20197;&#21551;&#21160;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#19988;&#20219;&#20309;&#20174;&#23494;&#38598;&#21040;&#31232;&#30095;&#30340;&#35757;&#32451;&#26041;&#26696;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#35745;&#31639;&#19978;&#26356;&#26377;&#25928;&#30340;&#20174;&#31232;&#30095;&#21040;&#31232;&#30095;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.02412</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38543;&#26426;&#21098;&#26525;&#36275;&#20197;&#21551;&#21160;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Why Random Pruning Is All We Need to Start Sparse. (arXiv:2210.02412v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;&#22914;&#26524;&#38543;&#26426;&#25513;&#30721;&#30340;&#23485;&#24230;&#27604;&#31232;&#30095;&#24615;&#30340;&#20498;&#25968;&#30340;&#23545;&#25968;&#22240;&#23376;&#22823;&#65292;&#21017;&#23427;&#20204;&#21487;&#20197;&#36817;&#20284;&#20219;&#24847;&#30446;&#26631;&#32593;&#32476;&#65292;&#22240;&#27492;&#38543;&#26426;&#21098;&#26525;&#36275;&#20197;&#21551;&#21160;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#19988;&#20219;&#20309;&#20174;&#23494;&#38598;&#21040;&#31232;&#30095;&#30340;&#35757;&#32451;&#26041;&#26696;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#35745;&#31639;&#19978;&#26356;&#26377;&#25928;&#30340;&#20174;&#31232;&#30095;&#21040;&#31232;&#30095;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#35777;&#26126;&#65292;&#38543;&#26426;&#21098;&#26525;&#21487;&#20197;&#23450;&#20041;&#20986;&#38750;&#24120;&#26377;&#25928;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#24471;&#21040;&#30340;&#31232;&#30095;&#32593;&#32476;&#36890;&#24120;&#21487;&#20197;&#19982;&#23494;&#38598;&#32467;&#26500;&#21644;&#26368;&#20808;&#36827;&#30340;&#8220;&#20013;&#24425;&#31080;&#8221;&#21098;&#26525;&#31639;&#27861;&#31454;&#20105;&#65292;&#23613;&#31649;&#23427;&#20204;&#19981;&#20381;&#36182;&#20110;&#35745;&#31639;&#26114;&#36149;&#30340;&#21098;&#26525;-&#35757;&#32451;&#36845;&#20195;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21021;&#22987;&#38454;&#27573;&#32472;&#21046;&#32780;&#19981;&#38656;&#35201;&#36807;&#22810;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#22914;&#26524;&#38543;&#26426;&#25513;&#30721;&#30340;&#23485;&#24230;&#27604;&#31232;&#30095;&#24615;&#30340;&#20498;&#25968;&#30340;&#23545;&#25968;&#22240;&#23376;&#22823;&#65292;&#21017;&#23427;&#20204;&#21487;&#20197;&#36817;&#20284;&#20219;&#24847;&#30446;&#26631;&#32593;&#32476;&#12290;&#36825;&#31181;&#36229;&#36807;&#21442;&#25968;&#21270;&#22240;&#23376;&#33267;&#23569;&#23545;&#20110;&#19977;&#23618;&#38543;&#26426;&#32593;&#32476;&#26159;&#24517;&#35201;&#30340;&#65292;&#36825;&#35299;&#37322;&#20102;&#38543;&#26426;&#32593;&#32476;&#22312;&#26356;&#39640;&#30340;&#31232;&#30095;&#24230;&#19979;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#31561;&#21040;&#39640;&#31232;&#30095;&#24230;&#27700;&#24179;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#30528;&#26356;&#31232;&#30095;&#30340;&#32593;&#32476;&#21253;&#21547;&#22312;&#38543;&#26426;&#30340;&#28304;&#32593;&#32476;&#20013;&#65292;&#22240;&#27492;&#20219;&#20309;&#20174;&#23494;&#38598;&#21040;&#31232;&#30095;&#30340;&#35757;&#32451;&#26041;&#26696;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#35745;&#31639;&#19978;&#26356;&#26377;&#25928;&#30340;&#20174;&#31232;&#30095;&#21040;&#31232;&#30095;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity $1 / \log(1/\text{sparsity})$. This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#31354;&#38388;&#34920;&#31034;&#20316;&#20026;&#31354;&#38388;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#26469;&#27714;&#35299;&#26102;&#21464;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#29616;&#26377;&#27714;&#35299;&#22120;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2210.00124</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#21464;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#31070;&#32463;&#31354;&#38388;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Spatial Representations for Time-dependent PDEs. (arXiv:2210.00124v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#31354;&#38388;&#34920;&#31034;&#20316;&#20026;&#31354;&#38388;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#26469;&#27714;&#35299;&#26102;&#21464;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#29616;&#26377;&#27714;&#35299;&#22120;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#31354;&#38388;&#34920;&#31034; (INSR) &#24050;&#34987;&#35777;&#26126;&#26159;&#31354;&#38388;&#20381;&#36182;&#21521;&#37327;&#22330;&#30340;&#26377;&#25928;&#34920;&#31034;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;INSR&#27714;&#35299;&#26102;&#21464;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#20256;&#32479;&#30340;PDE&#27714;&#35299;&#22120;&#24341;&#20837;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#31163;&#25955;&#21270;&#12290;&#24120;&#35265;&#30340;&#31354;&#38388;&#31163;&#25955;&#21270;&#21253;&#25324;&#32593;&#26684;&#21644;&#26080;&#32593;&#26684;&#28857;&#20113;&#65292;&#20854;&#20013;&#27599;&#20010;&#33258;&#30001;&#24230;&#23545;&#24212;&#20110;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#20301;&#32622;&#12290;&#34429;&#28982;&#36825;&#20123;&#26174;&#24335;&#31354;&#38388;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#30452;&#35266;&#22320;&#24314;&#27169;&#21644;&#29702;&#35299;&#65292;&#20294;&#36825;&#20123;&#34920;&#31034;&#26041;&#27861;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#65292;&#23545;&#20110;&#31934;&#24230;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#36866;&#24212;&#24615;&#37117;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#20445;&#25345;&#32463;&#20856;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65288;&#22914;&#26174;&#24335;/&#38544;&#24335;Euler&#65289;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;INSR&#20316;&#20026;&#26367;&#20195;&#31354;&#38388;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31354;&#38388;&#20449;&#24687;&#38544;&#21547;&#22312;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20013;&#12290;&#28982;&#21518;&#36890;&#36807;&#26102;&#38388;&#31215;&#20998;&#65292;&#32593;&#32476;&#26435;&#37325;&#38543;&#26102;&#38388;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#29616;&#26377;&#27714;&#35299;&#22120;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#26412;&#36523;&#23601;&#26159;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Spatial Representation (INSR) has emerged as an effective representation of spatially-dependent vector fields. This work explores solving time-dependent PDEs with INSR. Classical PDE solvers introduce both temporal and spatial discretizations. Common spatial discretizations include meshes and meshless point clouds, where each degree-of-freedom corresponds to a location in space. While these explicit spatial correspondences are intuitive to model and understand, these representations are not necessarily optimal for accuracy, memory usage, or adaptivity. Keeping the classical temporal discretization unchanged (e.g., explicit/implicit Euler), we explore INSR as an alternative spatial discretization, where spatial information is implicitly stored in the neural network weights. The network weights then evolve over time via time integration. Our approach does not require any training data generated by existing solvers because our approach is the solver itself. We validate our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#25968;&#25454;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#24182;&#37327;&#21270;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#65292;&#33021;&#22815;&#26816;&#27979;&#22797;&#26434;&#31354;&#38388;&#21644;&#22270;&#20687;&#20013;&#30340;&#22855;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00069</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Topological Singularity Detection at Multiple Scales. (arXiv:2210.00069v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#25968;&#25454;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#24182;&#37327;&#21270;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#65292;&#33021;&#22815;&#26816;&#27979;&#22797;&#26434;&#31354;&#38388;&#21644;&#22270;&#20687;&#20013;&#30340;&#22855;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35774;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#65292;&#23427;&#20551;&#23450;&#25968;&#25454;&#20301;&#20110;&#25110;&#25509;&#36817;&#20110;&#20302;&#22266;&#26377;&#32500;&#24230;&#30340;&#26410;&#30693;&#27969;&#24418;&#19978;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#38750;&#27969;&#24418;&#32467;&#26500;&#65292;&#21363;&#22855;&#24322;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#36825;&#31181;&#22855;&#24322;&#24615;&#22312;&#25554;&#20540;&#21644;&#25512;&#26029;&#20219;&#21153;&#20043;&#21069;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#25299;&#25169;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#65288;i&#65289;&#37327;&#21270;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#20135;&#29983;&#8220;&#27431;&#20960;&#37324;&#24471;&#24615;&#8221;&#35780;&#20998;&#65292;&#29992;&#20197;&#35780;&#20272;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22270;&#20687;&#25968;&#25454;&#20013;&#25429;&#33719;&#22797;&#26434;&#31354;&#38388;&#30340;&#22855;&#24322;&#24615;&#65292;&#21516;&#26102;&#25429;&#25417;&#22855;&#24322;&#32467;&#26500;&#21644;&#23616;&#37096;&#20960;&#20309;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#23454;&#29992;&#21644;&#20934;&#30830;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.15315</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#21453;&#24212;&#23454;&#29616;&#20998;&#23376;&#34920;&#31034;&#34701;&#21512;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
FusionRetro: Molecule Representation Fusion via In-context Reactions for Retrosynthetic Planning. (arXiv:2209.15315v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#23454;&#29992;&#21644;&#20934;&#30830;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21512;&#25104;&#35268;&#21010;&#30340;&#30446;&#26631;&#26159;&#20174;&#36215;&#22987;&#29289;&#36136;&#21040;&#30446;&#26631;&#20998;&#23376;&#35774;&#35745;&#20986;&#19968;&#20010;&#23436;&#25972;&#30340;&#22810;&#27493;&#21512;&#25104;&#36335;&#32447;&#12290;&#24403;&#21069;&#31574;&#30053;&#37319;&#29992;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#21363;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#21644;&#25628;&#32034;&#31639;&#27861;&#65292;&#21482;&#23558;&#20135;&#29289;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#27599;&#20010;&#35268;&#21010;&#27493;&#39588;&#30340;&#21453;&#24212;&#29289;&#65292;&#24182;&#24573;&#30053;&#20102;&#27839;&#30528;&#21512;&#25104;&#36335;&#32447;&#30340;&#26377;&#20215;&#20540;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25913;&#21892;&#21453;&#21512;&#25104;&#35268;&#21010;&#12290;&#25105;&#20204;&#23558;&#21512;&#25104;&#36335;&#32447;&#35270;&#20026;&#21453;&#24212;&#22270;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#19977;&#20010;&#21407;&#21017;&#27493;&#39588;&#26469;&#25972;&#21512;&#20449;&#24687;&#21644;&#39044;&#27979;&#21453;&#24212;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#19978;&#19979;&#25991;&#21453;&#24212;&#36827;&#34892;&#21453;&#21512;&#25104;&#35268;&#21010;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25972;&#20010;&#26694;&#26550;&#21487;&#20197;&#39640;&#25928;&#22320;&#31471;&#21040;&#31471;&#22320;&#20248;&#21270;&#65292;&#20135;&#29983;&#26356;&#23454;&#29992;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#34701;&#21512;&#20998;&#23376;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthetic planning aims to devise a complete multi-step synthetic route from starting materials to a target molecule. Current strategies use a decoupled approach of single-step retrosynthesis models and search algorithms, taking only the product as the input to predict the reactants for each planning step and ignoring valuable context information along the synthetic route. In this work, we propose a novel framework that utilizes context information for improved retrosynthetic planning. We view synthetic routes as reaction graphs and propose to incorporate context through three principled steps: encode molecules into embeddings, aggregate information over routes, and readout to predict reactants. Our approach is the first attempt to utilize in-context reactions for retrosynthetic planning. The entire framework can be efficiently optimized in an end-to-end fashion and produce more practical and accurate predictions. Comprehensive experiments demonstrate that by fusing in the context
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26694;&#26550;&#65292;&#20026;&#27599;&#20010;&#35266;&#27979;&#20540;&#25552;&#20379;&#31232;&#30095;&#30340;&#23616;&#37096;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#24182;&#23558;&#36825;&#20123;&#35268;&#21017;&#32858;&#21512;&#25104;&#21306;&#22495;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#20197;&#36866;&#24212;&#19981;&#31283;&#23450;&#30340;&#23454;&#29616;&#29615;&#22659;&#65292;&#24182;&#20135;&#29983;&#31283;&#20581;&#30340;&#25937;&#27982;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2209.14568</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#20316;&#20026;&#23616;&#37096;&#21644;&#21306;&#22495;&#21453;&#20107;&#23454;&#25919;&#31574;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies. (arXiv:2209.14568v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26694;&#26550;&#65292;&#20026;&#27599;&#20010;&#35266;&#27979;&#20540;&#25552;&#20379;&#31232;&#30095;&#30340;&#23616;&#37096;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#24182;&#23558;&#36825;&#20123;&#35268;&#21017;&#32858;&#21512;&#25104;&#21306;&#22495;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#20197;&#36866;&#24212;&#19981;&#31283;&#23450;&#30340;&#23454;&#29616;&#29615;&#22659;&#65292;&#24182;&#20135;&#29983;&#31283;&#20581;&#30340;&#25937;&#27982;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#38754;&#20020;&#30528;&#35768;&#22810;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#22914;&#30830;&#20445;&#31283;&#23450;&#24615;&#12289;&#32508;&#21512;&#22810;&#20010;CE&#20197;&#21450;&#25552;&#20379;&#21512;&#29702;&#24615;&#21644;&#31232;&#30095;&#24615;&#20445;&#35777;&#12290;&#20174;&#26356;&#23454;&#38469;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#35268;&#23450;&#30340;&#21453;&#20107;&#23454;&#25937;&#27982;&#25514;&#26045;&#36890;&#24120;&#19981;&#20250;&#34987;&#20010;&#20307;&#23436;&#20840;&#23454;&#26045;&#65292;&#24182;&#35777;&#26126;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;CE&#31639;&#27861;&#22312;&#36825;&#31181;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#24456;&#21487;&#33021;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#26694;&#26550;&#65292;&#20026;&#27599;&#20010;&#35266;&#27979;&#20540;&#25552;&#20379;&#31232;&#30095;&#30340;&#23616;&#37096;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#25552;&#20379;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#25913;&#21464;&#20915;&#31574;&#30340;&#20540;&#33539;&#22260;&#30340;&#35268;&#21017;&#12290;&#36825;&#20123;&#35268;&#21017;&#20316;&#20026;&#22810;&#26679;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24635;&#32467;&#65292;&#24182;&#20135;&#29983;&#31283;&#20581;&#30340;&#25937;&#27982;&#25514;&#26045;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#36825;&#20123;&#23616;&#37096;&#35268;&#21017;&#32858;&#21512;&#25104;&#21306;&#22495;&#21453;&#20107;&#23454;&#35268;&#21017;&#65292;&#35782;&#21035;&#25968;&#25454;&#23376;&#32452;&#30340;&#20849;&#20139;&#25937;&#27982;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#23616;&#37096;&#21644;&#21306;&#22495;&#35268;&#21017;&#26469;&#33258;&#20110;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CE) face several unresolved challenges, such as ensuring stability, synthesizing multiple CEs, and providing plausibility and sparsity guarantees. From a more practical point of view, recent studies [Pawelczyk et al., 2022] show that the prescribed counterfactual recourses are often not implemented exactly by individuals and demonstrate that most state-of-the-art CE algorithms are very likely to fail in this noisy environment. To address these issues, we propose a probabilistic framework that gives a sparse local counterfactual rule for each observation, providing rules that give a range of values capable of changing decisions with high probability. These rules serve as a summary of diverse counterfactual explanations and yield robust recourses. We further aggregate these local rules into a regional counterfactual rule, identifying shared recourses for subgroups of the data. Our local and regional rules are derived from the Random Forest algorithm, which of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32858;&#31867;&#21644;&#39044;&#27979;&#27169;&#22411;&#26469;&#23398;&#20064;&#28216;&#25103;&#26234;&#33021;&#20307;&#32676;&#38598;&#30340;&#28436;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Hebbian &#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512; LSTM &#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#20934;&#30830;&#39044;&#27979;&#32676;&#38598;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.06904</link><description>&lt;p&gt;
&#22522;&#20110;&#36203;&#27604;&#23398;&#20064;&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#32676;&#38598;&#28436;&#21270;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Forecasting Evolution of Clusters in Game Agents with Hebbian Learning. (arXiv:2209.06904v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32858;&#31867;&#21644;&#39044;&#27979;&#27169;&#22411;&#26469;&#23398;&#20064;&#28216;&#25103;&#26234;&#33021;&#20307;&#32676;&#38598;&#30340;&#28436;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Hebbian &#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512; LSTM &#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#20934;&#30830;&#39044;&#27979;&#32676;&#38598;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20363;&#22914;&#23454;&#26102;&#31574;&#30053;&#28216;&#25103;&#36890;&#24120;&#34987;&#26234;&#33021;&#20307;&#30340;&#38598;&#20307;&#34892;&#20026;&#25152;&#39537;&#21160;&#12290;&#20363;&#22914;&#22312;&#26143;&#38469;&#20105;&#38712;II&#20013;&#65292;&#20154;&#31867;&#29609;&#23478;&#20250;&#23558;&#31354;&#38388;&#25509;&#36817;&#30340;&#26234;&#33021;&#20307;&#20998;&#32452;&#25104;&#22242;&#38431;&#65292;&#24182;&#25511;&#21046;&#22242;&#38431;&#20987;&#36133;&#25932;&#20154;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28216;&#25103;&#20013;&#30340;&#26234;&#33021;&#20307;&#36827;&#34892;&#32858;&#31867;&#24050;&#34987;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#25511;&#21046;&#20197;&#21450;&#25552;&#20379;&#32473;&#28216;&#25103;&#29992;&#25143;&#30340;&#28216;&#25103;&#20998;&#26512;&#24037;&#20855;&#31561;&#31561;&#22810;&#20010;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#32858;&#31867;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#26159;&#22312;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#32676;&#38598;&#32423;&#21035;&#19978;&#30340;&#21160;&#24577;&#23398;&#20064;&#26041;&#38754;&#36824;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22411;AI&#27169;&#22411;&#65292;&#23558;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26469;&#39044;&#27979;&#26143;&#38469;&#20105;&#38712;II&#20013;&#32676;&#38598;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340; Hebbian &#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312; Set-to-Cluster &#27169;&#22359;&#20013;&#39640;&#25928;&#22320;&#21019;&#24314;&#21487;&#21464;&#25968;&#37327;&#30340;&#32676;&#38598;&#65292;&#20854;&#25512;&#29702;&#26102;&#38388;&#22797;&#26434;&#24230;&#20302;&#20110; K-means &#32858;&#31867;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#39044;&#27979;&#27169;&#22411;&#23398;&#20064;&#32676;&#38598;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#32676;&#38598;&#30340;&#24402;&#23646;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#22312;&#28216;&#25103;&#30340;&#22810;&#20010;&#22330;&#26223;&#19979;&#20934;&#30830;&#39044;&#27979;&#32676;&#38598;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multi-agent systems such as real-time strategy games are often driven by collective behavior of agents. For example, in StarCraft II, human players group spatially near agents into a team and control the team to defeat opponents. In this light, clustering the agents in the game has been used for various purposes such as the efficient control of the agents in multi-agent reinforcement learning and game analytic tools for the game users. However, despite the useful information provided by clustering, learning the dynamics of multi-agent systems at a cluster level has been rarely studied yet. In this paper, we present a hybrid AI model that couples unsupervised and self-supervised learning to forecast evolution of the clusters in StarCraft II. We develop an unsupervised Hebbian learning method in a set-to-cluster module to efficiently create a variable number of the clusters with lower inference time complexity than K-means clustering. Also, a long short-term memory based prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20114;&#34917;&#26680;&#23398;&#20064;&#65288;BCKL&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#26680;&#21270;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#21644;&#30701;&#31243;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#21487;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#22797;&#26434;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.09978</link><description>&lt;p&gt;
&#22810;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#36125;&#21494;&#26031;&#20114;&#34917;&#26680;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Complementary Kernelized Learning for Multidimensional Spatiotemporal Data. (arXiv:2208.09978v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20114;&#34917;&#26680;&#23398;&#20064;&#65288;BCKL&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#26680;&#21270;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#21644;&#30701;&#31243;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#21487;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#22797;&#26434;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#27010;&#29575;&#24314;&#27169;&#23545;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26102;&#31354;&#25968;&#25454;&#24448;&#24448;&#34920;&#29616;&#20986;&#38750;&#24179;&#31283;&#21644;&#38750;&#21487;&#20998;&#31163;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#24320;&#21457;&#26377;&#25928;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32479;&#35745;&#27169;&#22411;&#20197;&#36866;&#24212;&#21516;&#26102;&#21253;&#21547;&#38271;&#31243;&#21644;&#30701;&#31243;&#21464;&#21270;&#30340;&#38750;&#31283;&#24577;/&#19981;&#21487;&#20998;&#31163;&#36807;&#31243;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#30772;&#22351;/&#32570;&#22833;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550; - &#36125;&#21494;&#26031;&#20114;&#34917;&#26680;&#23398;&#20064;&#65288;BCKL&#65289; - &#29992;&#20110;&#23454;&#29616;&#22810;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#27010;&#29575;&#24314;&#27169;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#34920;&#24449;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;BCKL&#38598;&#25104;&#20102;&#20004;&#20010;&#20114;&#34917;&#26041;&#27861;&#8212;&#8212;&#26680;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#21644;&#30701;&#31243;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#32447;&#24615;&#20302;&#31209;&#22240;&#23376;&#20998;&#35299;&#32452;&#20214;&#26469;&#25429;&#33719;&#20840;&#23616;/&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#24182;&#20351;&#29992;&#30701;&#31243;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#26469;&#25429;&#33719;&#23616;&#37096;/&#30701;&#31243;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic modeling of multidimensional spatiotemporal data is critical to many real-world applications. As real-world spatiotemporal data often exhibits complex dependencies that are nonstationary and nonseparable, developing effective and computationally efficient statistical models to accommodate nonstationary/nonseparable processes containing both long-range and short-scale variations becomes a challenging task, in particular for large-scale datasets with various corruption/missing structures. In this paper, we propose a new statistical framework -- Bayesian Complementary Kernelized Learning (BCKL) -to achieve scalable probabilistic modeling for multidimensional spatiotemporal data. To effectively characterize complex dependencies, BCKL integrates two complementary approaches -- kernelized low-rank tensor factorization and short-range spatiotemporal Gaussian Processes. Specifically, we use a multi-linear low-rank factorization component to capture the global/long-range correla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#35299;&#32806;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20154;&#21338;&#24328;&#20013;&#33021;&#22815;&#20351;&#27599;&#20010;&#29609;&#23478;&#30340;&#35302;&#21457;&#21518;&#24724;&#25353;$O(\log T)$&#22686;&#38271;&#65292;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#24182;&#19988;&#26500;&#24314;&#21033;&#29992;&#20102;&#19968;&#20010;&#26356;&#20026;&#19968;&#33324;&#30340;&#30001;&#20855;&#26377;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26377;&#29702;&#20989;&#25968;&#23548;&#20986;&#30340;&#19981;&#21160;&#28857;&#32467;&#26524;&#65292;&#20197;&#21450;&#19968;&#20010;&#20984;&#21253;&#30340;&#26356;&#32454;&#33268;&#30340;&#21518;&#24724;&#30005;&#36335;&#65292;&#20445;&#30041;&#20102;RVU&#21512;&#36866;&#24615;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.09747</link><description>&lt;p&gt;
&#21338;&#24328;&#26641;&#20013;&#30340;NEAR-OPTIMAL PHI-REGRET&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal $\Phi$-Regret Learning in Extensive-Form Games. (arXiv:2208.09747v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#35299;&#32806;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20154;&#21338;&#24328;&#20013;&#33021;&#22815;&#20351;&#27599;&#20010;&#29609;&#23478;&#30340;&#35302;&#21457;&#21518;&#24724;&#25353;$O(\log T)$&#22686;&#38271;&#65292;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#24182;&#19988;&#26500;&#24314;&#21033;&#29992;&#20102;&#19968;&#20010;&#26356;&#20026;&#19968;&#33324;&#30340;&#30001;&#20855;&#26377;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26377;&#29702;&#20989;&#25968;&#23548;&#20986;&#30340;&#19981;&#21160;&#28857;&#32467;&#26524;&#65292;&#20197;&#21450;&#19968;&#20010;&#20984;&#21253;&#30340;&#26356;&#32454;&#33268;&#30340;&#21518;&#24724;&#30005;&#36335;&#65292;&#20445;&#30041;&#20102;RVU&#21512;&#36866;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#20154;&#23436;&#32654;&#22238;&#24518;&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#20013;&#24314;&#31435;&#20102;&#26377;&#25928;&#21644;&#35299;&#32806;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#20197;&#20415;&#27599;&#20010;&#29609;&#23478;&#30340;&#35302;&#21457;&#21518;&#24724;&#22312;T&#27425;&#28216;&#25103;&#37325;&#22797;&#21518;&#25353;$O(\log T)$&#22686;&#38271;&#12290;&#36825;&#30456;&#23545;&#20110;&#20808;&#21069;&#24050;&#30693;&#30340;&#35302;&#21457;&#21518;&#24724;&#36793;&#30028;$O(T^{1/4})$&#26377;&#25351;&#25968;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#35299;&#20915;&#20102;Bai&#31561;&#20154;&#65288;2022&#65289;&#25552;&#20986;&#30340;&#19968;&#20010;&#26368;&#36817;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#20316;&#20026;&#30452;&#25509;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#20445;&#35777;&#20197;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#24230;$\frac{\log T}{T}$&#25910;&#25947;&#21040;&#24191;&#27867;&#24418;&#24335;&#30340;&#30456;&#20851;&#22343;&#34913;&#21644;&#31895;&#30053;&#30340;&#30456;&#20851;&#22343;&#34913;&#12290;&#22312;&#29616;&#26377;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#26500;&#24314;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26356;&#20026;&#19968;&#33324;&#30340;&#30001;&#20855;&#26377;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26377;&#29702;&#20989;&#25968;&#23548;&#20986;&#30340;&#19981;&#21160;&#28857;&#32467;&#26524;&#65292;&#36825;&#26159;&#25105;&#20204;&#20026;&#35302;&#21457;&#20559;&#24046;&#20989;&#25968;&#65288;&#31895;&#30053;&#30340;&#65289;&#22266;&#23450;&#28857;&#25152;&#24314;&#31435;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26500;&#24314;&#21033;&#29992;&#20102;&#20984;&#21253;&#30340;&#26356;&#32454;&#33268;&#30340;&#21518;&#24724;&#30005;&#36335;&#65292;&#19982;&#20808;&#21069;&#30340;&#20445;&#35777;&#19981;&#21516;&#65292;&#23427;&#20445;&#30041;&#20102;RVU&#21512;&#36866;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish efficient and uncoupled learning dynamics so that, when employed by all players in multiplayer perfect-recall imperfect-information extensive-form games, the trigger regret of each player grows as $O(\log T)$ after $T$ repetitions of play. This improves exponentially over the prior best known trigger-regret bound of $O(T^{1/4})$, and settles a recent open question by Bai et al. (2022). As an immediate consequence, we guarantee convergence to the set of extensive-form correlated equilibria and coarse correlated equilibria at a near-optimal rate of $\frac{\log T}{T}$.  Building on prior work, at the heart of our construction lies a more general result regarding fixed points deriving from rational functions with polynomial degree, a property that we establish for the fixed points of (coarse) trigger deviation functions. Moreover, our construction leverages a refined regret circuit for the convex hull, which -- unlike prior guarantees -- preserves the RVU proper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#33539;&#20363;&#65292;&#21517;&#20026;ILLUME&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#20154;&#30340;&#24605;&#32500;&#26041;&#24335;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#65292;ILLUME&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.08241</link><description>&lt;p&gt;
ILLUME&#65306;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#33539;&#20363;&#65292;&#21517;&#20026;ILLUME&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#20154;&#30340;&#24605;&#32500;&#26041;&#24335;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#65292;ILLUME&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#23548;&#24050;&#34987;&#35777;&#26126;&#26159;&#26500;&#24314;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#24456;&#23569;&#19982;&#29992;&#25143;&#23545;&#29305;&#23450;&#31572;&#26696;&#30340;&#29702;&#24615;&#30456;&#19968;&#33268;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#23545;&#40784;&#24182;&#21152;&#24378;&#24120;&#35782;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#26426;&#29983;&#25104;&#25968;&#25454;&#30340;&#35843;&#25972;&#33539;&#20363;&#12290;&#25105;&#20204;&#30340;ILLUME&#25191;&#34892;&#20197;&#19979;&#24490;&#29615;&#65306;&#32473;&#23450;&#19968;&#20010;&#22270;&#20687;-&#38382;&#39064;-&#31572;&#26696;&#25552;&#31034;&#65292;VLM&#26679;&#26412;&#22810;&#20010;&#20505;&#36873;&#21407;&#29702;&#65292;&#20154;&#31867;&#35780;&#35770;&#23478;&#36890;&#36807;&#20559;&#22909;&#36873;&#25321;&#25552;&#20379;&#21453;&#39304;&#65292;&#29992;&#20110;&#24494;&#35843;&#12290;&#36825;&#20010;&#24490;&#29615;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36880;&#28176;&#38613;&#21051;&#20986;&#19982;&#20154;&#31867;&#24847;&#22270;&#30456;&#19968;&#33268;&#30340;VLM&#30340;&#29702;&#24615;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35814;&#23613;&#23454;&#39564;&#34920;&#26126;&#65292;ILLUME&#22312;&#20351;&#29992; significantly &#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20165;&#38656;&#35201; minimal &#21453;&#39304;&#30340;&#21516;&#26102;&#65292;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#20869;&#26680;&#29615;&#22659;&#19979;&#30340;&#26080;&#38480;&#23485;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#28145;&#23618;CNN&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#30340;&#31354;&#38388;&#23610;&#24230;&#65292;&#21363;&#20351;&#25968;&#25454;&#27809;&#26377;&#23616;&#37096;&#32467;&#26500;&#65292;&#28145;&#23618;CNN&#20063;&#21487;&#20197;&#23398;&#20064;&#65292;&#21482;&#35201;&#20840;&#23616;&#32467;&#26500;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.01003</link><description>&lt;p&gt;
&#23485;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#21040;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Can Be Learnt With Wide Convolutional Neural Networks?. (arXiv:2208.01003v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#20869;&#26680;&#29615;&#22659;&#19979;&#30340;&#26080;&#38480;&#23485;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#28145;&#23618;CNN&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#30340;&#31354;&#38388;&#23610;&#24230;&#65292;&#21363;&#20351;&#25968;&#25454;&#27809;&#26377;&#23616;&#37096;&#32467;&#26500;&#65292;&#28145;&#23618;CNN&#20063;&#21487;&#20197;&#23398;&#20064;&#65292;&#21482;&#35201;&#20840;&#23616;&#32467;&#26500;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;&#39640;&#32500;&#20989;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#20102;&#33258;&#28982;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#23616;&#37096;&#21644;&#20998;&#23618;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#22914;&#27492;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#30340;&#37327;&#21270;&#29702;&#35299;&#65292;&#22914;&#27867;&#21270;&#35823;&#24046;&#38543;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#30340;&#34928;&#20943;&#36895;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20869;&#26680;&#29615;&#22659;&#19979;&#30340;&#26080;&#38480;&#23485;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#24212;&#26680;&#30340;&#35889;&#27839;&#34989;&#20102;&#32593;&#32476;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#34920;&#24449;&#20102;&#20854;&#28176;&#36827;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#19982;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#32467;&#21512;&#36215;&#26469;&#65292;&#35777;&#26126;&#20102;&#28145;&#23618;CNN&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#30340;&#31354;&#38388;&#23610;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#30446;&#26631;&#20989;&#25968;&#20381;&#36182;&#20110;&#30456;&#37051;&#36755;&#20837;&#21464;&#37327;&#30340;&#20302;&#32500;&#23376;&#38598;&#65292;&#21017;&#35823;&#24046;&#30340;&#34928;&#20943;&#21463;&#21040;&#36825;&#20123;&#23376;&#38598;&#30340;&#26377;&#25928;&#32500;&#25968;&#30340;&#25511;&#21046;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#20989;&#25968;&#20381;&#36182;&#20110;&#39640;&#32500;&#32467;&#26500;&#65292;&#21017;&#26377;&#25928;&#32500;&#25968;&#21463;&#32593;&#32476;&#23485;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#25968;&#25454;&#27809;&#26377;&#23616;&#37096;&#32467;&#26500;&#65292;&#28145;&#23618;CNN&#20063;&#21487;&#20197;&#23398;&#20064;&#65292;&#21482;&#35201;&#20840;&#23616;&#32467;&#26500;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how convolutional neural networks (CNNs) can efficiently learn high-dimensional functions remains a fundamental challenge. A popular belief is that these models harness the local and hierarchical structure of natural data such as images. Yet, we lack a quantitative understanding of how such structure affects performance, e.g., the rate of decay of the generalisation error with the number of training samples. In this paper, we study infinitely-wide deep CNNs in the kernel regime. First, we show that the spectrum of the corresponding kernel inherits the hierarchical structure of the network, and we characterise its asymptotics. Then, we use this result together with generalisation bounds to prove that deep CNNs adapt to the spatial scale of the target function. In particular, we find that if the target function depends on low-dimensional subsets of adjacent input variables, then the decay of the error is controlled by the effective dimensionality of these subsets. Conversel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#27493;&#24341;&#20837;&#22122;&#22768;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.07827</link><description>&lt;p&gt;
&#22810;&#20803;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting. (arXiv:2207.07827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#27493;&#24341;&#20837;&#22122;&#22768;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;(M-LSTF)&#26159;&#19968;&#20010;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19981;&#21516;&#65292;M-LSTF&#20219;&#21153;&#20174;&#20004;&#20010;&#26041;&#38754;&#26356;&#20855;&#25361;&#25112;&#24615;&#65306;1) M-LSTF&#27169;&#22411;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#65307;2)&#22312;&#28378;&#21160;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#20004;&#20010;&#36830;&#32493;&#35757;&#32451;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#38543;&#30528;&#39044;&#27979;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#26356;&#26131;&#20110;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#26469;&#35299;&#20915;M-LSTF&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23618;&#38754;&#30340;&#35760;&#24518;&#32452;&#20214;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#24335;&#30340;&#26041;&#24335;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#36880;&#27493;&#22312;&#35757;&#32451;&#26679;&#26412;&#20013;&#24341;&#20837;&#20271;&#21162;&#21033;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate long sequence time-series forecasting (M-LSTF) is a practical but challenging problem. Unlike traditional timer-series forecasting tasks, M-LSTF tasks are more challenging from two aspects: 1) M-LSTF models need to learn time-series patterns both within and between multiple time features; 2) Under the rolling forecasting setting, the similarity between two consecutive training samples increases with the increasing prediction length, which makes models more prone to overfitting. In this paper, we propose a generalizable memory-driven Transformer to target M-LSTF problems. Specifically, we first propose a global-level memory component to drive the forecasting procedure by integrating multiple time-series features. In addition, we adopt a progressive fashion to train our model to increase its generalizability, in which we gradually introduce Bernoulli noises to training samples. Extensive experiments have been performed on five different datasets across multiple fields. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#20256;&#25773;&#30340;IBP&#27491;&#21017;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25193;&#22823;&#30340;&#39046;&#22495;&#19978;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#32467;&#21512;&#19968;&#31181;&#22522;&#20110;&#24265;&#20215;&#21306;&#38388;&#20256;&#25773;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#24341;&#20837;&#32593;&#32476;&#30340;&#21487;&#39564;&#35777;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#32593;&#32476;&#30340;&#39564;&#35777;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.14772</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#38388;&#20256;&#25773;&#30340;IBP&#27491;&#21017;&#21270;&#26041;&#27861;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#32593;&#32476;&#30340;&#39564;&#35777;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
IBP Regularization for Verified Adversarial Robustness via Branch-and-Bound. (arXiv:2206.14772v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#20256;&#25773;&#30340;IBP&#27491;&#21017;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25193;&#22823;&#30340;&#39046;&#22495;&#19978;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#32467;&#21512;&#19968;&#31181;&#22522;&#20110;&#24265;&#20215;&#21306;&#38388;&#20256;&#25773;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#24341;&#20837;&#32593;&#32476;&#30340;&#21487;&#39564;&#35777;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#32593;&#32476;&#30340;&#39564;&#35777;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#22312;&#25193;&#22823;&#30340;&#39046;&#22495;&#19978;&#36816;&#34892;&#25915;&#20987;&#24182;&#21521;&#30446;&#26631;&#20989;&#25968;&#20013;&#28155;&#21152;&#21508;&#31181;&#27491;&#21017;&#39033;&#26469;&#22686;&#21152;&#23545;&#25239;&#35757;&#32451;&#32593;&#32476;&#30340;&#21487;&#39564;&#35777;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#35201;&#20040;&#24615;&#33021;&#19981;&#20339;&#65292;&#35201;&#20040;&#38656;&#35201;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#20998;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39564;&#35777;&#35757;&#32451;&#31639;&#27861;IBP-R&#65292;&#23427;&#26082;&#31616;&#21333;&#21448;&#26377;&#25928;&#12290;IBP-R&#36890;&#36807;&#22312;&#25193;&#22823;&#30340;&#39046;&#22495;&#19978;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#32467;&#21512;&#19968;&#31181;&#22522;&#20110;&#24265;&#20215;&#21306;&#38388;&#20256;&#25773;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#24341;&#20837;&#32593;&#32476;&#30340;&#21487;&#39564;&#35777;&#24615;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#38750;&#20984;&#39564;&#35777;&#38382;&#39064;&#19982;&#20854;&#36817;&#20284;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IBP-R&#22312;CIFAR-10&#23567;&#25200;&#21160;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#31283;&#20581;&#24615;-&#20934;&#30830;&#24615;&#24179;&#34913;&#65292;&#21516;&#26102;&#27604;&#30456;&#20851;&#20808;&#21069;&#24037;&#20316;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#25903;&#31639;&#27861;UPB&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have tried to increase the verifiability of adversarially trained networks by running the attacks over domains larger than the original perturbations and adding various regularization terms to the objective. However, these algorithms either underperform or require complex and expensive stage-wise training procedures, hindering their practical applicability. We present IBP-R, a novel verified training algorithm that is both simple and effective. IBP-R induces network verifiability by coupling adversarial attacks on enlarged domains with a regularization term, based on inexpensive interval bound propagation, that minimizes the gap between the non-convex verification problem and its approximations. By leveraging recent branch-and-bound frameworks, we show that IBP-R obtains state-of-the-art verified robustness-accuracy trade-offs for small perturbations on CIFAR-10 while training significantly faster than relevant previous work. Additionally, we present UPB, a novel branching
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#33021;&#37327;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#24102;&#23545;&#31216;&#28388;&#27874;&#22120;&#30340;&#32447;&#24615;&#22270;&#21367;&#31215;&#21487;&#20197;&#22686;&#24378;&#39640;&#39057;&#29575;&#65292;&#20351;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2206.10991</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#19978;&#30340;&#33021;&#37327;&#29702;&#35299;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Understanding convolution on graphs via energies. (arXiv:2206.10991v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#33021;&#37327;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#24102;&#23545;&#31216;&#28388;&#27874;&#22120;&#30340;&#32447;&#24615;&#22270;&#21367;&#31215;&#21487;&#20197;&#22686;&#24378;&#39640;&#39057;&#29575;&#65292;&#20351;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#24120;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#25805;&#20316;&#65292;&#20854;&#20013;&#33410;&#28857;&#30340;&#29366;&#24577;&#26159;&#22522;&#20110;&#20854;&#37051;&#23621;&#25910;&#21040;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#26032;&#30340;&#12290;&#22823;&#22810;&#25968;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#37117;&#26159;&#20316;&#20026;&#22270;&#21367;&#31215;&#36827;&#34892;&#25805;&#20316;&#30340;&#65292;&#20854;&#20013;&#29305;&#24449;&#22312;&#34987;&#20256;&#25773;&#21040;&#36793;&#32536;&#20043;&#21069;&#36890;&#36807;&#20849;&#20139;&#30340;&#32447;&#24615;&#21464;&#25442;&#28151;&#21512;&#12290;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#22270;&#21367;&#31215;&#24050;&#32463;&#34920;&#29616;&#20986;&#20004;&#20010;&#38480;&#21046;&#65306;&#22312;heterophilic&#22270;&#19978;&#34920;&#29616;&#27424;&#20339;&#65292;&#24182;&#19988;&#36807;&#24230;&#24179;&#28369;&#12290;&#24120;&#35265;&#30340;&#30475;&#27861;&#26159;&#65292;&#36825;&#20004;&#31181;&#29616;&#35937;&#30340;&#21457;&#29983;&#26159;&#22240;&#20026;&#36825;&#31181;&#27169;&#22411;&#34920;&#29616;&#20026;&#20302;&#36890;&#28388;&#27874;&#22120;&#65292;&#24847;&#21619;&#30528;&#22312;&#22270;&#23618;&#38388;&#29305;&#24449;&#30340;Dirichlet&#33021;&#37327;&#20250;&#20943;&#23569;&#65292;&#23548;&#33268;&#24179;&#28369;&#25928;&#24212;&#65292;&#26368;&#32456;&#29305;&#24449;&#19981;&#20877;&#21487;&#21306;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20005;&#35880;&#22320;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#22270;&#21367;&#31215;&#27169;&#22411;&#23454;&#38469;&#19978;&#21487;&#20197;&#22686;&#24378;&#39640;&#39057;&#29575;&#29978;&#33267;&#24341;&#23548;&#19968;&#31181;&#25105;&#20204;&#25152;&#31216;&#30340;&#36807;&#24230;&#38160;&#21270;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#19982;&#36807;&#24230;&#24179;&#28369;&#30456;&#21453;&#12290;&#25105;&#20204;&#36890;&#36807;&#34920;&#26126;&#23545;&#31216;&#28388;&#27874;&#22120;&#30340;&#32447;&#24615;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#22270;&#24418;&#19978;&#30340;&#33021;&#37327;&#26368;&#23567;&#21270;&#38382;&#39064;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#33021;&#37327;&#20989;&#25968;&#24809;&#32602;&#39640;&#33021;&#20449;&#21495;&#65292;&#26377;&#25928;&#22320;&#25233;&#21046;&#20302;&#39057;&#65292;&#21516;&#26102;&#20419;&#36827;&#30456;&#20851;&#30340;&#39640;&#39057;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#22270;&#21367;&#31215;&#27169;&#22411;&#21487;&#20197;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with sy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#33258;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#21333;&#19968;Vision Transformer&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#19982;&#21333;&#27169;&#24577;&#34920;&#31034;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2206.08356</link><description>&lt;p&gt;
OmniMAE: &#22270;&#29255;&#21644;&#35270;&#39057;&#19978;&#30340;&#21333;&#19968;&#27169;&#22411;&#36974;&#34109;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
OmniMAE: Single Model Masked Pretraining on Images and Videos. (arXiv:2206.08356v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#33258;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#21333;&#19968;Vision Transformer&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#19982;&#21333;&#27169;&#24577;&#34920;&#31034;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20307;&#31995;&#32467;&#26500;&#22312;&#21508;&#31181;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#21464;&#24471;&#31454;&#20105;&#21147;&#21313;&#36275;&#65292;&#20854;&#20013;&#26368;&#33879;&#21517;&#30340;&#26159;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#26159;&#30740;&#31350;&#36825;&#20123;&#27169;&#24577;&#20043;&#38388;&#30340;&#38548;&#31163;&#65292;&#20294;&#26159;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#24847;&#21619;&#30528;&#21487;&#20197;&#20026;&#22810;&#20010;&#35270;&#35273;&#27169;&#24577;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#32479;&#19968;&#24314;&#27169;&#23581;&#35797;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#20026;&#35270;&#35273;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#26550;&#26500;&#65292;&#25110;&#19982;&#21333;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#26356;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36974;&#34109;&#33258;&#32534;&#30721;&#21487;&#20197;&#29992;&#20110;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;Vision Transformer&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20010;&#21333;&#19968;&#30340;&#27169;&#22411;&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#19982;&#21333;&#27169;&#24577;&#34920;&#31034;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21024;&#38500;90&#65285;&#30340;&#22270;&#20687;&#21644;95&#65285;&#30340;&#35270;&#39057;&#34917;&#19969;&#65292;&#21487;&#20197;&#23398;&#20064;&#35813;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26497;&#24555;&#30340;&#22823;&#22411;&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#19968;ViT-Hu
&lt;/p&gt;
&lt;p&gt;
Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Hu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31867;&#20284;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#30446;&#26631;&#22495;&#30340;&#20844;&#24179;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2206.03656</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65306;&#19968;&#31181;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classification via Domain Adaptation: A Dual Adversarial Learning Approach. (arXiv:2206.03656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03656
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31867;&#20284;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#30446;&#26631;&#22495;&#30340;&#20844;&#24179;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#27495;&#35270;&#21644;&#19981;&#20844;&#30340;&#38382;&#39064;&#21313;&#20998;&#20005;&#37325;&#65292;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#39640;&#39118;&#38505;&#24212;&#29992;&#30340;&#37319;&#29992;&#12290;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#26368;&#36817;&#30740;&#31350;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#31639;&#27861;&#23454;&#29616;&#20844;&#24179;&#21644;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#20844;&#24179;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26497;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#25935;&#24863;&#23646;&#24615;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#27491;&#21017;&#21270;&#27169;&#22411;&#23398;&#20064;&#25110;&#21518;&#22788;&#29702;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#27861;&#24459;&#25110;&#30417;&#31649;&#38480;&#21046;&#65292;&#25935;&#24863;&#23646;&#24615;&#24120;&#24120;&#26159;&#19981;&#23436;&#25972;&#29978;&#33267;&#19981;&#21487;&#29992;&#30340;&#12290;&#34429;&#28982;&#25105;&#20204;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#26469;&#35757;&#32451;&#30446;&#26631;&#22495;&#30340;&#20844;&#24179;&#27169;&#22411;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#20855;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#31867;&#20284;&#22495;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#31867;&#20284;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning (ML) models are becoming increasingly popular and are widely used in decision-making systems. However, studies have shown critical issues of ML discrimination and unfairness, which hinder their adoption on high-stake applications. Recent research on fair classifiers has drawn significant attention to developing effective algorithms to achieve fairness and good classification performance. Despite the great success of these fairness-aware machine learning models, most of the existing models require sensitive attributes to pre-process the data, regularize the model learning or post-process the prediction to have fair predictions. However, sensitive attributes are often incomplete or even unavailable due to privacy, legal or regulation restrictions. Though we lack the sensitive attribute for training a fair model in the target domain, there might exist a similar domain that has sensitive attributes. Thus, it is important to exploit auxiliary information from a simil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992; PAC-Bayesian &#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20999;&#29255;&#29926;&#30782;&#26031;&#22374;&#36317;&#31163;&#30340;&#27010;&#25324;&#29305;&#24615;&#30028;&#38480;&#21644;&#19968;&#31181;&#22522;&#20110;&#30028;&#38480;&#30340;&#20999;&#29255;&#20998;&#24067;&#23398;&#20064;&#27969;&#31243;&#65292;&#20197;&#25552;&#39640; SW &#30340;&#21028;&#21035;&#24230;&#12290;</title><link>http://arxiv.org/abs/2206.03230</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20999;&#29255;&#29926;&#30782;&#26031;&#22374;&#36317;&#31163;&#30340; PAC-Bayesian &#20809;&#29031;
&lt;/p&gt;
&lt;p&gt;
Shedding a PAC-Bayesian Light on Adaptive Sliced-Wasserstein Distances. (arXiv:2206.03230v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992; PAC-Bayesian &#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20999;&#29255;&#29926;&#30782;&#26031;&#22374;&#36317;&#31163;&#30340;&#27010;&#25324;&#29305;&#24615;&#30028;&#38480;&#21644;&#19968;&#31181;&#22522;&#20110;&#30028;&#38480;&#30340;&#20999;&#29255;&#20998;&#24067;&#23398;&#20064;&#27969;&#31243;&#65292;&#20197;&#25552;&#39640; SW &#30340;&#21028;&#21035;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;&#29926;&#30782;&#26031;&#22374;&#36317;&#31163;&#65288;SW&#65289;&#26159;&#29926;&#30782;&#26031;&#22374;&#36317;&#31163;&#30340;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#19988;&#29702;&#35770;&#22522;&#30784;&#33391;&#22909;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20854;&#32479;&#35745;&#29305;&#24615;&#65288;&#25110;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#20851;&#20110;&#20854;&#30456;&#23545;&#20110;&#8220;&#20999;&#29255;&#8221;&#30340;&#20998;&#24067;&#30340;&#27010;&#25324;&#29305;&#24615;&#65292;&#36229;&#36234;&#22343;&#21248;&#20998;&#24067;&#65289;&#65292;&#25991;&#29486;&#36164;&#26009;&#26497;&#20026;&#26377;&#38480;&#12290;&#20026;&#20102;&#20026;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#24102;&#26469;&#26032;&#30340;&#36129;&#29486;&#65292;&#26412;&#25991;&#21033;&#29992; PAC-Bayesian &#29702;&#35770;&#21644;&#19968;&#20010;&#26680;&#24515;&#35266;&#23519;&#32467;&#26524;&#65306;SW &#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24179;&#22343;&#39118;&#38505;&#65292;PAC-Bayesian &#30028;&#23450;&#20854;&#29305;&#24615;&#30340;&#37327;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19977;&#31181;&#32467;&#26524;&#65306;i&#65289;PAC-Bayesian &#30340;&#27010;&#25324;&#24615;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#25152;&#31216;&#30340;&#33258;&#36866;&#24212;&#20999;&#29255;&#29926;&#30782;&#26031;&#22374;&#36317;&#31163;&#65292;&#21363;&#30456;&#23545;&#20110;&#20219;&#24847;&#20998;&#24067;&#30340;&#20999;&#29255;&#65288;&#21253;&#25324;&#25968;&#25454;&#30456;&#20851;&#20998;&#24067;&#65289;&#23450;&#20041;&#30340; SW&#65307;ii&#65289;&#19968;&#31181;&#22522;&#20110;&#29702;&#35770;&#30028;&#38480;&#30340;&#21407;&#21017;&#24615;&#27969;&#31243;&#65292;&#29992;&#20110;&#23398;&#20064;&#20999;&#29255;&#20998;&#24067;&#65292;&#20197;&#24471;&#21040;&#26368;&#22823;&#21028;&#21035; SW&#65307;iii&#65289;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#30340;&#23454;&#35777;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sliced-Wasserstein distance (SW) is a computationally efficient and theoretically grounded alternative to the Wasserstein distance. Yet, the literature on its statistical properties -- or, more accurately, its generalization properties -- with respect to the distribution of slices, beyond the uniform measure, is scarce. To bring new contributions to this line of research, we leverage the PAC-Bayesian theory and a central observation that SW may be interpreted as an average risk, the quantity PAC-Bayesian bounds have been designed to characterize. We provide three types of results: i) PAC-Bayesian generalization bounds that hold on what we refer as adaptive Sliced-Wasserstein distances, i.e. SW defined with respect to arbitrary distributions of slices (among which data-dependent distributions), ii) a principled procedure to learn the distribution of slices that yields maximally discriminative SW, by optimizing our theoretical bounds, and iii) empirical illustrations of our theoretic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26174;&#33879;&#24615;&#21345;&#29255;&#65292;&#21363;&#32467;&#26500;&#21270;&#25991;&#26723;&#65292;&#25551;&#36848;&#20102;&#26174;&#33879;&#24615;&#26041;&#27861;&#30340;&#25805;&#20316;&#26041;&#24335;&#21450;&#20854;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#29992;&#25143;&#36873;&#25321;&#26041;&#27861;&#26102;&#24212;&#32771;&#34385;&#30340;10&#20010;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.02958</link><description>&lt;p&gt;
Saliency Cards: &#19968;&#20010;&#29992;&#20110;&#34920;&#24449;&#21644;&#27604;&#36739;&#26174;&#33879;&#24615;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Saliency Cards: A Framework to Characterize and Compare Saliency Methods. (arXiv:2206.02958v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26174;&#33879;&#24615;&#21345;&#29255;&#65292;&#21363;&#32467;&#26500;&#21270;&#25991;&#26723;&#65292;&#25551;&#36848;&#20102;&#26174;&#33879;&#24615;&#26041;&#27861;&#30340;&#25805;&#20316;&#26041;&#24335;&#21450;&#20854;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#29992;&#25143;&#36873;&#25321;&#26041;&#27861;&#26102;&#24212;&#32771;&#34385;&#30340;10&#20010;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#24615;&#26041;&#27861;&#26159;&#19968;&#31867;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#35745;&#31639;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24555;&#36895;&#21457;&#23637;&#65292;&#29992;&#25143;&#24456;&#38590;&#20102;&#35299;&#26032;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#22240;&#27492;&#20250;&#26681;&#25454;&#19981;&#21512;&#29702;&#30340;&#21407;&#22240;&#65288;&#22914;&#27969;&#34892;&#24230;&#65289;&#36873;&#25321;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26174;&#33879;&#24615;&#21345;&#29255;&#65306;&#32467;&#26500;&#21270;&#25991;&#26723;&#65292;&#25551;&#36848;&#26174;&#33879;&#24615;&#26041;&#27861;&#30340;&#25805;&#20316;&#26041;&#24335;&#21450;&#20854;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;25&#20010;&#26174;&#33879;&#24615;&#26041;&#27861;&#35770;&#25991;&#21644;33&#20010;&#26041;&#27861;&#35780;&#20272;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#29992;&#25143;&#36873;&#25321;&#26041;&#27861;&#26102;&#24212;&#32771;&#34385;&#30340;10&#20010;&#23646;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#23646;&#24615;&#20998;&#25104;&#19977;&#20010;&#31867;&#21035;&#65292;&#28085;&#30422;&#35745;&#31639;&#21644;&#35299;&#37322;&#26174;&#33879;&#24615;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model's output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency: methodology, or how the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#36817;&#20284;&#21033;&#26222;&#24076;&#33576;&#21644;&#24179;&#28369;&#20989;&#25968;&#30340;&#38745;&#24577;&#28857;&#38382;&#39064;&#12290;&#25552;&#20379;&#20102;&#26032;&#30340;&#39640;&#25928;&#31639;&#27861;&#21644;&#26500;&#36896;&#65292;&#20998;&#21035;&#22312;&#26377;&#38480;&#21644;&#38543;&#26426;&#24773;&#20917;&#19979;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2206.00846</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#20013;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21040;&#38745;&#24577;&#28857;
&lt;/p&gt;
&lt;p&gt;
Faster Rates of Convergence to Stationary Points in Differentially Private Optimization. (arXiv:2206.00846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#36817;&#20284;&#21033;&#26222;&#24076;&#33576;&#21644;&#24179;&#28369;&#20989;&#25968;&#30340;&#38745;&#24577;&#28857;&#38382;&#39064;&#12290;&#25552;&#20379;&#20102;&#26032;&#30340;&#39640;&#25928;&#31639;&#27861;&#21644;&#26500;&#36896;&#65292;&#20998;&#21035;&#22312;&#26377;&#38480;&#21644;&#38543;&#26426;&#24773;&#20917;&#19979;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;$(\varepsilon,\delta)$-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19979;&#65292;&#36817;&#20284;&#21033;&#26222;&#24076;&#33576;&#21644;&#24179;&#28369;&#20989;&#25968;&#30340;&#38745;&#24577;&#28857;&#30340;&#38382;&#39064;&#65292;&#28041;&#21450;&#20102;&#26377;&#38480;&#21644;&#21644;&#38543;&#26426;&#24773;&#20917;&#12290;&#22914;&#26524;$\|\nabla F(\widehat{w})\|\leq \alpha$&#65292;&#21017;&#31216;&#28857;$\widehat{w}$&#26159;&#20989;&#25968;$F:\mathbb{R}^d\rightarrow\mathbb{R}$&#30340;$\alpha$-&#38745;&#24577;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#21644;&#35774;&#32622;&#20013;&#25214;&#21040;&#19968;&#20010;$\tilde{O}\big(\big[\frac{\sqrt{d}}{n\varepsilon}\big]^{2/3}\big)$&#30340;&#38745;&#24577;&#28857;&#65292;&#20854;&#20013;$n$&#26159;&#26679;&#26412;&#25968;&#12290;&#36825;&#20248;&#20110;&#20197;&#21069;&#26368;&#20339;&#36895;&#29575;$\tilde{O}\big(\big[\frac{\sqrt{d}}{n\varepsilon}\big]^{1/2}\big)$&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#36896;&#65292;&#25913;&#36827;&#20102;&#38543;&#26426;&#20248;&#21270;&#35774;&#32622;&#20013;&#29616;&#26377;&#30340;&#36895;&#29575;&#65292;&#22312;&#35813;&#35774;&#32622;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#20154;&#21475;&#39118;&#38505;&#30340;&#36817;&#20284;&#38745;&#24577;&#28857;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#25214;&#21040;&#20102;&#19968;&#20010;$\tilde{O}\big(\frac{1}{n^{1/3}} + \big[\frac{\sqrt{d}}{n\varepsilon}\big]^{1/2}\big)$&#30340;&#20154;&#21475;&#39118;&#38505;&#38745;&#24577;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of approximating stationary points of Lipschitz and smooth functions under $(\varepsilon,\delta)$-differential privacy (DP) in both the finite-sum and stochastic settings. A point $\widehat{w}$ is called an $\alpha$-stationary point of a function $F:\mathbb{R}^d\rightarrow\mathbb{R}$ if $\|\nabla F(\widehat{w})\|\leq \alpha$. We provide a new efficient algorithm that finds an $\tilde{O}\big(\big[\frac{\sqrt{d}}{n\varepsilon}\big]^{2/3}\big)$-stationary point in the finite-sum setting, where $n$ is the number of samples. This improves on the previous best rate of $\tilde{O}\big(\big[\frac{\sqrt{d}}{n\varepsilon}\big]^{1/2}\big)$. We also give a new construction that improves over the existing rates in the stochastic optimization setting, where the goal is to find approximate stationary points of the population risk. Our construction finds a $\tilde{O}\big(\frac{1}{n^{1/3}} + \big[\frac{\sqrt{d}}{n\varepsilon}\big]^{1/2}\big)$-stationary point of the population risk 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#26426;&#20316;&#19994;&#35843;&#24230;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#39044;&#27979;&#30340;&#38745;&#24577;&#35843;&#24230;&#31639;&#27861;&#65292;&#22312;&#31867;&#22411;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#36807;&#21097;&#25104;&#26412;&#65292;&#23588;&#20854;&#22312;&#25250;&#21344;&#24335;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20316;&#19994;&#31867;&#22411;&#25345;&#32493;&#26102;&#38388;&#30456;&#24046;&#24456;&#22823;&#26102;&#20248;&#20110;&#38750;&#25250;&#21344;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2205.15695</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#25506;&#32034;&#23398;&#20064;&#39044;&#27979;&#30340;&#38745;&#24577;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Static Scheduling with Predictions Learned through Efficient Exploration. (arXiv:2205.15695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#26426;&#20316;&#19994;&#35843;&#24230;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#39044;&#27979;&#30340;&#38745;&#24577;&#35843;&#24230;&#31639;&#27861;&#65292;&#22312;&#31867;&#22411;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#36807;&#21097;&#25104;&#26412;&#65292;&#23588;&#20854;&#22312;&#25250;&#21344;&#24335;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20316;&#19994;&#31867;&#22411;&#25345;&#32493;&#26102;&#38388;&#30456;&#24046;&#24456;&#22823;&#26102;&#20248;&#20110;&#38750;&#25250;&#21344;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21333;&#26426;&#20316;&#19994;&#35843;&#24230;&#65292;&#27599;&#20010;&#20316;&#19994;&#37117;&#23646;&#20110;&#20915;&#23450;&#20854;&#25345;&#32493;&#26102;&#38388;&#20998;&#24067;&#30340;&#20316;&#19994;&#31867;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#31867;&#22411;&#29305;&#24449;&#24050;&#30693;&#30340;&#24773;&#20917;&#65292;&#28982;&#21518;&#36716;&#21521;&#20004;&#31181;&#23398;&#20064;&#24773;&#26223;&#65292;&#20854;&#20013;&#31867;&#22411;&#26410;&#30693;&#65306;&#38750;&#25250;&#21344;&#24335;&#38382;&#39064;&#65292;&#23427;&#35201;&#27714;&#23436;&#25104;&#24050;&#21551;&#21160;&#30340;&#20316;&#19994;&#65292;&#28982;&#21518;&#25165;&#33021;&#31227;&#21160;&#21040;&#21478;&#19968;&#20010;&#20316;&#19994;&#65307;&#21644;&#25250;&#21344;&#24335;&#38382;&#39064;&#65292;&#36825;&#37324;&#20316;&#19994;&#25191;&#34892;&#21487;&#20197;&#26242;&#20572;&#20197;&#20248;&#20808;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20316;&#19994;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#31639;&#27861;&#30456;&#23545;&#20110;&#24050;&#30693;&#31867;&#22411;&#30340;&#24615;&#33021;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#36807;&#21097;&#25104;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#38750;&#25250;&#21344;&#24335;&#24773;&#20917;&#30340;&#19979;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25250;&#21344;&#31639;&#27861;&#22312;&#19981;&#21516;&#20316;&#19994;&#31867;&#22411;&#25345;&#32493;&#26102;&#38388;&#30456;&#24046;&#24456;&#22823;&#26102;&#65292;&#29702;&#35770;&#19978;&#21644;&#36890;&#36807;&#27169;&#25311;&#30340;&#26041;&#24335;&#21487;&#20197;&#20248;&#20110;&#38750;&#25250;&#21344;&#21305;&#37197;&#65292;&#22312;&#31867;&#22411;&#25345;&#32493;&#26102;&#38388;&#24050;&#30693;&#26102;&#24182;&#19981;&#23384;&#22312;&#36825;&#31181;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study single-machine scheduling of jobs, each belonging to a job type that determines its duration distribution. We start by analyzing the scenario where the type characteristics are known and then move to two learning scenarios where the types are unknown: non-preemptive problems, where each started job must be completed before moving to another job; and preemptive problems, where job execution can be paused in the favor of moving to a different job. In both cases, we design algorithms that achieve sublinear excess cost, compared to the performance with known types, and prove lower bounds for the non-preemptive case. Notably, we demonstrate, both theoretically and through simulations, how preemptive algorithms can greatly outperform non-preemptive ones when the durations of different job types are far from one another, a phenomenon that does not occur when the type durations are known.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20420;&#32599;&#26031;&#22269;&#23478;&#23186;&#20307;&#22312;&#33521;&#35821;&#35835;&#32773;&#20013;&#20256;&#25773;&#30340;&#21465;&#20107;&#65292;&#27604;&#36739;Reddit&#19978;&#26377;&#20851;&#20420;&#20044;&#25112;&#20105;&#30340;&#35752;&#35770;&#65292;&#23637;&#31034;&#20102;&#20420;&#32599;&#26031;&#23186;&#20307;&#23545;&#21465;&#20107;&#30340;&#24433;&#21709;&#20197;&#21450;&#35752;&#35770;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2205.14484</link><description>&lt;p&gt;
&#20598;&#28982;&#24615;&#65306;&#21033;&#29992;&#35821;&#20041;&#25628;&#32034;&#22312;Reddit&#19978;&#36319;&#36394;&#20420;&#32599;&#26031;&#22269;&#23478;&#23186;&#20307;&#23545;&#20420;&#20044;&#25112;&#20105;&#30340;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Happenstance: Utilizing Semantic Search to Track Russian State Media Narratives about the Russo-Ukrainian War On Reddit. (arXiv:2205.14484v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20420;&#32599;&#26031;&#22269;&#23478;&#23186;&#20307;&#22312;&#33521;&#35821;&#35835;&#32773;&#20013;&#20256;&#25773;&#30340;&#21465;&#20107;&#65292;&#27604;&#36739;Reddit&#19978;&#26377;&#20851;&#20420;&#20044;&#25112;&#20105;&#30340;&#35752;&#35770;&#65292;&#23637;&#31034;&#20102;&#20420;&#32599;&#26031;&#23186;&#20307;&#23545;&#21465;&#20107;&#30340;&#24433;&#21709;&#20197;&#21450;&#35752;&#35770;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#32852;&#37030;&#20405;&#30053;&#20044;&#20811;&#20848;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20960;&#21608;&#65292;&#20420;&#32599;&#26031;&#22269;&#23478;&#23186;&#20307;&#21457;&#24067;&#20102;&#22823;&#37327;&#35823;&#23548;&#24615;&#21644;&#24443;&#24213;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#21327;&#35843;&#20449;&#24687;&#36816;&#21160;&#65292;&#20197;&#20415;&#20102;&#35299;&#20420;&#32599;&#26031;&#25919;&#24220;&#21521;&#33521;&#35821;&#35835;&#32773;&#23459;&#20256;&#30340;&#26368;&#31361;&#20986;&#30340;&#22269;&#23478;&#23186;&#20307;&#21465;&#20107;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MPNet&#23545;&#30001;&#21253;&#25324;&#20420;&#32599;&#26031;&#26032;&#30340;&#8220;&#20107;&#23454;&#26680;&#26597;&#8221;&#32593;&#31449;waronfakes.com&#22312;&#20869;&#30340;&#21313;&#20010;&#19981;&#21516;&#20146;&#20420;&#23459;&#20256;&#32593;&#31449;&#21457;&#24067;&#30340;&#25991;&#31456;&#36827;&#34892;&#21477;&#32423;&#20027;&#39064;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20687;katehon.com&#36825;&#26679;&#30340;&#23567;&#22411;&#32593;&#31449;&#22312;&#21457;&#24067;&#31245;&#21518;&#30001;&#20854;&#20182;&#20420;&#32599;&#26031;&#32593;&#31449;&#22238;&#21709;&#30340;&#20027;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#20998;&#26512;&#20102;&#36825;&#32452;&#20420;&#32599;&#26031;&#20449;&#24687;&#21465;&#20107;&#20043;&#21518;&#65292;&#25105;&#20204;&#20877;&#20998;&#26512;&#23427;&#20204;&#19982;r/Russia&#21644;&#20854;&#20182;10&#20010;&#25919;&#27835;&#23376;reddit&#19978;&#30340;&#21465;&#20107;&#21644;&#35752;&#35770;&#20027;&#39064;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20351;&#29992;MPNet&#21644;&#35821;&#20041;&#25628;&#32034;&#31639;&#27861;&#23558;&#36825;&#20123;&#23376;reddit&#19978;&#30340;&#21069;10,000&#26465;&#35780;&#35770;&#20998;&#21035;&#32763;&#35793;&#25104;&#20420;&#35821;&#21644;&#33521;&#35821;&#65292;&#28982;&#21518;&#20351;&#29992;MUSE&#36328;&#35821;&#35328;&#23884;&#20837;&#25214;&#21040;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#35780;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Reddit&#19978;&#26377;&#20851;&#20420;&#20044;&#25112;&#20105;&#30340;&#35752;&#35770;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#20197;&#21450;&#36825;&#20123;&#35752;&#35770;&#22914;&#20309;&#21463;&#21040;&#22269;&#23478;&#23186;&#20307;&#21465;&#20107;&#30340;&#24433;&#21709;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the buildup to and in the weeks following the Russian Federation's invasion of Ukraine, Russian state media outlets output torrents of misleading and outright false information. In this work, we study this coordinated information campaign in order to understand the most prominent state media narratives touted by the Russian government to English-speaking audiences. To do this, we first perform sentence-level topic analysis using the large-language model MPNet on articles published by ten different pro-Russian propaganda websites including the new Russian "fact-checking" website waronfakes.com. Within this ecosystem, we show that smaller websites like katehon.com were highly effective at publishing topics that were later echoed by other Russian sites. After analyzing this set of Russian information narratives, we then analyze their correspondence with narratives and topics of discussion on the r/Russia and 10 other political subreddits. Using MPNet and a semantic search algorithm, we
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13743</link><description>&lt;p&gt;
&#24102;&#26377;&#20559;&#22909;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#24178;&#39044;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13743
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#24178;&#39044;&#65288;AR&#65289;&#30340;&#38382;&#39064;&#26159;&#35745;&#31639;&#29992;&#25143;&#25191;&#34892;&#19968;&#31995;&#21015;&#25805;&#20316;&#20197;&#39072;&#35206;&#19981;&#33391;&#26426;&#22120;&#20915;&#31574;&#30340;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#23454;&#26045;&#25552;&#20986;&#36807;&#39640;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;AR&#26041;&#27861;&#37117;&#20551;&#35774;&#25152;&#26377;&#29992;&#25143;&#30340;&#25805;&#20316;&#25104;&#26412;&#30456;&#21516;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21521;&#26576;&#20123;&#29992;&#25143;&#25512;&#33616;&#26114;&#36149;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEAR&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#20010;&#21487;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#20219;&#20309;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;PEAR&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#21521;&#30446;&#26631;&#29992;&#25143;&#21457;&#20986;&#36873;&#25321;&#38598;&#26597;&#35810;&#26469;&#36845;&#20195;&#22320;&#25913;&#21892;&#23545;&#25805;&#20316;&#25104;&#26412;&#30340;&#20272;&#35745;&#20540;&#12290;&#36825;&#20123;&#26597;&#35810;&#30340;&#35745;&#31639;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#35745;&#31639;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#25104;&#26412;&#20272;&#35745;&#21644;&#29992;&#25143;&#21709;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#12290;PEAR&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#23454;&#29616;AR&#20219;&#21153;&#25152;&#38656;&#36798;&#25104;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#20197;&#21450;&#25191;&#34892;&#27599;&#20010;&#25805;&#20316;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;AR&#20219;&#21153;&#26469;&#35780;&#20272;PEAR&#65292;&#24182;&#26174;&#31034;&#20854;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#26356;&#20026;&#32463;&#27982;&#23454;&#29992;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#20013;&#21033;&#29992;&#26377;&#33021;&#21147;&#21644;&#26080;&#33021;&#21147;&#30340;&#25945;&#24072;&#26469;&#24341;&#23548;&#36951;&#24536;&#65292;&#20197;&#20415;&#38543;&#26102;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#26576;&#20010;&#38598;&#21512;&#25110;&#31867;&#21035;&#30340;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.08096</link><description>&lt;p&gt;
&#31967;&#31957;&#30340;&#25945;&#23398;&#20250;&#23548;&#33268;&#36951;&#24536;&#21527;&#65311;&#20351;&#29992;&#26080;&#33021;&#25945;&#24072;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#36827;&#34892;&#21462;&#28040;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher. (arXiv:2205.08096v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08096
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#20013;&#21033;&#29992;&#26377;&#33021;&#21147;&#21644;&#26080;&#33021;&#21147;&#30340;&#25945;&#24072;&#26469;&#24341;&#23548;&#36951;&#24536;&#65292;&#20197;&#20415;&#38543;&#26102;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#26576;&#20010;&#38598;&#21512;&#25110;&#31867;&#21035;&#30340;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#35201;&#36981;&#23432;&#26032;&#20852;&#30340;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#65292;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#23427;&#20415;&#20110;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#26576;&#20010;&#38598;&#21512;&#25110;&#31867;&#21035;&#30340;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#20013;&#21033;&#29992;&#26377;&#33021;&#21147;&#21644;&#26080;&#33021;&#21147;&#30340;&#25945;&#24072;&#24341;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#36873;&#25321;&#24615;&#22320;&#23558;&#26377;&#33021;&#21147;&#21644;&#26080;&#33021;&#21147;&#25945;&#24072;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#19981;&#21253;&#21547;&#20219;&#20309;&#34987;&#36951;&#24536;&#25968;&#25454;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#24555;&#36895;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#8220;&#38646;&#37325;&#26032;&#35757;&#32451;&#36951;&#24536;&#65288;ZRF&#65289;&#8221;&#25351;&#26631;&#26469;&#35780;&#20272;&#20219;&#20309;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#21462;&#28040;&#23398;&#20064;&#25351;&#26631;&#19981;&#21516;&#65292;ZRF&#24471;&#20998;...
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has become an important area of research due to an increasing need for machine learning (ML) applications to comply with the emerging data privacy regulations. It facilitates the provision for removal of certain set or class of data from an already trained ML model without requiring retraining from scratch. Recently, several efforts have been put in to make unlearning to be effective and efficient. We propose a novel machine unlearning method by exploring the utility of competent and incompetent teachers in a student-teacher framework to induce forgetfulness. The knowledge from the competent and incompetent teachers is selectively transferred to the student to obtain a model that doesn't contain any information about the forget data. We experimentally show that this method generalizes well, is fast and effective. Furthermore, we introduce the zero retrain forgetting (ZRF) metric to evaluate any unlearning method. Unlike the existing unlearning metrics, the ZRF score 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RoMFAC&#31639;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#37325;&#22797;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#23545;&#20110;&#24322;&#24120;&#29366;&#24577;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#33719;&#24471;&#20986;&#33394;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.07229</link><description>&lt;p&gt;
RoMFAC: &#19968;&#31181;&#23545;&#20110;&#29366;&#24577;&#24322;&#24120;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22343;&#22330;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states. (arXiv:2205.07229v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RoMFAC&#31639;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#37325;&#22797;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#23545;&#20110;&#24322;&#24120;&#29366;&#24577;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#33719;&#24471;&#20986;&#33394;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#29366;&#24577;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#65292;&#20294;&#26159;&#35266;&#23519;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#26234;&#33021;&#20307;&#20570;&#20986;&#38169;&#35823;&#30340;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;RoMFAC&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#20989;&#25968;&#21644;&#19968;&#20010;&#20195;&#34920;&#29366;&#24577;&#24178;&#25200;&#24433;&#21709;&#30340;&#34892;&#21160;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#28436;&#21592;&#12290;&#21516;&#26102;&#65292;RoMFAC&#36824;&#24341;&#20837;&#19968;&#20010;&#37325;&#22797;&#30340;&#27491;&#21017;&#21270;&#34892;&#21160;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#28436;&#21592;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent deep reinforcement learning makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement learning (MFAC) is well-known in the multi-agent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two innovations: 1) a new objective function of training actors, composed of a \emph{policy gradient function} that is related to the expected cumulative discount reward on sampled clean states and an \emph{action loss function} that represents the difference between actions taken on clean and adversarial states; and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a 
&lt;/p&gt;</description></item><item><title>BrainIB&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#24320;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#20998;&#26512;fMRI&#22270;&#20687;&#20013;&#30340;&#21151;&#33021;&#36830;&#25509;&#26102;&#33021;&#22815;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2205.03612</link><description>&lt;p&gt;
BrainIB&#65306;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#30340;&#21487;&#35299;&#37322;&#24615;&#33041;&#32593;&#32476;&#31934;&#31070;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck. (arXiv:2205.03612v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03612
&lt;/p&gt;
&lt;p&gt;
BrainIB&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#24320;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#20998;&#26512;fMRI&#22270;&#20687;&#20013;&#30340;&#21151;&#33021;&#36830;&#25509;&#26102;&#33021;&#22815;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#29983;&#29289;&#26426;&#21046;&#32780;&#38750;&#20027;&#35266;&#30151;&#29366;&#23545;&#31934;&#31070;&#38556;&#30861;&#36827;&#34892;&#35786;&#26029;&#30340;&#26032;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#20849;&#35782;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#21151;&#33021;&#36830;&#25509;&#65288;FC&#65289;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#21644;&#20581;&#24247;&#23545;&#29031;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#29992;&#20110;&#30830;&#23450;&#22823;&#33041;&#26631;&#35760;&#29289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35786;&#26029;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#24773;&#20917;&#65288;&#30001;&#20110;&#35757;&#32451;&#26679;&#26412;&#19981;&#36275;&#65289;&#65292;&#22312;&#26032;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#34920;&#29616;&#24046;&#12290;&#27492;&#22806;&#65292;&#38590;&#20197;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#12289;&#21487;&#38752;&#30340;&#22823;&#33041;&#29983;&#29289;&#26631;&#35760;&#29289;&#26469;&#35299;&#37322;&#28508;&#22312;&#30340;&#35786;&#26029;&#20915;&#31574;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#20854;&#21487;&#33021;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BrainIB&#65292;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#33879;&#21517;&#30340;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#26469;&#20998;&#26512;&#21151;&#33021;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;fMRI&#65289;&#12290;BrainIB&#33021;&#22815;&#35782;&#21035;&#22823;&#33041;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65288;&#21363;&#23376;&#22270;&#65289;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing a new diagnostic models based on the underlying biological mechanisms rather than subjective symptoms for psychiatric disorders is an emerging consensus. Recently, machine learning-based classifiers using functional connectivity (FC) for psychiatric disorders and healthy controls are developed to identify brain markers. However, existing machine learningbased diagnostic models are prone to over-fitting (due to insufficient training samples) and perform poorly in new test environment. Furthermore, it is difficult to obtain explainable and reliable brain biomarkers elucidating the underlying diagnostic decisions. These issues hinder their possible clinical applications. In this work, we propose BrainIB, a new graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI), by leveraging the famed Information Bottleneck (IB) principle. BrainIB is able to identify the most informative edges in the brain (i.e., subgraph) and generalizes well to unseen 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;(AL-PINNs)&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#30340;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#34913;&#27599;&#20010;&#25439;&#22833;&#32452;&#20214;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.01059</link><description>&lt;p&gt;
&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;&#65288;AL-PINNs&#65289;
&lt;/p&gt;
&lt;p&gt;
Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs). (arXiv:2205.01059v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;(AL-PINNs)&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#30340;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#34913;&#27599;&#20010;&#25439;&#22833;&#32452;&#20214;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#31185;&#23398;&#35745;&#31639;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26480;&#20986;&#24212;&#29992;&#65292;&#23427;&#20204;&#26159;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#35299;&#30340;&#24378;&#22823;&#36924;&#36817;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;&#65288;AL-PINNs&#65289;&#29992;&#20110;PINNs&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#35270;&#20026;PDE&#27531;&#24046;&#20248;&#21270;&#38382;&#39064;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36890;&#36807;&#24212;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;&#65292;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#21464;&#25104;&#20102;&#19968;&#20010;&#39034;&#24207;&#30340;&#26368;&#22823;-&#26368;&#23567;&#38382;&#39064;&#65292;&#20351;&#21487;&#20197;&#23398;&#20064;&#30340;&#21442;&#25968;&#955;&#33021;&#33258;&#36866;&#24212;&#24179;&#34913;&#27599;&#20010;&#25439;&#22833;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#24207;&#21015;&#25910;&#25947;&#20110;Helmholtz&#12289;&#31896;&#24615;Burgers&#21644;Klein-Gordon&#26041;&#31243;&#30340;&#23454;&#38469;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have become a prominent application of deep learning in scientific computation, as they are powerful approximators of solutions to nonlinear partial differential equations (PDEs). There have been numerous attempts to facilitate the training process of PINNs by adjusting the weight of each component of the loss function, called adaptive loss-balancing algorithms. In this paper, we propose an Augmented Lagrangian relaxation method for PINNs (AL-PINNs). We treat the initial and boundary conditions as constraints for the optimization problem of the PDE residual. By employing Augmented Lagrangian relaxation, the constrained optimization problem becomes a sequential max-min problem so that the learnable parameters $\lambda$ adaptively balance each loss component. Our theoretical analysis reveals that the sequence of minimizers of the proposed loss functions converges to an actual solution for the Helmholtz, viscous Burgers, and Klein--Gordon equations
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#20272;&#35745;&#26041;&#27861;&#25913;&#32534;&#21040;&#20102;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#19978;&#65292;&#21629;&#21517;&#20026;TREX&#21644;BoostIn&#65292;&#26088;&#22312;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;GBDT&#30340;&#39044;&#27979;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.00359</link><description>&lt;p&gt;
&#36866;&#24212;&#24182;&#35780;&#20272;&#29992;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#24433;&#21709;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees. (arXiv:2205.00359v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#20272;&#35745;&#26041;&#27861;&#25913;&#32534;&#21040;&#20102;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#19978;&#65292;&#21629;&#21517;&#20026;TREX&#21644;BoostIn&#65292;&#26088;&#22312;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;GBDT&#30340;&#39044;&#27979;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#20272;&#35745;&#20998;&#26512;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#26356;&#25913;&#22914;&#20309;&#23548;&#33268;&#19981;&#21516;&#30340;&#27169;&#22411;&#39044;&#27979;&#65307;&#36825;&#31181;&#20998;&#26512;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#39044;&#27979;&#12289;&#20570;&#20986;&#36825;&#20123;&#39044;&#27979;&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24433;&#21709;&#20272;&#35745;&#25216;&#26415;&#37117;&#26159;&#20026;&#20855;&#26377;&#36830;&#32493;&#21442;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#30340;&#12290;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#21035;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#40657;&#30418;&#23376;&#65292;&#20855;&#26377;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299; GBDT &#39044;&#27979;&#24182;&#26222;&#36941;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#26368;&#36817;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#20272;&#35745;&#26041;&#27861;&#36866;&#24212;&#21040;&#20102; GBDT &#19978;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992; representer-point &#26041;&#27861;&#21644; TracIn &#26041;&#27861;&#25913;&#32534;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026; TREX &#21644; BoostIn&#65307;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/jjbrophy47/tree_influence &#19978;&#25214;&#21040;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#26041;&#27861;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982; LeafInfluence &#21644;&#20854;&#20182;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence estimation analyzes how changes to the training data can lead to different model predictions; this analysis can help us better understand these predictions, the models making those predictions, and the data sets they're trained on. However, most influence-estimation techniques are designed for deep learning models with continuous parameters. Gradient-boosted decision trees (GBDTs) are a powerful and widely-used class of models; however, these models are black boxes with opaque decision-making processes. In the pursuit of better understanding GBDT predictions and generally improving these models, we adapt recent and popular influence-estimation methods designed for deep learning models to GBDTs. Specifically, we adapt representer-point methods and TracIn, denoting our new methods TREX and BoostIn, respectively; source code is available at https://github.com/jjbrophy47/tree_influence. We compare these methods to LeafInfluence and other baselines using 5 different evaluation mea
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20934;&#21017;&#65292;&#20351;&#24471;&#22810;&#36890;&#36947;&#32447;&#24615;&#28388;&#27874;&#22120;&#38454;&#27573;&#36755;&#20986;&#22788;&#25928;&#26524;&#26356;&#22909;&#65292;&#21487;&#19982;&#21518;&#28388;&#27874;&#22120;&#38454;&#27573;&#32467;&#21512;&#26377;&#25928;&#21435;&#38500;&#27531;&#30041;&#28151;&#21709;&#12290;</title><link>http://arxiv.org/abs/2204.02978</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21548;&#21147;&#35774;&#22791;&#36731;&#37327;&#32423;&#21435;&#28151;&#21709;&#20004;&#38454;&#27573;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A neural network-supported two-stage algorithm for lightweight dereverberation on hearing devices. (arXiv:2204.02978v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02978
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20934;&#21017;&#65292;&#20351;&#24471;&#22810;&#36890;&#36947;&#32447;&#24615;&#28388;&#27874;&#22120;&#38454;&#27573;&#36755;&#20986;&#22788;&#25928;&#26524;&#26356;&#22909;&#65292;&#21487;&#19982;&#21518;&#28388;&#27874;&#22120;&#38454;&#27573;&#32467;&#21512;&#26377;&#25928;&#21435;&#38500;&#27531;&#30041;&#28151;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#36890;&#36947;&#22810;&#24103;&#32447;&#24615;&#28388;&#27874;&#22120;&#32467;&#21512;&#21333;&#36890;&#36947;&#21333;&#24103;&#21518;&#28388;&#27874;&#22120;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#21435;&#28151;&#21709;&#31639;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#26102;&#38388;&#33539;&#22260;&#19979;&#21435;&#28151;&#21709;&#24615;&#33021;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#20102;&#22312;&#22810;&#36890;&#36947;&#32447;&#24615;&#28388;&#27874;&#22120;&#38454;&#27573;&#36755;&#20986;&#22788;&#30452;&#25509;&#20248;&#21270;&#20934;&#21017;&#32467;&#26524;&#27604;&#25918;&#32622;&#22312;DNN&#36755;&#20986;&#22788;&#20248;&#21270;PSD&#20272;&#35745;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#21435;&#38500;&#21487;&#34987;&#28388;&#27874;&#22120;&#35775;&#38382;&#30340;&#33539;&#22260;&#20869;&#30340;&#28151;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#26089;&#26399;&#21040;&#20013;&#26399;&#28151;&#21709;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A two-stage lightweight online dereverberation algorithm for hearing devices is presented in this paper. The approach combines a multi-channel multi-frame linear filter with a single-channel single-frame post-filter. Both components rely on power spectral density (PSD) estimates provided by deep neural networks (DNNs). By deriving new metrics analyzing the dereverberation performance in various time ranges, we confirm that directly optimizing for a criterion at the output of the multi-channel linear filtering stage results in a more efficient dereverberation as compared to placing the criterion at the output of the DNN to optimize the PSD estimation. More concretely, we show that training this stage end-to-end helps further remove the reverberation in the range accessible to the filter, thus increasing the \textit{early-to-moderate} reverberation ratio. We argue and demonstrate that it can then be well combined with a post-filtering stage to efficiently suppress the residual late rever
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#25991;&#26412;&#30340;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#32626;&#24694;&#24847;&#21442;&#25968;&#21521;&#37327;&#26469;&#25581;&#31034;&#31169;&#20154;&#29992;&#25143;&#25991;&#26412;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;mini-batches&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#29992;&#25143;&#21644;&#38271;&#24207;&#21015;&#65292;&#25552;&#31034;&#20102;&#25991;&#26412;&#39046;&#22495;&#30340;FL&#27604;&#20808;&#21069;&#35748;&#20026;&#30340;&#26356;&#33030;&#24369;&#12290;</title><link>http://arxiv.org/abs/2201.12675</link><description>&lt;p&gt;
&#30772;&#22351;&#32773;&#65306;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21464;&#24418;&#37329;&#21018;&#38544;&#31169;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models. (arXiv:2201.12675v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#25991;&#26412;&#30340;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#32626;&#24694;&#24847;&#21442;&#25968;&#21521;&#37327;&#26469;&#25581;&#31034;&#31169;&#20154;&#29992;&#25143;&#25991;&#26412;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;mini-batches&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#29992;&#25143;&#21644;&#38271;&#24207;&#21015;&#65292;&#25552;&#31034;&#20102;&#25991;&#26412;&#39046;&#22495;&#30340;FL&#27604;&#20808;&#21069;&#35748;&#20026;&#30340;&#26356;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#30340;&#26680;&#24515;&#21407;&#21017;&#26159;&#22312;&#19981;&#38598;&#20013;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#24378;&#35843;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;FL&#20013;&#20351;&#29992;&#30340;&#26799;&#24230;&#26356;&#26032;&#21487;&#33021;&#27844;&#38706;&#29992;&#25143;&#20449;&#24687;&#12290;&#23613;&#31649;FL&#22312;&#25991;&#26412;&#24212;&#29992;&#39046;&#22495;&#65288;&#20363;&#22914;&#20987;&#38190;&#39044;&#27979;&#65289;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#23545;&#20110;FL&#38544;&#31169;&#30340;&#20960;&#20046;&#25152;&#26377;&#25915;&#20987;&#37117;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#32626;&#24694;&#24847;&#21442;&#25968;&#21521;&#37327;&#26469;&#25581;&#31034;&#31169;&#20154;&#29992;&#25143;&#25991;&#26412;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#22320;&#36827;&#34892;mini-batches&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#29992;&#25143;&#21644;&#38271;&#24207;&#21015;&#12290;&#19982;&#20197;&#24448;&#38024;&#23545;FL&#30340;&#25915;&#20987;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#31181;&#25915;&#20987;&#21033;&#29992;&#20102;Transformer&#26550;&#26500;&#21644;&#26631;&#35760;&#23884;&#20837;(token embedding)&#30340;&#29305;&#24615;&#65292;&#20998;&#21035;&#25552;&#21462;&#26631;&#35760;&#21644;&#20301;&#32622;&#23884;&#20837;&#20197;&#26816;&#32034;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#25991;&#26412;&#39046;&#22495;&#30340;FL&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#33021;&#22815;&#25269;&#24481;&#38544;&#31169;&#25915;&#20987;&#65292;&#20294;&#27604;&#20808;&#21069;&#35748;&#20026;&#30340;&#26356;&#21152;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#20801;&#35768;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;ML&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#12290;&#22240;&#20026;&#36825;&#20123;&#35831;&#27714;&#21487;&#33021;&#20250;&#28041;&#21450;&#21040;&#26080;&#27861;&#35775;&#38382;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.05629</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning. (arXiv:2201.05629v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05629
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#20801;&#35768;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;ML&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#12290;&#22240;&#20026;&#36825;&#20123;&#35831;&#27714;&#21487;&#33021;&#20250;&#28041;&#21450;&#21040;&#26080;&#27861;&#35775;&#38382;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#38544;&#31169;&#27861;&#35268;&#36171;&#20104;&#20844;&#27665;&#34987;&#20135;&#21697;&#12289;&#26381;&#21153;&#21644;&#20844;&#21496;&#36951;&#24536;&#30340;&#26435;&#21033;&#12290;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#32780;&#35328;&#65292;&#36825;&#38656;&#35201;&#20174;&#23384;&#20648;&#24402;&#26723;&#21644;ML&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#12290;&#30001;&#20110;ML&#24212;&#29992;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26426;&#22120;&#36951;&#24536;&#24050;&#25104;&#20026;&#19968;&#20010;&#19981;&#26029;&#20986;&#29616;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#34987;&#36951;&#24536;&#35831;&#27714;&#20197;&#21024;&#38500;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;ML&#27169;&#22411;&#20013;&#30340;&#19968;&#23450;&#38598;&#21512;&#25110;&#31867;&#21035;&#30340;&#25968;&#25454;&#30340;&#24418;&#24335;&#25552;&#20986;&#12290;&#23454;&#38469;&#32771;&#34385;&#38459;&#27490;&#20002;&#24323;&#21024;&#38500;&#30340;&#25968;&#25454;&#21518;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#23569;&#25968;&#30740;&#31350;&#20351;&#29992;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#25110;&#22312;&#35757;&#32451;&#26399;&#38388;&#23384;&#20648;&#30340;&#19968;&#20123;&#20803;&#25968;&#25454;&#26356;&#26032;&#36951;&#24536;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#36807;&#31243;&#25110;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#65292;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#23454;&#29616;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, in many cases, no data related to the training process or training samples may be accessible for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26368;&#22823;&#29109;&#21644;&#38169;&#35823;&#39044;&#27979;&#19978;&#20351;&#29992;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;MEEP&#65289;&#65292;&#36890;&#36807;&#24809;&#32602;&#33258;&#20449;&#24230;&#36807;&#39640;&#30340;&#39044;&#27979;&#26469;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.12218</link><description>&lt;p&gt;
&#22312;&#38169;&#35823;&#39044;&#27979;&#19978;&#20351;&#29992;&#26368;&#22823;&#29109;(MEEP)&#65306;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation. (arXiv:2112.12218v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26368;&#22823;&#29109;&#21644;&#38169;&#35823;&#39044;&#27979;&#19978;&#20351;&#29992;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;MEEP&#65289;&#65292;&#36890;&#36807;&#24809;&#32602;&#33258;&#20449;&#24230;&#36807;&#39640;&#30340;&#39044;&#27979;&#26469;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#35266;&#23519;&#21040;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#29978;&#33267;&#22312;&#39640;&#19981;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65292;&#23548;&#33268;&#27169;&#22411;&#26657;&#20934;&#19981;&#33391;&#12289;&#19981;&#21487;&#38752;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38169;&#35823;&#39044;&#27979;&#65288;MEEP&#65289;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#20998;&#21106;&#32593;&#32476;&#65292;&#23427;&#26377;&#36873;&#25321;&#22320;&#23545;&#33258;&#20449;&#24230;&#36807;&#39640;&#30340;&#39044;&#27979;&#36827;&#34892;&#24809;&#32602;&#65292;&#20165;&#20851;&#27880;&#38169;&#35823;&#20998;&#31867;&#30340;&#20687;&#32032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#31070;&#32463;&#32467;&#26500;&#19981;&#21152;&#20559;&#35265;&#65292;&#19981;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#21487;&#19982;&#22810;&#20010;&#20998;&#21106;&#25439;&#22833;&#20989;&#25968;&#37197;&#23545;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65306;&#33041;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#30340;&#30333;&#36136;&#39640;&#20449;&#21495;&#30149;&#21464;&#21644;&#24515;&#33039;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#30340;&#24515;&#25151;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;MEEP&#19982;&#26631;&#20934;&#20998;&#21106;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#22312;&#27169;&#22411;&#26657;&#20934;&#26041;&#38754;&#65292;&#32780;&#19988;&#22312;&#20998;&#21106;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#30456;&#27604;&#20110;&#22522;&#32447;&#27169;&#22411;&#37117;&#20250;&#24102;&#26469;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks achieved remarkable progress in medical image segmentation tasks. However, it has recently been observed that they tend to produce overconfident estimates, even in situations of high uncertainty, leading to poorly calibrated and unreliable models. In this work we introduce Maximum Entropy on Erroneous Predictions (MEEP), a training strategy for segmentation networks which selectively penalizes overconfident predictions, focusing only on misclassified pixels. Our method is agnostic to the neural architecture, does not increase model complexity and can be coupled with multiple segmentation loss functions. We benchmark the proposed strategy in two challenging segmentation tasks: white matter hyperintensity lesions in magnetic resonance images (MRI) of the brain, and atrial segmentation in cardiac MRI. The experimental results demonstrate that coupling MEEP with standard segmentation losses leads to improvements not only in terms of model calibration, but also i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#38382;&#39064;&#21644;&#24674;&#22797;&#27010;&#29575;&#20998;&#24067;&#65292;&#21487;&#20197;&#30830;&#23450;&#21407;&#26412;&#26080;&#27861;&#30830;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2112.11602</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#26377;&#38480;&#20840;&#23616;&#28151;&#28102;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Inference Despite Limited Global Confounding via Mixture Models. (arXiv:2112.11602v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#38382;&#39064;&#21644;&#24674;&#22797;&#27010;&#29575;&#20998;&#24067;&#65292;&#21487;&#20197;&#30830;&#23450;&#21407;&#26412;&#26080;&#27861;&#30830;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#26159;&#19968;&#32452;$n$&#20010;&#38543;&#26426;&#21464;&#37327;&#65288;&#22270;&#30340;&#39030;&#28857;&#65289;&#19978;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;; &#36125;&#21494;&#26031;&#32593;&#32476;&#20998;&#24067;&#65288;BND&#65289;&#26159;&#22312;&#22270;&#19978;&#39532;&#23572;&#21487;&#22827;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#26377;&#38480;$k$-&#28151;&#21512;&#30001;&#19968;&#20010;&#26356;&#22823;&#30340;&#22270;&#24418;&#24335;&#21270;&#34920;&#31034;&#65292;&#35813;&#22270;&#20855;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#38544;&#34255;&#8221;&#65288;&#25110;&#8220;&#28508;&#22312;&#8221;&#65289;&#38543;&#26426;&#21464;&#37327;$U$&#65292;&#20854;&#33539;&#22260;&#20026;$\{1,\ldots,k\}$&#65292;&#24182;&#19988;$U$&#21040;&#27599;&#20010;&#20854;&#20182;&#39030;&#28857;&#37117;&#26377;&#19968;&#20010;&#26377;&#21521;&#36793;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#26159;&#22522;&#26412;&#30340;&#65292;&#20854;&#20013;$U$&#27169;&#25311;&#20102;&#22810;&#20010;&#32676;&#20307;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#20351;&#24471;&#21487;&#35266;&#23519;&#30340;DAG&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#27169;&#31946;&#19981;&#28165;&#12290;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#38382;&#39064;&#24182;&#24674;&#22797;$U$&#19978;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20256;&#32479;&#19978;&#26080;&#27861;&#30830;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#21487;&#30830;&#23450;&#12290;&#36890;&#36807;&#23558;&#20854;&#32422;&#21270;&#20026;&#26356;&#20026;&#30740;&#31350;&#30340;&#8220;&#31354;&#8221;&#22270;&#20013;&#30340;&#8220;&#20056;&#31215;&#8221;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23398;&#20064;&#38750;&#31354;DAG&#30340;&#28151;&#21512;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ random variables (the vertices); a Bayesian Network Distribution (BND) is a probability distribution on the random variables that is Markovian on the graph. A finite $k$-mixture of such models is graphically represented by a larger graph which has an additional "hidden" (or "latent") random variable $U$, ranging in $\{1,\ldots,k\}$, and a directed edge from $U$ to every other vertex. Models of this type are fundamental to causal inference, where $U$ models an unobserved confounding effect of multiple populations, obscuring the causal relationships in the observable DAG. By solving the mixture problem and recovering the joint probability distribution on $U$, traditionally unidentifiable causal relationships become identifiable. Using a reduction to the more well-studied "product" case on empty graphs, we give the first algorithm to learn mixtures of non-empty DAGs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#29992;MMD&#33539;&#25968;&#25511;&#21046;Wasserstein&#36317;&#31163;&#30340;&#26465;&#20214;&#65292;&#38024;&#23545;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#25552;&#20986;&#20102;HLRIP&#23646;&#24615;&#65292;&#36890;&#36807;&#23548;&#20986;&#30340;&#26032;&#26680;&#33539;&#25968;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;Wasserstein&#36317;&#31163;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2112.00423</link><description>&lt;p&gt;
&#29992;&#26680;&#33539;&#25968;&#25511;&#21046;Wasserstein&#36317;&#31163;&#65292;&#24182;&#22312;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Controlling Wasserstein Distances by Kernel Norms with Application to Compressive Statistical Learning. (arXiv:2112.00423v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.00423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#29992;MMD&#33539;&#25968;&#25511;&#21046;Wasserstein&#36317;&#31163;&#30340;&#26465;&#20214;&#65292;&#38024;&#23545;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#25552;&#20986;&#20102;HLRIP&#23646;&#24615;&#65292;&#36890;&#36807;&#23548;&#20986;&#30340;&#26032;&#26680;&#33539;&#25968;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;Wasserstein&#36317;&#31163;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#27604;&#36739;&#27010;&#29575;&#20998;&#24067;&#26159;&#20851;&#38190;&#12290;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#21644;Wasserstein&#36317;&#31163;&#26159;&#20004;&#31867;&#27010;&#29575;&#20998;&#24067;&#36317;&#31163;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20123;&#26465;&#20214;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;MMD&#33539;&#25968;&#26469;&#25511;&#21046;Wasserstein&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21463;&#21040;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#65288;CSL&#65289;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#36164;&#28304;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#22312;&#21333;&#20010;&#21521;&#37327;&#65288;&#31216;&#20026;&#33609;&#22270;&#65289;&#20013;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#25429;&#25417;&#19982;&#32771;&#34385;&#30340;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;CSL&#32467;&#26524;&#30340;&#21551;&#21457;&#19979;&#24341;&#20837;&#20102;H\"older Lower Restricted Isometric Property&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#23646;&#24615;&#23545;&#20110;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#20855;&#26377;&#26377;&#36259;&#30340;&#20445;&#35777;&#12290;&#22522;&#20110;MMD&#21644;Wasserstein&#36317;&#31163;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;MMD&#23548;&#20986;&#30340;&#26032;&#26680;&#33539;&#25968;&#30340;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20026;&#22312;&#21387;&#32553;&#32479;&#35745;&#23398;&#20064;&#20013;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#20004;&#27493;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;Wasserstein&#36317;&#31163;&#30456;&#23545;&#20110;CSL&#20013;&#20854;&#20182;&#24120;&#29992;&#36317;&#31163;&#30340;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing probability distributions is at the crux of many machine learning algorithms. Maximum Mean Discrepancies (MMD) and Wasserstein distances are two classes of distances between probability distributions that have attracted abundant attention in past years. This paper establishes some conditions under which the Wasserstein distance can be controlled by MMD norms. Our work is motivated by the compressive statistical learning (CSL) theory, a general framework for resource-efficient large scale learning in which the training data is summarized in a single vector (called sketch) that captures the information relevant to the considered learning task. Inspired by existing results in CSL, we introduce the H\"older Lower Restricted Isometric Property and show that this property comes with interesting guarantees for compressive statistical learning. Based on the relations between the MMD and the Wasserstein distances, we provide guarantees for compressive statistical learning by introduci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#26377;&#25928;&#30340;&#26426;&#22120;&#25968;&#25454;&#36951;&#24536;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#35823;&#24046;&#26368;&#22823;&#21270;&#30340;&#22122;&#22768;&#29983;&#25104;&#21644;&#25439;&#20260;-&#20462;&#22797;&#30340;&#26435;&#37325;&#25805;&#20316;&#26469;&#21024;&#38500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#29305;&#23450;&#25968;&#25454;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2111.08947</link><description>&lt;p&gt;
&#24555;&#36895;&#26377;&#25928;&#30340;&#26426;&#22120;&#25968;&#25454;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Fast Yet Effective Machine Unlearning. (arXiv:2111.08947v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#26377;&#25928;&#30340;&#26426;&#22120;&#25968;&#25454;&#36951;&#24536;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#35823;&#24046;&#26368;&#22823;&#21270;&#30340;&#22122;&#22768;&#29983;&#25104;&#21644;&#25439;&#20260;-&#20462;&#22797;&#30340;&#26435;&#37325;&#25805;&#20316;&#26469;&#21024;&#38500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#29305;&#23450;&#25968;&#25454;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#36951;&#24536;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#21152;&#24378;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#25105;&#20204;&#33021;&#21542;&#22312;&#19981;&#26597;&#30475;&#23436;&#25972;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#21333;&#20010;&#25110;&#22810;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#65311;&#65288;ii&#65289;&#25105;&#20204;&#33021;&#21542;&#20351;&#24555;&#36895;&#30340;&#36951;&#24536;&#36807;&#31243;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#28145;&#24230;&#32593;&#32476;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#25968;&#25454;&#36951;&#24536;&#26694;&#26550;&#65292;&#37319;&#29992;&#35823;&#24046;&#26368;&#22823;&#21270;&#30340;&#22122;&#22768;&#29983;&#25104;&#21644;&#25439;&#20260;-&#20462;&#22797;&#30340;&#26435;&#37325;&#25805;&#20316;&#65292;&#20026;&#19978;&#36848;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#38024;&#23545;&#24453;&#36951;&#24536;&#31867;&#21035;&#30340;&#35823;&#24046;&#26368;&#22823;&#21270;&#22122;&#22768;&#30697;&#38453;&#65292;&#24182;&#21033;&#29992;&#35813;&#22122;&#22768;&#30697;&#38453;&#25805;&#20316;&#27169;&#22411;&#26435;&#37325;&#20197;&#36951;&#24536;&#30446;&#26631;&#31867;&#21035;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#25439;&#20260;&#21644;&#20462;&#22797;&#27493;&#39588;&#26469;&#25511;&#21046;&#32593;&#32476;&#26435;&#37325;&#30340;&#25805;&#20316;&#12290;&#22312;&#25439;&#20260;&#27493;&#39588;&#20013;&#65292;&#23558;&#22122;&#22768;&#30697;&#38453;&#24212;&#29992;&#20110;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#23545;&#24212;&#30340;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#12290;&#22312;&#20462;&#22797;&#27493;&#39588;&#20013;&#65292;&#20351;&#29992;&#21097;&#20313;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#25805;&#20316;&#21518;&#30340;&#26435;&#37325;&#36827;&#34892;&#24494;&#35843;&#12290;&#21508;&#31181;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlearning the data observed during the training of a machine learning (ML) model is an important task that can play a pivotal role in fortifying the privacy and security of ML-based applications. This paper raises the following questions: (i) can we unlearn a single or multiple class(es) of data from a ML model without looking at the full training data even once? (ii) can we make the process of unlearning fast and scalable to large datasets, and generalize it to different deep networks? We introduce a novel machine unlearning framework with error-maximizing noise generation and impair-repair based weight manipulation that offers an efficient solution to the above questions. An error-maximizing noise matrix is learned for the class to be unlearned using the original model. The noise matrix is used to manipulate the model weights to unlearn the targeted class of data. We introduce impair and repair steps for a controlled manipulation of the network weights. In the impair step, the noise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#32479;&#35745;&#21327;&#20316;&#31639;&#27861;&#26694;&#26550;&#65292;&#31649;&#29702;&#20248;&#21270;&#35823;&#24046;&#36890;&#37327;&#21644;&#28436;&#21270;&#20013;&#30340;&#32479;&#35745;&#35823;&#24046;&#36890;&#37327;&#12290;&#35813;&#26694;&#26550;&#36890;&#29992;&#24615;&#24378;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#20989;&#25968;&#21644;&#20998;&#21306;&#26063;&#65292;&#24182;&#21551;&#21457;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.09215</link><description>&lt;p&gt;
&#38754;&#21521;&#36890;&#29992;&#21644;&#39640;&#25928;&#40657;&#30418;&#20248;&#21270;&#30340;&#26368;&#20248;&#32479;&#35745;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Optimum-statistical Collaboration Towards General and Efficient Black-box Optimization. (arXiv:2106.09215v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#32479;&#35745;&#21327;&#20316;&#31639;&#27861;&#26694;&#26550;&#65292;&#31649;&#29702;&#20248;&#21270;&#35823;&#24046;&#36890;&#37327;&#21644;&#28436;&#21270;&#20013;&#30340;&#32479;&#35745;&#35823;&#24046;&#36890;&#37327;&#12290;&#35813;&#26694;&#26550;&#36890;&#29992;&#24615;&#24378;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#20989;&#25968;&#21644;&#20998;&#21306;&#26063;&#65292;&#24182;&#21551;&#21457;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20998;&#23618;&#36172;&#21338;&#26426;&#24335;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#20013;&#20998;&#36776;&#29575;&#21644;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#36827;&#34892;&#20851;&#38190;&#38416;&#36848;&#65292;&#24341;&#23548;&#26356;&#20026;&#36890;&#29992;&#30340;&#20998;&#26512;&#21644;&#26356;&#39640;&#25928;&#30340;&#31639;&#27861;&#35774;&#35745;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#20248;&#32479;&#35745;&#21327;&#20316;&#65292;&#19968;&#31181;&#31649;&#29702;&#20248;&#21270;&#35823;&#24046;&#36890;&#37327;&#21644;&#20248;&#21270;&#36807;&#31243;&#20013;&#28436;&#21270;&#30340;&#32479;&#35745;&#35823;&#24046;&#36890;&#37327;&#30456;&#20114;&#20316;&#29992;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27492;&#26694;&#26550;&#30340;&#36890;&#29992;&#20998;&#26512;&#65292;&#32780;&#19981;&#38656;&#35201;&#25351;&#23450;&#32479;&#35745;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#30340;&#24418;&#24335;&#12290;&#30001;&#20110;&#20854;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#20998;&#26512;&#21487;&#24212;&#29992;&#20110;&#28385;&#36275;&#19981;&#21516;&#23616;&#37096;&#24179;&#28369;&#24615;&#20551;&#35774;&#21644;&#20855;&#26377;&#19981;&#21516;&#23616;&#37096;&#26368;&#20248;&#20540;&#25968;&#37327;&#30340;&#22823;&#37327;&#20989;&#25968;&#21644;&#20998;&#21306;&#26063;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#20316;&#21697;&#25152;&#30740;&#31350;&#30340;&#20989;&#25968;&#31867;&#35201;&#20016;&#23500;&#24471;&#22810;&#12290;&#35813;&#26694;&#26550;&#36824;&#21551;&#21457;&#25105;&#20204;&#25552;&#20986;&#26356;&#22909;&#30340;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#26041;&#27861;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we make the key delineation on the roles of resolution and statistical uncertainty in hierarchical bandits-based black-box optimization algorithms, guiding a more general analysis and a more efficient algorithm design. We introduce the \textit{optimum-statistical collaboration}, an algorithm framework of managing the interaction between optimization error flux and statistical error flux evolving in the optimization process. We provide a general analysis of this framework without specifying the forms of statistical error and uncertainty quantifier. Our framework and its analysis, due to their generality, can be applied to a large family of functions and partitions that satisfy different local smoothness assumptions and have different numbers of local optimums, which is much richer than the class of functions studied in prior works. Our framework also inspires us to propose a better measure of the statistical uncertainty and consequently a variance-adaptive algorithm \text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26641;&#27169;&#22411;&#20013;&#35745;&#31639;Shapley&#20540;&#30340;&#20004;&#31181;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#22120;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#26641;&#32467;&#26500;&#65292;&#24182;&#25506;&#35752;&#20102;Shapley&#20540;&#20316;&#20026;&#23616;&#37096;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.03820</link><description>&lt;p&gt;
&#35299;&#37322;&#26641;&#27169;&#22411;&#30340;&#20934;&#30830;Shapley&#20540;
&lt;/p&gt;
&lt;p&gt;
Accurate Shapley Values for explaining tree-based models. (arXiv:2106.03820v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26641;&#27169;&#22411;&#20013;&#35745;&#31639;Shapley&#20540;&#30340;&#20004;&#31181;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#22120;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#26641;&#32467;&#26500;&#65292;&#24182;&#25506;&#35752;&#20102;Shapley&#20540;&#20316;&#20026;&#23616;&#37096;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#24191;&#27867;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#20272;&#35745;&#21644;&#35299;&#37322;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#25512;&#35770;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;Shapley&#20540;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#26641;&#32467;&#26500;&#39640;&#25928;&#22320;&#35745;&#31639;Shapley&#20540;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#27169;&#25311;&#21644;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25910;&#30410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Shapley&#20540;&#20316;&#20026;&#23616;&#37096;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;Python&#21253;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley Values (SV) are widely used in explainable AI, but their estimation and interpretation can be challenging, leading to inaccurate inferences and explanations. As a starting point, we remind an invariance principle for SV and derive the correct approach for computing the SV of categorical variables that are particularly sensitive to the encoding used. In the case of tree-based models, we introduce two estimators of Shapley Values that exploit the tree structure efficiently and are more accurate than state-of-the-art methods. Simulations and comparisons are performed with state-of-the-art algorithms and show the practical gain of our approach. Finally, we discuss the limitations of Shapley Values as a local explanation. These methods are available as a Python package.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#29366;&#24577;&#21644;&#21160;&#20316;&#36873;&#39033;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36873;&#39033;&#30340;&#32456;&#27490;&#26465;&#20214;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#21040;&#30340;&#36873;&#39033;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2010.02756</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#32456;&#27490;&#35780;&#20272;&#23398;&#20064;&#22810;&#26679;&#21270;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Options via InfoMax Termination Critic. (arXiv:2010.02756v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.02756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#29366;&#24577;&#21644;&#21160;&#20316;&#36873;&#39033;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36873;&#39033;&#30340;&#32456;&#27490;&#26465;&#20214;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#21040;&#30340;&#36873;&#39033;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#21160;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#25345;&#32493;&#24615;&#21160;&#20316;&#36873;&#39033;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#36873;&#39033;&#21487;&#20197;&#20316;&#20026;&#21487;&#37325;&#29992;&#30340;&#26500;&#24314;&#27169;&#22359;&#26469;&#21152;&#36895;&#36801;&#31227;&#23398;&#20064;&#65292;&#20294;&#26159;&#20026;&#26410;&#30693;&#20219;&#21153;&#20998;&#24067;&#23398;&#20064;&#21487;&#37325;&#29992;&#36873;&#39033;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#21463;&#21040;&#22522;&#20110;&#20114;&#20449;&#24687;&#25216;&#26415;&#30340;&#25216;&#33021;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#36890;&#36807;&#26368;&#22823;&#21270;&#36873;&#39033;&#21644;&#23545;&#24212;&#29366;&#24577;&#36716;&#25442;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36873;&#39033;&#30340;&#32456;&#27490;&#26465;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#23548;&#20986;&#20102;&#36825;&#31181;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#21487;&#25193;&#23637;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#31216;&#20043;&#20026;InfoMax Termination Critic&#65288;IMTC&#65289;&#31639;&#27861;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IMTC&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#36873;&#39033;&#30340;&#22810;&#26679;&#24615;&#65292;&#19988;&#19981;&#38656;&#35201;&#22806;&#37096;&#22870;&#21169;&#65292;&#20165;&#32467;&#21512;&#20869;&#22312;&#30340;&#36873;&#39033;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#36873;&#39033;&#36716;&#31227;&#21040;&#21508;&#31181;&#20219;&#21153;&#20013;&#27979;&#35797;&#20854;&#21487;&#37325;&#29992;&#24615;&#65292;&#35777;&#23454;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of autonomously learning reusable temporally extended actions, or options, in reinforcement learning. While options can speed up transfer learning by serving as reusable building blocks, learning reusable options for unknown task distribution remains challenging. Motivated by the recent success of mutual information (MI) based skill learning, we hypothesize that more diverse options are more reusable. To this end, we propose a method for learning termination conditions of options by maximizing MI between options and corresponding state transitions. We derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. Our experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards combined with an intrinsic option learning method. Moreover, we test the reusability of learned options by transferring options into various tasks, confirming that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20026;2&#30340;&#24102;&#38480;&#21046;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#23450;&#20102;&#24403;&#38544;&#34255;&#21442;&#25968;&#20998;&#24067;&#20110;&#26377;&#30028;&#22495;&#26102;&#65292;&#32593;&#32476;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#38646;&#36924;&#36817;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2008.08427</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#24102;&#38480;&#21046;&#30340;&#38543;&#26426;&#26435;&#37325;&#26377;&#22810;&#22823;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Powerful are Shallow Neural Networks with Bandlimited Random Weights?. (arXiv:2008.08427v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.08427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20026;2&#30340;&#24102;&#38480;&#21046;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#23450;&#20102;&#24403;&#38544;&#34255;&#21442;&#25968;&#20998;&#24067;&#20110;&#26377;&#30028;&#22495;&#26102;&#65292;&#32593;&#32476;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#38646;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#20026;2&#30340;&#24102;&#38480;&#21046;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#38543;&#26426;&#32593;&#32476;&#26159;&#25351;&#38544;&#34255;&#23618;&#21442;&#25968;&#34987;&#20923;&#32467;&#24182;&#36171;&#20104;&#38543;&#26426;&#20998;&#37197;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21482;&#26377;&#36755;&#20986;&#23618;&#21442;&#25968;&#36890;&#36807;&#25439;&#22833;&#26368;&#23567;&#21270;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;&#38543;&#26426;&#26435;&#37325;&#30340;&#38544;&#34255;&#23618;&#26159;&#36991;&#20813;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#24050;&#34987;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#25152;&#37319;&#29992;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#26159;&#26222;&#36866;&#36924;&#36817;&#22120;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20107;&#23454;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#24403;&#38544;&#34255;&#21442;&#25968;&#20998;&#24067;&#20110;&#26377;&#30028;&#22495;&#26102;&#65292;&#32593;&#32476;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#38646;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#29305;&#21035;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#24179;&#20961;&#36924;&#36817;&#35823;&#24046;&#19979;&#30028;&#12290;&#35777;&#26126;&#21033;&#29992;&#20102;Ridgelet&#20998;&#26512;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#35856;&#27874;&#20998;&#26512;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#20102;&#32463;&#20856;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#29305;&#21035;&#26159;&#20449;&#21495;&#22312;&#26576;&#31181;&#38480;&#21046;&#19979;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the expressive power of depth-2 bandlimited random neural networks. A random net is a neural network where the hidden layer parameters are frozen with random assignment, and only the output layer parameters are trained by loss minimization. Using random weights for a hidden layer is an effective method to avoid non-convex optimization in standard gradient descent learning. It has also been adopted in recent deep learning theories. Despite the well-known fact that a neural network is a universal approximator, in this study, we mathematically show that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error. In particular, we derive a new nontrivial approximation error lower bound. The proof utilizes the technique of ridgelet analysis, a harmonic analysis method designed for neural networks. This method is inspired by fundamental principles in classical signal processing, specifically the idea that signals with limit
&lt;/p&gt;</description></item></channel></rss>